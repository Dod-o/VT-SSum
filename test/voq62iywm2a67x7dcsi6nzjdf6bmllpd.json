{
    "id": "voq62iywm2a67x7dcsi6nzjdf6bmllpd",
    "title": "Reductions in Machine Learning",
    "info": {
        "author": [
            "Alina Beygelzimer, IBM Thomas J. Watson Research Center",
            "Bianca Zadrozny, Computer Science Department, Fluminense Federal University",
            "John Langford, Microsoft Research"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_beygelzimer_zadrozny_langford_riml/",
    "segmentation": [
        [
            "OK, good morning and welcome to the machine Learning Reductions tutorial.",
            "My name is Bianca said Rosie I'm from Fluminense Federal University in Brazil and the other speakers are going to be Alina Bagels.",
            "Emeran John Langford from IBM Research and Yahoo Research, so I'm going to be the first speaker.",
            "I'm going to talk for about 40 minutes and then we're going to have a short break and then move on to Alina and then John.",
            "So, um.",
            "To move here with.",
            "Stand here.",
            "Oh OK, I'll try to speak up.",
            "I thought them I could be.",
            "There are a microphone for the front.",
            "I'll move the mic closer.",
            "Oh, the mic is only for the King, OK?",
            "I'll speak louder, so um.",
            "What is going to say is that you are welcome to ask questions, since this is a tutorial, we're hoping that people interact with us, so you're welcome to ask questions L as long as I go and not in the end.",
            "So."
        ],
        [
            "So we know that in practice applying machine learning is often difficult because we have algorithms and tool boxes for solving problems that are like the standard machine learning problems like classification, and most often we have applications that require more complex types of problems such as cost sensitive classification, hierarchical classification, structure classification, then going on to even more complex problems like machine translation, and we'd like to scale up these.",
            "Algorithms that we have to be able to solve these more complex problems like ranking etc.",
            "So this story is going to be about 1 possible approach for moving up from simple.",
            "The simple algorithms that we have to be able to solve this more complex problems."
        ],
        [
            "So actually we would have like 2 approaches that we can think about to solve these more complex problems which would be designed you algorithms from scratch or try to modify the algorithms in some way to be able to apply them to these more.",
            "Complex problems.",
            "Which is fine.",
            "Is one line of research and the but the approach that we're talking about today is the 2nd approach, which is about how we can use reuse the old algorithms in a way in a robust way.",
            "To achieve good performance for these other, more complex problems.",
            "So we'd like to be able to reuse what we've done in the past in a formal fashion that we will guarantee performance and.",
            "So this is tutorial is going to be about the second approach.",
            "How we can do that?",
            "How can we reuse algorithms for solving simple machine learning problems to scale them up to more?",
            "Complex problems."
        ],
        [
            "So what is the simple problem that I'm talking about?",
            "So today we're going to focus as our core problem as the one that we know how to solve is going to be binary classification.",
            "And to make things very clear.",
            "Here's our definition of what is binary classification, so this is what we expecting that we know how to do well.",
            "So expecting that we have good algorithms for solving this problem.",
            "In fact, this is a problem that most people have focused for a long time.",
            "For the last 4050 or so.",
            "What we are.",
            "Calling the binary classification problem is a problem where we have a finite set of training.",
            "Examples of the form X1Y one through XN YYM, and we'd like to produce a classifier.",
            "Which you call age, which is a mapping from X 201 S 0 instead.",
            "So we assume that the Y values are binary, so there are 01.",
            "And the X is an arbitrary space which most often will be like a vector space or feature space, but we are not requiring that in the definition, so.",
            "We're leaving that open, so depending on the matter, they're going to use X could be different, so here we don't.",
            "We're not making any assumptions about the data except that we are assuming that.",
            "We're gonna want to.",
            "There's an unknown distribution D. / X and 01.",
            "So it's a joint distribution over X and 01 and we want to minimize.",
            "Find the classifier that minimizes the loss on this same distribution.",
            "So we're not making any extra assumptions if we make extra assumptions which the algorithms will make, then they're going to carry through what we do later with the algorithm.",
            "To begin with, we are requiring this as the minimum that we are requiring that the algorithm will do will do this if the algorithm does this, then we can guarantee other things that we're going to show later.",
            "So we the algorithm is in one wants to find an age which is the hypothesis with small 01 loss so.",
            "And the losses over the same distribution which we draw the training data from.",
            "And we want to find an age will which leads to to a small spectral loss or the error difference between the predicted value.",
            "This age of X, which would be the prediction and why?",
            "Which is the true label.",
            "So this is our core problem that we are assuming that we know how to solve.",
            "Of course it's not always that easy, but we assume that we have that done and now we would like to scale.",
            "I mean use that as a basis for other solving other problems."
        ],
        [
            "So what we want from from a reduction?",
            "So I think most people when they think about even this name reduction sounds like simplistic that we want to get a difficult problem and make it be solve it with a simple thing.",
            "So it looks like we're simplifying the problem and losing something with this, but in fact we want the reduction to be robust, which means that we're going to use the binary classifier in such a way that we want to guarantee good performance on the problem that we care about.",
            "So we actually show that.",
            "When a reduction, we try to show that if we have good performance on the 01 last problem on the binary classification problem, then we show that we have good performance in the more complex problem we're reducing to classification.",
            "So it's not simply just getting the problem, making it look like classification and solving it using classification, which should lose.",
            "We've seen would be losing something now.",
            "In fact, we're guaranteed that we're carrying the performance, so we're not doing it in.",
            "Any fashion we're doing in a row?"
        ],
        [
            "Mustache.",
            "And second, they should be modular in the sense that we want to be able to plug any.",
            "Classifier any classifier learner that we have.",
            "And reuse them in within the reduction in a similar manner, so we don't want it to look inside what the like, what the algorithm is doing, where you're going to be using the algorithm as a black box and then build their reduction as composition of modules."
        ],
        [
            "So to start with, I'm going to talk about a very simple reduction.",
            "Which is a reduction from importance weighted classification to binary classification to standard binary classification.",
            "Then later I'm going to give another example of reduction which is a little bit more.",
            "Complicated then John and Arlene are going to talk about reductions from more complex problems to classification.",
            "So the first example we're going to see.",
            "Is this simple importance weighted classification?",
            "Example where.",
            "Aside from having the features X and the wise, we also have AC.",
            "Which is actually which is.",
            "You can think about is awaits, which is the importance of that particular example.",
            "So this comes up all the time in many as like many different algorithms.",
            "For instance, in boosting that you have to have weights for the examples.",
            "So and then in practice you can modify many standard algorithms to actually take these weights as input.",
            "So here we want to be able to get a classifier that then.",
            "Does not accept weights as input and then make it use the weights and we make it very precise what.",
            "Should do with these weights, right?",
            "Because?",
            "Now we have a distribution.",
            "The over acts which are the features 01 which are the labels and the value between zero and infinite, which is the weight or the cost.",
            "So for each example it has this associated weights and what we want to do.",
            "Is find a classifier that minimizes, instead of minimizing just the error, which would be the difference between?",
            "I mean how many times it gets this incorrect.",
            "The error is now.",
            "Multiplied with the C value, which means that if you make a mistake one.",
            "An example that has a bigger civil that would be a bigger cost, and if you haven't exactly make an error.",
            "In an example with this lower see value.",
            "So in practice, of course we would like to have all the examples correct, But if we can't have all the examples correct then we want to go for get the ones that have the higher seed values correct?",
            "So this is the last function that we want to minimize, so see, this is very similar to the classification problem.",
            "It's it adds a little bit more to the standard classification problem, which is the weight.",
            "And now we want to be able to solve this using standard classification algorithm.",
            "So this is a common scenario that they set up years many times in boosting for instance, but also it also is is a case where one class is rare and has a bigger cost than the other class.",
            "So these C values would be the same for all the examples belonging to the rare class and which they would be higher than the examples in the more frequent class.",
            "So this eval is here would be.",
            "You could make them for this problem that we often have diff having different costs for false positive versus false negatives.",
            "We can use this particular case also to solve that problem.",
            "So now we're gonna.",
            "OK, so people of course have seen these problems this problem before and they have other approaches besides the reduction approach to solve it, which the first one is actually to just change the particular classifier learner.",
            "So you go inside it and find a way for it to use the weights in some sense and then try to minimize the correct loss.",
            "So if you know a lot about your.",
            "Your classification algorithm in a lot of cases it can be easy to do that specially let's say in naive Bayes there's like standard ways you can just use the weights and they're going to.",
            "Give good results, but in a number of algorithms for instant decision trees it doesn't work very well when you introduce because of the way the algorithm was designed.",
            "Once you start introducing these weights, it doesn't do what you're expecting it would do in practice if you try to modify it.",
            "Because of heuristics and other things.",
            "So because there are sticks were based in the case where examples had were uniform.",
            "Like all they have the same weight when you start modifying the algorithm, it can do something weird.",
            "The second approach would be to actually move from classification to do like a two step.",
            "Approach of estimating.",
            "Um?",
            "The.",
            "School back later."
        ],
        [
            "Note that.",
            "You didn't say this, but for training we know the C values for the training examples, but we don't know the C values very often.",
            "For the test example, so we only know the C values for the training examples in the for the test examples we have the X values and we want to predict based on that right.",
            "We don't know the C values for the test data, that's why.",
            "Here.",
            "In this approach, we would estimate the probability of the example belonging to the positive class.",
            "An art and then it's made the probability of not.",
            "It's not belonging to the positive class and also estimating the costs of an example.",
            "So it would be a separate function, so it can given an X you would give us the cost.",
            "So this is like a regression problem and then we would combine them.",
            "And use Bayes optimal decision rule to decide which class you would go to.",
            "So this approach is not very robust because when you combine these two models there, the errors could like sum up and things like that so.",
            "And also with require estimating these probabilities, which is more difficult than classification and also estimating the costs which could be.",
            "Hard.",
            "And so we don't wanna see.",
            "Approach is not very robust.",
            "So the reductions are production approach pictorially would be to create."
        ],
        [
            "Some.",
            "Map our goal is to minimize the loss function indeed, so we want to create.",
            "And that box called R which get examples that are drawn from D, which is the distribution in the problem that we want to solve in this case, is the important sway did.",
            "Classification problems, so we want to be able to get examples from the.",
            "We want to minimize the L, which is that loss function over the importance weighted sample.",
            "And we want to be able to transform it through the reduction so that we can use.",
            "An algorithm from optimizing 01 loss, which is our algorithm, which is the algorithm that solves the binary classification core problem that I talked about before.",
            "So we transform D. The regional distribution into ADI Prime, which is a distribution that.",
            "If we apply this.",
            "Binary classification algorithm.",
            "Then we're going to get a hypothesis out of this, which is a classifier.",
            "Which optimizes 01 loss in the Deprime distribution, which is different than the D distribution, the regional distribution, and we can show that.",
            "Um?",
            "After we get this age, we can use the age in such a way that we call the RH.",
            "Function which here we would call it a our inverse function, which gets the predictions from the binary classifier and makes predictions in the original problem.",
            "So we call it R&R inverse because here with getting examples from the regional problem, transforming into examples for this standard problem binary classification, we get a binary classification hypothesis back.",
            "And now we transform it into the.",
            "Hypothesis with the reduction applied to it, which is the R age hypothesis.",
            "So in this simple example, the there is not even this are in first, so we we were.",
            "This is like a general picture of what the reductions would do.",
            "We get the examples in the regional distribution, transform them applied the binary.",
            "Learning and this could be done.",
            "More than one way more complex reductions and more.",
            "More times than here we we get back to the original problem.",
            "So we have.",
            "We are transforming age into our age.",
            "With small loss.",
            "Search that if age does well on the D prime distribution in terms of 01 loss, our age is going to be do well on the regional distribution with the L loss, which is the loss in the problem that we care about.",
            "The actual problem that we're trying to solve.",
            "So we not only want to do that, we want to do that guarantee that we're carrying through, like good performance in the 01 lost a good performance in the loss that we care about."
        ],
        [
            "So in this particular importance weighted classification problem, we can use a simple theorem that we call the distribution shift theorem.",
            "To derive a reduction.",
            "So the theorem says that for any importance weighted distribution D and any classifier age.",
            "If we let the prime.",
            "Or get a new distribution to prime.",
            "Bing importance weighted distribution so.",
            "The D prime is the weighted by C over the expected value of C. So this means we change from the original distribution to distribution of.",
            "Our examples appear with probability proportional to see.",
            "So we have a distribution of examples and now we want the examples to appear.",
            "Examples that have bigger way to appear more often than examples that have lower weight so.",
            "Ben, if we learn a classifier on the D prime distribution, then we're going to be minimizing.",
            "The L loss, which is the importance weighted loss if the classifier minimizes the 01 loss.",
            "So.",
            "This is kind of intuitive, so if you get the original training sample and now we replicate, I mean this would be the more intuitive way.",
            "This is not going to be the right way to do it, but if you have the original samples and say an example have cost 10 and you replicate it 10 times and one example has caused two and replicated two times.",
            "And one example has cost one.",
            "You keep it once in the training set and then you run a classifier.",
            "On that training sample, then it's going to pay more attention to the examples that appear more often.",
            "So then it's going to minimize the loss which has weights even though your algorithm is minimizing the 01 loss.",
            "So intuitively we can think about it.",
            "That way.",
            "You could or this is one way of solving it.",
            "Actually practice people do that, replicating the example here we just formalizing it's saying OK, this is a distribution shift, so we want to move from the distributed distribution, which is original distribution to ADI Prime where.",
            "Examples are weighted.",
            "And we minimizing.",
            "01 loss, so here's the proof, which I'm not going to go into.",
            "Detail is very simple proof, just showing in fact that if you minimize 01 loss on the distribution.",
            "Uh, which?",
            "Wherever example with me is weighted by C for minimize it on this distribution, then the same of minimizing this loss here, which is the last that we care about.",
            "And here's this distribution shift.",
            "So this is called distribution.",
            "So this is just formalizing what people know about, so even we call this like folk theorem.",
            "So it's like a very simple case where we know that if it's intuitive if you replicate the examples.",
            "Then the algorithm will change.",
            "Their behavior will focus more on the ones that are more important."
        ],
        [
            "So, but this will appear very often reductions because one key thing that we do is change the distribution of examples.",
            "So now how do we change the distribution of examples?",
            "So I was giving an example where we wanted to change the distribution examples.",
            "We are replicating example, so we want to have examples with more weight.",
            "We're going to replicate the examples.",
            "This is approach.",
            "Would be.",
            "Which is the approach that we call resampling with replacement, which would be equivalent to like a roulette wheel algorithm where you have a roulette wheel with the size here.",
            "Size here being proportional to the weight.",
            "And due to the weights of the examples, so we have like one part of for each example with weight with size proportional to the weight of the example and then you would sample examples proportionally to the weights and you allowed replicates.",
            "So you would go replicates would go select.",
            "Each one example at a time and then you would go through this wheel, selecting one example and you get examples proportionately to the way this would be equivalent to replicating, except that if you have real value costs then.",
            "You get the correct distribution from this.",
            "But the problem with this is that it actually gives you duplicate examples because it's sample with replacement so.",
            "Your one example may appear more than than once on your knew training sets, and especially the ones with bigger weights.",
            "They're going to appear very often, whereas the ones with lower weight are not going to appear.",
            "Some of them are not going to appear.",
            "So in practice, we see that this is not a good approach for solving the importance weighted problem because.",
            "A lot of example, a lot of algorithms cannot do well do well with these replicate examples, and in fact this makes the.",
            "Resample said not being drawn in the penalty firm from the prime, because if we draw if we had a D prime at Rudy Prime distribution where we could sample examples from where you would not expect to see so many duplicate examples of.",
            "You can see there's something wrong with that.",
            "Um so.",
            "A possible approach would be to do re sampling without replacement, which is sampling M examples from a set of size M using the same roulette wheel approach.",
            "So which means that once an example comes, you select that example, you wouldn't select that example again.",
            "And that is not a very good because because also, we're not going to have examples drawn from the prime in the case, for instance, where you draw am examples from the regional data set without replacement were going exactly the same training set back, so that's not.",
            "Good good approach.",
            "So the approach that we use more often when we want to change distributions in reduction."
        ],
        [
            "Is the rejection sampling approach.",
            "Where?",
            "And instead of doing re sampling with replacement or re sampling without replacement and getting examples randomly from the training sets, we actually.",
            "Go through all the examples and decide whether or not to keep the example with probability proportional to.",
            "See so it's pictorial.",
            "You can think about going through the training set and deciding to a keeper, not this example in the way you decide is flip a coin Ann, and compare with bias C / C, where C hat is.",
            "Would be like the maximum and upper bound value on on C, so you'd have to have an upper bound value on the possible values on your weights so that you can normalize.",
            "All this seems to be between zero and one.",
            "Then you randomly decide whether to keep example with probability of C. / C had you go through the training set and you get a new training set, which now is going to be much smaller than.",
            "The original training set because some examples will have.",
            "Very small weights and these examples are going to appear in the training set with very very small probability and some are going to have bigger weights which are going to be closer to 1.",
            "The probability that you're going to see them in the training set.",
            "So now you're going to have.",
            "Um?",
            "A much smaller training set, but at least you can.",
            "We have a re sample data set which is drawn independently from the head, which is what we want for the reduction.",
            "So if we use rejection sampling, we get the right distribution that we want for we can plug this knew training set into the origonal into the binary classifier learner, and we're going to learn from the correct distribution, minimizing the correct loss.",
            "So this is what we will want."
        ],
        [
            "So the algorithm actually that we are using practice.",
            "Um?",
            "Does the same does the rejection sampling for a number of times, so it's like a bagging style algorithm which we call costing which repeats this rejection sampling more than once.",
            "It does that Prince 10 times like it's says that and creates from the original training sets 10 different.",
            "Knew training sets which are drawn from the correct distribution through rejection sampling and we train a classifier for each one of these, so they're all doing the same thing which is minimizing the weighted loss function.",
            "And when we want to classify an example, we do a majority voting, so we get 10 to 10 different hypothesis on an example X and we make them vote.",
            "So this is the approach that we.",
            "This is what the reduction that we use.",
            "We could have this by being just one and then just run once and get.",
            "The hypothesis would be minimizing the correct.",
            "The correct distribution, but because of for reducing variance because each time we do this we're losing some of the examples.",
            "We do this many times just to recover."
        ],
        [
            "That so this is a some results that we had on the KDD 98 data set applying the costing reduction using as classification algorithms.",
            "Naive bayes.",
            "This is a boosted naive Bayes, C 4.5 and SVM.",
            "Then line up here with you were the results from the.",
            "Katie did 98 Cup which is about 15,000 the profit.",
            "Which I mean, you can show that the skated United Cup isn't is.",
            "You can solve it using importance weighted classification and then the green results are using.",
            "Resampling with replacement and the blue results are using the rejection sampling.",
            "The actual costing reduction that we talked about.",
            "So we see that we get an improvement specially for C 4.5 because there is sampling with replacement doesn't work very well here.",
            "In the case of C 4.5.",
            "So now that we've seen a simple example of reduction, I'm going to make some statements about why we want to continue working with reductions and want to try to scale.",
            "Use them to solve more complex problems.",
            "So first is that they work well in practice, so there are lots of examples where we."
        ],
        [
            "Elections have succeeded, for example in.",
            "Multiclass classification.",
            "Using binary classification we have good results.",
            "Boosting also can be seen as a reduction from weak learnability too.",
            "Strong learnability, so it's also case where reductions have worked well in practice."
        ],
        [
            "This.",
            "We can, so as I said from the beginning, we can you reuse highly optimized learning algorithms which have been developed to solve the binary classification problem.",
            "And even if people continue working on binary classification, continue getting good results, we can actually use them for.",
            "For this is getting and different new algorithms for solving a problem from and different base algorithms that we have.",
            "We're going to be able to use them and get different algorithms."
        ],
        [
            "Solving the new problems.",
            "One very important aspect is this one, that I think is this theoretical aspect because you can transfer performance guarantees from one problem to another.",
            "So because assigning the reductions we always show how the error in the original problem transfers to the error or the are they regret.",
            "As we'll see later in the problem that we care about.",
            "So we don't make any assumptions.",
            "All the assumptions are going to be like hidden in the actual binary classification algorithm that you're going to use, and we're going to be able to give very precise guarantees of the type if you get epsilon performance in the binary classification problem, then you get this.",
            "K Times epsilon performance, something like that in the regional.",
            "In the new problem so.",
            "These are relative performance guarantees, but in a sense there they are very strong because you don't have to make any assumptions to get them you.",
            "If you can guarantee the performance on the original problem, then the care is true."
        ],
        [
            "Sorry.",
            "Get lost.",
            "But the assumption is mostly about minimizing the 01 loss on the training set, no, no, it's minimizing the 01 loss on the distribution.",
            "On the distribution of examples that from which the training examples were drawn.",
            "So, so that's not minimum.",
            "I mean, the original assumption is that we have a very good classification algorithm which can minimize 01 loss on the distribution of examples from where the training examples.",
            "And then the fourth point here is that reductions compose, so.",
            "What this means is that you can get as we'll see later examples that John and Eileen are going to talk about.",
            "In many cases, we're going to reduce from, say, multiclass classification to importance weighted classification.",
            "Then we know that importance weighted classification, which reduces to binary classification.",
            "So we can start like building bigger and bigger blocks from small blocks, so this comparable composing effect is very interesting.",
            "So in general.",
            "In technology and science, we want, you know, solve bigger problems by breaking it into subproblems and reductions give away for us to do this.",
            "And also the fifth point is that you see, like in machine learning.",
            "All these knew different problems are are coming up and we don't have like an unified theory of machine learning.",
            "How do these problems relate?",
            "What are the common aspects between them?",
            "How difficult are them one regarding the order, so I think that we lack like a backbone theoretical backbone for machine learning and this might be a way of going about doing this.",
            "Organizing prediction problems in general into one framework.",
            "So that's.",
            "One also, very why relations?",
            "Why won't we want to?"
        ],
        [
            "Do this.",
            "So next I'm going to be talking about one reduction from quantile classification to solve the quantized sorry, pontile regression problem.",
            "And then later, Alina is going to talk about multiclass classification, cost sensitive multiclass classification also.",
            "And then a little bit also about ranking reductions from ranking to classification.",
            "And then.",
            "John is going to be talking about learning with partial feedback.",
            "Should be the later part of the talk, so this is an outline of the rest of the talk, so I'm going to talk about for about more 10 minutes about quantile regression.",
            "Then we have a break after that."
        ],
        [
            "So quantile regression.",
            "I don't know if many people have heard about this before, so we usually when we talk about regression so we know the regression we have.",
            "We want to predict the real value number.",
            "But usually when we talk about regression we're talking about.",
            "Oh then the mean regression to the mean when where we want to estimate the mean value of the function.",
            "The conditional mean value of the function.",
            "So usually we're going to minimize squared error loss, which would lead to finding the conditional mean value of the function.",
            "So this is the standard regression.",
            "So there is another former regression which is called the quantile regression, and here we're going to show how to reduce the quantile regression problem into the classification problem so.",
            "This problem instead of.",
            "Finding the conditional mean you want to find a conditional quantile distribution.",
            "For instance, the median is respect is a special case of the other can't.",
            "I'll so we'll see that later in a little bit more detail and advantages that we find the conditional mean function is more.",
            "It's going to be more robust to outliers than conditional.",
            "Sorry, the condition median is going to be more robust than condition mean and also.",
            "We can, by predicting other quantiles, we can describe different segments of the conditional distribution, so we don't have to just predict the mean or the median.",
            "We can predict the 90th quantile the 80th time, depending on the application.",
            "This can be interesting.",
            "There are other.",
            "There have been other methods for solving the quantile regression problem.",
            "So the first known method is conquers method for the linear quantile regression, which assumes that the conditional quantizer linear function of X.",
            "So this would be the equivalent of.",
            "Linear regression to the mean squared error loss here would have a linear, so it makes an assumption about the form of the function, which we will not make in production, except that if you use like a linear classifier then we'd have some kind of linearity assumption.",
            "So as I said, the assumptions are hidden within the binary classification.",
            "If you have a better or binary classification algorithm which does not make strong assumptions, then this will carry through the reduction.",
            "The other approach is the.",
            "It's a kernel approach.",
            "Nonparametric quantile estimation method which shows that you can solve the nonparametric anti estimation problem as a QP.",
            "So there's no assume predetermined form.",
            "But then you're gonna have to choose the kernel and apparent para meters and all that as you doing for diffuse, like any like an SVM.",
            "For classification, we're going to have to do that here for the quantile problem.",
            "So it is an approach which is inspired by.",
            "Approach classification approaches, but is the modification for this particular problem."
        ],
        [
            "So just to make things more clear, what I'm going to define what is a conditional Q quantile?",
            "Where Q is a value between zero and one, so the.",
            "In the case of the medium medium, the the Q is 0.5, so.",
            "What we want to act a conditional quantiles is a value such that the distribution has probability Q of being below.",
            "Oh I have so I have X is the is G function right?",
            "So there is a probability of Q of being below F of X, the conditional quantile being the conditional value of the distribution being below.",
            "There's a probability of Q, an probability one of minus Q of being above that value.",
            "So for instance, the medium is a value such that the condition distribution that has .5 probability of being above that value and .5 probability of being below the value.",
            "So the basic observation is that.",
            "You can.",
            "If you minimize the the this function here, this loss function, which is the absolute loss.",
            "Instead of minimizing the squared error loss, then you'll find the conditional medium.",
            "So this is the basically quantile.",
            "Regression is about regression using this loss function, whereas squared error, loss regression.",
            "You'd have a squared error loss here and then would be finding the conditional mean.",
            "This is the only difference.",
            "So the conditional medium using this uses this loss.",
            "What about all the quantiles?",
            "So if you want to find the 80th quantile, which means that Q = 0.8, then you'd have this pin pinball loss where.",
            "The last four above.",
            "Zero if you make errors so, so you can think about this as saying that error if you say the true value is 5.",
            "If you say that it's six it is or like 6 is worse than saying that it's for the painting on the loss.",
            "If you care more about being right on the upside or being right on the wrong side.",
            "So so.",
            "So this will shift your errors for being more positive or being more negative depending on the quantile values.",
            "So, but the loss function.",
            "Is going to be the.",
            "This loss function here where you wait the example where you there is weighted by.",
            "The how if it's below or above.",
            "The If it's positive or negative is going to be wait.",
            "If it's positive is weighted by Q and if it's negative."
        ],
        [
            "It's gonna be weighted by.",
            "1 -- Q.",
            "So the reduction from quantile regression to binary classification actually will reduce using what we call the quantum reduction from quantile regression to importance weighted classification and then from part is very classification where you can use causing to reduce to binary classification.",
            "So basically reduced the importance weighted classification.",
            "So what here is just a formal definition of the quantile regression problem.",
            "So we have examples draw from the Ann.",
            "We want to find this mapping from X to 01.",
            "So here we are making this interval B 001.",
            "So we're assuming that the the wise are also in the interval 01 which practice you could have regressions, the the value could be any real number, but here we are restricting it to be in the interval 01.",
            "So if you have a problem where you have example values of bigger an unrestricted, then you have to normalize them to be in the 01 interval so.",
            "The quantile regression that we are assuming the values are restricted."
        ],
        [
            "Bounded there between 0 and.",
            "And we want to find an estimate of the CUV conditional quantile of the given X.",
            "So we want to find that.",
            "So."
        ],
        [
            "What the?",
            "OK, I'll just move to this.",
            "So what the actual algorithm is going to do is you get the actual reduction is going to do is you get the regional training set as and you create T training sets where each one of them.",
            "Is created in.",
            "In this fashion you."
        ],
        [
            "Here.",
            "You have you get each example YXY where X is any feature.",
            "The feature values and Y is the.",
            "The label, which is a value between zero and one.",
            "And then we make the label B1 if the value of Y is below some value T. Which is.",
            "Sorry, is above the value T which is you can think about it as A's agreed value.",
            "So the grade from zero and one you have for each you do like zero, 0.01, zero point, 0 two.",
            "And you do this for each value of T. So if for the first classifier and say T is a is a small value, so.",
            "Most of the examples will have one for the label here.",
            "Only the ones that have values below T would have 0.",
            "And the weights of the examples.",
            "So we're reducing the importance weighted classification.",
            "So each example is going to have.",
            "XY, and this is the weight.",
            "So the wait is going to be Q.",
            "If the example is positive and 1 -- Q if the example is negative, which means that for each particular value of T Now that we have in the grid if the example is above that value, it's going to have label one and wait queue.",
            "If it's below, the value is going to have labels zero and wait 1 -- Q.",
            "So we do this for Grid an.",
            "We can use an values of T so agreed with equal space grid values and would be like zero 1 / N. An minus 1 / N one so would have equal space, so we can show that if we use an equal space B and then we add in most 1 / N term to the law.",
            "So if we add more and more classifiers, make a finer grid, get the value of an bigger, then we're going to be reducing the loss.",
            "And in practice, we can choose the grid as we go, like make that make it be more adaptive.",
            "So we see that we need more classifiers in a in a place we add more crossfire.",
            "So this is a detail, but you can think about is the equal space going from each classifier is going to make a prediction whether or not the example is above or below some value zero 0.01 zero point 0 two 0.99.",
            "And each of them is making that prediction, but the prediction.",
            "But each classifier is trained with weights so.",
            "So then it's going to do the right thing if you don't put the weight is not good.",
            "Do the right thing so you have to have the weights.",
            "And then to predict we get the example run through all the T classifiers.",
            "And get the expected value of the answer, which each one of them is going to say 01.",
            "So if.",
            "We take the average from their response and because we are predicting a value between 01, this is going to be the prediction that we're going to make so we can show that this is the."
        ],
        [
            "Correct prediction.",
            "So this is pictorially.",
            "So you can in.",
            "Each classifier here is going to be predicting whether or not X is Bill is.",
            "Below some threshold, so you'd expect that the predictions are going to be 1111 until the point where the actual value.",
            "Is gonna be the threshold is going to be the correct value so after that everyone is going to be above.",
            "All the classifiers are going to say that before this they're all going to say that the value is is below the correct value, and then when he gets to the correct value after this, it's going to say that all all of them are going to say that they are above the value of T. But of course, the classifiers make mistakes, so you could have a case where you have 1101101 and then at the end but expects to be 0.",
            "'cause we're saying predicting whether or not is above 0.999.",
            "Something like that, so we don't care about whether it makes mistakes.",
            "We just take the average.",
            "And we can show that this this works.",
            "Some sense.",
            "So predicting.",
            "Here we're doing a case where we learn one classifier per.",
            "Threshold, whereas we could also learn one classifier with an extra feature T, But this would be relying on the classifier to do this."
        ],
        [
            "So we don't do that kind of.",
            "Is consistent.",
            "OK so I."
        ],
        [
            "Sure.",
            "OK, So what we show?",
            "Is this I'm running out of time, but what we show is that.",
            "The regret in estimating the medium.",
            "It's gonna be less than the importance weighted regret.",
            "So regret was what does this mean?",
            "Is the difference between the this would be the expected value of the.",
            "Loss of Y minus the median of exo medium wax would be the best you could do in, you know, because the problem is known.",
            "Deterministic, so the median is would be the conditional median.",
            "The correct median of the function.",
            "So this would be the minimum possible loss because for each value of X there is a distribution of values of Y, so this would be the minimum.",
            "Possible loss, and this is the last that we get with the counting algorithm.",
            "So this difference we called the regret.",
            "Which is how much we lose by using the quality algorithm versus predicting the best possible function.",
            "And we can show that this is bounded by the importance weighted regret, which is the law.",
            "Last that we get.",
            "On the Deprime distribution with, which is the distribution with weights versus the minimum possible loss.",
            "That we get.",
            "So if we have an algorithm which is perfect for doing partners weighted classification, that would have perfect.",
            "Perfect quantile regression ALS.",
            "I am.",
            "If we compose this with the binary classification.",
            "Reduction, I mean there is actually costing cost reduction because C is bounded by by one.",
            "Since see the see, here is the weight and the weights are all Q or 1 -- Q.",
            "Then we have C less than one, which means that the regret in estimating the medium is less than the binary breath.",
            "For no so this for all contact simultaneous.",
            "They hold for any contact.",
            "Yeah.",
            "Scale stop somewhere in between because you're pointing out or it can return something from zero and one in the media is not supported.",
            "No so so we are assuming that the Y values are between 01.",
            "So before you even start doing anything with the problem you have to re normalize your problem so the medium is going to be also be between 01 because you have assuming that we have a special."
        ],
        [
            "Quantile regression problem where the values are between 01.",
            "So here is just a prefer some slides with the prediction performance where we compare the quantic wanting reduction using logistic regression and J48 with which is decision tree.",
            "Against.",
            "The linear quantile regression and the kernel quantile regression, the Y values their normalized quantile loss.",
            "So for each problem here, the entire loss could be we are using the.",
            "We are normalizing it so that the best result is 1.",
            "Sorry, the worst result is 1.",
            "The worst result is all is going to be one and the best result is, like in this case is about 0.8 so.",
            "Here.",
            "One thing with shape for it is getting the best results for this data set with different quantiles.",
            "Your .1 zero point 5 and 0.9.",
            "This is another data set where.",
            "Results there closer.",
            "There's a California housing datasets again J 480 quality with my friend is doing better there.",
            "And here is the Boston housing the sun where.",
            "Quantity with logistics regression is doing better.",
            "The kernel method is doing well for the 0.1 and 0.9 cases, so this is a comparison.",
            "We have like more if you look at the papers, we have much more detailed comparison with numbers and lot more data centers.",
            "So the difficulty with like this is the kernel method is that.",
            "We have to optimize for, you know, finding the.",
            "Doing like cross validation many times like this is slow process and not always.",
            "Think we?",
            "We did that but.",
            "It makes like it's going to make you strong assumptions, whereas here, for instance, we're plugging logistic regression and J48.",
            "So depending on the problem, one of them is doing better than others to be better here.",
            "Logistic regression is there, so we have this flexibility of choosing the base classifier, but I don't have so.",
            "As we know, we can't predict that for classification.",
            "Oh no, how it grows with a like a learning curve kind of thing.",
            "No, that's good.",
            "We use the whole datasets from.",
            "For out for the regular squared, yes."
        ],
        [
            "So now we're gonna stop, and then Alina is gonna come back in 5 minutes to start talking about the multiclass classification.",
            "I don't."
        ],
        [
            "It's OK, I'll just hold it.",
            "Thank you.",
            "Alright, will begin again.",
            "Kate.",
            "I will show to other families of loss functions that can be optimized with an algorithm that's designed to optimize 01 loss.",
            "There are pretty general families of functions.",
            "The first one basically can encode any loss function defined on individual examples with discrete output spaces that can potentially be very large.",
            "I'll describe productions that are pretty efficient and work for large decision spaces.",
            "So let's begin with multiclass."
        ],
        [
            "Classification it's a simple generalization of the binary classification problem that Bianca described.",
            "We have some distribution D overacts cross Y where Y is a key label space and we want to find a classifier where mapping from acts the way that minimizes the probability that when we draw an example from D, this classifier disagrees with the label.",
            "Right, so I'm sure you all know the multipla."
        ],
        [
            "Classification of very simple reduction that's commonly used for this problem is so-called one against all.",
            "You create key binary classification problems again.",
            "Case the number of classes.",
            "Where for class Y1 per class for a class I you're trying to predict is the label I or not distinguishing this class from all the other classes?",
            "Right, so when you have a multiclass training example, you create key binary examples, one per class, and the binary label is just the indicator function, which tells you whether the label is I for a problem I.",
            "It's a very common reduction.",
            "None you learn key classifiers when you want to make a multiclass prediction on and you ask you, but you're on all these classifiers on acts.",
            "Some of them will predict 0, some of them will predict one, you randomize over the ones that predict one or over all the labels.",
            "If none of them predicts predicts one.",
            "And again, we can use for the analysis.",
            "We can use this simple trick where we add the index of the problem into the features so we can think of learning a single classifier.",
            "We give it acts and acts and then index of the problem and it predicts whether or not the label of this axe is I.",
            "The benefit of learning a single classifier, at least in theory, is that you can think of this reduction as a transformation that takes a multiclass distribution and converts it into a binary distribution.",
            "Right, so this is just the transformation that takes a multiclass distribution.",
            "Distribution number multiclass K. Class examples and converts it into a distribution over binary examples.",
            "And there is a well defined notion of being used binary distribution Q.",
            "This is a process that you can use to generate examples from the distribution.",
            "You draw a multiclass example from D, then you draw a random I index of the problem, and then you output.",
            "Accent I, as your feature vector and the indicator value weather wise.",
            "One or not.",
            "It should be high.",
            "There should be an.",
            "OK um."
        ],
        [
            "So let's look at a theorem that can come with that.",
            "We can prove about this reduction for all multiclass distributions D and all binary classifiers F. The induced multiclass loss.",
            "It's bounded by Chemainus Time Key minus one times the binary loss of the classifier on the induced distribution.",
            "Right, so again, this is.",
            "The definition of the induced distribution.",
            "And we have a theorem that holds for all multiclass problems and all binary classifiers.",
            "The result in multiclass laws of the argmax classifier of the one against all classifier is bounded by key minus one times the binary loss of the classifier that we use classifier F on the induced distribution Q.",
            "So it's very instructive to look at the proof, and the proof is very simple, so it's convenient to look at half as an adversary trying to induce a large multiclass regret without.",
            "I'm sorry, multiclass loss, without making a lot of binary mistakes, so there's three different failure modes.",
            "The first one is in green, shown in green.",
            "You can have a false negative and no false positives.",
            "Which means that you will eat all the classifiers and all of them predict zeros.",
            "The reduction can do nothing better than just pick a random label.",
            "Which means, since there is only one label that's correct, the reduction will make a mistake and K -- 1 over key cases fraction of the time over the randomness in the reduction.",
            "And that's induced with a single binary mistake.",
            "So single binary mistake.",
            "Out of key, possible binary mistakes can make the multiclass classifier error K -- 1 over key fraction of the times."
        ],
        [
            "There are also two other failure modes, so you can have a positive number of false positives and no false negative, and you can similarly analyze these cases, But the first case is the most efficient way for the adversary to make mistakes.",
            "Where again we're thinking of F. This binary classifier that we use.",
            "To build our multiclass classifier as an adversary, because we want the worst case analysis.",
            "The reason why I gave this analysis is that you can observe that there isn't a symmetry in the failure modes.",
            "And they make."
        ],
        [
            "And the classifier less likely to make a false negative.",
            "You can actually improve their reduction.",
            "So by doing this analysis you can observe this symmetry.",
            "And you can get instead of reducing to binary classification."
        ],
        [
            "You can reduce the importance weighted classification that Bianca talked about and then use the costing method to reduce the binary classification and you will get a factor of two improvement just by making this observation and theory and a consistent improvement in practice.",
            "So the point again, the point that I'm trying to make is that by doing this type of analysis you can drive.",
            "You can derive a better reduction.",
            "But we still have a problem that there is a dependence on key, so the multiclass even was this fix.",
            "The word in the worst case, the multiclass regret, will be keyword two times the binary regret.",
            "Which is pretty bad for problems where Ki is large.",
            "Can we make it more robust?",
            "Yes we can.",
            "We can completely remove the dependence and key by using some redundancy."
        ],
        [
            "Use an error correcting output codes.",
            "So imagine that you have a binary matrix where each column corresponds to your class.",
            "So you have K columns and then you have some number of rows.",
            "Each row encodes a binary problem.",
            "Each row defines a subset.",
            "So for example, the cells that are white in the given row define the subset corresponding to the throat.",
            "Right, and you're just trying to predict for every row you're trying to predict whether the correct classes in that subset or not.",
            "One against all is actually COC was the diagonal matrix where you start to distinguish each class from all the other classes.",
            "And then when you get in, you ax.",
            "You believe all the classifiers that you've learned and you decode to the vector that's closest in the Hamming distance.",
            "Am I going to slowly?",
            "Yes, OK.",
            "There is a theorem that comes with the so C for all multiclass problems.",
            "For all binary classifiers, EOC, the multiclass laws of this reduction is bounded by two M, where M is the number of rows epsilon which is the average binary mistake over D, where D is the minimum distance between between any pair of columns in the matrix and the analysis is very simple, you just observe that you need at least D over to binary mistakes in order to induce the multiclass mistake.",
            "And that basically implies the theorem.",
            "There are good codes that have a large minimum distance.",
            "For example, you can use other more codes.",
            "There is a very simple recursive construction of other more codes which basically gives you."
        ],
        [
            "Um?",
            "For epsilon, so the multiclass loss is bounded by four times the average binary loss.",
            "There is no dependence on key.",
            "There is a fear, a common fear associated with reductions that they create heart problems.",
            "Where they may create hard problems, which means that the statements."
        ],
        [
            "You get or not very useful.",
            "Also, the original problem can be noisy, which means that they induce problem will most likely be also noisy, which also means that the statements are not very useful.",
            "One way of addressing this fear is looking at the regret instead of loss.",
            "So regret is something that can be defined for any loss function, and it's just the difference between the loss suffered by the classifier on that problem.",
            "On the problem that you care about minus the smallest possible loss on that problem, right?",
            "So you subtract off the inherent noise that's in the problem, and you want to bound this avoidable loss.",
            "That's due to suboptimal prediction.",
            "And this fear is.",
            "Maybe people care more about this fear for CSC because the problems are not necessarily natural.",
            "You want to predict whether given label isn't some random subset of labels."
        ],
        [
            "K. And there is a problem with one against all and the sushi."
        ],
        [
            "Wait, there is also one statement that they wanted to emphasize.",
            "If we have an optimal classifier for the induced problem, it seems natural to require that the reduction recovers the optimal solution to the original problem.",
            "Right, so you I reduction creates an induced problem.",
            "You have a very good algorithm for the induced problem.",
            "You want to carry this predictive ability.",
            "You can you want to transform it back to the original problem.",
            "And there is."
        ],
        [
            "Um?",
            "There is a very simple example that shows that one against all anti cocs are inconsistent in the sense that given an optimal classifier, these reductions do not give you an optimal multiclass classifier.",
            "The example is basically so this is the simplest possible example where you don't have a majority class, you have three labels and be sorry the conditional probabilities of these three labels.",
            "So the first label has probability of 1/2 minus some Delta and the other two classes have probability slightly less than 1/4.",
            "Right, and this is actually the issue with the Matrix as well for.",
            "For a three class problem.",
            "So your the optimal binary prediction is the label one or not is no the base optimal production.",
            "Similarly for Class 2 and for Class 3.",
            "So the multiclass classifier can.",
            "Can do no better than just pick random label an it will be sub optimal.",
            "You can try to fix."
        ],
        [
            "This problem being consistency of one against Ole and ECC by reducing through regression instead of classification.",
            "So instead of predicting is label I or not, you're trying to predict what's the probability of label I versus all the other classes.",
            "Um?",
            "The regret this is a consistent reduction, but unfortunately the regret associated with it is sqrt 2 R and this is bad because R is bounded between zero and one.",
            "Right, so square root is actually much worse than having a linear dependence on our where.",
            "Here Are's an average squared loss regret, but also there is this probe introduction from squared loss regression to binary classification that comes with this guarantee that squared error regret is bounded by classification regret.",
            "So this is also the binary classification are as well.",
            "By making this reduction consistent by using a soft production by trying to predict the probability of one class versus all the other classes for one against all.",
            "You're actually making this square dependence on R. On the average binary regret.",
            "There is a similar effects for ECC.",
            "There is a probabilistic version office USC.",
            "Which also has the square root dependence on R, and these two reductions also have pretty bad dependence on this should be key on the number of classes.",
            "Per example, so they make a prediction.",
            "Then we can multiclass prediction.",
            "The complexity of doing that, where to learn on the multiclass example, the complexity of doing that is linear in one against all and quadratic and EOC."
        ],
        [
            "So there are some questions.",
            "Is there a consistent reduction that does not have the square dependence?",
            "Is there a consistent reduction that is sufficient that requires just lock key complexity to train and test on a single multiclass example, which would match an information theoretic lower bound because he needs at least locato write down, the answer can be buff beach shiftable reduction that performs only pairwise comparisons between the classes.",
            "Because you want more natural problems.",
            "And I'll show you a family of algorithms that give you these bounds, so you can have a constant.",
            "Times our dependence.",
            "In the regret analysis, so the ratio of the multiclass regret to the binary regret is 5.5.",
            "And you can do that with a lock K computation where again case the number of classes.",
            "Or if you want to improve the regret dependence and are willing to pay.",
            "To have an order of computation, you can have these results.",
            "Immediately."
        ],
        [
            "When you start thinking about an efficient way of reducing multiclass classification to binary classification, you're thinking of a binary tree.",
            "Here is a very simple reduction that's actually used, and I'll show you that it may not be the best way to reduce multiclass classification to binary classification.",
            "So you split the set of labels in half and you try to predict the probability.",
            "What's the probability that the label is in that subset versus the other subset?",
            "And then you recurse on both sides using some binary tree structure.",
            "Right here is the example that shows it's a problem with this approach.",
            "So let's say you have three labels and you have the conditional probabilities in the first line.",
            "An optimal classifier at the root would predict either.",
            "You would predict the left subtree because the sum of conditional probabilities is 0.55.",
            "While actually the best choice is label 3.",
            "Right, so you want to predict the Max instead of the sum.",
            "Right, so again, the problems that you want to predict at nodes is not what's the probability that the correct label is in this sub tree versus dotsub.",
            "Do you actually want to predict where the Max is?",
            "This is called the theory induction, and it's actually fairly commonly used."
        ],
        [
            "Kim will see a better production.",
            "You have you have a binary tree structure on the set of labels.",
            "And we will look at the three from the Leafs towards the Earth.",
            "You pair the labels arbitrarily and you learn the classifier to predict the winner.",
            "Um?",
            "The winner between them so.",
            "This classifier predicts is the label one or two.",
            "Yes, the little the label three or four and so on.",
            "The second level of classifiers would be chosen between the winners.",
            "Conditional winners from this branch.",
            "And from this branch.",
            "So each normally predicts the bust of a pair of winners from the previous round."
        ],
        [
            "And then to make a prediction, multiclass prediction and acts once you learn the classifiers at the at the non leaf nodes you just follow a chain of predictions from dirt to some leave and predict that leave.",
            "So how do you train?",
            "This is an example that shows the training procedure on ax, 3."
        ],
        [
            "So there is a path associated with label 3.",
            "You train this classifier on acts, left because three is to the left.",
            "Then you train this classifier on acts, right conditioned on this classifier, getting the label correctly.",
            "So you use classifiers on the path from the root of the leaf.",
            "To filter out examples that propagate towards given up.",
            "And that's essential to form the right prediction problems at the nodes.",
            "Right, because the notes want to predict if this classifier made a mistake.",
            "This classifier should have no preference for the label.",
            "K."
        ],
        [
            "It's important to form the right training sets.",
            "Classifiers from levels closer to the leaves serve as filters for the examples.",
            "Used to train classifiers closer to the root.",
            "It's an important observation that you can use batch and online learners with this reduction as well as with any reduction.",
            "So you can have online algorithms, but none of these notes and you will be just up."
        ],
        [
            "Ethan them.",
            "In an online fashion?",
            "So it's important to understand that any reduction can essentially be used with online algorithms.",
            "And we will see that this reduction comes with a guarantee that multiclass to binary."
        ],
        [
            "I regret ratio is bounded by central to the three dots by lock key.",
            "And then bubble at some robustness and remove this dependence on key completely.",
            "What year the same reduction can be used to actually solve a more general problem.",
            "Of course, sensitive multiclass classification you have where you have a distribution D / X cross vectors of length ski where where this vector specifies the cost of each choice.",
            "So not only your matchmaking multiclass prediction, but each production is associated with its own cost.",
            "And you can essentially encode any loss function defined on individual examples this way.",
            "And you want to find the multiclass classifier that minimizes the expected cost with respect to this distribution.",
            "To the underlying distribution.",
            "And of course, costs are not available at this time, so it's just you just want to learn the multiclass classifier minimizes the expected."
        ],
        [
            "Cost."
        ],
        [
            "So do encode multiclass classification.",
            "You will have cost factors that have a single zero and all ones there is only one class.",
            "That's correct and all the other classes have lost one.",
            "You can encode multi label classification very easily.",
            "You'll have some subset of zeros and the rest will be one.",
            "The rest will do.",
            "OK, so how do we train this filter 3?"
        ],
        [
            "In the cost sensitive case.",
            "Let's look at the non leaf node that chooses between label A and label B right?",
            "So there are winners from the previous round.",
            "Let Y be left if the cost of a is less than the cost of B is at most the cost of peace, so a would be the winner.",
            "That you want this classifier to predict and try it otherwise, and then you train on XY with important weight equal to the difference.",
            "The absolute difference in costs.",
            "So we will reduce the importance weighted classification.",
            "The Bianca talked about where each example has a cost associated with it that tells how important it is to predict this example correctly.",
            "Right and the importance weight will be the difference in costs for a given node conditions.",
            "An axe every edge in the tree is associated with some label.",
            "Given the classifiers at the nodes closer to the leaves.",
            "Every arch will be associated with some label.",
            "So this node is predicting between A&B conditioned on Max.",
            "And you want the importance of that production to be proportional to the difference in costs.",
            "Again, the induced distribution is well defined.",
            "By the reduction you can think of this reduction as a transformation from cost sensitive multiclass examples to importance weighted binary examples.",
            "It's just a machine that takes one distribution distribution of 1 type and transforms it into a distribution of another type that will be fat to an importance weighted algorithm.",
            "And then we can compose this reduction with cost and to remove the weights to the sample.",
            "In order to remove the weights."
        ],
        [
            "And here's a theorem that comes with this approach.",
            "For all cost sensitive problems.",
            "An classifier said the nodes the result in cost sensitive regret is bounded by the average binary regret over the nodes times the expected sum of weights over the nodes.",
            "Right, so every node was associated with some weight.",
            "There's some expectation of that weight.",
            "And you take the sum over the nodes.",
            "Alternatively, we can bound the cost sensitive regret by K. / 2 times.",
            "The average binary regret.",
            "So it's the smaller.",
            "Of the son or curator.",
            "This is a computational analysis that follows directly from.",
            "The reduction and all of them match information theoretic lower bounds.",
            "You need at least order of K here to just treat the vector of costs.",
            "Right to train an example."
        ],
        [
            "So what does it imply in the multiclass keys in the multiclass keys there is only one weight per level, that is 1.",
            "All the other weights is 0, so the sum of weights is essentially bounded by the depth.",
            "Which is where Luke case coming from.",
            "So the ratio of the cost center of the multiclass regret to binary regardless log key.",
            "And we don't need to reduce the cost, and in this case because you either an example is filtered where it's propagated with weight one.",
            "So in the multi class keys there is no need to reduce the constant.",
            "Can we make it more robust?",
            "So the 1st.",
            "Attempt would be to just run multiple.",
            "I should make this observation that the filter filter tree is essentially a single elimination tournament on the labels.",
            "You paired the labels.",
            "You can think of labels as players which playing and then once a player loses its out of the tournament.",
            "Right, so you can think about running multiple single elimination tournaments and then choosing the majority.",
            "For example, the majority plurality winner.",
            "But this is of no use because you're basically given the adversary more chances to make a mistake.",
            "You're you're increasing the dots.",
            "And we saw that the bound the regret ratio depends on the dots, so this is not very useful.",
            "But it turns out that you can help.",
            "You can have interdependant tournaments in lock keyed apps.",
            "You can have lock key tournaments in lucky ducks instead of log squared.",
            "Squared K dubs.",
            "So instead of running lock key, single elimination tournaments independently and having lock squared doubts, you can have interdependant tournaments and I'll show you how.",
            "In lock key drops.",
            "Which would allow us to go from look key in the regret bound to a constant.",
            "So no player.",
            "Will be playing twice in the same round and still you can.",
            "You can have interdependant tournaments.",
            "This way since each tournament."
        ],
        [
            "It is shown in its own color.",
            "So once a player loses in the first tournament, it goes in place in the SEC Tournament.",
            "Once it loses, it goes into third tournament.",
            "In the next round, and you pair the players aggressively, so if you can pair players that can play in the same tournament, you do so.",
            "And it turns out that you can have lock key, single elimination tournaments in lock in order flow key.",
            "Rounds then you will have lock key winners, which will play in the weighted single elimination tournament.",
            "To determine the final winner.",
            "Which will also add a low key to the Dubs found.",
            "But the game going what allows us to reduce the regret ratio from lucky to constant.",
            "Yes, having lock the absence that the flock squared apps."
        ],
        [
            "So E yes, the amount of redundancy.",
            "So that's the number of times you can lose before you're out of the tournament.",
            "That's a very simple computational bound that comes from the construction, and that's a guarantee that we get.",
            "And this plot simply built that function.",
            "So for a fixed key.",
            "I think he was.",
            "1001 thousand 24 in this example.",
            "And this is the level of robustness, so this is the elimination.",
            "Factor, and this is how the regret bound decreases as you add more robustness.",
            "So this is the regret bound.",
            "This is the ratio of the multiclass regret to the binary like that, and that's how it drops as you add more tournaments, more robustness."
        ],
        [
            "Right, and that's a particular value of of E, so 1 E is for Locky.",
            "The regret ratio is 5.5.",
            "It was a lock key computation, for example, both training and test time.",
            "And also note that the binary problems that they have at the nodes are comparing only pairs of examples.",
            "Oh, I'm sorry.",
            "Pairs of labels.",
            "OK."
        ],
        [
            "I won't mention ranking there."
        ],
        [
            "Briefly and then, John will continue.",
            "The tutorial.",
            "This is an example of loss functions where the loss function is defined on subsets of examples rather than on individual examples.",
            "It has some universe U of objects and you have some subset.",
            "You are given some subset of of these objects coming from some distribution over subsets and you want, let's say for simplicity that these objects have binary labels.",
            "There's also some conditional distribution over the labels and you want to sort.",
            "The objects in the subset so that when the label said revealed you have a sequence of zeros followed by sequence of ones.",
            "Right, so you want the objects to be sorted.",
            "I commonly used loss function.",
            "This AUC loss, which is it's just a normalized bubble sort distance to the sort that order normalized by the number of zeros times the number of ones, which is the number of pairs that you can have miss sorted.",
            "Right there are two general approaches to the ranking problem.",
            "You can learn a scoring function on the entire universe of objects that Maps a given object to a real value, and then given a subset you validate all the elements in that subset using a fan rank according to the values of F. And they'll turn that if approaches to learn the classifier to predict given two objects, whether one should be ordered before the 2nd and then sort.",
            "So notice that this classifier may not induce a linear order, and on the set.",
            "So you need some algorithm that resolves the cycles that you may have in age.",
            "And the motivation is that a good total linear ordering may be impossible to achieve, so this function may be difficult to learn well.",
            "There may not be a good.",
            "Total ordering function.",
            "While such an age can be learned well.",
            "So that's the motivation for the reduction approach.",
            "Even though the ranking problem may seem very different, there is very tight redux."
        ],
        [
            "And from thereon can problem the classification and that's actually generalized to a class of functions.",
            "Other than you see loss.",
            "It's a very general class of ranking functions, and you can read about it and I alone and mccree's paper.",
            "Which is supposed to be on the website.",
            "I'm done."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, good morning and welcome to the machine Learning Reductions tutorial.",
                    "label": 0
                },
                {
                    "sent": "My name is Bianca said Rosie I'm from Fluminense Federal University in Brazil and the other speakers are going to be Alina Bagels.",
                    "label": 0
                },
                {
                    "sent": "Emeran John Langford from IBM Research and Yahoo Research, so I'm going to be the first speaker.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk for about 40 minutes and then we're going to have a short break and then move on to Alina and then John.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "To move here with.",
                    "label": 0
                },
                {
                    "sent": "Stand here.",
                    "label": 0
                },
                {
                    "sent": "Oh OK, I'll try to speak up.",
                    "label": 0
                },
                {
                    "sent": "I thought them I could be.",
                    "label": 0
                },
                {
                    "sent": "There are a microphone for the front.",
                    "label": 0
                },
                {
                    "sent": "I'll move the mic closer.",
                    "label": 0
                },
                {
                    "sent": "Oh, the mic is only for the King, OK?",
                    "label": 0
                },
                {
                    "sent": "I'll speak louder, so um.",
                    "label": 0
                },
                {
                    "sent": "What is going to say is that you are welcome to ask questions, since this is a tutorial, we're hoping that people interact with us, so you're welcome to ask questions L as long as I go and not in the end.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we know that in practice applying machine learning is often difficult because we have algorithms and tool boxes for solving problems that are like the standard machine learning problems like classification, and most often we have applications that require more complex types of problems such as cost sensitive classification, hierarchical classification, structure classification, then going on to even more complex problems like machine translation, and we'd like to scale up these.",
                    "label": 1
                },
                {
                    "sent": "Algorithms that we have to be able to solve these more complex problems like ranking etc.",
                    "label": 0
                },
                {
                    "sent": "So this story is going to be about 1 possible approach for moving up from simple.",
                    "label": 0
                },
                {
                    "sent": "The simple algorithms that we have to be able to solve this more complex problems.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So actually we would have like 2 approaches that we can think about to solve these more complex problems which would be designed you algorithms from scratch or try to modify the algorithms in some way to be able to apply them to these more.",
                    "label": 0
                },
                {
                    "sent": "Complex problems.",
                    "label": 0
                },
                {
                    "sent": "Which is fine.",
                    "label": 0
                },
                {
                    "sent": "Is one line of research and the but the approach that we're talking about today is the 2nd approach, which is about how we can use reuse the old algorithms in a way in a robust way.",
                    "label": 0
                },
                {
                    "sent": "To achieve good performance for these other, more complex problems.",
                    "label": 0
                },
                {
                    "sent": "So we'd like to be able to reuse what we've done in the past in a formal fashion that we will guarantee performance and.",
                    "label": 0
                },
                {
                    "sent": "So this is tutorial is going to be about the second approach.",
                    "label": 1
                },
                {
                    "sent": "How we can do that?",
                    "label": 0
                },
                {
                    "sent": "How can we reuse algorithms for solving simple machine learning problems to scale them up to more?",
                    "label": 0
                },
                {
                    "sent": "Complex problems.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is the simple problem that I'm talking about?",
                    "label": 0
                },
                {
                    "sent": "So today we're going to focus as our core problem as the one that we know how to solve is going to be binary classification.",
                    "label": 1
                },
                {
                    "sent": "And to make things very clear.",
                    "label": 0
                },
                {
                    "sent": "Here's our definition of what is binary classification, so this is what we expecting that we know how to do well.",
                    "label": 0
                },
                {
                    "sent": "So expecting that we have good algorithms for solving this problem.",
                    "label": 1
                },
                {
                    "sent": "In fact, this is a problem that most people have focused for a long time.",
                    "label": 0
                },
                {
                    "sent": "For the last 4050 or so.",
                    "label": 0
                },
                {
                    "sent": "What we are.",
                    "label": 1
                },
                {
                    "sent": "Calling the binary classification problem is a problem where we have a finite set of training.",
                    "label": 0
                },
                {
                    "sent": "Examples of the form X1Y one through XN YYM, and we'd like to produce a classifier.",
                    "label": 0
                },
                {
                    "sent": "Which you call age, which is a mapping from X 201 S 0 instead.",
                    "label": 0
                },
                {
                    "sent": "So we assume that the Y values are binary, so there are 01.",
                    "label": 0
                },
                {
                    "sent": "And the X is an arbitrary space which most often will be like a vector space or feature space, but we are not requiring that in the definition, so.",
                    "label": 0
                },
                {
                    "sent": "We're leaving that open, so depending on the matter, they're going to use X could be different, so here we don't.",
                    "label": 1
                },
                {
                    "sent": "We're not making any assumptions about the data except that we are assuming that.",
                    "label": 0
                },
                {
                    "sent": "We're gonna want to.",
                    "label": 0
                },
                {
                    "sent": "There's an unknown distribution D. / X and 01.",
                    "label": 0
                },
                {
                    "sent": "So it's a joint distribution over X and 01 and we want to minimize.",
                    "label": 0
                },
                {
                    "sent": "Find the classifier that minimizes the loss on this same distribution.",
                    "label": 0
                },
                {
                    "sent": "So we're not making any extra assumptions if we make extra assumptions which the algorithms will make, then they're going to carry through what we do later with the algorithm.",
                    "label": 0
                },
                {
                    "sent": "To begin with, we are requiring this as the minimum that we are requiring that the algorithm will do will do this if the algorithm does this, then we can guarantee other things that we're going to show later.",
                    "label": 1
                },
                {
                    "sent": "So we the algorithm is in one wants to find an age which is the hypothesis with small 01 loss so.",
                    "label": 0
                },
                {
                    "sent": "And the losses over the same distribution which we draw the training data from.",
                    "label": 0
                },
                {
                    "sent": "And we want to find an age will which leads to to a small spectral loss or the error difference between the predicted value.",
                    "label": 0
                },
                {
                    "sent": "This age of X, which would be the prediction and why?",
                    "label": 0
                },
                {
                    "sent": "Which is the true label.",
                    "label": 0
                },
                {
                    "sent": "So this is our core problem that we are assuming that we know how to solve.",
                    "label": 1
                },
                {
                    "sent": "Of course it's not always that easy, but we assume that we have that done and now we would like to scale.",
                    "label": 0
                },
                {
                    "sent": "I mean use that as a basis for other solving other problems.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we want from from a reduction?",
                    "label": 1
                },
                {
                    "sent": "So I think most people when they think about even this name reduction sounds like simplistic that we want to get a difficult problem and make it be solve it with a simple thing.",
                    "label": 1
                },
                {
                    "sent": "So it looks like we're simplifying the problem and losing something with this, but in fact we want the reduction to be robust, which means that we're going to use the binary classifier in such a way that we want to guarantee good performance on the problem that we care about.",
                    "label": 0
                },
                {
                    "sent": "So we actually show that.",
                    "label": 0
                },
                {
                    "sent": "When a reduction, we try to show that if we have good performance on the 01 last problem on the binary classification problem, then we show that we have good performance in the more complex problem we're reducing to classification.",
                    "label": 0
                },
                {
                    "sent": "So it's not simply just getting the problem, making it look like classification and solving it using classification, which should lose.",
                    "label": 0
                },
                {
                    "sent": "We've seen would be losing something now.",
                    "label": 0
                },
                {
                    "sent": "In fact, we're guaranteed that we're carrying the performance, so we're not doing it in.",
                    "label": 0
                },
                {
                    "sent": "Any fashion we're doing in a row?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mustache.",
                    "label": 0
                },
                {
                    "sent": "And second, they should be modular in the sense that we want to be able to plug any.",
                    "label": 1
                },
                {
                    "sent": "Classifier any classifier learner that we have.",
                    "label": 0
                },
                {
                    "sent": "And reuse them in within the reduction in a similar manner, so we don't want it to look inside what the like, what the algorithm is doing, where you're going to be using the algorithm as a black box and then build their reduction as composition of modules.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to start with, I'm going to talk about a very simple reduction.",
                    "label": 0
                },
                {
                    "sent": "Which is a reduction from importance weighted classification to binary classification to standard binary classification.",
                    "label": 0
                },
                {
                    "sent": "Then later I'm going to give another example of reduction which is a little bit more.",
                    "label": 0
                },
                {
                    "sent": "Complicated then John and Arlene are going to talk about reductions from more complex problems to classification.",
                    "label": 0
                },
                {
                    "sent": "So the first example we're going to see.",
                    "label": 0
                },
                {
                    "sent": "Is this simple importance weighted classification?",
                    "label": 0
                },
                {
                    "sent": "Example where.",
                    "label": 0
                },
                {
                    "sent": "Aside from having the features X and the wise, we also have AC.",
                    "label": 0
                },
                {
                    "sent": "Which is actually which is.",
                    "label": 0
                },
                {
                    "sent": "You can think about is awaits, which is the importance of that particular example.",
                    "label": 0
                },
                {
                    "sent": "So this comes up all the time in many as like many different algorithms.",
                    "label": 0
                },
                {
                    "sent": "For instance, in boosting that you have to have weights for the examples.",
                    "label": 0
                },
                {
                    "sent": "So and then in practice you can modify many standard algorithms to actually take these weights as input.",
                    "label": 0
                },
                {
                    "sent": "So here we want to be able to get a classifier that then.",
                    "label": 0
                },
                {
                    "sent": "Does not accept weights as input and then make it use the weights and we make it very precise what.",
                    "label": 0
                },
                {
                    "sent": "Should do with these weights, right?",
                    "label": 0
                },
                {
                    "sent": "Because?",
                    "label": 0
                },
                {
                    "sent": "Now we have a distribution.",
                    "label": 0
                },
                {
                    "sent": "The over acts which are the features 01 which are the labels and the value between zero and infinite, which is the weight or the cost.",
                    "label": 0
                },
                {
                    "sent": "So for each example it has this associated weights and what we want to do.",
                    "label": 0
                },
                {
                    "sent": "Is find a classifier that minimizes, instead of minimizing just the error, which would be the difference between?",
                    "label": 1
                },
                {
                    "sent": "I mean how many times it gets this incorrect.",
                    "label": 0
                },
                {
                    "sent": "The error is now.",
                    "label": 0
                },
                {
                    "sent": "Multiplied with the C value, which means that if you make a mistake one.",
                    "label": 0
                },
                {
                    "sent": "An example that has a bigger civil that would be a bigger cost, and if you haven't exactly make an error.",
                    "label": 0
                },
                {
                    "sent": "In an example with this lower see value.",
                    "label": 0
                },
                {
                    "sent": "So in practice, of course we would like to have all the examples correct, But if we can't have all the examples correct then we want to go for get the ones that have the higher seed values correct?",
                    "label": 0
                },
                {
                    "sent": "So this is the last function that we want to minimize, so see, this is very similar to the classification problem.",
                    "label": 0
                },
                {
                    "sent": "It's it adds a little bit more to the standard classification problem, which is the weight.",
                    "label": 0
                },
                {
                    "sent": "And now we want to be able to solve this using standard classification algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is a common scenario that they set up years many times in boosting for instance, but also it also is is a case where one class is rare and has a bigger cost than the other class.",
                    "label": 1
                },
                {
                    "sent": "So these C values would be the same for all the examples belonging to the rare class and which they would be higher than the examples in the more frequent class.",
                    "label": 0
                },
                {
                    "sent": "So this eval is here would be.",
                    "label": 0
                },
                {
                    "sent": "You could make them for this problem that we often have diff having different costs for false positive versus false negatives.",
                    "label": 0
                },
                {
                    "sent": "We can use this particular case also to solve that problem.",
                    "label": 0
                },
                {
                    "sent": "So now we're gonna.",
                    "label": 0
                },
                {
                    "sent": "OK, so people of course have seen these problems this problem before and they have other approaches besides the reduction approach to solve it, which the first one is actually to just change the particular classifier learner.",
                    "label": 0
                },
                {
                    "sent": "So you go inside it and find a way for it to use the weights in some sense and then try to minimize the correct loss.",
                    "label": 0
                },
                {
                    "sent": "So if you know a lot about your.",
                    "label": 0
                },
                {
                    "sent": "Your classification algorithm in a lot of cases it can be easy to do that specially let's say in naive Bayes there's like standard ways you can just use the weights and they're going to.",
                    "label": 0
                },
                {
                    "sent": "Give good results, but in a number of algorithms for instant decision trees it doesn't work very well when you introduce because of the way the algorithm was designed.",
                    "label": 0
                },
                {
                    "sent": "Once you start introducing these weights, it doesn't do what you're expecting it would do in practice if you try to modify it.",
                    "label": 0
                },
                {
                    "sent": "Because of heuristics and other things.",
                    "label": 0
                },
                {
                    "sent": "So because there are sticks were based in the case where examples had were uniform.",
                    "label": 0
                },
                {
                    "sent": "Like all they have the same weight when you start modifying the algorithm, it can do something weird.",
                    "label": 0
                },
                {
                    "sent": "The second approach would be to actually move from classification to do like a two step.",
                    "label": 0
                },
                {
                    "sent": "Approach of estimating.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "School back later.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Note that.",
                    "label": 0
                },
                {
                    "sent": "You didn't say this, but for training we know the C values for the training examples, but we don't know the C values very often.",
                    "label": 0
                },
                {
                    "sent": "For the test example, so we only know the C values for the training examples in the for the test examples we have the X values and we want to predict based on that right.",
                    "label": 0
                },
                {
                    "sent": "We don't know the C values for the test data, that's why.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "In this approach, we would estimate the probability of the example belonging to the positive class.",
                    "label": 0
                },
                {
                    "sent": "An art and then it's made the probability of not.",
                    "label": 0
                },
                {
                    "sent": "It's not belonging to the positive class and also estimating the costs of an example.",
                    "label": 0
                },
                {
                    "sent": "So it would be a separate function, so it can given an X you would give us the cost.",
                    "label": 0
                },
                {
                    "sent": "So this is like a regression problem and then we would combine them.",
                    "label": 0
                },
                {
                    "sent": "And use Bayes optimal decision rule to decide which class you would go to.",
                    "label": 0
                },
                {
                    "sent": "So this approach is not very robust because when you combine these two models there, the errors could like sum up and things like that so.",
                    "label": 0
                },
                {
                    "sent": "And also with require estimating these probabilities, which is more difficult than classification and also estimating the costs which could be.",
                    "label": 0
                },
                {
                    "sent": "Hard.",
                    "label": 0
                },
                {
                    "sent": "And so we don't wanna see.",
                    "label": 0
                },
                {
                    "sent": "Approach is not very robust.",
                    "label": 0
                },
                {
                    "sent": "So the reductions are production approach pictorially would be to create.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some.",
                    "label": 0
                },
                {
                    "sent": "Map our goal is to minimize the loss function indeed, so we want to create.",
                    "label": 0
                },
                {
                    "sent": "And that box called R which get examples that are drawn from D, which is the distribution in the problem that we want to solve in this case, is the important sway did.",
                    "label": 0
                },
                {
                    "sent": "Classification problems, so we want to be able to get examples from the.",
                    "label": 0
                },
                {
                    "sent": "We want to minimize the L, which is that loss function over the importance weighted sample.",
                    "label": 0
                },
                {
                    "sent": "And we want to be able to transform it through the reduction so that we can use.",
                    "label": 0
                },
                {
                    "sent": "An algorithm from optimizing 01 loss, which is our algorithm, which is the algorithm that solves the binary classification core problem that I talked about before.",
                    "label": 0
                },
                {
                    "sent": "So we transform D. The regional distribution into ADI Prime, which is a distribution that.",
                    "label": 1
                },
                {
                    "sent": "If we apply this.",
                    "label": 0
                },
                {
                    "sent": "Binary classification algorithm.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to get a hypothesis out of this, which is a classifier.",
                    "label": 0
                },
                {
                    "sent": "Which optimizes 01 loss in the Deprime distribution, which is different than the D distribution, the regional distribution, and we can show that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "After we get this age, we can use the age in such a way that we call the RH.",
                    "label": 0
                },
                {
                    "sent": "Function which here we would call it a our inverse function, which gets the predictions from the binary classifier and makes predictions in the original problem.",
                    "label": 0
                },
                {
                    "sent": "So we call it R&R inverse because here with getting examples from the regional problem, transforming into examples for this standard problem binary classification, we get a binary classification hypothesis back.",
                    "label": 0
                },
                {
                    "sent": "And now we transform it into the.",
                    "label": 0
                },
                {
                    "sent": "Hypothesis with the reduction applied to it, which is the R age hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So in this simple example, the there is not even this are in first, so we we were.",
                    "label": 0
                },
                {
                    "sent": "This is like a general picture of what the reductions would do.",
                    "label": 0
                },
                {
                    "sent": "We get the examples in the regional distribution, transform them applied the binary.",
                    "label": 0
                },
                {
                    "sent": "Learning and this could be done.",
                    "label": 0
                },
                {
                    "sent": "More than one way more complex reductions and more.",
                    "label": 0
                },
                {
                    "sent": "More times than here we we get back to the original problem.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "We are transforming age into our age.",
                    "label": 0
                },
                {
                    "sent": "With small loss.",
                    "label": 0
                },
                {
                    "sent": "Search that if age does well on the D prime distribution in terms of 01 loss, our age is going to be do well on the regional distribution with the L loss, which is the loss in the problem that we care about.",
                    "label": 1
                },
                {
                    "sent": "The actual problem that we're trying to solve.",
                    "label": 0
                },
                {
                    "sent": "So we not only want to do that, we want to do that guarantee that we're carrying through, like good performance in the 01 lost a good performance in the loss that we care about.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in this particular importance weighted classification problem, we can use a simple theorem that we call the distribution shift theorem.",
                    "label": 0
                },
                {
                    "sent": "To derive a reduction.",
                    "label": 0
                },
                {
                    "sent": "So the theorem says that for any importance weighted distribution D and any classifier age.",
                    "label": 1
                },
                {
                    "sent": "If we let the prime.",
                    "label": 0
                },
                {
                    "sent": "Or get a new distribution to prime.",
                    "label": 0
                },
                {
                    "sent": "Bing importance weighted distribution so.",
                    "label": 0
                },
                {
                    "sent": "The D prime is the weighted by C over the expected value of C. So this means we change from the original distribution to distribution of.",
                    "label": 0
                },
                {
                    "sent": "Our examples appear with probability proportional to see.",
                    "label": 0
                },
                {
                    "sent": "So we have a distribution of examples and now we want the examples to appear.",
                    "label": 0
                },
                {
                    "sent": "Examples that have bigger way to appear more often than examples that have lower weight so.",
                    "label": 0
                },
                {
                    "sent": "Ben, if we learn a classifier on the D prime distribution, then we're going to be minimizing.",
                    "label": 0
                },
                {
                    "sent": "The L loss, which is the importance weighted loss if the classifier minimizes the 01 loss.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is kind of intuitive, so if you get the original training sample and now we replicate, I mean this would be the more intuitive way.",
                    "label": 0
                },
                {
                    "sent": "This is not going to be the right way to do it, but if you have the original samples and say an example have cost 10 and you replicate it 10 times and one example has caused two and replicated two times.",
                    "label": 0
                },
                {
                    "sent": "And one example has cost one.",
                    "label": 0
                },
                {
                    "sent": "You keep it once in the training set and then you run a classifier.",
                    "label": 0
                },
                {
                    "sent": "On that training sample, then it's going to pay more attention to the examples that appear more often.",
                    "label": 0
                },
                {
                    "sent": "So then it's going to minimize the loss which has weights even though your algorithm is minimizing the 01 loss.",
                    "label": 0
                },
                {
                    "sent": "So intuitively we can think about it.",
                    "label": 0
                },
                {
                    "sent": "That way.",
                    "label": 0
                },
                {
                    "sent": "You could or this is one way of solving it.",
                    "label": 0
                },
                {
                    "sent": "Actually practice people do that, replicating the example here we just formalizing it's saying OK, this is a distribution shift, so we want to move from the distributed distribution, which is original distribution to ADI Prime where.",
                    "label": 0
                },
                {
                    "sent": "Examples are weighted.",
                    "label": 0
                },
                {
                    "sent": "And we minimizing.",
                    "label": 0
                },
                {
                    "sent": "01 loss, so here's the proof, which I'm not going to go into.",
                    "label": 0
                },
                {
                    "sent": "Detail is very simple proof, just showing in fact that if you minimize 01 loss on the distribution.",
                    "label": 0
                },
                {
                    "sent": "Uh, which?",
                    "label": 0
                },
                {
                    "sent": "Wherever example with me is weighted by C for minimize it on this distribution, then the same of minimizing this loss here, which is the last that we care about.",
                    "label": 0
                },
                {
                    "sent": "And here's this distribution shift.",
                    "label": 0
                },
                {
                    "sent": "So this is called distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is just formalizing what people know about, so even we call this like folk theorem.",
                    "label": 0
                },
                {
                    "sent": "So it's like a very simple case where we know that if it's intuitive if you replicate the examples.",
                    "label": 0
                },
                {
                    "sent": "Then the algorithm will change.",
                    "label": 0
                },
                {
                    "sent": "Their behavior will focus more on the ones that are more important.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, but this will appear very often reductions because one key thing that we do is change the distribution of examples.",
                    "label": 0
                },
                {
                    "sent": "So now how do we change the distribution of examples?",
                    "label": 1
                },
                {
                    "sent": "So I was giving an example where we wanted to change the distribution examples.",
                    "label": 0
                },
                {
                    "sent": "We are replicating example, so we want to have examples with more weight.",
                    "label": 0
                },
                {
                    "sent": "We're going to replicate the examples.",
                    "label": 0
                },
                {
                    "sent": "This is approach.",
                    "label": 0
                },
                {
                    "sent": "Would be.",
                    "label": 0
                },
                {
                    "sent": "Which is the approach that we call resampling with replacement, which would be equivalent to like a roulette wheel algorithm where you have a roulette wheel with the size here.",
                    "label": 1
                },
                {
                    "sent": "Size here being proportional to the weight.",
                    "label": 0
                },
                {
                    "sent": "And due to the weights of the examples, so we have like one part of for each example with weight with size proportional to the weight of the example and then you would sample examples proportionally to the weights and you allowed replicates.",
                    "label": 0
                },
                {
                    "sent": "So you would go replicates would go select.",
                    "label": 0
                },
                {
                    "sent": "Each one example at a time and then you would go through this wheel, selecting one example and you get examples proportionately to the way this would be equivalent to replicating, except that if you have real value costs then.",
                    "label": 0
                },
                {
                    "sent": "You get the correct distribution from this.",
                    "label": 0
                },
                {
                    "sent": "But the problem with this is that it actually gives you duplicate examples because it's sample with replacement so.",
                    "label": 0
                },
                {
                    "sent": "Your one example may appear more than than once on your knew training sets, and especially the ones with bigger weights.",
                    "label": 0
                },
                {
                    "sent": "They're going to appear very often, whereas the ones with lower weight are not going to appear.",
                    "label": 0
                },
                {
                    "sent": "Some of them are not going to appear.",
                    "label": 0
                },
                {
                    "sent": "So in practice, we see that this is not a good approach for solving the importance weighted problem because.",
                    "label": 0
                },
                {
                    "sent": "A lot of example, a lot of algorithms cannot do well do well with these replicate examples, and in fact this makes the.",
                    "label": 0
                },
                {
                    "sent": "Resample said not being drawn in the penalty firm from the prime, because if we draw if we had a D prime at Rudy Prime distribution where we could sample examples from where you would not expect to see so many duplicate examples of.",
                    "label": 0
                },
                {
                    "sent": "You can see there's something wrong with that.",
                    "label": 0
                },
                {
                    "sent": "Um so.",
                    "label": 0
                },
                {
                    "sent": "A possible approach would be to do re sampling without replacement, which is sampling M examples from a set of size M using the same roulette wheel approach.",
                    "label": 1
                },
                {
                    "sent": "So which means that once an example comes, you select that example, you wouldn't select that example again.",
                    "label": 0
                },
                {
                    "sent": "And that is not a very good because because also, we're not going to have examples drawn from the prime in the case, for instance, where you draw am examples from the regional data set without replacement were going exactly the same training set back, so that's not.",
                    "label": 0
                },
                {
                    "sent": "Good good approach.",
                    "label": 0
                },
                {
                    "sent": "So the approach that we use more often when we want to change distributions in reduction.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is the rejection sampling approach.",
                    "label": 1
                },
                {
                    "sent": "Where?",
                    "label": 0
                },
                {
                    "sent": "And instead of doing re sampling with replacement or re sampling without replacement and getting examples randomly from the training sets, we actually.",
                    "label": 0
                },
                {
                    "sent": "Go through all the examples and decide whether or not to keep the example with probability proportional to.",
                    "label": 1
                },
                {
                    "sent": "See so it's pictorial.",
                    "label": 0
                },
                {
                    "sent": "You can think about going through the training set and deciding to a keeper, not this example in the way you decide is flip a coin Ann, and compare with bias C / C, where C hat is.",
                    "label": 1
                },
                {
                    "sent": "Would be like the maximum and upper bound value on on C, so you'd have to have an upper bound value on the possible values on your weights so that you can normalize.",
                    "label": 1
                },
                {
                    "sent": "All this seems to be between zero and one.",
                    "label": 0
                },
                {
                    "sent": "Then you randomly decide whether to keep example with probability of C. / C had you go through the training set and you get a new training set, which now is going to be much smaller than.",
                    "label": 0
                },
                {
                    "sent": "The original training set because some examples will have.",
                    "label": 0
                },
                {
                    "sent": "Very small weights and these examples are going to appear in the training set with very very small probability and some are going to have bigger weights which are going to be closer to 1.",
                    "label": 0
                },
                {
                    "sent": "The probability that you're going to see them in the training set.",
                    "label": 0
                },
                {
                    "sent": "So now you're going to have.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "A much smaller training set, but at least you can.",
                    "label": 0
                },
                {
                    "sent": "We have a re sample data set which is drawn independently from the head, which is what we want for the reduction.",
                    "label": 0
                },
                {
                    "sent": "So if we use rejection sampling, we get the right distribution that we want for we can plug this knew training set into the origonal into the binary classifier learner, and we're going to learn from the correct distribution, minimizing the correct loss.",
                    "label": 0
                },
                {
                    "sent": "So this is what we will want.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the algorithm actually that we are using practice.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Does the same does the rejection sampling for a number of times, so it's like a bagging style algorithm which we call costing which repeats this rejection sampling more than once.",
                    "label": 0
                },
                {
                    "sent": "It does that Prince 10 times like it's says that and creates from the original training sets 10 different.",
                    "label": 0
                },
                {
                    "sent": "Knew training sets which are drawn from the correct distribution through rejection sampling and we train a classifier for each one of these, so they're all doing the same thing which is minimizing the weighted loss function.",
                    "label": 0
                },
                {
                    "sent": "And when we want to classify an example, we do a majority voting, so we get 10 to 10 different hypothesis on an example X and we make them vote.",
                    "label": 1
                },
                {
                    "sent": "So this is the approach that we.",
                    "label": 0
                },
                {
                    "sent": "This is what the reduction that we use.",
                    "label": 0
                },
                {
                    "sent": "We could have this by being just one and then just run once and get.",
                    "label": 0
                },
                {
                    "sent": "The hypothesis would be minimizing the correct.",
                    "label": 0
                },
                {
                    "sent": "The correct distribution, but because of for reducing variance because each time we do this we're losing some of the examples.",
                    "label": 0
                },
                {
                    "sent": "We do this many times just to recover.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That so this is a some results that we had on the KDD 98 data set applying the costing reduction using as classification algorithms.",
                    "label": 0
                },
                {
                    "sent": "Naive bayes.",
                    "label": 0
                },
                {
                    "sent": "This is a boosted naive Bayes, C 4.5 and SVM.",
                    "label": 0
                },
                {
                    "sent": "Then line up here with you were the results from the.",
                    "label": 0
                },
                {
                    "sent": "Katie did 98 Cup which is about 15,000 the profit.",
                    "label": 0
                },
                {
                    "sent": "Which I mean, you can show that the skated United Cup isn't is.",
                    "label": 0
                },
                {
                    "sent": "You can solve it using importance weighted classification and then the green results are using.",
                    "label": 0
                },
                {
                    "sent": "Resampling with replacement and the blue results are using the rejection sampling.",
                    "label": 0
                },
                {
                    "sent": "The actual costing reduction that we talked about.",
                    "label": 0
                },
                {
                    "sent": "So we see that we get an improvement specially for C 4.5 because there is sampling with replacement doesn't work very well here.",
                    "label": 0
                },
                {
                    "sent": "In the case of C 4.5.",
                    "label": 0
                },
                {
                    "sent": "So now that we've seen a simple example of reduction, I'm going to make some statements about why we want to continue working with reductions and want to try to scale.",
                    "label": 0
                },
                {
                    "sent": "Use them to solve more complex problems.",
                    "label": 0
                },
                {
                    "sent": "So first is that they work well in practice, so there are lots of examples where we.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Elections have succeeded, for example in.",
                    "label": 0
                },
                {
                    "sent": "Multiclass classification.",
                    "label": 0
                },
                {
                    "sent": "Using binary classification we have good results.",
                    "label": 1
                },
                {
                    "sent": "Boosting also can be seen as a reduction from weak learnability too.",
                    "label": 0
                },
                {
                    "sent": "Strong learnability, so it's also case where reductions have worked well in practice.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "We can, so as I said from the beginning, we can you reuse highly optimized learning algorithms which have been developed to solve the binary classification problem.",
                    "label": 1
                },
                {
                    "sent": "And even if people continue working on binary classification, continue getting good results, we can actually use them for.",
                    "label": 0
                },
                {
                    "sent": "For this is getting and different new algorithms for solving a problem from and different base algorithms that we have.",
                    "label": 1
                },
                {
                    "sent": "We're going to be able to use them and get different algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Solving the new problems.",
                    "label": 0
                },
                {
                    "sent": "One very important aspect is this one, that I think is this theoretical aspect because you can transfer performance guarantees from one problem to another.",
                    "label": 1
                },
                {
                    "sent": "So because assigning the reductions we always show how the error in the original problem transfers to the error or the are they regret.",
                    "label": 0
                },
                {
                    "sent": "As we'll see later in the problem that we care about.",
                    "label": 0
                },
                {
                    "sent": "So we don't make any assumptions.",
                    "label": 0
                },
                {
                    "sent": "All the assumptions are going to be like hidden in the actual binary classification algorithm that you're going to use, and we're going to be able to give very precise guarantees of the type if you get epsilon performance in the binary classification problem, then you get this.",
                    "label": 0
                },
                {
                    "sent": "K Times epsilon performance, something like that in the regional.",
                    "label": 0
                },
                {
                    "sent": "In the new problem so.",
                    "label": 0
                },
                {
                    "sent": "These are relative performance guarantees, but in a sense there they are very strong because you don't have to make any assumptions to get them you.",
                    "label": 0
                },
                {
                    "sent": "If you can guarantee the performance on the original problem, then the care is true.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Get lost.",
                    "label": 0
                },
                {
                    "sent": "But the assumption is mostly about minimizing the 01 loss on the training set, no, no, it's minimizing the 01 loss on the distribution.",
                    "label": 0
                },
                {
                    "sent": "On the distribution of examples that from which the training examples were drawn.",
                    "label": 0
                },
                {
                    "sent": "So, so that's not minimum.",
                    "label": 0
                },
                {
                    "sent": "I mean, the original assumption is that we have a very good classification algorithm which can minimize 01 loss on the distribution of examples from where the training examples.",
                    "label": 0
                },
                {
                    "sent": "And then the fourth point here is that reductions compose, so.",
                    "label": 1
                },
                {
                    "sent": "What this means is that you can get as we'll see later examples that John and Eileen are going to talk about.",
                    "label": 1
                },
                {
                    "sent": "In many cases, we're going to reduce from, say, multiclass classification to importance weighted classification.",
                    "label": 0
                },
                {
                    "sent": "Then we know that importance weighted classification, which reduces to binary classification.",
                    "label": 0
                },
                {
                    "sent": "So we can start like building bigger and bigger blocks from small blocks, so this comparable composing effect is very interesting.",
                    "label": 1
                },
                {
                    "sent": "So in general.",
                    "label": 0
                },
                {
                    "sent": "In technology and science, we want, you know, solve bigger problems by breaking it into subproblems and reductions give away for us to do this.",
                    "label": 0
                },
                {
                    "sent": "And also the fifth point is that you see, like in machine learning.",
                    "label": 0
                },
                {
                    "sent": "All these knew different problems are are coming up and we don't have like an unified theory of machine learning.",
                    "label": 0
                },
                {
                    "sent": "How do these problems relate?",
                    "label": 0
                },
                {
                    "sent": "What are the common aspects between them?",
                    "label": 0
                },
                {
                    "sent": "How difficult are them one regarding the order, so I think that we lack like a backbone theoretical backbone for machine learning and this might be a way of going about doing this.",
                    "label": 0
                },
                {
                    "sent": "Organizing prediction problems in general into one framework.",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                },
                {
                    "sent": "One also, very why relations?",
                    "label": 0
                },
                {
                    "sent": "Why won't we want to?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do this.",
                    "label": 0
                },
                {
                    "sent": "So next I'm going to be talking about one reduction from quantile classification to solve the quantized sorry, pontile regression problem.",
                    "label": 0
                },
                {
                    "sent": "And then later, Alina is going to talk about multiclass classification, cost sensitive multiclass classification also.",
                    "label": 0
                },
                {
                    "sent": "And then a little bit also about ranking reductions from ranking to classification.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "John is going to be talking about learning with partial feedback.",
                    "label": 0
                },
                {
                    "sent": "Should be the later part of the talk, so this is an outline of the rest of the talk, so I'm going to talk about for about more 10 minutes about quantile regression.",
                    "label": 0
                },
                {
                    "sent": "Then we have a break after that.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So quantile regression.",
                    "label": 0
                },
                {
                    "sent": "I don't know if many people have heard about this before, so we usually when we talk about regression so we know the regression we have.",
                    "label": 0
                },
                {
                    "sent": "We want to predict the real value number.",
                    "label": 0
                },
                {
                    "sent": "But usually when we talk about regression we're talking about.",
                    "label": 0
                },
                {
                    "sent": "Oh then the mean regression to the mean when where we want to estimate the mean value of the function.",
                    "label": 0
                },
                {
                    "sent": "The conditional mean value of the function.",
                    "label": 0
                },
                {
                    "sent": "So usually we're going to minimize squared error loss, which would lead to finding the conditional mean value of the function.",
                    "label": 0
                },
                {
                    "sent": "So this is the standard regression.",
                    "label": 0
                },
                {
                    "sent": "So there is another former regression which is called the quantile regression, and here we're going to show how to reduce the quantile regression problem into the classification problem so.",
                    "label": 0
                },
                {
                    "sent": "This problem instead of.",
                    "label": 0
                },
                {
                    "sent": "Finding the conditional mean you want to find a conditional quantile distribution.",
                    "label": 0
                },
                {
                    "sent": "For instance, the median is respect is a special case of the other can't.",
                    "label": 0
                },
                {
                    "sent": "I'll so we'll see that later in a little bit more detail and advantages that we find the conditional mean function is more.",
                    "label": 0
                },
                {
                    "sent": "It's going to be more robust to outliers than conditional.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the condition median is going to be more robust than condition mean and also.",
                    "label": 0
                },
                {
                    "sent": "We can, by predicting other quantiles, we can describe different segments of the conditional distribution, so we don't have to just predict the mean or the median.",
                    "label": 1
                },
                {
                    "sent": "We can predict the 90th quantile the 80th time, depending on the application.",
                    "label": 0
                },
                {
                    "sent": "This can be interesting.",
                    "label": 0
                },
                {
                    "sent": "There are other.",
                    "label": 0
                },
                {
                    "sent": "There have been other methods for solving the quantile regression problem.",
                    "label": 1
                },
                {
                    "sent": "So the first known method is conquers method for the linear quantile regression, which assumes that the conditional quantizer linear function of X.",
                    "label": 0
                },
                {
                    "sent": "So this would be the equivalent of.",
                    "label": 0
                },
                {
                    "sent": "Linear regression to the mean squared error loss here would have a linear, so it makes an assumption about the form of the function, which we will not make in production, except that if you use like a linear classifier then we'd have some kind of linearity assumption.",
                    "label": 0
                },
                {
                    "sent": "So as I said, the assumptions are hidden within the binary classification.",
                    "label": 0
                },
                {
                    "sent": "If you have a better or binary classification algorithm which does not make strong assumptions, then this will carry through the reduction.",
                    "label": 0
                },
                {
                    "sent": "The other approach is the.",
                    "label": 1
                },
                {
                    "sent": "It's a kernel approach.",
                    "label": 0
                },
                {
                    "sent": "Nonparametric quantile estimation method which shows that you can solve the nonparametric anti estimation problem as a QP.",
                    "label": 0
                },
                {
                    "sent": "So there's no assume predetermined form.",
                    "label": 0
                },
                {
                    "sent": "But then you're gonna have to choose the kernel and apparent para meters and all that as you doing for diffuse, like any like an SVM.",
                    "label": 0
                },
                {
                    "sent": "For classification, we're going to have to do that here for the quantile problem.",
                    "label": 0
                },
                {
                    "sent": "So it is an approach which is inspired by.",
                    "label": 0
                },
                {
                    "sent": "Approach classification approaches, but is the modification for this particular problem.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to make things more clear, what I'm going to define what is a conditional Q quantile?",
                    "label": 0
                },
                {
                    "sent": "Where Q is a value between zero and one, so the.",
                    "label": 0
                },
                {
                    "sent": "In the case of the medium medium, the the Q is 0.5, so.",
                    "label": 0
                },
                {
                    "sent": "What we want to act a conditional quantiles is a value such that the distribution has probability Q of being below.",
                    "label": 0
                },
                {
                    "sent": "Oh I have so I have X is the is G function right?",
                    "label": 1
                },
                {
                    "sent": "So there is a probability of Q of being below F of X, the conditional quantile being the conditional value of the distribution being below.",
                    "label": 0
                },
                {
                    "sent": "There's a probability of Q, an probability one of minus Q of being above that value.",
                    "label": 0
                },
                {
                    "sent": "So for instance, the medium is a value such that the condition distribution that has .5 probability of being above that value and .5 probability of being below the value.",
                    "label": 0
                },
                {
                    "sent": "So the basic observation is that.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "If you minimize the the this function here, this loss function, which is the absolute loss.",
                    "label": 0
                },
                {
                    "sent": "Instead of minimizing the squared error loss, then you'll find the conditional medium.",
                    "label": 0
                },
                {
                    "sent": "So this is the basically quantile.",
                    "label": 0
                },
                {
                    "sent": "Regression is about regression using this loss function, whereas squared error, loss regression.",
                    "label": 0
                },
                {
                    "sent": "You'd have a squared error loss here and then would be finding the conditional mean.",
                    "label": 0
                },
                {
                    "sent": "This is the only difference.",
                    "label": 1
                },
                {
                    "sent": "So the conditional medium using this uses this loss.",
                    "label": 0
                },
                {
                    "sent": "What about all the quantiles?",
                    "label": 0
                },
                {
                    "sent": "So if you want to find the 80th quantile, which means that Q = 0.8, then you'd have this pin pinball loss where.",
                    "label": 0
                },
                {
                    "sent": "The last four above.",
                    "label": 0
                },
                {
                    "sent": "Zero if you make errors so, so you can think about this as saying that error if you say the true value is 5.",
                    "label": 0
                },
                {
                    "sent": "If you say that it's six it is or like 6 is worse than saying that it's for the painting on the loss.",
                    "label": 0
                },
                {
                    "sent": "If you care more about being right on the upside or being right on the wrong side.",
                    "label": 0
                },
                {
                    "sent": "So so.",
                    "label": 0
                },
                {
                    "sent": "So this will shift your errors for being more positive or being more negative depending on the quantile values.",
                    "label": 0
                },
                {
                    "sent": "So, but the loss function.",
                    "label": 0
                },
                {
                    "sent": "Is going to be the.",
                    "label": 0
                },
                {
                    "sent": "This loss function here where you wait the example where you there is weighted by.",
                    "label": 0
                },
                {
                    "sent": "The how if it's below or above.",
                    "label": 0
                },
                {
                    "sent": "The If it's positive or negative is going to be wait.",
                    "label": 0
                },
                {
                    "sent": "If it's positive is weighted by Q and if it's negative.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's gonna be weighted by.",
                    "label": 0
                },
                {
                    "sent": "1 -- Q.",
                    "label": 0
                },
                {
                    "sent": "So the reduction from quantile regression to binary classification actually will reduce using what we call the quantum reduction from quantile regression to importance weighted classification and then from part is very classification where you can use causing to reduce to binary classification.",
                    "label": 0
                },
                {
                    "sent": "So basically reduced the importance weighted classification.",
                    "label": 0
                },
                {
                    "sent": "So what here is just a formal definition of the quantile regression problem.",
                    "label": 1
                },
                {
                    "sent": "So we have examples draw from the Ann.",
                    "label": 0
                },
                {
                    "sent": "We want to find this mapping from X to 01.",
                    "label": 0
                },
                {
                    "sent": "So here we are making this interval B 001.",
                    "label": 0
                },
                {
                    "sent": "So we're assuming that the the wise are also in the interval 01 which practice you could have regressions, the the value could be any real number, but here we are restricting it to be in the interval 01.",
                    "label": 0
                },
                {
                    "sent": "So if you have a problem where you have example values of bigger an unrestricted, then you have to normalize them to be in the 01 interval so.",
                    "label": 1
                },
                {
                    "sent": "The quantile regression that we are assuming the values are restricted.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bounded there between 0 and.",
                    "label": 0
                },
                {
                    "sent": "And we want to find an estimate of the CUV conditional quantile of the given X.",
                    "label": 0
                },
                {
                    "sent": "So we want to find that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What the?",
                    "label": 0
                },
                {
                    "sent": "OK, I'll just move to this.",
                    "label": 0
                },
                {
                    "sent": "So what the actual algorithm is going to do is you get the actual reduction is going to do is you get the regional training set as and you create T training sets where each one of them.",
                    "label": 0
                },
                {
                    "sent": "Is created in.",
                    "label": 0
                },
                {
                    "sent": "In this fashion you.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "You have you get each example YXY where X is any feature.",
                    "label": 0
                },
                {
                    "sent": "The feature values and Y is the.",
                    "label": 0
                },
                {
                    "sent": "The label, which is a value between zero and one.",
                    "label": 0
                },
                {
                    "sent": "And then we make the label B1 if the value of Y is below some value T. Which is.",
                    "label": 0
                },
                {
                    "sent": "Sorry, is above the value T which is you can think about it as A's agreed value.",
                    "label": 0
                },
                {
                    "sent": "So the grade from zero and one you have for each you do like zero, 0.01, zero point, 0 two.",
                    "label": 0
                },
                {
                    "sent": "And you do this for each value of T. So if for the first classifier and say T is a is a small value, so.",
                    "label": 0
                },
                {
                    "sent": "Most of the examples will have one for the label here.",
                    "label": 0
                },
                {
                    "sent": "Only the ones that have values below T would have 0.",
                    "label": 0
                },
                {
                    "sent": "And the weights of the examples.",
                    "label": 0
                },
                {
                    "sent": "So we're reducing the importance weighted classification.",
                    "label": 0
                },
                {
                    "sent": "So each example is going to have.",
                    "label": 0
                },
                {
                    "sent": "XY, and this is the weight.",
                    "label": 0
                },
                {
                    "sent": "So the wait is going to be Q.",
                    "label": 0
                },
                {
                    "sent": "If the example is positive and 1 -- Q if the example is negative, which means that for each particular value of T Now that we have in the grid if the example is above that value, it's going to have label one and wait queue.",
                    "label": 0
                },
                {
                    "sent": "If it's below, the value is going to have labels zero and wait 1 -- Q.",
                    "label": 0
                },
                {
                    "sent": "So we do this for Grid an.",
                    "label": 0
                },
                {
                    "sent": "We can use an values of T so agreed with equal space grid values and would be like zero 1 / N. An minus 1 / N one so would have equal space, so we can show that if we use an equal space B and then we add in most 1 / N term to the law.",
                    "label": 0
                },
                {
                    "sent": "So if we add more and more classifiers, make a finer grid, get the value of an bigger, then we're going to be reducing the loss.",
                    "label": 0
                },
                {
                    "sent": "And in practice, we can choose the grid as we go, like make that make it be more adaptive.",
                    "label": 0
                },
                {
                    "sent": "So we see that we need more classifiers in a in a place we add more crossfire.",
                    "label": 0
                },
                {
                    "sent": "So this is a detail, but you can think about is the equal space going from each classifier is going to make a prediction whether or not the example is above or below some value zero 0.01 zero point 0 two 0.99.",
                    "label": 0
                },
                {
                    "sent": "And each of them is making that prediction, but the prediction.",
                    "label": 0
                },
                {
                    "sent": "But each classifier is trained with weights so.",
                    "label": 0
                },
                {
                    "sent": "So then it's going to do the right thing if you don't put the weight is not good.",
                    "label": 0
                },
                {
                    "sent": "Do the right thing so you have to have the weights.",
                    "label": 0
                },
                {
                    "sent": "And then to predict we get the example run through all the T classifiers.",
                    "label": 0
                },
                {
                    "sent": "And get the expected value of the answer, which each one of them is going to say 01.",
                    "label": 0
                },
                {
                    "sent": "So if.",
                    "label": 0
                },
                {
                    "sent": "We take the average from their response and because we are predicting a value between 01, this is going to be the prediction that we're going to make so we can show that this is the.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Correct prediction.",
                    "label": 0
                },
                {
                    "sent": "So this is pictorially.",
                    "label": 0
                },
                {
                    "sent": "So you can in.",
                    "label": 0
                },
                {
                    "sent": "Each classifier here is going to be predicting whether or not X is Bill is.",
                    "label": 0
                },
                {
                    "sent": "Below some threshold, so you'd expect that the predictions are going to be 1111 until the point where the actual value.",
                    "label": 0
                },
                {
                    "sent": "Is gonna be the threshold is going to be the correct value so after that everyone is going to be above.",
                    "label": 0
                },
                {
                    "sent": "All the classifiers are going to say that before this they're all going to say that the value is is below the correct value, and then when he gets to the correct value after this, it's going to say that all all of them are going to say that they are above the value of T. But of course, the classifiers make mistakes, so you could have a case where you have 1101101 and then at the end but expects to be 0.",
                    "label": 0
                },
                {
                    "sent": "'cause we're saying predicting whether or not is above 0.999.",
                    "label": 0
                },
                {
                    "sent": "Something like that, so we don't care about whether it makes mistakes.",
                    "label": 0
                },
                {
                    "sent": "We just take the average.",
                    "label": 0
                },
                {
                    "sent": "And we can show that this this works.",
                    "label": 0
                },
                {
                    "sent": "Some sense.",
                    "label": 0
                },
                {
                    "sent": "So predicting.",
                    "label": 0
                },
                {
                    "sent": "Here we're doing a case where we learn one classifier per.",
                    "label": 0
                },
                {
                    "sent": "Threshold, whereas we could also learn one classifier with an extra feature T, But this would be relying on the classifier to do this.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we don't do that kind of.",
                    "label": 0
                },
                {
                    "sent": "Is consistent.",
                    "label": 0
                },
                {
                    "sent": "OK so I.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we show?",
                    "label": 0
                },
                {
                    "sent": "Is this I'm running out of time, but what we show is that.",
                    "label": 0
                },
                {
                    "sent": "The regret in estimating the medium.",
                    "label": 1
                },
                {
                    "sent": "It's gonna be less than the importance weighted regret.",
                    "label": 0
                },
                {
                    "sent": "So regret was what does this mean?",
                    "label": 0
                },
                {
                    "sent": "Is the difference between the this would be the expected value of the.",
                    "label": 0
                },
                {
                    "sent": "Loss of Y minus the median of exo medium wax would be the best you could do in, you know, because the problem is known.",
                    "label": 0
                },
                {
                    "sent": "Deterministic, so the median is would be the conditional median.",
                    "label": 0
                },
                {
                    "sent": "The correct median of the function.",
                    "label": 0
                },
                {
                    "sent": "So this would be the minimum possible loss because for each value of X there is a distribution of values of Y, so this would be the minimum.",
                    "label": 0
                },
                {
                    "sent": "Possible loss, and this is the last that we get with the counting algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this difference we called the regret.",
                    "label": 0
                },
                {
                    "sent": "Which is how much we lose by using the quality algorithm versus predicting the best possible function.",
                    "label": 0
                },
                {
                    "sent": "And we can show that this is bounded by the importance weighted regret, which is the law.",
                    "label": 0
                },
                {
                    "sent": "Last that we get.",
                    "label": 1
                },
                {
                    "sent": "On the Deprime distribution with, which is the distribution with weights versus the minimum possible loss.",
                    "label": 0
                },
                {
                    "sent": "That we get.",
                    "label": 0
                },
                {
                    "sent": "So if we have an algorithm which is perfect for doing partners weighted classification, that would have perfect.",
                    "label": 0
                },
                {
                    "sent": "Perfect quantile regression ALS.",
                    "label": 0
                },
                {
                    "sent": "I am.",
                    "label": 0
                },
                {
                    "sent": "If we compose this with the binary classification.",
                    "label": 0
                },
                {
                    "sent": "Reduction, I mean there is actually costing cost reduction because C is bounded by by one.",
                    "label": 0
                },
                {
                    "sent": "Since see the see, here is the weight and the weights are all Q or 1 -- Q.",
                    "label": 0
                },
                {
                    "sent": "Then we have C less than one, which means that the regret in estimating the medium is less than the binary breath.",
                    "label": 1
                },
                {
                    "sent": "For no so this for all contact simultaneous.",
                    "label": 0
                },
                {
                    "sent": "They hold for any contact.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Scale stop somewhere in between because you're pointing out or it can return something from zero and one in the media is not supported.",
                    "label": 0
                },
                {
                    "sent": "No so so we are assuming that the Y values are between 01.",
                    "label": 0
                },
                {
                    "sent": "So before you even start doing anything with the problem you have to re normalize your problem so the medium is going to be also be between 01 because you have assuming that we have a special.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Quantile regression problem where the values are between 01.",
                    "label": 0
                },
                {
                    "sent": "So here is just a prefer some slides with the prediction performance where we compare the quantic wanting reduction using logistic regression and J48 with which is decision tree.",
                    "label": 0
                },
                {
                    "sent": "Against.",
                    "label": 0
                },
                {
                    "sent": "The linear quantile regression and the kernel quantile regression, the Y values their normalized quantile loss.",
                    "label": 1
                },
                {
                    "sent": "So for each problem here, the entire loss could be we are using the.",
                    "label": 0
                },
                {
                    "sent": "We are normalizing it so that the best result is 1.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the worst result is 1.",
                    "label": 0
                },
                {
                    "sent": "The worst result is all is going to be one and the best result is, like in this case is about 0.8 so.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "One thing with shape for it is getting the best results for this data set with different quantiles.",
                    "label": 0
                },
                {
                    "sent": "Your .1 zero point 5 and 0.9.",
                    "label": 0
                },
                {
                    "sent": "This is another data set where.",
                    "label": 0
                },
                {
                    "sent": "Results there closer.",
                    "label": 0
                },
                {
                    "sent": "There's a California housing datasets again J 480 quality with my friend is doing better there.",
                    "label": 0
                },
                {
                    "sent": "And here is the Boston housing the sun where.",
                    "label": 0
                },
                {
                    "sent": "Quantity with logistics regression is doing better.",
                    "label": 0
                },
                {
                    "sent": "The kernel method is doing well for the 0.1 and 0.9 cases, so this is a comparison.",
                    "label": 0
                },
                {
                    "sent": "We have like more if you look at the papers, we have much more detailed comparison with numbers and lot more data centers.",
                    "label": 0
                },
                {
                    "sent": "So the difficulty with like this is the kernel method is that.",
                    "label": 0
                },
                {
                    "sent": "We have to optimize for, you know, finding the.",
                    "label": 0
                },
                {
                    "sent": "Doing like cross validation many times like this is slow process and not always.",
                    "label": 0
                },
                {
                    "sent": "Think we?",
                    "label": 0
                },
                {
                    "sent": "We did that but.",
                    "label": 0
                },
                {
                    "sent": "It makes like it's going to make you strong assumptions, whereas here, for instance, we're plugging logistic regression and J48.",
                    "label": 0
                },
                {
                    "sent": "So depending on the problem, one of them is doing better than others to be better here.",
                    "label": 0
                },
                {
                    "sent": "Logistic regression is there, so we have this flexibility of choosing the base classifier, but I don't have so.",
                    "label": 0
                },
                {
                    "sent": "As we know, we can't predict that for classification.",
                    "label": 0
                },
                {
                    "sent": "Oh no, how it grows with a like a learning curve kind of thing.",
                    "label": 0
                },
                {
                    "sent": "No, that's good.",
                    "label": 0
                },
                {
                    "sent": "We use the whole datasets from.",
                    "label": 0
                },
                {
                    "sent": "For out for the regular squared, yes.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we're gonna stop, and then Alina is gonna come back in 5 minutes to start talking about the multiclass classification.",
                    "label": 0
                },
                {
                    "sent": "I don't.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's OK, I'll just hold it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Alright, will begin again.",
                    "label": 0
                },
                {
                    "sent": "Kate.",
                    "label": 0
                },
                {
                    "sent": "I will show to other families of loss functions that can be optimized with an algorithm that's designed to optimize 01 loss.",
                    "label": 0
                },
                {
                    "sent": "There are pretty general families of functions.",
                    "label": 0
                },
                {
                    "sent": "The first one basically can encode any loss function defined on individual examples with discrete output spaces that can potentially be very large.",
                    "label": 1
                },
                {
                    "sent": "I'll describe productions that are pretty efficient and work for large decision spaces.",
                    "label": 0
                },
                {
                    "sent": "So let's begin with multiclass.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Classification it's a simple generalization of the binary classification problem that Bianca described.",
                    "label": 0
                },
                {
                    "sent": "We have some distribution D overacts cross Y where Y is a key label space and we want to find a classifier where mapping from acts the way that minimizes the probability that when we draw an example from D, this classifier disagrees with the label.",
                    "label": 1
                },
                {
                    "sent": "Right, so I'm sure you all know the multipla.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Classification of very simple reduction that's commonly used for this problem is so-called one against all.",
                    "label": 0
                },
                {
                    "sent": "You create key binary classification problems again.",
                    "label": 0
                },
                {
                    "sent": "Case the number of classes.",
                    "label": 0
                },
                {
                    "sent": "Where for class Y1 per class for a class I you're trying to predict is the label I or not distinguishing this class from all the other classes?",
                    "label": 1
                },
                {
                    "sent": "Right, so when you have a multiclass training example, you create key binary examples, one per class, and the binary label is just the indicator function, which tells you whether the label is I for a problem I.",
                    "label": 0
                },
                {
                    "sent": "It's a very common reduction.",
                    "label": 0
                },
                {
                    "sent": "None you learn key classifiers when you want to make a multiclass prediction on and you ask you, but you're on all these classifiers on acts.",
                    "label": 0
                },
                {
                    "sent": "Some of them will predict 0, some of them will predict one, you randomize over the ones that predict one or over all the labels.",
                    "label": 0
                },
                {
                    "sent": "If none of them predicts predicts one.",
                    "label": 0
                },
                {
                    "sent": "And again, we can use for the analysis.",
                    "label": 0
                },
                {
                    "sent": "We can use this simple trick where we add the index of the problem into the features so we can think of learning a single classifier.",
                    "label": 0
                },
                {
                    "sent": "We give it acts and acts and then index of the problem and it predicts whether or not the label of this axe is I.",
                    "label": 0
                },
                {
                    "sent": "The benefit of learning a single classifier, at least in theory, is that you can think of this reduction as a transformation that takes a multiclass distribution and converts it into a binary distribution.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is just the transformation that takes a multiclass distribution.",
                    "label": 0
                },
                {
                    "sent": "Distribution number multiclass K. Class examples and converts it into a distribution over binary examples.",
                    "label": 0
                },
                {
                    "sent": "And there is a well defined notion of being used binary distribution Q.",
                    "label": 0
                },
                {
                    "sent": "This is a process that you can use to generate examples from the distribution.",
                    "label": 0
                },
                {
                    "sent": "You draw a multiclass example from D, then you draw a random I index of the problem, and then you output.",
                    "label": 0
                },
                {
                    "sent": "Accent I, as your feature vector and the indicator value weather wise.",
                    "label": 0
                },
                {
                    "sent": "One or not.",
                    "label": 0
                },
                {
                    "sent": "It should be high.",
                    "label": 0
                },
                {
                    "sent": "There should be an.",
                    "label": 0
                },
                {
                    "sent": "OK um.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's look at a theorem that can come with that.",
                    "label": 0
                },
                {
                    "sent": "We can prove about this reduction for all multiclass distributions D and all binary classifiers F. The induced multiclass loss.",
                    "label": 1
                },
                {
                    "sent": "It's bounded by Chemainus Time Key minus one times the binary loss of the classifier on the induced distribution.",
                    "label": 0
                },
                {
                    "sent": "Right, so again, this is.",
                    "label": 0
                },
                {
                    "sent": "The definition of the induced distribution.",
                    "label": 0
                },
                {
                    "sent": "And we have a theorem that holds for all multiclass problems and all binary classifiers.",
                    "label": 0
                },
                {
                    "sent": "The result in multiclass laws of the argmax classifier of the one against all classifier is bounded by key minus one times the binary loss of the classifier that we use classifier F on the induced distribution Q.",
                    "label": 0
                },
                {
                    "sent": "So it's very instructive to look at the proof, and the proof is very simple, so it's convenient to look at half as an adversary trying to induce a large multiclass regret without.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry, multiclass loss, without making a lot of binary mistakes, so there's three different failure modes.",
                    "label": 0
                },
                {
                    "sent": "The first one is in green, shown in green.",
                    "label": 0
                },
                {
                    "sent": "You can have a false negative and no false positives.",
                    "label": 0
                },
                {
                    "sent": "Which means that you will eat all the classifiers and all of them predict zeros.",
                    "label": 0
                },
                {
                    "sent": "The reduction can do nothing better than just pick a random label.",
                    "label": 0
                },
                {
                    "sent": "Which means, since there is only one label that's correct, the reduction will make a mistake and K -- 1 over key cases fraction of the time over the randomness in the reduction.",
                    "label": 0
                },
                {
                    "sent": "And that's induced with a single binary mistake.",
                    "label": 0
                },
                {
                    "sent": "So single binary mistake.",
                    "label": 0
                },
                {
                    "sent": "Out of key, possible binary mistakes can make the multiclass classifier error K -- 1 over key fraction of the times.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are also two other failure modes, so you can have a positive number of false positives and no false negative, and you can similarly analyze these cases, But the first case is the most efficient way for the adversary to make mistakes.",
                    "label": 0
                },
                {
                    "sent": "Where again we're thinking of F. This binary classifier that we use.",
                    "label": 0
                },
                {
                    "sent": "To build our multiclass classifier as an adversary, because we want the worst case analysis.",
                    "label": 0
                },
                {
                    "sent": "The reason why I gave this analysis is that you can observe that there isn't a symmetry in the failure modes.",
                    "label": 0
                },
                {
                    "sent": "And they make.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the classifier less likely to make a false negative.",
                    "label": 0
                },
                {
                    "sent": "You can actually improve their reduction.",
                    "label": 0
                },
                {
                    "sent": "So by doing this analysis you can observe this symmetry.",
                    "label": 0
                },
                {
                    "sent": "And you can get instead of reducing to binary classification.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can reduce the importance weighted classification that Bianca talked about and then use the costing method to reduce the binary classification and you will get a factor of two improvement just by making this observation and theory and a consistent improvement in practice.",
                    "label": 1
                },
                {
                    "sent": "So the point again, the point that I'm trying to make is that by doing this type of analysis you can drive.",
                    "label": 0
                },
                {
                    "sent": "You can derive a better reduction.",
                    "label": 1
                },
                {
                    "sent": "But we still have a problem that there is a dependence on key, so the multiclass even was this fix.",
                    "label": 0
                },
                {
                    "sent": "The word in the worst case, the multiclass regret, will be keyword two times the binary regret.",
                    "label": 0
                },
                {
                    "sent": "Which is pretty bad for problems where Ki is large.",
                    "label": 0
                },
                {
                    "sent": "Can we make it more robust?",
                    "label": 0
                },
                {
                    "sent": "Yes we can.",
                    "label": 0
                },
                {
                    "sent": "We can completely remove the dependence and key by using some redundancy.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Use an error correcting output codes.",
                    "label": 1
                },
                {
                    "sent": "So imagine that you have a binary matrix where each column corresponds to your class.",
                    "label": 0
                },
                {
                    "sent": "So you have K columns and then you have some number of rows.",
                    "label": 1
                },
                {
                    "sent": "Each row encodes a binary problem.",
                    "label": 0
                },
                {
                    "sent": "Each row defines a subset.",
                    "label": 1
                },
                {
                    "sent": "So for example, the cells that are white in the given row define the subset corresponding to the throat.",
                    "label": 0
                },
                {
                    "sent": "Right, and you're just trying to predict for every row you're trying to predict whether the correct classes in that subset or not.",
                    "label": 0
                },
                {
                    "sent": "One against all is actually COC was the diagonal matrix where you start to distinguish each class from all the other classes.",
                    "label": 0
                },
                {
                    "sent": "And then when you get in, you ax.",
                    "label": 0
                },
                {
                    "sent": "You believe all the classifiers that you've learned and you decode to the vector that's closest in the Hamming distance.",
                    "label": 1
                },
                {
                    "sent": "Am I going to slowly?",
                    "label": 0
                },
                {
                    "sent": "Yes, OK.",
                    "label": 0
                },
                {
                    "sent": "There is a theorem that comes with the so C for all multiclass problems.",
                    "label": 0
                },
                {
                    "sent": "For all binary classifiers, EOC, the multiclass laws of this reduction is bounded by two M, where M is the number of rows epsilon which is the average binary mistake over D, where D is the minimum distance between between any pair of columns in the matrix and the analysis is very simple, you just observe that you need at least D over to binary mistakes in order to induce the multiclass mistake.",
                    "label": 1
                },
                {
                    "sent": "And that basically implies the theorem.",
                    "label": 0
                },
                {
                    "sent": "There are good codes that have a large minimum distance.",
                    "label": 0
                },
                {
                    "sent": "For example, you can use other more codes.",
                    "label": 0
                },
                {
                    "sent": "There is a very simple recursive construction of other more codes which basically gives you.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "For epsilon, so the multiclass loss is bounded by four times the average binary loss.",
                    "label": 1
                },
                {
                    "sent": "There is no dependence on key.",
                    "label": 1
                },
                {
                    "sent": "There is a fear, a common fear associated with reductions that they create heart problems.",
                    "label": 0
                },
                {
                    "sent": "Where they may create hard problems, which means that the statements.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You get or not very useful.",
                    "label": 0
                },
                {
                    "sent": "Also, the original problem can be noisy, which means that they induce problem will most likely be also noisy, which also means that the statements are not very useful.",
                    "label": 1
                },
                {
                    "sent": "One way of addressing this fear is looking at the regret instead of loss.",
                    "label": 0
                },
                {
                    "sent": "So regret is something that can be defined for any loss function, and it's just the difference between the loss suffered by the classifier on that problem.",
                    "label": 1
                },
                {
                    "sent": "On the problem that you care about minus the smallest possible loss on that problem, right?",
                    "label": 0
                },
                {
                    "sent": "So you subtract off the inherent noise that's in the problem, and you want to bound this avoidable loss.",
                    "label": 1
                },
                {
                    "sent": "That's due to suboptimal prediction.",
                    "label": 1
                },
                {
                    "sent": "And this fear is.",
                    "label": 0
                },
                {
                    "sent": "Maybe people care more about this fear for CSC because the problems are not necessarily natural.",
                    "label": 0
                },
                {
                    "sent": "You want to predict whether given label isn't some random subset of labels.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "K. And there is a problem with one against all and the sushi.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wait, there is also one statement that they wanted to emphasize.",
                    "label": 0
                },
                {
                    "sent": "If we have an optimal classifier for the induced problem, it seems natural to require that the reduction recovers the optimal solution to the original problem.",
                    "label": 1
                },
                {
                    "sent": "Right, so you I reduction creates an induced problem.",
                    "label": 0
                },
                {
                    "sent": "You have a very good algorithm for the induced problem.",
                    "label": 0
                },
                {
                    "sent": "You want to carry this predictive ability.",
                    "label": 0
                },
                {
                    "sent": "You can you want to transform it back to the original problem.",
                    "label": 0
                },
                {
                    "sent": "And there is.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "There is a very simple example that shows that one against all anti cocs are inconsistent in the sense that given an optimal classifier, these reductions do not give you an optimal multiclass classifier.",
                    "label": 1
                },
                {
                    "sent": "The example is basically so this is the simplest possible example where you don't have a majority class, you have three labels and be sorry the conditional probabilities of these three labels.",
                    "label": 0
                },
                {
                    "sent": "So the first label has probability of 1/2 minus some Delta and the other two classes have probability slightly less than 1/4.",
                    "label": 0
                },
                {
                    "sent": "Right, and this is actually the issue with the Matrix as well for.",
                    "label": 0
                },
                {
                    "sent": "For a three class problem.",
                    "label": 0
                },
                {
                    "sent": "So your the optimal binary prediction is the label one or not is no the base optimal production.",
                    "label": 0
                },
                {
                    "sent": "Similarly for Class 2 and for Class 3.",
                    "label": 0
                },
                {
                    "sent": "So the multiclass classifier can.",
                    "label": 0
                },
                {
                    "sent": "Can do no better than just pick random label an it will be sub optimal.",
                    "label": 0
                },
                {
                    "sent": "You can try to fix.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This problem being consistency of one against Ole and ECC by reducing through regression instead of classification.",
                    "label": 0
                },
                {
                    "sent": "So instead of predicting is label I or not, you're trying to predict what's the probability of label I versus all the other classes.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The regret this is a consistent reduction, but unfortunately the regret associated with it is sqrt 2 R and this is bad because R is bounded between zero and one.",
                    "label": 0
                },
                {
                    "sent": "Right, so square root is actually much worse than having a linear dependence on our where.",
                    "label": 0
                },
                {
                    "sent": "Here Are's an average squared loss regret, but also there is this probe introduction from squared loss regression to binary classification that comes with this guarantee that squared error regret is bounded by classification regret.",
                    "label": 1
                },
                {
                    "sent": "So this is also the binary classification are as well.",
                    "label": 0
                },
                {
                    "sent": "By making this reduction consistent by using a soft production by trying to predict the probability of one class versus all the other classes for one against all.",
                    "label": 0
                },
                {
                    "sent": "You're actually making this square dependence on R. On the average binary regret.",
                    "label": 0
                },
                {
                    "sent": "There is a similar effects for ECC.",
                    "label": 0
                },
                {
                    "sent": "There is a probabilistic version office USC.",
                    "label": 1
                },
                {
                    "sent": "Which also has the square root dependence on R, and these two reductions also have pretty bad dependence on this should be key on the number of classes.",
                    "label": 0
                },
                {
                    "sent": "Per example, so they make a prediction.",
                    "label": 0
                },
                {
                    "sent": "Then we can multiclass prediction.",
                    "label": 0
                },
                {
                    "sent": "The complexity of doing that, where to learn on the multiclass example, the complexity of doing that is linear in one against all and quadratic and EOC.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are some questions.",
                    "label": 0
                },
                {
                    "sent": "Is there a consistent reduction that does not have the square dependence?",
                    "label": 1
                },
                {
                    "sent": "Is there a consistent reduction that is sufficient that requires just lock key complexity to train and test on a single multiclass example, which would match an information theoretic lower bound because he needs at least locato write down, the answer can be buff beach shiftable reduction that performs only pairwise comparisons between the classes.",
                    "label": 1
                },
                {
                    "sent": "Because you want more natural problems.",
                    "label": 0
                },
                {
                    "sent": "And I'll show you a family of algorithms that give you these bounds, so you can have a constant.",
                    "label": 0
                },
                {
                    "sent": "Times our dependence.",
                    "label": 0
                },
                {
                    "sent": "In the regret analysis, so the ratio of the multiclass regret to the binary regret is 5.5.",
                    "label": 0
                },
                {
                    "sent": "And you can do that with a lock K computation where again case the number of classes.",
                    "label": 0
                },
                {
                    "sent": "Or if you want to improve the regret dependence and are willing to pay.",
                    "label": 0
                },
                {
                    "sent": "To have an order of computation, you can have these results.",
                    "label": 0
                },
                {
                    "sent": "Immediately.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When you start thinking about an efficient way of reducing multiclass classification to binary classification, you're thinking of a binary tree.",
                    "label": 0
                },
                {
                    "sent": "Here is a very simple reduction that's actually used, and I'll show you that it may not be the best way to reduce multiclass classification to binary classification.",
                    "label": 0
                },
                {
                    "sent": "So you split the set of labels in half and you try to predict the probability.",
                    "label": 0
                },
                {
                    "sent": "What's the probability that the label is in that subset versus the other subset?",
                    "label": 0
                },
                {
                    "sent": "And then you recurse on both sides using some binary tree structure.",
                    "label": 0
                },
                {
                    "sent": "Right here is the example that shows it's a problem with this approach.",
                    "label": 0
                },
                {
                    "sent": "So let's say you have three labels and you have the conditional probabilities in the first line.",
                    "label": 0
                },
                {
                    "sent": "An optimal classifier at the root would predict either.",
                    "label": 0
                },
                {
                    "sent": "You would predict the left subtree because the sum of conditional probabilities is 0.55.",
                    "label": 0
                },
                {
                    "sent": "While actually the best choice is label 3.",
                    "label": 0
                },
                {
                    "sent": "Right, so you want to predict the Max instead of the sum.",
                    "label": 0
                },
                {
                    "sent": "Right, so again, the problems that you want to predict at nodes is not what's the probability that the correct label is in this sub tree versus dotsub.",
                    "label": 0
                },
                {
                    "sent": "Do you actually want to predict where the Max is?",
                    "label": 0
                },
                {
                    "sent": "This is called the theory induction, and it's actually fairly commonly used.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kim will see a better production.",
                    "label": 0
                },
                {
                    "sent": "You have you have a binary tree structure on the set of labels.",
                    "label": 0
                },
                {
                    "sent": "And we will look at the three from the Leafs towards the Earth.",
                    "label": 0
                },
                {
                    "sent": "You pair the labels arbitrarily and you learn the classifier to predict the winner.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The winner between them so.",
                    "label": 0
                },
                {
                    "sent": "This classifier predicts is the label one or two.",
                    "label": 0
                },
                {
                    "sent": "Yes, the little the label three or four and so on.",
                    "label": 0
                },
                {
                    "sent": "The second level of classifiers would be chosen between the winners.",
                    "label": 0
                },
                {
                    "sent": "Conditional winners from this branch.",
                    "label": 0
                },
                {
                    "sent": "And from this branch.",
                    "label": 0
                },
                {
                    "sent": "So each normally predicts the bust of a pair of winners from the previous round.",
                    "label": 1
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then to make a prediction, multiclass prediction and acts once you learn the classifiers at the at the non leaf nodes you just follow a chain of predictions from dirt to some leave and predict that leave.",
                    "label": 0
                },
                {
                    "sent": "So how do you train?",
                    "label": 0
                },
                {
                    "sent": "This is an example that shows the training procedure on ax, 3.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there is a path associated with label 3.",
                    "label": 0
                },
                {
                    "sent": "You train this classifier on acts, left because three is to the left.",
                    "label": 0
                },
                {
                    "sent": "Then you train this classifier on acts, right conditioned on this classifier, getting the label correctly.",
                    "label": 1
                },
                {
                    "sent": "So you use classifiers on the path from the root of the leaf.",
                    "label": 0
                },
                {
                    "sent": "To filter out examples that propagate towards given up.",
                    "label": 0
                },
                {
                    "sent": "And that's essential to form the right prediction problems at the nodes.",
                    "label": 0
                },
                {
                    "sent": "Right, because the notes want to predict if this classifier made a mistake.",
                    "label": 0
                },
                {
                    "sent": "This classifier should have no preference for the label.",
                    "label": 0
                },
                {
                    "sent": "K.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's important to form the right training sets.",
                    "label": 1
                },
                {
                    "sent": "Classifiers from levels closer to the leaves serve as filters for the examples.",
                    "label": 0
                },
                {
                    "sent": "Used to train classifiers closer to the root.",
                    "label": 0
                },
                {
                    "sent": "It's an important observation that you can use batch and online learners with this reduction as well as with any reduction.",
                    "label": 0
                },
                {
                    "sent": "So you can have online algorithms, but none of these notes and you will be just up.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ethan them.",
                    "label": 0
                },
                {
                    "sent": "In an online fashion?",
                    "label": 0
                },
                {
                    "sent": "So it's important to understand that any reduction can essentially be used with online algorithms.",
                    "label": 0
                },
                {
                    "sent": "And we will see that this reduction comes with a guarantee that multiclass to binary.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I regret ratio is bounded by central to the three dots by lock key.",
                    "label": 1
                },
                {
                    "sent": "And then bubble at some robustness and remove this dependence on key completely.",
                    "label": 1
                },
                {
                    "sent": "What year the same reduction can be used to actually solve a more general problem.",
                    "label": 0
                },
                {
                    "sent": "Of course, sensitive multiclass classification you have where you have a distribution D / X cross vectors of length ski where where this vector specifies the cost of each choice.",
                    "label": 1
                },
                {
                    "sent": "So not only your matchmaking multiclass prediction, but each production is associated with its own cost.",
                    "label": 0
                },
                {
                    "sent": "And you can essentially encode any loss function defined on individual examples this way.",
                    "label": 0
                },
                {
                    "sent": "And you want to find the multiclass classifier that minimizes the expected cost with respect to this distribution.",
                    "label": 0
                },
                {
                    "sent": "To the underlying distribution.",
                    "label": 0
                },
                {
                    "sent": "And of course, costs are not available at this time, so it's just you just want to learn the multiclass classifier minimizes the expected.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cost.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So do encode multiclass classification.",
                    "label": 0
                },
                {
                    "sent": "You will have cost factors that have a single zero and all ones there is only one class.",
                    "label": 0
                },
                {
                    "sent": "That's correct and all the other classes have lost one.",
                    "label": 0
                },
                {
                    "sent": "You can encode multi label classification very easily.",
                    "label": 0
                },
                {
                    "sent": "You'll have some subset of zeros and the rest will be one.",
                    "label": 0
                },
                {
                    "sent": "The rest will do.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do we train this filter 3?",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the cost sensitive case.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the non leaf node that chooses between label A and label B right?",
                    "label": 0
                },
                {
                    "sent": "So there are winners from the previous round.",
                    "label": 0
                },
                {
                    "sent": "Let Y be left if the cost of a is less than the cost of B is at most the cost of peace, so a would be the winner.",
                    "label": 1
                },
                {
                    "sent": "That you want this classifier to predict and try it otherwise, and then you train on XY with important weight equal to the difference.",
                    "label": 0
                },
                {
                    "sent": "The absolute difference in costs.",
                    "label": 0
                },
                {
                    "sent": "So we will reduce the importance weighted classification.",
                    "label": 1
                },
                {
                    "sent": "The Bianca talked about where each example has a cost associated with it that tells how important it is to predict this example correctly.",
                    "label": 0
                },
                {
                    "sent": "Right and the importance weight will be the difference in costs for a given node conditions.",
                    "label": 0
                },
                {
                    "sent": "An axe every edge in the tree is associated with some label.",
                    "label": 1
                },
                {
                    "sent": "Given the classifiers at the nodes closer to the leaves.",
                    "label": 0
                },
                {
                    "sent": "Every arch will be associated with some label.",
                    "label": 0
                },
                {
                    "sent": "So this node is predicting between A&B conditioned on Max.",
                    "label": 0
                },
                {
                    "sent": "And you want the importance of that production to be proportional to the difference in costs.",
                    "label": 0
                },
                {
                    "sent": "Again, the induced distribution is well defined.",
                    "label": 0
                },
                {
                    "sent": "By the reduction you can think of this reduction as a transformation from cost sensitive multiclass examples to importance weighted binary examples.",
                    "label": 0
                },
                {
                    "sent": "It's just a machine that takes one distribution distribution of 1 type and transforms it into a distribution of another type that will be fat to an importance weighted algorithm.",
                    "label": 0
                },
                {
                    "sent": "And then we can compose this reduction with cost and to remove the weights to the sample.",
                    "label": 1
                },
                {
                    "sent": "In order to remove the weights.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here's a theorem that comes with this approach.",
                    "label": 0
                },
                {
                    "sent": "For all cost sensitive problems.",
                    "label": 1
                },
                {
                    "sent": "An classifier said the nodes the result in cost sensitive regret is bounded by the average binary regret over the nodes times the expected sum of weights over the nodes.",
                    "label": 1
                },
                {
                    "sent": "Right, so every node was associated with some weight.",
                    "label": 0
                },
                {
                    "sent": "There's some expectation of that weight.",
                    "label": 0
                },
                {
                    "sent": "And you take the sum over the nodes.",
                    "label": 0
                },
                {
                    "sent": "Alternatively, we can bound the cost sensitive regret by K. / 2 times.",
                    "label": 0
                },
                {
                    "sent": "The average binary regret.",
                    "label": 0
                },
                {
                    "sent": "So it's the smaller.",
                    "label": 0
                },
                {
                    "sent": "Of the son or curator.",
                    "label": 0
                },
                {
                    "sent": "This is a computational analysis that follows directly from.",
                    "label": 0
                },
                {
                    "sent": "The reduction and all of them match information theoretic lower bounds.",
                    "label": 0
                },
                {
                    "sent": "You need at least order of K here to just treat the vector of costs.",
                    "label": 0
                },
                {
                    "sent": "Right to train an example.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what does it imply in the multiclass keys in the multiclass keys there is only one weight per level, that is 1.",
                    "label": 1
                },
                {
                    "sent": "All the other weights is 0, so the sum of weights is essentially bounded by the depth.",
                    "label": 1
                },
                {
                    "sent": "Which is where Luke case coming from.",
                    "label": 0
                },
                {
                    "sent": "So the ratio of the cost center of the multiclass regret to binary regardless log key.",
                    "label": 0
                },
                {
                    "sent": "And we don't need to reduce the cost, and in this case because you either an example is filtered where it's propagated with weight one.",
                    "label": 0
                },
                {
                    "sent": "So in the multi class keys there is no need to reduce the constant.",
                    "label": 1
                },
                {
                    "sent": "Can we make it more robust?",
                    "label": 1
                },
                {
                    "sent": "So the 1st.",
                    "label": 0
                },
                {
                    "sent": "Attempt would be to just run multiple.",
                    "label": 1
                },
                {
                    "sent": "I should make this observation that the filter filter tree is essentially a single elimination tournament on the labels.",
                    "label": 1
                },
                {
                    "sent": "You paired the labels.",
                    "label": 0
                },
                {
                    "sent": "You can think of labels as players which playing and then once a player loses its out of the tournament.",
                    "label": 0
                },
                {
                    "sent": "Right, so you can think about running multiple single elimination tournaments and then choosing the majority.",
                    "label": 0
                },
                {
                    "sent": "For example, the majority plurality winner.",
                    "label": 0
                },
                {
                    "sent": "But this is of no use because you're basically given the adversary more chances to make a mistake.",
                    "label": 0
                },
                {
                    "sent": "You're you're increasing the dots.",
                    "label": 1
                },
                {
                    "sent": "And we saw that the bound the regret ratio depends on the dots, so this is not very useful.",
                    "label": 0
                },
                {
                    "sent": "But it turns out that you can help.",
                    "label": 0
                },
                {
                    "sent": "You can have interdependant tournaments in lock keyed apps.",
                    "label": 0
                },
                {
                    "sent": "You can have lock key tournaments in lucky ducks instead of log squared.",
                    "label": 0
                },
                {
                    "sent": "Squared K dubs.",
                    "label": 0
                },
                {
                    "sent": "So instead of running lock key, single elimination tournaments independently and having lock squared doubts, you can have interdependant tournaments and I'll show you how.",
                    "label": 0
                },
                {
                    "sent": "In lock key drops.",
                    "label": 0
                },
                {
                    "sent": "Which would allow us to go from look key in the regret bound to a constant.",
                    "label": 0
                },
                {
                    "sent": "So no player.",
                    "label": 1
                },
                {
                    "sent": "Will be playing twice in the same round and still you can.",
                    "label": 0
                },
                {
                    "sent": "You can have interdependant tournaments.",
                    "label": 0
                },
                {
                    "sent": "This way since each tournament.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is shown in its own color.",
                    "label": 0
                },
                {
                    "sent": "So once a player loses in the first tournament, it goes in place in the SEC Tournament.",
                    "label": 0
                },
                {
                    "sent": "Once it loses, it goes into third tournament.",
                    "label": 0
                },
                {
                    "sent": "In the next round, and you pair the players aggressively, so if you can pair players that can play in the same tournament, you do so.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that you can have lock key, single elimination tournaments in lock in order flow key.",
                    "label": 0
                },
                {
                    "sent": "Rounds then you will have lock key winners, which will play in the weighted single elimination tournament.",
                    "label": 1
                },
                {
                    "sent": "To determine the final winner.",
                    "label": 0
                },
                {
                    "sent": "Which will also add a low key to the Dubs found.",
                    "label": 0
                },
                {
                    "sent": "But the game going what allows us to reduce the regret ratio from lucky to constant.",
                    "label": 0
                },
                {
                    "sent": "Yes, having lock the absence that the flock squared apps.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So E yes, the amount of redundancy.",
                    "label": 0
                },
                {
                    "sent": "So that's the number of times you can lose before you're out of the tournament.",
                    "label": 0
                },
                {
                    "sent": "That's a very simple computational bound that comes from the construction, and that's a guarantee that we get.",
                    "label": 0
                },
                {
                    "sent": "And this plot simply built that function.",
                    "label": 0
                },
                {
                    "sent": "So for a fixed key.",
                    "label": 0
                },
                {
                    "sent": "I think he was.",
                    "label": 0
                },
                {
                    "sent": "1001 thousand 24 in this example.",
                    "label": 0
                },
                {
                    "sent": "And this is the level of robustness, so this is the elimination.",
                    "label": 0
                },
                {
                    "sent": "Factor, and this is how the regret bound decreases as you add more robustness.",
                    "label": 0
                },
                {
                    "sent": "So this is the regret bound.",
                    "label": 0
                },
                {
                    "sent": "This is the ratio of the multiclass regret to the binary like that, and that's how it drops as you add more tournaments, more robustness.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, and that's a particular value of of E, so 1 E is for Locky.",
                    "label": 0
                },
                {
                    "sent": "The regret ratio is 5.5.",
                    "label": 1
                },
                {
                    "sent": "It was a lock key computation, for example, both training and test time.",
                    "label": 1
                },
                {
                    "sent": "And also note that the binary problems that they have at the nodes are comparing only pairs of examples.",
                    "label": 0
                },
                {
                    "sent": "Oh, I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "Pairs of labels.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I won't mention ranking there.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Briefly and then, John will continue.",
                    "label": 0
                },
                {
                    "sent": "The tutorial.",
                    "label": 0
                },
                {
                    "sent": "This is an example of loss functions where the loss function is defined on subsets of examples rather than on individual examples.",
                    "label": 0
                },
                {
                    "sent": "It has some universe U of objects and you have some subset.",
                    "label": 0
                },
                {
                    "sent": "You are given some subset of of these objects coming from some distribution over subsets and you want, let's say for simplicity that these objects have binary labels.",
                    "label": 0
                },
                {
                    "sent": "There's also some conditional distribution over the labels and you want to sort.",
                    "label": 1
                },
                {
                    "sent": "The objects in the subset so that when the label said revealed you have a sequence of zeros followed by sequence of ones.",
                    "label": 0
                },
                {
                    "sent": "Right, so you want the objects to be sorted.",
                    "label": 0
                },
                {
                    "sent": "I commonly used loss function.",
                    "label": 0
                },
                {
                    "sent": "This AUC loss, which is it's just a normalized bubble sort distance to the sort that order normalized by the number of zeros times the number of ones, which is the number of pairs that you can have miss sorted.",
                    "label": 0
                },
                {
                    "sent": "Right there are two general approaches to the ranking problem.",
                    "label": 1
                },
                {
                    "sent": "You can learn a scoring function on the entire universe of objects that Maps a given object to a real value, and then given a subset you validate all the elements in that subset using a fan rank according to the values of F. And they'll turn that if approaches to learn the classifier to predict given two objects, whether one should be ordered before the 2nd and then sort.",
                    "label": 1
                },
                {
                    "sent": "So notice that this classifier may not induce a linear order, and on the set.",
                    "label": 0
                },
                {
                    "sent": "So you need some algorithm that resolves the cycles that you may have in age.",
                    "label": 1
                },
                {
                    "sent": "And the motivation is that a good total linear ordering may be impossible to achieve, so this function may be difficult to learn well.",
                    "label": 0
                },
                {
                    "sent": "There may not be a good.",
                    "label": 1
                },
                {
                    "sent": "Total ordering function.",
                    "label": 0
                },
                {
                    "sent": "While such an age can be learned well.",
                    "label": 0
                },
                {
                    "sent": "So that's the motivation for the reduction approach.",
                    "label": 0
                },
                {
                    "sent": "Even though the ranking problem may seem very different, there is very tight redux.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And from thereon can problem the classification and that's actually generalized to a class of functions.",
                    "label": 0
                },
                {
                    "sent": "Other than you see loss.",
                    "label": 0
                },
                {
                    "sent": "It's a very general class of ranking functions, and you can read about it and I alone and mccree's paper.",
                    "label": 0
                },
                {
                    "sent": "Which is supposed to be on the website.",
                    "label": 0
                },
                {
                    "sent": "I'm done.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}