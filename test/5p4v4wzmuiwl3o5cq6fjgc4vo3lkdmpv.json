{
    "id": "5p4v4wzmuiwl3o5cq6fjgc4vo3lkdmpv",
    "title": "A Chaining Algorithm for Online Nonparametric Regression",
    "info": {
        "author": [
            "S\u00e9bastien Gerchinovitz, Department of Mathematics and their applications, \u00c9cole normale sup\u00e9rieure Paris"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_gerchinovitz_nonparametric_regression/",
    "segmentation": [
        [
            "So this is joint work with Jager with defending his PhD tomorrow afternoon at I'll say University.",
            "So."
        ],
        [
            "The work is about on line regression with individual sequences, so very close heading to the one of the previous talk.",
            "So the game protocol is the following.",
            "At every round the environment reveals the input XT, so this is not a fixed design setting.",
            "This is the minor difference with the previous talk.",
            "When the forecasted chooses a prediction white hat, which is a real number, and then the environment chooses and reveals the true observational YT, which is compared to the focus is prediction through the square loss.",
            "And the goal is to minimize the regrets which is the difference between the cumulative loss of the forecaster and that of the best function F in some large non parametric functions.",
            "Set F from X2R.",
            "So I'll give examples in the next slide.",
            "And we are concerned with guarantees that hold a uniformly of all individual sequences with bounded observations.",
            "OK."
        ],
        [
            "Our first contribution in this setting was to design an explicit algorithm with a Dudley type regret bound.",
            "So why did it?",
            "I triggered man?",
            "We mean a regret bound here of the function set F. That's a, that's a.",
            "That's where you can see that there is a delay entropy integral.",
            "Which can then be upper bounded through.",
            "Standard the approximation guarantees to to get regret bounds in various settings, so here at Infinity of F, epsilon is a minimal number of balls of radius epsilon to cover the set F in the soup, nor so this is a lot of that is the metric entropy at scale epsilon, and it is slightly weaker than the stronger notion of a sequential entropy.",
            "But we was used, for example the earlier paper, but lacking in Sweden.",
            "But you know non constructive fashion so we design an algorithm that as this type of mound and then just upper bounding the metric entropy in various settings enables still to get optimal requirements for Lipschitz functions, pizza Holder functions or the set of sparse convex combinations for example."
        ],
        [
            "OK, so I'll give you a brief idea of the the main tools that we used to construct our algorithm.",
            "So we use the chaining technique, which means that we approximate any function F with a series sequence of finer and finer approximations, like by zero of F in a set of 0.1 of FF1, and so on.",
            "Where for any key by KF is closed with up to a distance of gamma over 2 to the key and FK is a minimal gamma took to the Kia covering.",
            "And this enabled us to.",
            "Two, so we rewrote the objective of the learner, which is this information that appears in the regrets.",
            "By rewriting F as the sum of the coarsest approximation \u03c0 zero of F of F. Plus some of incriments which led us to perform two aggregation tasks simultaneously.",
            "We use the standard exponentially weighted average forecaster at a high scale level to mimic the best function PI0F.",
            "So in the course is set at 0.",
            "And in simultaneously we developed a new multi scale version of the exponentiated gradient algorithm to be competitive against all the increments by key of F minus by K -- 1 of, but simultaneously for every key.",
            "And so this was a key to get the delay entropy integral that appears in the regret now.",
            "So this the fact of using two scale aggregation actually already appeared in previous work by Chazelle, Bianchi, and Lugosi.",
            "For different losses like the absolute on the Douglas Beautiful Square loss, the multiscale linearisation was key to.",
            "To get our regret bounds."
        ],
        [
            "OK, so let's finish with the second contribution, which was how to make this apparently inefficient algorithm practical, at least in a particular case.",
            "So we looked at the keys of the Holder functions where without proper care you need to update exponentially many weights at every round.",
            "But this is so if you use optimal coverings, and if instead you use a slightly suboptimal coverings, then you can do this efficiently by approximating any Lipschitz functions with piecewise constant functions or Holder functions with piecewise polynomial functions to get the Chris is set at zero and then refining it through dyadic discretisation, then just using the same form of algorithm, you can get almost optimal regret bounds, you just lose luck factors.",
            "But with a polynomial time and space complexities, thank you."
        ],
        [
            "See you at the cluster."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is joint work with Jager with defending his PhD tomorrow afternoon at I'll say University.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The work is about on line regression with individual sequences, so very close heading to the one of the previous talk.",
                    "label": 1
                },
                {
                    "sent": "So the game protocol is the following.",
                    "label": 0
                },
                {
                    "sent": "At every round the environment reveals the input XT, so this is not a fixed design setting.",
                    "label": 1
                },
                {
                    "sent": "This is the minor difference with the previous talk.",
                    "label": 1
                },
                {
                    "sent": "When the forecasted chooses a prediction white hat, which is a real number, and then the environment chooses and reveals the true observational YT, which is compared to the focus is prediction through the square loss.",
                    "label": 1
                },
                {
                    "sent": "And the goal is to minimize the regrets which is the difference between the cumulative loss of the forecaster and that of the best function F in some large non parametric functions.",
                    "label": 1
                },
                {
                    "sent": "Set F from X2R.",
                    "label": 0
                },
                {
                    "sent": "So I'll give examples in the next slide.",
                    "label": 0
                },
                {
                    "sent": "And we are concerned with guarantees that hold a uniformly of all individual sequences with bounded observations.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our first contribution in this setting was to design an explicit algorithm with a Dudley type regret bound.",
                    "label": 1
                },
                {
                    "sent": "So why did it?",
                    "label": 0
                },
                {
                    "sent": "I triggered man?",
                    "label": 0
                },
                {
                    "sent": "We mean a regret bound here of the function set F. That's a, that's a.",
                    "label": 0
                },
                {
                    "sent": "That's where you can see that there is a delay entropy integral.",
                    "label": 0
                },
                {
                    "sent": "Which can then be upper bounded through.",
                    "label": 1
                },
                {
                    "sent": "Standard the approximation guarantees to to get regret bounds in various settings, so here at Infinity of F, epsilon is a minimal number of balls of radius epsilon to cover the set F in the soup, nor so this is a lot of that is the metric entropy at scale epsilon, and it is slightly weaker than the stronger notion of a sequential entropy.",
                    "label": 0
                },
                {
                    "sent": "But we was used, for example the earlier paper, but lacking in Sweden.",
                    "label": 0
                },
                {
                    "sent": "But you know non constructive fashion so we design an algorithm that as this type of mound and then just upper bounding the metric entropy in various settings enables still to get optimal requirements for Lipschitz functions, pizza Holder functions or the set of sparse convex combinations for example.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'll give you a brief idea of the the main tools that we used to construct our algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we use the chaining technique, which means that we approximate any function F with a series sequence of finer and finer approximations, like by zero of F in a set of 0.1 of FF1, and so on.",
                    "label": 1
                },
                {
                    "sent": "Where for any key by KF is closed with up to a distance of gamma over 2 to the key and FK is a minimal gamma took to the Kia covering.",
                    "label": 0
                },
                {
                    "sent": "And this enabled us to.",
                    "label": 0
                },
                {
                    "sent": "Two, so we rewrote the objective of the learner, which is this information that appears in the regrets.",
                    "label": 0
                },
                {
                    "sent": "By rewriting F as the sum of the coarsest approximation \u03c0 zero of F of F. Plus some of incriments which led us to perform two aggregation tasks simultaneously.",
                    "label": 0
                },
                {
                    "sent": "We use the standard exponentially weighted average forecaster at a high scale level to mimic the best function PI0F.",
                    "label": 1
                },
                {
                    "sent": "So in the course is set at 0.",
                    "label": 0
                },
                {
                    "sent": "And in simultaneously we developed a new multi scale version of the exponentiated gradient algorithm to be competitive against all the increments by key of F minus by K -- 1 of, but simultaneously for every key.",
                    "label": 0
                },
                {
                    "sent": "And so this was a key to get the delay entropy integral that appears in the regret now.",
                    "label": 0
                },
                {
                    "sent": "So this the fact of using two scale aggregation actually already appeared in previous work by Chazelle, Bianchi, and Lugosi.",
                    "label": 0
                },
                {
                    "sent": "For different losses like the absolute on the Douglas Beautiful Square loss, the multiscale linearisation was key to.",
                    "label": 0
                },
                {
                    "sent": "To get our regret bounds.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's finish with the second contribution, which was how to make this apparently inefficient algorithm practical, at least in a particular case.",
                    "label": 0
                },
                {
                    "sent": "So we looked at the keys of the Holder functions where without proper care you need to update exponentially many weights at every round.",
                    "label": 1
                },
                {
                    "sent": "But this is so if you use optimal coverings, and if instead you use a slightly suboptimal coverings, then you can do this efficiently by approximating any Lipschitz functions with piecewise constant functions or Holder functions with piecewise polynomial functions to get the Chris is set at zero and then refining it through dyadic discretisation, then just using the same form of algorithm, you can get almost optimal regret bounds, you just lose luck factors.",
                    "label": 0
                },
                {
                    "sent": "But with a polynomial time and space complexities, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See you at the cluster.",
                    "label": 0
                }
            ]
        }
    }
}