{
    "id": "2e7hdruw3sdhbfphc4ohhqp5vcnyznqq",
    "title": "A Scalable Pattern Mining Approach to Web Graph Compression with Communities",
    "info": {
        "author": [
            "Greg Buehrer, Microsoft Research"
        ],
        "published": "Feb. 25, 2008",
        "recorded": "February 2008",
        "category": [
            "Top->Computer Science->Web Mining"
        ]
    },
    "url": "http://videolectures.net/wsdm08_buehrer_spma/",
    "segmentation": [
        [
            "OK, so essentially what we're talking about is compressing the web graph."
        ],
        [
            "Let's start with a quick motivation here.",
            "We have the Internet, the web.",
            "And we have questions we'd like to ask about the structure of the web, such as who links to me, or how many apps is it for me to Kevin Bacon's homepage?",
            "Or what's the growth or impact of a particular social network?",
            "Another question is, are these pages that mind my crawlers crawling?",
            "Are they part of a link farm?",
            "So to answer these questions, we build what's called a link server, and essentially it's a system that lifts the web as a graph into RAM or streaming on disk, where every page in the web is.",
            "The node in this graph, and every hyperlink on a page is a directed edge."
        ],
        [
            "And there are a lot of challenges to building a system like this.",
            "The only one I'll address here today is compressing the graph, and in particular there are several features of this graph or components of this graph.",
            "There are the in links, typically in adjacency list formed out links the URLs and then potentially metadata and the only thing I'm compressing here is the in links in the out links.",
            "So there is existing work on this topic.",
            "What was found was that if we sort the URLs by their lexicographically by their domain and then their page, we can get the nodes that link to each other to be near each other in this order.",
            "And this has a clear benefit.",
            "One is what's called reference coding, and I have an example of.",
            "Two nodes here, and let's assume they're in the same domain and we see their adjacency lists.",
            "And what we can do is instead of encoding these lists directly.",
            "We can keep a reference to a previous node, so for example, no two is similar to node one, so we just reference back to node one, but it's a little different.",
            "It has a link to node 5 and it doesn't have a link to node two, so we'd say plus 5 -- 2.",
            "And this is called reference coding, and it's quite effective.",
            "Another form of encoding the graph is called gap coding.",
            "Numbers four out links.",
            "We sort them in increasing order and we code the Delta.",
            "So for example, instead of two 1012 fourteen 18 we'd code 82224.",
            "And then by doing this we have a lot of repeating numbers.",
            "12345 small numbers since what we said was near numbers linked to each other.",
            "And then we can use Huffman coding to code these with just a couple of bits.",
            "And this is again called gap coding.",
            "Another technology is called data codes, and essentially what these are.",
            "Is there a flat code that tends to mimic Huffman coding based on a power law distribution or the slope of the power law distribution and the benefit is you don't have a Huffman table.",
            "Look up and you don't have this computation time."
        ],
        [
            "Our approach is a little different.",
            "What we do is we look for dense bipartite graph.",
            "For these directed graphs.",
            "And here I'll draw an example.",
            "We have four source nodes at the top and five destination nodes at the bottom, and all the source nodes point to all the destination nodes.",
            "So what we do is we collapse this into what we call a virtual."
        ],
        [
            "Load where all the source node just point to one new node that we're going to add to the graph, and then this new node points to these destination nodes so it's one level of indirection and we get the compressed all the links into this virtual node so that we can represent it for multiple uses for every source.",
            "It's pretty straight 4."
        ],
        [
            "But the challenge is, how do we find these?",
            "We know the graph is very large and we can't walk the graph to find enough of these bipartite cliques so that we can get good compression.",
            "So what we do is we cast this adjacency list as a transactional data set.",
            "So for data mining people this is a series of rows, just like an adjacency list is where we have sets of labels in each row.",
            "Then what we'll do is we'll use frequent itemset mining to try to discover these bipartite cliques or directed by partic leagues.",
            "So for example, when you normally frequent itemset mining for Association rules or something like this, what you try to find his Co occurrences of instances or Co occurrences of labels, for example milk and bread Co occur three times.",
            "So we'd say milk and bread is a frequent itemset based on some minimum minimum support threshold.",
            "We could say two or three here, and it shows up three times.",
            "Well, we have exactly the same thing in the adjacency list.",
            "So for example, in the out linked list or we could say that in linked lists too, it works as effective.",
            "1213 and 14 occur in all four out links, so we can mine for this frequent itemset and then we can represent this as a virtual node.",
            "Then nodes 123 and four will point to this new virtual node and the virtual node will point to 1213 and 14.",
            "The problem is, itemset mining is an exponential computation with respect to the label input and decreasing support.",
            "The support is the minimum number of transactions or rows that a subset has to exist in for us to consider it relevant.",
            "And in this case we have a lot of labels, typical data mining algorithms.",
            "We have, things you'll see on the Internet over hundreds or thousands of labels, sometimes 10s of thousands of labels here in the web graph we have 20 billion labels because any node can appear in any out link list.",
            "And we also want to mine at very low support a support of two even a supporter.",
            "Two will give us a significant compression if the list is long."
        ],
        [
            "So what we did was we developed this approximate itemset mining algorithm and then we embedded it into our compression mechanism.",
            "And it basically follows these four steps.",
            "The first thing we do is we cluster groups of nodes based on their similarity were similarity is essentially the intersection of their out linked lists and then given these clusters we take each cluster, hopefully small.",
            "But if it's not a small cluster then there are highly similar and we look for patterns.",
            "We use data mining techniques to find patterns in this small group would create a virtual note for that small group and we substitute that virtual node into the graph and then what's nice is we can iterate.",
            "And for each iteration, each node is touched one time, so we don't have to worry about compressing the same nodes over and over or looking at the same nodes over and over in this mining process, our clustering process will group each node into one bucket at a time."
        ],
        [
            "So the clustering step will use min hashing.",
            "K way min hashing and essentially what we do is we take a min hash for each row or each adjacency list and and we develop a sample for that.",
            "So we take them in element in some permutation space and we do that for every row and then we do that K times to make this N by K matrix."
        ],
        [
            "This is an example.",
            "And then we sort this matrix lexicographically.",
            "Again, we have one row for each node in the graph, and we have K columns.",
            "And this sort biases this sampling process left to right, but it's kind of the price we pay to maintain a low log linear complexity."
        ],
        [
            "Now once we have it sorted, we start reversing the columns an each set each range in a column that has the same value.",
            "We know that this is represents a cluster.",
            "Or at least at least the cluster.",
            "And we also know that each row in that range row ID has at least one element in common with every other element in that range.",
            "Every other node in that range, and then we walk across to the matrix.",
            "Sub dividing in a telescoping fashion.",
            "Finding other ranges sub ranges that have the same value so you can see in the first column.",
            "They all have hash a so we know that these first four rows all have the same.",
            "At least one value in common and we know that the 1st three at least have two and we know that the first 2 at least have three and we just go across this in recursive fashion until the size of the number of rows is small.",
            "Then we pass that to the mining stage.",
            "If we get all the way to K. And it's a still very large set.",
            "It doesn't really worry us because mining a large set when we have a large sample where they're all the same values is relatively easy because the overlap is high."
        ],
        [
            "Now the mining step.",
            "Basically we look at all the nodes and here's an example.",
            "Let's assume that this was passed to the minor and these are the out links.",
            "Once we go to the mining stage, we drop all the hashes.",
            "We just used the hashes to group.",
            "So the first thing we do is we build a histogram.",
            "And the purpose is that we're going to build a prefix tree, and we want to maximize the overlap in this prefix tree.",
            "So if we sort these out link lists based on decreasing frequency, then it will maximize our overlap in this prefix tree.",
            "So for example.",
            "Label one or node ID one appeared in all eight rows or all eight.",
            "Node out link lists."
        ],
        [
            "Anything that appeared just one time we don't really need it because they can't really help us with compression, so we just delete those from the out link list.",
            "We don't actually delete him, we just don't add him to the prefix tree.",
            "Then we sort these in decreasing frequency order."
        ],
        [
            "Now we Add all these to a prefix tree.",
            "Sometimes it's called try.",
            "So here's adding the first out link set to a try.",
            "And the first number is is the pattern 12356 ten 12:15 at it you can see that stout link set for the first node and then we keep a list of all the node IDs to have this pattern.",
            "So now we've added the second node ID 102 and you can see 102 has 123, and since it overlapped then we didn't have to add any nodes to our prefix tree.",
            "This is similar to FP growth.",
            "It's a single order projection.",
            "Then here we Add all the node ID list to the prefix tree."
        ],
        [
            "Now, once we have this prefix tree, we want to find the longest patterns we can find in this because they have the potential to give us the best compression.",
            "So what we do is we walked to a leaf.",
            "And we add that leaf to a table where this table is storing the position in the prefix tree, where we can generate this pattern and in this table it's really pretty much just a pointer, but I'm just showing the length of the pattern, the elements in the list and the value of the compression.",
            "We know we have to add a pointer for every time we add a virtual notes we subtract 1 for that in the length and the frequency we have to subtract 1 from the frequency because we still have to represent this so that we keep the same graph.",
            "Then we walk up from the leaf to the root.",
            "Anytime we see a list that's longer, we add it to the potential virtual node list table.",
            "The reason being that this could offer us better compression, but we know that.",
            "Say for example, this node 31 can give us better compression than 36, so we don't add that to the table.",
            "So here we are, adding the next one.",
            "We don't add the top node because it has a pattern length of one which can't give us compression.",
            "And then we go to another leaf and we do the same thing.",
            "We color the nodes as we walk along so we don't have to traverse a node more than once.",
            "And this last branch we don't traverse it because it only has.",
            "One element."
        ],
        [
            "So now we sort this based on decreasing ability to compress the graph.",
            "And the thing to point out here is that these patterns could interact with each other such that some patterns won't be able to give us any compression.",
            "So we start with the best compression possible and you can think of this as we have said intersections and once we remove one virtual node that eliminates the ability to remove another virtual node."
        ],
        [
            "So here's what it looks like after we add a virtual nodes.",
            "And remember we have to actually maintain these edges to keep the graph.",
            "And we didn't use virtual node 4 because it no longer existed in our graph as we were altering it.",
            "It's kind of quick, but."
        ],
        [
            "If you want more details, you can look at the paper.",
            "It's spelled out a little better.",
            "So let's just look at a quick empirical evaluation.",
            "We want to evaluate it along its ability to compress how fast it is, or its ability to scale paralyzed and.",
            "The number of patterns we discovered.",
            "We don't want to add 10X more virtual nodes than exists in the graph, or it would.",
            "Hurt us as much as help us.",
            "And these datasets were available from the web graph Framework folks laboratory for Web algorithmics.",
            "Really nice data sets, actually."
        ],
        [
            "So here's a compression afforded by the virtual nodes.",
            "Just by reducing by adding these virtual nodes and reducing the number of edges in the graph.",
            "And you can see in the general case where in the five to 7X compression ratio, which is pretty high, this one graph down here web based.",
            "It has an average out degree of only eight, so there isn't much I can find in this graph."
        ],
        [
            "Once we remove once we have these virtual nodes.",
            "There's no reason why we can't use gap coding on top of it, so this is after we remove the virtual nodes and then add on gap coding and this could be Huffman coding or data coding or any coding scheme you prefer.",
            "And now what?",
            "You see, we're in the 10 to 15 range.",
            "In the in the common case."
        ],
        [
            "And this compares us with the web graph frameworks.",
            "Bits per edge.",
            "And you can see where compareable to them and this is the number of virtual nodes we add.",
            "And on average it's about 20% more nodes in the graph and these numbers take into consideration the offset for the additional nodes."
        ],
        [
            "Scalability is pretty good.",
            "What we did was we took the UK 2006 data set, which was the largest publicly available one and we just took it in 250 million chunk edges.",
            "Edge chunks and measured mining time.",
            "Now as I said as you saw from the previous graph."
        ],
        [
            "We really only need 2.",
            "Compressed to three or four passes, and then we're basically done.",
            "We don't find anymore path."
        ],
        [
            "So if you look at this graph where.",
            "Along for passes, it's pretty reasonable."
        ],
        [
            "And the one property we want to minimize the number of dereferences we need to make with the graph.",
            "So the blue graph here you can see this is the average number of dereferences.",
            "Additional dereferences you have to make in addition to getting the first list or the number of virtual nodes.",
            "And after 4 passes you around 1.2 and it never really goes above 1.45 or so dereferences, which is a property we would like to keep."
        ],
        [
            "So.",
            "All these graphs are actually ordered by URL already and we don't leverage it because we hash and sort.",
            "But one thing it does let us do is evaluate how far apart our communities are.",
            "And what we do is we take the log of the standard deviation of the out links in the virtual node, so things that have a small standard deviation the virtual node are near each other and this total order sorted by URL and things that are far apart.",
            "Things that have a high standard deviation are far apart and what we want to do is we do find patterns that you wouldn't find with the typical reference window where you're only looking back a small number of nodes.",
            "You can see that there's a significant even at the standard deviation of 10 to the four.",
            "We still find 7 or 8% of our compression or virtual notes comes from those."
        ],
        [
            "The data mining folks are, and I'm not sure if this crowd is overly interested.",
            "But then many folks like to inspect how this evaluates against traditional itemset mining.",
            "And so I just have some charts.",
            "Or if you want to come up to me after the talk and talk about it.",
            "Basically we find patterns that are very long that traditional items that money doesn't find and that were the pink one and you can see we find patterns that are length over 1000 in traditional items of mine tends not to find these patterns and we have a constant runtime.",
            "Regardless of support, because we don't do anything with support and you can see most itemset mining algorithms will scale exponentially with support, which is the red line.",
            "It paralyzes pretty well.",
            "When we go from.",
            "One core to 8 cores on a multicore.",
            "We go from 110 seconds to 1.5 seconds or so and it's near linear speedup.",
            "Greater than 7.5.",
            "And these patterns also give us improved compression."
        ],
        [
            "So take home messages basically.",
            "We have a web graph compression scheme that's compareable compression technology or compression afforded with the state of the art.",
            "It doesn't necessarily require URL ordering or particular labeling for the nodes in the graph.",
            "It provides seeds for Community discovery that can be far apart in this URL ordering, meaning that they are in separate domains.",
            "Has a high compression ratio.",
            "Scales pretty well and it can be extended.",
            "You could use a different mining algorithm if you wanted to on the back end, or you could use a different clustering scheme if you wanted to.",
            "It also has.",
            "Bounded complexity log linear and the number of edges in the graph.",
            "And it's an interesting use case for pattern mining."
        ],
        [
            "For ongoing work.",
            "We're working on computations on the compressed graph where things like page rank, where the computation is a function of the number of edges in the graph.",
            "We're trying to see if we can get commensurate speedup.",
            "The meaning we don't have to look as many edges because we don't have as many edges, and to see if well if it will converge in the same number of iterations.",
            "And we're also looking at how hard or easy it is to update the graph, which typically with when you're compressing something to the last bit, it's never easy to update.",
            "And we want to run this on the full graph."
        ],
        [
            "And also like to thank the external references too.",
            "Any questions?",
            "Questions.",
            "In a scenario like World Wide Web where it follows long tail distribution power law so only the short head web pages, they will have a lot of out links.",
            "In case of the long tail web pages where you have very a lot of web pages but they have very few out links, then how well this approach will work.",
            "My thing is, it'll work pretty well and it's not actually pages that have a large number out links.",
            "Probably honest comments pages that have a large number of in links, but the argument is valid that some pages will have a large number in links compared to some pages will have a small number in links or out links, but actually the because this minimize permutations approximated moved to the Jaccard coefficient discard coefficient isn't the best for us because.",
            "It tends to be pretty low for something that has a really large set with something that has a small set.",
            "The Jakarta coefficient, it really all we care about is intersection, but our strategy basically uses Chicago vision, But besides that most pages have a left navigation, have a top navigation.",
            "These types of things where the links are the same for any given domain, regardless if there are a large number of links out links on it, and these tend to be the things that we capture.",
            "In, in the general case.",
            "Also, mirror domains are easy to capture this way too.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so essentially what we're talking about is compressing the web graph.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's start with a quick motivation here.",
                    "label": 0
                },
                {
                    "sent": "We have the Internet, the web.",
                    "label": 0
                },
                {
                    "sent": "And we have questions we'd like to ask about the structure of the web, such as who links to me, or how many apps is it for me to Kevin Bacon's homepage?",
                    "label": 1
                },
                {
                    "sent": "Or what's the growth or impact of a particular social network?",
                    "label": 0
                },
                {
                    "sent": "Another question is, are these pages that mind my crawlers crawling?",
                    "label": 1
                },
                {
                    "sent": "Are they part of a link farm?",
                    "label": 0
                },
                {
                    "sent": "So to answer these questions, we build what's called a link server, and essentially it's a system that lifts the web as a graph into RAM or streaming on disk, where every page in the web is.",
                    "label": 0
                },
                {
                    "sent": "The node in this graph, and every hyperlink on a page is a directed edge.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there are a lot of challenges to building a system like this.",
                    "label": 0
                },
                {
                    "sent": "The only one I'll address here today is compressing the graph, and in particular there are several features of this graph or components of this graph.",
                    "label": 0
                },
                {
                    "sent": "There are the in links, typically in adjacency list formed out links the URLs and then potentially metadata and the only thing I'm compressing here is the in links in the out links.",
                    "label": 0
                },
                {
                    "sent": "So there is existing work on this topic.",
                    "label": 0
                },
                {
                    "sent": "What was found was that if we sort the URLs by their lexicographically by their domain and then their page, we can get the nodes that link to each other to be near each other in this order.",
                    "label": 0
                },
                {
                    "sent": "And this has a clear benefit.",
                    "label": 0
                },
                {
                    "sent": "One is what's called reference coding, and I have an example of.",
                    "label": 0
                },
                {
                    "sent": "Two nodes here, and let's assume they're in the same domain and we see their adjacency lists.",
                    "label": 0
                },
                {
                    "sent": "And what we can do is instead of encoding these lists directly.",
                    "label": 0
                },
                {
                    "sent": "We can keep a reference to a previous node, so for example, no two is similar to node one, so we just reference back to node one, but it's a little different.",
                    "label": 1
                },
                {
                    "sent": "It has a link to node 5 and it doesn't have a link to node two, so we'd say plus 5 -- 2.",
                    "label": 0
                },
                {
                    "sent": "And this is called reference coding, and it's quite effective.",
                    "label": 1
                },
                {
                    "sent": "Another form of encoding the graph is called gap coding.",
                    "label": 0
                },
                {
                    "sent": "Numbers four out links.",
                    "label": 0
                },
                {
                    "sent": "We sort them in increasing order and we code the Delta.",
                    "label": 0
                },
                {
                    "sent": "So for example, instead of two 1012 fourteen 18 we'd code 82224.",
                    "label": 0
                },
                {
                    "sent": "And then by doing this we have a lot of repeating numbers.",
                    "label": 1
                },
                {
                    "sent": "12345 small numbers since what we said was near numbers linked to each other.",
                    "label": 0
                },
                {
                    "sent": "And then we can use Huffman coding to code these with just a couple of bits.",
                    "label": 0
                },
                {
                    "sent": "And this is again called gap coding.",
                    "label": 0
                },
                {
                    "sent": "Another technology is called data codes, and essentially what these are.",
                    "label": 0
                },
                {
                    "sent": "Is there a flat code that tends to mimic Huffman coding based on a power law distribution or the slope of the power law distribution and the benefit is you don't have a Huffman table.",
                    "label": 1
                },
                {
                    "sent": "Look up and you don't have this computation time.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our approach is a little different.",
                    "label": 1
                },
                {
                    "sent": "What we do is we look for dense bipartite graph.",
                    "label": 1
                },
                {
                    "sent": "For these directed graphs.",
                    "label": 0
                },
                {
                    "sent": "And here I'll draw an example.",
                    "label": 0
                },
                {
                    "sent": "We have four source nodes at the top and five destination nodes at the bottom, and all the source nodes point to all the destination nodes.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we collapse this into what we call a virtual.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Load where all the source node just point to one new node that we're going to add to the graph, and then this new node points to these destination nodes so it's one level of indirection and we get the compressed all the links into this virtual node so that we can represent it for multiple uses for every source.",
                    "label": 0
                },
                {
                    "sent": "It's pretty straight 4.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the challenge is, how do we find these?",
                    "label": 0
                },
                {
                    "sent": "We know the graph is very large and we can't walk the graph to find enough of these bipartite cliques so that we can get good compression.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we cast this adjacency list as a transactional data set.",
                    "label": 1
                },
                {
                    "sent": "So for data mining people this is a series of rows, just like an adjacency list is where we have sets of labels in each row.",
                    "label": 0
                },
                {
                    "sent": "Then what we'll do is we'll use frequent itemset mining to try to discover these bipartite cliques or directed by partic leagues.",
                    "label": 0
                },
                {
                    "sent": "So for example, when you normally frequent itemset mining for Association rules or something like this, what you try to find his Co occurrences of instances or Co occurrences of labels, for example milk and bread Co occur three times.",
                    "label": 0
                },
                {
                    "sent": "So we'd say milk and bread is a frequent itemset based on some minimum minimum support threshold.",
                    "label": 0
                },
                {
                    "sent": "We could say two or three here, and it shows up three times.",
                    "label": 0
                },
                {
                    "sent": "Well, we have exactly the same thing in the adjacency list.",
                    "label": 0
                },
                {
                    "sent": "So for example, in the out linked list or we could say that in linked lists too, it works as effective.",
                    "label": 0
                },
                {
                    "sent": "1213 and 14 occur in all four out links, so we can mine for this frequent itemset and then we can represent this as a virtual node.",
                    "label": 0
                },
                {
                    "sent": "Then nodes 123 and four will point to this new virtual node and the virtual node will point to 1213 and 14.",
                    "label": 0
                },
                {
                    "sent": "The problem is, itemset mining is an exponential computation with respect to the label input and decreasing support.",
                    "label": 0
                },
                {
                    "sent": "The support is the minimum number of transactions or rows that a subset has to exist in for us to consider it relevant.",
                    "label": 0
                },
                {
                    "sent": "And in this case we have a lot of labels, typical data mining algorithms.",
                    "label": 0
                },
                {
                    "sent": "We have, things you'll see on the Internet over hundreds or thousands of labels, sometimes 10s of thousands of labels here in the web graph we have 20 billion labels because any node can appear in any out link list.",
                    "label": 0
                },
                {
                    "sent": "And we also want to mine at very low support a support of two even a supporter.",
                    "label": 0
                },
                {
                    "sent": "Two will give us a significant compression if the list is long.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we did was we developed this approximate itemset mining algorithm and then we embedded it into our compression mechanism.",
                    "label": 1
                },
                {
                    "sent": "And it basically follows these four steps.",
                    "label": 0
                },
                {
                    "sent": "The first thing we do is we cluster groups of nodes based on their similarity were similarity is essentially the intersection of their out linked lists and then given these clusters we take each cluster, hopefully small.",
                    "label": 0
                },
                {
                    "sent": "But if it's not a small cluster then there are highly similar and we look for patterns.",
                    "label": 0
                },
                {
                    "sent": "We use data mining techniques to find patterns in this small group would create a virtual note for that small group and we substitute that virtual node into the graph and then what's nice is we can iterate.",
                    "label": 1
                },
                {
                    "sent": "And for each iteration, each node is touched one time, so we don't have to worry about compressing the same nodes over and over or looking at the same nodes over and over in this mining process, our clustering process will group each node into one bucket at a time.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the clustering step will use min hashing.",
                    "label": 0
                },
                {
                    "sent": "K way min hashing and essentially what we do is we take a min hash for each row or each adjacency list and and we develop a sample for that.",
                    "label": 0
                },
                {
                    "sent": "So we take them in element in some permutation space and we do that for every row and then we do that K times to make this N by K matrix.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is an example.",
                    "label": 0
                },
                {
                    "sent": "And then we sort this matrix lexicographically.",
                    "label": 0
                },
                {
                    "sent": "Again, we have one row for each node in the graph, and we have K columns.",
                    "label": 0
                },
                {
                    "sent": "And this sort biases this sampling process left to right, but it's kind of the price we pay to maintain a low log linear complexity.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now once we have it sorted, we start reversing the columns an each set each range in a column that has the same value.",
                    "label": 0
                },
                {
                    "sent": "We know that this is represents a cluster.",
                    "label": 0
                },
                {
                    "sent": "Or at least at least the cluster.",
                    "label": 0
                },
                {
                    "sent": "And we also know that each row in that range row ID has at least one element in common with every other element in that range.",
                    "label": 0
                },
                {
                    "sent": "Every other node in that range, and then we walk across to the matrix.",
                    "label": 0
                },
                {
                    "sent": "Sub dividing in a telescoping fashion.",
                    "label": 0
                },
                {
                    "sent": "Finding other ranges sub ranges that have the same value so you can see in the first column.",
                    "label": 0
                },
                {
                    "sent": "They all have hash a so we know that these first four rows all have the same.",
                    "label": 0
                },
                {
                    "sent": "At least one value in common and we know that the 1st three at least have two and we know that the first 2 at least have three and we just go across this in recursive fashion until the size of the number of rows is small.",
                    "label": 0
                },
                {
                    "sent": "Then we pass that to the mining stage.",
                    "label": 0
                },
                {
                    "sent": "If we get all the way to K. And it's a still very large set.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really worry us because mining a large set when we have a large sample where they're all the same values is relatively easy because the overlap is high.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the mining step.",
                    "label": 0
                },
                {
                    "sent": "Basically we look at all the nodes and here's an example.",
                    "label": 0
                },
                {
                    "sent": "Let's assume that this was passed to the minor and these are the out links.",
                    "label": 0
                },
                {
                    "sent": "Once we go to the mining stage, we drop all the hashes.",
                    "label": 0
                },
                {
                    "sent": "We just used the hashes to group.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we do is we build a histogram.",
                    "label": 0
                },
                {
                    "sent": "And the purpose is that we're going to build a prefix tree, and we want to maximize the overlap in this prefix tree.",
                    "label": 0
                },
                {
                    "sent": "So if we sort these out link lists based on decreasing frequency, then it will maximize our overlap in this prefix tree.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "Label one or node ID one appeared in all eight rows or all eight.",
                    "label": 0
                },
                {
                    "sent": "Node out link lists.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anything that appeared just one time we don't really need it because they can't really help us with compression, so we just delete those from the out link list.",
                    "label": 0
                },
                {
                    "sent": "We don't actually delete him, we just don't add him to the prefix tree.",
                    "label": 0
                },
                {
                    "sent": "Then we sort these in decreasing frequency order.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we Add all these to a prefix tree.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it's called try.",
                    "label": 0
                },
                {
                    "sent": "So here's adding the first out link set to a try.",
                    "label": 0
                },
                {
                    "sent": "And the first number is is the pattern 12356 ten 12:15 at it you can see that stout link set for the first node and then we keep a list of all the node IDs to have this pattern.",
                    "label": 0
                },
                {
                    "sent": "So now we've added the second node ID 102 and you can see 102 has 123, and since it overlapped then we didn't have to add any nodes to our prefix tree.",
                    "label": 0
                },
                {
                    "sent": "This is similar to FP growth.",
                    "label": 0
                },
                {
                    "sent": "It's a single order projection.",
                    "label": 0
                },
                {
                    "sent": "Then here we Add all the node ID list to the prefix tree.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, once we have this prefix tree, we want to find the longest patterns we can find in this because they have the potential to give us the best compression.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we walked to a leaf.",
                    "label": 0
                },
                {
                    "sent": "And we add that leaf to a table where this table is storing the position in the prefix tree, where we can generate this pattern and in this table it's really pretty much just a pointer, but I'm just showing the length of the pattern, the elements in the list and the value of the compression.",
                    "label": 0
                },
                {
                    "sent": "We know we have to add a pointer for every time we add a virtual notes we subtract 1 for that in the length and the frequency we have to subtract 1 from the frequency because we still have to represent this so that we keep the same graph.",
                    "label": 0
                },
                {
                    "sent": "Then we walk up from the leaf to the root.",
                    "label": 0
                },
                {
                    "sent": "Anytime we see a list that's longer, we add it to the potential virtual node list table.",
                    "label": 0
                },
                {
                    "sent": "The reason being that this could offer us better compression, but we know that.",
                    "label": 0
                },
                {
                    "sent": "Say for example, this node 31 can give us better compression than 36, so we don't add that to the table.",
                    "label": 0
                },
                {
                    "sent": "So here we are, adding the next one.",
                    "label": 0
                },
                {
                    "sent": "We don't add the top node because it has a pattern length of one which can't give us compression.",
                    "label": 0
                },
                {
                    "sent": "And then we go to another leaf and we do the same thing.",
                    "label": 0
                },
                {
                    "sent": "We color the nodes as we walk along so we don't have to traverse a node more than once.",
                    "label": 0
                },
                {
                    "sent": "And this last branch we don't traverse it because it only has.",
                    "label": 0
                },
                {
                    "sent": "One element.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we sort this based on decreasing ability to compress the graph.",
                    "label": 0
                },
                {
                    "sent": "And the thing to point out here is that these patterns could interact with each other such that some patterns won't be able to give us any compression.",
                    "label": 0
                },
                {
                    "sent": "So we start with the best compression possible and you can think of this as we have said intersections and once we remove one virtual node that eliminates the ability to remove another virtual node.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's what it looks like after we add a virtual nodes.",
                    "label": 0
                },
                {
                    "sent": "And remember we have to actually maintain these edges to keep the graph.",
                    "label": 0
                },
                {
                    "sent": "And we didn't use virtual node 4 because it no longer existed in our graph as we were altering it.",
                    "label": 0
                },
                {
                    "sent": "It's kind of quick, but.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you want more details, you can look at the paper.",
                    "label": 0
                },
                {
                    "sent": "It's spelled out a little better.",
                    "label": 0
                },
                {
                    "sent": "So let's just look at a quick empirical evaluation.",
                    "label": 0
                },
                {
                    "sent": "We want to evaluate it along its ability to compress how fast it is, or its ability to scale paralyzed and.",
                    "label": 0
                },
                {
                    "sent": "The number of patterns we discovered.",
                    "label": 0
                },
                {
                    "sent": "We don't want to add 10X more virtual nodes than exists in the graph, or it would.",
                    "label": 0
                },
                {
                    "sent": "Hurt us as much as help us.",
                    "label": 0
                },
                {
                    "sent": "And these datasets were available from the web graph Framework folks laboratory for Web algorithmics.",
                    "label": 0
                },
                {
                    "sent": "Really nice data sets, actually.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's a compression afforded by the virtual nodes.",
                    "label": 0
                },
                {
                    "sent": "Just by reducing by adding these virtual nodes and reducing the number of edges in the graph.",
                    "label": 0
                },
                {
                    "sent": "And you can see in the general case where in the five to 7X compression ratio, which is pretty high, this one graph down here web based.",
                    "label": 0
                },
                {
                    "sent": "It has an average out degree of only eight, so there isn't much I can find in this graph.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Once we remove once we have these virtual nodes.",
                    "label": 0
                },
                {
                    "sent": "There's no reason why we can't use gap coding on top of it, so this is after we remove the virtual nodes and then add on gap coding and this could be Huffman coding or data coding or any coding scheme you prefer.",
                    "label": 0
                },
                {
                    "sent": "And now what?",
                    "label": 0
                },
                {
                    "sent": "You see, we're in the 10 to 15 range.",
                    "label": 0
                },
                {
                    "sent": "In the in the common case.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this compares us with the web graph frameworks.",
                    "label": 0
                },
                {
                    "sent": "Bits per edge.",
                    "label": 0
                },
                {
                    "sent": "And you can see where compareable to them and this is the number of virtual nodes we add.",
                    "label": 0
                },
                {
                    "sent": "And on average it's about 20% more nodes in the graph and these numbers take into consideration the offset for the additional nodes.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scalability is pretty good.",
                    "label": 0
                },
                {
                    "sent": "What we did was we took the UK 2006 data set, which was the largest publicly available one and we just took it in 250 million chunk edges.",
                    "label": 0
                },
                {
                    "sent": "Edge chunks and measured mining time.",
                    "label": 0
                },
                {
                    "sent": "Now as I said as you saw from the previous graph.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We really only need 2.",
                    "label": 0
                },
                {
                    "sent": "Compressed to three or four passes, and then we're basically done.",
                    "label": 0
                },
                {
                    "sent": "We don't find anymore path.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you look at this graph where.",
                    "label": 0
                },
                {
                    "sent": "Along for passes, it's pretty reasonable.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the one property we want to minimize the number of dereferences we need to make with the graph.",
                    "label": 0
                },
                {
                    "sent": "So the blue graph here you can see this is the average number of dereferences.",
                    "label": 0
                },
                {
                    "sent": "Additional dereferences you have to make in addition to getting the first list or the number of virtual nodes.",
                    "label": 0
                },
                {
                    "sent": "And after 4 passes you around 1.2 and it never really goes above 1.45 or so dereferences, which is a property we would like to keep.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "All these graphs are actually ordered by URL already and we don't leverage it because we hash and sort.",
                    "label": 0
                },
                {
                    "sent": "But one thing it does let us do is evaluate how far apart our communities are.",
                    "label": 0
                },
                {
                    "sent": "And what we do is we take the log of the standard deviation of the out links in the virtual node, so things that have a small standard deviation the virtual node are near each other and this total order sorted by URL and things that are far apart.",
                    "label": 0
                },
                {
                    "sent": "Things that have a high standard deviation are far apart and what we want to do is we do find patterns that you wouldn't find with the typical reference window where you're only looking back a small number of nodes.",
                    "label": 0
                },
                {
                    "sent": "You can see that there's a significant even at the standard deviation of 10 to the four.",
                    "label": 0
                },
                {
                    "sent": "We still find 7 or 8% of our compression or virtual notes comes from those.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The data mining folks are, and I'm not sure if this crowd is overly interested.",
                    "label": 0
                },
                {
                    "sent": "But then many folks like to inspect how this evaluates against traditional itemset mining.",
                    "label": 0
                },
                {
                    "sent": "And so I just have some charts.",
                    "label": 0
                },
                {
                    "sent": "Or if you want to come up to me after the talk and talk about it.",
                    "label": 0
                },
                {
                    "sent": "Basically we find patterns that are very long that traditional items that money doesn't find and that were the pink one and you can see we find patterns that are length over 1000 in traditional items of mine tends not to find these patterns and we have a constant runtime.",
                    "label": 0
                },
                {
                    "sent": "Regardless of support, because we don't do anything with support and you can see most itemset mining algorithms will scale exponentially with support, which is the red line.",
                    "label": 0
                },
                {
                    "sent": "It paralyzes pretty well.",
                    "label": 0
                },
                {
                    "sent": "When we go from.",
                    "label": 0
                },
                {
                    "sent": "One core to 8 cores on a multicore.",
                    "label": 0
                },
                {
                    "sent": "We go from 110 seconds to 1.5 seconds or so and it's near linear speedup.",
                    "label": 0
                },
                {
                    "sent": "Greater than 7.5.",
                    "label": 0
                },
                {
                    "sent": "And these patterns also give us improved compression.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So take home messages basically.",
                    "label": 0
                },
                {
                    "sent": "We have a web graph compression scheme that's compareable compression technology or compression afforded with the state of the art.",
                    "label": 0
                },
                {
                    "sent": "It doesn't necessarily require URL ordering or particular labeling for the nodes in the graph.",
                    "label": 0
                },
                {
                    "sent": "It provides seeds for Community discovery that can be far apart in this URL ordering, meaning that they are in separate domains.",
                    "label": 0
                },
                {
                    "sent": "Has a high compression ratio.",
                    "label": 0
                },
                {
                    "sent": "Scales pretty well and it can be extended.",
                    "label": 0
                },
                {
                    "sent": "You could use a different mining algorithm if you wanted to on the back end, or you could use a different clustering scheme if you wanted to.",
                    "label": 0
                },
                {
                    "sent": "It also has.",
                    "label": 0
                },
                {
                    "sent": "Bounded complexity log linear and the number of edges in the graph.",
                    "label": 0
                },
                {
                    "sent": "And it's an interesting use case for pattern mining.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For ongoing work.",
                    "label": 0
                },
                {
                    "sent": "We're working on computations on the compressed graph where things like page rank, where the computation is a function of the number of edges in the graph.",
                    "label": 0
                },
                {
                    "sent": "We're trying to see if we can get commensurate speedup.",
                    "label": 0
                },
                {
                    "sent": "The meaning we don't have to look as many edges because we don't have as many edges, and to see if well if it will converge in the same number of iterations.",
                    "label": 0
                },
                {
                    "sent": "And we're also looking at how hard or easy it is to update the graph, which typically with when you're compressing something to the last bit, it's never easy to update.",
                    "label": 0
                },
                {
                    "sent": "And we want to run this on the full graph.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And also like to thank the external references too.",
                    "label": 0
                },
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "In a scenario like World Wide Web where it follows long tail distribution power law so only the short head web pages, they will have a lot of out links.",
                    "label": 0
                },
                {
                    "sent": "In case of the long tail web pages where you have very a lot of web pages but they have very few out links, then how well this approach will work.",
                    "label": 0
                },
                {
                    "sent": "My thing is, it'll work pretty well and it's not actually pages that have a large number out links.",
                    "label": 1
                },
                {
                    "sent": "Probably honest comments pages that have a large number of in links, but the argument is valid that some pages will have a large number in links compared to some pages will have a small number in links or out links, but actually the because this minimize permutations approximated moved to the Jaccard coefficient discard coefficient isn't the best for us because.",
                    "label": 1
                },
                {
                    "sent": "It tends to be pretty low for something that has a really large set with something that has a small set.",
                    "label": 0
                },
                {
                    "sent": "The Jakarta coefficient, it really all we care about is intersection, but our strategy basically uses Chicago vision, But besides that most pages have a left navigation, have a top navigation.",
                    "label": 0
                },
                {
                    "sent": "These types of things where the links are the same for any given domain, regardless if there are a large number of links out links on it, and these tend to be the things that we capture.",
                    "label": 0
                },
                {
                    "sent": "In, in the general case.",
                    "label": 0
                },
                {
                    "sent": "Also, mirror domains are easy to capture this way too.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}