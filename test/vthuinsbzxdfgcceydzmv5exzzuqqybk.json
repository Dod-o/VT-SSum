{
    "id": "vthuinsbzxdfgcceydzmv5exzzuqqybk",
    "title": "Monte Carlo and the mind",
    "info": {
        "author": [
            "Tom Griffiths, Computational Cognitive Science Lab, Department of Psychology, UC Berkeley"
        ],
        "published": "June 15, 2010",
        "recorded": "May 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Monte Carlo Methods"
        ]
    },
    "url": "http://videolectures.net/mlss2010_griffiths_mcm/",
    "segmentation": [
        [
            "So I'm going to continue in this theme of talking about connections between human learning and machine learning, and today the topic is going to be Monte Carlo methods."
        ],
        [
            "So I think there are two kind of obvious ways in which Monte Carlo methods can be useful for.",
            "Both cognitive science and machine learning, so something which is relevant to both cognitive science machine learning is being able to solve.",
            "These problems are probabilistic inference.",
            "So you know we've been talking about Bayesian methods, and you know making Bayesian models and so on.",
            "But one of the things that you run into as soon as you've defined your fancy probabilistic model is that it turns out to be impossible to actually do Bayesian inference using that model.",
            "So Monte Carlo methods are something which we can use to solve that problem, and that's important if you're developing a model of human cognition, or if you're making a machine learning system.",
            "The second, which is more relevant direct to cognitive Sciences.",
            "That the fact that we can use those Monte Carlo methods as a scheme for approximating probabilistic inference might make us wonder whether they could also be used by people as a scheme for approximating probabilistic inference, and one of the questions that we've sort of repeatedly been addressing is this question about levels of analysis and algorithms and implementations and so on, and so maybe something like Monte Carlo methods gives us a device that we can use for trying to bridge those levels of analysis, looking at something which is a pretty good way of approximating probabilistic computation in computers, and thinking about whether the algorithms are.",
            "Implementation that people might be using for solving these kinds of problems respects that that's the same properties as those good solutions that we've come across in computer science and statistics, and so this is at the heart of an idea that we've been sort of talking about, called rational process models, where the idea is to take that sort of principle of rationality, which is expressed in our computational level analysis, and maybe push it down a level further and think about what might constitute good approximations to those ideal solutions and how those might turn into hypothesis that we could have about what's going on inside people's heads.",
            "So I'm going to talk about these two different uses of Monte Carlo methods.",
            "Just as a sort of the first thing I'm going to do is provide a kind of introduction to."
        ],
        [
            "The basic ideas behind Monte Carlo, so the sort of context in which you normally use the Monte Carlo method is when you want to approximate an expectation.",
            "So the idea is there's some probability distribution you have P of X and some function F of X, and you're really interested in sort of knowing something like the expectation.",
            "The average of this function F of X over this probability distribution P of X.",
            "So for discrete space this is just the sum over the values of X of F of X * P of X.",
            "For continuous space this would be the integral over XF of X * P X.",
            "So this is kind of you can think about it as the average of F when you're generating extra probability distribution P. So this sounds like a kind of fairly abstract quantity, but most of the questions that we're actually interested in answering from probabilistic models are things that we can normally formulate in terms of expectation."
        ],
        [
            "So you know you're kind of Canonical, sort of simple example of an expectation that sort of boring example might be something like if you wanted to calculate the average number of spots on a die, then you know what that would constitute as taking your function F of X.",
            "Well, you just take it to be, say, the values X is one through 6F of X equal to X and then P of X being uniform over those values.",
            "But there are also more interesting questions that you might want to ask.",
            "For example, you might want to know the probability that two observations belong to the same mixture component when you're fitting something like a mixture model.",
            "The kinds of things that I was talking about yesterday.",
            "One question you could have is whether you know these two objects belong to the same cluster, and that's something that you could compute with a posterior distribution over clusterings of the objects, and it ends up being an expectation where X would be our assignments of observations to components F of X would be one if two observations belong to the same component and 0 otherwise, and then P of X might be a posterior distribution over assignments.",
            "So a lot of the kinds of questions you might want to ask your Bayesian model, or things that you could end up formulating in terms of an expectation, anything which is.",
            "Probability that you want to derive from that model is an expectation normally where this function F is an indicator function for the property that you're interested in, and in other cases, if you have something like a continuous quantity, the expectation can give you an estimate of that quantity, like you can calculate the posterior mean.",
            "That's just an average of the values of X over the probability distribution P of X.",
            "So the Monte Carlo principle is the idea that whenever we have something that takes this form there."
        ],
        [
            "Very simple way that we can get an approximate answer and the idea is that instead of summing over all of the values of X weighted by their probabilities, what we do is we generate some values of X from the probability distribution P and then we average over those, giving each of those samples equal weight, right?",
            "So the idea is that what we're doing is constructing a discrete approximation to our probability distribution.",
            "The probability distribution says here all of the possible values of X, and I'm going to assign probabilities to those and what we're going to do is draw some values from that distribution and then assign them weights and kind of use that as a proxy for the probability distribution that we want to take an average over.",
            "So the Monte Carlo scheme that we can use is then approximate our expectation by the sum over those values that we've generated from this distribution P of F of X and then we just divide by the number of those values and that's reflecting the fact that we're giving each of those equal weight in that approximation to the probability distribution.",
            "So this should be a familiar idea at something that you know.",
            "I think everyone sort of is familiar with the idea of averaging.",
            "You can kind of think about why this works in terms of this idea of constructing this discrete approximation.",
            "If you do something like look at the ad."
        ],
        [
            "Number of spots on a die roll and look at what happens as you increase the number of rolls of the die.",
            "Then what you find is that your estimate of the average number of spots that's produced by using this method you know fairly quickly converges to the true value.",
            "Here the true value is given by this dotted line, and this is just a consequence of the law of large numbers.",
            "So provided you get a large enough sample that you generate enough samples from your probability distribution, you're going to end up with a good approximation to that target expectation.",
            "So this sort of general purpose sort of Monte Carlo scheme is something that I think we've probably all seen, but it's also something which doesn't always work, so you know, it seems like this is a good sort of general purpose trick for approximating probability distributions, but there are a few ways in which."
        ],
        [
            "This simple scheme might not work out, so one problem is that you might not be able to actually get a a way of sampling from the probability distribution that you're interested in.",
            "So in fact, you know there's a relatively small number of distributions from which it's easy to sample and sometimes just generating samples itself might be something which is computationally hard, so the math still works if you're able to generate samples from the probability distribution, you'll end up with a good approximation to your expectation, but the computer science part is more difficult that actually just generating samples from the probability distribution might be something that's difficult."
        ],
        [
            "Another problem is that if you've got something where you have a large discrete state space, like the example that I gave where we're interested in whether two observations get assigned to the same component, then you know then this ends up being computationally expensive because just generating the samples can be something which takes a lot of time because you essentially have to enumerate all of those possibilities to workout the probabilities and then come up with a scheme for sampling from that large set of discrete possibilities.",
            "So, for example, if you take a mixture model, you have an observations and K components.",
            "Then there came to the end possible component assignments for those observations, and so you can imagine as N gets large and K gets large, this number increases rapidly and so sampling directly from the probability distribution is something that can be pretty much."
        ],
        [
            "Intractable.",
            "Another problem is that sometimes we don't know everything about our probability distribution that we need in order to be able to generate samples.",
            "So we want to be able to generate samples from distributions where, say, we might only know the probability of each state up to a multiplicative constant, and this sounds like a weird situation here, like when's that when do you not know enough about your probability distribution to be able to generate samples from it?",
            "Well, a Canonical case where this?"
        ],
        [
            "Happens is actually in doing Bayesian inference, so if you think about what you're doing when you're doing Bayesian inference, you want to compute your posterior distribution.",
            "You do that by multiplying your likelihood and your prior, and then you need to do this step which is normalizing this over the space of all of your hypothesis.",
            "Well, if you imagine that you have a large discrete hypothesis space, or a situation where there's an integral down here over some continuous hypothesis space, and that integral isn't analytic, then you've got a problem because you know it's actually really, really hard for you to compute what the actual posterior probability of each thing might be.",
            "So it's easy to compute this part on top.",
            "That just means multiplying 2 numbers together for each of your hypothesis, but it can be hard to compute this part on the bottom, so a common situation when you're doing Bayesian inference is that you know the part on the top.",
            "You don't know the power on the bottom, and so you know your posterior distribution only up to that multiplicative constant.",
            "So this is something which statistical physicists run into with their models.",
            "Do they call this computing the partition function?",
            "It's, uh, sort of basic computational challenge, but it would be nice if we could have methods for generating samples from probability distributions that didn't require us to know exactly the probability of each hypothesis.",
            "But just know the probability something which is proportional to that probability because this is constant overall hypothesis.",
            "Then we know this probability distribution up to a multiplicative constant, and so these kinds of problems have then LED statisticians and physicists and computer scientists are developing more sophisticated sampling schemes that allow you to generate samples from more complex probability distributions and allow you to generate samples in cases where you might not know everything about your Dick."
        ],
        [
            "Bution so these modern Monte Carlo methods are what I'm going to talk about today.",
            "I gotta talk about two kinds of methods, one class which are based on what's called important sampling and one class called Markov Chain Monte Carlo algorithms.",
            "So I'm only going to focus on sampling algorithms today as ways of doing Bayesian inference.",
            "It's worth noting that there are alternative ways of doing approximating Bayesian inference which don't rely on this Monte Carlo principle, and don't use the kind of stochastic approximation, so things like variational inference or numerical quadrature.",
            "I'm not going to talk about those, but if you're interested in implementing Bayesian models, it's worth sort of going off and reading about these things.",
            "These are good schemes for coming up with approximations to Bayesian inference.",
            "They just don't use this kind of stochastic property that I'm focusing on today.",
            "So the first of these methods, important sampling."
        ],
        [
            "Basically, has the idea that what we're going to do because we can't generate from the probability distribution that we want to generate from is we're going to generate from the wrong distribution and then kind of try and correct for the fact that we did that, and there's a very simple sort of mathematical motivation for doing this.",
            "That sort of makes it clear that this might be something that's OK, so the idea is we want to approximate our expectation here.",
            "Here I'm just using a continuous state space, so we have this integral over the values of XF of XPX.",
            "And we're going to do is multiply and divide by another probability distribution Q of X, so you know there's some constraints that we've introduced here, so we don't want Q of X to be 0 anytime when P of X is not zero, but as long as that's OK, then we can do this and all that we've done is sort of introduced this thing and we multiply and divide, and so it's just sort of introduced into our equation.",
            "And that's alright.",
            "But as soon as you see something that's in this form, then you can recognize that this is another expectation.",
            "So this is now the expectation of this quantity.",
            "F of XP of X on Q of X with respect to the probability distribution Q of X and so Now the idea is we can approximate our expectation by instead of generating from P of X generating from Q of X and then taking the values of this function F of XP of X on Q of X for the values that we've sampled from this probability distribution and we just average over those.",
            "So this has a nice interpretation because you can think about these extra things that we've introduced in here as weights, which we're going to apply to our samples.",
            "So that even though we're sort of assigning uniform weight in principle when we're computing this expectation, really the weight which each of these values of X is going to have in terms of its contribution to our evaluation of this function is going to depend on this ratio of P of X to Q of X at that value of X.",
            "So if you kind of think about what this means, it means that something which has a higher probability under this distribution P, then it has under Q is going to get more weight and something that has a higher probability under Q than it has under P is going to get less weight.",
            "So what these weights are doing is compensating for the fact that.",
            "You know if something has higher probability than Q, then it's going to be under represented in our sample.",
            "We're going to having sampled from Q, not have enough of those things, so we should assign more weight to those.",
            "Then that's going to be like something from P an if we've got a higher probability under Q than under P, then we've got too many of those things in our sample, and so we should down.",
            "Wait, the man.",
            "We do that by assigning it away, which is less than one because P is less than Q.",
            "Makes sense, yeah?"
        ],
        [
            "OK, so the basic idea you know this is a sort of this is a nice easy distribution that we could sample from that.",
            "I'm just going to use this as my example, so here's our target distribution P. We have proposal distribution Q and normally we try and choose cues that it has heavier tails and P or something like that so that we avoid situations where these weights become extreme because you get extreme weights in situations where Q is small but P is large and that can basically skew your sample.",
            "But the idea is that we choose our proposal, something which it's easy to sample from.",
            "And then we can generate samples from the proposal and correct for the fact that we're sampling from distribution and effectively get something like samples from our target distribution P, even though it's hard to generate actual samples from that distribution so."
        ],
        [
            "A slight variant on this method is instead of sort of doing it the way that I said before where we have these weights which are computed in terms of taking these ratios and then normalizing by and you can do another sort of trick where you think about approximating.",
            "The the two parts of this integral separately but using the same samples and you come up with a slightly different important sampling scheme.",
            "It's exactly the same kind of idea, though what we do is generate our samples and then we assign each sample await which is this ratio that we had in our simpler important sampling scheme, but then normalized over the set of samples that were drawn from this probability distribution Q.",
            "So I'm not going to drive this one, you can just trust me that it works, but you can see that it kind of makes sense all that we're doing is making sure that these weights.",
            "So basically normalized, so we're making sure that the weight sum up to one, and so this takes us back to this idea of building a discrete approximation to our target probability distribution.",
            "The way we build our discrete approximation as we generated a set of values of X from this probability distribution Q, and then we assign those values weights which are going to end up summing up to one, and then we average function over that weighted approximation.",
            "Nice about this simple alternative.",
            "Sorry more complex alternative important sampling scheme is that it also works when P of X is only known up to a multiplicative constant, so we don't actually have to know the value of P of X because you can see here we have P of X in the numerator, wakes up here and then we're normalizing all of our weights.",
            "And because we're normalizing anything which is a multiplicative constant in P is going to cancel out.",
            "So this is a scheme that you can use for sampling from a probability distribution when you don't actually know how to normalize that probability distribution.",
            "And as a consequence it's a scheme that we can use for doing Bayesian inference.",
            "Because it doesn't require us to compute that nasty denominated that we had to compute in order to be able to evaluate the probability of a particular hypothesis.",
            "So I'll come back and talk about this in a second as a nice way of approximating Bayesian inference.",
            "I'm just going to say something which is kind of interesting and surprising about important sampling.",
            "So one thing that you might think is that because this is a way that sort of seems like an approximate way of doing our Monte Carlo, it seems like you know I said we can use it in situations where we don't know how to generate from our true distribution, and we can sort of do our best with another distribution.",
            "One thing that's surprising is that."
        ],
        [
            "It's actually possible to do better than simple Monte Carlo using important sampling, so you can workout the asymptotic variance of this estimator an when you work that out.",
            "So this is the what you find is that the variance depends on one term which has to do with the integral which you're trying to solve, which you also find if you calculate the acidotic variance of simple Monte Carlo.",
            "But also that takes into account the the weights which you're assigning to your your samples.",
            "So in order to control the variance you don't want these weights to be particularly extreme.",
            "You can also sort of, you know, take the derivative of this and solve and try and workout how to minimize this asymptotic variance and then from that try and derive the optimal distribution queue for generating your samples from.",
            "Well, this is the optimal distribution in terms of minimizing the asymptotic variance.",
            "It kind of requires you to know the solution to your expectation already, so this isn't a practical scheme for doing important sampling, because if you knew the value of this expectation, you wouldn't have to do important sampling, but looking at the form of this equation is quite interesting, so there's the main thing that you can observe is that.",
            "Basically what it says is that you should take Q of X proportional to the absolute value of F of X minus your expectation times P of X.",
            "So you should take a distribution which has large probabilities assigned to cases where essentially the values of F of X are large or influential in terms of their effect on that expectation, and P of X is large, so you're kind of thinking about trying to cover those regions which are going to have a big influence on the expectation that you're getting out.",
            "That's the first observation.",
            "The second observation is that this distribution is not just P of X, right?",
            "And that tells you what I was saying before, which is that the the the important sampling can actually be a better method than simple Monte Carlo.",
            "So we think of kind of simple Monte Carlo is a kind of gold standard for a way of approximating an expectation, but in fact if you're interested in getting the best approximation you can, often it can be better to not use simple Monte Carlo, but to use an appropriately designed important sampler so you can kind of."
        ],
        [
            "See, this is the optimal important sampler for that distribution that I showed you before an what this optimal important sampler does it.",
            "This is just for calculating the expectation of this probability distribution in blue.",
            "What it's doing is putting extra weight in these tail regions out here because those are the things are going to make the largest contribution to the expectation.",
            "So this probability distribution has low probability in regions where the actual expectation lies.",
            "The expectation of this distribution is 0, because anything that's around there isn't going to make much contribution to the expectation that you end up evaluating as you get out towards the tails.",
            "If you got a single extreme value out here that could throw off your.",
            "The expectation, So what it does is it tries to get a good concentration of values that are in these tales so as to avoid getting these very high variance estimates.",
            "So this is kind of an interesting instead of perhaps counterintuitive idea that using something like important sampling can be a better way of sampling from your probability distribution or evaluating your expectation than just sampling from your probability distribution."
        ],
        [
            "And this is showing just the variance of using simple Monte Carlo, and that optimal important sampler for that distribution.",
            "These are different estimates of the expectation as a function of the number of samples you take.",
            "The ratio of the variances and you can see that this simple Monte Carlo scheme has a higher variance than the optimal important sampling.",
            "OK, so let's go back to that idea that I mentioned that we can use this more stuff."
        ],
        [
            "Dicated important sampler in cases where we don't know P of X, we only know it up to a multiplicative constant.",
            "Well, that's useful because it gives us a way of doing Bayesian inference and a very."
        ],
        [
            "Simple scheme for doing Bayesian inference is something called likelihood weighting.",
            "So the idea is that what you do is use your prior as your proposal distribution.",
            "Your target is the posterior distribution, and if you kind of go through and work out well, you should be waiting things by the ratio of the posterior to the prior year.",
            "I doubt the posterior distribution and you cancel the common terms.",
            "Then you use the fact that you only need to sort of you know everything which is a multiplicative constant is going to cancel out.",
            "You get to the fact that the weights that you should assign your samples are just going to be proportional to the likelihood which is associated with those samples."
        ],
        [
            "So we have a very simple scheme for approximating Bayesian inference.",
            "If we wanted to approximate some expectation of a function F of H across our posterior distribution, pH given D, the way that we can do that is generate a bunch of samples of age from our prior distribution and then wait those samples by something which is proportional to the likelihood.",
            "So we end up with an expectation which is the sum over the sample hypothesis of the likelihood of that hypothesis times the function of that hypothesis.",
            "Normalized by the sum of the likelihoods across those hypothesis that we generated, and so you can kind of think about this.",
            "This is kind of like a little mini Bayesian inference, right?",
            "Instead of doing a Bayesian inference where we have to look at all of the hypothesis in our hypothesis space, we do that.",
            "Looking just at a subset of those hypothesis that we generated from the prior distribution.",
            "So likelihood weighting is not in general a good way of doing Bayesian inference.",
            "The member what I was saying about the performance of important samplers was how well it works is going to depend on kind of how big those weights get, and that really comes down to how well your target distribution is approximated by your proposal distribution.",
            "So in this case we're approximating our target distribution, the posterior with a proposal distribution, which is the prior.",
            "So in a case where your posterior is very different from your prior.",
            "You're going to end up with a bad estimate being produced by important sampling, but if you're in a situation where your posteriors reasonably close to your prior, then you're not necessarily.",
            "You know you're not going to go particularly badly wrong by generating samples from the prior waiting by the likelihood and normalizing, and so if you're in a situation where you're trying to make an inference from, say, a small amount of data, then you're in a situation where posteriors and priors should be pretty close together, and this kind of scheme can be a simple way of calculating the predictions that your Bayesian model makes.",
            "The reason why I've sort of highlighted this here is that it also gives us a way of connecting to these questions about how it is that people could go about approximating something like Bayesian inference.",
            "So it has a nice connection to a kind of psychological process model that people have explored in cognitive psychology."
        ],
        [
            "So this is a kind of model that I talked about a little bit yesterday.",
            "What's called an exemplar model in an example model, you just assume that decisions are being made by storing your previous events in memory and then activating those events by similarity.",
            "Whenever you encounter a new stimulus.",
            "So the example of the categorization model that I gave yesterday says that the probability that you choose category C given a stimulus X, you get by summing over all of your previously encountered.",
            "Object stimuli, So what we're going to come over is an indicator function, which tells you whether that stimulus belongs to that category and then waited by the similarity of the observed stimulus.",
            "X to that example, which you have stored in memory, and then that those weights are being normalized by summing over all of the stimuli that you have stored in memory.",
            "So this says assign the thing to the category which has the most similar examples to the thing that you see in terms of.",
            "That's the way to maximize this choice probability.",
            "But really, what you should be doing is storing.",
            "This example is in memory.",
            "Activating them, normalizing those activations and then just paying attention to those things that belong to the particular category that you're interested in, and as that sum gets larger than the probability that belongs to that category increases, you can give that kind of general form for this."
        ],
        [
            "Example model, so the more general form would say if you're trying to solve some problem, not just categorization.",
            "Example models have been used for things like function learning where you want to be able to predict what response somebody will make when they're making a continuous response based on a continuous input.",
            "I've been used for explaining decision making, a variety of other kinds of things.",
            "The idea is that the response that you expect people to make to a stimulus X depends on some function which you're interested in.",
            "So for example, if you're trying to learn the relationship between inputs and some continuous outputs, that function would reflect the continuous output value that was associated with that previously seen stimulus X.",
            "So you take that function, you average it over those examples which you previously seen stored in memory, weighted by the similarity of X to your stored.",
            "Example XI and then again normalizing this by the sum of the similarity over all of the stored exemplars.",
            "So what you might notice in looking at that equation is that it looks kind of familiar."
        ],
        [
            "Not a coincidence, so that kind of computation that you're doing an example model is equivalent to the kind of computation that we're doing when we are approximating the expectation over posterior distribution using important sampling using likelihood waiting.",
            "So here we have the sum over all of our example are stored in memory.",
            "This function that we're interested in, weighted by the similarity of the stimulus to the thing that stored in memory and the normalized, whereas in order to approximate our Bayesian expectation we take this function over all of those hypothesis that we have in memory.",
            "Sorry, the hypothesis that we sampled from the prior then wait those by the likelihood, normalizing by the sum over all of those have others that we sampled from the prior.",
            "So the idea is that if you go around the world kind of encountering things which you can store in memory, that kind of gives you a prior distribution which you can use for making certain kinds of Bayesian inferences.",
            "For example, if what you want to do is something which relates to, say, identifying a sound that you hear, or denoising that sound, so you assume that you hear that sound in a noisy way, and you want to try and figure out what it was.",
            "Then what you could do is kind of have stored examples of previous sounds that you've heard in memory.",
            "Those would constitute your hypothesis as to the identity of the sound that you heard and then activate those sounds based on the similarity to the current sound that you hear.",
            "As long as you're activating those sounds in a way which corresponds to a reasonable likelihood function like a Gaussian function or something like that, then this scheme of storing things in memory and then activating them based on the new stimuli that you encounter is going to implement this kind of Bayesian inference, and so this gives us a simple scheme for doing Bayesian inference.",
            "You just sort of walk around.",
            "Every time you encounter something that could be the solution to one of the inductor problems that you care about, you start in memory and then every time you want to make a difference and you encounter some data, you activate those hypothesis that you have stored in memory that are consistent with the data that you saw based on the likelihood.",
            "And then you normalize over that sample set and basically what you're going to be doing is important.",
            "Sampling using samples from the prior distribution, which is the set of stimulus set of hypothesis that you encountered in your environment.",
            "So everybody sort of makes sense.",
            "Yeah, OK, so I'll show."
        ],
        [
            "Do a simple example.",
            "This is the case that Josh talked about on the first day this problem of predicting the future.",
            "So if I tell you how much money are movies made so far, predicting how much money is going to make if I tell you how long somebody's lived predicting something about their lifespan, we can think about analyzing these problems as estimating some quantity total, the total duration or extent of a phenomenon having observed T, the current duration or extent of that phenomenon, we have to define a likelihood.",
            "The likelihood in this case is zero.",
            "40 total less than TL.",
            "Swan over T turtle.",
            "Assuming that you encounter that phenomenon at a random point in its duration so that you hear about that movie at a point that's uniformly distributed in terms of the between zero and the total amount of money that it makes, or encountering that person at a point that's uniformly distributed between when they're born and when they end up dying.",
            "Then we can use an example and model to approximate these kinds of computations.",
            "Calculating the posterior distribution over teetotal.",
            "Given T, we can look at different kinds of constraints.",
            "You can constrain the kinds of examples that people are generating.",
            "You can have a memory limited case where there's a limit on the total number of examples that people could generate, or a computation limited case where you assume that the bottleneck in computation is Justin paying attention to those generated.",
            "Those those examples that you generate, which are greater than.",
            "This should be T here.",
            "The example that you saw.",
            "So the idea here is that here we are restricting the total number of samples from the prior that are used in this computation.",
            "Here we're restricting the total number of prize that make a contribution.",
            "As a consequence, having a nonzero likelihood when you're evaluating one of these prediction problems, but the basic."
        ],
        [
            "Result is that here are these different distributions that we ask people about.",
            "Life spans.",
            "The grosses of movies.",
            "The reins of Pharaohs, the length of poems, the time in the House of Representatives, the run times in movies and then down here the black line is the optimal Bayesian prediction and the black circles, which are a little hard to see, are the human judgments.",
            "And each of the unfilled markers corresponds to one of these important sampling approximations.",
            "But here, using only five samples from that distribution which is shown above.",
            "So basically, if you look at these important sampling approximations, it doesn't matter whether we're using pure sampling from the well.",
            "So the first of these are the computational case or the memory limited case, and then we also show for comparison just pure sampling from the posterior distribution.",
            "These two important sampling cases computation limited on the memory limited case.",
            "It doesn't really matter which one of those we use, we actually end up with something which is a pretty good approximation to the true Bayesian answer in a pretty good approximation to the answers that people give.",
            "It's not fantastic, but it shows that this kind of Bayesian inference is something that you could do with a relatively small number of samples from the prior distribution.",
            "So if you think about you know how people make these prediction problems.",
            "One simple answer is that they could go around and make some observations of the world around them and store those examples in memory, and then when you ask him to make a prediction, how long is somebody going to live or how much money is this movie going to make?",
            "Just call up a few of those examples based on similarity to the current situation and as a consequence end up making pretty accurate predictions that conform to the appropriate prior distributions in these domains so."
        ],
        [
            "The basic idea about important sampling, then, is it's a general scheme that we can use for sampling from complex distributions that have similar relatives.",
            "Things like posterior distribution when we might know how to generate from the prior or for some other more complex distribution when there's another similar distribution we can think of constructing to approximate, it gives us a way of sampling from posterior distributions in cases where you have the prior and posterior close together, something that can be more efficient and simple Monte Carlo and it has nice links to example models in psychology, but perhaps one of the.",
            "The most interesting aspects of important sampling is that it also gives us a way of thinking about how people could go about updating their beliefs as data come in.",
            "So I've so far been talking about a kind of static inference problem where you want to make an inference about some hypothesis.",
            "You observe some data, and once you're done, then that's it, and you go on, you get on with the rest of your life.",
            "But really, the idea that people might be doing something like Bayesian inference means that you have to have an explanation not just for how they update their beliefs right now, but how they update their beliefs every time they get new data.",
            "If your Bayesian, you should be going around the world and every time you get a piece of information you should be calculating your posterior distribution.",
            "And if you're doing that, that imposes you know additional computational."
        ],
        [
            "Challenges, so if you have to compute your posterior distribution over hypotheses every time you observe a new piece of data, then that means that all of the costs of computation are going to be compounded like if it takes you some amount of time to calculate, update, update your posterior distribution, then that's just going to be multiplied by the amount of data that you see, so it would be nice if we had a scheme for updating our posterior distribution that allowed us to exploit the same kind of thing that we see in important sampling where you can have rather than the whole set of hypothesis you need to evaluate.",
            "Some subset of those and have as a consequence a much smaller hypothesis space that you need to work with, so we can actually do this by exploiting a property of Bayesian inference that you can kind of call yesterday's posterior is today's prior.",
            "It's the idea that if what you wanted to do is compute your posterior distribution of hypothesis, given an observations, and you already had your posterior distribution overseas, given the first N -- 1 observations in this case, I'm assuming that observations are independent condition hypothesis.",
            "But you can write down an equivalent formula if you assume that those dependencies between these data points.",
            "But the basic idea is that what you can do is take your posterior distribution after N -- 1 data points and then multiply it by the likelihood that's associated with that end of data point.",
            "And when you normalize, that's equivalent to doing Bayesian inference in the way that we've been talking about it, where you just start out with your prior and then you observe all your data and then you apply Bayes rule.",
            "You get exactly the same answer, because this is just proportional to this quantity and you know this is proportional to the prior times likelihood and so on.",
            "So everything works out.",
            "So this means that this is.",
            "This expression, yesterday's posteriors.",
            "Today's prior says that you can use the posterior distribution that you've established from the other data that you've observed as a prior distribution for interpreting the next observation that you make, and so that suggests a scheme that we could use for efficiently approximating Bayesian inference and being able to go around and update our beliefs in a way which allows us to attractively make sense of the world around us.",
            "And we can do that by repeatedly using important sampling.",
            "So the idea is that we can use yesterday's posterior.",
            "Which is going to be today's prior as a distribution from which we generate samples and then having generated those samples we re weight them by our likelihood and that gives us an approximation to today's posterior distribution.",
            "Well, you can do that recursively, right?",
            "So if the way that I'm representing yesterday's posterior distribution is in terms of a set of samples, well, then I could have used the posterior distribution I had before that and then related those samples and so on and so on and so on.",
            "So the idea is if you can approximate one posterior distribution in terms of.",
            "The previous posterior distribution and a new likelihood.",
            "Then you can approximate the previous posterior distribution in terms of the posterior distribution before that and the likelihood and then do that and do that and do that.",
            "And at some point you just get all the way back to the prior distribution and you've got the very simple important sample that we started out with.",
            "So this scheme for updating our beliefs or something that's known as a particle filter."
        ],
        [
            "Just schematically the way that it works.",
            "The idea is that what we start out with is a set of samples from the posterior distribution over hypothesis given N -- 1 observations.",
            "We then re wait those other sees by the likelihood and then we sample from that set of weighted hypothesis and we end up with some samples which are going to sample from our posterior distribution given an observations.",
            "And so this is just using that important sampling scheme where we take the distribution which we have.",
            "Based on the previous N -- 1 observations as our proposal distribution, in order to end up with some samples from our target distribution an obviously you can apply this recursively.",
            "So the way that you could obtain your samples given N -- 1 is by doing the same thing where.",
            "Here's an minus one, and then his N -- 2 and so on and so on and so on until you go all the way back and you just start out with some samples from the prior distribution.",
            "So you might notice that this particular algorithm doesn't seem very good, so there's one sort of obvious problem that you're going to have a few.",
            "Use this algorithm.",
            "Can anyone think about what the problem might be?",
            "I've kind of made it hard to see here by making these two dots look like they correspond to different things.",
            "So one way to think about it is how many distinct hypotheses do we have over here versus how many distinct hypotheses we have over here, right?",
            "So over here we have 4 over here we have three distinct hypotheses.",
            "We have two samples of this one, So what happens overtime as you're just going to end up losing hypothesis because the only thing that can happen here is we generate.",
            "You know, we have some hypothesis, we relate them we sample.",
            "Every time you re sample, you're going to be losing diversity in that sample, so actually using this algorithm to as an effective way of maintaining a posterior distribution over many successive data points, you need to also introduce the stage where you kind of move these guys around a little bit, or have some other way of introducing variation into the set of particles that you're using, and so there's a variety of schemes for doing that, but provided you do that in an appropriate way, then you're going to end up with something which is basically a dynamic approximation to the posterior distribution, relying only on a relatively small number of samples at each time.",
            "So the particle filter is kind of interesting because it gives you a way of doing Bayesian inference with limited memory load, right?",
            "There's only a small number of hypothesis you're maintaining at anytime you're updating those to reflect the new beliefs that you have, and as a consequence you can keep track of the posterior distribution just using a set of samples.",
            "And so it makes it much more tractable to work with some complicated probability models, and it gives us a hypothesis about how people might be able to sort of maintain the probability distributions over hypothesis as they get more data."
        ],
        [
            "This kind of scheme also works with dynamic hypothesis.",
            "So if you had a situation where there was not just data being generated from a single hypothesis, but data being generated from one hypothesis on day one, and then that influencing some hypothesis you have on day two and so on and so on and so on.",
            "So this is a simple hidden Markov model you can think about.",
            "This is something like, say the weather in Sardinia, right?",
            "So you have some observation of the weather looking out of the window.",
            "There's some underlying process which is generating that the weather today is going to influence the weather.",
            "Tomorrow you get some observation of the weather tomorrow.",
            "The weather tomorrow is going to influence the weather.",
            "The day after you get some observation of that.",
            "So if you want to be maintaining a posterior distribution over what these underlying states are, you need to take into account both the previous state and the data that you've acquired, so the so if you're trying to pick the weather, you know two days from now.",
            "You want to be able to take your the information that you've got from all of the data points that you've seen so far, as well as a data point that you observe on that day and then compute your posterior distribution.",
            "Given all of those data points for that particular hypothesis, and so you can do this, provided you have the distribution over that hypothesis given the previous data points and the likelihood of that observation, and when you one way you can calculate this probability distribution over the hypothesis given the previous data points is.",
            "Using the dynamics of the model.",
            "So if you have a distribution over the weather on that day and you have a way of predicting how the weather on the next day depends on the weather on the previous day, then you can take your posterior distribution over the weather that day and then multiply it by you know the distribution which tells you about the dynamics and then some out that variable which tells you about the weather on the previous day.",
            "So this is just a standard set of marginalization that you have seen in lots of the models that we're using.",
            "Well.",
            "The idea is that.",
            "This is just like the previous example, all that we've done is introduced.",
            "This extra bit of dynamics, but if you have a set of samples which are samples from the posterior distribution over hypotheses given days one through 3, then you can compute.",
            "You can end up with a set of samples from the posterior distribution overseas for days, one through 4 by taking those samples, modifying them in a way which reflects the dynamics of the underlying process, and then we re waiting by the likelihood to integrate the information that you get from observing the world on there."
        ],
        [
            "Today and so if you do that, you end up with a particle filtering scheme where we take our samples from the posterior given days, one through three.",
            "We then sample from the distribution which reflects the dynamics.",
            "So for each of these things, we're going to move it around, reflecting the probability distribution over the weather on day four given the weather on day three.",
            "Sorry, the probability distribution over the weather on day four based on the weather on day three, so this is kind of using our expectations about how that probabilistic process works, and then we wait those by the likelihood and then we re sample.",
            "And in this case you don't run into that same kind of problem about losing diversity.",
            "There are some intrinsic dynamics in the process which allow you to appropriately update your beliefs.",
            "But I'm in a way where you're not guaranteed to be ending up with something which is different on every something, which is the same on every successive iteration, because there's this process where you're moving things around already, you can end up with hypotheses moving around the space in a much more structured way, and you can also end up with situations where you're generating multiple samples based on a single previous sample.",
            "So like this, but where those are now as a consequence of the intrinsic dynamics of the process going to end up looking like different predictions about the weather that will happen on the next day."
        ],
        [
            "So the basic idea about particle filters is something which is kind of exciting from the perspective of thinking about how people might do something like probabilistic inference, it gives us a kind of general scheme that we could use for defining these rational process models for updating probability distributions overtime.",
            "So the idea is that you know if we want to understand how people do complex things like, you know, form clusters of the observations in the world around them, or learn things about language or any other kinds of things we've been analyzing is Bayesian inference that involves a problem of calculating a posterior distribution given a succession.",
            "Succession of observations and we can solve that problem by using something like a particle filter, provided we have enough particles.",
            "Well, this becomes an interesting kind of psychological hypothesis.",
            "If we think about what happens to particle filters as they get fewer particles than they actually need to give us a good approximation to the posterior distribution.",
            "So one interesting case which has been used to challenge Bayesian models of cognition is the fact that you see what are called order effects in human learning.",
            "So people learn different things when you give them the data that they see in different orders.",
            "So you might learn.",
            "One thing, if you see the data in one order and a different thing if you see the data in another order and this is something that kind of on the surface is inconsistent with the predictions that Bayesian models make.",
            "So if you have a model which assumes that the data you're seeing are generated from a process that stationary or generated in a way that's independent condition on the hypothesis, then those models are not going to predict order effects.",
            "They're going to say it doesn't matter what order you see the data in, you're going to end up making the same kind of inference right, as long as the data independent, it doesn't matter what order you see them in.",
            "All that matters is you know what the statistics of those data are.",
            "So the fact that you see order effects in human learning has been used to argue that Bayesian models are somehow wrong in the way that they're describing human learning.",
            "Well, if you use a particle filter and use it with relatively small number of particles, you actually see order effects, which can be consistent with the order effects you see in human learning.",
            "So what happens if you have a particle filter with a relatively small number of particles is you can kind of get committed to some hypothesis early on and then find it very hard to get away from those hypothesis, and that's something that we often see in human learning too.",
            "It's something called primacy effect, where you're more influenced by the first bit of data that you see.",
            "Then you are by subsequent data and so using this kind of approximation can give us a way of understanding what sort of factors might lead to those sorts of errors, and perhaps a way to bridge the gap between these high level computational level analysis.",
            "An understanding the kinds of algorithms that people might be using for performing probabilistic inference.",
            "And so we've used this kind of approach to make models of categorization using that during the process mixture model I was talking about yesterday.",
            "This is actually a way to dynamically assign.",
            "Objects to clusters and update those beliefs overtime.",
            "Other people have used them for modeling things like associative learning or change point detection, where you're trying to figure out when there's a change in the distribution from which you're seeing examples being generated, and one particular interesting case that we've looked at is sentence processing.",
            "So understanding a sentence is something that's one of these.",
            "Probabilistic inferences that requires you to update your beliefs as you're getting more data overtime.",
            "So every word that I produce is a clue as to the syntactic structure of the sentence that I'm articulating.",
            "And so in order to understand me, you need to be doing something like updating your beliefs about what sentence I'm saying.",
            "As, as I say, each of those words so a particle filter is a good way to think about solving that kind of problem.",
            "You can be getting, you know each piece of information and updating your beliefs about the appropriate syntactic structures and what we've shown is that you can actually use a particle filter to explain some effects which.",
            "Which are about people being misled as to the interpretation of a sentence.",
            "What are called garden path effects where you can kind of lead people to believe that a sentence is going one way and then it can end up going in a completely different direction.",
            "So our classic example of a garden path sentence is the horse raced past the barn fell.",
            "Right, everybody understood that sentence.",
            "Yeah, was it hard?",
            "So, so those are the kinds of things where you know the error that you make is kind of committing to a particular kind of syntactic structure where you think that the horses running past the barn, right?",
            "And the you know you're sort of setting up a particular set of interpretation of that sentence.",
            "And what you end up with is getting this extra piece of information at the end that tells you that that interpretation is wrong and you can explain some of those surprising effects in terms of what happens when you take a particle filter and get committed to a particular semantic interpretation early with a relatively small set of hypothesis about, you know what the underlying syntactic structure might be OK, so I talked about 2 uses of Monte Carlo methods.",
            "Let's take our two minute break."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to continue in this theme of talking about connections between human learning and machine learning, and today the topic is going to be Monte Carlo methods.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I think there are two kind of obvious ways in which Monte Carlo methods can be useful for.",
                    "label": 0
                },
                {
                    "sent": "Both cognitive science and machine learning, so something which is relevant to both cognitive science machine learning is being able to solve.",
                    "label": 0
                },
                {
                    "sent": "These problems are probabilistic inference.",
                    "label": 0
                },
                {
                    "sent": "So you know we've been talking about Bayesian methods, and you know making Bayesian models and so on.",
                    "label": 0
                },
                {
                    "sent": "But one of the things that you run into as soon as you've defined your fancy probabilistic model is that it turns out to be impossible to actually do Bayesian inference using that model.",
                    "label": 0
                },
                {
                    "sent": "So Monte Carlo methods are something which we can use to solve that problem, and that's important if you're developing a model of human cognition, or if you're making a machine learning system.",
                    "label": 0
                },
                {
                    "sent": "The second, which is more relevant direct to cognitive Sciences.",
                    "label": 0
                },
                {
                    "sent": "That the fact that we can use those Monte Carlo methods as a scheme for approximating probabilistic inference might make us wonder whether they could also be used by people as a scheme for approximating probabilistic inference, and one of the questions that we've sort of repeatedly been addressing is this question about levels of analysis and algorithms and implementations and so on, and so maybe something like Monte Carlo methods gives us a device that we can use for trying to bridge those levels of analysis, looking at something which is a pretty good way of approximating probabilistic computation in computers, and thinking about whether the algorithms are.",
                    "label": 0
                },
                {
                    "sent": "Implementation that people might be using for solving these kinds of problems respects that that's the same properties as those good solutions that we've come across in computer science and statistics, and so this is at the heart of an idea that we've been sort of talking about, called rational process models, where the idea is to take that sort of principle of rationality, which is expressed in our computational level analysis, and maybe push it down a level further and think about what might constitute good approximations to those ideal solutions and how those might turn into hypothesis that we could have about what's going on inside people's heads.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk about these two different uses of Monte Carlo methods.",
                    "label": 1
                },
                {
                    "sent": "Just as a sort of the first thing I'm going to do is provide a kind of introduction to.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The basic ideas behind Monte Carlo, so the sort of context in which you normally use the Monte Carlo method is when you want to approximate an expectation.",
                    "label": 0
                },
                {
                    "sent": "So the idea is there's some probability distribution you have P of X and some function F of X, and you're really interested in sort of knowing something like the expectation.",
                    "label": 0
                },
                {
                    "sent": "The average of this function F of X over this probability distribution P of X.",
                    "label": 1
                },
                {
                    "sent": "So for discrete space this is just the sum over the values of X of F of X * P of X.",
                    "label": 0
                },
                {
                    "sent": "For continuous space this would be the integral over XF of X * P X.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of you can think about it as the average of F when you're generating extra probability distribution P. So this sounds like a kind of fairly abstract quantity, but most of the questions that we're actually interested in answering from probabilistic models are things that we can normally formulate in terms of expectation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you know you're kind of Canonical, sort of simple example of an expectation that sort of boring example might be something like if you wanted to calculate the average number of spots on a die, then you know what that would constitute as taking your function F of X.",
                    "label": 0
                },
                {
                    "sent": "Well, you just take it to be, say, the values X is one through 6F of X equal to X and then P of X being uniform over those values.",
                    "label": 0
                },
                {
                    "sent": "But there are also more interesting questions that you might want to ask.",
                    "label": 0
                },
                {
                    "sent": "For example, you might want to know the probability that two observations belong to the same mixture component when you're fitting something like a mixture model.",
                    "label": 1
                },
                {
                    "sent": "The kinds of things that I was talking about yesterday.",
                    "label": 0
                },
                {
                    "sent": "One question you could have is whether you know these two objects belong to the same cluster, and that's something that you could compute with a posterior distribution over clusterings of the objects, and it ends up being an expectation where X would be our assignments of observations to components F of X would be one if two observations belong to the same component and 0 otherwise, and then P of X might be a posterior distribution over assignments.",
                    "label": 1
                },
                {
                    "sent": "So a lot of the kinds of questions you might want to ask your Bayesian model, or things that you could end up formulating in terms of an expectation, anything which is.",
                    "label": 0
                },
                {
                    "sent": "Probability that you want to derive from that model is an expectation normally where this function F is an indicator function for the property that you're interested in, and in other cases, if you have something like a continuous quantity, the expectation can give you an estimate of that quantity, like you can calculate the posterior mean.",
                    "label": 0
                },
                {
                    "sent": "That's just an average of the values of X over the probability distribution P of X.",
                    "label": 0
                },
                {
                    "sent": "So the Monte Carlo principle is the idea that whenever we have something that takes this form there.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very simple way that we can get an approximate answer and the idea is that instead of summing over all of the values of X weighted by their probabilities, what we do is we generate some values of X from the probability distribution P and then we average over those, giving each of those samples equal weight, right?",
                    "label": 0
                },
                {
                    "sent": "So the idea is that what we're doing is constructing a discrete approximation to our probability distribution.",
                    "label": 0
                },
                {
                    "sent": "The probability distribution says here all of the possible values of X, and I'm going to assign probabilities to those and what we're going to do is draw some values from that distribution and then assign them weights and kind of use that as a proxy for the probability distribution that we want to take an average over.",
                    "label": 0
                },
                {
                    "sent": "So the Monte Carlo scheme that we can use is then approximate our expectation by the sum over those values that we've generated from this distribution P of F of X and then we just divide by the number of those values and that's reflecting the fact that we're giving each of those equal weight in that approximation to the probability distribution.",
                    "label": 1
                },
                {
                    "sent": "So this should be a familiar idea at something that you know.",
                    "label": 0
                },
                {
                    "sent": "I think everyone sort of is familiar with the idea of averaging.",
                    "label": 0
                },
                {
                    "sent": "You can kind of think about why this works in terms of this idea of constructing this discrete approximation.",
                    "label": 0
                },
                {
                    "sent": "If you do something like look at the ad.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Number of spots on a die roll and look at what happens as you increase the number of rolls of the die.",
                    "label": 0
                },
                {
                    "sent": "Then what you find is that your estimate of the average number of spots that's produced by using this method you know fairly quickly converges to the true value.",
                    "label": 0
                },
                {
                    "sent": "Here the true value is given by this dotted line, and this is just a consequence of the law of large numbers.",
                    "label": 1
                },
                {
                    "sent": "So provided you get a large enough sample that you generate enough samples from your probability distribution, you're going to end up with a good approximation to that target expectation.",
                    "label": 0
                },
                {
                    "sent": "So this sort of general purpose sort of Monte Carlo scheme is something that I think we've probably all seen, but it's also something which doesn't always work, so you know, it seems like this is a good sort of general purpose trick for approximating probability distributions, but there are a few ways in which.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This simple scheme might not work out, so one problem is that you might not be able to actually get a a way of sampling from the probability distribution that you're interested in.",
                    "label": 0
                },
                {
                    "sent": "So in fact, you know there's a relatively small number of distributions from which it's easy to sample and sometimes just generating samples itself might be something which is computationally hard, so the math still works if you're able to generate samples from the probability distribution, you'll end up with a good approximation to your expectation, but the computer science part is more difficult that actually just generating samples from the probability distribution might be something that's difficult.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another problem is that if you've got something where you have a large discrete state space, like the example that I gave where we're interested in whether two observations get assigned to the same component, then you know then this ends up being computationally expensive because just generating the samples can be something which takes a lot of time because you essentially have to enumerate all of those possibilities to workout the probabilities and then come up with a scheme for sampling from that large set of discrete possibilities.",
                    "label": 0
                },
                {
                    "sent": "So, for example, if you take a mixture model, you have an observations and K components.",
                    "label": 1
                },
                {
                    "sent": "Then there came to the end possible component assignments for those observations, and so you can imagine as N gets large and K gets large, this number increases rapidly and so sampling directly from the probability distribution is something that can be pretty much.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Intractable.",
                    "label": 0
                },
                {
                    "sent": "Another problem is that sometimes we don't know everything about our probability distribution that we need in order to be able to generate samples.",
                    "label": 0
                },
                {
                    "sent": "So we want to be able to generate samples from distributions where, say, we might only know the probability of each state up to a multiplicative constant, and this sounds like a weird situation here, like when's that when do you not know enough about your probability distribution to be able to generate samples from it?",
                    "label": 1
                },
                {
                    "sent": "Well, a Canonical case where this?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Happens is actually in doing Bayesian inference, so if you think about what you're doing when you're doing Bayesian inference, you want to compute your posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "You do that by multiplying your likelihood and your prior, and then you need to do this step which is normalizing this over the space of all of your hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Well, if you imagine that you have a large discrete hypothesis space, or a situation where there's an integral down here over some continuous hypothesis space, and that integral isn't analytic, then you've got a problem because you know it's actually really, really hard for you to compute what the actual posterior probability of each thing might be.",
                    "label": 0
                },
                {
                    "sent": "So it's easy to compute this part on top.",
                    "label": 0
                },
                {
                    "sent": "That just means multiplying 2 numbers together for each of your hypothesis, but it can be hard to compute this part on the bottom, so a common situation when you're doing Bayesian inference is that you know the part on the top.",
                    "label": 1
                },
                {
                    "sent": "You don't know the power on the bottom, and so you know your posterior distribution only up to that multiplicative constant.",
                    "label": 0
                },
                {
                    "sent": "So this is something which statistical physicists run into with their models.",
                    "label": 0
                },
                {
                    "sent": "Do they call this computing the partition function?",
                    "label": 0
                },
                {
                    "sent": "It's, uh, sort of basic computational challenge, but it would be nice if we could have methods for generating samples from probability distributions that didn't require us to know exactly the probability of each hypothesis.",
                    "label": 0
                },
                {
                    "sent": "But just know the probability something which is proportional to that probability because this is constant overall hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Then we know this probability distribution up to a multiplicative constant, and so these kinds of problems have then LED statisticians and physicists and computer scientists are developing more sophisticated sampling schemes that allow you to generate samples from more complex probability distributions and allow you to generate samples in cases where you might not know everything about your Dick.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bution so these modern Monte Carlo methods are what I'm going to talk about today.",
                    "label": 1
                },
                {
                    "sent": "I gotta talk about two kinds of methods, one class which are based on what's called important sampling and one class called Markov Chain Monte Carlo algorithms.",
                    "label": 0
                },
                {
                    "sent": "So I'm only going to focus on sampling algorithms today as ways of doing Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "It's worth noting that there are alternative ways of doing approximating Bayesian inference which don't rely on this Monte Carlo principle, and don't use the kind of stochastic approximation, so things like variational inference or numerical quadrature.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about those, but if you're interested in implementing Bayesian models, it's worth sort of going off and reading about these things.",
                    "label": 0
                },
                {
                    "sent": "These are good schemes for coming up with approximations to Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "They just don't use this kind of stochastic property that I'm focusing on today.",
                    "label": 0
                },
                {
                    "sent": "So the first of these methods, important sampling.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically, has the idea that what we're going to do because we can't generate from the probability distribution that we want to generate from is we're going to generate from the wrong distribution and then kind of try and correct for the fact that we did that, and there's a very simple sort of mathematical motivation for doing this.",
                    "label": 1
                },
                {
                    "sent": "That sort of makes it clear that this might be something that's OK, so the idea is we want to approximate our expectation here.",
                    "label": 0
                },
                {
                    "sent": "Here I'm just using a continuous state space, so we have this integral over the values of XF of XPX.",
                    "label": 0
                },
                {
                    "sent": "And we're going to do is multiply and divide by another probability distribution Q of X, so you know there's some constraints that we've introduced here, so we don't want Q of X to be 0 anytime when P of X is not zero, but as long as that's OK, then we can do this and all that we've done is sort of introduced this thing and we multiply and divide, and so it's just sort of introduced into our equation.",
                    "label": 0
                },
                {
                    "sent": "And that's alright.",
                    "label": 0
                },
                {
                    "sent": "But as soon as you see something that's in this form, then you can recognize that this is another expectation.",
                    "label": 0
                },
                {
                    "sent": "So this is now the expectation of this quantity.",
                    "label": 0
                },
                {
                    "sent": "F of XP of X on Q of X with respect to the probability distribution Q of X and so Now the idea is we can approximate our expectation by instead of generating from P of X generating from Q of X and then taking the values of this function F of XP of X on Q of X for the values that we've sampled from this probability distribution and we just average over those.",
                    "label": 0
                },
                {
                    "sent": "So this has a nice interpretation because you can think about these extra things that we've introduced in here as weights, which we're going to apply to our samples.",
                    "label": 0
                },
                {
                    "sent": "So that even though we're sort of assigning uniform weight in principle when we're computing this expectation, really the weight which each of these values of X is going to have in terms of its contribution to our evaluation of this function is going to depend on this ratio of P of X to Q of X at that value of X.",
                    "label": 0
                },
                {
                    "sent": "So if you kind of think about what this means, it means that something which has a higher probability under this distribution P, then it has under Q is going to get more weight and something that has a higher probability under Q than it has under P is going to get less weight.",
                    "label": 0
                },
                {
                    "sent": "So what these weights are doing is compensating for the fact that.",
                    "label": 0
                },
                {
                    "sent": "You know if something has higher probability than Q, then it's going to be under represented in our sample.",
                    "label": 0
                },
                {
                    "sent": "We're going to having sampled from Q, not have enough of those things, so we should assign more weight to those.",
                    "label": 0
                },
                {
                    "sent": "Then that's going to be like something from P an if we've got a higher probability under Q than under P, then we've got too many of those things in our sample, and so we should down.",
                    "label": 0
                },
                {
                    "sent": "Wait, the man.",
                    "label": 0
                },
                {
                    "sent": "We do that by assigning it away, which is less than one because P is less than Q.",
                    "label": 0
                },
                {
                    "sent": "Makes sense, yeah?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the basic idea you know this is a sort of this is a nice easy distribution that we could sample from that.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to use this as my example, so here's our target distribution P. We have proposal distribution Q and normally we try and choose cues that it has heavier tails and P or something like that so that we avoid situations where these weights become extreme because you get extreme weights in situations where Q is small but P is large and that can basically skew your sample.",
                    "label": 0
                },
                {
                    "sent": "But the idea is that we choose our proposal, something which it's easy to sample from.",
                    "label": 0
                },
                {
                    "sent": "And then we can generate samples from the proposal and correct for the fact that we're sampling from distribution and effectively get something like samples from our target distribution P, even though it's hard to generate actual samples from that distribution so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A slight variant on this method is instead of sort of doing it the way that I said before where we have these weights which are computed in terms of taking these ratios and then normalizing by and you can do another sort of trick where you think about approximating.",
                    "label": 0
                },
                {
                    "sent": "The the two parts of this integral separately but using the same samples and you come up with a slightly different important sampling scheme.",
                    "label": 0
                },
                {
                    "sent": "It's exactly the same kind of idea, though what we do is generate our samples and then we assign each sample await which is this ratio that we had in our simpler important sampling scheme, but then normalized over the set of samples that were drawn from this probability distribution Q.",
                    "label": 0
                },
                {
                    "sent": "So I'm not going to drive this one, you can just trust me that it works, but you can see that it kind of makes sense all that we're doing is making sure that these weights.",
                    "label": 0
                },
                {
                    "sent": "So basically normalized, so we're making sure that the weight sum up to one, and so this takes us back to this idea of building a discrete approximation to our target probability distribution.",
                    "label": 0
                },
                {
                    "sent": "The way we build our discrete approximation as we generated a set of values of X from this probability distribution Q, and then we assign those values weights which are going to end up summing up to one, and then we average function over that weighted approximation.",
                    "label": 0
                },
                {
                    "sent": "Nice about this simple alternative.",
                    "label": 0
                },
                {
                    "sent": "Sorry more complex alternative important sampling scheme is that it also works when P of X is only known up to a multiplicative constant, so we don't actually have to know the value of P of X because you can see here we have P of X in the numerator, wakes up here and then we're normalizing all of our weights.",
                    "label": 1
                },
                {
                    "sent": "And because we're normalizing anything which is a multiplicative constant in P is going to cancel out.",
                    "label": 0
                },
                {
                    "sent": "So this is a scheme that you can use for sampling from a probability distribution when you don't actually know how to normalize that probability distribution.",
                    "label": 0
                },
                {
                    "sent": "And as a consequence it's a scheme that we can use for doing Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "Because it doesn't require us to compute that nasty denominated that we had to compute in order to be able to evaluate the probability of a particular hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So I'll come back and talk about this in a second as a nice way of approximating Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to say something which is kind of interesting and surprising about important sampling.",
                    "label": 0
                },
                {
                    "sent": "So one thing that you might think is that because this is a way that sort of seems like an approximate way of doing our Monte Carlo, it seems like you know I said we can use it in situations where we don't know how to generate from our true distribution, and we can sort of do our best with another distribution.",
                    "label": 0
                },
                {
                    "sent": "One thing that's surprising is that.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's actually possible to do better than simple Monte Carlo using important sampling, so you can workout the asymptotic variance of this estimator an when you work that out.",
                    "label": 0
                },
                {
                    "sent": "So this is the what you find is that the variance depends on one term which has to do with the integral which you're trying to solve, which you also find if you calculate the acidotic variance of simple Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "But also that takes into account the the weights which you're assigning to your your samples.",
                    "label": 0
                },
                {
                    "sent": "So in order to control the variance you don't want these weights to be particularly extreme.",
                    "label": 0
                },
                {
                    "sent": "You can also sort of, you know, take the derivative of this and solve and try and workout how to minimize this asymptotic variance and then from that try and derive the optimal distribution queue for generating your samples from.",
                    "label": 0
                },
                {
                    "sent": "Well, this is the optimal distribution in terms of minimizing the asymptotic variance.",
                    "label": 1
                },
                {
                    "sent": "It kind of requires you to know the solution to your expectation already, so this isn't a practical scheme for doing important sampling, because if you knew the value of this expectation, you wouldn't have to do important sampling, but looking at the form of this equation is quite interesting, so there's the main thing that you can observe is that.",
                    "label": 0
                },
                {
                    "sent": "Basically what it says is that you should take Q of X proportional to the absolute value of F of X minus your expectation times P of X.",
                    "label": 0
                },
                {
                    "sent": "So you should take a distribution which has large probabilities assigned to cases where essentially the values of F of X are large or influential in terms of their effect on that expectation, and P of X is large, so you're kind of thinking about trying to cover those regions which are going to have a big influence on the expectation that you're getting out.",
                    "label": 0
                },
                {
                    "sent": "That's the first observation.",
                    "label": 0
                },
                {
                    "sent": "The second observation is that this distribution is not just P of X, right?",
                    "label": 0
                },
                {
                    "sent": "And that tells you what I was saying before, which is that the the the important sampling can actually be a better method than simple Monte Carlo.",
                    "label": 0
                },
                {
                    "sent": "So we think of kind of simple Monte Carlo is a kind of gold standard for a way of approximating an expectation, but in fact if you're interested in getting the best approximation you can, often it can be better to not use simple Monte Carlo, but to use an appropriately designed important sampler so you can kind of.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See, this is the optimal important sampler for that distribution that I showed you before an what this optimal important sampler does it.",
                    "label": 0
                },
                {
                    "sent": "This is just for calculating the expectation of this probability distribution in blue.",
                    "label": 0
                },
                {
                    "sent": "What it's doing is putting extra weight in these tail regions out here because those are the things are going to make the largest contribution to the expectation.",
                    "label": 0
                },
                {
                    "sent": "So this probability distribution has low probability in regions where the actual expectation lies.",
                    "label": 0
                },
                {
                    "sent": "The expectation of this distribution is 0, because anything that's around there isn't going to make much contribution to the expectation that you end up evaluating as you get out towards the tails.",
                    "label": 0
                },
                {
                    "sent": "If you got a single extreme value out here that could throw off your.",
                    "label": 0
                },
                {
                    "sent": "The expectation, So what it does is it tries to get a good concentration of values that are in these tales so as to avoid getting these very high variance estimates.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of an interesting instead of perhaps counterintuitive idea that using something like important sampling can be a better way of sampling from your probability distribution or evaluating your expectation than just sampling from your probability distribution.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is showing just the variance of using simple Monte Carlo, and that optimal important sampler for that distribution.",
                    "label": 0
                },
                {
                    "sent": "These are different estimates of the expectation as a function of the number of samples you take.",
                    "label": 0
                },
                {
                    "sent": "The ratio of the variances and you can see that this simple Monte Carlo scheme has a higher variance than the optimal important sampling.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's go back to that idea that I mentioned that we can use this more stuff.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dicated important sampler in cases where we don't know P of X, we only know it up to a multiplicative constant.",
                    "label": 0
                },
                {
                    "sent": "Well, that's useful because it gives us a way of doing Bayesian inference and a very.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Simple scheme for doing Bayesian inference is something called likelihood weighting.",
                    "label": 1
                },
                {
                    "sent": "So the idea is that what you do is use your prior as your proposal distribution.",
                    "label": 1
                },
                {
                    "sent": "Your target is the posterior distribution, and if you kind of go through and work out well, you should be waiting things by the ratio of the posterior to the prior year.",
                    "label": 0
                },
                {
                    "sent": "I doubt the posterior distribution and you cancel the common terms.",
                    "label": 0
                },
                {
                    "sent": "Then you use the fact that you only need to sort of you know everything which is a multiplicative constant is going to cancel out.",
                    "label": 0
                },
                {
                    "sent": "You get to the fact that the weights that you should assign your samples are just going to be proportional to the likelihood which is associated with those samples.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have a very simple scheme for approximating Bayesian inference.",
                    "label": 1
                },
                {
                    "sent": "If we wanted to approximate some expectation of a function F of H across our posterior distribution, pH given D, the way that we can do that is generate a bunch of samples of age from our prior distribution and then wait those samples by something which is proportional to the likelihood.",
                    "label": 0
                },
                {
                    "sent": "So we end up with an expectation which is the sum over the sample hypothesis of the likelihood of that hypothesis times the function of that hypothesis.",
                    "label": 0
                },
                {
                    "sent": "Normalized by the sum of the likelihoods across those hypothesis that we generated, and so you can kind of think about this.",
                    "label": 0
                },
                {
                    "sent": "This is kind of like a little mini Bayesian inference, right?",
                    "label": 0
                },
                {
                    "sent": "Instead of doing a Bayesian inference where we have to look at all of the hypothesis in our hypothesis space, we do that.",
                    "label": 0
                },
                {
                    "sent": "Looking just at a subset of those hypothesis that we generated from the prior distribution.",
                    "label": 0
                },
                {
                    "sent": "So likelihood weighting is not in general a good way of doing Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "The member what I was saying about the performance of important samplers was how well it works is going to depend on kind of how big those weights get, and that really comes down to how well your target distribution is approximated by your proposal distribution.",
                    "label": 0
                },
                {
                    "sent": "So in this case we're approximating our target distribution, the posterior with a proposal distribution, which is the prior.",
                    "label": 0
                },
                {
                    "sent": "So in a case where your posterior is very different from your prior.",
                    "label": 0
                },
                {
                    "sent": "You're going to end up with a bad estimate being produced by important sampling, but if you're in a situation where your posteriors reasonably close to your prior, then you're not necessarily.",
                    "label": 0
                },
                {
                    "sent": "You know you're not going to go particularly badly wrong by generating samples from the prior waiting by the likelihood and normalizing, and so if you're in a situation where you're trying to make an inference from, say, a small amount of data, then you're in a situation where posteriors and priors should be pretty close together, and this kind of scheme can be a simple way of calculating the predictions that your Bayesian model makes.",
                    "label": 0
                },
                {
                    "sent": "The reason why I've sort of highlighted this here is that it also gives us a way of connecting to these questions about how it is that people could go about approximating something like Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "So it has a nice connection to a kind of psychological process model that people have explored in cognitive psychology.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a kind of model that I talked about a little bit yesterday.",
                    "label": 0
                },
                {
                    "sent": "What's called an exemplar model in an example model, you just assume that decisions are being made by storing your previous events in memory and then activating those events by similarity.",
                    "label": 1
                },
                {
                    "sent": "Whenever you encounter a new stimulus.",
                    "label": 0
                },
                {
                    "sent": "So the example of the categorization model that I gave yesterday says that the probability that you choose category C given a stimulus X, you get by summing over all of your previously encountered.",
                    "label": 0
                },
                {
                    "sent": "Object stimuli, So what we're going to come over is an indicator function, which tells you whether that stimulus belongs to that category and then waited by the similarity of the observed stimulus.",
                    "label": 0
                },
                {
                    "sent": "X to that example, which you have stored in memory, and then that those weights are being normalized by summing over all of the stimuli that you have stored in memory.",
                    "label": 0
                },
                {
                    "sent": "So this says assign the thing to the category which has the most similar examples to the thing that you see in terms of.",
                    "label": 0
                },
                {
                    "sent": "That's the way to maximize this choice probability.",
                    "label": 0
                },
                {
                    "sent": "But really, what you should be doing is storing.",
                    "label": 0
                },
                {
                    "sent": "This example is in memory.",
                    "label": 0
                },
                {
                    "sent": "Activating them, normalizing those activations and then just paying attention to those things that belong to the particular category that you're interested in, and as that sum gets larger than the probability that belongs to that category increases, you can give that kind of general form for this.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example model, so the more general form would say if you're trying to solve some problem, not just categorization.",
                    "label": 0
                },
                {
                    "sent": "Example models have been used for things like function learning where you want to be able to predict what response somebody will make when they're making a continuous response based on a continuous input.",
                    "label": 0
                },
                {
                    "sent": "I've been used for explaining decision making, a variety of other kinds of things.",
                    "label": 0
                },
                {
                    "sent": "The idea is that the response that you expect people to make to a stimulus X depends on some function which you're interested in.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you're trying to learn the relationship between inputs and some continuous outputs, that function would reflect the continuous output value that was associated with that previously seen stimulus X.",
                    "label": 0
                },
                {
                    "sent": "So you take that function, you average it over those examples which you previously seen stored in memory, weighted by the similarity of X to your stored.",
                    "label": 0
                },
                {
                    "sent": "Example XI and then again normalizing this by the sum of the similarity over all of the stored exemplars.",
                    "label": 0
                },
                {
                    "sent": "So what you might notice in looking at that equation is that it looks kind of familiar.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not a coincidence, so that kind of computation that you're doing an example model is equivalent to the kind of computation that we're doing when we are approximating the expectation over posterior distribution using important sampling using likelihood waiting.",
                    "label": 0
                },
                {
                    "sent": "So here we have the sum over all of our example are stored in memory.",
                    "label": 0
                },
                {
                    "sent": "This function that we're interested in, weighted by the similarity of the stimulus to the thing that stored in memory and the normalized, whereas in order to approximate our Bayesian expectation we take this function over all of those hypothesis that we have in memory.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the hypothesis that we sampled from the prior then wait those by the likelihood, normalizing by the sum over all of those have others that we sampled from the prior.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that if you go around the world kind of encountering things which you can store in memory, that kind of gives you a prior distribution which you can use for making certain kinds of Bayesian inferences.",
                    "label": 0
                },
                {
                    "sent": "For example, if what you want to do is something which relates to, say, identifying a sound that you hear, or denoising that sound, so you assume that you hear that sound in a noisy way, and you want to try and figure out what it was.",
                    "label": 0
                },
                {
                    "sent": "Then what you could do is kind of have stored examples of previous sounds that you've heard in memory.",
                    "label": 0
                },
                {
                    "sent": "Those would constitute your hypothesis as to the identity of the sound that you heard and then activate those sounds based on the similarity to the current sound that you hear.",
                    "label": 0
                },
                {
                    "sent": "As long as you're activating those sounds in a way which corresponds to a reasonable likelihood function like a Gaussian function or something like that, then this scheme of storing things in memory and then activating them based on the new stimuli that you encounter is going to implement this kind of Bayesian inference, and so this gives us a simple scheme for doing Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "You just sort of walk around.",
                    "label": 0
                },
                {
                    "sent": "Every time you encounter something that could be the solution to one of the inductor problems that you care about, you start in memory and then every time you want to make a difference and you encounter some data, you activate those hypothesis that you have stored in memory that are consistent with the data that you saw based on the likelihood.",
                    "label": 0
                },
                {
                    "sent": "And then you normalize over that sample set and basically what you're going to be doing is important.",
                    "label": 0
                },
                {
                    "sent": "Sampling using samples from the prior distribution, which is the set of stimulus set of hypothesis that you encountered in your environment.",
                    "label": 0
                },
                {
                    "sent": "So everybody sort of makes sense.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so I'll show.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do a simple example.",
                    "label": 0
                },
                {
                    "sent": "This is the case that Josh talked about on the first day this problem of predicting the future.",
                    "label": 1
                },
                {
                    "sent": "So if I tell you how much money are movies made so far, predicting how much money is going to make if I tell you how long somebody's lived predicting something about their lifespan, we can think about analyzing these problems as estimating some quantity total, the total duration or extent of a phenomenon having observed T, the current duration or extent of that phenomenon, we have to define a likelihood.",
                    "label": 0
                },
                {
                    "sent": "The likelihood in this case is zero.",
                    "label": 0
                },
                {
                    "sent": "40 total less than TL.",
                    "label": 0
                },
                {
                    "sent": "Swan over T turtle.",
                    "label": 0
                },
                {
                    "sent": "Assuming that you encounter that phenomenon at a random point in its duration so that you hear about that movie at a point that's uniformly distributed in terms of the between zero and the total amount of money that it makes, or encountering that person at a point that's uniformly distributed between when they're born and when they end up dying.",
                    "label": 0
                },
                {
                    "sent": "Then we can use an example and model to approximate these kinds of computations.",
                    "label": 0
                },
                {
                    "sent": "Calculating the posterior distribution over teetotal.",
                    "label": 0
                },
                {
                    "sent": "Given T, we can look at different kinds of constraints.",
                    "label": 1
                },
                {
                    "sent": "You can constrain the kinds of examples that people are generating.",
                    "label": 0
                },
                {
                    "sent": "You can have a memory limited case where there's a limit on the total number of examples that people could generate, or a computation limited case where you assume that the bottleneck in computation is Justin paying attention to those generated.",
                    "label": 0
                },
                {
                    "sent": "Those those examples that you generate, which are greater than.",
                    "label": 0
                },
                {
                    "sent": "This should be T here.",
                    "label": 0
                },
                {
                    "sent": "The example that you saw.",
                    "label": 1
                },
                {
                    "sent": "So the idea here is that here we are restricting the total number of samples from the prior that are used in this computation.",
                    "label": 0
                },
                {
                    "sent": "Here we're restricting the total number of prize that make a contribution.",
                    "label": 0
                },
                {
                    "sent": "As a consequence, having a nonzero likelihood when you're evaluating one of these prediction problems, but the basic.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Result is that here are these different distributions that we ask people about.",
                    "label": 0
                },
                {
                    "sent": "Life spans.",
                    "label": 0
                },
                {
                    "sent": "The grosses of movies.",
                    "label": 0
                },
                {
                    "sent": "The reins of Pharaohs, the length of poems, the time in the House of Representatives, the run times in movies and then down here the black line is the optimal Bayesian prediction and the black circles, which are a little hard to see, are the human judgments.",
                    "label": 0
                },
                {
                    "sent": "And each of the unfilled markers corresponds to one of these important sampling approximations.",
                    "label": 0
                },
                {
                    "sent": "But here, using only five samples from that distribution which is shown above.",
                    "label": 0
                },
                {
                    "sent": "So basically, if you look at these important sampling approximations, it doesn't matter whether we're using pure sampling from the well.",
                    "label": 0
                },
                {
                    "sent": "So the first of these are the computational case or the memory limited case, and then we also show for comparison just pure sampling from the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "These two important sampling cases computation limited on the memory limited case.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really matter which one of those we use, we actually end up with something which is a pretty good approximation to the true Bayesian answer in a pretty good approximation to the answers that people give.",
                    "label": 0
                },
                {
                    "sent": "It's not fantastic, but it shows that this kind of Bayesian inference is something that you could do with a relatively small number of samples from the prior distribution.",
                    "label": 0
                },
                {
                    "sent": "So if you think about you know how people make these prediction problems.",
                    "label": 0
                },
                {
                    "sent": "One simple answer is that they could go around and make some observations of the world around them and store those examples in memory, and then when you ask him to make a prediction, how long is somebody going to live or how much money is this movie going to make?",
                    "label": 0
                },
                {
                    "sent": "Just call up a few of those examples based on similarity to the current situation and as a consequence end up making pretty accurate predictions that conform to the appropriate prior distributions in these domains so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The basic idea about important sampling, then, is it's a general scheme that we can use for sampling from complex distributions that have similar relatives.",
                    "label": 1
                },
                {
                    "sent": "Things like posterior distribution when we might know how to generate from the prior or for some other more complex distribution when there's another similar distribution we can think of constructing to approximate, it gives us a way of sampling from posterior distributions in cases where you have the prior and posterior close together, something that can be more efficient and simple Monte Carlo and it has nice links to example models in psychology, but perhaps one of the.",
                    "label": 1
                },
                {
                    "sent": "The most interesting aspects of important sampling is that it also gives us a way of thinking about how people could go about updating their beliefs as data come in.",
                    "label": 0
                },
                {
                    "sent": "So I've so far been talking about a kind of static inference problem where you want to make an inference about some hypothesis.",
                    "label": 0
                },
                {
                    "sent": "You observe some data, and once you're done, then that's it, and you go on, you get on with the rest of your life.",
                    "label": 0
                },
                {
                    "sent": "But really, the idea that people might be doing something like Bayesian inference means that you have to have an explanation not just for how they update their beliefs right now, but how they update their beliefs every time they get new data.",
                    "label": 0
                },
                {
                    "sent": "If your Bayesian, you should be going around the world and every time you get a piece of information you should be calculating your posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "And if you're doing that, that imposes you know additional computational.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Challenges, so if you have to compute your posterior distribution over hypotheses every time you observe a new piece of data, then that means that all of the costs of computation are going to be compounded like if it takes you some amount of time to calculate, update, update your posterior distribution, then that's just going to be multiplied by the amount of data that you see, so it would be nice if we had a scheme for updating our posterior distribution that allowed us to exploit the same kind of thing that we see in important sampling where you can have rather than the whole set of hypothesis you need to evaluate.",
                    "label": 0
                },
                {
                    "sent": "Some subset of those and have as a consequence a much smaller hypothesis space that you need to work with, so we can actually do this by exploiting a property of Bayesian inference that you can kind of call yesterday's posterior is today's prior.",
                    "label": 1
                },
                {
                    "sent": "It's the idea that if what you wanted to do is compute your posterior distribution of hypothesis, given an observations, and you already had your posterior distribution overseas, given the first N -- 1 observations in this case, I'm assuming that observations are independent condition hypothesis.",
                    "label": 0
                },
                {
                    "sent": "But you can write down an equivalent formula if you assume that those dependencies between these data points.",
                    "label": 0
                },
                {
                    "sent": "But the basic idea is that what you can do is take your posterior distribution after N -- 1 data points and then multiply it by the likelihood that's associated with that end of data point.",
                    "label": 0
                },
                {
                    "sent": "And when you normalize, that's equivalent to doing Bayesian inference in the way that we've been talking about it, where you just start out with your prior and then you observe all your data and then you apply Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "You get exactly the same answer, because this is just proportional to this quantity and you know this is proportional to the prior times likelihood and so on.",
                    "label": 0
                },
                {
                    "sent": "So everything works out.",
                    "label": 0
                },
                {
                    "sent": "So this means that this is.",
                    "label": 0
                },
                {
                    "sent": "This expression, yesterday's posteriors.",
                    "label": 0
                },
                {
                    "sent": "Today's prior says that you can use the posterior distribution that you've established from the other data that you've observed as a prior distribution for interpreting the next observation that you make, and so that suggests a scheme that we could use for efficiently approximating Bayesian inference and being able to go around and update our beliefs in a way which allows us to attractively make sense of the world around us.",
                    "label": 1
                },
                {
                    "sent": "And we can do that by repeatedly using important sampling.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that we can use yesterday's posterior.",
                    "label": 0
                },
                {
                    "sent": "Which is going to be today's prior as a distribution from which we generate samples and then having generated those samples we re weight them by our likelihood and that gives us an approximation to today's posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "Well, you can do that recursively, right?",
                    "label": 0
                },
                {
                    "sent": "So if the way that I'm representing yesterday's posterior distribution is in terms of a set of samples, well, then I could have used the posterior distribution I had before that and then related those samples and so on and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So the idea is if you can approximate one posterior distribution in terms of.",
                    "label": 0
                },
                {
                    "sent": "The previous posterior distribution and a new likelihood.",
                    "label": 0
                },
                {
                    "sent": "Then you can approximate the previous posterior distribution in terms of the posterior distribution before that and the likelihood and then do that and do that and do that.",
                    "label": 0
                },
                {
                    "sent": "And at some point you just get all the way back to the prior distribution and you've got the very simple important sample that we started out with.",
                    "label": 0
                },
                {
                    "sent": "So this scheme for updating our beliefs or something that's known as a particle filter.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just schematically the way that it works.",
                    "label": 0
                },
                {
                    "sent": "The idea is that what we start out with is a set of samples from the posterior distribution over hypothesis given N -- 1 observations.",
                    "label": 1
                },
                {
                    "sent": "We then re wait those other sees by the likelihood and then we sample from that set of weighted hypothesis and we end up with some samples which are going to sample from our posterior distribution given an observations.",
                    "label": 0
                },
                {
                    "sent": "And so this is just using that important sampling scheme where we take the distribution which we have.",
                    "label": 1
                },
                {
                    "sent": "Based on the previous N -- 1 observations as our proposal distribution, in order to end up with some samples from our target distribution an obviously you can apply this recursively.",
                    "label": 0
                },
                {
                    "sent": "So the way that you could obtain your samples given N -- 1 is by doing the same thing where.",
                    "label": 0
                },
                {
                    "sent": "Here's an minus one, and then his N -- 2 and so on and so on and so on until you go all the way back and you just start out with some samples from the prior distribution.",
                    "label": 0
                },
                {
                    "sent": "So you might notice that this particular algorithm doesn't seem very good, so there's one sort of obvious problem that you're going to have a few.",
                    "label": 0
                },
                {
                    "sent": "Use this algorithm.",
                    "label": 0
                },
                {
                    "sent": "Can anyone think about what the problem might be?",
                    "label": 0
                },
                {
                    "sent": "I've kind of made it hard to see here by making these two dots look like they correspond to different things.",
                    "label": 0
                },
                {
                    "sent": "So one way to think about it is how many distinct hypotheses do we have over here versus how many distinct hypotheses we have over here, right?",
                    "label": 0
                },
                {
                    "sent": "So over here we have 4 over here we have three distinct hypotheses.",
                    "label": 0
                },
                {
                    "sent": "We have two samples of this one, So what happens overtime as you're just going to end up losing hypothesis because the only thing that can happen here is we generate.",
                    "label": 0
                },
                {
                    "sent": "You know, we have some hypothesis, we relate them we sample.",
                    "label": 0
                },
                {
                    "sent": "Every time you re sample, you're going to be losing diversity in that sample, so actually using this algorithm to as an effective way of maintaining a posterior distribution over many successive data points, you need to also introduce the stage where you kind of move these guys around a little bit, or have some other way of introducing variation into the set of particles that you're using, and so there's a variety of schemes for doing that, but provided you do that in an appropriate way, then you're going to end up with something which is basically a dynamic approximation to the posterior distribution, relying only on a relatively small number of samples at each time.",
                    "label": 0
                },
                {
                    "sent": "So the particle filter is kind of interesting because it gives you a way of doing Bayesian inference with limited memory load, right?",
                    "label": 1
                },
                {
                    "sent": "There's only a small number of hypothesis you're maintaining at anytime you're updating those to reflect the new beliefs that you have, and as a consequence you can keep track of the posterior distribution just using a set of samples.",
                    "label": 0
                },
                {
                    "sent": "And so it makes it much more tractable to work with some complicated probability models, and it gives us a hypothesis about how people might be able to sort of maintain the probability distributions over hypothesis as they get more data.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This kind of scheme also works with dynamic hypothesis.",
                    "label": 0
                },
                {
                    "sent": "So if you had a situation where there was not just data being generated from a single hypothesis, but data being generated from one hypothesis on day one, and then that influencing some hypothesis you have on day two and so on and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So this is a simple hidden Markov model you can think about.",
                    "label": 0
                },
                {
                    "sent": "This is something like, say the weather in Sardinia, right?",
                    "label": 0
                },
                {
                    "sent": "So you have some observation of the weather looking out of the window.",
                    "label": 0
                },
                {
                    "sent": "There's some underlying process which is generating that the weather today is going to influence the weather.",
                    "label": 0
                },
                {
                    "sent": "Tomorrow you get some observation of the weather tomorrow.",
                    "label": 0
                },
                {
                    "sent": "The weather tomorrow is going to influence the weather.",
                    "label": 0
                },
                {
                    "sent": "The day after you get some observation of that.",
                    "label": 0
                },
                {
                    "sent": "So if you want to be maintaining a posterior distribution over what these underlying states are, you need to take into account both the previous state and the data that you've acquired, so the so if you're trying to pick the weather, you know two days from now.",
                    "label": 0
                },
                {
                    "sent": "You want to be able to take your the information that you've got from all of the data points that you've seen so far, as well as a data point that you observe on that day and then compute your posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "Given all of those data points for that particular hypothesis, and so you can do this, provided you have the distribution over that hypothesis given the previous data points and the likelihood of that observation, and when you one way you can calculate this probability distribution over the hypothesis given the previous data points is.",
                    "label": 0
                },
                {
                    "sent": "Using the dynamics of the model.",
                    "label": 0
                },
                {
                    "sent": "So if you have a distribution over the weather on that day and you have a way of predicting how the weather on the next day depends on the weather on the previous day, then you can take your posterior distribution over the weather that day and then multiply it by you know the distribution which tells you about the dynamics and then some out that variable which tells you about the weather on the previous day.",
                    "label": 0
                },
                {
                    "sent": "So this is just a standard set of marginalization that you have seen in lots of the models that we're using.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "The idea is that.",
                    "label": 0
                },
                {
                    "sent": "This is just like the previous example, all that we've done is introduced.",
                    "label": 0
                },
                {
                    "sent": "This extra bit of dynamics, but if you have a set of samples which are samples from the posterior distribution over hypotheses given days one through 3, then you can compute.",
                    "label": 0
                },
                {
                    "sent": "You can end up with a set of samples from the posterior distribution overseas for days, one through 4 by taking those samples, modifying them in a way which reflects the dynamics of the underlying process, and then we re waiting by the likelihood to integrate the information that you get from observing the world on there.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Today and so if you do that, you end up with a particle filtering scheme where we take our samples from the posterior given days, one through three.",
                    "label": 1
                },
                {
                    "sent": "We then sample from the distribution which reflects the dynamics.",
                    "label": 1
                },
                {
                    "sent": "So for each of these things, we're going to move it around, reflecting the probability distribution over the weather on day four given the weather on day three.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the probability distribution over the weather on day four based on the weather on day three, so this is kind of using our expectations about how that probabilistic process works, and then we wait those by the likelihood and then we re sample.",
                    "label": 0
                },
                {
                    "sent": "And in this case you don't run into that same kind of problem about losing diversity.",
                    "label": 0
                },
                {
                    "sent": "There are some intrinsic dynamics in the process which allow you to appropriately update your beliefs.",
                    "label": 0
                },
                {
                    "sent": "But I'm in a way where you're not guaranteed to be ending up with something which is different on every something, which is the same on every successive iteration, because there's this process where you're moving things around already, you can end up with hypotheses moving around the space in a much more structured way, and you can also end up with situations where you're generating multiple samples based on a single previous sample.",
                    "label": 0
                },
                {
                    "sent": "So like this, but where those are now as a consequence of the intrinsic dynamics of the process going to end up looking like different predictions about the weather that will happen on the next day.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the basic idea about particle filters is something which is kind of exciting from the perspective of thinking about how people might do something like probabilistic inference, it gives us a kind of general scheme that we could use for defining these rational process models for updating probability distributions overtime.",
                    "label": 1
                },
                {
                    "sent": "So the idea is that you know if we want to understand how people do complex things like, you know, form clusters of the observations in the world around them, or learn things about language or any other kinds of things we've been analyzing is Bayesian inference that involves a problem of calculating a posterior distribution given a succession.",
                    "label": 0
                },
                {
                    "sent": "Succession of observations and we can solve that problem by using something like a particle filter, provided we have enough particles.",
                    "label": 0
                },
                {
                    "sent": "Well, this becomes an interesting kind of psychological hypothesis.",
                    "label": 0
                },
                {
                    "sent": "If we think about what happens to particle filters as they get fewer particles than they actually need to give us a good approximation to the posterior distribution.",
                    "label": 1
                },
                {
                    "sent": "So one interesting case which has been used to challenge Bayesian models of cognition is the fact that you see what are called order effects in human learning.",
                    "label": 0
                },
                {
                    "sent": "So people learn different things when you give them the data that they see in different orders.",
                    "label": 0
                },
                {
                    "sent": "So you might learn.",
                    "label": 0
                },
                {
                    "sent": "One thing, if you see the data in one order and a different thing if you see the data in another order and this is something that kind of on the surface is inconsistent with the predictions that Bayesian models make.",
                    "label": 0
                },
                {
                    "sent": "So if you have a model which assumes that the data you're seeing are generated from a process that stationary or generated in a way that's independent condition on the hypothesis, then those models are not going to predict order effects.",
                    "label": 0
                },
                {
                    "sent": "They're going to say it doesn't matter what order you see the data in, you're going to end up making the same kind of inference right, as long as the data independent, it doesn't matter what order you see them in.",
                    "label": 0
                },
                {
                    "sent": "All that matters is you know what the statistics of those data are.",
                    "label": 0
                },
                {
                    "sent": "So the fact that you see order effects in human learning has been used to argue that Bayesian models are somehow wrong in the way that they're describing human learning.",
                    "label": 0
                },
                {
                    "sent": "Well, if you use a particle filter and use it with relatively small number of particles, you actually see order effects, which can be consistent with the order effects you see in human learning.",
                    "label": 0
                },
                {
                    "sent": "So what happens if you have a particle filter with a relatively small number of particles is you can kind of get committed to some hypothesis early on and then find it very hard to get away from those hypothesis, and that's something that we often see in human learning too.",
                    "label": 0
                },
                {
                    "sent": "It's something called primacy effect, where you're more influenced by the first bit of data that you see.",
                    "label": 0
                },
                {
                    "sent": "Then you are by subsequent data and so using this kind of approximation can give us a way of understanding what sort of factors might lead to those sorts of errors, and perhaps a way to bridge the gap between these high level computational level analysis.",
                    "label": 1
                },
                {
                    "sent": "An understanding the kinds of algorithms that people might be using for performing probabilistic inference.",
                    "label": 0
                },
                {
                    "sent": "And so we've used this kind of approach to make models of categorization using that during the process mixture model I was talking about yesterday.",
                    "label": 0
                },
                {
                    "sent": "This is actually a way to dynamically assign.",
                    "label": 0
                },
                {
                    "sent": "Objects to clusters and update those beliefs overtime.",
                    "label": 0
                },
                {
                    "sent": "Other people have used them for modeling things like associative learning or change point detection, where you're trying to figure out when there's a change in the distribution from which you're seeing examples being generated, and one particular interesting case that we've looked at is sentence processing.",
                    "label": 0
                },
                {
                    "sent": "So understanding a sentence is something that's one of these.",
                    "label": 0
                },
                {
                    "sent": "Probabilistic inferences that requires you to update your beliefs as you're getting more data overtime.",
                    "label": 0
                },
                {
                    "sent": "So every word that I produce is a clue as to the syntactic structure of the sentence that I'm articulating.",
                    "label": 0
                },
                {
                    "sent": "And so in order to understand me, you need to be doing something like updating your beliefs about what sentence I'm saying.",
                    "label": 0
                },
                {
                    "sent": "As, as I say, each of those words so a particle filter is a good way to think about solving that kind of problem.",
                    "label": 0
                },
                {
                    "sent": "You can be getting, you know each piece of information and updating your beliefs about the appropriate syntactic structures and what we've shown is that you can actually use a particle filter to explain some effects which.",
                    "label": 0
                },
                {
                    "sent": "Which are about people being misled as to the interpretation of a sentence.",
                    "label": 0
                },
                {
                    "sent": "What are called garden path effects where you can kind of lead people to believe that a sentence is going one way and then it can end up going in a completely different direction.",
                    "label": 0
                },
                {
                    "sent": "So our classic example of a garden path sentence is the horse raced past the barn fell.",
                    "label": 0
                },
                {
                    "sent": "Right, everybody understood that sentence.",
                    "label": 0
                },
                {
                    "sent": "Yeah, was it hard?",
                    "label": 0
                },
                {
                    "sent": "So, so those are the kinds of things where you know the error that you make is kind of committing to a particular kind of syntactic structure where you think that the horses running past the barn, right?",
                    "label": 0
                },
                {
                    "sent": "And the you know you're sort of setting up a particular set of interpretation of that sentence.",
                    "label": 0
                },
                {
                    "sent": "And what you end up with is getting this extra piece of information at the end that tells you that that interpretation is wrong and you can explain some of those surprising effects in terms of what happens when you take a particle filter and get committed to a particular semantic interpretation early with a relatively small set of hypothesis about, you know what the underlying syntactic structure might be OK, so I talked about 2 uses of Monte Carlo methods.",
                    "label": 0
                },
                {
                    "sent": "Let's take our two minute break.",
                    "label": 0
                }
            ]
        }
    }
}