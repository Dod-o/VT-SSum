{
    "id": "yydv24koensvxyhotfypqosqm4civbmv",
    "title": "Graphical Models",
    "info": {
        "author": [
            "Zoubin Ghahramani, Department of Engineering, University of Cambridge"
        ],
        "published": "Jan. 15, 2013",
        "recorded": "April 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/mlss2012_ghahramani_graphical_models/",
    "segmentation": [
        [
            "What I'd like to do in the next visit, our in 15 minutes is to give you a very quick overview of graphical model background.",
            "Parameter learning and structure learning inference, parameter learning and structure learning.",
            "So for this it's useful for me to ask you how many of you have studied graphical models outside of this summer school in the past, OK?",
            "Almost everybody.",
            "How many of you have not studied.",
            "Traffic allows a few people.",
            "OK, that's good.",
            "How many of you know?",
            "Message passing in graphical models.",
            "Slightly more advanced topic OK and how?",
            "If you don't know it?",
            "OK, good.",
            "Alright, so I think this will be useful given based on that polling result, OK?",
            "So graphical MoD."
        ],
        [
            "Rules are a way of representing knowledge about complex domains.",
            "The basic idea is that if you have a complex domain in terms of some probabilistic model.",
            "Then graphical model is a way of representing dependencies between the variables in your model.",
            "The nodes in a graphical model correspond to random variables ABCD in this case, and the edges represent statistical dependencies between your variables."
        ],
        [
            "Well, why do we need graphical models where graphical models so prominent in the machine learning Community these days?",
            "Well, first of all, they're very intuitive.",
            "Graphs are intuitive way of representing and visualizing the relationship between a bunch of variables.",
            "And of course, a lot of other fields use diagrams or graphs to represent relationships between things.",
            "From a statistical point of view, graphs are useful in abstracting out the conditional independence relationships between variables.",
            "From the details of their parametric form.",
            "So if I have a graph like the one."
        ],
        [
            "On this slide here."
        ],
        [
            "And I can ask the question, is a dependent on B given that we know the value of C just by looking at the graph, I don't have to say."
        ],
        [
            "I don't have to write down all the equations or do anything like make an assumption about whether this is Gaussian or Poisson or Dirichlet or anything like that.",
            "I can make a general statement about what depends on what and that is hugely useful from an abstraction point of view."
        ],
        [
            "And that aspect of graphical models.",
            "Becomes computationally very useful, so graphical models because of this conditional independence allow us to define message passing algorithms that implement probabilistic inference efficiently.",
            "So for example, we can ask we can answer queries like what is the probability of the variable a given that we know the variable C takes on some value.",
            "By passing messages on this graph without having to enumerate all the variables and integrate everything out OK.",
            "So graphical models are really a very nice marriage of statistics.",
            "Graph theory in computer science, and this last aspect of it.",
            "The message passing algorithm aspect of it is what's really made a difference in terms of scaling up probabilistic modeling to very large systems."
        ],
        [
            "So I'm going to focus on.",
            "I'm going to talk about directed graphs 1st and then a little bit about factor graphs and undirected graphs.",
            "Now directed graphs, sometimes called Bayesian networks, but they don't necessarily as a footnote here, in that, for example, Bayesian networks, they don't have to be learned using Bayesian methods, it's just.",
            "You know a naming convention that I don't particularly like.",
            "There's sometimes also called belief networks.",
            "But let's call them directed acyclic graphical models.",
            "This may be less contentious.",
            "Um?",
            "These directed graphs correspond to a particular factorization of the joint probability distribution over the variables.",
            "So for example, this graph here corresponds to the factorization of this joint distribution into the probability of a times the probability of B * C given A&B etc.",
            "Where in general what we have is that the joint distribution is a product of the probabilities of each variable given its parents in the graph.",
            "So the parents of CRA&B.",
            "That's why this term appears here, and that's the general form of the factory factorization for graphical model for a directed graphical model.",
            "OK. And this factorization actually captures the conditional independence relationships between the variables."
        ],
        [
            "OK, and So what do we mean by that?",
            "Well, given a graph, we can now say exactly what that graph says about the probability distributions that are consistent with that graph.",
            "What it says for any particular graph is statements of the Form X is conditionally independent from Y given V. X&Y are some nodes in the graph and V is some set of nodes and that occurs when the set of nodes V separates X from Y in the graph, so D separation in stands for dependency separation.",
            "In a directed graph, but I won't go over the definition of it just because it's a little tedious, but you could look.",
            "You could look through it and try to understand it.",
            "The idea behind this D separation is that we're trying to capture kind of intuitively.",
            "Weather all information.",
            "Uh.",
            "From X to Y and vice versa has to pass through the nodes V if everything between X&Y has to pass through V. Then once we know the X&Y become independent of each other, OK.",
            "I mean, you could think of a lot of analogies like you know the flow of information or the flow of electricity or whatever.",
            "You know.",
            "Just think of it that way and then the actual semantics are a little annoying.",
            "But you know that you can prove that these are things that follow from where they follow from.",
            "Guess.",
            "The sum rule in the product rule, yes.",
            "They follow from the same rule in product rule.",
            "If they didn't and they follow from."
        ],
        [
            "That's the only thing they follow from the sum rule, the product rule and this statement OK."
        ],
        [
            "So it could take, you know, 10 pages of you dear pearls classic book to prove this stuff.",
            "But all he's doing is showing you the sum rule in the product rule and the factorization OK?",
            "Now the corollary for this is that for any variable X we can define something called the Markov boundary, and that insulates X.",
            "From all the other variables, and that's the set of parents of X children of X and the parents of the other parents of the children of X.",
            "Given that set of variables X is independent of everything else.",
            "OK."
        ],
        [
            "So graphical models are kind of used in two very different ways and I just want to give you a flavor for the two ways."
        ],
        [
            "One of 'em is when we write down a graph like this and we think of the particular variables as being something interpretable like.",
            "The temperature at this temperature sensor in the pressure, this valve, etc.",
            "And this is a graphical model that relates these things to each other OK.",
            "So this is like as a way of doing knowledge representation in a particular domain."
        ],
        [
            "Another place where graphical models appear, sorry is just in descriptions of statistical models like this.",
            "So for example, consider the following simple model.",
            "I have a set of N data points generated ID from a Gaussian with mean mu and standard deviation Sigma.",
            "Now if I want to build a model of this, what that corresponds to is the idea that the joint distribution of X1 through XN, mu and Sigma factors in a particular way.",
            "This is the IID assumption.",
            "I said each of these depends on mu and Sigma right?",
            "And then I might have a prior.",
            "I haven't said that in the sentence here, but maybe my prior on mu and Sigma factors in this way.",
            "Let's say that's a particular.",
            "Model it corresponds to a factorization of this joint distribution.",
            "We can rewrite that as a graph.",
            "That graph looks like this.",
            "Mu and Sigma are the parents of X1 through XN and now because often in graphs like this we have repetitions of variables.",
            "So you know we have these X one through XN with this... notation.",
            "More concise notation is this thing called a plate, which just puts a.",
            "In this case a dashed box around X with.",
            "AA.",
            "Label here that indicates the number of times this variable is repeated in the graph, so XN is repeated big end times OK, this is just shorthand for this graph, so you'll see this sort of notation in a lot of machine learning papers OK. Alright."
        ],
        [
            "So now let's talk about inference in graphical models.",
            "Considering consider the following graph.",
            "What is inference inference corresponds to evaluating the probability distribution over some set of variables given the values of another set of variables.",
            "For example, what is the probability of a given that we know the value of C?",
            "Alright, so for example, assume that each variable is binary.",
            "How do we compute this?",
            "Well, here is a naive method.",
            "I write down the joint of A&C is the sum over BD&E of the joint of ABCD&E.",
            "There are 16 terms in this sum.",
            "Given the graph and maybe the probability distributions on each of the edges in the graph, I can actually evaluate this for all 16 of these terms.",
            "And then I can sum over a.",
            "To evaluate these two terms and then I can divide these by these, which give me these two terms which gives me this probability distribution probability of a given C. OK, this is a naive procedure because I went through writing down the whole joint distribution and evaluating all the configurations.",
            "And it took me 20 terms to compute this.",
            "Clearly if I had 100 binary variables, this would be a really bad idea, right?",
            "But we know something about this joint distribution.",
            "We know that it factors in a particular way, so let's rearrange the computations while keeping them exact."
        ],
        [
            "At.",
            "So I take this same graph and now this is just anecdotal for this particular factorization, I plug it in here.",
            "This sum is still expensive to compute.",
            "But I plug it in here and then I notice something about this.",
            "I notice that E. Only appears here so I can bring this some over E all the way in here.",
            "And I can sum over E exactly because I know that the sum over E is 1 when I have the conditional distribution of E over the two values of Y.",
            "So this thing sums to one.",
            "Similarly, I can take the sum over D and bring it in and this thing sums to one exactly.",
            "So in fact this whole thing simplifies the sum over B of P of a times P of B times Pfc, given A&B.",
            "And that ends up being about 8 terms depending on how you compute it.",
            "The first operation is 4 terms.",
            "The other two operations are two terms each.",
            "So what does that correspond to in the graph that corresponds to the fact that I wanted to know a given C?",
            "OK, it turns out if I know a given its parents, it doesn't have parents, is children and the parents of his children, that would be B, then everything else is independent, right?",
            "So in fact, if I look at the Markov blanket of a, the only thing that depends on?",
            "Is B&C?",
            "So that's all I needed to have in this.",
            "I needed to have a B&C in these expressions, the whole rest of the graph, even if it has 98 other variables in it, just drops out.",
            "But of course we don't want to do these things by hand for every graph.",
            "What we really want is general purpose algorithms, where you write down the graph in some way on the computer, and the computer automatically figures out how to apply the sum and product rule efficiently with the conditional independence relationships that are encoded in your graph.",
            "And clearly by doing this on sparse graphs, we can get exponential gains over the naive method."
        ],
        [
            "OK, so how do we actually do this?",
            "Let's focus on particular example of factor graph propagation.",
            "This is sort of.",
            "An implementation of belief propagation that's particularly nice and simple to write down.",
            "The idea of this is you take your joint distribution, which is in this particular example.",
            "Let's say it looks like this.",
            "And you instead of writing it as a directed graph, we're going to write it as a factor graph.",
            "This is just another graphical formalism which with similar properties I can talk about the differences of somebody's interested later.",
            "The way we write this is rewrite it in terms of a product of factors.",
            "So for each one of these terms in here, we write down a factor which is a function of the variables involved in these terms.",
            "The first term is just a function of X1, the second term is a function of X2 and X1, so we can actually group those together into this factor F1, which is a function of X1 and X2, and we write the factor graph like this.",
            "The nodes represent the variables and.",
            "These filled dots represent the factors.",
            "So F1 is a factor relating X1 and X2.",
            "Similarly for F2 and F3 here.",
            "And we're going to focus on what are called singly connected factor graphs.",
            "The singly connected factor graph corresponds to one in which the underlying structure of the graph is a tree.",
            "OK.",
            "In other words, there is between any pair of nodes there is at most one path.",
            "OK, so this is an example of a multiply connected factor graph or not, simply connected factor graph and I can tell you.",
            "Later, if you're interested how to do inference in these graphs?",
            "Any questions about this?"
        ],
        [
            "OK, so that's just the now the general description of what a factor graph is.",
            "The joint probability is written as a product of factors, so the joint probability here is one over some normalizing constant of the product.",
            "Over these J factors, where each factor depends on sub some subset of the vector of all the variables.",
            "And.",
            "We have these two kinds of nodes that variable nodes in the factor nodes, which I described in the previous slide.",
            "These factors are non negative functions of their arguments.",
            "OK, if the arguments are discrete, for example if they're binary, then we can think of the factors as tables with non negative entries in them, OK?",
            "So that's a factor graph and now."
        ],
        [
            "Now we're going to think about how to implement inference in a factor graph, how to implement the sum and product rule.",
            "If you have a factor graph for your data for your model.",
            "OK, so here's some notation.",
            "Let N of X denote the set of factor nodes that are connected to variable X.",
            "So."
        ],
        [
            "So for example, variable four is connected to factors three and four, so N A4 is 3, four in this case."
        ],
        [
            "And similarly, let's say N of F denotes the set of variable nodes that are connected to factor F, so it's a bit of overload of this neighborhood notation OK?",
            "So we can compute the probabilities by propagating messages from variable notes of factor nodes and vice versa, and is basically these two equations OK, we have in general messages from variables to factors.",
            "So here is the message mu from variable X to factor.",
            "FA message is going to be an unnormalized probability.",
            "Distribution, so if X is mean, it could be normalized.",
            "If you want to that, then you can just keep track of the normalization constants.",
            "But a message here in this case is a function of X.",
            "So if X is binary, that message is a 2 by 1 vector OK. And the message from variable X to factor F is the product over all the factors.",
            "H that are connected to X, not including the factor F. Of the messages from Factor H to variable X.",
            "So if we want to visualize this."
        ],
        [
            "On the graph."
        ],
        [
            "We had a message from X to F. So let's look at this graph here."
        ],
        [
            "I.",
            "Let's say the message from X2 to F3 is the product of the messages from F1 to X2 and the message from F2 techs two.",
            "OK, we'd taken elementwise product of these vectors coming in.",
            "And we re scale or not?",
            "If we want to and we pass them on in the other direction.",
            "Super easy to implement.",
            "OK."
        ],
        [
            "The message from factor F to variable X.",
            "Is given by the following equation.",
            "The message from F to X is also a function of the variable.",
            "X is the sum over all variables, not including X.",
            "Of the factor F multiplied by the messages from all the other variables that factor F. OK, so again it's taking incoming messages.",
            "Multiplying them by its local factor, then summing out all variables other than the variable X that is sending the message to.",
            "I tried to explain the notation here."
        ],
        [
            "So.",
            "You know the message from factor F12X2 takes the message from X1 to F, multiplies it by the factor F1, which is a function of X1 and X2.",
            "It sums out X one and that becomes a function of X2 that it sends on 2X2.",
            "OK."
        ],
        [
            "That's it, these two equations.",
            "If you run them in a particular order.",
            "Until no other messages are left to be computed.",
            "If you're affected, graph is singly connected, what you get."
        ],
        [
            "This is just a repeat of these two equations.",
            "This is a statement of the order.",
            "If a variable has only one factor as a neighbor, it can initiate this message propagation.",
            "Then, once a variable is received, all the messages from its neighboring factors.",
            "Then we can compute the marginal probability of that variable by taking the product of all those messages.",
            "An re normalizing it.",
            "OK. Any questions about that?",
            "How many people have seen this before?",
            "Raise your hand if you've seen it before.",
            "OK, quite a lot of you.",
            "How many people had not seen this before?",
            "OK. Alright.",
            "So."
        ],
        [
            "So this is very nice as a very general algorithm.",
            "It turns out that it's a generalization of things that have been very well known for decades.",
            "For example, if I take a hidden Markov model.",
            "The directed graph for an HMM looks like this.",
            "These are observed sequence of data and this is a sequence of hidden variables that are meant to explain the observed data.",
            "In the hidden Markov model there is something called the forward backward algorithm.",
            "That's just factor graph propagation, and it shouldn't be surprising.",
            "It's not like wow, what a coincidence.",
            "These two things that were invented by completely different fields happen to be the same algorithm is not surprising at all, right?",
            "Because they're following the rules of probability using a graph that better be the same if they're not the same.",
            "One of 'em is wrong.",
            "Right now, Kalman smoothing in a state space model.",
            "That thing that I took two semesters of.",
            "It could have been explained as just factor graph propagation with Gaussian variables as well.",
            "OK. OK."
        ],
        [
            "Now there is a lot of nice software for graphical models.",
            "I've just listed a few of them here with some comments about them.",
            "There is a great resource that maybe a few years old that Kevin Murphy generated, which lists a whole bunch of graphical model software toolkits.",
            "And you know of course, different software toolkits emphasize different kinds of models and things like that, OK?"
        ],
        [
            "So here's just the summary of the basics of graphical models.",
            "Inference is the problem of figuring out the probability of the variables of interest given the observed variables while summing out the other variables.",
            "You're not interested in.",
            "That's the basic thing we're trying to do.",
            "For singly connected graphs, we have these things called belief propagation and factor graph propagation.",
            "Kalman filtering and forward backward are special cases of that.",
            "Now what do we do for multiply connected graphs OK?",
            "Well, if we want to do exact inference in multiply connected graphs, there is a very nice algorithm called the Junction tree algorithm.",
            "It's been known since 1988 or so.",
            "And basically you can think of this as converting your multiply connected graph into a single connected graph, but over super nodes.",
            "That correspond to combinations of sets of variables.",
            "You do that conversion and then you run factor graph propagation on that converted graph.",
            "That's basically intuitively what it corresponds to.",
            "That conversion is not trivial.",
            "There might be many ways of converting it, but you know that's the basic idea.",
            "That can be very slow, because that conversion of your multiply connected graph into a single connected graph over super nodes.",
            "Is something that could be kind of expensive?",
            "It could end up with super nodes with exponentially many states in them.",
            "So in practice, what's actually done most often is to just run factor graph propagation.",
            "Acting like the graph is singly connected when it's not.",
            "Close your eyes and press the go button and cross your fingers and hope for the best.",
            "OK.",
            "There's a lot of analysis of that, and I could say more about that.",
            "Actually, this that's called loopy belief propagation.",
            "Yes.",
            "Weather proof sketch for the.",
            "Proof sketch for loopy belief propagation.",
            "OK, so I can.",
            "I can certainly give you a sketch of a sketch.",
            "Hyper sketch so.",
            "In general loopy belief propagation, which is just factor graph propagation or belief propagation on a graph that has loops like you know we saw before, sorry I don't know what happened here.",
            "OK.",
            "In general that."
        ],
        [
            "Yeah, this is a loopy graph.",
            "Very small, easy loopy graph, but in general that algorithm is not guaranteed to converge.",
            "OK, so you could run these messages and the beliefs would keep going up and down, up and down.",
            "It's possible for that to happen if it converges, it might not give the right answer.",
            "In some special cases it's been analyzed and shown to be able to converge.",
            "For example, in the Gaussian case it's been analyzed by people like, yeah, you're Weiss in graphs that have a single loop or a few very large loops where the influence becomes weaker and weaker than you can show convergence of certain kinds, and there is a very nice theory that's been developed around this, based on an idea called the Beta Free Energy.",
            "Where essentially what people have shown is that loopy belief propagation in graph, if it converges, it converges to.",
            "Fixed points of the beta free energy.",
            "The other thing that's interesting about loopy belief propagation is that this beta free energy interpretation means that we can now come up with convergent approximate inference algorithms for these kinds of graphs that are based around minimizing that energy directly, rather than applying loopy belief propagation and then the last interesting thing I want to say is that factor graph propagation or loopy belief propagation.",
            "In multiply connected graphs is also equivalent to a particular form of the expectation propagation EP algorithm.",
            "OK, so there are very beautiful links between all of these different methods that are worth knowing about.",
            "OK.",
            "So."
        ],
        [
            "So that's the summary of inference and then now for the next 45 minutes or so.",
            "Should we take a 5 minute break?",
            "OK. For the next 5 minutes, we're going to take a break and for the following 40 minutes, just so that you're encouraged to come back, we're going to talk about parameter and structure learning in graphs."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I'd like to do in the next visit, our in 15 minutes is to give you a very quick overview of graphical model background.",
                    "label": 0
                },
                {
                    "sent": "Parameter learning and structure learning inference, parameter learning and structure learning.",
                    "label": 0
                },
                {
                    "sent": "So for this it's useful for me to ask you how many of you have studied graphical models outside of this summer school in the past, OK?",
                    "label": 0
                },
                {
                    "sent": "Almost everybody.",
                    "label": 0
                },
                {
                    "sent": "How many of you have not studied.",
                    "label": 0
                },
                {
                    "sent": "Traffic allows a few people.",
                    "label": 0
                },
                {
                    "sent": "OK, that's good.",
                    "label": 0
                },
                {
                    "sent": "How many of you know?",
                    "label": 0
                },
                {
                    "sent": "Message passing in graphical models.",
                    "label": 1
                },
                {
                    "sent": "Slightly more advanced topic OK and how?",
                    "label": 0
                },
                {
                    "sent": "If you don't know it?",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I think this will be useful given based on that polling result, OK?",
                    "label": 0
                },
                {
                    "sent": "So graphical MoD.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rules are a way of representing knowledge about complex domains.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is that if you have a complex domain in terms of some probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "Then graphical model is a way of representing dependencies between the variables in your model.",
                    "label": 0
                },
                {
                    "sent": "The nodes in a graphical model correspond to random variables ABCD in this case, and the edges represent statistical dependencies between your variables.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, why do we need graphical models where graphical models so prominent in the machine learning Community these days?",
                    "label": 1
                },
                {
                    "sent": "Well, first of all, they're very intuitive.",
                    "label": 1
                },
                {
                    "sent": "Graphs are intuitive way of representing and visualizing the relationship between a bunch of variables.",
                    "label": 0
                },
                {
                    "sent": "And of course, a lot of other fields use diagrams or graphs to represent relationships between things.",
                    "label": 1
                },
                {
                    "sent": "From a statistical point of view, graphs are useful in abstracting out the conditional independence relationships between variables.",
                    "label": 0
                },
                {
                    "sent": "From the details of their parametric form.",
                    "label": 0
                },
                {
                    "sent": "So if I have a graph like the one.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On this slide here.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I can ask the question, is a dependent on B given that we know the value of C just by looking at the graph, I don't have to say.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I don't have to write down all the equations or do anything like make an assumption about whether this is Gaussian or Poisson or Dirichlet or anything like that.",
                    "label": 0
                },
                {
                    "sent": "I can make a general statement about what depends on what and that is hugely useful from an abstraction point of view.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that aspect of graphical models.",
                    "label": 0
                },
                {
                    "sent": "Becomes computationally very useful, so graphical models because of this conditional independence allow us to define message passing algorithms that implement probabilistic inference efficiently.",
                    "label": 1
                },
                {
                    "sent": "So for example, we can ask we can answer queries like what is the probability of the variable a given that we know the variable C takes on some value.",
                    "label": 1
                },
                {
                    "sent": "By passing messages on this graph without having to enumerate all the variables and integrate everything out OK.",
                    "label": 1
                },
                {
                    "sent": "So graphical models are really a very nice marriage of statistics.",
                    "label": 0
                },
                {
                    "sent": "Graph theory in computer science, and this last aspect of it.",
                    "label": 0
                },
                {
                    "sent": "The message passing algorithm aspect of it is what's really made a difference in terms of scaling up probabilistic modeling to very large systems.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to focus on.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about directed graphs 1st and then a little bit about factor graphs and undirected graphs.",
                    "label": 0
                },
                {
                    "sent": "Now directed graphs, sometimes called Bayesian networks, but they don't necessarily as a footnote here, in that, for example, Bayesian networks, they don't have to be learned using Bayesian methods, it's just.",
                    "label": 1
                },
                {
                    "sent": "You know a naming convention that I don't particularly like.",
                    "label": 0
                },
                {
                    "sent": "There's sometimes also called belief networks.",
                    "label": 1
                },
                {
                    "sent": "But let's call them directed acyclic graphical models.",
                    "label": 0
                },
                {
                    "sent": "This may be less contentious.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "These directed graphs correspond to a particular factorization of the joint probability distribution over the variables.",
                    "label": 1
                },
                {
                    "sent": "So for example, this graph here corresponds to the factorization of this joint distribution into the probability of a times the probability of B * C given A&B etc.",
                    "label": 0
                },
                {
                    "sent": "Where in general what we have is that the joint distribution is a product of the probabilities of each variable given its parents in the graph.",
                    "label": 0
                },
                {
                    "sent": "So the parents of CRA&B.",
                    "label": 0
                },
                {
                    "sent": "That's why this term appears here, and that's the general form of the factory factorization for graphical model for a directed graphical model.",
                    "label": 0
                },
                {
                    "sent": "OK. And this factorization actually captures the conditional independence relationships between the variables.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and So what do we mean by that?",
                    "label": 0
                },
                {
                    "sent": "Well, given a graph, we can now say exactly what that graph says about the probability distributions that are consistent with that graph.",
                    "label": 0
                },
                {
                    "sent": "What it says for any particular graph is statements of the Form X is conditionally independent from Y given V. X&Y are some nodes in the graph and V is some set of nodes and that occurs when the set of nodes V separates X from Y in the graph, so D separation in stands for dependency separation.",
                    "label": 0
                },
                {
                    "sent": "In a directed graph, but I won't go over the definition of it just because it's a little tedious, but you could look.",
                    "label": 0
                },
                {
                    "sent": "You could look through it and try to understand it.",
                    "label": 0
                },
                {
                    "sent": "The idea behind this D separation is that we're trying to capture kind of intuitively.",
                    "label": 0
                },
                {
                    "sent": "Weather all information.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "From X to Y and vice versa has to pass through the nodes V if everything between X&Y has to pass through V. Then once we know the X&Y become independent of each other, OK.",
                    "label": 0
                },
                {
                    "sent": "I mean, you could think of a lot of analogies like you know the flow of information or the flow of electricity or whatever.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "Just think of it that way and then the actual semantics are a little annoying.",
                    "label": 0
                },
                {
                    "sent": "But you know that you can prove that these are things that follow from where they follow from.",
                    "label": 0
                },
                {
                    "sent": "Guess.",
                    "label": 0
                },
                {
                    "sent": "The sum rule in the product rule, yes.",
                    "label": 0
                },
                {
                    "sent": "They follow from the same rule in product rule.",
                    "label": 0
                },
                {
                    "sent": "If they didn't and they follow from.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's the only thing they follow from the sum rule, the product rule and this statement OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it could take, you know, 10 pages of you dear pearls classic book to prove this stuff.",
                    "label": 0
                },
                {
                    "sent": "But all he's doing is showing you the sum rule in the product rule and the factorization OK?",
                    "label": 0
                },
                {
                    "sent": "Now the corollary for this is that for any variable X we can define something called the Markov boundary, and that insulates X.",
                    "label": 0
                },
                {
                    "sent": "From all the other variables, and that's the set of parents of X children of X and the parents of the other parents of the children of X.",
                    "label": 0
                },
                {
                    "sent": "Given that set of variables X is independent of everything else.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So graphical models are kind of used in two very different ways and I just want to give you a flavor for the two ways.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One of 'em is when we write down a graph like this and we think of the particular variables as being something interpretable like.",
                    "label": 0
                },
                {
                    "sent": "The temperature at this temperature sensor in the pressure, this valve, etc.",
                    "label": 0
                },
                {
                    "sent": "And this is a graphical model that relates these things to each other OK.",
                    "label": 0
                },
                {
                    "sent": "So this is like as a way of doing knowledge representation in a particular domain.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another place where graphical models appear, sorry is just in descriptions of statistical models like this.",
                    "label": 0
                },
                {
                    "sent": "So for example, consider the following simple model.",
                    "label": 1
                },
                {
                    "sent": "I have a set of N data points generated ID from a Gaussian with mean mu and standard deviation Sigma.",
                    "label": 1
                },
                {
                    "sent": "Now if I want to build a model of this, what that corresponds to is the idea that the joint distribution of X1 through XN, mu and Sigma factors in a particular way.",
                    "label": 0
                },
                {
                    "sent": "This is the IID assumption.",
                    "label": 0
                },
                {
                    "sent": "I said each of these depends on mu and Sigma right?",
                    "label": 0
                },
                {
                    "sent": "And then I might have a prior.",
                    "label": 0
                },
                {
                    "sent": "I haven't said that in the sentence here, but maybe my prior on mu and Sigma factors in this way.",
                    "label": 0
                },
                {
                    "sent": "Let's say that's a particular.",
                    "label": 0
                },
                {
                    "sent": "Model it corresponds to a factorization of this joint distribution.",
                    "label": 0
                },
                {
                    "sent": "We can rewrite that as a graph.",
                    "label": 0
                },
                {
                    "sent": "That graph looks like this.",
                    "label": 0
                },
                {
                    "sent": "Mu and Sigma are the parents of X1 through XN and now because often in graphs like this we have repetitions of variables.",
                    "label": 0
                },
                {
                    "sent": "So you know we have these X one through XN with this... notation.",
                    "label": 0
                },
                {
                    "sent": "More concise notation is this thing called a plate, which just puts a.",
                    "label": 0
                },
                {
                    "sent": "In this case a dashed box around X with.",
                    "label": 0
                },
                {
                    "sent": "AA.",
                    "label": 0
                },
                {
                    "sent": "Label here that indicates the number of times this variable is repeated in the graph, so XN is repeated big end times OK, this is just shorthand for this graph, so you'll see this sort of notation in a lot of machine learning papers OK. Alright.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let's talk about inference in graphical models.",
                    "label": 0
                },
                {
                    "sent": "Considering consider the following graph.",
                    "label": 1
                },
                {
                    "sent": "What is inference inference corresponds to evaluating the probability distribution over some set of variables given the values of another set of variables.",
                    "label": 1
                },
                {
                    "sent": "For example, what is the probability of a given that we know the value of C?",
                    "label": 1
                },
                {
                    "sent": "Alright, so for example, assume that each variable is binary.",
                    "label": 0
                },
                {
                    "sent": "How do we compute this?",
                    "label": 0
                },
                {
                    "sent": "Well, here is a naive method.",
                    "label": 0
                },
                {
                    "sent": "I write down the joint of A&C is the sum over BD&E of the joint of ABCD&E.",
                    "label": 0
                },
                {
                    "sent": "There are 16 terms in this sum.",
                    "label": 0
                },
                {
                    "sent": "Given the graph and maybe the probability distributions on each of the edges in the graph, I can actually evaluate this for all 16 of these terms.",
                    "label": 0
                },
                {
                    "sent": "And then I can sum over a.",
                    "label": 0
                },
                {
                    "sent": "To evaluate these two terms and then I can divide these by these, which give me these two terms which gives me this probability distribution probability of a given C. OK, this is a naive procedure because I went through writing down the whole joint distribution and evaluating all the configurations.",
                    "label": 0
                },
                {
                    "sent": "And it took me 20 terms to compute this.",
                    "label": 0
                },
                {
                    "sent": "Clearly if I had 100 binary variables, this would be a really bad idea, right?",
                    "label": 0
                },
                {
                    "sent": "But we know something about this joint distribution.",
                    "label": 0
                },
                {
                    "sent": "We know that it factors in a particular way, so let's rearrange the computations while keeping them exact.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "So I take this same graph and now this is just anecdotal for this particular factorization, I plug it in here.",
                    "label": 0
                },
                {
                    "sent": "This sum is still expensive to compute.",
                    "label": 0
                },
                {
                    "sent": "But I plug it in here and then I notice something about this.",
                    "label": 0
                },
                {
                    "sent": "I notice that E. Only appears here so I can bring this some over E all the way in here.",
                    "label": 0
                },
                {
                    "sent": "And I can sum over E exactly because I know that the sum over E is 1 when I have the conditional distribution of E over the two values of Y.",
                    "label": 0
                },
                {
                    "sent": "So this thing sums to one.",
                    "label": 0
                },
                {
                    "sent": "Similarly, I can take the sum over D and bring it in and this thing sums to one exactly.",
                    "label": 0
                },
                {
                    "sent": "So in fact this whole thing simplifies the sum over B of P of a times P of B times Pfc, given A&B.",
                    "label": 0
                },
                {
                    "sent": "And that ends up being about 8 terms depending on how you compute it.",
                    "label": 1
                },
                {
                    "sent": "The first operation is 4 terms.",
                    "label": 1
                },
                {
                    "sent": "The other two operations are two terms each.",
                    "label": 0
                },
                {
                    "sent": "So what does that correspond to in the graph that corresponds to the fact that I wanted to know a given C?",
                    "label": 0
                },
                {
                    "sent": "OK, it turns out if I know a given its parents, it doesn't have parents, is children and the parents of his children, that would be B, then everything else is independent, right?",
                    "label": 0
                },
                {
                    "sent": "So in fact, if I look at the Markov blanket of a, the only thing that depends on?",
                    "label": 0
                },
                {
                    "sent": "Is B&C?",
                    "label": 0
                },
                {
                    "sent": "So that's all I needed to have in this.",
                    "label": 1
                },
                {
                    "sent": "I needed to have a B&C in these expressions, the whole rest of the graph, even if it has 98 other variables in it, just drops out.",
                    "label": 0
                },
                {
                    "sent": "But of course we don't want to do these things by hand for every graph.",
                    "label": 0
                },
                {
                    "sent": "What we really want is general purpose algorithms, where you write down the graph in some way on the computer, and the computer automatically figures out how to apply the sum and product rule efficiently with the conditional independence relationships that are encoded in your graph.",
                    "label": 1
                },
                {
                    "sent": "And clearly by doing this on sparse graphs, we can get exponential gains over the naive method.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so how do we actually do this?",
                    "label": 0
                },
                {
                    "sent": "Let's focus on particular example of factor graph propagation.",
                    "label": 1
                },
                {
                    "sent": "This is sort of.",
                    "label": 0
                },
                {
                    "sent": "An implementation of belief propagation that's particularly nice and simple to write down.",
                    "label": 0
                },
                {
                    "sent": "The idea of this is you take your joint distribution, which is in this particular example.",
                    "label": 0
                },
                {
                    "sent": "Let's say it looks like this.",
                    "label": 0
                },
                {
                    "sent": "And you instead of writing it as a directed graph, we're going to write it as a factor graph.",
                    "label": 0
                },
                {
                    "sent": "This is just another graphical formalism which with similar properties I can talk about the differences of somebody's interested later.",
                    "label": 0
                },
                {
                    "sent": "The way we write this is rewrite it in terms of a product of factors.",
                    "label": 0
                },
                {
                    "sent": "So for each one of these terms in here, we write down a factor which is a function of the variables involved in these terms.",
                    "label": 0
                },
                {
                    "sent": "The first term is just a function of X1, the second term is a function of X2 and X1, so we can actually group those together into this factor F1, which is a function of X1 and X2, and we write the factor graph like this.",
                    "label": 0
                },
                {
                    "sent": "The nodes represent the variables and.",
                    "label": 0
                },
                {
                    "sent": "These filled dots represent the factors.",
                    "label": 0
                },
                {
                    "sent": "So F1 is a factor relating X1 and X2.",
                    "label": 0
                },
                {
                    "sent": "Similarly for F2 and F3 here.",
                    "label": 0
                },
                {
                    "sent": "And we're going to focus on what are called singly connected factor graphs.",
                    "label": 1
                },
                {
                    "sent": "The singly connected factor graph corresponds to one in which the underlying structure of the graph is a tree.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "In other words, there is between any pair of nodes there is at most one path.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is an example of a multiply connected factor graph or not, simply connected factor graph and I can tell you.",
                    "label": 1
                },
                {
                    "sent": "Later, if you're interested how to do inference in these graphs?",
                    "label": 0
                },
                {
                    "sent": "Any questions about this?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's just the now the general description of what a factor graph is.",
                    "label": 0
                },
                {
                    "sent": "The joint probability is written as a product of factors, so the joint probability here is one over some normalizing constant of the product.",
                    "label": 1
                },
                {
                    "sent": "Over these J factors, where each factor depends on sub some subset of the vector of all the variables.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We have these two kinds of nodes that variable nodes in the factor nodes, which I described in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "These factors are non negative functions of their arguments.",
                    "label": 0
                },
                {
                    "sent": "OK, if the arguments are discrete, for example if they're binary, then we can think of the factors as tables with non negative entries in them, OK?",
                    "label": 1
                },
                {
                    "sent": "So that's a factor graph and now.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we're going to think about how to implement inference in a factor graph, how to implement the sum and product rule.",
                    "label": 0
                },
                {
                    "sent": "If you have a factor graph for your data for your model.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's some notation.",
                    "label": 0
                },
                {
                    "sent": "Let N of X denote the set of factor nodes that are connected to variable X.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for example, variable four is connected to factors three and four, so N A4 is 3, four in this case.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And similarly, let's say N of F denotes the set of variable nodes that are connected to factor F, so it's a bit of overload of this neighborhood notation OK?",
                    "label": 1
                },
                {
                    "sent": "So we can compute the probabilities by propagating messages from variable notes of factor nodes and vice versa, and is basically these two equations OK, we have in general messages from variables to factors.",
                    "label": 0
                },
                {
                    "sent": "So here is the message mu from variable X to factor.",
                    "label": 0
                },
                {
                    "sent": "FA message is going to be an unnormalized probability.",
                    "label": 0
                },
                {
                    "sent": "Distribution, so if X is mean, it could be normalized.",
                    "label": 0
                },
                {
                    "sent": "If you want to that, then you can just keep track of the normalization constants.",
                    "label": 1
                },
                {
                    "sent": "But a message here in this case is a function of X.",
                    "label": 0
                },
                {
                    "sent": "So if X is binary, that message is a 2 by 1 vector OK. And the message from variable X to factor F is the product over all the factors.",
                    "label": 0
                },
                {
                    "sent": "H that are connected to X, not including the factor F. Of the messages from Factor H to variable X.",
                    "label": 0
                },
                {
                    "sent": "So if we want to visualize this.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the graph.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We had a message from X to F. So let's look at this graph here.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Let's say the message from X2 to F3 is the product of the messages from F1 to X2 and the message from F2 techs two.",
                    "label": 0
                },
                {
                    "sent": "OK, we'd taken elementwise product of these vectors coming in.",
                    "label": 0
                },
                {
                    "sent": "And we re scale or not?",
                    "label": 0
                },
                {
                    "sent": "If we want to and we pass them on in the other direction.",
                    "label": 0
                },
                {
                    "sent": "Super easy to implement.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The message from factor F to variable X.",
                    "label": 1
                },
                {
                    "sent": "Is given by the following equation.",
                    "label": 1
                },
                {
                    "sent": "The message from F to X is also a function of the variable.",
                    "label": 1
                },
                {
                    "sent": "X is the sum over all variables, not including X.",
                    "label": 1
                },
                {
                    "sent": "Of the factor F multiplied by the messages from all the other variables that factor F. OK, so again it's taking incoming messages.",
                    "label": 0
                },
                {
                    "sent": "Multiplying them by its local factor, then summing out all variables other than the variable X that is sending the message to.",
                    "label": 0
                },
                {
                    "sent": "I tried to explain the notation here.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You know the message from factor F12X2 takes the message from X1 to F, multiplies it by the factor F1, which is a function of X1 and X2.",
                    "label": 0
                },
                {
                    "sent": "It sums out X one and that becomes a function of X2 that it sends on 2X2.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's it, these two equations.",
                    "label": 0
                },
                {
                    "sent": "If you run them in a particular order.",
                    "label": 0
                },
                {
                    "sent": "Until no other messages are left to be computed.",
                    "label": 0
                },
                {
                    "sent": "If you're affected, graph is singly connected, what you get.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is just a repeat of these two equations.",
                    "label": 0
                },
                {
                    "sent": "This is a statement of the order.",
                    "label": 0
                },
                {
                    "sent": "If a variable has only one factor as a neighbor, it can initiate this message propagation.",
                    "label": 1
                },
                {
                    "sent": "Then, once a variable is received, all the messages from its neighboring factors.",
                    "label": 0
                },
                {
                    "sent": "Then we can compute the marginal probability of that variable by taking the product of all those messages.",
                    "label": 0
                },
                {
                    "sent": "An re normalizing it.",
                    "label": 0
                },
                {
                    "sent": "OK. Any questions about that?",
                    "label": 0
                },
                {
                    "sent": "How many people have seen this before?",
                    "label": 0
                },
                {
                    "sent": "Raise your hand if you've seen it before.",
                    "label": 0
                },
                {
                    "sent": "OK, quite a lot of you.",
                    "label": 0
                },
                {
                    "sent": "How many people had not seen this before?",
                    "label": 0
                },
                {
                    "sent": "OK. Alright.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is very nice as a very general algorithm.",
                    "label": 0
                },
                {
                    "sent": "It turns out that it's a generalization of things that have been very well known for decades.",
                    "label": 0
                },
                {
                    "sent": "For example, if I take a hidden Markov model.",
                    "label": 1
                },
                {
                    "sent": "The directed graph for an HMM looks like this.",
                    "label": 0
                },
                {
                    "sent": "These are observed sequence of data and this is a sequence of hidden variables that are meant to explain the observed data.",
                    "label": 0
                },
                {
                    "sent": "In the hidden Markov model there is something called the forward backward algorithm.",
                    "label": 0
                },
                {
                    "sent": "That's just factor graph propagation, and it shouldn't be surprising.",
                    "label": 1
                },
                {
                    "sent": "It's not like wow, what a coincidence.",
                    "label": 0
                },
                {
                    "sent": "These two things that were invented by completely different fields happen to be the same algorithm is not surprising at all, right?",
                    "label": 0
                },
                {
                    "sent": "Because they're following the rules of probability using a graph that better be the same if they're not the same.",
                    "label": 0
                },
                {
                    "sent": "One of 'em is wrong.",
                    "label": 0
                },
                {
                    "sent": "Right now, Kalman smoothing in a state space model.",
                    "label": 0
                },
                {
                    "sent": "That thing that I took two semesters of.",
                    "label": 0
                },
                {
                    "sent": "It could have been explained as just factor graph propagation with Gaussian variables as well.",
                    "label": 0
                },
                {
                    "sent": "OK. OK.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now there is a lot of nice software for graphical models.",
                    "label": 1
                },
                {
                    "sent": "I've just listed a few of them here with some comments about them.",
                    "label": 0
                },
                {
                    "sent": "There is a great resource that maybe a few years old that Kevin Murphy generated, which lists a whole bunch of graphical model software toolkits.",
                    "label": 0
                },
                {
                    "sent": "And you know of course, different software toolkits emphasize different kinds of models and things like that, OK?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's just the summary of the basics of graphical models.",
                    "label": 0
                },
                {
                    "sent": "Inference is the problem of figuring out the probability of the variables of interest given the observed variables while summing out the other variables.",
                    "label": 0
                },
                {
                    "sent": "You're not interested in.",
                    "label": 0
                },
                {
                    "sent": "That's the basic thing we're trying to do.",
                    "label": 0
                },
                {
                    "sent": "For singly connected graphs, we have these things called belief propagation and factor graph propagation.",
                    "label": 1
                },
                {
                    "sent": "Kalman filtering and forward backward are special cases of that.",
                    "label": 0
                },
                {
                    "sent": "Now what do we do for multiply connected graphs OK?",
                    "label": 1
                },
                {
                    "sent": "Well, if we want to do exact inference in multiply connected graphs, there is a very nice algorithm called the Junction tree algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's been known since 1988 or so.",
                    "label": 0
                },
                {
                    "sent": "And basically you can think of this as converting your multiply connected graph into a single connected graph, but over super nodes.",
                    "label": 0
                },
                {
                    "sent": "That correspond to combinations of sets of variables.",
                    "label": 0
                },
                {
                    "sent": "You do that conversion and then you run factor graph propagation on that converted graph.",
                    "label": 0
                },
                {
                    "sent": "That's basically intuitively what it corresponds to.",
                    "label": 0
                },
                {
                    "sent": "That conversion is not trivial.",
                    "label": 0
                },
                {
                    "sent": "There might be many ways of converting it, but you know that's the basic idea.",
                    "label": 0
                },
                {
                    "sent": "That can be very slow, because that conversion of your multiply connected graph into a single connected graph over super nodes.",
                    "label": 0
                },
                {
                    "sent": "Is something that could be kind of expensive?",
                    "label": 0
                },
                {
                    "sent": "It could end up with super nodes with exponentially many states in them.",
                    "label": 0
                },
                {
                    "sent": "So in practice, what's actually done most often is to just run factor graph propagation.",
                    "label": 0
                },
                {
                    "sent": "Acting like the graph is singly connected when it's not.",
                    "label": 0
                },
                {
                    "sent": "Close your eyes and press the go button and cross your fingers and hope for the best.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of analysis of that, and I could say more about that.",
                    "label": 0
                },
                {
                    "sent": "Actually, this that's called loopy belief propagation.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Weather proof sketch for the.",
                    "label": 0
                },
                {
                    "sent": "Proof sketch for loopy belief propagation.",
                    "label": 0
                },
                {
                    "sent": "OK, so I can.",
                    "label": 0
                },
                {
                    "sent": "I can certainly give you a sketch of a sketch.",
                    "label": 0
                },
                {
                    "sent": "Hyper sketch so.",
                    "label": 0
                },
                {
                    "sent": "In general loopy belief propagation, which is just factor graph propagation or belief propagation on a graph that has loops like you know we saw before, sorry I don't know what happened here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "In general that.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, this is a loopy graph.",
                    "label": 0
                },
                {
                    "sent": "Very small, easy loopy graph, but in general that algorithm is not guaranteed to converge.",
                    "label": 0
                },
                {
                    "sent": "OK, so you could run these messages and the beliefs would keep going up and down, up and down.",
                    "label": 0
                },
                {
                    "sent": "It's possible for that to happen if it converges, it might not give the right answer.",
                    "label": 0
                },
                {
                    "sent": "In some special cases it's been analyzed and shown to be able to converge.",
                    "label": 0
                },
                {
                    "sent": "For example, in the Gaussian case it's been analyzed by people like, yeah, you're Weiss in graphs that have a single loop or a few very large loops where the influence becomes weaker and weaker than you can show convergence of certain kinds, and there is a very nice theory that's been developed around this, based on an idea called the Beta Free Energy.",
                    "label": 0
                },
                {
                    "sent": "Where essentially what people have shown is that loopy belief propagation in graph, if it converges, it converges to.",
                    "label": 0
                },
                {
                    "sent": "Fixed points of the beta free energy.",
                    "label": 0
                },
                {
                    "sent": "The other thing that's interesting about loopy belief propagation is that this beta free energy interpretation means that we can now come up with convergent approximate inference algorithms for these kinds of graphs that are based around minimizing that energy directly, rather than applying loopy belief propagation and then the last interesting thing I want to say is that factor graph propagation or loopy belief propagation.",
                    "label": 0
                },
                {
                    "sent": "In multiply connected graphs is also equivalent to a particular form of the expectation propagation EP algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are very beautiful links between all of these different methods that are worth knowing about.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the summary of inference and then now for the next 45 minutes or so.",
                    "label": 0
                },
                {
                    "sent": "Should we take a 5 minute break?",
                    "label": 0
                },
                {
                    "sent": "OK. For the next 5 minutes, we're going to take a break and for the following 40 minutes, just so that you're encouraged to come back, we're going to talk about parameter and structure learning in graphs.",
                    "label": 0
                }
            ]
        }
    }
}