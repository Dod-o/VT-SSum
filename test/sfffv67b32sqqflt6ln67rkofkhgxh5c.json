{
    "id": "sfffv67b32sqqflt6ln67rkofkhgxh5c",
    "title": "Optimistic Initialization and Greediness Lead to Polynomial Time Learning in Factored MDPs",
    "info": {
        "author": [
            "Istvan Szita, E\u00f6tv\u00f6s Lor\u00e1nd University"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_szita_oigl/",
    "segmentation": [
        [
            "So while at."
        ],
        [
            "I'm going to start with reinforcement learning, so we have this agent who makes decisions in an unknown world, make some observations, collect some reward, and tries to maximize collected reward."
        ],
        [
            "And now the question comes that what kind of observation do we have?",
            "And if you look at a picture like this whole, you can see that it's equivalent to the picture who real with reordered pixels, so it's meaningless unless some kind of interpretation and well, what what seems to be a good approach is having some.",
            "Structured observations, I don't know what kind of structured observations.",
            "I don't know what the structure, but this seems to be a good start.",
            "Anne Anne."
        ],
        [
            "And now the next next question is how to solve another task.",
            "Now just the motivation not going to answer the question, but model seems useful because we can reuse experience from previous trials.",
            "We can learn offline and all observations are many cases structured.",
            "And of course we don't have this structure.",
            "And well, if we Add all this stuff together then we get factored MDP's.",
            "Of course if I was a. Neural network guide.",
            "Then I would say that it's a neural network, but I'm going to say, factored MDP."
        ],
        [
            "So factor then these are just ordinary MDP's where everything is factored like we have affective state space."
        ],
        [
            "And what makes it a very easy to handle that all functions we're going to define will depend on a few variables only, like we will have factored diner."
        ],
        [
            "Six straight state transfer dynamics.",
            "All components will depend on a few other components and there are affected rewards.",
            "Their reward is."
        ],
        [
            "Some of such simple functions and."
        ],
        [
            "And we might have or might not have factored very functions and now not of warning is dubyk cause the optimal value function is in general not factored.",
            "So if we use this approximation then we will make an error.",
            "But while we might make the problem solver so."
        ],
        [
            "Unknown, an MDP is empty hearts so which place to either being exponential time worst case or we have to settle for non optimal solution.",
            "For example, have approximating the value function as sum of factors, factoring it into several terms.",
            "Um?",
            "And there are a couple of approaches that do either of this.",
            "I'm going to speak only of one of those, and later on it turns out why.",
            "Of this actually fractured valutation, the."
        ],
        [
            "As an approximate value iteration where you have the set of basis functions, you collect that into neat matrix and then you define a projection matrix, which is some kind of normalized version of Edge transposed so that its Max norm is equal to 1, and so we don't get non convergence behavior.",
            "And actually this iteration can be solved.",
            "Quickly for factored MDP's and by quickly I mean that it's quick both in running time and then we can prove polynomial we can prove polynomial T as well.",
            "And actually, of course, we're going to make an error.",
            "Which is if our basis functions are OK, then the error will be small, but we're going to make an error anyway.",
            "And that's a problem."
        ],
        [
            "That will be a problem.",
            "Now how to learn in an FM DPS?",
            "Well, we don't know the structure.",
            "We don't have the rewards.",
            "We don't know the."
        ],
        [
            "Dynamics and I'm not going to talk about the first 2."
        ],
        [
            "Because it's time to transfer to optimism."
        ],
        [
            "So.",
            "After all, the agent has tried few action sequences than it faces the Explore exploit dilemma where we should go for exploring better options or do the best things according to."
        ],
        [
            "And knowledge and the easy answer is the when you face uncertain TB, opt."
        ],
        [
            "Mystic and if you're updated then you even get some."
        ],
        [
            "Spears or you actually get the reward that you were hoping for.",
            "Now how to?"
        ],
        [
            "Combined this in to the peas."
        ],
        [
            "Oh, how if we make a transition modeling factored MDP's?",
            "How does it look like we have conditional probability tables probably which start out with zeros in each entry.",
            "So this is our initial model and let's make it optimistic.",
            "Let's add ago."
        ],
        [
            "Not enough aid and component for each.",
            "I'm table which says that if I'm going to go here then I'm going to get $100 and that's going to be very good for me.",
            "And actually my model says that I'm going to go here with probability.",
            "One big cause.",
            "I've been here once and I've been here for 0 * 0 times.",
            "That's going to be a better model, but it's going to be an optimistic one.",
            "Now."
        ],
        [
            "A only and after that I greedily simply act really.",
            "And what does happen here?",
            "According to initial model, each state has very high value.",
            "Infrequently visited states of it's very similar actually to what Zico spoke about a few minutes ago.",
            "Then the model starts to be more accurate and actually the effect of this disturbance becomes smaller.",
            "And in states where we haven't been many times there, the effect of this.",
            "Initialization is still very active, so the value of this state and the value of.",
            "Less often tried actions will be much, much higher.",
            "So we're going to explore."
        ],
        [
            "So there over Adam is very very simple.",
            "We just optimize the model at this transition probability tables.",
            "Optimistically, we had the Garden of Eden State and then for each time step we solve the model or resolve our current model and make it really step according to our current problem.",
            "And there's a problem that this is not an exact solution because we are going to be quick we.",
            "Don't want to take potentially exponential time, so we're the best we can hope for.",
            "Is that we converge to the approximate value function that are planner converge to an it does.",
            "Um the elements."
        ],
        [
            "Proof and I'm not going to explain the proof, only very rough sketch sketch sketch.",
            "Space standard, like if the transition probabilities are close to the optimal ones, then the true approximated very function is going to be similar to the.",
            "An approximated version, and then if you get the components more exact than the competition gets more exact.",
            "That's pretty trivial and you have some kind of concentration inequality there.",
            "All kinds of these pack and pack MDP proofs have have this."
        ],
        [
            "Inequalities and now the interesting place is very well you need to show that the very function remains optimistic and now you want to bound the distance of the true values from the approximated values and.",
            "Well, by azimuth inequality you get some lower bounds with which has one over square root of an term.",
            "And you get bonus that's provided by the optimistic initialization, which has a 1 / N term which is just in the case much much quicker.",
            "But actually, that's not.",
            "That's such a great problem, because we only need epsilon optimality.",
            "And if this value is is very high, then by the time.",
            "It goes this.",
            "Curve goes under this curve.",
            "By the time we have explored everything.",
            "Um?",
            "And we need another technical thing that the when we do approximate valuation iteration, then the projection preserves optimism and that's non trivial.",
            "And if we have some transformation of H transposed, then it's true.",
            "If we have some different matrix then it's either not sure we weren't able to show that, but there is.",
            "There is some projection for which we are able to show that.",
            "It preserves op."
        ],
        [
            "And now if we have optimism preservation then, well, we can say that for a long time the very function is optimistic enough to boost expiration and then after polynomial number of steps we have learned all parameters.",
            "And then we're going to be almost as good as the planner.",
            "Which is what you can hope for now."
        ],
        [
            "Well, we are obviously not the first ones who tried to learn factored MDP transitions.",
            "There are various extensions of factored extensions of Y cubed, armex mbie.",
            "And they are the only problem is that they all make the assumption that they have access to.",
            "An optimum.",
            "Good planner that gives you an optimal value function.",
            "And, well, you know the only stuff you can plug in.",
            "There are exponential worst case running time.",
            "So any fee are your parents not optimal then?",
            "You don't have common theorems anymore."
        ],
        [
            "And now there were two things that are was amazing.",
            "What about unknown rewards?",
            "While usually you make the assumption that OK, if you could learn their transition probabilities, then you can also learn the rewards.",
            "You assume that they are known for affecting MDP is this is not quite true because you cannot observe the reward components.",
            "You can only observe there some.",
            "So you have to solve a linear regression problem, but at the same time you have to be optimistic.",
            "And for about that, we have a poster on UI with Carlos and Tom Wash and Michael Lipman."
        ],
        [
            "And what about?",
            "I'm not structure well cost, just talked about that and I updated my slides in real time so.",
            "We can also do that and."
        ],
        [
            "So whether they can message, I was told it.",
            "I always need to give some take a message and it if model starts out optimistically enough then you get expression for free.",
            "You don't need to explore explicitly.",
            "That's all, thank you for you."
        ],
        [
            "Any questions?",
            "I'll ask you a question so I guess what some experimental results could look like.",
            "Like having implemented this network, I have so this is current work that's going on.",
            "I just implemented the algorithm and.",
            "On oh, it's working.",
            "It's really the only problem we have.",
            "It's really hard to find good test problems except other than sending.",
            "Let me ask you another question then.",
            "So when you have this lies about previous work, so you mentioned that there was a plan at the bottom saying that previous work.",
            "I guess the computation time was still exponential.",
            "So no, it's polynomial.",
            "But where we don't have access to an optimal planner, so we might not get to the optimal policy.",
            "We're going to converge to something, but in polynomial time.",
            "I see and we can bomb the error, but then the error has to be loose because otherwise I mean we know that approximating is NP parts or something.",
            "Yes, I can show you the bound.",
            "Yeah, here's the bounds.",
            "So it basically says that if you can represent exactly the value function with this basis functions, then you will.",
            "You will converge to the optimum if you can also.",
            "So it's basically how good your basis function are.",
            "Right, OK?",
            "Going back to the specification of your problem.",
            "Maybe?",
            "Sorry, so when.",
            "My question was whether the function of media like this.",
            "It's not linear, it's it's a for each component.",
            "It's a conditional probability table, so it's it's a table of discrete values.",
            "And for the factor rewards there, I reaction that the reward is a linear combination of simple reward functions.",
            "Again.",
            "What is your final?",
            "I this operation has a fixed point where you write the star and we start here.",
            "That's kind of dumb and equation and you can get actually a value function from that.",
            "You can go back to very function space and there well it is something you don't know anything about, except that it's it's error can be bounded from the optimum.",
            "Does this imply something about the greedy policy you get from the cross?",
            "About Yep.",
            "We crossed my country.",
            "The value function of any general.",
            "Yeah, that's true.",
            "But if you generate the goal, I think it's pretty easy to get a statement about the greedy policy from that, but we actually haven't made that.",
            "This is very non technical question.",
            "What circumstances do you imagine being able to observe a reward but not being able to observe the components I?",
            "OK, so you you might not really know what the components are.",
            "You might want to learn the structure and you're trying out.",
            "Structure of the mythologist algorithm and then, well, you marked up some reward factors and you're trying to decompose the real scalar valued signal to some components, but there might not be such a decomposition.",
            "Oh alright, so let's take our speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So while at.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to start with reinforcement learning, so we have this agent who makes decisions in an unknown world, make some observations, collect some reward, and tries to maximize collected reward.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now the question comes that what kind of observation do we have?",
                    "label": 0
                },
                {
                    "sent": "And if you look at a picture like this whole, you can see that it's equivalent to the picture who real with reordered pixels, so it's meaningless unless some kind of interpretation and well, what what seems to be a good approach is having some.",
                    "label": 0
                },
                {
                    "sent": "Structured observations, I don't know what kind of structured observations.",
                    "label": 1
                },
                {
                    "sent": "I don't know what the structure, but this seems to be a good start.",
                    "label": 0
                },
                {
                    "sent": "Anne Anne.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now the next next question is how to solve another task.",
                    "label": 1
                },
                {
                    "sent": "Now just the motivation not going to answer the question, but model seems useful because we can reuse experience from previous trials.",
                    "label": 1
                },
                {
                    "sent": "We can learn offline and all observations are many cases structured.",
                    "label": 0
                },
                {
                    "sent": "And of course we don't have this structure.",
                    "label": 0
                },
                {
                    "sent": "And well, if we Add all this stuff together then we get factored MDP's.",
                    "label": 0
                },
                {
                    "sent": "Of course if I was a. Neural network guide.",
                    "label": 0
                },
                {
                    "sent": "Then I would say that it's a neural network, but I'm going to say, factored MDP.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So factor then these are just ordinary MDP's where everything is factored like we have affective state space.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what makes it a very easy to handle that all functions we're going to define will depend on a few variables only, like we will have factored diner.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Six straight state transfer dynamics.",
                    "label": 0
                },
                {
                    "sent": "All components will depend on a few other components and there are affected rewards.",
                    "label": 0
                },
                {
                    "sent": "Their reward is.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some of such simple functions and.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we might have or might not have factored very functions and now not of warning is dubyk cause the optimal value function is in general not factored.",
                    "label": 0
                },
                {
                    "sent": "So if we use this approximation then we will make an error.",
                    "label": 1
                },
                {
                    "sent": "But while we might make the problem solver so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unknown, an MDP is empty hearts so which place to either being exponential time worst case or we have to settle for non optimal solution.",
                    "label": 0
                },
                {
                    "sent": "For example, have approximating the value function as sum of factors, factoring it into several terms.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And there are a couple of approaches that do either of this.",
                    "label": 0
                },
                {
                    "sent": "I'm going to speak only of one of those, and later on it turns out why.",
                    "label": 0
                },
                {
                    "sent": "Of this actually fractured valutation, the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As an approximate value iteration where you have the set of basis functions, you collect that into neat matrix and then you define a projection matrix, which is some kind of normalized version of Edge transposed so that its Max norm is equal to 1, and so we don't get non convergence behavior.",
                    "label": 1
                },
                {
                    "sent": "And actually this iteration can be solved.",
                    "label": 1
                },
                {
                    "sent": "Quickly for factored MDP's and by quickly I mean that it's quick both in running time and then we can prove polynomial we can prove polynomial T as well.",
                    "label": 0
                },
                {
                    "sent": "And actually, of course, we're going to make an error.",
                    "label": 0
                },
                {
                    "sent": "Which is if our basis functions are OK, then the error will be small, but we're going to make an error anyway.",
                    "label": 0
                },
                {
                    "sent": "And that's a problem.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That will be a problem.",
                    "label": 0
                },
                {
                    "sent": "Now how to learn in an FM DPS?",
                    "label": 0
                },
                {
                    "sent": "Well, we don't know the structure.",
                    "label": 0
                },
                {
                    "sent": "We don't have the rewards.",
                    "label": 0
                },
                {
                    "sent": "We don't know the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dynamics and I'm not going to talk about the first 2.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because it's time to transfer to optimism.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "After all, the agent has tried few action sequences than it faces the Explore exploit dilemma where we should go for exploring better options or do the best things according to.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And knowledge and the easy answer is the when you face uncertain TB, opt.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mystic and if you're updated then you even get some.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spears or you actually get the reward that you were hoping for.",
                    "label": 0
                },
                {
                    "sent": "Now how to?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Combined this in to the peas.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh, how if we make a transition modeling factored MDP's?",
                    "label": 0
                },
                {
                    "sent": "How does it look like we have conditional probability tables probably which start out with zeros in each entry.",
                    "label": 0
                },
                {
                    "sent": "So this is our initial model and let's make it optimistic.",
                    "label": 0
                },
                {
                    "sent": "Let's add ago.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not enough aid and component for each.",
                    "label": 0
                },
                {
                    "sent": "I'm table which says that if I'm going to go here then I'm going to get $100 and that's going to be very good for me.",
                    "label": 0
                },
                {
                    "sent": "And actually my model says that I'm going to go here with probability.",
                    "label": 0
                },
                {
                    "sent": "One big cause.",
                    "label": 0
                },
                {
                    "sent": "I've been here once and I've been here for 0 * 0 times.",
                    "label": 0
                },
                {
                    "sent": "That's going to be a better model, but it's going to be an optimistic one.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A only and after that I greedily simply act really.",
                    "label": 0
                },
                {
                    "sent": "And what does happen here?",
                    "label": 0
                },
                {
                    "sent": "According to initial model, each state has very high value.",
                    "label": 0
                },
                {
                    "sent": "Infrequently visited states of it's very similar actually to what Zico spoke about a few minutes ago.",
                    "label": 0
                },
                {
                    "sent": "Then the model starts to be more accurate and actually the effect of this disturbance becomes smaller.",
                    "label": 0
                },
                {
                    "sent": "And in states where we haven't been many times there, the effect of this.",
                    "label": 0
                },
                {
                    "sent": "Initialization is still very active, so the value of this state and the value of.",
                    "label": 0
                },
                {
                    "sent": "Less often tried actions will be much, much higher.",
                    "label": 0
                },
                {
                    "sent": "So we're going to explore.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there over Adam is very very simple.",
                    "label": 0
                },
                {
                    "sent": "We just optimize the model at this transition probability tables.",
                    "label": 0
                },
                {
                    "sent": "Optimistically, we had the Garden of Eden State and then for each time step we solve the model or resolve our current model and make it really step according to our current problem.",
                    "label": 0
                },
                {
                    "sent": "And there's a problem that this is not an exact solution because we are going to be quick we.",
                    "label": 0
                },
                {
                    "sent": "Don't want to take potentially exponential time, so we're the best we can hope for.",
                    "label": 0
                },
                {
                    "sent": "Is that we converge to the approximate value function that are planner converge to an it does.",
                    "label": 0
                },
                {
                    "sent": "Um the elements.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Proof and I'm not going to explain the proof, only very rough sketch sketch sketch.",
                    "label": 0
                },
                {
                    "sent": "Space standard, like if the transition probabilities are close to the optimal ones, then the true approximated very function is going to be similar to the.",
                    "label": 0
                },
                {
                    "sent": "An approximated version, and then if you get the components more exact than the competition gets more exact.",
                    "label": 0
                },
                {
                    "sent": "That's pretty trivial and you have some kind of concentration inequality there.",
                    "label": 0
                },
                {
                    "sent": "All kinds of these pack and pack MDP proofs have have this.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Inequalities and now the interesting place is very well you need to show that the very function remains optimistic and now you want to bound the distance of the true values from the approximated values and.",
                    "label": 0
                },
                {
                    "sent": "Well, by azimuth inequality you get some lower bounds with which has one over square root of an term.",
                    "label": 0
                },
                {
                    "sent": "And you get bonus that's provided by the optimistic initialization, which has a 1 / N term which is just in the case much much quicker.",
                    "label": 0
                },
                {
                    "sent": "But actually, that's not.",
                    "label": 0
                },
                {
                    "sent": "That's such a great problem, because we only need epsilon optimality.",
                    "label": 0
                },
                {
                    "sent": "And if this value is is very high, then by the time.",
                    "label": 0
                },
                {
                    "sent": "It goes this.",
                    "label": 0
                },
                {
                    "sent": "Curve goes under this curve.",
                    "label": 0
                },
                {
                    "sent": "By the time we have explored everything.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And we need another technical thing that the when we do approximate valuation iteration, then the projection preserves optimism and that's non trivial.",
                    "label": 1
                },
                {
                    "sent": "And if we have some transformation of H transposed, then it's true.",
                    "label": 0
                },
                {
                    "sent": "If we have some different matrix then it's either not sure we weren't able to show that, but there is.",
                    "label": 0
                },
                {
                    "sent": "There is some projection for which we are able to show that.",
                    "label": 0
                },
                {
                    "sent": "It preserves op.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now if we have optimism preservation then, well, we can say that for a long time the very function is optimistic enough to boost expiration and then after polynomial number of steps we have learned all parameters.",
                    "label": 1
                },
                {
                    "sent": "And then we're going to be almost as good as the planner.",
                    "label": 0
                },
                {
                    "sent": "Which is what you can hope for now.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, we are obviously not the first ones who tried to learn factored MDP transitions.",
                    "label": 0
                },
                {
                    "sent": "There are various extensions of factored extensions of Y cubed, armex mbie.",
                    "label": 0
                },
                {
                    "sent": "And they are the only problem is that they all make the assumption that they have access to.",
                    "label": 0
                },
                {
                    "sent": "An optimum.",
                    "label": 0
                },
                {
                    "sent": "Good planner that gives you an optimal value function.",
                    "label": 0
                },
                {
                    "sent": "And, well, you know the only stuff you can plug in.",
                    "label": 0
                },
                {
                    "sent": "There are exponential worst case running time.",
                    "label": 0
                },
                {
                    "sent": "So any fee are your parents not optimal then?",
                    "label": 0
                },
                {
                    "sent": "You don't have common theorems anymore.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now there were two things that are was amazing.",
                    "label": 0
                },
                {
                    "sent": "What about unknown rewards?",
                    "label": 0
                },
                {
                    "sent": "While usually you make the assumption that OK, if you could learn their transition probabilities, then you can also learn the rewards.",
                    "label": 0
                },
                {
                    "sent": "You assume that they are known for affecting MDP is this is not quite true because you cannot observe the reward components.",
                    "label": 1
                },
                {
                    "sent": "You can only observe there some.",
                    "label": 1
                },
                {
                    "sent": "So you have to solve a linear regression problem, but at the same time you have to be optimistic.",
                    "label": 0
                },
                {
                    "sent": "And for about that, we have a poster on UI with Carlos and Tom Wash and Michael Lipman.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what about?",
                    "label": 0
                },
                {
                    "sent": "I'm not structure well cost, just talked about that and I updated my slides in real time so.",
                    "label": 0
                },
                {
                    "sent": "We can also do that and.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So whether they can message, I was told it.",
                    "label": 0
                },
                {
                    "sent": "I always need to give some take a message and it if model starts out optimistically enough then you get expression for free.",
                    "label": 1
                },
                {
                    "sent": "You don't need to explore explicitly.",
                    "label": 0
                },
                {
                    "sent": "That's all, thank you for you.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any questions?",
                    "label": 0
                },
                {
                    "sent": "I'll ask you a question so I guess what some experimental results could look like.",
                    "label": 0
                },
                {
                    "sent": "Like having implemented this network, I have so this is current work that's going on.",
                    "label": 0
                },
                {
                    "sent": "I just implemented the algorithm and.",
                    "label": 0
                },
                {
                    "sent": "On oh, it's working.",
                    "label": 0
                },
                {
                    "sent": "It's really the only problem we have.",
                    "label": 0
                },
                {
                    "sent": "It's really hard to find good test problems except other than sending.",
                    "label": 0
                },
                {
                    "sent": "Let me ask you another question then.",
                    "label": 0
                },
                {
                    "sent": "So when you have this lies about previous work, so you mentioned that there was a plan at the bottom saying that previous work.",
                    "label": 0
                },
                {
                    "sent": "I guess the computation time was still exponential.",
                    "label": 0
                },
                {
                    "sent": "So no, it's polynomial.",
                    "label": 0
                },
                {
                    "sent": "But where we don't have access to an optimal planner, so we might not get to the optimal policy.",
                    "label": 0
                },
                {
                    "sent": "We're going to converge to something, but in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "I see and we can bomb the error, but then the error has to be loose because otherwise I mean we know that approximating is NP parts or something.",
                    "label": 0
                },
                {
                    "sent": "Yes, I can show you the bound.",
                    "label": 0
                },
                {
                    "sent": "Yeah, here's the bounds.",
                    "label": 0
                },
                {
                    "sent": "So it basically says that if you can represent exactly the value function with this basis functions, then you will.",
                    "label": 0
                },
                {
                    "sent": "You will converge to the optimum if you can also.",
                    "label": 0
                },
                {
                    "sent": "So it's basically how good your basis function are.",
                    "label": 0
                },
                {
                    "sent": "Right, OK?",
                    "label": 0
                },
                {
                    "sent": "Going back to the specification of your problem.",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "Sorry, so when.",
                    "label": 0
                },
                {
                    "sent": "My question was whether the function of media like this.",
                    "label": 0
                },
                {
                    "sent": "It's not linear, it's it's a for each component.",
                    "label": 0
                },
                {
                    "sent": "It's a conditional probability table, so it's it's a table of discrete values.",
                    "label": 0
                },
                {
                    "sent": "And for the factor rewards there, I reaction that the reward is a linear combination of simple reward functions.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                },
                {
                    "sent": "What is your final?",
                    "label": 0
                },
                {
                    "sent": "I this operation has a fixed point where you write the star and we start here.",
                    "label": 0
                },
                {
                    "sent": "That's kind of dumb and equation and you can get actually a value function from that.",
                    "label": 0
                },
                {
                    "sent": "You can go back to very function space and there well it is something you don't know anything about, except that it's it's error can be bounded from the optimum.",
                    "label": 0
                },
                {
                    "sent": "Does this imply something about the greedy policy you get from the cross?",
                    "label": 0
                },
                {
                    "sent": "About Yep.",
                    "label": 0
                },
                {
                    "sent": "We crossed my country.",
                    "label": 0
                },
                {
                    "sent": "The value function of any general.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's true.",
                    "label": 0
                },
                {
                    "sent": "But if you generate the goal, I think it's pretty easy to get a statement about the greedy policy from that, but we actually haven't made that.",
                    "label": 0
                },
                {
                    "sent": "This is very non technical question.",
                    "label": 0
                },
                {
                    "sent": "What circumstances do you imagine being able to observe a reward but not being able to observe the components I?",
                    "label": 0
                },
                {
                    "sent": "OK, so you you might not really know what the components are.",
                    "label": 0
                },
                {
                    "sent": "You might want to learn the structure and you're trying out.",
                    "label": 0
                },
                {
                    "sent": "Structure of the mythologist algorithm and then, well, you marked up some reward factors and you're trying to decompose the real scalar valued signal to some components, but there might not be such a decomposition.",
                    "label": 0
                },
                {
                    "sent": "Oh alright, so let's take our speaker.",
                    "label": 0
                }
            ]
        }
    }
}