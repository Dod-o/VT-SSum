{
    "id": "sktkq7ra4exp7xcpejvsvbxt2ubmqejq",
    "title": "Nearest Neighbors in High-Dimensional Data: The Emergence and Influence of Hubs",
    "info": {
        "author": [
            "Milo\u0161 Radovanovi\u0107, Department of Mathematics and Informatics, University of Novi Sad"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_radovanovic_nnh/",
    "segmentation": [
        [
            "So good afternoon, my name is milosch and I will be speaking about.",
            "My part of my PhD work, which I'm doing at the University of Novi Sad and with collaboration with.",
            "Alexandra Cinepolis from University of Hildesheim and my supervisor media not gonna reach.",
            "So we."
        ],
        [
            "All.",
            "You're probably familiar with the term curse of the mission Ality, which.",
            "Basically refers to.",
            "All the ways a high dimensionality can make life miserable in machine learning, data mining, indexing, data analysis and all all sorts of fields.",
            "And I would like to draw your attention to one particular aspect of Crystal.",
            "The curse of dimensionality, which is distance concentration and this refers to the tendency of distances between.",
            "All pairs of points in a high dimensional data set to become almost equal.",
            "And it has been argued that this affects the very meaningfulness of the notion of nearest neighbors.",
            "And demonstrated that indexing, classification, regression and many other.",
            "Tasks are affected by it, and here are some references.",
            "But we will study related."
        ],
        [
            "Comment to this, which concerns directed graphs of K nearest neighbors in.",
            "In the typical active space settings.",
            "So basically we are we have a data set in a vector space.",
            "Coordinates are real numbers and we have a distance metric and we define this NK of X the number of occurrences of point X.",
            "Simply as a number of times, X occurs among key nearest neighbors of all other points in a data set.",
            "Basically an K of X is the in degree of node X in the key nearest neighbor directed graph.",
            "So we every point is a node and we draw an arc from a node to its nearest neighbors and.",
            "The degrees obviously constant K and in degree is what you're studying here.",
            "And it was observed in several applications.",
            "And that the distribution of NK can become skewed.",
            "And this resulted in the emergence of hubs, that is, points with very high.",
            "And K and the fields in question are music retrieval, speech recognition, fingerprint identification, and there are a couple of others and has generally been recognized as a problematic situation.",
            "So those hubs somehow did not did not seem right."
        ],
        [
            "And basically the question was asked what caused the skewness of NK in those applications.",
            "So one question was, was it an artifact of data so?",
            "Are some songs more similar to others?",
            "Do some people have fingerprints or voices that are hard to distinguish from other peoples?",
            "Or was it specific to the modeling algorithms?",
            "So the choice of features?",
            "Or is there something we should know about Gaussian mixture models?",
            "Or is it something more general and?",
            "You probably guessed from this.",
            "Follow up that we were aiming for this last point."
        ],
        [
            "So to illustrate, we take random data set.",
            "This is, I think, 10,000 points drawn.",
            "The coordinates are drawn from a uniform distribution.",
            "From 01 range in three dimensions and we plot the distribution of five occurrences, that is, the degree in the five nearest neighbor graph.",
            "And we superimpose.",
            "Plots for several distance metrics that is Euclidean L2 norm, the L 0.5 norm.",
            "There is a fractional norm which was suggested for high dimensional data and cost sign distance and we can see that it's basically a binomial distribution which would be expected.",
            "For instance, if the graph was generated as a random graph.",
            "Basically, we didn't randomly choose coordinates, but we randomly picked."
        ],
        [
            "Neighbors.",
            "However, if we increase dimensionality, let's say 20.",
            "The distribution all distributions become skewed to the right and we see that some points have a much larger number of care occurrences or in degree than expected.",
            "Remember the expected value is 5, so here we have one which is over 40 and."
        ],
        [
            "We increased emotionality even further.",
            "We now have to use a log log plots and see that some points have more than 100K occurrences.",
            "So the question is.",
            "I mean, these are random points.",
            "Why is there anything special about them?"
        ],
        [
            "And we try to answer this with these following charts.",
            "So if we plot on the horizontal axis, the distance of a point from the mean of the data set and on the vertical axis the five occurrences.",
            "We can see that in three dimensions that the same uniform data used earlier, there is no correlation, however.",
            "When we increase."
        ],
        [
            "Nationality correlation starts to appear and what you see up there is experiment correlation between the two.",
            "I have the two values and increase the machine."
        ],
        [
            "Further, and we get even stronger correlation."
        ],
        [
            "So the next question is what causes this?",
            "Well, I won't linger too long on this because we're I would like to speak also about the effects of this phenomenon, but.",
            "First, draw your attention to this concentration again so the distance concentration is usually expressed as a ratio between some measure spread.",
            "For example, standard deviation and some measure of magnitude.",
            "For example, the expected value of.",
            "R. Let's say all pairwise distances in a data set, or all distances from distances from all points to one specific point, and if this ratio converges to 0, is the increases it is set at distances concentrate, so it has been shown that as dimensionality increases, basically all points can be referred to as lying approximately on a sphere centered at basically some arbitrary point.",
            "But the data set mean would be the most.",
            "Appropriate as a sphere will be the smallest and distribution of distances to the data set mean.",
            "Regardless of concentration always has some non negligible variance.",
            "So basically even if this variance shrinks to zero as the increases, it will still exist for any finite D. So we can always expect in no matter how many dimensions we have to have points closer to the data set mean then.",
            "Then others.",
            "The other thing is that points which are close closer to the data set mean tend to be close to other points.",
            "Basically, that's something that holds more or less regardless of dimensionality.",
            "But the main point is that this tendency is amplified by high dimensionality.",
            "So basically in high dimensionality, if we have a point which is a little bit closer to the data set mean then then some other point it becomes.",
            "Much closer on average to all other points in the data set, and basically it causes this point to have an advantage when looking for nearest neighbors and filling those nearest neighbors."
        ],
        [
            "Tests.",
            "So these are all other examples basically.",
            "So let's move on to real data and the two important factors for real data are obviously dependent attributes and points are grouped or clustered together.",
            "And we took 50 different datasets from well known repositories and considered Euclidean across time distances as appropriate to the data set and made a bunch of measurements.",
            "For example, we measured the standardized third moment of and 10.",
            "So we fixed K at 10.",
            "This is the third most important way to describe a statistical mean.",
            "Variance sent an skewness, and we also measure the what.",
            "I also showed in the charts this percolation between and then and the distance from the data set mean.",
            "So."
        ],
        [
            "Regarding dependent attributes, we concluded that the skewness of NK depends on intrinsic dimensionality of a data set, so we shouldn't we.",
            "With which have shown this in three different ways, just to be sure, so one way is that we computed the maximum likelihood estimate of interesting dimensionality overall 50 datasets.",
            "And then we looked at the correlations over all the vectors in the 50 datasets of dimensionality and the skewness.",
            "Which is very high, but also of the intrinsic dimensionality of skewness, which is even higher.",
            "An in case that it wasn't there was an accident, we.",
            "Did the same trick that Francois did in his paper, where he argued that distance concentration.",
            "Depends on intrinsic dimensionality and that is we shuffled.",
            "The elements of every attribute, so it might seem a bit crude, but that way we actually keep individual attribute distributions at the same time we lose all dependencies between attributes and basically raise the intrinsic dimensionality to the embedding dimensionality.",
            "And this is just part."
        ],
        [
            "The big Table 50 datasets with columns like N is the number of points these the emotionality, the Emily and so on, and I like to draw your attention to the last two columns.",
            "This is the skewness of the original data, and to write a bit is the skewness of the shuffled data.",
            "So we can see that practically in all instances, the skewness is considerably increased by by shuffling and."
        ],
        [
            "The 3rd way we examined the effect of dimensionality reduction on skewness.",
            "So this chart shows on the vertical axis the skewness of N 10 and the horizontal axis.",
            "Is the number of features we keep by doing PCA, so we can see here for three real datasets, these colored color charts that if we move from right to left, this cuteness stays relatively constant until we reach some fairly fairly low number of features.",
            "And then it's gonna start suddenly drop, which is in contrast with behavior when we have random data where the entrance is embedding dimensionality czar.",
            "Are equal, whereas we reduce the nationality.",
            "We don't do PCA there, we just chop off attributes.",
            "Obviously it has.",
            "Sharper much sharper drop.",
            "The other point here is that if we want to get rid of the skewness phenomenon.",
            "Well, a dimensionality reduction is not something that can help us.",
            "It might not be so simple because we would have to be very."
        ],
        [
            "Messing with emotionally reduction so.",
            "For grouping, we conclude that hubs are in proximity of cluster centers in real data, and for this we did.",
            "We measured speculation between an."
        ],
        [
            "Then and the distance from the closest cluster mean where we perform clustering using K means.",
            "And again, if we look at the two columns, we can see that the correlation from the of the distances cluster mean is usually greater than the correlation with the distance to the data set mean.",
            "And one more thing I would."
        ],
        [
            "To point out is that in high dimensions points with low NK can also be considered distance based outliers 'cause they're far away from other points and.",
            "A good point is that their existence this time is caused actually by high dimensionality.",
            "At the same time, they are very."
        ],
        [
            "Well correlated with, for instance, at least one established notion of a distance based outlier.",
            "So here we show a scatterplot between the distance from the Keith nearest neighbor case 20 and NK, and the parts were interested are the ones that are circled are actually.",
            "Outliers."
        ],
        [
            "And hub skin because their probabilistic outliers actually.",
            "And here if we look at the probability density of the distance from the data set mean of a standard normal distribution.",
            "Which has an analytical form so it can be plotted, thank you.",
            "Hubs are to the left and outliers are to the right and basically both both of them are probabilistically lying in a hurry."
        ],
        [
            "And of lower density.",
            "So let's move on to finally machine learning.",
            "How does this affect machine learning?",
            "Basically for classification?",
            "We distinguish papers into bad ones and good ones based on whether labels don't match or do match.",
            "And what we find is that bad cops can appear.",
            "So basically the question is how do how do they originate and what is their influence on classification algorithms?"
        ],
        [
            "So I will skip the details here and jump straight to the answers.",
            "Where I say the backups originate from a combination of high dimensionality and violation of the cluster assumption.",
            "So cluster assumption."
        ],
        [
            "You're probably all familiar from from semi supervised learning is the.",
            "Uh, is the.",
            "Uh.",
            "Assumption that most pairs of points in a cluster should be of the same class.",
            "So we measured this.",
            "The degree to which a cluster assumption is violated and found that there is very good correlation between the total number of.",
            "The total sum of Vadnais in a data set and the cluster assumption violation and so basically the point is that if you want to violate the cluster assumption, that is if you want to run a decision boundary through.",
            "Through high density region it would be better for you to do it in low dimensions and high dimension because in high dimensions you would get.",
            "You will get bad hubs.",
            "And this influences classifiers, mechanic, classifiers, backups provide erroneous class info."
        ],
        [
            "And to many other points, so we introduce.",
            "Standardized bad hardness.",
            "That is, we take BNK and standardize it, and we wait.",
            "The vote of each neighbor by weighing down bad hubs.",
            "And we basically increase."
        ],
        [
            "Is accuracy on practically every day is that we tried and an SVM is?"
        ],
        [
            "Also consider consider this.",
            "Consider the Gaussian kernel and.",
            "And we we attack.",
            "Actually as we end by progressively moving points from the training sets in the order of decreasing."
        ],
        [
            "Bad scale currencies and what we get is that we ruin the performance of SVM's significantly more than with random removal, and this is because bad hubs tend to be good support vectors.",
            "Thank you again.",
            "I'm in trouble.",
            "So what this table actually means?",
            "Please ask me at the poster session or later."
        ],
        [
            "Also, we examine data boosts which assigns weights to training points to be considered by weak learners, and if the weight is higher than this point is more problematic.",
            "And initially Adaboost sets the weights equal to all equal but."
        ],
        [
            "And it's also known that outliers can harm other Adaboost.",
            "Uh, yes no.",
            "But we also argue that hubs can be problematic and."
        ],
        [
            "What we do is.",
            "Assign initial weights differently then then equal, so we actually weigh down hubs and outliers.",
            "Going to have higher weight to regular points.",
            "And again we achieve a performance performance gains and also if we plot binned accuracies, that is by decreasing K occurrences we see that hubsand outliers are the ones that are getting.",
            "How should I say Adaboost generalizes worse on hubs and outliers?"
        ],
        [
            "So As for clustering, I will just point out that for this and based clustering, there are two objectives minimize within cluster distance and maximize between cluster distance and the skewness of NK effects both objectives.",
            "So for outliers it was known that.",
            "They don't last well because of high within cluster distance, but we also showed that they don't trust hubs don't last well also, but because of low doing cluster distance."
        ],
        [
            "And I will."
        ],
        [
            "Skip the details."
        ],
        [
            "Regrettably and final information retrieval, we retrieve documents most similar to query documents using standard settings.",
            "Backwards representation and causing similarity, and we find that hubs harm precision.",
            "By being persistently there in the top top results.",
            "So we actually do is increase the distance of hubs to all other points where hubs are those points with NK more than two standard deviations large another mean, and we get again precision gains, so the bars the black bars are with distance increments and white bars are without.",
            "So to conclude."
        ],
        [
            "Hopefully in time.",
            "Miss Evenki is what we found.",
            "We think we hope is an understudied phenomenon that can have a strong impact on many things.",
            "Any future work?",
            "I mean that we have many ideas for future work.",
            "Beside the theoretical study of the impact on this is based machine learning example possibly impact on non disease based ML?",
            "Like what does have to do with how can express probabilities, probability distributions?",
            "Seating enter to clustering algorithms, outlier detection, reverse cannon queries, time series mining and finally use skewness of NK to somehow.",
            "Estimate intrinsic dimensionality about data."
        ],
        [
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So good afternoon, my name is milosch and I will be speaking about.",
                    "label": 0
                },
                {
                    "sent": "My part of my PhD work, which I'm doing at the University of Novi Sad and with collaboration with.",
                    "label": 1
                },
                {
                    "sent": "Alexandra Cinepolis from University of Hildesheim and my supervisor media not gonna reach.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All.",
                    "label": 0
                },
                {
                    "sent": "You're probably familiar with the term curse of the mission Ality, which.",
                    "label": 0
                },
                {
                    "sent": "Basically refers to.",
                    "label": 0
                },
                {
                    "sent": "All the ways a high dimensionality can make life miserable in machine learning, data mining, indexing, data analysis and all all sorts of fields.",
                    "label": 0
                },
                {
                    "sent": "And I would like to draw your attention to one particular aspect of Crystal.",
                    "label": 0
                },
                {
                    "sent": "The curse of dimensionality, which is distance concentration and this refers to the tendency of distances between.",
                    "label": 1
                },
                {
                    "sent": "All pairs of points in a high dimensional data set to become almost equal.",
                    "label": 0
                },
                {
                    "sent": "And it has been argued that this affects the very meaningfulness of the notion of nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "And demonstrated that indexing, classification, regression and many other.",
                    "label": 0
                },
                {
                    "sent": "Tasks are affected by it, and here are some references.",
                    "label": 0
                },
                {
                    "sent": "But we will study related.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Comment to this, which concerns directed graphs of K nearest neighbors in.",
                    "label": 0
                },
                {
                    "sent": "In the typical active space settings.",
                    "label": 0
                },
                {
                    "sent": "So basically we are we have a data set in a vector space.",
                    "label": 0
                },
                {
                    "sent": "Coordinates are real numbers and we have a distance metric and we define this NK of X the number of occurrences of point X.",
                    "label": 0
                },
                {
                    "sent": "Simply as a number of times, X occurs among key nearest neighbors of all other points in a data set.",
                    "label": 1
                },
                {
                    "sent": "Basically an K of X is the in degree of node X in the key nearest neighbor directed graph.",
                    "label": 0
                },
                {
                    "sent": "So we every point is a node and we draw an arc from a node to its nearest neighbors and.",
                    "label": 0
                },
                {
                    "sent": "The degrees obviously constant K and in degree is what you're studying here.",
                    "label": 0
                },
                {
                    "sent": "And it was observed in several applications.",
                    "label": 1
                },
                {
                    "sent": "And that the distribution of NK can become skewed.",
                    "label": 0
                },
                {
                    "sent": "And this resulted in the emergence of hubs, that is, points with very high.",
                    "label": 0
                },
                {
                    "sent": "And K and the fields in question are music retrieval, speech recognition, fingerprint identification, and there are a couple of others and has generally been recognized as a problematic situation.",
                    "label": 0
                },
                {
                    "sent": "So those hubs somehow did not did not seem right.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And basically the question was asked what caused the skewness of NK in those applications.",
                    "label": 1
                },
                {
                    "sent": "So one question was, was it an artifact of data so?",
                    "label": 0
                },
                {
                    "sent": "Are some songs more similar to others?",
                    "label": 1
                },
                {
                    "sent": "Do some people have fingerprints or voices that are hard to distinguish from other peoples?",
                    "label": 1
                },
                {
                    "sent": "Or was it specific to the modeling algorithms?",
                    "label": 0
                },
                {
                    "sent": "So the choice of features?",
                    "label": 0
                },
                {
                    "sent": "Or is there something we should know about Gaussian mixture models?",
                    "label": 0
                },
                {
                    "sent": "Or is it something more general and?",
                    "label": 0
                },
                {
                    "sent": "You probably guessed from this.",
                    "label": 0
                },
                {
                    "sent": "Follow up that we were aiming for this last point.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to illustrate, we take random data set.",
                    "label": 0
                },
                {
                    "sent": "This is, I think, 10,000 points drawn.",
                    "label": 0
                },
                {
                    "sent": "The coordinates are drawn from a uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "From 01 range in three dimensions and we plot the distribution of five occurrences, that is, the degree in the five nearest neighbor graph.",
                    "label": 0
                },
                {
                    "sent": "And we superimpose.",
                    "label": 0
                },
                {
                    "sent": "Plots for several distance metrics that is Euclidean L2 norm, the L 0.5 norm.",
                    "label": 0
                },
                {
                    "sent": "There is a fractional norm which was suggested for high dimensional data and cost sign distance and we can see that it's basically a binomial distribution which would be expected.",
                    "label": 0
                },
                {
                    "sent": "For instance, if the graph was generated as a random graph.",
                    "label": 0
                },
                {
                    "sent": "Basically, we didn't randomly choose coordinates, but we randomly picked.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Neighbors.",
                    "label": 0
                },
                {
                    "sent": "However, if we increase dimensionality, let's say 20.",
                    "label": 0
                },
                {
                    "sent": "The distribution all distributions become skewed to the right and we see that some points have a much larger number of care occurrences or in degree than expected.",
                    "label": 0
                },
                {
                    "sent": "Remember the expected value is 5, so here we have one which is over 40 and.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We increased emotionality even further.",
                    "label": 0
                },
                {
                    "sent": "We now have to use a log log plots and see that some points have more than 100K occurrences.",
                    "label": 0
                },
                {
                    "sent": "So the question is.",
                    "label": 0
                },
                {
                    "sent": "I mean, these are random points.",
                    "label": 0
                },
                {
                    "sent": "Why is there anything special about them?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we try to answer this with these following charts.",
                    "label": 0
                },
                {
                    "sent": "So if we plot on the horizontal axis, the distance of a point from the mean of the data set and on the vertical axis the five occurrences.",
                    "label": 0
                },
                {
                    "sent": "We can see that in three dimensions that the same uniform data used earlier, there is no correlation, however.",
                    "label": 0
                },
                {
                    "sent": "When we increase.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nationality correlation starts to appear and what you see up there is experiment correlation between the two.",
                    "label": 0
                },
                {
                    "sent": "I have the two values and increase the machine.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Further, and we get even stronger correlation.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the next question is what causes this?",
                    "label": 0
                },
                {
                    "sent": "Well, I won't linger too long on this because we're I would like to speak also about the effects of this phenomenon, but.",
                    "label": 0
                },
                {
                    "sent": "First, draw your attention to this concentration again so the distance concentration is usually expressed as a ratio between some measure spread.",
                    "label": 0
                },
                {
                    "sent": "For example, standard deviation and some measure of magnitude.",
                    "label": 0
                },
                {
                    "sent": "For example, the expected value of.",
                    "label": 0
                },
                {
                    "sent": "R. Let's say all pairwise distances in a data set, or all distances from distances from all points to one specific point, and if this ratio converges to 0, is the increases it is set at distances concentrate, so it has been shown that as dimensionality increases, basically all points can be referred to as lying approximately on a sphere centered at basically some arbitrary point.",
                    "label": 0
                },
                {
                    "sent": "But the data set mean would be the most.",
                    "label": 0
                },
                {
                    "sent": "Appropriate as a sphere will be the smallest and distribution of distances to the data set mean.",
                    "label": 1
                },
                {
                    "sent": "Regardless of concentration always has some non negligible variance.",
                    "label": 0
                },
                {
                    "sent": "So basically even if this variance shrinks to zero as the increases, it will still exist for any finite D. So we can always expect in no matter how many dimensions we have to have points closer to the data set mean then.",
                    "label": 0
                },
                {
                    "sent": "Then others.",
                    "label": 0
                },
                {
                    "sent": "The other thing is that points which are close closer to the data set mean tend to be close to other points.",
                    "label": 1
                },
                {
                    "sent": "Basically, that's something that holds more or less regardless of dimensionality.",
                    "label": 0
                },
                {
                    "sent": "But the main point is that this tendency is amplified by high dimensionality.",
                    "label": 0
                },
                {
                    "sent": "So basically in high dimensionality, if we have a point which is a little bit closer to the data set mean then then some other point it becomes.",
                    "label": 0
                },
                {
                    "sent": "Much closer on average to all other points in the data set, and basically it causes this point to have an advantage when looking for nearest neighbors and filling those nearest neighbors.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tests.",
                    "label": 0
                },
                {
                    "sent": "So these are all other examples basically.",
                    "label": 0
                },
                {
                    "sent": "So let's move on to real data and the two important factors for real data are obviously dependent attributes and points are grouped or clustered together.",
                    "label": 1
                },
                {
                    "sent": "And we took 50 different datasets from well known repositories and considered Euclidean across time distances as appropriate to the data set and made a bunch of measurements.",
                    "label": 0
                },
                {
                    "sent": "For example, we measured the standardized third moment of and 10.",
                    "label": 0
                },
                {
                    "sent": "So we fixed K at 10.",
                    "label": 0
                },
                {
                    "sent": "This is the third most important way to describe a statistical mean.",
                    "label": 0
                },
                {
                    "sent": "Variance sent an skewness, and we also measure the what.",
                    "label": 0
                },
                {
                    "sent": "I also showed in the charts this percolation between and then and the distance from the data set mean.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Regarding dependent attributes, we concluded that the skewness of NK depends on intrinsic dimensionality of a data set, so we shouldn't we.",
                    "label": 1
                },
                {
                    "sent": "With which have shown this in three different ways, just to be sure, so one way is that we computed the maximum likelihood estimate of interesting dimensionality overall 50 datasets.",
                    "label": 0
                },
                {
                    "sent": "And then we looked at the correlations over all the vectors in the 50 datasets of dimensionality and the skewness.",
                    "label": 0
                },
                {
                    "sent": "Which is very high, but also of the intrinsic dimensionality of skewness, which is even higher.",
                    "label": 0
                },
                {
                    "sent": "An in case that it wasn't there was an accident, we.",
                    "label": 0
                },
                {
                    "sent": "Did the same trick that Francois did in his paper, where he argued that distance concentration.",
                    "label": 0
                },
                {
                    "sent": "Depends on intrinsic dimensionality and that is we shuffled.",
                    "label": 0
                },
                {
                    "sent": "The elements of every attribute, so it might seem a bit crude, but that way we actually keep individual attribute distributions at the same time we lose all dependencies between attributes and basically raise the intrinsic dimensionality to the embedding dimensionality.",
                    "label": 0
                },
                {
                    "sent": "And this is just part.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The big Table 50 datasets with columns like N is the number of points these the emotionality, the Emily and so on, and I like to draw your attention to the last two columns.",
                    "label": 0
                },
                {
                    "sent": "This is the skewness of the original data, and to write a bit is the skewness of the shuffled data.",
                    "label": 0
                },
                {
                    "sent": "So we can see that practically in all instances, the skewness is considerably increased by by shuffling and.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The 3rd way we examined the effect of dimensionality reduction on skewness.",
                    "label": 1
                },
                {
                    "sent": "So this chart shows on the vertical axis the skewness of N 10 and the horizontal axis.",
                    "label": 0
                },
                {
                    "sent": "Is the number of features we keep by doing PCA, so we can see here for three real datasets, these colored color charts that if we move from right to left, this cuteness stays relatively constant until we reach some fairly fairly low number of features.",
                    "label": 0
                },
                {
                    "sent": "And then it's gonna start suddenly drop, which is in contrast with behavior when we have random data where the entrance is embedding dimensionality czar.",
                    "label": 0
                },
                {
                    "sent": "Are equal, whereas we reduce the nationality.",
                    "label": 0
                },
                {
                    "sent": "We don't do PCA there, we just chop off attributes.",
                    "label": 0
                },
                {
                    "sent": "Obviously it has.",
                    "label": 0
                },
                {
                    "sent": "Sharper much sharper drop.",
                    "label": 0
                },
                {
                    "sent": "The other point here is that if we want to get rid of the skewness phenomenon.",
                    "label": 0
                },
                {
                    "sent": "Well, a dimensionality reduction is not something that can help us.",
                    "label": 0
                },
                {
                    "sent": "It might not be so simple because we would have to be very.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Messing with emotionally reduction so.",
                    "label": 0
                },
                {
                    "sent": "For grouping, we conclude that hubs are in proximity of cluster centers in real data, and for this we did.",
                    "label": 1
                },
                {
                    "sent": "We measured speculation between an.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then and the distance from the closest cluster mean where we perform clustering using K means.",
                    "label": 0
                },
                {
                    "sent": "And again, if we look at the two columns, we can see that the correlation from the of the distances cluster mean is usually greater than the correlation with the distance to the data set mean.",
                    "label": 0
                },
                {
                    "sent": "And one more thing I would.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To point out is that in high dimensions points with low NK can also be considered distance based outliers 'cause they're far away from other points and.",
                    "label": 1
                },
                {
                    "sent": "A good point is that their existence this time is caused actually by high dimensionality.",
                    "label": 0
                },
                {
                    "sent": "At the same time, they are very.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well correlated with, for instance, at least one established notion of a distance based outlier.",
                    "label": 0
                },
                {
                    "sent": "So here we show a scatterplot between the distance from the Keith nearest neighbor case 20 and NK, and the parts were interested are the ones that are circled are actually.",
                    "label": 0
                },
                {
                    "sent": "Outliers.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And hub skin because their probabilistic outliers actually.",
                    "label": 1
                },
                {
                    "sent": "And here if we look at the probability density of the distance from the data set mean of a standard normal distribution.",
                    "label": 0
                },
                {
                    "sent": "Which has an analytical form so it can be plotted, thank you.",
                    "label": 1
                },
                {
                    "sent": "Hubs are to the left and outliers are to the right and basically both both of them are probabilistically lying in a hurry.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And of lower density.",
                    "label": 0
                },
                {
                    "sent": "So let's move on to finally machine learning.",
                    "label": 0
                },
                {
                    "sent": "How does this affect machine learning?",
                    "label": 1
                },
                {
                    "sent": "Basically for classification?",
                    "label": 0
                },
                {
                    "sent": "We distinguish papers into bad ones and good ones based on whether labels don't match or do match.",
                    "label": 1
                },
                {
                    "sent": "And what we find is that bad cops can appear.",
                    "label": 0
                },
                {
                    "sent": "So basically the question is how do how do they originate and what is their influence on classification algorithms?",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I will skip the details here and jump straight to the answers.",
                    "label": 0
                },
                {
                    "sent": "Where I say the backups originate from a combination of high dimensionality and violation of the cluster assumption.",
                    "label": 0
                },
                {
                    "sent": "So cluster assumption.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You're probably all familiar from from semi supervised learning is the.",
                    "label": 0
                },
                {
                    "sent": "Uh, is the.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Assumption that most pairs of points in a cluster should be of the same class.",
                    "label": 1
                },
                {
                    "sent": "So we measured this.",
                    "label": 1
                },
                {
                    "sent": "The degree to which a cluster assumption is violated and found that there is very good correlation between the total number of.",
                    "label": 0
                },
                {
                    "sent": "The total sum of Vadnais in a data set and the cluster assumption violation and so basically the point is that if you want to violate the cluster assumption, that is if you want to run a decision boundary through.",
                    "label": 0
                },
                {
                    "sent": "Through high density region it would be better for you to do it in low dimensions and high dimension because in high dimensions you would get.",
                    "label": 0
                },
                {
                    "sent": "You will get bad hubs.",
                    "label": 1
                },
                {
                    "sent": "And this influences classifiers, mechanic, classifiers, backups provide erroneous class info.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And to many other points, so we introduce.",
                    "label": 1
                },
                {
                    "sent": "Standardized bad hardness.",
                    "label": 0
                },
                {
                    "sent": "That is, we take BNK and standardize it, and we wait.",
                    "label": 0
                },
                {
                    "sent": "The vote of each neighbor by weighing down bad hubs.",
                    "label": 1
                },
                {
                    "sent": "And we basically increase.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is accuracy on practically every day is that we tried and an SVM is?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also consider consider this.",
                    "label": 0
                },
                {
                    "sent": "Consider the Gaussian kernel and.",
                    "label": 1
                },
                {
                    "sent": "And we we attack.",
                    "label": 0
                },
                {
                    "sent": "Actually as we end by progressively moving points from the training sets in the order of decreasing.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bad scale currencies and what we get is that we ruin the performance of SVM's significantly more than with random removal, and this is because bad hubs tend to be good support vectors.",
                    "label": 1
                },
                {
                    "sent": "Thank you again.",
                    "label": 0
                },
                {
                    "sent": "I'm in trouble.",
                    "label": 0
                },
                {
                    "sent": "So what this table actually means?",
                    "label": 0
                },
                {
                    "sent": "Please ask me at the poster session or later.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, we examine data boosts which assigns weights to training points to be considered by weak learners, and if the weight is higher than this point is more problematic.",
                    "label": 0
                },
                {
                    "sent": "And initially Adaboost sets the weights equal to all equal but.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it's also known that outliers can harm other Adaboost.",
                    "label": 0
                },
                {
                    "sent": "Uh, yes no.",
                    "label": 0
                },
                {
                    "sent": "But we also argue that hubs can be problematic and.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we do is.",
                    "label": 0
                },
                {
                    "sent": "Assign initial weights differently then then equal, so we actually weigh down hubs and outliers.",
                    "label": 0
                },
                {
                    "sent": "Going to have higher weight to regular points.",
                    "label": 0
                },
                {
                    "sent": "And again we achieve a performance performance gains and also if we plot binned accuracies, that is by decreasing K occurrences we see that hubsand outliers are the ones that are getting.",
                    "label": 0
                },
                {
                    "sent": "How should I say Adaboost generalizes worse on hubs and outliers?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So As for clustering, I will just point out that for this and based clustering, there are two objectives minimize within cluster distance and maximize between cluster distance and the skewness of NK effects both objectives.",
                    "label": 1
                },
                {
                    "sent": "So for outliers it was known that.",
                    "label": 0
                },
                {
                    "sent": "They don't last well because of high within cluster distance, but we also showed that they don't trust hubs don't last well also, but because of low doing cluster distance.",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I will.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Skip the details.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Regrettably and final information retrieval, we retrieve documents most similar to query documents using standard settings.",
                    "label": 1
                },
                {
                    "sent": "Backwards representation and causing similarity, and we find that hubs harm precision.",
                    "label": 0
                },
                {
                    "sent": "By being persistently there in the top top results.",
                    "label": 0
                },
                {
                    "sent": "So we actually do is increase the distance of hubs to all other points where hubs are those points with NK more than two standard deviations large another mean, and we get again precision gains, so the bars the black bars are with distance increments and white bars are without.",
                    "label": 0
                },
                {
                    "sent": "So to conclude.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hopefully in time.",
                    "label": 0
                },
                {
                    "sent": "Miss Evenki is what we found.",
                    "label": 0
                },
                {
                    "sent": "We think we hope is an understudied phenomenon that can have a strong impact on many things.",
                    "label": 1
                },
                {
                    "sent": "Any future work?",
                    "label": 0
                },
                {
                    "sent": "I mean that we have many ideas for future work.",
                    "label": 0
                },
                {
                    "sent": "Beside the theoretical study of the impact on this is based machine learning example possibly impact on non disease based ML?",
                    "label": 0
                },
                {
                    "sent": "Like what does have to do with how can express probabilities, probability distributions?",
                    "label": 1
                },
                {
                    "sent": "Seating enter to clustering algorithms, outlier detection, reverse cannon queries, time series mining and finally use skewness of NK to somehow.",
                    "label": 0
                },
                {
                    "sent": "Estimate intrinsic dimensionality about data.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}