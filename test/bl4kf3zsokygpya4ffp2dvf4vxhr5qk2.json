{
    "id": "bl4kf3zsokygpya4ffp2dvf4vxhr5qk2",
    "title": "The Lazy Traveling Salesman - Memory Management for Large - Scale Link Discovery",
    "info": {
        "author": [
            "Axel-Cyrille Ngonga Ngomo, University of Leipzig"
        ],
        "published": "July 28, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2016_ngonga_ngomo_link_discovery/",
    "segmentation": [
        [
            "And I'm going to talk about memory management for large scale link discovery and or get you to spot where the lazy traveling salesman actually comes into play.",
            "And this is work that was done together with our PhD student Morfydd has."
        ],
        [
            "I'm just going to delve right into business.",
            "I think we all know what Link discovery is and why it is important.",
            "I mean, we have plenty of data available on the link data web on the web.",
            "In general we want to connect these datasets so as to be able to carry out a very complex tasks.",
            "Formally, the way we defining discovery is that we are given two sets of resources as anti as well as a certain relation R and what we are to find is a set M. Of pairs St that are such that RST holds.",
            "Now this is a tricky problem.",
            "A difficult problem for a lot of relations, which means that declarative solutions funding discovery actually apply an approximation approach where they basically say OK, I'm going to find a set M prime that is such that St is in M prime if a certain complex similarity measure for S&T is larger than the value Theta.",
            "Imagine, for example, that you are a guitar sales retailer.",
            "You get guitars from a lot of different companies, so you get different RDF files from them that describe the details.",
            "The guitar that you got I want to connect these datasets to basically see what I have to say.",
            "Model of guitars.",
            "So basically here your relation would be same model.",
            "You would get these things.",
            "You would basically most commonly look at the description of these things in RDF and try to find the similarity function that is such that it basically gives you the same model.",
            "This case would be something like looking at the model name and maybe even looking at the label of this set instrument.",
            "Huawei is."
        ],
        [
            "Link discovery difficult, but are several reasons for it.",
            "One is the time complexity issue.",
            "If you have two large knowledge bases and you really try to do this comparison in a naive way, you basically end up doing North Square comparisons, which takes a lot of time.",
            "Simple example, if a similarity comparison takes one millisecond and you try to compare cities from DB Pedia and Joe names in a naive way will take approximately 69 days.",
            "To run that computation, and that's just for two classes of these datasets, imagine what would happen if you try to do that on the whole link data cloud.",
            "Now, a lot of efficient approaches have been developed over the last years to actually deal with this time complexity problem will get into why.",
            "In some cases the implementations don't work, so that's what we're mainly going to deal with today.",
            "But for the sake of completeness I also have to point out that the second problem that we have is actually the complexity of specifications you finding that complex similarity function.",
            "Talked about as well as to set threshold so that we can actually find a set M prime that approximate EMS the values they set and are actually looking for.",
            "Well, it's a complex problem.",
            "There's a lot of papers on the subject matter we're not going to be concerned with that."
        ],
        [
            "Today so the major problem we're going to look into today is what happens if the datasets that I have in my source at my target do not fit in memory?",
            "This is a common problem because we have really large datasets available on the linked data web nowadays.",
            "For example, the largest datasets contain more than 20 billion triples, and we know that we have approximately 150 billion triples already available and indexed, and that's just basically the top of the iceberg.",
            "However, not all data providers have huge infrastructures to actually run really completely linking tasks using a lot of memory.",
            "That's the one for 100 problem.",
            "On the other hand, is most of the solutions that exist for link discovery.",
            "Assume that you have enough memory.",
            "So basically they're all in memory solutions, so the problem becomes, how do we actually deal with this problem of linking when we don't have enough memory?",
            "How do we then?",
            "Rule over the interplay between the memory and the hard drive."
        ],
        [
            "So our goal was to devise a generic, time efficient approach to compute this set in prime.",
            "This approximation of the mapping that we actually looking for for cases where S + C is larger than the size of CCS or memory South is our set of sources aunties or set of targets, i.e.",
            "When I sent it on fit in memory, we wanted to ensure the completeness of results.",
            "I even wanted to carry out all the tasks that are necessary to actually compute the set and prime and the main bottleneck that we had to address was basically.",
            "The efficient access to the hard drive you actually want to minimize the number of times that we actually fetch data from the hard drive into memory, and we call our solution Chrome so that I can show it this nice little picture."
        ],
        [
            "OK. How do we go about doing that?",
            "The first insight that we used, or that we looked upon was that most approaches rely most time efficient approaches rely on a divide and merge paradigm, and I'm just going to explain to you how these paradigms work with an example before then giving you the formal details of what I actually mean.",
            "So one of the examples of time efficient algorithms for link discoveries.",
            "HR three.",
            "It deals basically with data that is distributed within a set in Euclidean space.",
            "Of any space, really Minkowski metrics.",
            "And then the idea is that you can't.",
            "We can translate this condition that says the similarity of S of T&T must be larger or equal to Theta two basically a distance condition where we set distance between SMT must be less or equal than a value Delta and this value Delta is basically with of any of these squares.",
            "Now for a given subset of S which is found in this particular square.",
            "So the square you see this larger point.",
            "Really, we know that all the solutions I oddities that are such that the distance from S2 to be less or equal to Delta actually to be found in this larger square.",
            "So what we can do is we can tier or space into squares 2 dimensional squares if it's more dimensional is basically hyper cubes.",
            "Hypercubes off with Delta.",
            "We can then look at all the hypercubes around the hypercube, where the subset of South is that we're particularly interested in at the moment, and run our computations there.",
            "So we actually divide the space.",
            "We've run our computations within that those little blocks of space, and then we merge the results at the end to actually get the final results.",
            "And with that we can.",
            "Actually, it's actually proven that the results are complete."
        ],
        [
            "So formally, what a lot of our purchase do is simply define a set S which is a set of subsets of input source data Set S with the condition that the Union of these subsets actually is the S that we had at the beginning.",
            "The set of sources that do the same for the target, and this is the trick.",
            "This is what is the most important actually for these approaches that define a mapping between the set of subsets of S. And the power set of the set of subsets of T and these particular mapping has particular characteristics.",
            "The elements of SI only need to be compared with the elements in the set that are in U of SI and the Union of the results.",
            "Overall, yes, eyes actually gives you this state and the set M prime there enough proofs in paper related to that.",
            "What this means here is that this Blue Square would be one of the SI, so basically.",
            "One of the portions of Esther very interested in and the mu function would actually give us the 8 red squares plus the Blue Square and say this is basically the set of keys with which we need to compare this particular SI to get or the result."
        ],
        [
            "We can then translate this problem basically to a set of tasks.",
            "So if you have all the size and we have all them you oversize or we can basically say you have a mapping between those and the tasks.",
            "Basically consists of comparing each XI, which each TJ is in U of SI.",
            "This can basically be translated into a task graph.",
            "It is rather obvious I think.",
            "So the set of vertices is basically as follows.",
            "As Union T we have a weight function for the nodes of the graph, which basically tells us how many elements we find in there.",
            "So if we."
        ],
        [
            "Over here, if South is there Ms Office are the blue dots and elements of share the red dots.",
            "You basically simply count the number of blue dots that you have here.",
            "That gives you the size of Cy and if you want to get the size of the T duties in this square.",
            "Basically simply count the number of red."
        ],
        [
            "Dot alright, so for the sake of simplicity or using smaller graph and the types of graphs that you actually get when you run this kind of computations, let's just assume that we have S1S2 and S3.",
            "These are the sizes of the different subsets and we have T1T2 and G3.",
            "Now given the new function, we can actually now start building the task graph.",
            "The weight of the edges between 2 between SSI and TJ.",
            "Is basically simply the size of times the size of TJ and it tells us how many computations will actually have to run to basically complete this task and we can build it up and we get such a task graph.",
            "I hope that's clear so far."
        ],
        [
            "Now the whole idea then becomes how do we actually traverse this graph in an intelligent manner intelligent manner, meaning what we actually want to do is run the tasks in such a way that the computer only fetches data from memory really from the hard drive really rarely.",
            "So we want basically asked the memory the data to stay in memory as long as possible.",
            "The approach that we propose has two steps.",
            "The first step is a clustering where we basically find groups of nodes that fit in memory.",
            "Basically want to execute together and discuss scheduling step.",
            "We basically want to find the right sequences of groups of nodes so as to maintain as much data in memory as possible and as I said we'll use these little graph as example."
        ],
        [
            "So step one clustering.",
            "How do we go about doing that?",
            "There obviously a lot of possible approaches here.",
            "We looked at two approaches, a naive approach and a greedy approach.",
            "The reason why we took simple approaches is basically due simply to the size of the task graphs that you get in the examples that ask, Ross had millions of edges, so any approach that has a complexity that goes in the quadratics already becomes rather difficult to use.",
            "It produces too much overhead.",
            "So the naive approach is actually rather simple.",
            "What it does is it looks at each XI and basically tries to collect all the TJ's around this as I connected to this essay, there are such that the total size of SI plus the corresponding TJ's is less or equal than C and that is basically a cluster.",
            "So if we were to start at S1, we would basically.",
            "Take T1 and T2 these sizes here at 3 + 2 + 1, which is 6 that fits the memory.",
            "There's no more edge 4X4 S one.",
            "We basically say that's a cluster.",
            "Go to the next node, which is S2.",
            "We take T-12.",
            "One other size of two.",
            "We have S2 in there.",
            "That's already four.",
            "We take T2.",
            "That's five, and there's no other edge that we can actually take that is such that the group is as a size less than C, which has a size of seven in this example, which basically means we go to the next group.",
            "And that would be the Orange Group, and that's basically the kind of edge labeling edge coloring that we would get."
        ],
        [
            "One can obviously imagine a greedy approach in this case that more looks at the weights of the edges.",
            "So here would basically start by taking the edge that has the highest waist weight, i.e.",
            "The largest tasks that would be that blue edge, and we're basically trying to find edges that are connected to this edge virus via nodes, and that are the largest possible and that we will basically group the data.",
            "What happens here is that this already has a size of seven, so we can't fit more memory.",
            "We now have to start the next group.",
            "We basically started up there, and as you can see, we travel.",
            "We traverse the graph.",
            "These some of these weights of these three nodes is basically 7, so will stop there and continue coloring or graph.",
            "So with that, we've grouped the nodes that was basically the first thing we wanted to do.",
            "Now we have groups of nodes.",
            "We want to find the right sequence, an."
        ],
        [
            "This is obviously a rather tricky problem.",
            "So the output of our clustering is a sequence of clusters and we want to find a sequence of clusters that is such that consecutive clusters share as much data as possible.",
            "The more data they share, the less data we actually have to fetch from the hard drive to be able to execute the tasks within the group.",
            "So we want to maximize the overlap of the generated sequence.",
            "What I'm informally is the following.",
            "If we take two groups of node GINGJ, the overlap is basically the number of elements within the vertex.",
            "The vertices that they share.",
            "So if we look at these two groups, we have the Orange Group right there, and you have the blue Group.",
            "Here the overlap of G4 and G1 would be 4 simply because they share is 3.",
            "And we can obviously based on that define the overlap function for a sequence that's basically the sum of the overlaps of consecutive groups.",
            "So if you look at the sequence G4G 1G2.",
            "Apologies G for this should be G3.",
            "My mistake then the number would be 9.",
            "That would basically be the overlap value that we have here."
        ],
        [
            "So how do we go about doing that?",
            "Again, we want approaches that are simple but approaches good results.",
            "The first approach that we suggest is a best effort based approach.",
            "You basically give the approach at certain time, say.",
            "Optimize for within this time frame and then give me the sequence that you found.",
            "We start with the sequence of nodes that came from the clustering and what we basically do is simply random permutations.",
            "We take two groups within the sequence.",
            "We swap them and we check whether the overlap function actually has improved or not.",
            "If it has improved, then we keep the sequence.",
            "If not, we simply let the nodes where they were and try another pay.",
            "Now what's important here is that we do not need to know.",
            "The global value of the overlap function.",
            "If we needed to do that, it would mean that would have to check the whole sequence and that will simply take too long.",
            "What we can use is it Rick and what this basically says.",
            "This nice little question.",
            "All it says is that if you take 2 nodes from a sequence and you swap those nodes, actually only interested in the difference that you produce in the local boundaries at the left and right of each of these nodes two and if that value actually increases.",
            "There is no doubt your overall function increases because you don't change the overlap of any other parts.",
            "So if you look at this sequence where we have one overlap of 0, three and two by simply swapping G4, Angie, three, we actually have a case where we increase the overall overlap score."
        ],
        [
            "We can obviously also imagine a greedy approach which starts with a random cluster, then takes the next cluster with the largest overlap and basically runs across the whole set of clusters.",
            "That way.",
            "Major problem here, however, is that we need global knowledge because we need to know what is the best cluster for a particular cluster, i.e.",
            "The closer the highest overlap.",
            "But with that we can also produce sequences that are pretty good.",
            "So with that we've actually covered to the two steps that we were interested in.",
            "The question is how do we perform so the way they approach runs is we have the sequence, the sequence of tasks.",
            "We basically define the sequence in which two groups should be executed and the memory works like a cache, which basically means that we ask the memory, hey, tell me, do you have this group?",
            "If the memory has the group then basically returns it to the computer basically then computes this similarity scores.",
            "If the groups are not available, that's basically the moment where we have to look into the hard drive.",
            "Which basically means that we can compare our approach with existing.",
            "Caching mechanisms."
        ],
        [
            "So the datasets that we used where 1 million labels from DBP Generations version of April 2015 and 0.8 million places from link geodata, I think the hardware is relatively straightforward.",
            "What we measured, where the total runtime as well as the hit ratio, i.e.",
            "How often were we able to find things on in memory?",
            "How often did we have not to actually access?"
        ],
        [
            "The hard drive.",
            "So we started by letting the clustering approach and as expected.",
            "Basically the greedy approach, which is here has the better hit ratio, which basically means that it is able to put data where it should be so that we don't have to access the hard drive too often.",
            "However, I have to say here we basically just considered 1000 points, i.e the size of S and the size of the way 1000.",
            "Yeah, there were 10,000 for the pre experiments.",
            "What we see here we started over.",
            "Time explodes if you basically go to a 10 times more data, you get approximately 100 times the runtime, which is definitely not something that you want to do so."
        ],
        [
            "So although the naive approach is more efficient, the greedy approach is more effective, but has such a high overhead that is actually pointless to use it.",
            "So we use the naive approach."
        ],
        [
            "For clustering.",
            "And when we look at scheduling, we basically get this similar picture, which basically means that the best effort approach does not perform as well pertaining to hit ratios.",
            "However, it scales well enough on large datasets that we actually had to use it.",
            "So the best effort approach is more time efficient.",
            "The greedy approach is more effective, but the best of what approach is what we used for scheduling."
        ],
        [
            "As I said before, approach is very similar to caching approaches, so we could actually compare our works with standard caching solutions that are available out there and what we were able to show is that approach actually organizes data well enough that we could be significantly better than the existing standard caching solutions, which is actually the goal we wanted to reach.",
            "In some cases there hit ratio was not as good As for example as well.",
            "If you in this case that's again with 10,000.",
            "Items in memory, but overall our approach performed better respect through runtime, so we were more time efficient and we actually have higher hit rates in most cases."
        ],
        [
            "Last thing that we wanted to know is can't, does it mean that we can actually know discovery with limited memory on really large datasets and we were able to show that we can.",
            "Indeed we basically increase the size of the data that we had in memory.",
            "Basically 100,000 to 800,000 points.",
            "We used standard hardware, basically with 10 gigabytes of RAM and the data definitely did not fit in there and we're still able to run link discovery on that one interesting result.",
            "It was the sub quadratic growth of runtime, which basically means that normally you would expect that if you increase the amount of data by two actually or runtime because of the problem, quadratic runtime should increase by approximately 4.",
            "What we could show is that we are basically under that runtime runtime is more linear with the number of mappings that needs to be generated, which is nice.",
            "So for example for LGD falling Geo data, we were able to generate between 360.",
            "And 370 mappings per second.",
            "That was iterate on."
        ],
        [
            "What we did.",
            "So to conclude, I presented Norm which is a two step approach for link discovery that allows you to run link discovery on large amounts of data with limited amounts of memory.",
            "It basically relies on the divide and merge paradigm and ensures that link data sets of arbitrary size can be run even mid small memory.",
            "Well, the one thing that we did not look into and that we definitely plan to look into is parallel implementations.",
            "So the whole idea of working with this graph when you then have to run with server processors.",
            "Becomes really, really interesting, especially when you think of even larger architectures where you have in memory distributed memory solutions such as.",
            "Spark.",
            "Obviously we would like to look into blocking an order.",
            "Similar approaches and see whether we can use that there.",
            "So basically when M prime does not have to be computed, complete."
        ],
        [
            "That's it from my side.",
            "Thank you very much for listing and I do invite you to The Hobbit event where we talk about these things even more.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'm going to talk about memory management for large scale link discovery and or get you to spot where the lazy traveling salesman actually comes into play.",
                    "label": 0
                },
                {
                    "sent": "And this is work that was done together with our PhD student Morfydd has.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm just going to delve right into business.",
                    "label": 0
                },
                {
                    "sent": "I think we all know what Link discovery is and why it is important.",
                    "label": 0
                },
                {
                    "sent": "I mean, we have plenty of data available on the link data web on the web.",
                    "label": 0
                },
                {
                    "sent": "In general we want to connect these datasets so as to be able to carry out a very complex tasks.",
                    "label": 0
                },
                {
                    "sent": "Formally, the way we defining discovery is that we are given two sets of resources as anti as well as a certain relation R and what we are to find is a set M. Of pairs St that are such that RST holds.",
                    "label": 0
                },
                {
                    "sent": "Now this is a tricky problem.",
                    "label": 0
                },
                {
                    "sent": "A difficult problem for a lot of relations, which means that declarative solutions funding discovery actually apply an approximation approach where they basically say OK, I'm going to find a set M prime that is such that St is in M prime if a certain complex similarity measure for S&T is larger than the value Theta.",
                    "label": 0
                },
                {
                    "sent": "Imagine, for example, that you are a guitar sales retailer.",
                    "label": 0
                },
                {
                    "sent": "You get guitars from a lot of different companies, so you get different RDF files from them that describe the details.",
                    "label": 0
                },
                {
                    "sent": "The guitar that you got I want to connect these datasets to basically see what I have to say.",
                    "label": 0
                },
                {
                    "sent": "Model of guitars.",
                    "label": 0
                },
                {
                    "sent": "So basically here your relation would be same model.",
                    "label": 0
                },
                {
                    "sent": "You would get these things.",
                    "label": 0
                },
                {
                    "sent": "You would basically most commonly look at the description of these things in RDF and try to find the similarity function that is such that it basically gives you the same model.",
                    "label": 0
                },
                {
                    "sent": "This case would be something like looking at the model name and maybe even looking at the label of this set instrument.",
                    "label": 0
                },
                {
                    "sent": "Huawei is.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Link discovery difficult, but are several reasons for it.",
                    "label": 0
                },
                {
                    "sent": "One is the time complexity issue.",
                    "label": 1
                },
                {
                    "sent": "If you have two large knowledge bases and you really try to do this comparison in a naive way, you basically end up doing North Square comparisons, which takes a lot of time.",
                    "label": 0
                },
                {
                    "sent": "Simple example, if a similarity comparison takes one millisecond and you try to compare cities from DB Pedia and Joe names in a naive way will take approximately 69 days.",
                    "label": 1
                },
                {
                    "sent": "To run that computation, and that's just for two classes of these datasets, imagine what would happen if you try to do that on the whole link data cloud.",
                    "label": 0
                },
                {
                    "sent": "Now, a lot of efficient approaches have been developed over the last years to actually deal with this time complexity problem will get into why.",
                    "label": 0
                },
                {
                    "sent": "In some cases the implementations don't work, so that's what we're mainly going to deal with today.",
                    "label": 1
                },
                {
                    "sent": "But for the sake of completeness I also have to point out that the second problem that we have is actually the complexity of specifications you finding that complex similarity function.",
                    "label": 0
                },
                {
                    "sent": "Talked about as well as to set threshold so that we can actually find a set M prime that approximate EMS the values they set and are actually looking for.",
                    "label": 0
                },
                {
                    "sent": "Well, it's a complex problem.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of papers on the subject matter we're not going to be concerned with that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Today so the major problem we're going to look into today is what happens if the datasets that I have in my source at my target do not fit in memory?",
                    "label": 0
                },
                {
                    "sent": "This is a common problem because we have really large datasets available on the linked data web nowadays.",
                    "label": 0
                },
                {
                    "sent": "For example, the largest datasets contain more than 20 billion triples, and we know that we have approximately 150 billion triples already available and indexed, and that's just basically the top of the iceberg.",
                    "label": 0
                },
                {
                    "sent": "However, not all data providers have huge infrastructures to actually run really completely linking tasks using a lot of memory.",
                    "label": 0
                },
                {
                    "sent": "That's the one for 100 problem.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, is most of the solutions that exist for link discovery.",
                    "label": 0
                },
                {
                    "sent": "Assume that you have enough memory.",
                    "label": 0
                },
                {
                    "sent": "So basically they're all in memory solutions, so the problem becomes, how do we actually deal with this problem of linking when we don't have enough memory?",
                    "label": 0
                },
                {
                    "sent": "How do we then?",
                    "label": 0
                },
                {
                    "sent": "Rule over the interplay between the memory and the hard drive.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our goal was to devise a generic, time efficient approach to compute this set in prime.",
                    "label": 1
                },
                {
                    "sent": "This approximation of the mapping that we actually looking for for cases where S + C is larger than the size of CCS or memory South is our set of sources aunties or set of targets, i.e.",
                    "label": 0
                },
                {
                    "sent": "When I sent it on fit in memory, we wanted to ensure the completeness of results.",
                    "label": 1
                },
                {
                    "sent": "I even wanted to carry out all the tasks that are necessary to actually compute the set and prime and the main bottleneck that we had to address was basically.",
                    "label": 0
                },
                {
                    "sent": "The efficient access to the hard drive you actually want to minimize the number of times that we actually fetch data from the hard drive into memory, and we call our solution Chrome so that I can show it this nice little picture.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. How do we go about doing that?",
                    "label": 0
                },
                {
                    "sent": "The first insight that we used, or that we looked upon was that most approaches rely most time efficient approaches rely on a divide and merge paradigm, and I'm just going to explain to you how these paradigms work with an example before then giving you the formal details of what I actually mean.",
                    "label": 1
                },
                {
                    "sent": "So one of the examples of time efficient algorithms for link discoveries.",
                    "label": 0
                },
                {
                    "sent": "HR three.",
                    "label": 0
                },
                {
                    "sent": "It deals basically with data that is distributed within a set in Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "Of any space, really Minkowski metrics.",
                    "label": 0
                },
                {
                    "sent": "And then the idea is that you can't.",
                    "label": 0
                },
                {
                    "sent": "We can translate this condition that says the similarity of S of T&T must be larger or equal to Theta two basically a distance condition where we set distance between SMT must be less or equal than a value Delta and this value Delta is basically with of any of these squares.",
                    "label": 0
                },
                {
                    "sent": "Now for a given subset of S which is found in this particular square.",
                    "label": 0
                },
                {
                    "sent": "So the square you see this larger point.",
                    "label": 0
                },
                {
                    "sent": "Really, we know that all the solutions I oddities that are such that the distance from S2 to be less or equal to Delta actually to be found in this larger square.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is we can tier or space into squares 2 dimensional squares if it's more dimensional is basically hyper cubes.",
                    "label": 0
                },
                {
                    "sent": "Hypercubes off with Delta.",
                    "label": 0
                },
                {
                    "sent": "We can then look at all the hypercubes around the hypercube, where the subset of South is that we're particularly interested in at the moment, and run our computations there.",
                    "label": 0
                },
                {
                    "sent": "So we actually divide the space.",
                    "label": 0
                },
                {
                    "sent": "We've run our computations within that those little blocks of space, and then we merge the results at the end to actually get the final results.",
                    "label": 0
                },
                {
                    "sent": "And with that we can.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's actually proven that the results are complete.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So formally, what a lot of our purchase do is simply define a set S which is a set of subsets of input source data Set S with the condition that the Union of these subsets actually is the S that we had at the beginning.",
                    "label": 0
                },
                {
                    "sent": "The set of sources that do the same for the target, and this is the trick.",
                    "label": 0
                },
                {
                    "sent": "This is what is the most important actually for these approaches that define a mapping between the set of subsets of S. And the power set of the set of subsets of T and these particular mapping has particular characteristics.",
                    "label": 0
                },
                {
                    "sent": "The elements of SI only need to be compared with the elements in the set that are in U of SI and the Union of the results.",
                    "label": 0
                },
                {
                    "sent": "Overall, yes, eyes actually gives you this state and the set M prime there enough proofs in paper related to that.",
                    "label": 0
                },
                {
                    "sent": "What this means here is that this Blue Square would be one of the SI, so basically.",
                    "label": 0
                },
                {
                    "sent": "One of the portions of Esther very interested in and the mu function would actually give us the 8 red squares plus the Blue Square and say this is basically the set of keys with which we need to compare this particular SI to get or the result.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can then translate this problem basically to a set of tasks.",
                    "label": 0
                },
                {
                    "sent": "So if you have all the size and we have all them you oversize or we can basically say you have a mapping between those and the tasks.",
                    "label": 0
                },
                {
                    "sent": "Basically consists of comparing each XI, which each TJ is in U of SI.",
                    "label": 0
                },
                {
                    "sent": "This can basically be translated into a task graph.",
                    "label": 0
                },
                {
                    "sent": "It is rather obvious I think.",
                    "label": 0
                },
                {
                    "sent": "So the set of vertices is basically as follows.",
                    "label": 0
                },
                {
                    "sent": "As Union T we have a weight function for the nodes of the graph, which basically tells us how many elements we find in there.",
                    "label": 0
                },
                {
                    "sent": "So if we.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Over here, if South is there Ms Office are the blue dots and elements of share the red dots.",
                    "label": 0
                },
                {
                    "sent": "You basically simply count the number of blue dots that you have here.",
                    "label": 0
                },
                {
                    "sent": "That gives you the size of Cy and if you want to get the size of the T duties in this square.",
                    "label": 0
                },
                {
                    "sent": "Basically simply count the number of red.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dot alright, so for the sake of simplicity or using smaller graph and the types of graphs that you actually get when you run this kind of computations, let's just assume that we have S1S2 and S3.",
                    "label": 0
                },
                {
                    "sent": "These are the sizes of the different subsets and we have T1T2 and G3.",
                    "label": 0
                },
                {
                    "sent": "Now given the new function, we can actually now start building the task graph.",
                    "label": 1
                },
                {
                    "sent": "The weight of the edges between 2 between SSI and TJ.",
                    "label": 0
                },
                {
                    "sent": "Is basically simply the size of times the size of TJ and it tells us how many computations will actually have to run to basically complete this task and we can build it up and we get such a task graph.",
                    "label": 1
                },
                {
                    "sent": "I hope that's clear so far.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the whole idea then becomes how do we actually traverse this graph in an intelligent manner intelligent manner, meaning what we actually want to do is run the tasks in such a way that the computer only fetches data from memory really from the hard drive really rarely.",
                    "label": 0
                },
                {
                    "sent": "So we want basically asked the memory the data to stay in memory as long as possible.",
                    "label": 0
                },
                {
                    "sent": "The approach that we propose has two steps.",
                    "label": 0
                },
                {
                    "sent": "The first step is a clustering where we basically find groups of nodes that fit in memory.",
                    "label": 0
                },
                {
                    "sent": "Basically want to execute together and discuss scheduling step.",
                    "label": 0
                },
                {
                    "sent": "We basically want to find the right sequences of groups of nodes so as to maintain as much data in memory as possible and as I said we'll use these little graph as example.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So step one clustering.",
                    "label": 0
                },
                {
                    "sent": "How do we go about doing that?",
                    "label": 0
                },
                {
                    "sent": "There obviously a lot of possible approaches here.",
                    "label": 0
                },
                {
                    "sent": "We looked at two approaches, a naive approach and a greedy approach.",
                    "label": 0
                },
                {
                    "sent": "The reason why we took simple approaches is basically due simply to the size of the task graphs that you get in the examples that ask, Ross had millions of edges, so any approach that has a complexity that goes in the quadratics already becomes rather difficult to use.",
                    "label": 0
                },
                {
                    "sent": "It produces too much overhead.",
                    "label": 0
                },
                {
                    "sent": "So the naive approach is actually rather simple.",
                    "label": 0
                },
                {
                    "sent": "What it does is it looks at each XI and basically tries to collect all the TJ's around this as I connected to this essay, there are such that the total size of SI plus the corresponding TJ's is less or equal than C and that is basically a cluster.",
                    "label": 0
                },
                {
                    "sent": "So if we were to start at S1, we would basically.",
                    "label": 0
                },
                {
                    "sent": "Take T1 and T2 these sizes here at 3 + 2 + 1, which is 6 that fits the memory.",
                    "label": 0
                },
                {
                    "sent": "There's no more edge 4X4 S one.",
                    "label": 0
                },
                {
                    "sent": "We basically say that's a cluster.",
                    "label": 0
                },
                {
                    "sent": "Go to the next node, which is S2.",
                    "label": 0
                },
                {
                    "sent": "We take T-12.",
                    "label": 0
                },
                {
                    "sent": "One other size of two.",
                    "label": 0
                },
                {
                    "sent": "We have S2 in there.",
                    "label": 0
                },
                {
                    "sent": "That's already four.",
                    "label": 0
                },
                {
                    "sent": "We take T2.",
                    "label": 0
                },
                {
                    "sent": "That's five, and there's no other edge that we can actually take that is such that the group is as a size less than C, which has a size of seven in this example, which basically means we go to the next group.",
                    "label": 0
                },
                {
                    "sent": "And that would be the Orange Group, and that's basically the kind of edge labeling edge coloring that we would get.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One can obviously imagine a greedy approach in this case that more looks at the weights of the edges.",
                    "label": 0
                },
                {
                    "sent": "So here would basically start by taking the edge that has the highest waist weight, i.e.",
                    "label": 0
                },
                {
                    "sent": "The largest tasks that would be that blue edge, and we're basically trying to find edges that are connected to this edge virus via nodes, and that are the largest possible and that we will basically group the data.",
                    "label": 0
                },
                {
                    "sent": "What happens here is that this already has a size of seven, so we can't fit more memory.",
                    "label": 0
                },
                {
                    "sent": "We now have to start the next group.",
                    "label": 0
                },
                {
                    "sent": "We basically started up there, and as you can see, we travel.",
                    "label": 0
                },
                {
                    "sent": "We traverse the graph.",
                    "label": 0
                },
                {
                    "sent": "These some of these weights of these three nodes is basically 7, so will stop there and continue coloring or graph.",
                    "label": 0
                },
                {
                    "sent": "So with that, we've grouped the nodes that was basically the first thing we wanted to do.",
                    "label": 1
                },
                {
                    "sent": "Now we have groups of nodes.",
                    "label": 1
                },
                {
                    "sent": "We want to find the right sequence, an.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is obviously a rather tricky problem.",
                    "label": 0
                },
                {
                    "sent": "So the output of our clustering is a sequence of clusters and we want to find a sequence of clusters that is such that consecutive clusters share as much data as possible.",
                    "label": 0
                },
                {
                    "sent": "The more data they share, the less data we actually have to fetch from the hard drive to be able to execute the tasks within the group.",
                    "label": 0
                },
                {
                    "sent": "So we want to maximize the overlap of the generated sequence.",
                    "label": 0
                },
                {
                    "sent": "What I'm informally is the following.",
                    "label": 0
                },
                {
                    "sent": "If we take two groups of node GINGJ, the overlap is basically the number of elements within the vertex.",
                    "label": 0
                },
                {
                    "sent": "The vertices that they share.",
                    "label": 0
                },
                {
                    "sent": "So if we look at these two groups, we have the Orange Group right there, and you have the blue Group.",
                    "label": 0
                },
                {
                    "sent": "Here the overlap of G4 and G1 would be 4 simply because they share is 3.",
                    "label": 0
                },
                {
                    "sent": "And we can obviously based on that define the overlap function for a sequence that's basically the sum of the overlaps of consecutive groups.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the sequence G4G 1G2.",
                    "label": 0
                },
                {
                    "sent": "Apologies G for this should be G3.",
                    "label": 0
                },
                {
                    "sent": "My mistake then the number would be 9.",
                    "label": 0
                },
                {
                    "sent": "That would basically be the overlap value that we have here.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do we go about doing that?",
                    "label": 0
                },
                {
                    "sent": "Again, we want approaches that are simple but approaches good results.",
                    "label": 0
                },
                {
                    "sent": "The first approach that we suggest is a best effort based approach.",
                    "label": 0
                },
                {
                    "sent": "You basically give the approach at certain time, say.",
                    "label": 0
                },
                {
                    "sent": "Optimize for within this time frame and then give me the sequence that you found.",
                    "label": 0
                },
                {
                    "sent": "We start with the sequence of nodes that came from the clustering and what we basically do is simply random permutations.",
                    "label": 0
                },
                {
                    "sent": "We take two groups within the sequence.",
                    "label": 0
                },
                {
                    "sent": "We swap them and we check whether the overlap function actually has improved or not.",
                    "label": 0
                },
                {
                    "sent": "If it has improved, then we keep the sequence.",
                    "label": 0
                },
                {
                    "sent": "If not, we simply let the nodes where they were and try another pay.",
                    "label": 0
                },
                {
                    "sent": "Now what's important here is that we do not need to know.",
                    "label": 0
                },
                {
                    "sent": "The global value of the overlap function.",
                    "label": 0
                },
                {
                    "sent": "If we needed to do that, it would mean that would have to check the whole sequence and that will simply take too long.",
                    "label": 0
                },
                {
                    "sent": "What we can use is it Rick and what this basically says.",
                    "label": 0
                },
                {
                    "sent": "This nice little question.",
                    "label": 0
                },
                {
                    "sent": "All it says is that if you take 2 nodes from a sequence and you swap those nodes, actually only interested in the difference that you produce in the local boundaries at the left and right of each of these nodes two and if that value actually increases.",
                    "label": 0
                },
                {
                    "sent": "There is no doubt your overall function increases because you don't change the overlap of any other parts.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this sequence where we have one overlap of 0, three and two by simply swapping G4, Angie, three, we actually have a case where we increase the overall overlap score.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can obviously also imagine a greedy approach which starts with a random cluster, then takes the next cluster with the largest overlap and basically runs across the whole set of clusters.",
                    "label": 0
                },
                {
                    "sent": "That way.",
                    "label": 0
                },
                {
                    "sent": "Major problem here, however, is that we need global knowledge because we need to know what is the best cluster for a particular cluster, i.e.",
                    "label": 0
                },
                {
                    "sent": "The closer the highest overlap.",
                    "label": 0
                },
                {
                    "sent": "But with that we can also produce sequences that are pretty good.",
                    "label": 0
                },
                {
                    "sent": "So with that we've actually covered to the two steps that we were interested in.",
                    "label": 0
                },
                {
                    "sent": "The question is how do we perform so the way they approach runs is we have the sequence, the sequence of tasks.",
                    "label": 0
                },
                {
                    "sent": "We basically define the sequence in which two groups should be executed and the memory works like a cache, which basically means that we ask the memory, hey, tell me, do you have this group?",
                    "label": 0
                },
                {
                    "sent": "If the memory has the group then basically returns it to the computer basically then computes this similarity scores.",
                    "label": 0
                },
                {
                    "sent": "If the groups are not available, that's basically the moment where we have to look into the hard drive.",
                    "label": 0
                },
                {
                    "sent": "Which basically means that we can compare our approach with existing.",
                    "label": 0
                },
                {
                    "sent": "Caching mechanisms.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the datasets that we used where 1 million labels from DBP Generations version of April 2015 and 0.8 million places from link geodata, I think the hardware is relatively straightforward.",
                    "label": 0
                },
                {
                    "sent": "What we measured, where the total runtime as well as the hit ratio, i.e.",
                    "label": 0
                },
                {
                    "sent": "How often were we able to find things on in memory?",
                    "label": 0
                },
                {
                    "sent": "How often did we have not to actually access?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The hard drive.",
                    "label": 0
                },
                {
                    "sent": "So we started by letting the clustering approach and as expected.",
                    "label": 0
                },
                {
                    "sent": "Basically the greedy approach, which is here has the better hit ratio, which basically means that it is able to put data where it should be so that we don't have to access the hard drive too often.",
                    "label": 0
                },
                {
                    "sent": "However, I have to say here we basically just considered 1000 points, i.e the size of S and the size of the way 1000.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there were 10,000 for the pre experiments.",
                    "label": 0
                },
                {
                    "sent": "What we see here we started over.",
                    "label": 0
                },
                {
                    "sent": "Time explodes if you basically go to a 10 times more data, you get approximately 100 times the runtime, which is definitely not something that you want to do so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So although the naive approach is more efficient, the greedy approach is more effective, but has such a high overhead that is actually pointless to use it.",
                    "label": 0
                },
                {
                    "sent": "So we use the naive approach.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For clustering.",
                    "label": 0
                },
                {
                    "sent": "And when we look at scheduling, we basically get this similar picture, which basically means that the best effort approach does not perform as well pertaining to hit ratios.",
                    "label": 0
                },
                {
                    "sent": "However, it scales well enough on large datasets that we actually had to use it.",
                    "label": 0
                },
                {
                    "sent": "So the best effort approach is more time efficient.",
                    "label": 0
                },
                {
                    "sent": "The greedy approach is more effective, but the best of what approach is what we used for scheduling.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I said before, approach is very similar to caching approaches, so we could actually compare our works with standard caching solutions that are available out there and what we were able to show is that approach actually organizes data well enough that we could be significantly better than the existing standard caching solutions, which is actually the goal we wanted to reach.",
                    "label": 0
                },
                {
                    "sent": "In some cases there hit ratio was not as good As for example as well.",
                    "label": 0
                },
                {
                    "sent": "If you in this case that's again with 10,000.",
                    "label": 0
                },
                {
                    "sent": "Items in memory, but overall our approach performed better respect through runtime, so we were more time efficient and we actually have higher hit rates in most cases.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Last thing that we wanted to know is can't, does it mean that we can actually know discovery with limited memory on really large datasets and we were able to show that we can.",
                    "label": 0
                },
                {
                    "sent": "Indeed we basically increase the size of the data that we had in memory.",
                    "label": 0
                },
                {
                    "sent": "Basically 100,000 to 800,000 points.",
                    "label": 0
                },
                {
                    "sent": "We used standard hardware, basically with 10 gigabytes of RAM and the data definitely did not fit in there and we're still able to run link discovery on that one interesting result.",
                    "label": 0
                },
                {
                    "sent": "It was the sub quadratic growth of runtime, which basically means that normally you would expect that if you increase the amount of data by two actually or runtime because of the problem, quadratic runtime should increase by approximately 4.",
                    "label": 0
                },
                {
                    "sent": "What we could show is that we are basically under that runtime runtime is more linear with the number of mappings that needs to be generated, which is nice.",
                    "label": 0
                },
                {
                    "sent": "So for example for LGD falling Geo data, we were able to generate between 360.",
                    "label": 0
                },
                {
                    "sent": "And 370 mappings per second.",
                    "label": 0
                },
                {
                    "sent": "That was iterate on.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we did.",
                    "label": 0
                },
                {
                    "sent": "So to conclude, I presented Norm which is a two step approach for link discovery that allows you to run link discovery on large amounts of data with limited amounts of memory.",
                    "label": 0
                },
                {
                    "sent": "It basically relies on the divide and merge paradigm and ensures that link data sets of arbitrary size can be run even mid small memory.",
                    "label": 0
                },
                {
                    "sent": "Well, the one thing that we did not look into and that we definitely plan to look into is parallel implementations.",
                    "label": 0
                },
                {
                    "sent": "So the whole idea of working with this graph when you then have to run with server processors.",
                    "label": 0
                },
                {
                    "sent": "Becomes really, really interesting, especially when you think of even larger architectures where you have in memory distributed memory solutions such as.",
                    "label": 0
                },
                {
                    "sent": "Spark.",
                    "label": 0
                },
                {
                    "sent": "Obviously we would like to look into blocking an order.",
                    "label": 0
                },
                {
                    "sent": "Similar approaches and see whether we can use that there.",
                    "label": 0
                },
                {
                    "sent": "So basically when M prime does not have to be computed, complete.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's it from my side.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much for listing and I do invite you to The Hobbit event where we talk about these things even more.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}