{
    "id": "2l2uulzx2wh4do2ksztv54bjuntu7w7c",
    "title": "LSD-SLAM: Large-Scale Direct Monocular SLAM",
    "info": {
        "author": [
            "Jakob Engel, Faculty of Informatics, TU Munich"
        ],
        "published": "Oct. 29, 2014",
        "recorded": "September 2014",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/eccv2014_engel_monocular_slam/",
    "segmentation": [
        [
            "Hello everybody, I'm going to present Alice DSLAM which stands for large scale direct monocular slam's joint work with Thomas Drops and Daniel Cremers."
        ],
        [
            "I want to start by just showing you a video and what you can see here is I'm moving the camera around while I'm moving it around, which is just a normal webcam.",
            "It's being tracked in real time and the environment is reconstructed on the screen there on the CPU of laptop."
        ],
        [
            "So this is obviously structure in motion or monocular slam, and it's a very popular topic.",
            "There has been a lot of research on that in the last 15 years.",
            "Here are just some examples.",
            "And there has been a lot of progress in the last 15 years, which to know part is to no small part, is due to better computers we have now but also the methods that we use now are not the same like as the methods we used 15 years ago.",
            "Still.",
            "Most of these methods are based at some point or another on key points or features or landmarks."
        ],
        [
            "But in some point the pipeline is always this that you have the input images and then you extract some kind of features, Sift, surf.",
            "It might also be small line segments or patches.",
            "Take take out the images and just work on these feature observations to reconstruct geometry.",
            "So for tracking, minimize for instance reprojection error for mapping you estimate point positions, Patch normals, line positions and so on.",
            "Now the direct approach and Alice.",
            "This lamb is a fully direct method, completely skipped this step.",
            "So in contrast to the free talks, it's all before we do not use features.",
            "Instead we take the raw images as they come and then do tracking and mapping directly on full images.",
            "So for tracking we minimize the photometric error, which is minimization of intensity differences instead of point different distances and for mapping we estimate kind of the most basic geometric information there is, that is.",
            "Pixelwise depth information."
        ],
        [
            "Now, why would you want to do that?",
            "Here's an example.",
            "You can see a key point by SLAM system, and others do slam on the right.",
            "Tracking is actually done in real time at 30 frames per second.",
            "We just show keyframes here.",
            "The big difference is that why Keypoint based approaches can only use image corners or straight line segments?",
            "Islam uses everything that's colored in the image.",
            "That is, everything that has gradient.",
            "Which gives a much denser and nicer map with less outliers.",
            "What you can see there?"
        ],
        [
            "So how does it work to do monocular slam in real time entirely without key points?",
            "And I hope that in this talk I can give you a rough idea of how we can do that.",
            "So there's three main parts, tracking, depth estimation and global map optimization, which I'm going to go through one after the other now, so let's talk."
        ],
        [
            "With tracking.",
            "For now, let's assume we have an old keyframe with a depth map.",
            "And we get a new keyframe and we want to track that."
        ],
        [
            "So we have the old keyframe and I'm always going to visualize depth Maps as color coded depth superimposed on the RGB on the black and white image.",
            "Um now?"
        ],
        [
            "We get a new camera frame and we can sort of render this new camera frame from the keyframes perspective so we can back up into the keyframe using the depth.",
            "And now assume."
        ],
        [
            "Brightness constancy we can say OK, those two images should be the same so we put it in our function sum over all pixels and minimize it."
        ],
        [
            "Minimize it using the Gauss Newton algorithm, which turns out to be very similar to a forward compositional formulation of Lucas Kanade, which has been around for over 25 years I think.",
            "Um?",
            "And directly uses all the pixels to directly get the six degrees of freedom pose of the camera relative to the keyframe now."
        ],
        [
            "Of course, that's kind of the romanticized clean version.",
            "In practice, you need to add some small things.",
            "For instance, we do a multiresolution approach to attract large motions in order to overcome the nonconvexity of the error function.",
            "We use a human norm instead of L2 norm in order to be robust to outliers and occlusions, and moving objects and so on.",
            "We introduce a sort of statistical normalization which respects all error sources on residual including depth noise and intensity noise.",
            "And then it looks roughly like this.",
            "So here you can see the image first on a core scale and then find out finer being tracked.",
            "It actually runs very fast in real time on a single CPU core.",
            "You can track it in real time no problem."
        ],
        [
            "So that's the tracking part.",
            "The thing is, for in order to do that, we need a depth map and this depth map.",
            "If you're using a depth camera, you get it for free, so that is why this kind of tracking is very popular for Kinect based approaches.",
            "Here we have to have to estimate it."
        ],
        [
            "That's the epitome."
        ],
        [
            "In component we represent depth Maps probabilistically, so for each pixel for which we have depth, which are all the pixels that have image gradient for all the color pixels, here we have Gaussian probability distribution on the inverse depth of that pixel.",
            "Under Gaussian probability distribution is step by step refined or filtered overtime by integrating more and more stereo comparisons with other frames.",
            "I'm starting with a very small baseline, explicitly taking advantage of the fact that we have a video so we can start with a small baseline and slowly increase the baseline and keep all the information in order to make it run in real time on the CPU, we do a very strict information selection, so we only do stereo if it's actually worth the cost.",
            "Which kind of motivates to hold simulants idea because for pixels where there's no gradients, there is not going to give us any information.",
            "So we leave them out.",
            "In practice, it looks somewhat like this, so here you can see the April Alliance segments that are searched in each frame it slow motion.",
            "You can see that in the beginning if new new hypothesis are initialized, there's a large red line, meaning that there's still a lot of variance, and we don't know where to look at, but then it gets smaller and smaller, and in the end it's just these small green lines tracking kind of the edge or corner or whatever radiant it is.",
            "We then add small edge, preserving smoothing to kind of put a prior in there towards smooth depth Maps.",
            "Distance based keyframe selection to select new keyframes."
        ],
        [
            "And that's the depth map estimation component.",
            "So we have this loop.",
            "Existing depth map is used to track and you frame.",
            "This frame is then used to either make up a new keyframe or refined than UK.",
            "The old key frame using stereo and then the new keyframe is used to track new frames.",
            "There is this classical circular dependency which we need to initialize.",
            "This is actually one of the nice things we can."
        ],
        [
            "Just initialize it using a random depth map so we start off with random depth map and then take this kind of alternating optimization between pose and depth map.",
            "Take care of it, which works in most cases in practice.",
            "Um?",
            "But so far this is just in the dormitory.",
            "It's no slam and it's not large scale slam.",
            "So that's where the third part comes in."
        ],
        [
            "Global map optimization.",
            "So we do that by representing the world as a number of key frames and doing a pose graph of those key."
        ],
        [
            "The first important thing that you need to notice here is that monocular slam, so the absolute scale is not observable, so drift overtime.",
            "So after a certain time after loop we not only have accumulated error in position and rotation, but also the scale has changed and we need to take that into account."
        ],
        [
            "And we do that by proposing a novel version of direct tracking on SIM three directly, so rotation, translation, and scale.",
            "Um?",
            "To directly line 2 keyframes with two depth Maps and get the scaled rotational translation between them.",
            "So we start off with the same tracking as before, just minimizing the photometric error, and now we add two small things.",
            "So first."
        ],
        [
            "We replace rigid body motion by rigid body motion plus scale.",
            "Mathematically, it works the exact same way.",
            "And 2nd we added a second residual term which says OK in addition to image intensity staying the same.",
            "Also the depth should stay the same, so that's the second term which kind of is the difference of depth values?",
            "You can minimize it."
        ],
        [
            "Zach time way so.",
            "Gauss Newton optimization with with a little twist, multiresolution approach, human norm statistical normalization, which also takes care of balancing these two different units against each other.",
            "Um, and then we."
        ],
        [
            "Put the result into a post graph framework where we optimize over deposes in the graph.",
            "Each keyframe has a 7 dimensional post, so each keyframe has position, rotation and scale.",
            "In the plots and videos will kind of visualize the scale as the size of the camera first term."
        ],
        [
            "Now, in practice it looked like this, so the camera moves more and more.",
            "Key frames are created more and more constraints are found and the more constraints are found, the more the individual depth Maps shift into place and are scaled rotated and translated to make up a nice model.",
            "And you can also see that this model includes edges, for instance, or densely textured surfaces, not just points."
        ],
        [
            "So this is the whole algorithm.",
            "What I showed you so far doesn't really qualify as large scale, so let me show you."
        ],
        [
            "Example, which is a bit more large scale.",
            "Here you can see the random initialization, it's just a green random dirt and now the camera starts moving and it converges quite quickly to a good depth map.",
            "Black means very close to the camera, then red, green, blue is very far away.",
            "This is about a 6 minutes trajectory.",
            "Computations done in real time only on the CPU.",
            "Actually the whole algorithm is optimizable to a GPU and could run much faster on GPU even.",
            "Now you can see the accumulated drift and also scale drift, and there's one duplicated found whole thing shifts into place and you get a nice big map.",
            "The total of 800 keyframes, 11,000 constraints and 51 million points."
        ],
        [
            "Here's another example which is about twice as big.",
            "Um?",
            "12 minutes, 36,000 frames tracked in total, 1000 keyframes and roughly 18,000 constraints.",
            "One nice thing is that for each point in a depth map we not only have the depth, but we also have a measure on how accurate it is.",
            "So what we do for these visualizations is we just put a threshold on the variance and say OK, we just want display the points that have a certain accuracy so we don't display all 100 million points, which would be a lot of points.",
            "We only display the ones that are sufficiently accurate, which gives a very nice map which.",
            "Also agrees with Google Maps.",
            "Now."
        ],
        [
            "Because we can do that in real time on a CPU, we can also do it on the smartphone.",
            "This short preview for our eyes.",
            "My paper, which is going to be published tomorrow.",
            "Officially I think in Munich, so here we do the same thing on smartphone and use it for a small augmented reality application.",
            "Again, you can imagine that having the semi dense representation where you have a lot more information about edges in the image as well.",
            "Gives you better opportunities to do augmented reality."
        ],
        [
            "OK, so to summarize.",
            "There are three main components that make LSD slam work direct tracking, which is direct image alignment which is then used both for loop closure, constraint, constraint tracking as well as initial frame tracking.",
            "Um?",
            "We give a kind of general formulation as Gauss Newton minimization only manifolds.",
            "Which time can be used?"
        ],
        [
            "Cement stereo by filtering over many small baseline stereo comparisons and Bayesian Fusion of the information we get coupled with a strict information selection, so we only do stereo if it's actually worth the cost.",
            "And that makes it run in real time on CPU.",
            "And third pose graph optimization, including scale.",
            "So simply post graph optimization to make a large scale global consistent map outfit."
        ],
        [
            "Now.",
            "To conclude.",
            "LSD slam is a large scale, fully direct monocular slam system.",
            "We don't even need a separate keypoint based initializer.",
            "It's it's a very first system that's without key points and can work on this scale.",
            "It runs in real time on CPU you can expect even much faster results on the GPU and probably most interesting Lee, we're also decided to publish the source code within the next week, so we're going to put it open source, including some datasets, both videos, so you can try it out without having a webcam as well as point clouds.",
            "If you just want to use a point cloud for further processing.",
            "Thanks for the attention.",
            "Questions great results and I'm a big big fan of direct intensity based methods are really glad to see these.",
            "My question is could you comment on how this differs from the work that was done in the early 90s by the Sarnov Group and direct intensity based methods for 3D reconstruction?",
            "From what I could tell, the main additions here were the large scale and the real time.",
            "Are there any additional things that I'm to be honest?",
            "I'm not entirely sure which work you're referring to.",
            "It started with the worth of.",
            "Work of Keith Hannah and undone.",
            "The reconstruction of 3D camera motion and depth using direct intensity based methods and then it continued to the plane plus parallax work that lots of us did.",
            "Also using direct intensity based method.",
            "If you're not familiar with, I'll give you some references later, like the only work that early that I'm a familiar of is from Canada and Solinski and matches actually where they do this similar kind of depth map estimation, but only for planar motion and without tracking.",
            "So I'll give you some references later, yes.",
            "Hi I have a over here.",
            "Well, you have your intensity constant intensity assumption and I saw this works for outside scenes.",
            "So how do you handle this?",
            "Can you comment on what happens if the intensity changes if the lighting changes, so you're right and some of the sequences I showed there were pretty drastic lightning changes auto shutter.",
            "Being complicated so it actually works fairly well even if there is lighting changes, because we only use image regions with certain gradient and lightning changes only have have a bad effect if the lightning change is bigger than the gradient you're using.",
            "So if there is a black and white edge and before and after the white is a bit brighter and the black is a bit brighter, but apart from that the edge stays the same place, then it doesn't actually make a difference.",
            "We have experimental support for kind of a feen lightning estimation, so estimated the whole frame got a bit brighter and compensate for that, but it's not in there yet.",
            "Hi I have a question from the balcony.",
            "Do you have any quantitative results on the drift, especially in the large scale as well as on the pose as well as the three point reconstruction?",
            "Yes, so we have benchmark results on the pose estimation accuracy.",
            "Unfortunately, there's not many good benchmarks out there for monocular slam, neither for stereo or for depth depth or kynec based slam, and in both cases or formula slam you have certain unobservable modes which you don't have for stereo or depth based slam.",
            "So many of the directories don't work for any monocular for anymore algorithm, but the ones that do work we do have numeric results in them.",
            "Maybe I missed this, but how do you detect loop closure from the your keyframes, yes, good question.",
            "So there's two ways we can do that.",
            "So one thing we do is we just try to track on a very coarse resolution on close by frames that have a certain overlap, which is very fast, because if you do it on a small resolution it's 30 * 40 pixels and you can very quickly decide whether it's a potential disclosure or not.",
            "In order to detect the really large loop closures, which for instance, the one where I showed the one big loop.",
            "Where you can't do that, do it.",
            "That way we use an appearance based loop closure detection algorithm open popping up in that case to kind of tell us.",
            "OK, this frame might be a loop closure.",
            "Try tracking it.",
            "But for the for the other sequence I showed with the chairs and umbrellas, we didn't use him up because there all the loops are so small that the accumulated drift is small enough.",
            "Hey I have a question.",
            "I can imagine that intensity based approach breaks faster part if the baseline bit between 2 frames is is to far away, yes, do you have any experiments of benchmarks on this?",
            "How far you can go here so we don't have numbers, but by experience you're right it does.",
            "It is inherently based on having a video and not only individual frames with a large baseline.",
            "So we need a certain frame rate.",
            "Let's say 30 frames per second and then it's no problem because then you have those small baselines always or like either you have a small baseline or you have a good initialization for tracking from the previous frame.",
            "If you only want to apply it to 5 frames per second video or 10 frames per second video, then the motion has to be sufficiently small.",
            "OK, so then let's thank the speakers of the session again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello everybody, I'm going to present Alice DSLAM which stands for large scale direct monocular slam's joint work with Thomas Drops and Daniel Cremers.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to start by just showing you a video and what you can see here is I'm moving the camera around while I'm moving it around, which is just a normal webcam.",
                    "label": 0
                },
                {
                    "sent": "It's being tracked in real time and the environment is reconstructed on the screen there on the CPU of laptop.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is obviously structure in motion or monocular slam, and it's a very popular topic.",
                    "label": 0
                },
                {
                    "sent": "There has been a lot of research on that in the last 15 years.",
                    "label": 0
                },
                {
                    "sent": "Here are just some examples.",
                    "label": 0
                },
                {
                    "sent": "And there has been a lot of progress in the last 15 years, which to know part is to no small part, is due to better computers we have now but also the methods that we use now are not the same like as the methods we used 15 years ago.",
                    "label": 0
                },
                {
                    "sent": "Still.",
                    "label": 0
                },
                {
                    "sent": "Most of these methods are based at some point or another on key points or features or landmarks.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But in some point the pipeline is always this that you have the input images and then you extract some kind of features, Sift, surf.",
                    "label": 1
                },
                {
                    "sent": "It might also be small line segments or patches.",
                    "label": 0
                },
                {
                    "sent": "Take take out the images and just work on these feature observations to reconstruct geometry.",
                    "label": 0
                },
                {
                    "sent": "So for tracking, minimize for instance reprojection error for mapping you estimate point positions, Patch normals, line positions and so on.",
                    "label": 0
                },
                {
                    "sent": "Now the direct approach and Alice.",
                    "label": 0
                },
                {
                    "sent": "This lamb is a fully direct method, completely skipped this step.",
                    "label": 0
                },
                {
                    "sent": "So in contrast to the free talks, it's all before we do not use features.",
                    "label": 0
                },
                {
                    "sent": "Instead we take the raw images as they come and then do tracking and mapping directly on full images.",
                    "label": 0
                },
                {
                    "sent": "So for tracking we minimize the photometric error, which is minimization of intensity differences instead of point different distances and for mapping we estimate kind of the most basic geometric information there is, that is.",
                    "label": 0
                },
                {
                    "sent": "Pixelwise depth information.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, why would you want to do that?",
                    "label": 1
                },
                {
                    "sent": "Here's an example.",
                    "label": 0
                },
                {
                    "sent": "You can see a key point by SLAM system, and others do slam on the right.",
                    "label": 0
                },
                {
                    "sent": "Tracking is actually done in real time at 30 frames per second.",
                    "label": 0
                },
                {
                    "sent": "We just show keyframes here.",
                    "label": 0
                },
                {
                    "sent": "The big difference is that why Keypoint based approaches can only use image corners or straight line segments?",
                    "label": 1
                },
                {
                    "sent": "Islam uses everything that's colored in the image.",
                    "label": 0
                },
                {
                    "sent": "That is, everything that has gradient.",
                    "label": 0
                },
                {
                    "sent": "Which gives a much denser and nicer map with less outliers.",
                    "label": 0
                },
                {
                    "sent": "What you can see there?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how does it work to do monocular slam in real time entirely without key points?",
                    "label": 1
                },
                {
                    "sent": "And I hope that in this talk I can give you a rough idea of how we can do that.",
                    "label": 0
                },
                {
                    "sent": "So there's three main parts, tracking, depth estimation and global map optimization, which I'm going to go through one after the other now, so let's talk.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With tracking.",
                    "label": 0
                },
                {
                    "sent": "For now, let's assume we have an old keyframe with a depth map.",
                    "label": 0
                },
                {
                    "sent": "And we get a new keyframe and we want to track that.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have the old keyframe and I'm always going to visualize depth Maps as color coded depth superimposed on the RGB on the black and white image.",
                    "label": 0
                },
                {
                    "sent": "Um now?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We get a new camera frame and we can sort of render this new camera frame from the keyframes perspective so we can back up into the keyframe using the depth.",
                    "label": 0
                },
                {
                    "sent": "And now assume.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Brightness constancy we can say OK, those two images should be the same so we put it in our function sum over all pixels and minimize it.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Minimize it using the Gauss Newton algorithm, which turns out to be very similar to a forward compositional formulation of Lucas Kanade, which has been around for over 25 years I think.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And directly uses all the pixels to directly get the six degrees of freedom pose of the camera relative to the keyframe now.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course, that's kind of the romanticized clean version.",
                    "label": 0
                },
                {
                    "sent": "In practice, you need to add some small things.",
                    "label": 0
                },
                {
                    "sent": "For instance, we do a multiresolution approach to attract large motions in order to overcome the nonconvexity of the error function.",
                    "label": 0
                },
                {
                    "sent": "We use a human norm instead of L2 norm in order to be robust to outliers and occlusions, and moving objects and so on.",
                    "label": 1
                },
                {
                    "sent": "We introduce a sort of statistical normalization which respects all error sources on residual including depth noise and intensity noise.",
                    "label": 0
                },
                {
                    "sent": "And then it looks roughly like this.",
                    "label": 0
                },
                {
                    "sent": "So here you can see the image first on a core scale and then find out finer being tracked.",
                    "label": 0
                },
                {
                    "sent": "It actually runs very fast in real time on a single CPU core.",
                    "label": 0
                },
                {
                    "sent": "You can track it in real time no problem.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the tracking part.",
                    "label": 0
                },
                {
                    "sent": "The thing is, for in order to do that, we need a depth map and this depth map.",
                    "label": 0
                },
                {
                    "sent": "If you're using a depth camera, you get it for free, so that is why this kind of tracking is very popular for Kinect based approaches.",
                    "label": 0
                },
                {
                    "sent": "Here we have to have to estimate it.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's the epitome.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In component we represent depth Maps probabilistically, so for each pixel for which we have depth, which are all the pixels that have image gradient for all the color pixels, here we have Gaussian probability distribution on the inverse depth of that pixel.",
                    "label": 0
                },
                {
                    "sent": "Under Gaussian probability distribution is step by step refined or filtered overtime by integrating more and more stereo comparisons with other frames.",
                    "label": 0
                },
                {
                    "sent": "I'm starting with a very small baseline, explicitly taking advantage of the fact that we have a video so we can start with a small baseline and slowly increase the baseline and keep all the information in order to make it run in real time on the CPU, we do a very strict information selection, so we only do stereo if it's actually worth the cost.",
                    "label": 1
                },
                {
                    "sent": "Which kind of motivates to hold simulants idea because for pixels where there's no gradients, there is not going to give us any information.",
                    "label": 0
                },
                {
                    "sent": "So we leave them out.",
                    "label": 0
                },
                {
                    "sent": "In practice, it looks somewhat like this, so here you can see the April Alliance segments that are searched in each frame it slow motion.",
                    "label": 0
                },
                {
                    "sent": "You can see that in the beginning if new new hypothesis are initialized, there's a large red line, meaning that there's still a lot of variance, and we don't know where to look at, but then it gets smaller and smaller, and in the end it's just these small green lines tracking kind of the edge or corner or whatever radiant it is.",
                    "label": 0
                },
                {
                    "sent": "We then add small edge, preserving smoothing to kind of put a prior in there towards smooth depth Maps.",
                    "label": 0
                },
                {
                    "sent": "Distance based keyframe selection to select new keyframes.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that's the depth map estimation component.",
                    "label": 0
                },
                {
                    "sent": "So we have this loop.",
                    "label": 0
                },
                {
                    "sent": "Existing depth map is used to track and you frame.",
                    "label": 0
                },
                {
                    "sent": "This frame is then used to either make up a new keyframe or refined than UK.",
                    "label": 0
                },
                {
                    "sent": "The old key frame using stereo and then the new keyframe is used to track new frames.",
                    "label": 0
                },
                {
                    "sent": "There is this classical circular dependency which we need to initialize.",
                    "label": 0
                },
                {
                    "sent": "This is actually one of the nice things we can.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just initialize it using a random depth map so we start off with random depth map and then take this kind of alternating optimization between pose and depth map.",
                    "label": 0
                },
                {
                    "sent": "Take care of it, which works in most cases in practice.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But so far this is just in the dormitory.",
                    "label": 0
                },
                {
                    "sent": "It's no slam and it's not large scale slam.",
                    "label": 0
                },
                {
                    "sent": "So that's where the third part comes in.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Global map optimization.",
                    "label": 0
                },
                {
                    "sent": "So we do that by representing the world as a number of key frames and doing a pose graph of those key.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first important thing that you need to notice here is that monocular slam, so the absolute scale is not observable, so drift overtime.",
                    "label": 0
                },
                {
                    "sent": "So after a certain time after loop we not only have accumulated error in position and rotation, but also the scale has changed and we need to take that into account.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we do that by proposing a novel version of direct tracking on SIM three directly, so rotation, translation, and scale.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "To directly line 2 keyframes with two depth Maps and get the scaled rotational translation between them.",
                    "label": 0
                },
                {
                    "sent": "So we start off with the same tracking as before, just minimizing the photometric error, and now we add two small things.",
                    "label": 0
                },
                {
                    "sent": "So first.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We replace rigid body motion by rigid body motion plus scale.",
                    "label": 0
                },
                {
                    "sent": "Mathematically, it works the exact same way.",
                    "label": 0
                },
                {
                    "sent": "And 2nd we added a second residual term which says OK in addition to image intensity staying the same.",
                    "label": 0
                },
                {
                    "sent": "Also the depth should stay the same, so that's the second term which kind of is the difference of depth values?",
                    "label": 0
                },
                {
                    "sent": "You can minimize it.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Zach time way so.",
                    "label": 0
                },
                {
                    "sent": "Gauss Newton optimization with with a little twist, multiresolution approach, human norm statistical normalization, which also takes care of balancing these two different units against each other.",
                    "label": 0
                },
                {
                    "sent": "Um, and then we.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Put the result into a post graph framework where we optimize over deposes in the graph.",
                    "label": 0
                },
                {
                    "sent": "Each keyframe has a 7 dimensional post, so each keyframe has position, rotation and scale.",
                    "label": 0
                },
                {
                    "sent": "In the plots and videos will kind of visualize the scale as the size of the camera first term.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, in practice it looked like this, so the camera moves more and more.",
                    "label": 0
                },
                {
                    "sent": "Key frames are created more and more constraints are found and the more constraints are found, the more the individual depth Maps shift into place and are scaled rotated and translated to make up a nice model.",
                    "label": 0
                },
                {
                    "sent": "And you can also see that this model includes edges, for instance, or densely textured surfaces, not just points.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the whole algorithm.",
                    "label": 0
                },
                {
                    "sent": "What I showed you so far doesn't really qualify as large scale, so let me show you.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Example, which is a bit more large scale.",
                    "label": 0
                },
                {
                    "sent": "Here you can see the random initialization, it's just a green random dirt and now the camera starts moving and it converges quite quickly to a good depth map.",
                    "label": 0
                },
                {
                    "sent": "Black means very close to the camera, then red, green, blue is very far away.",
                    "label": 0
                },
                {
                    "sent": "This is about a 6 minutes trajectory.",
                    "label": 0
                },
                {
                    "sent": "Computations done in real time only on the CPU.",
                    "label": 0
                },
                {
                    "sent": "Actually the whole algorithm is optimizable to a GPU and could run much faster on GPU even.",
                    "label": 0
                },
                {
                    "sent": "Now you can see the accumulated drift and also scale drift, and there's one duplicated found whole thing shifts into place and you get a nice big map.",
                    "label": 0
                },
                {
                    "sent": "The total of 800 keyframes, 11,000 constraints and 51 million points.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's another example which is about twice as big.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "12 minutes, 36,000 frames tracked in total, 1000 keyframes and roughly 18,000 constraints.",
                    "label": 1
                },
                {
                    "sent": "One nice thing is that for each point in a depth map we not only have the depth, but we also have a measure on how accurate it is.",
                    "label": 0
                },
                {
                    "sent": "So what we do for these visualizations is we just put a threshold on the variance and say OK, we just want display the points that have a certain accuracy so we don't display all 100 million points, which would be a lot of points.",
                    "label": 0
                },
                {
                    "sent": "We only display the ones that are sufficiently accurate, which gives a very nice map which.",
                    "label": 0
                },
                {
                    "sent": "Also agrees with Google Maps.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because we can do that in real time on a CPU, we can also do it on the smartphone.",
                    "label": 0
                },
                {
                    "sent": "This short preview for our eyes.",
                    "label": 0
                },
                {
                    "sent": "My paper, which is going to be published tomorrow.",
                    "label": 0
                },
                {
                    "sent": "Officially I think in Munich, so here we do the same thing on smartphone and use it for a small augmented reality application.",
                    "label": 0
                },
                {
                    "sent": "Again, you can imagine that having the semi dense representation where you have a lot more information about edges in the image as well.",
                    "label": 0
                },
                {
                    "sent": "Gives you better opportunities to do augmented reality.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so to summarize.",
                    "label": 0
                },
                {
                    "sent": "There are three main components that make LSD slam work direct tracking, which is direct image alignment which is then used both for loop closure, constraint, constraint tracking as well as initial frame tracking.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We give a kind of general formulation as Gauss Newton minimization only manifolds.",
                    "label": 0
                },
                {
                    "sent": "Which time can be used?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cement stereo by filtering over many small baseline stereo comparisons and Bayesian Fusion of the information we get coupled with a strict information selection, so we only do stereo if it's actually worth the cost.",
                    "label": 1
                },
                {
                    "sent": "And that makes it run in real time on CPU.",
                    "label": 0
                },
                {
                    "sent": "And third pose graph optimization, including scale.",
                    "label": 0
                },
                {
                    "sent": "So simply post graph optimization to make a large scale global consistent map outfit.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "To conclude.",
                    "label": 0
                },
                {
                    "sent": "LSD slam is a large scale, fully direct monocular slam system.",
                    "label": 1
                },
                {
                    "sent": "We don't even need a separate keypoint based initializer.",
                    "label": 0
                },
                {
                    "sent": "It's it's a very first system that's without key points and can work on this scale.",
                    "label": 0
                },
                {
                    "sent": "It runs in real time on CPU you can expect even much faster results on the GPU and probably most interesting Lee, we're also decided to publish the source code within the next week, so we're going to put it open source, including some datasets, both videos, so you can try it out without having a webcam as well as point clouds.",
                    "label": 0
                },
                {
                    "sent": "If you just want to use a point cloud for further processing.",
                    "label": 0
                },
                {
                    "sent": "Thanks for the attention.",
                    "label": 0
                },
                {
                    "sent": "Questions great results and I'm a big big fan of direct intensity based methods are really glad to see these.",
                    "label": 0
                },
                {
                    "sent": "My question is could you comment on how this differs from the work that was done in the early 90s by the Sarnov Group and direct intensity based methods for 3D reconstruction?",
                    "label": 0
                },
                {
                    "sent": "From what I could tell, the main additions here were the large scale and the real time.",
                    "label": 0
                },
                {
                    "sent": "Are there any additional things that I'm to be honest?",
                    "label": 0
                },
                {
                    "sent": "I'm not entirely sure which work you're referring to.",
                    "label": 0
                },
                {
                    "sent": "It started with the worth of.",
                    "label": 0
                },
                {
                    "sent": "Work of Keith Hannah and undone.",
                    "label": 0
                },
                {
                    "sent": "The reconstruction of 3D camera motion and depth using direct intensity based methods and then it continued to the plane plus parallax work that lots of us did.",
                    "label": 0
                },
                {
                    "sent": "Also using direct intensity based method.",
                    "label": 0
                },
                {
                    "sent": "If you're not familiar with, I'll give you some references later, like the only work that early that I'm a familiar of is from Canada and Solinski and matches actually where they do this similar kind of depth map estimation, but only for planar motion and without tracking.",
                    "label": 0
                },
                {
                    "sent": "So I'll give you some references later, yes.",
                    "label": 0
                },
                {
                    "sent": "Hi I have a over here.",
                    "label": 0
                },
                {
                    "sent": "Well, you have your intensity constant intensity assumption and I saw this works for outside scenes.",
                    "label": 0
                },
                {
                    "sent": "So how do you handle this?",
                    "label": 0
                },
                {
                    "sent": "Can you comment on what happens if the intensity changes if the lighting changes, so you're right and some of the sequences I showed there were pretty drastic lightning changes auto shutter.",
                    "label": 0
                },
                {
                    "sent": "Being complicated so it actually works fairly well even if there is lighting changes, because we only use image regions with certain gradient and lightning changes only have have a bad effect if the lightning change is bigger than the gradient you're using.",
                    "label": 0
                },
                {
                    "sent": "So if there is a black and white edge and before and after the white is a bit brighter and the black is a bit brighter, but apart from that the edge stays the same place, then it doesn't actually make a difference.",
                    "label": 0
                },
                {
                    "sent": "We have experimental support for kind of a feen lightning estimation, so estimated the whole frame got a bit brighter and compensate for that, but it's not in there yet.",
                    "label": 0
                },
                {
                    "sent": "Hi I have a question from the balcony.",
                    "label": 0
                },
                {
                    "sent": "Do you have any quantitative results on the drift, especially in the large scale as well as on the pose as well as the three point reconstruction?",
                    "label": 0
                },
                {
                    "sent": "Yes, so we have benchmark results on the pose estimation accuracy.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, there's not many good benchmarks out there for monocular slam, neither for stereo or for depth depth or kynec based slam, and in both cases or formula slam you have certain unobservable modes which you don't have for stereo or depth based slam.",
                    "label": 0
                },
                {
                    "sent": "So many of the directories don't work for any monocular for anymore algorithm, but the ones that do work we do have numeric results in them.",
                    "label": 0
                },
                {
                    "sent": "Maybe I missed this, but how do you detect loop closure from the your keyframes, yes, good question.",
                    "label": 0
                },
                {
                    "sent": "So there's two ways we can do that.",
                    "label": 0
                },
                {
                    "sent": "So one thing we do is we just try to track on a very coarse resolution on close by frames that have a certain overlap, which is very fast, because if you do it on a small resolution it's 30 * 40 pixels and you can very quickly decide whether it's a potential disclosure or not.",
                    "label": 0
                },
                {
                    "sent": "In order to detect the really large loop closures, which for instance, the one where I showed the one big loop.",
                    "label": 0
                },
                {
                    "sent": "Where you can't do that, do it.",
                    "label": 0
                },
                {
                    "sent": "That way we use an appearance based loop closure detection algorithm open popping up in that case to kind of tell us.",
                    "label": 0
                },
                {
                    "sent": "OK, this frame might be a loop closure.",
                    "label": 0
                },
                {
                    "sent": "Try tracking it.",
                    "label": 0
                },
                {
                    "sent": "But for the for the other sequence I showed with the chairs and umbrellas, we didn't use him up because there all the loops are so small that the accumulated drift is small enough.",
                    "label": 0
                },
                {
                    "sent": "Hey I have a question.",
                    "label": 0
                },
                {
                    "sent": "I can imagine that intensity based approach breaks faster part if the baseline bit between 2 frames is is to far away, yes, do you have any experiments of benchmarks on this?",
                    "label": 0
                },
                {
                    "sent": "How far you can go here so we don't have numbers, but by experience you're right it does.",
                    "label": 0
                },
                {
                    "sent": "It is inherently based on having a video and not only individual frames with a large baseline.",
                    "label": 0
                },
                {
                    "sent": "So we need a certain frame rate.",
                    "label": 0
                },
                {
                    "sent": "Let's say 30 frames per second and then it's no problem because then you have those small baselines always or like either you have a small baseline or you have a good initialization for tracking from the previous frame.",
                    "label": 0
                },
                {
                    "sent": "If you only want to apply it to 5 frames per second video or 10 frames per second video, then the motion has to be sufficiently small.",
                    "label": 0
                },
                {
                    "sent": "OK, so then let's thank the speakers of the session again.",
                    "label": 0
                }
            ]
        }
    }
}