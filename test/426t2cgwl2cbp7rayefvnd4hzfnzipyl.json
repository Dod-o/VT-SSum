{
    "id": "426t2cgwl2cbp7rayefvnd4hzfnzipyl",
    "title": "Learning Feature Hierarchies by Learning Deep Generative Models",
    "info": {
        "author": [
            "Ruslan Salakhutdinov, Machine Learning Department, Carnegie Mellon University"
        ],
        "published": "March 26, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Feature Selection"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_salakhutdinov_lfhldgm/",
    "segmentation": [
        [
            "Alright, so now."
        ],
        [
            "So after this segue into deep belief, net at the end that he didn't have time to talk about.",
            "So have Ruslaan selected a who did his PhD with Jeff Hinton in Toronto and now is a postdoc at MIT.",
            "And he's going to talk about learning feature hierarchies by learning deep generative models.",
            "Thank you, Simone.",
            "So this is a brief talk outline.",
            "I'm going to talk about my view of generative versus discriminative interface.",
            "Then I'm going to say just a few things about deep belief networks, not too much, 'cause I think a lot of people in that community probably know about them, but what I"
        ],
        [
            "Will concentrate on is kernel learning with deep belief networks.",
            "In particular, concentrate on Gaussian processes and normally nonlinear version of neighborhood component analysis.",
            "And then I'll just say a few things about both machines.",
            "These sort of a little different beasts from deep belief networks.",
            "So throughout this talk I'm going to make an assumption that in many cases you have very high."
        ],
        [
            "I have high dimensional, highly structured data and you have lots and lots of unlabeled data and very small amount of labeled data.",
            "So throughout all my talk keep that in mind, and that's precisely the configuration I'm working with.",
            "And of course in applications such as information retrieval and machine vision, you have plenty of unlabeled data just available online.",
            "So let me be a little bit more precise.",
            "Suppose I have a set of IID training samples XLY some L. Is there?",
            "And.",
            "Now if you look at the discriminative models, discriminative models model, conditional distribution of labeled labels, given the input data right and in models like logistic regression, Gaussian process support recognition, that's what you have.",
            "But if you have a large supply of unlabeled data, well, what you have to do, no matter what you do, you have to make some kind of assumptions about unlabeled data.",
            "There is a lot of research."
        ],
        [
            "Going on in sort of trying to make an assumption that there is some smooth manifold that the data comes from Laplacian nagging match that Community does that.",
            "And you have to make some assumptions that otherwise you can't use unlabeled data.",
            "Particularly trying to use it within the discriminative models.",
            "So the key point of deep generative models, so at least how they're being used in this communities, is following.",
            "You're going to learn probabilistic model.",
            "Of your unlabeled data.",
            "In particular, is going to be hierarchical probabilistic model.",
            "Then you're going to use the loan parameters Theta.",
            "Start that you get by learning probabilistic model and you're going to initialize the discriminative model such as a neural network.",
            "OK, and then you're going to slightly just that discriminative model for specific tasks that you might have.",
            "That you might trying to solve.",
            "So one key thing that I want to point out here is that most of the information in learning these parameters will be coming from learning this model, because you may imagine you have tons and tons of unlabeled data and just a little bit of tuning of these parameters will be coming from the labeled data.",
            "That's the interface that."
        ],
        [
            "At least the way I see machine learning community in particular community who are doing working with the belief networks are working and one particularly nice application is that whenever you're learning this model here, you don't need labels.",
            "In particular, you don't have any knowledge of whatever subsequent discriminative task you might be solving.",
            "OK, so let me just say a couple of slides about deep belief networks.",
            "I think there's going to be looked after me also about deep belief networks.",
            "Building block of deep belief networks are these little modules called restricted Boltzmann machines?",
            "They have a special by part type type of structure you."
        ],
        [
            "Sets of visible units.",
            "You can imagine these as pixels in your image or word counts in your document and you have binary stochastic hidden units and you can just write down the joint distribution of our visible and hidden units in this form, and this is nothing more than a Markov random field with hidden variables.",
            "Log linear model both machines, however you want to call it and the idea of deep belief networks is that you can in unsupervised fashion, learn this deep hierarchy by just learning these little modules one layer at a time.",
            "And the intuition here is that you learn this module here.",
            "Then you sample the states of the hidden variables given the states of the visible variables and you just treat it as the data for training next layer module.",
            "So effectively what you're doing here is you taking the data and you gradually pushing it all the way up here.",
            "OK, that's"
        ],
        [
            "That's the intuition that allows you to learn some reasonable representations and and people have discovered that this way of training models really gives you very nice representations that you can use for in many tasks such as vision, object recognition, text retrieval and such.",
            "I'll talk a little bit about these models as well, but let me now concentrate on kernel learning.",
            "The idea here is that these deep models can actually be used to learn kernel functions.",
            "For for discriminative models such as support vector machines, aggressive Gaussian processes and such, and there's been some work, for example, trying to learn covariance kernel of a Gaussian process.",
            "And the idea here is exactly the same as I spelled before.",
            "We're going to learn a deep generative model that models the input data in entirely unsupervised way.",
            "You can do it by using a deep belief network or debossing machine.",
            "Then you're going to use these parameters to initialize a kernel function.",
            "OK, and the kernel function is now going to be parameterized by these parameters W. And then I."
        ],
        [
            "Just think about these framework of deep learning is that you can actually use backpropagation algorithm to refine these parameters of your kernel function for specific tasks that you might have.",
            "So let me give one specific example an example of learning covariance function for Gaussian process.",
            "OK, here's here's a pictorial representation.",
            "These are input images, images of faces, you learn stack of restricted Boltzmann machines that defines a function here.",
            "That Maps the input down to some representation.",
            "We can call it feature representation and then once you have that function, that nonlinear function you can just simply define a kernel in this way.",
            "Just a squared exponential kernel.",
            "You can also think of it as just defining or just."
        ],
        [
            "Building a Gaussian process in this space instead of building Gaussian process in this space.",
            "That's the intuition.",
            "But the nice thing is that this function F is differentiable, meaning that you can actually learn these parameters of your kernel function by maximizing marginal likelihood and backpropagating through the entire network.",
            "So let's let's look at a particular example.",
            "Here's an interesting example.",
            "Suppose you're trying to predict an orientation of a face.",
            "So this is the input.",
            "These are 28 by 28 images of.",
            "I think of 20 people.",
            "They've been cropped and rotated and you're trying to predict the rotation of the face.",
            "OK, so the labeled training data you have 1000 labeled training examples and you're trying to predict the orientation at the test time you have 1000 labeled test batches of new people, so these actually new faces an you again trying to predict the orientation of the face.",
            "Now, if you build the Gaussian process with squared with exponential kernel, you get something like 16 degree."
        ],
        [
            "Addiction so we can predict up to 16 degrees which which is not bad and we know that Gaussian processes really work well in practice, so it's very hard to beat.",
            "These these guys, people who worked with Gaussian processes probably know that as a regression tool.",
            "That's a fantastic tool.",
            "So what if we do the following?",
            "Well, suppose I give you unlabeled data, so I give you 12,000 face page batches of the same people in the training data.",
            "So all I've done is just edit more unlabeled data, that's all.",
            "Now I'm going to learn a deep belief network that has this structure.",
            "It has since it has 74 visible units.",
            "These are pixels of your image.",
            "You have 1000 Kenyans and 1000 hidden units.",
            "If you just learn that particular deep belief networks again here when we're learning this model, we are not.",
            "We don't know what this model is going to be used for.",
            "All we're trying to do is we're trying to understand how what makes a face to face, what kind of structure we have in in data.",
            "That's all we're doing.",
            "Now, if you build the Gaussian process in this space exactly the same Gaussian process, you can get looking squared error about 11 degrees from 16 degrees to 11 degrees.",
            "That's good, but the interesting thing, if you actually try to fine tune the covariance function."
        ],
        [
            "By backpropagating through the marginal likelihood you can get 6, seven, 6.5 degrees, so you can fairly accurately predict the orientation of the face.",
            "And again, this is just one example, let me show another example.",
            "This same style models could be used for dimensionality reduction.",
            "And again, this is this was done on the Reuters corpus, where we have 800,000 documents and we used simple bag of words representation and we built this deep belief network.",
            "And again I want to emphasize here that the entire model was built in completely unsupervised way.",
            "There were no labels used in training this model.",
            "OK, and so this is sort of the kind of structure that it extracts as opposed to SVD type of model.",
            "And you know this model extracts very nice representations in particularly puts European Community next to disasters and accidents.",
            "So that was we thought that was great because the data was actually collected in 97 and 98 and so.",
            "Yeah.",
            "So."
        ],
        [
            "This is just this is just.",
            "Again, exactly the same algorithm, and again exactly the same tradeoff between discriminative and generative training, where you're trying to model the input distribution generatively, and then you just using discriminative models to find to find unit using autoencoder type models.",
            "One other interesting thing you can do with these types of models is nonlinear neighborhood component analysis.",
            "And the intuition here is that suppose I give you 2 images and I tell you that they belong to the same class.",
            "If I tell you that they belong to the same class, what you want to do is you want to map them down to some low dimensional space such that those two images are mapped close to each other.",
            "OK, that's sort of the objective function that you optimizing, But again, here I want to stress out that you're doing unsupervised learning to learn."
        ],
        [
            "Transformation completely unsupervised way.",
            "And then you do discriminative sort of fine tuning where you're trying to optimize for a discriminative objective function.",
            "Or you're trying to refine these weights such that things that have the same class label have similar mappings in the low dimensional space.",
            "So for example, you can do interesting things like.",
            "This is a 2 dimensional mappings.",
            "Learn for me just so you can see it."
        ],
        [
            "Nicely arranges all the digits and this is done on the test data.",
            "And here's you can do the same thing with documents, so you can see that also arranges them in a nice topics.",
            "These on 20 newsgroup datasets.",
            "So finally I just want to say a few words about the Boltzmann machines.",
            "These are a little bit different flavored generative models.",
            "You can think of them as Markov random fields with hidden variables.",
            "Um, that's all they are.",
            "You can write the joint.",
            "You can write the probability of V as having this expression, so again, it's a log linear model with hidden variables and we call it the boss machine.",
            "There is a fast grid initialization, just like I talked about for deep belief network, so you can initialize by this model by learning one layer at a time.",
            "And again, you're doing a supervised learning generative learning to get high level representations, and then you can use."
        ],
        [
            "The labeled data to slightly adjust your model for for whatever task you might have and you know you can do fun things with these models, such as.",
            "The nice thing about building generative models is that you can actually generate data from them and see what they learn.",
            "And then you can also fine tune them for discriminative task and see how well they do so.",
            "Here are the model samples.",
            "This is North images.",
            "These are 96 by 96 images and you can see these images of objects like airplanes, cars, trucks and people with guns.",
            "Like like these guys, an animal and animals.",
            "And these are samples generated from the model, so you can see the model is actually able to capture a lot of structure in the data.",
            "And again if you look at the performance discriminative performance of this model, you can get the test error of seven point 2%.",
            "This is compared to support vector machines and logistic regressions that don't quite compete with that model.",
            "Of course, learning deep generative models in general is a big subject in itself.",
            "Coming up with efficient.",
            "Inference and learning algorithms is is is a big problem in itself and you know, we don't.",
            "We don't necessarily do a very fantastic job in learning these models in particular.",
            "If you look at the images."
        ],
        [
            "Rated by the model, you can see that the model is a little bit disbalanced in the sense that it generates a lot of people with guns.",
            "So you can see these guys and that's a failure of Markov chain Monte Carlo type of algorithm to sort of what's called persistency.",
            "D to be able to mix between the modes.",
            "And finally, you can do fun things like image completion.",
            "Right, you can do fun things like these are your test images and here I want to emphasize this is a very nice data set because these images are not part of the training data.",
            "So for example there is no cowboy in the training data.",
            "It's just he just doesn't exist.",
            "There is a different objects.",
            "And now if you give, this is what you give to the algorithm and this is what it feels in so you can see that it's able to sort of recognize that.",
            "If you have half a lack, there should be another half a leg here and there should be an arm, probably with a gun.",
            "And things like airplanes if you give it part of the wing, it sort of figures out that they should be another wing, so so it does.",
            "It's able to capture a lot of interesting structure in the images, even though these are simplistic images and this was 1 interesting example this was."
        ],
        [
            "Nelk donkey, but there is no donkey in the training data and if I give it to my algorithm and say well do completion, this is what it completes.",
            "Because there are horses in the training data, sort of this form so it's able to."
        ],
        [
            "Figure out that they should be ahead.",
            "And.",
            "I'm done.",
            "So we have time for a few questions.",
            "And then all my grantier you had, I believe 505 hundred 2000 and then 30.",
            "Why don't you just do the similarity in the 2000 dimension?",
            "Uh huh, yeah.",
            "So typically the way we want to use these models.",
            "For example, if you want to use these models for information retrieval, what you want to do is you want to map high dimensional inputs into low dimensional space such that you can do K nearest neighbors much faster than in the original input space original.",
            "Input right, So what you really want to do to do fast retrieval is you really want to compress down to some low dimensional space, and you can also use these tools for visualizing the data.",
            "Home about why you blocked the space right before the final compression.",
            "There is no particular reason that compression could have been 1000, it's just that they have these models already trained.",
            "So you can just build something on top of them.",
            "But there is no particular reason to just have 2000 here could have been 1000."
        ],
        [
            "So 500 I think the results wouldn't differ by much.",
            "I was wondering you mentioned you can use these things to learn Curtis and I was wondering how you enforce the public sentiment in Spain.",
            "And Secondly there are 4 central St actually refer performance and it would be better to use.",
            "Furino Labrador picture authenticity.",
            "So for example in that particular kernel, right?",
            "If you add sort of stuff on the diagonal, you would enforce positive definite constraints.",
            "So we sort of we don't explicitly in full enforce, it just comes out of.",
            "Of the way we define the Colonel.",
            "So one interesting question.",
            "It would be, could you also try to learn the kernel for support vector machine?",
            "Particularly interested in classification file style models?",
            "What people do right now is they typically train these models and they stick a support vector machine just on the top layer.",
            "But I think you can also try to use back propagation and actually try to learn.",
            "The kernel for support vector machines.",
            "There is a lot of I guess there is parallel workshop where people are really trying to figure out how to learn.",
            "Kernel functions for different algorithms and so far.",
            "If you only use labeled data, you need a lot of labeled data.",
            "Learning kernel function, but this works.",
            "Sort of suggests that maybe if you have unlabeled data, maybe you can learn a reasonable kernel function or try to learn what similar.",
            "Advantages are not equal function as opposed to.",
            "Netflix.",
            "Yes, absolutely, because if you just have a neural network you will overfit grossly like.",
            "Imagine what happens with these kernel functions is that their little bit weird because you have a kernel function and you have 80,000 parameters in the kernel function.",
            "So it's like for every parameter in your model you have like 1000 hyperparameters, which is odd, but nevertheless most of the information in those parameters really come from modeling the input distribution.",
            "And that's what makes a difference.",
            "That's why you don't.",
            "You don't overfit or.",
            "Do we have time for two minutes?",
            "What is the distribution?",
            "So we try to model the only distribution there.",
            "Either she added mobile and they say you're you're talking small files in space, like without like Alien Band program for instance.",
            "And you have experience with that bike.",
            "Training Internet normal and then actually saying I'd like to see naked microphone screen for iPhone so so that comes to the question.",
            "Suppose I have the following model.",
            "I have an input image and have it first pixel to the 1st between zero.",
            "I say that's why zero those pixels one.",
            "I say it's class one and everything is going on the image data that these models will not work.",
            "But I claim that that's a silly problem.",
            "The most extreme case, so right, so right?",
            "So when people are doing right now, he's actually you can take your top level representation as well as your bottom level presentations.",
            "Teach them together and use that as your organization.",
            "So even if there is some little piece in the training data or part of the training data, that's really important for discrimination.",
            "They're obviously degenerative model tractor model.",
            "Everything is going over there.",
            "So you can do the distinction together and try to figure out what's important.",
            "Since the divorce.",
            "That's before the fine tuning.",
            "In the face.",
            "It seems to me that it's almost like a feature generation incomplete if he gives other irrelevant generates whatever Fisher is they fit in a discrete model and to define your learning.",
            "It would be connecting.",
            "So you can use if you have some other algorithms that did that produces nice features in statistical data into your discriminative model.",
            "That should be fine, it's just I guess people before just really think about it saying that.",
            "Actually just doing feature generation job.",
            "You can think in the in the plane.",
            "Possible thing you can think of it believe networks as a black box that given the input, produces some output and that's a mouthful, is much better representation of the.",
            "The nice thing about the belief networks is that you can actually back propagate within Gaia network to suggest around.",
            "So it's not like.",
            "I mean I can hit series features and I throw them into my into my discriminative model because I can't.",
            "Just what's going on inside of the signatures, whereas in these models language.",
            "OK, first language.",
            "I forgot."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so now.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So after this segue into deep belief, net at the end that he didn't have time to talk about.",
                    "label": 0
                },
                {
                    "sent": "So have Ruslaan selected a who did his PhD with Jeff Hinton in Toronto and now is a postdoc at MIT.",
                    "label": 0
                },
                {
                    "sent": "And he's going to talk about learning feature hierarchies by learning deep generative models.",
                    "label": 1
                },
                {
                    "sent": "Thank you, Simone.",
                    "label": 0
                },
                {
                    "sent": "So this is a brief talk outline.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about my view of generative versus discriminative interface.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to say just a few things about deep belief networks, not too much, 'cause I think a lot of people in that community probably know about them, but what I",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will concentrate on is kernel learning with deep belief networks.",
                    "label": 0
                },
                {
                    "sent": "In particular, concentrate on Gaussian processes and normally nonlinear version of neighborhood component analysis.",
                    "label": 0
                },
                {
                    "sent": "And then I'll just say a few things about both machines.",
                    "label": 0
                },
                {
                    "sent": "These sort of a little different beasts from deep belief networks.",
                    "label": 0
                },
                {
                    "sent": "So throughout this talk I'm going to make an assumption that in many cases you have very high.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I have high dimensional, highly structured data and you have lots and lots of unlabeled data and very small amount of labeled data.",
                    "label": 0
                },
                {
                    "sent": "So throughout all my talk keep that in mind, and that's precisely the configuration I'm working with.",
                    "label": 0
                },
                {
                    "sent": "And of course in applications such as information retrieval and machine vision, you have plenty of unlabeled data just available online.",
                    "label": 0
                },
                {
                    "sent": "So let me be a little bit more precise.",
                    "label": 0
                },
                {
                    "sent": "Suppose I have a set of IID training samples XLY some L. Is there?",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "Now if you look at the discriminative models, discriminative models model, conditional distribution of labeled labels, given the input data right and in models like logistic regression, Gaussian process support recognition, that's what you have.",
                    "label": 0
                },
                {
                    "sent": "But if you have a large supply of unlabeled data, well, what you have to do, no matter what you do, you have to make some kind of assumptions about unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "There is a lot of research.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Going on in sort of trying to make an assumption that there is some smooth manifold that the data comes from Laplacian nagging match that Community does that.",
                    "label": 0
                },
                {
                    "sent": "And you have to make some assumptions that otherwise you can't use unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Particularly trying to use it within the discriminative models.",
                    "label": 0
                },
                {
                    "sent": "So the key point of deep generative models, so at least how they're being used in this communities, is following.",
                    "label": 1
                },
                {
                    "sent": "You're going to learn probabilistic model.",
                    "label": 1
                },
                {
                    "sent": "Of your unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "In particular, is going to be hierarchical probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "Then you're going to use the loan parameters Theta.",
                    "label": 0
                },
                {
                    "sent": "Start that you get by learning probabilistic model and you're going to initialize the discriminative model such as a neural network.",
                    "label": 1
                },
                {
                    "sent": "OK, and then you're going to slightly just that discriminative model for specific tasks that you might have.",
                    "label": 0
                },
                {
                    "sent": "That you might trying to solve.",
                    "label": 1
                },
                {
                    "sent": "So one key thing that I want to point out here is that most of the information in learning these parameters will be coming from learning this model, because you may imagine you have tons and tons of unlabeled data and just a little bit of tuning of these parameters will be coming from the labeled data.",
                    "label": 0
                },
                {
                    "sent": "That's the interface that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At least the way I see machine learning community in particular community who are doing working with the belief networks are working and one particularly nice application is that whenever you're learning this model here, you don't need labels.",
                    "label": 0
                },
                {
                    "sent": "In particular, you don't have any knowledge of whatever subsequent discriminative task you might be solving.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me just say a couple of slides about deep belief networks.",
                    "label": 0
                },
                {
                    "sent": "I think there's going to be looked after me also about deep belief networks.",
                    "label": 0
                },
                {
                    "sent": "Building block of deep belief networks are these little modules called restricted Boltzmann machines?",
                    "label": 1
                },
                {
                    "sent": "They have a special by part type type of structure you.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sets of visible units.",
                    "label": 0
                },
                {
                    "sent": "You can imagine these as pixels in your image or word counts in your document and you have binary stochastic hidden units and you can just write down the joint distribution of our visible and hidden units in this form, and this is nothing more than a Markov random field with hidden variables.",
                    "label": 0
                },
                {
                    "sent": "Log linear model both machines, however you want to call it and the idea of deep belief networks is that you can in unsupervised fashion, learn this deep hierarchy by just learning these little modules one layer at a time.",
                    "label": 0
                },
                {
                    "sent": "And the intuition here is that you learn this module here.",
                    "label": 0
                },
                {
                    "sent": "Then you sample the states of the hidden variables given the states of the visible variables and you just treat it as the data for training next layer module.",
                    "label": 0
                },
                {
                    "sent": "So effectively what you're doing here is you taking the data and you gradually pushing it all the way up here.",
                    "label": 0
                },
                {
                    "sent": "OK, that's",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's the intuition that allows you to learn some reasonable representations and and people have discovered that this way of training models really gives you very nice representations that you can use for in many tasks such as vision, object recognition, text retrieval and such.",
                    "label": 0
                },
                {
                    "sent": "I'll talk a little bit about these models as well, but let me now concentrate on kernel learning.",
                    "label": 0
                },
                {
                    "sent": "The idea here is that these deep models can actually be used to learn kernel functions.",
                    "label": 1
                },
                {
                    "sent": "For for discriminative models such as support vector machines, aggressive Gaussian processes and such, and there's been some work, for example, trying to learn covariance kernel of a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "And the idea here is exactly the same as I spelled before.",
                    "label": 0
                },
                {
                    "sent": "We're going to learn a deep generative model that models the input data in entirely unsupervised way.",
                    "label": 1
                },
                {
                    "sent": "You can do it by using a deep belief network or debossing machine.",
                    "label": 0
                },
                {
                    "sent": "Then you're going to use these parameters to initialize a kernel function.",
                    "label": 0
                },
                {
                    "sent": "OK, and the kernel function is now going to be parameterized by these parameters W. And then I.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just think about these framework of deep learning is that you can actually use backpropagation algorithm to refine these parameters of your kernel function for specific tasks that you might have.",
                    "label": 0
                },
                {
                    "sent": "So let me give one specific example an example of learning covariance function for Gaussian process.",
                    "label": 1
                },
                {
                    "sent": "OK, here's here's a pictorial representation.",
                    "label": 0
                },
                {
                    "sent": "These are input images, images of faces, you learn stack of restricted Boltzmann machines that defines a function here.",
                    "label": 0
                },
                {
                    "sent": "That Maps the input down to some representation.",
                    "label": 0
                },
                {
                    "sent": "We can call it feature representation and then once you have that function, that nonlinear function you can just simply define a kernel in this way.",
                    "label": 0
                },
                {
                    "sent": "Just a squared exponential kernel.",
                    "label": 0
                },
                {
                    "sent": "You can also think of it as just defining or just.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Building a Gaussian process in this space instead of building Gaussian process in this space.",
                    "label": 0
                },
                {
                    "sent": "That's the intuition.",
                    "label": 0
                },
                {
                    "sent": "But the nice thing is that this function F is differentiable, meaning that you can actually learn these parameters of your kernel function by maximizing marginal likelihood and backpropagating through the entire network.",
                    "label": 0
                },
                {
                    "sent": "So let's let's look at a particular example.",
                    "label": 0
                },
                {
                    "sent": "Here's an interesting example.",
                    "label": 0
                },
                {
                    "sent": "Suppose you're trying to predict an orientation of a face.",
                    "label": 1
                },
                {
                    "sent": "So this is the input.",
                    "label": 0
                },
                {
                    "sent": "These are 28 by 28 images of.",
                    "label": 0
                },
                {
                    "sent": "I think of 20 people.",
                    "label": 0
                },
                {
                    "sent": "They've been cropped and rotated and you're trying to predict the rotation of the face.",
                    "label": 0
                },
                {
                    "sent": "OK, so the labeled training data you have 1000 labeled training examples and you're trying to predict the orientation at the test time you have 1000 labeled test batches of new people, so these actually new faces an you again trying to predict the orientation of the face.",
                    "label": 1
                },
                {
                    "sent": "Now, if you build the Gaussian process with squared with exponential kernel, you get something like 16 degree.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Addiction so we can predict up to 16 degrees which which is not bad and we know that Gaussian processes really work well in practice, so it's very hard to beat.",
                    "label": 0
                },
                {
                    "sent": "These these guys, people who worked with Gaussian processes probably know that as a regression tool.",
                    "label": 0
                },
                {
                    "sent": "That's a fantastic tool.",
                    "label": 0
                },
                {
                    "sent": "So what if we do the following?",
                    "label": 0
                },
                {
                    "sent": "Well, suppose I give you unlabeled data, so I give you 12,000 face page batches of the same people in the training data.",
                    "label": 1
                },
                {
                    "sent": "So all I've done is just edit more unlabeled data, that's all.",
                    "label": 1
                },
                {
                    "sent": "Now I'm going to learn a deep belief network that has this structure.",
                    "label": 0
                },
                {
                    "sent": "It has since it has 74 visible units.",
                    "label": 0
                },
                {
                    "sent": "These are pixels of your image.",
                    "label": 0
                },
                {
                    "sent": "You have 1000 Kenyans and 1000 hidden units.",
                    "label": 0
                },
                {
                    "sent": "If you just learn that particular deep belief networks again here when we're learning this model, we are not.",
                    "label": 0
                },
                {
                    "sent": "We don't know what this model is going to be used for.",
                    "label": 0
                },
                {
                    "sent": "All we're trying to do is we're trying to understand how what makes a face to face, what kind of structure we have in in data.",
                    "label": 0
                },
                {
                    "sent": "That's all we're doing.",
                    "label": 0
                },
                {
                    "sent": "Now, if you build the Gaussian process in this space exactly the same Gaussian process, you can get looking squared error about 11 degrees from 16 degrees to 11 degrees.",
                    "label": 1
                },
                {
                    "sent": "That's good, but the interesting thing, if you actually try to fine tune the covariance function.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By backpropagating through the marginal likelihood you can get 6, seven, 6.5 degrees, so you can fairly accurately predict the orientation of the face.",
                    "label": 0
                },
                {
                    "sent": "And again, this is just one example, let me show another example.",
                    "label": 0
                },
                {
                    "sent": "This same style models could be used for dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "And again, this is this was done on the Reuters corpus, where we have 800,000 documents and we used simple bag of words representation and we built this deep belief network.",
                    "label": 0
                },
                {
                    "sent": "And again I want to emphasize here that the entire model was built in completely unsupervised way.",
                    "label": 1
                },
                {
                    "sent": "There were no labels used in training this model.",
                    "label": 0
                },
                {
                    "sent": "OK, and so this is sort of the kind of structure that it extracts as opposed to SVD type of model.",
                    "label": 1
                },
                {
                    "sent": "And you know this model extracts very nice representations in particularly puts European Community next to disasters and accidents.",
                    "label": 0
                },
                {
                    "sent": "So that was we thought that was great because the data was actually collected in 97 and 98 and so.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is just this is just.",
                    "label": 0
                },
                {
                    "sent": "Again, exactly the same algorithm, and again exactly the same tradeoff between discriminative and generative training, where you're trying to model the input distribution generatively, and then you just using discriminative models to find to find unit using autoencoder type models.",
                    "label": 0
                },
                {
                    "sent": "One other interesting thing you can do with these types of models is nonlinear neighborhood component analysis.",
                    "label": 1
                },
                {
                    "sent": "And the intuition here is that suppose I give you 2 images and I tell you that they belong to the same class.",
                    "label": 0
                },
                {
                    "sent": "If I tell you that they belong to the same class, what you want to do is you want to map them down to some low dimensional space such that those two images are mapped close to each other.",
                    "label": 0
                },
                {
                    "sent": "OK, that's sort of the objective function that you optimizing, But again, here I want to stress out that you're doing unsupervised learning to learn.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Transformation completely unsupervised way.",
                    "label": 0
                },
                {
                    "sent": "And then you do discriminative sort of fine tuning where you're trying to optimize for a discriminative objective function.",
                    "label": 0
                },
                {
                    "sent": "Or you're trying to refine these weights such that things that have the same class label have similar mappings in the low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "So for example, you can do interesting things like.",
                    "label": 0
                },
                {
                    "sent": "This is a 2 dimensional mappings.",
                    "label": 0
                },
                {
                    "sent": "Learn for me just so you can see it.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nicely arranges all the digits and this is done on the test data.",
                    "label": 0
                },
                {
                    "sent": "And here's you can do the same thing with documents, so you can see that also arranges them in a nice topics.",
                    "label": 0
                },
                {
                    "sent": "These on 20 newsgroup datasets.",
                    "label": 0
                },
                {
                    "sent": "So finally I just want to say a few words about the Boltzmann machines.",
                    "label": 0
                },
                {
                    "sent": "These are a little bit different flavored generative models.",
                    "label": 0
                },
                {
                    "sent": "You can think of them as Markov random fields with hidden variables.",
                    "label": 1
                },
                {
                    "sent": "Um, that's all they are.",
                    "label": 0
                },
                {
                    "sent": "You can write the joint.",
                    "label": 0
                },
                {
                    "sent": "You can write the probability of V as having this expression, so again, it's a log linear model with hidden variables and we call it the boss machine.",
                    "label": 0
                },
                {
                    "sent": "There is a fast grid initialization, just like I talked about for deep belief network, so you can initialize by this model by learning one layer at a time.",
                    "label": 0
                },
                {
                    "sent": "And again, you're doing a supervised learning generative learning to get high level representations, and then you can use.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The labeled data to slightly adjust your model for for whatever task you might have and you know you can do fun things with these models, such as.",
                    "label": 0
                },
                {
                    "sent": "The nice thing about building generative models is that you can actually generate data from them and see what they learn.",
                    "label": 0
                },
                {
                    "sent": "And then you can also fine tune them for discriminative task and see how well they do so.",
                    "label": 0
                },
                {
                    "sent": "Here are the model samples.",
                    "label": 1
                },
                {
                    "sent": "This is North images.",
                    "label": 0
                },
                {
                    "sent": "These are 96 by 96 images and you can see these images of objects like airplanes, cars, trucks and people with guns.",
                    "label": 0
                },
                {
                    "sent": "Like like these guys, an animal and animals.",
                    "label": 0
                },
                {
                    "sent": "And these are samples generated from the model, so you can see the model is actually able to capture a lot of structure in the data.",
                    "label": 0
                },
                {
                    "sent": "And again if you look at the performance discriminative performance of this model, you can get the test error of seven point 2%.",
                    "label": 0
                },
                {
                    "sent": "This is compared to support vector machines and logistic regressions that don't quite compete with that model.",
                    "label": 0
                },
                {
                    "sent": "Of course, learning deep generative models in general is a big subject in itself.",
                    "label": 1
                },
                {
                    "sent": "Coming up with efficient.",
                    "label": 0
                },
                {
                    "sent": "Inference and learning algorithms is is is a big problem in itself and you know, we don't.",
                    "label": 0
                },
                {
                    "sent": "We don't necessarily do a very fantastic job in learning these models in particular.",
                    "label": 0
                },
                {
                    "sent": "If you look at the images.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rated by the model, you can see that the model is a little bit disbalanced in the sense that it generates a lot of people with guns.",
                    "label": 0
                },
                {
                    "sent": "So you can see these guys and that's a failure of Markov chain Monte Carlo type of algorithm to sort of what's called persistency.",
                    "label": 0
                },
                {
                    "sent": "D to be able to mix between the modes.",
                    "label": 0
                },
                {
                    "sent": "And finally, you can do fun things like image completion.",
                    "label": 1
                },
                {
                    "sent": "Right, you can do fun things like these are your test images and here I want to emphasize this is a very nice data set because these images are not part of the training data.",
                    "label": 0
                },
                {
                    "sent": "So for example there is no cowboy in the training data.",
                    "label": 0
                },
                {
                    "sent": "It's just he just doesn't exist.",
                    "label": 0
                },
                {
                    "sent": "There is a different objects.",
                    "label": 0
                },
                {
                    "sent": "And now if you give, this is what you give to the algorithm and this is what it feels in so you can see that it's able to sort of recognize that.",
                    "label": 0
                },
                {
                    "sent": "If you have half a lack, there should be another half a leg here and there should be an arm, probably with a gun.",
                    "label": 0
                },
                {
                    "sent": "And things like airplanes if you give it part of the wing, it sort of figures out that they should be another wing, so so it does.",
                    "label": 0
                },
                {
                    "sent": "It's able to capture a lot of interesting structure in the images, even though these are simplistic images and this was 1 interesting example this was.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nelk donkey, but there is no donkey in the training data and if I give it to my algorithm and say well do completion, this is what it completes.",
                    "label": 0
                },
                {
                    "sent": "Because there are horses in the training data, sort of this form so it's able to.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Figure out that they should be ahead.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I'm done.",
                    "label": 0
                },
                {
                    "sent": "So we have time for a few questions.",
                    "label": 0
                },
                {
                    "sent": "And then all my grantier you had, I believe 505 hundred 2000 and then 30.",
                    "label": 0
                },
                {
                    "sent": "Why don't you just do the similarity in the 2000 dimension?",
                    "label": 0
                },
                {
                    "sent": "Uh huh, yeah.",
                    "label": 0
                },
                {
                    "sent": "So typically the way we want to use these models.",
                    "label": 0
                },
                {
                    "sent": "For example, if you want to use these models for information retrieval, what you want to do is you want to map high dimensional inputs into low dimensional space such that you can do K nearest neighbors much faster than in the original input space original.",
                    "label": 0
                },
                {
                    "sent": "Input right, So what you really want to do to do fast retrieval is you really want to compress down to some low dimensional space, and you can also use these tools for visualizing the data.",
                    "label": 0
                },
                {
                    "sent": "Home about why you blocked the space right before the final compression.",
                    "label": 0
                },
                {
                    "sent": "There is no particular reason that compression could have been 1000, it's just that they have these models already trained.",
                    "label": 0
                },
                {
                    "sent": "So you can just build something on top of them.",
                    "label": 0
                },
                {
                    "sent": "But there is no particular reason to just have 2000 here could have been 1000.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So 500 I think the results wouldn't differ by much.",
                    "label": 0
                },
                {
                    "sent": "I was wondering you mentioned you can use these things to learn Curtis and I was wondering how you enforce the public sentiment in Spain.",
                    "label": 0
                },
                {
                    "sent": "And Secondly there are 4 central St actually refer performance and it would be better to use.",
                    "label": 0
                },
                {
                    "sent": "Furino Labrador picture authenticity.",
                    "label": 0
                },
                {
                    "sent": "So for example in that particular kernel, right?",
                    "label": 0
                },
                {
                    "sent": "If you add sort of stuff on the diagonal, you would enforce positive definite constraints.",
                    "label": 0
                },
                {
                    "sent": "So we sort of we don't explicitly in full enforce, it just comes out of.",
                    "label": 0
                },
                {
                    "sent": "Of the way we define the Colonel.",
                    "label": 0
                },
                {
                    "sent": "So one interesting question.",
                    "label": 0
                },
                {
                    "sent": "It would be, could you also try to learn the kernel for support vector machine?",
                    "label": 0
                },
                {
                    "sent": "Particularly interested in classification file style models?",
                    "label": 0
                },
                {
                    "sent": "What people do right now is they typically train these models and they stick a support vector machine just on the top layer.",
                    "label": 0
                },
                {
                    "sent": "But I think you can also try to use back propagation and actually try to learn.",
                    "label": 0
                },
                {
                    "sent": "The kernel for support vector machines.",
                    "label": 0
                },
                {
                    "sent": "There is a lot of I guess there is parallel workshop where people are really trying to figure out how to learn.",
                    "label": 0
                },
                {
                    "sent": "Kernel functions for different algorithms and so far.",
                    "label": 0
                },
                {
                    "sent": "If you only use labeled data, you need a lot of labeled data.",
                    "label": 0
                },
                {
                    "sent": "Learning kernel function, but this works.",
                    "label": 0
                },
                {
                    "sent": "Sort of suggests that maybe if you have unlabeled data, maybe you can learn a reasonable kernel function or try to learn what similar.",
                    "label": 0
                },
                {
                    "sent": "Advantages are not equal function as opposed to.",
                    "label": 0
                },
                {
                    "sent": "Netflix.",
                    "label": 0
                },
                {
                    "sent": "Yes, absolutely, because if you just have a neural network you will overfit grossly like.",
                    "label": 0
                },
                {
                    "sent": "Imagine what happens with these kernel functions is that their little bit weird because you have a kernel function and you have 80,000 parameters in the kernel function.",
                    "label": 0
                },
                {
                    "sent": "So it's like for every parameter in your model you have like 1000 hyperparameters, which is odd, but nevertheless most of the information in those parameters really come from modeling the input distribution.",
                    "label": 0
                },
                {
                    "sent": "And that's what makes a difference.",
                    "label": 0
                },
                {
                    "sent": "That's why you don't.",
                    "label": 0
                },
                {
                    "sent": "You don't overfit or.",
                    "label": 0
                },
                {
                    "sent": "Do we have time for two minutes?",
                    "label": 0
                },
                {
                    "sent": "What is the distribution?",
                    "label": 0
                },
                {
                    "sent": "So we try to model the only distribution there.",
                    "label": 0
                },
                {
                    "sent": "Either she added mobile and they say you're you're talking small files in space, like without like Alien Band program for instance.",
                    "label": 0
                },
                {
                    "sent": "And you have experience with that bike.",
                    "label": 0
                },
                {
                    "sent": "Training Internet normal and then actually saying I'd like to see naked microphone screen for iPhone so so that comes to the question.",
                    "label": 0
                },
                {
                    "sent": "Suppose I have the following model.",
                    "label": 0
                },
                {
                    "sent": "I have an input image and have it first pixel to the 1st between zero.",
                    "label": 0
                },
                {
                    "sent": "I say that's why zero those pixels one.",
                    "label": 0
                },
                {
                    "sent": "I say it's class one and everything is going on the image data that these models will not work.",
                    "label": 0
                },
                {
                    "sent": "But I claim that that's a silly problem.",
                    "label": 0
                },
                {
                    "sent": "The most extreme case, so right, so right?",
                    "label": 0
                },
                {
                    "sent": "So when people are doing right now, he's actually you can take your top level representation as well as your bottom level presentations.",
                    "label": 0
                },
                {
                    "sent": "Teach them together and use that as your organization.",
                    "label": 0
                },
                {
                    "sent": "So even if there is some little piece in the training data or part of the training data, that's really important for discrimination.",
                    "label": 0
                },
                {
                    "sent": "They're obviously degenerative model tractor model.",
                    "label": 0
                },
                {
                    "sent": "Everything is going over there.",
                    "label": 0
                },
                {
                    "sent": "So you can do the distinction together and try to figure out what's important.",
                    "label": 0
                },
                {
                    "sent": "Since the divorce.",
                    "label": 0
                },
                {
                    "sent": "That's before the fine tuning.",
                    "label": 0
                },
                {
                    "sent": "In the face.",
                    "label": 0
                },
                {
                    "sent": "It seems to me that it's almost like a feature generation incomplete if he gives other irrelevant generates whatever Fisher is they fit in a discrete model and to define your learning.",
                    "label": 0
                },
                {
                    "sent": "It would be connecting.",
                    "label": 0
                },
                {
                    "sent": "So you can use if you have some other algorithms that did that produces nice features in statistical data into your discriminative model.",
                    "label": 0
                },
                {
                    "sent": "That should be fine, it's just I guess people before just really think about it saying that.",
                    "label": 0
                },
                {
                    "sent": "Actually just doing feature generation job.",
                    "label": 0
                },
                {
                    "sent": "You can think in the in the plane.",
                    "label": 0
                },
                {
                    "sent": "Possible thing you can think of it believe networks as a black box that given the input, produces some output and that's a mouthful, is much better representation of the.",
                    "label": 0
                },
                {
                    "sent": "The nice thing about the belief networks is that you can actually back propagate within Gaia network to suggest around.",
                    "label": 0
                },
                {
                    "sent": "So it's not like.",
                    "label": 0
                },
                {
                    "sent": "I mean I can hit series features and I throw them into my into my discriminative model because I can't.",
                    "label": 0
                },
                {
                    "sent": "Just what's going on inside of the signatures, whereas in these models language.",
                    "label": 0
                },
                {
                    "sent": "OK, first language.",
                    "label": 0
                },
                {
                    "sent": "I forgot.",
                    "label": 0
                }
            ]
        }
    }
}