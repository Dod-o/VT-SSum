{
    "id": "odt4k4ci7eygvb5nw6y665ciuxlgvybi",
    "title": "Sparsity analsysis of term weighting schemes and application to text classification",
    "info": {
        "author": [
            "Janez Brank, Artificial Intelligence Laboratory, Jo\u017eef Stefan Institute"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "February 2005",
        "category": [
            "Top->Computer Science->Text Mining"
        ]
    },
    "url": "http://videolectures.net/slsfs05_brank_satws/",
    "segmentation": [
        [
            "This is joint work and it's going to be presented by an expression.",
            "So if so, I'll be talking about this concept of sparsity in.",
            "Feature selection in the context of the text classification.",
            "So we all know that when we."
        ],
        [
            "We're doing text categorization.",
            "We have a large number of features.",
            "Basically, each word is a feature, and it's often helpful if we can select.",
            "And good subset of features before beginning the training.",
            "And there exists a multitude of different feature ranking schemes.",
            "So usually we have some heuristic which which computes a score for each feature and then we rank the features by decreasing order of these scores and we select the first few from this ranking.",
            "So we saw many examples of that in the.",
            "In the first talk this morning.",
            "So the.",
            "The question often is how?",
            "How do we describe and characterize these different feature rankings and how do we sort of compare them and?",
            "And in this talk I'll be trying to present the idea of.",
            "Using the concept of sparsity to characterize these different feature rankings, and I'll try to point out some of the ways in which this concept is helpful or interesting so.",
            "The term sparsity comes from the from the fact that our vectors which we represent the documents are usually sparse, so we have we have a vector of TF IDF weights where this one component for each word in our vocabulary.",
            "But Becausw document usually contains just a few of the words.",
            "Most of the components are zero, so it's a very sparse vector, and if we apply feature selection and remove some of the words before we begin training, the representation is even sparser.",
            "There are on average even fewer.",
            "Different words in a document, even fewer nonzero components per vector, so that's how we can.",
            "We can define this concept of sparsity, and we can.",
            "We can look at how feature selection affects the sparsity of our of our documents of our vectors.",
            "So here."
        ],
        [
            "Here's just an overview of the of the feature weighting schemes that I'll be I'll be looking at in this talk.",
            "So most of these are were more or less well known things.",
            "We've seen several of them on the 1st of this morning already, so.",
            "We know its ratio, for example, which basically looks at the.",
            "So the odds of something is the probability of that thing divided by 1 minus the probability of that thing.",
            "So for example, here we have the.",
            "The odds that the term T occurs within within documents of Class C and the the odds that it occurs in documents which you do not belong to, the Class C. So this ratio means that if a term is, for example, if it does occur in some positive documents, but in no negative documents, we have a very high odds ratio.",
            "On the other hand, we have a term which doesn't occur in positive documents, but does occur negative once it has very low odds ratio and the others are somewhere in between.",
            "So for example, practically all the terms which do not occur in any negative document will be ranked very highly bias ratio, even if the term itself is very rare, for example.",
            "So it's sort of quite different from any other feature ranking schemes.",
            "Then information gain is under very well known.",
            "Heuristic, so here we are basically.",
            "I'm looking at sort of how much information about about the class we gained by knowing the presence or absence of a particular term.",
            "Here's another well known feature weighting or feature scoring statistic which is modeled on the chi squared idea from statistics.",
            "So basically we're trying to see if the presence of the world and the membership in the class are kind of correlated, and if they turn out to be independent and the chi squared heuristic will be 0 and this sort of terms would be would be ranked very poorly by this feature weighting scheme.",
            "So basically they would almost always be discarded.",
            "And here, here's another waiting, which is which is often used in the information retrieval literature.",
            "It's basically just a very similar thing to the odds ratio, so again, it looks at the number of documents which contain the word and the class, or just the word, but not the class, and so on.",
            "And if you look at this formula in more detail, we could, you could see that, for example, the rate of these two things is impacts very similar to the odds that the term occurs in.",
            "You know, in a positive document and the radio, the other two is similar to the outside occurs in in a negative document, so it's very similar with just a slightly different kind of smoothing of this ratios.",
            "Some other, some other feature weightings."
        ],
        [
            "Teams will, in fact.",
            "Here's here's just one reasonably realistic weighting scheme, which is that based on document frequency.",
            "So people sometimes simply discard the rarest words and keep the ones which have high document frequencies of words which occur in many documents.",
            "Of course, before we do that, we usually discard the most frequent words, which are the stop words, but then of the ones which are left after that, the more common ones are perhaps more likely to be useful.",
            "Here is 1 which is just which is just included here for the sake of completeness, is just the opposite of document frequency.",
            "So in principle we could rank words from the least common to the most common, and then keep the least common ones.",
            "But of course in practice this is not really a very good idea, so we just included here, so other counterpart to this one and.",
            "Here's some some more interesting."
        ],
        [
            "Feature weightings this is again a concept which we already heard on the on the 1st talk this morning, so it might be a good idea to train a linear classifier for our class and use the weights from this classifier as a way of evaluating the features.",
            "So linear classifier would compute predictions based on the formula such as this one.",
            "So basically it would.",
            "It will take a linear combination of the of the of the words of the word frequencies for our document and add some some constant to that and and look at the sign of the of the result and So what the learning algorithm has learned in this case.",
            "Are these weights WI from the linear combination and the constant be so if some weight is close to 0 then this feature will not have much of an effect on the predictions.",
            "So regardless of whether that word.",
            "Appears in the document or not.",
            "The predictions will still be about the same, so if this kind of work is not.",
            "If it doesn't have any effect on the predictions, and apparently the learning algorithm didn't really find it useful for predicting the class, so it's probably not important for learning killer.",
            "That's why it seems like a reasonable thing to discard such features, so we could say that the the absolute value of this weight could be a score for these words, and we would rank the words in decreasing order of these absolute values so that.",
            "The ones that we keep are the ones with high absolute values and we discard the ones which have weights close to zero, and in our experiments will be using linear models trained using either the linear SVM algorithm or the perception algorithm.",
            "And I don't think which is perhaps worth emphasizing here, is that?",
            "It might be expensive to train such a model, for example, to train a linear SVM on our full data set, especially with large and.",
            "It might often be a good idea to use just a subset of the documents of the full training set to train this model, and then we can still use the weights of that model to rank the features sent.",
            "It might turn out that the performance is not really much worse so that it's this feature selection step.",
            "Doesn't get too expensive in cases where we might be concerned about this, so we'll see some examples of that training, which is the subset can still work reasonably well.",
            "No."
        ],
        [
            "So for many of these feature rankings which I presented, we have a fairly good understanding of how they work because we have sort of explicit formulas which tell us how they, how they score each feature, and we know that the ranking is based on these scores and we know why certain feature has been ranked highly or poorly or whatever because we have this explicit formulas.",
            "But for rankings based on SVM or or perception or some other linear classifier, we don't really have such an explicit.",
            "We can just tell how they assign weights to towards.",
            "We just hope that during that learning algorithm which they performed the obtained some good weights for the individual words.",
            "So it would be interesting if we somehow characterize these feature rankings to get a better understanding of them and.",
            "Maybe sort of have some more insight in how they how they work as rankings so we can try using the concept of sparsity as as one way to get another look at these feature rankings.",
            "So as I mentioned on the 1st slide we will define sparsity as the average number of different words per document after we perform some feature selection.",
            "Or Alternatively we can say that this is the average number of nonzero components per vector which represents our documents.",
            "So one I think about this concept is that.",
            "It has a very direct connection to the memory requirements so that because we, because our actors are sparse, we only represent the non zero components explicitly memory.",
            "So the sparse that they are, the less memory they will consume.",
            "And also if we compute such things as dot products or.",
            "Compute predictions using conceive based model or something like that.",
            "Again, what matters is only the non zero components of our vectors.",
            "So this this parser they are the less CPU time they will consume so it's kind of.",
            "It's a nice concept also because it has a more direct connection to these.",
            "Practical requirements and one way in which we can characterize a feature ranking is to plot this sort of sparsity curve, which shows us how how the sparsity of the documents grows.",
            "Well, in fact, sparsity in fact decreases.",
            "They grow less and less sparse if we add more and more features.",
            "So basically, this curve would tell us how quickly the average number of nonzero components in our documents grows as we add more and more features from a particular ranking.",
            "If a curve grows quickly, we know that the ranking is adding frequent words at that time, whereas if it grows slowly, we know that it's adding some sort of less common words at that point in the ranking, so."
        ],
        [
            "Here's this, so this comparison of sparsity curves.",
            "The hope is that this might give us some idea of how these different feature rankings work, especially the ones for which we don't have explicit formulas, so.",
            "Here in the horizontal axis we have the number of features and on the vertical axis we have this concept of sparsity.",
            "So basically this is the average number of.",
            "Of non zero components in our vectors or the average number of different words in our documents.",
            "So this is based on the Reuters corpus volume, volume one on a very large subset of that corpus.",
            "So one extreme is of course the feature ranking which prefers the most frequent features first.",
            "So this is basically ranking based on document frequency, which which of course produces the most quickly growing curve.",
            "And this is this is the curve that we see here and on the other extreme which have.",
            "The ranking, which prefers the least frequent features first.",
            "Not that this is really a good ranking anyway, and this gives us the slow slowest growing curve which we see here and everything else of course is in the middle and then.",
            "For for many rankings for which we have explicit formulas, we can sort of roughly expect what sort of shape the curve will have in advance, and we sort of understand why the shape is such as it is.",
            "For example, here we have.",
            "In this blue line is the curve for information gain, and we see that it grows relatively quickly compared to most of the other curves, which means that for example.",
            "Initially, it prefers prefers terms which are which have sort of high information gain when compared with the class value and if occur.",
            "If a term is very rare.",
            "It's not likely to have high information gain, so it doesn't tend to be ranked so highly, whereas at the end, for example, we also have a very steep climb because this is where all the common words appear, which are equally common in both classes and don't really give us any information about the class values, so they tend to rank at the end and then the sort of the less common words tend to be somewhere in the middle, so we have some sort of understanding.",
            "Quite this curve has such a shape, and then on the other hand, we have for example also ratio.",
            "Where again we can look at the formula and then we can look at its curve and we can also see why the shape is such as it is because odds ratio.",
            "Will rank highly at the beginning of his ranking, the words which.",
            "Have a sort of which which appear in some positive documents, but in no or almost no negative documents.",
            "So this means that even if a word is very rare, as long as it doesn't appear in any negative documents, it will have a very high odds ratio and you appear at the beginning of the ranking, which is why it's curve grows so slowly because it picks up large number of very rare words initially, and that's why it's one of the more slowly growing curves.",
            "And we could also look at the curve for chi squared for example.",
            "Which we which we see here and.",
            "Again, we would have a similar explanation for his shape as we have in the case of mutual information.",
            "Here's the Robertson Spark Jones curve, which which is similar to the one for odds ratio because as we saw through the formula, is based on very similar concepts, but the sort of the interesting thing in my opinion on this on this graph is that we also have the curves for feature selection rankings based on the models of linear classifiers, so the ones for which we don't have explicit formulas.",
            "So this gives us some idea how these rankings.",
            "Compared to the other to the other feature selection rankings.",
            "So here we have basically all the three curves here.",
            "In the middle we have the curve for the future ranking based on the weights of SVM model.",
            "We have in fact two versions of SVN, One which has trained the SVM model on the complete training set and one which is trained on just half of the training set.",
            "As I mentioned earlier, it might be a good idea to save time and doesn't necessarily degrade performance very much, and we see that the resulting rankings.",
            "Effect very similar as far as sparsity are concerned, because these curves really run along very closely to each other and then the ranking based on perception weights.",
            "As we can see, also has similar characteristics in terms of sparsity.",
            "This barbed wire type of line here in the middle and we see that as far as preferring rare or common features is concerned, this sort of rankings tend to prefer relatively common features, although not as common as mutual information.",
            "But they certainly don't prefer rare features in the sense that also issue does, so this gives us some idea of of the characteristics of the ranking based on the SVM weights and how it compares to this other rankings.",
            "We consider it somewhere in the middle, but closer to the ones which.",
            "Which prefer more common words initially, so here's 1 interesting way in which we can use the concept of sparsity to get another look at these different feature ranking criteria."
        ],
        [
            "Our another way in which we can use sparsity is when comparing different feature rankings.",
            "So when people compare different feature rankings, they often use the number of features is the independent variable.",
            "So we ask ourselves how does the performance depend on the number of features?",
            "What is the performance?",
            "For example the F1 measure or whatever?",
            "What is the performance when we use the first 100 features of this and that or the other feature ranking?",
            "And using the number of features in this way is somewhat unfair towards rankings which prefer less frequent features.",
            "For example loss ratio.",
            "Hold alt ratio chooses a large number of very infrequent words initially, and if we allow it to use just a small number of features, maybe just 100 features, it might turn out that.",
            "It shows 100 rare features and most of the documents will be really quite empty, but at this point so it doesn't really have a good chance to perform well where some other ranking which prefers more common features initially.",
            "For example the one based on information gain.",
            "We already have reasonably non zero vectors for most documents and it might perform much better, but really.",
            "Sort of, we aren't giving us ratio a fair chance.",
            "In a way, it would be kind of nicer to allow it to use more features because it's using clearer features and there's no harm in allowing to use a larger number of features, because if we are in the end concerned with memory or CPU time requirements, then what matters is the sparsity really not not so much.",
            "The number of features directly, so it might be interesting to compare different feature rankings based on sparsity rather than on the number of features.",
            "And this might give us sort of an easier way to compare the different rankings on a graph, at least not necessarily in the end for windowing evaluation.",
            "So he'll just show some examples of graphs which show the performance of different feature rankings as a function of the number of features, and then a graph which shows the performance as a function of sparsity.",
            "And we will see how this affects ranking search results issue and its comparison with the other rankings."
        ],
        [
            "So here we have a graph which shows the performance of different rankings as a function of the number of features we have, the number of features on the vertical on the horizontal axis and the F measure on the vertical axis.",
            "And here are again the same.",
            "Same rankings that we saw in the previous chart and notations is also the same.",
            "Navbase was used as the classifier.",
            "In the end we write it, it's not, it's not volume to its volume.",
            "One of course is well known corpus from the year 2000.",
            "So we use leave base that is, there is the classifier in the end and we can see here for example that.",
            "Odds ratio, which is this purple line down here.",
            "It seems to perform really poorly if you, for example, compare it with the other rankings.",
            "It has a really terribly low performance, but on the other hand, for example, mutual information has really good performance.",
            "This small number of features, but the idea is that we aren't really giving us ratio affair chance because with 100 features it shows very rare features and cannot really cover all the documents well enough at this point.",
            "So we can see that if we allow it to use a larger number of features, it can perform much better and sort of becomes compareable to mutual information for example.",
            "Or for example here we see the Robertson Spark Jones waiting which is just the version of Office ratio performance.",
            "Even better than mutual information.",
            "But we have to allow it to use a suitably large number of features.",
            "Incidentally we also see that the rankings based on the weight of an SVM perform really well compared to the other rankings and there wasn't really any harm.",
            "In using for example just an SVM model, trains are just half of the training set rather than on the entire training set.",
            "We can compare these two lines.",
            "The Green one is for the for the essay model train.",
            "Other full training set, and the blue one for the one train of the half of the training set, and we can see that the comparison performance is really compareable.",
            "But anyway, if we keep in mind this shape of the odds ratio curve for example, which seems to be really abysmal here, and we can then see look at the next graph which will show.",
            "The same performance is a function of sparsity.",
            "Will see that also ratio doesn't seem so useless anymore.",
            "So the."
        ],
        [
            "This graph again the F measure is on the on the vertical axis and here we have the sparsity on the horizontal axis and.",
            "The odds ratio curve now appears here and we can see that it performs really comparably good with mutual information, and in fact is often better if we compare them at the same level of sparsity rather than at the same number of features, because simply also ratio because it prefers rare features, needs to take more of them before it achieves a reasonable sparsity, which gives it a reasonably good representation of the documents, and so suddenly all of these different feature rankings.",
            "So sort of our kind of easier to compare on this graph and the differences are no longer so huge except for the ones which are really which are really bad.",
            "So this is kind of 1 idea why it might be a good thing to use sparsity when when plotting these things and comparing the different feature rankings, because it gives them all sort of puts them on a more equal footing than using the number of features as the independent variable and.",
            "Here, here."
        ],
        [
            "Just one more idea how we hoped to use sparsity, perhaps to obtain some concrete benefits as well, although as we see this one didn't really lead to any real improvements.",
            "So we have we have many categories in our datasets maybe.",
            "We use 16, but the full data set has 100 and three categories and so on.",
            "Each of them is really a binary classification problem of its own, and we can obtain different feature rankings for each of these categories and.",
            "At the end we have to choose how many how many features from the from the beginning of the of the ranking we will use to learn and then to classify.",
            "But Alternatively, instead of choosing a number of features, we can also choose a sparsity and then keep as many features as are necessary to achieve that level of sparsity and.",
            "It turns out that the best number of features so so if you look at each category and look at which number of features gives you the best performance for that category.",
            "This can vary very much from one category to another.",
            "Some of them need a lot of features, some a lot fewer features, so the hope was that maybe the sparsity doesn't vary as much.",
            "And so makes it kind of firstly makes it easier to compare average performance across categories and also might give you.",
            "A better foundation for determining the cutoff point.",
            "So for example.",
            "Instead of choosing the best number of features for each category, we might try to simplify things and fix the same number of features for all categories, but if the best number of features varies so much from one category to another, then maybe it might be a better idea to fix a constant level of sparsity and use that for each category, which will then allow us to use a different level, a different number of features for each category, because it will adapt to the distributions.",
            "And the rankings over different categories.",
            "So this was sort of 1 idea which the next we will compare on the next chart on the next slide and we will see that it sort of.",
            "The."
        ],
        [
            "Doesn't really work very well, so here we have different feature rankings and the average performance over those 16 categories and the blue bars give us the performance which we can obtain if we try to fix the same number of features for all categories.",
            "So we decide that we will use maybe 100 or 1000 or whatever features for each category.",
            "And then compute the average performance over that and we select the number of features in the end so that the performance is the best and the purple bars show us the same sort of thing.",
            "But we figured the sparsity instead of the number of features, so we will say that, for example, we want to have a sparsity of 1 or 10 or whatever for each category, and then choose as many features for that category as necessary to achieve that sparsity.",
            "And again we averaged the performance in the end.",
            "And the hope was that this would sort of fixing things based on sparsity instead of a number of features that this would help us improve the performance.",
            "But as this chart shows, it doesn't really lead to any significant improvements, or at least not very significant ones, sort of.",
            "There are some very small improvements and.",
            "But at least we see that it doesn't lead to any decrease in performance either, so so that sparsity is in a way as good a way of defining these cut offs as the number of features would be in here.",
            "Just for comparison, we have the results which we would obtain if we selected the best number of features or the best sparsity is equivalent anyway separately for each category, and then average that over categories.",
            "Of course this gives us even better performance, so in a way.",
            "This also shows that this this type of fixed cut off levels, which will be the same overall categories is not really such a good idea, so this is kind of not a very positive result as far as we are concerned.",
            "So close to the."
        ],
        [
            "And.",
            "So now we we presented this concept of sparsity.",
            "We try to show that it's.",
            "It's a useful and interesting thing when comparing the different feature selection methods, because it sort of puts them on a more equal basis.",
            "It has very direct connection with memory and CPU time consumption, unlike the number of features perhaps and we try to use it as a set of criteria and we saw that although it's not really much better, it's also not any worse than using the number of features.",
            "And here at the end I would just like to point out some ideas for future."
        ],
        [
            "Work in which we are working at the moment.",
            "So our hope is that we would be able to characterize the feature ranking schemes in terms of other characteristics besides just sparsity curves.",
            "So we saw how we can describe each feature ranking by the sparsity curve.",
            "We could also try to introduce other characteristic curves.",
            "For example, one thing that we were looking at this is what's the total, how quickly the total sum of information gain grows over all the terms which have been selected by a feature ranking up to a particular point.",
            "But we could try to come up with other characteristics as well and sort of idea that we were having, which might come to some good results eventually.",
            "Is that if we can characterize a good ranking in terms of some characteristics like this, which would be relatively easy to compute, perhaps.",
            "It would be.",
            "It would be great if we could eventually come up with a set of characteristics which indicates what is a good feature ranking so that in the end, if we if we can come up with a new feature ranking, perhaps artificially by synthesizing by choosing things at random.",
            "We might be able to obtain a good ranking simply by synthesizing one which matches the characteristics of a previously known good ranking, and in that way, perhaps one could determine the characteristics of a good ranking on one category or one data set an artificially and perhaps cheaply obtain new rankings with comparably good performance simply by trying to match its characteristics, and in that way it might be possible to obtain good rankings without going through a really expensive process like training and SVM, just in order to obtain.",
            "And SVM based feature ranking.",
            "So this is kind of some ideas which we are exploring at the moment and might be a sort of continuation of the of the work presented here and that's it.",
            "Thank you.",
            "Perhaps you could go back to the slide that you had already had the.",
            "Yes, whenever against the.",
            "This one, yeah.",
            "Looks like that."
        ],
        [
            "Second group is negative, so you've got this define PPO, but could you have you got any intuition why some of the other pairs?",
            "Initially they increased them, but then they go up again.",
            "The information gain is this one.",
            "Yeah, well, you know I don't have really a very clear intuition.",
            "I would say that at the end.",
            "There is perhaps yes, so the question is from my point with the question is not so much why there is a peak at the end, but while there is this low point in the middle.",
            "I don't really know for sure, but I guess it's.",
            "So it's probably obtains a large number of.",
            "Yeah, perhaps because it is adopts a large number of infrequent features in the middle perhaps, and that might hurt the performance so much.",
            "But yes, I agree, I don't have a really clear idea of why this sort of dropping performance in the middle.",
            "Can you go back to the number of features with respect sparsity?",
            "Yeah, so you can notice like that.",
            "There is like this S shape transitioning in all of these like information gain realized and then for us."
        ],
        [
            "Damn right, you don't have the you don't have this transition is the same like most common words, yes, I mean it, it doesn't have this.",
            "You know why?",
            "Or I mean, well, sort of we can sort of friend why why?",
            "For example things like the mutual information curve have this client but at the end you know these are the really common words which appear equally sort of frequently in both classes, both the positive and negative class.",
            "And for example in the case of odds ratio at the end you have all the words which are common.",
            "In the negative class but not in the positive class.",
            "And that again is quite a lot of very common words, and that also gives you a very low odds ratio and put them at the end of the ranking.",
            "So apparently in the in the case of in the case of SVM, it is kind of apparently reasonably good at putting many of the low frequency words towards the end of the ranking, and that's why it has a slow ascent here towards the end rather than a steep one like like the others did.",
            "So I guess this means that SVM.",
            "Sort of.",
            "Managed to put many of the less frequent words which are not useful for predicting the class.",
            "It put them at the end and not maybe in the middle like some of the others did, which have a slow slow part here in the middle.",
            "Have you checked whether like the best F1 is when this transition happens this?",
            "Well I haven't really checked that, but sort of my if you look at the information gain curves for example, we saw that the best F1 is the relatively smaller number of features and.",
            "Well, probably not as late as here when the when the transition occurs.",
            "OK, thanks.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is joint work and it's going to be presented by an expression.",
                    "label": 0
                },
                {
                    "sent": "So if so, I'll be talking about this concept of sparsity in.",
                    "label": 0
                },
                {
                    "sent": "Feature selection in the context of the text classification.",
                    "label": 1
                },
                {
                    "sent": "So we all know that when we.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're doing text categorization.",
                    "label": 0
                },
                {
                    "sent": "We have a large number of features.",
                    "label": 0
                },
                {
                    "sent": "Basically, each word is a feature, and it's often helpful if we can select.",
                    "label": 0
                },
                {
                    "sent": "And good subset of features before beginning the training.",
                    "label": 0
                },
                {
                    "sent": "And there exists a multitude of different feature ranking schemes.",
                    "label": 1
                },
                {
                    "sent": "So usually we have some heuristic which which computes a score for each feature and then we rank the features by decreasing order of these scores and we select the first few from this ranking.",
                    "label": 0
                },
                {
                    "sent": "So we saw many examples of that in the.",
                    "label": 0
                },
                {
                    "sent": "In the first talk this morning.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "The question often is how?",
                    "label": 0
                },
                {
                    "sent": "How do we describe and characterize these different feature rankings and how do we sort of compare them and?",
                    "label": 0
                },
                {
                    "sent": "And in this talk I'll be trying to present the idea of.",
                    "label": 0
                },
                {
                    "sent": "Using the concept of sparsity to characterize these different feature rankings, and I'll try to point out some of the ways in which this concept is helpful or interesting so.",
                    "label": 0
                },
                {
                    "sent": "The term sparsity comes from the from the fact that our vectors which we represent the documents are usually sparse, so we have we have a vector of TF IDF weights where this one component for each word in our vocabulary.",
                    "label": 0
                },
                {
                    "sent": "But Becausw document usually contains just a few of the words.",
                    "label": 0
                },
                {
                    "sent": "Most of the components are zero, so it's a very sparse vector, and if we apply feature selection and remove some of the words before we begin training, the representation is even sparser.",
                    "label": 0
                },
                {
                    "sent": "There are on average even fewer.",
                    "label": 1
                },
                {
                    "sent": "Different words in a document, even fewer nonzero components per vector, so that's how we can.",
                    "label": 0
                },
                {
                    "sent": "We can define this concept of sparsity, and we can.",
                    "label": 0
                },
                {
                    "sent": "We can look at how feature selection affects the sparsity of our of our documents of our vectors.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's just an overview of the of the feature weighting schemes that I'll be I'll be looking at in this talk.",
                    "label": 1
                },
                {
                    "sent": "So most of these are were more or less well known things.",
                    "label": 1
                },
                {
                    "sent": "We've seen several of them on the 1st of this morning already, so.",
                    "label": 0
                },
                {
                    "sent": "We know its ratio, for example, which basically looks at the.",
                    "label": 0
                },
                {
                    "sent": "So the odds of something is the probability of that thing divided by 1 minus the probability of that thing.",
                    "label": 0
                },
                {
                    "sent": "So for example, here we have the.",
                    "label": 1
                },
                {
                    "sent": "The odds that the term T occurs within within documents of Class C and the the odds that it occurs in documents which you do not belong to, the Class C. So this ratio means that if a term is, for example, if it does occur in some positive documents, but in no negative documents, we have a very high odds ratio.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, we have a term which doesn't occur in positive documents, but does occur negative once it has very low odds ratio and the others are somewhere in between.",
                    "label": 0
                },
                {
                    "sent": "So for example, practically all the terms which do not occur in any negative document will be ranked very highly bias ratio, even if the term itself is very rare, for example.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of quite different from any other feature ranking schemes.",
                    "label": 0
                },
                {
                    "sent": "Then information gain is under very well known.",
                    "label": 0
                },
                {
                    "sent": "Heuristic, so here we are basically.",
                    "label": 0
                },
                {
                    "sent": "I'm looking at sort of how much information about about the class we gained by knowing the presence or absence of a particular term.",
                    "label": 0
                },
                {
                    "sent": "Here's another well known feature weighting or feature scoring statistic which is modeled on the chi squared idea from statistics.",
                    "label": 0
                },
                {
                    "sent": "So basically we're trying to see if the presence of the world and the membership in the class are kind of correlated, and if they turn out to be independent and the chi squared heuristic will be 0 and this sort of terms would be would be ranked very poorly by this feature weighting scheme.",
                    "label": 0
                },
                {
                    "sent": "So basically they would almost always be discarded.",
                    "label": 0
                },
                {
                    "sent": "And here, here's another waiting, which is which is often used in the information retrieval literature.",
                    "label": 0
                },
                {
                    "sent": "It's basically just a very similar thing to the odds ratio, so again, it looks at the number of documents which contain the word and the class, or just the word, but not the class, and so on.",
                    "label": 0
                },
                {
                    "sent": "And if you look at this formula in more detail, we could, you could see that, for example, the rate of these two things is impacts very similar to the odds that the term occurs in.",
                    "label": 0
                },
                {
                    "sent": "You know, in a positive document and the radio, the other two is similar to the outside occurs in in a negative document, so it's very similar with just a slightly different kind of smoothing of this ratios.",
                    "label": 0
                },
                {
                    "sent": "Some other, some other feature weightings.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Teams will, in fact.",
                    "label": 0
                },
                {
                    "sent": "Here's here's just one reasonably realistic weighting scheme, which is that based on document frequency.",
                    "label": 0
                },
                {
                    "sent": "So people sometimes simply discard the rarest words and keep the ones which have high document frequencies of words which occur in many documents.",
                    "label": 0
                },
                {
                    "sent": "Of course, before we do that, we usually discard the most frequent words, which are the stop words, but then of the ones which are left after that, the more common ones are perhaps more likely to be useful.",
                    "label": 0
                },
                {
                    "sent": "Here is 1 which is just which is just included here for the sake of completeness, is just the opposite of document frequency.",
                    "label": 0
                },
                {
                    "sent": "So in principle we could rank words from the least common to the most common, and then keep the least common ones.",
                    "label": 0
                },
                {
                    "sent": "But of course in practice this is not really a very good idea, so we just included here, so other counterpart to this one and.",
                    "label": 0
                },
                {
                    "sent": "Here's some some more interesting.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Feature weightings this is again a concept which we already heard on the on the 1st talk this morning, so it might be a good idea to train a linear classifier for our class and use the weights from this classifier as a way of evaluating the features.",
                    "label": 0
                },
                {
                    "sent": "So linear classifier would compute predictions based on the formula such as this one.",
                    "label": 0
                },
                {
                    "sent": "So basically it would.",
                    "label": 0
                },
                {
                    "sent": "It will take a linear combination of the of the of the words of the word frequencies for our document and add some some constant to that and and look at the sign of the of the result and So what the learning algorithm has learned in this case.",
                    "label": 0
                },
                {
                    "sent": "Are these weights WI from the linear combination and the constant be so if some weight is close to 0 then this feature will not have much of an effect on the predictions.",
                    "label": 0
                },
                {
                    "sent": "So regardless of whether that word.",
                    "label": 0
                },
                {
                    "sent": "Appears in the document or not.",
                    "label": 0
                },
                {
                    "sent": "The predictions will still be about the same, so if this kind of work is not.",
                    "label": 0
                },
                {
                    "sent": "If it doesn't have any effect on the predictions, and apparently the learning algorithm didn't really find it useful for predicting the class, so it's probably not important for learning killer.",
                    "label": 1
                },
                {
                    "sent": "That's why it seems like a reasonable thing to discard such features, so we could say that the the absolute value of this weight could be a score for these words, and we would rank the words in decreasing order of these absolute values so that.",
                    "label": 0
                },
                {
                    "sent": "The ones that we keep are the ones with high absolute values and we discard the ones which have weights close to zero, and in our experiments will be using linear models trained using either the linear SVM algorithm or the perception algorithm.",
                    "label": 0
                },
                {
                    "sent": "And I don't think which is perhaps worth emphasizing here, is that?",
                    "label": 0
                },
                {
                    "sent": "It might be expensive to train such a model, for example, to train a linear SVM on our full data set, especially with large and.",
                    "label": 0
                },
                {
                    "sent": "It might often be a good idea to use just a subset of the documents of the full training set to train this model, and then we can still use the weights of that model to rank the features sent.",
                    "label": 1
                },
                {
                    "sent": "It might turn out that the performance is not really much worse so that it's this feature selection step.",
                    "label": 0
                },
                {
                    "sent": "Doesn't get too expensive in cases where we might be concerned about this, so we'll see some examples of that training, which is the subset can still work reasonably well.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for many of these feature rankings which I presented, we have a fairly good understanding of how they work because we have sort of explicit formulas which tell us how they, how they score each feature, and we know that the ranking is based on these scores and we know why certain feature has been ranked highly or poorly or whatever because we have this explicit formulas.",
                    "label": 0
                },
                {
                    "sent": "But for rankings based on SVM or or perception or some other linear classifier, we don't really have such an explicit.",
                    "label": 0
                },
                {
                    "sent": "We can just tell how they assign weights to towards.",
                    "label": 0
                },
                {
                    "sent": "We just hope that during that learning algorithm which they performed the obtained some good weights for the individual words.",
                    "label": 0
                },
                {
                    "sent": "So it would be interesting if we somehow characterize these feature rankings to get a better understanding of them and.",
                    "label": 0
                },
                {
                    "sent": "Maybe sort of have some more insight in how they how they work as rankings so we can try using the concept of sparsity as as one way to get another look at these feature rankings.",
                    "label": 0
                },
                {
                    "sent": "So as I mentioned on the 1st slide we will define sparsity as the average number of different words per document after we perform some feature selection.",
                    "label": 1
                },
                {
                    "sent": "Or Alternatively we can say that this is the average number of nonzero components per vector which represents our documents.",
                    "label": 0
                },
                {
                    "sent": "So one I think about this concept is that.",
                    "label": 0
                },
                {
                    "sent": "It has a very direct connection to the memory requirements so that because we, because our actors are sparse, we only represent the non zero components explicitly memory.",
                    "label": 0
                },
                {
                    "sent": "So the sparse that they are, the less memory they will consume.",
                    "label": 0
                },
                {
                    "sent": "And also if we compute such things as dot products or.",
                    "label": 0
                },
                {
                    "sent": "Compute predictions using conceive based model or something like that.",
                    "label": 0
                },
                {
                    "sent": "Again, what matters is only the non zero components of our vectors.",
                    "label": 0
                },
                {
                    "sent": "So this this parser they are the less CPU time they will consume so it's kind of.",
                    "label": 0
                },
                {
                    "sent": "It's a nice concept also because it has a more direct connection to these.",
                    "label": 0
                },
                {
                    "sent": "Practical requirements and one way in which we can characterize a feature ranking is to plot this sort of sparsity curve, which shows us how how the sparsity of the documents grows.",
                    "label": 0
                },
                {
                    "sent": "Well, in fact, sparsity in fact decreases.",
                    "label": 0
                },
                {
                    "sent": "They grow less and less sparse if we add more and more features.",
                    "label": 0
                },
                {
                    "sent": "So basically, this curve would tell us how quickly the average number of nonzero components in our documents grows as we add more and more features from a particular ranking.",
                    "label": 1
                },
                {
                    "sent": "If a curve grows quickly, we know that the ranking is adding frequent words at that time, whereas if it grows slowly, we know that it's adding some sort of less common words at that point in the ranking, so.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's this, so this comparison of sparsity curves.",
                    "label": 1
                },
                {
                    "sent": "The hope is that this might give us some idea of how these different feature rankings work, especially the ones for which we don't have explicit formulas, so.",
                    "label": 0
                },
                {
                    "sent": "Here in the horizontal axis we have the number of features and on the vertical axis we have this concept of sparsity.",
                    "label": 1
                },
                {
                    "sent": "So basically this is the average number of.",
                    "label": 0
                },
                {
                    "sent": "Of non zero components in our vectors or the average number of different words in our documents.",
                    "label": 0
                },
                {
                    "sent": "So this is based on the Reuters corpus volume, volume one on a very large subset of that corpus.",
                    "label": 0
                },
                {
                    "sent": "So one extreme is of course the feature ranking which prefers the most frequent features first.",
                    "label": 0
                },
                {
                    "sent": "So this is basically ranking based on document frequency, which which of course produces the most quickly growing curve.",
                    "label": 0
                },
                {
                    "sent": "And this is this is the curve that we see here and on the other extreme which have.",
                    "label": 0
                },
                {
                    "sent": "The ranking, which prefers the least frequent features first.",
                    "label": 0
                },
                {
                    "sent": "Not that this is really a good ranking anyway, and this gives us the slow slowest growing curve which we see here and everything else of course is in the middle and then.",
                    "label": 0
                },
                {
                    "sent": "For for many rankings for which we have explicit formulas, we can sort of roughly expect what sort of shape the curve will have in advance, and we sort of understand why the shape is such as it is.",
                    "label": 0
                },
                {
                    "sent": "For example, here we have.",
                    "label": 0
                },
                {
                    "sent": "In this blue line is the curve for information gain, and we see that it grows relatively quickly compared to most of the other curves, which means that for example.",
                    "label": 0
                },
                {
                    "sent": "Initially, it prefers prefers terms which are which have sort of high information gain when compared with the class value and if occur.",
                    "label": 0
                },
                {
                    "sent": "If a term is very rare.",
                    "label": 0
                },
                {
                    "sent": "It's not likely to have high information gain, so it doesn't tend to be ranked so highly, whereas at the end, for example, we also have a very steep climb because this is where all the common words appear, which are equally common in both classes and don't really give us any information about the class values, so they tend to rank at the end and then the sort of the less common words tend to be somewhere in the middle, so we have some sort of understanding.",
                    "label": 0
                },
                {
                    "sent": "Quite this curve has such a shape, and then on the other hand, we have for example also ratio.",
                    "label": 0
                },
                {
                    "sent": "Where again we can look at the formula and then we can look at its curve and we can also see why the shape is such as it is because odds ratio.",
                    "label": 0
                },
                {
                    "sent": "Will rank highly at the beginning of his ranking, the words which.",
                    "label": 0
                },
                {
                    "sent": "Have a sort of which which appear in some positive documents, but in no or almost no negative documents.",
                    "label": 1
                },
                {
                    "sent": "So this means that even if a word is very rare, as long as it doesn't appear in any negative documents, it will have a very high odds ratio and you appear at the beginning of the ranking, which is why it's curve grows so slowly because it picks up large number of very rare words initially, and that's why it's one of the more slowly growing curves.",
                    "label": 0
                },
                {
                    "sent": "And we could also look at the curve for chi squared for example.",
                    "label": 0
                },
                {
                    "sent": "Which we which we see here and.",
                    "label": 0
                },
                {
                    "sent": "Again, we would have a similar explanation for his shape as we have in the case of mutual information.",
                    "label": 0
                },
                {
                    "sent": "Here's the Robertson Spark Jones curve, which which is similar to the one for odds ratio because as we saw through the formula, is based on very similar concepts, but the sort of the interesting thing in my opinion on this on this graph is that we also have the curves for feature selection rankings based on the models of linear classifiers, so the ones for which we don't have explicit formulas.",
                    "label": 0
                },
                {
                    "sent": "So this gives us some idea how these rankings.",
                    "label": 0
                },
                {
                    "sent": "Compared to the other to the other feature selection rankings.",
                    "label": 0
                },
                {
                    "sent": "So here we have basically all the three curves here.",
                    "label": 0
                },
                {
                    "sent": "In the middle we have the curve for the future ranking based on the weights of SVM model.",
                    "label": 0
                },
                {
                    "sent": "We have in fact two versions of SVN, One which has trained the SVM model on the complete training set and one which is trained on just half of the training set.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned earlier, it might be a good idea to save time and doesn't necessarily degrade performance very much, and we see that the resulting rankings.",
                    "label": 0
                },
                {
                    "sent": "Effect very similar as far as sparsity are concerned, because these curves really run along very closely to each other and then the ranking based on perception weights.",
                    "label": 0
                },
                {
                    "sent": "As we can see, also has similar characteristics in terms of sparsity.",
                    "label": 0
                },
                {
                    "sent": "This barbed wire type of line here in the middle and we see that as far as preferring rare or common features is concerned, this sort of rankings tend to prefer relatively common features, although not as common as mutual information.",
                    "label": 0
                },
                {
                    "sent": "But they certainly don't prefer rare features in the sense that also issue does, so this gives us some idea of of the characteristics of the ranking based on the SVM weights and how it compares to this other rankings.",
                    "label": 0
                },
                {
                    "sent": "We consider it somewhere in the middle, but closer to the ones which.",
                    "label": 0
                },
                {
                    "sent": "Which prefer more common words initially, so here's 1 interesting way in which we can use the concept of sparsity to get another look at these different feature ranking criteria.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our another way in which we can use sparsity is when comparing different feature rankings.",
                    "label": 1
                },
                {
                    "sent": "So when people compare different feature rankings, they often use the number of features is the independent variable.",
                    "label": 1
                },
                {
                    "sent": "So we ask ourselves how does the performance depend on the number of features?",
                    "label": 0
                },
                {
                    "sent": "What is the performance?",
                    "label": 0
                },
                {
                    "sent": "For example the F1 measure or whatever?",
                    "label": 1
                },
                {
                    "sent": "What is the performance when we use the first 100 features of this and that or the other feature ranking?",
                    "label": 0
                },
                {
                    "sent": "And using the number of features in this way is somewhat unfair towards rankings which prefer less frequent features.",
                    "label": 0
                },
                {
                    "sent": "For example loss ratio.",
                    "label": 0
                },
                {
                    "sent": "Hold alt ratio chooses a large number of very infrequent words initially, and if we allow it to use just a small number of features, maybe just 100 features, it might turn out that.",
                    "label": 0
                },
                {
                    "sent": "It shows 100 rare features and most of the documents will be really quite empty, but at this point so it doesn't really have a good chance to perform well where some other ranking which prefers more common features initially.",
                    "label": 0
                },
                {
                    "sent": "For example the one based on information gain.",
                    "label": 0
                },
                {
                    "sent": "We already have reasonably non zero vectors for most documents and it might perform much better, but really.",
                    "label": 0
                },
                {
                    "sent": "Sort of, we aren't giving us ratio a fair chance.",
                    "label": 0
                },
                {
                    "sent": "In a way, it would be kind of nicer to allow it to use more features because it's using clearer features and there's no harm in allowing to use a larger number of features, because if we are in the end concerned with memory or CPU time requirements, then what matters is the sparsity really not not so much.",
                    "label": 0
                },
                {
                    "sent": "The number of features directly, so it might be interesting to compare different feature rankings based on sparsity rather than on the number of features.",
                    "label": 0
                },
                {
                    "sent": "And this might give us sort of an easier way to compare the different rankings on a graph, at least not necessarily in the end for windowing evaluation.",
                    "label": 0
                },
                {
                    "sent": "So he'll just show some examples of graphs which show the performance of different feature rankings as a function of the number of features, and then a graph which shows the performance as a function of sparsity.",
                    "label": 0
                },
                {
                    "sent": "And we will see how this affects ranking search results issue and its comparison with the other rankings.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we have a graph which shows the performance of different rankings as a function of the number of features we have, the number of features on the vertical on the horizontal axis and the F measure on the vertical axis.",
                    "label": 0
                },
                {
                    "sent": "And here are again the same.",
                    "label": 0
                },
                {
                    "sent": "Same rankings that we saw in the previous chart and notations is also the same.",
                    "label": 0
                },
                {
                    "sent": "Navbase was used as the classifier.",
                    "label": 0
                },
                {
                    "sent": "In the end we write it, it's not, it's not volume to its volume.",
                    "label": 0
                },
                {
                    "sent": "One of course is well known corpus from the year 2000.",
                    "label": 0
                },
                {
                    "sent": "So we use leave base that is, there is the classifier in the end and we can see here for example that.",
                    "label": 0
                },
                {
                    "sent": "Odds ratio, which is this purple line down here.",
                    "label": 0
                },
                {
                    "sent": "It seems to perform really poorly if you, for example, compare it with the other rankings.",
                    "label": 0
                },
                {
                    "sent": "It has a really terribly low performance, but on the other hand, for example, mutual information has really good performance.",
                    "label": 0
                },
                {
                    "sent": "This small number of features, but the idea is that we aren't really giving us ratio affair chance because with 100 features it shows very rare features and cannot really cover all the documents well enough at this point.",
                    "label": 0
                },
                {
                    "sent": "So we can see that if we allow it to use a larger number of features, it can perform much better and sort of becomes compareable to mutual information for example.",
                    "label": 0
                },
                {
                    "sent": "Or for example here we see the Robertson Spark Jones waiting which is just the version of Office ratio performance.",
                    "label": 0
                },
                {
                    "sent": "Even better than mutual information.",
                    "label": 0
                },
                {
                    "sent": "But we have to allow it to use a suitably large number of features.",
                    "label": 0
                },
                {
                    "sent": "Incidentally we also see that the rankings based on the weight of an SVM perform really well compared to the other rankings and there wasn't really any harm.",
                    "label": 0
                },
                {
                    "sent": "In using for example just an SVM model, trains are just half of the training set rather than on the entire training set.",
                    "label": 0
                },
                {
                    "sent": "We can compare these two lines.",
                    "label": 0
                },
                {
                    "sent": "The Green one is for the for the essay model train.",
                    "label": 0
                },
                {
                    "sent": "Other full training set, and the blue one for the one train of the half of the training set, and we can see that the comparison performance is really compareable.",
                    "label": 0
                },
                {
                    "sent": "But anyway, if we keep in mind this shape of the odds ratio curve for example, which seems to be really abysmal here, and we can then see look at the next graph which will show.",
                    "label": 0
                },
                {
                    "sent": "The same performance is a function of sparsity.",
                    "label": 0
                },
                {
                    "sent": "Will see that also ratio doesn't seem so useless anymore.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This graph again the F measure is on the on the vertical axis and here we have the sparsity on the horizontal axis and.",
                    "label": 0
                },
                {
                    "sent": "The odds ratio curve now appears here and we can see that it performs really comparably good with mutual information, and in fact is often better if we compare them at the same level of sparsity rather than at the same number of features, because simply also ratio because it prefers rare features, needs to take more of them before it achieves a reasonable sparsity, which gives it a reasonably good representation of the documents, and so suddenly all of these different feature rankings.",
                    "label": 0
                },
                {
                    "sent": "So sort of our kind of easier to compare on this graph and the differences are no longer so huge except for the ones which are really which are really bad.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of 1 idea why it might be a good thing to use sparsity when when plotting these things and comparing the different feature rankings, because it gives them all sort of puts them on a more equal footing than using the number of features as the independent variable and.",
                    "label": 0
                },
                {
                    "sent": "Here, here.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just one more idea how we hoped to use sparsity, perhaps to obtain some concrete benefits as well, although as we see this one didn't really lead to any real improvements.",
                    "label": 0
                },
                {
                    "sent": "So we have we have many categories in our datasets maybe.",
                    "label": 0
                },
                {
                    "sent": "We use 16, but the full data set has 100 and three categories and so on.",
                    "label": 0
                },
                {
                    "sent": "Each of them is really a binary classification problem of its own, and we can obtain different feature rankings for each of these categories and.",
                    "label": 1
                },
                {
                    "sent": "At the end we have to choose how many how many features from the from the beginning of the of the ranking we will use to learn and then to classify.",
                    "label": 0
                },
                {
                    "sent": "But Alternatively, instead of choosing a number of features, we can also choose a sparsity and then keep as many features as are necessary to achieve that level of sparsity and.",
                    "label": 0
                },
                {
                    "sent": "It turns out that the best number of features so so if you look at each category and look at which number of features gives you the best performance for that category.",
                    "label": 0
                },
                {
                    "sent": "This can vary very much from one category to another.",
                    "label": 0
                },
                {
                    "sent": "Some of them need a lot of features, some a lot fewer features, so the hope was that maybe the sparsity doesn't vary as much.",
                    "label": 0
                },
                {
                    "sent": "And so makes it kind of firstly makes it easier to compare average performance across categories and also might give you.",
                    "label": 1
                },
                {
                    "sent": "A better foundation for determining the cutoff point.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "Instead of choosing the best number of features for each category, we might try to simplify things and fix the same number of features for all categories, but if the best number of features varies so much from one category to another, then maybe it might be a better idea to fix a constant level of sparsity and use that for each category, which will then allow us to use a different level, a different number of features for each category, because it will adapt to the distributions.",
                    "label": 1
                },
                {
                    "sent": "And the rankings over different categories.",
                    "label": 0
                },
                {
                    "sent": "So this was sort of 1 idea which the next we will compare on the next chart on the next slide and we will see that it sort of.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Doesn't really work very well, so here we have different feature rankings and the average performance over those 16 categories and the blue bars give us the performance which we can obtain if we try to fix the same number of features for all categories.",
                    "label": 0
                },
                {
                    "sent": "So we decide that we will use maybe 100 or 1000 or whatever features for each category.",
                    "label": 0
                },
                {
                    "sent": "And then compute the average performance over that and we select the number of features in the end so that the performance is the best and the purple bars show us the same sort of thing.",
                    "label": 0
                },
                {
                    "sent": "But we figured the sparsity instead of the number of features, so we will say that, for example, we want to have a sparsity of 1 or 10 or whatever for each category, and then choose as many features for that category as necessary to achieve that sparsity.",
                    "label": 0
                },
                {
                    "sent": "And again we averaged the performance in the end.",
                    "label": 0
                },
                {
                    "sent": "And the hope was that this would sort of fixing things based on sparsity instead of a number of features that this would help us improve the performance.",
                    "label": 0
                },
                {
                    "sent": "But as this chart shows, it doesn't really lead to any significant improvements, or at least not very significant ones, sort of.",
                    "label": 0
                },
                {
                    "sent": "There are some very small improvements and.",
                    "label": 0
                },
                {
                    "sent": "But at least we see that it doesn't lead to any decrease in performance either, so so that sparsity is in a way as good a way of defining these cut offs as the number of features would be in here.",
                    "label": 0
                },
                {
                    "sent": "Just for comparison, we have the results which we would obtain if we selected the best number of features or the best sparsity is equivalent anyway separately for each category, and then average that over categories.",
                    "label": 1
                },
                {
                    "sent": "Of course this gives us even better performance, so in a way.",
                    "label": 0
                },
                {
                    "sent": "This also shows that this this type of fixed cut off levels, which will be the same overall categories is not really such a good idea, so this is kind of not a very positive result as far as we are concerned.",
                    "label": 0
                },
                {
                    "sent": "So close to the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So now we we presented this concept of sparsity.",
                    "label": 0
                },
                {
                    "sent": "We try to show that it's.",
                    "label": 0
                },
                {
                    "sent": "It's a useful and interesting thing when comparing the different feature selection methods, because it sort of puts them on a more equal basis.",
                    "label": 0
                },
                {
                    "sent": "It has very direct connection with memory and CPU time consumption, unlike the number of features perhaps and we try to use it as a set of criteria and we saw that although it's not really much better, it's also not any worse than using the number of features.",
                    "label": 1
                },
                {
                    "sent": "And here at the end I would just like to point out some ideas for future.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Work in which we are working at the moment.",
                    "label": 0
                },
                {
                    "sent": "So our hope is that we would be able to characterize the feature ranking schemes in terms of other characteristics besides just sparsity curves.",
                    "label": 1
                },
                {
                    "sent": "So we saw how we can describe each feature ranking by the sparsity curve.",
                    "label": 0
                },
                {
                    "sent": "We could also try to introduce other characteristic curves.",
                    "label": 0
                },
                {
                    "sent": "For example, one thing that we were looking at this is what's the total, how quickly the total sum of information gain grows over all the terms which have been selected by a feature ranking up to a particular point.",
                    "label": 0
                },
                {
                    "sent": "But we could try to come up with other characteristics as well and sort of idea that we were having, which might come to some good results eventually.",
                    "label": 0
                },
                {
                    "sent": "Is that if we can characterize a good ranking in terms of some characteristics like this, which would be relatively easy to compute, perhaps.",
                    "label": 0
                },
                {
                    "sent": "It would be.",
                    "label": 1
                },
                {
                    "sent": "It would be great if we could eventually come up with a set of characteristics which indicates what is a good feature ranking so that in the end, if we if we can come up with a new feature ranking, perhaps artificially by synthesizing by choosing things at random.",
                    "label": 0
                },
                {
                    "sent": "We might be able to obtain a good ranking simply by synthesizing one which matches the characteristics of a previously known good ranking, and in that way, perhaps one could determine the characteristics of a good ranking on one category or one data set an artificially and perhaps cheaply obtain new rankings with comparably good performance simply by trying to match its characteristics, and in that way it might be possible to obtain good rankings without going through a really expensive process like training and SVM, just in order to obtain.",
                    "label": 0
                },
                {
                    "sent": "And SVM based feature ranking.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of some ideas which we are exploring at the moment and might be a sort of continuation of the of the work presented here and that's it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Perhaps you could go back to the slide that you had already had the.",
                    "label": 0
                },
                {
                    "sent": "Yes, whenever against the.",
                    "label": 0
                },
                {
                    "sent": "This one, yeah.",
                    "label": 0
                },
                {
                    "sent": "Looks like that.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second group is negative, so you've got this define PPO, but could you have you got any intuition why some of the other pairs?",
                    "label": 0
                },
                {
                    "sent": "Initially they increased them, but then they go up again.",
                    "label": 0
                },
                {
                    "sent": "The information gain is this one.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well, you know I don't have really a very clear intuition.",
                    "label": 0
                },
                {
                    "sent": "I would say that at the end.",
                    "label": 0
                },
                {
                    "sent": "There is perhaps yes, so the question is from my point with the question is not so much why there is a peak at the end, but while there is this low point in the middle.",
                    "label": 0
                },
                {
                    "sent": "I don't really know for sure, but I guess it's.",
                    "label": 0
                },
                {
                    "sent": "So it's probably obtains a large number of.",
                    "label": 0
                },
                {
                    "sent": "Yeah, perhaps because it is adopts a large number of infrequent features in the middle perhaps, and that might hurt the performance so much.",
                    "label": 0
                },
                {
                    "sent": "But yes, I agree, I don't have a really clear idea of why this sort of dropping performance in the middle.",
                    "label": 0
                },
                {
                    "sent": "Can you go back to the number of features with respect sparsity?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you can notice like that.",
                    "label": 0
                },
                {
                    "sent": "There is like this S shape transitioning in all of these like information gain realized and then for us.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Damn right, you don't have the you don't have this transition is the same like most common words, yes, I mean it, it doesn't have this.",
                    "label": 1
                },
                {
                    "sent": "You know why?",
                    "label": 0
                },
                {
                    "sent": "Or I mean, well, sort of we can sort of friend why why?",
                    "label": 0
                },
                {
                    "sent": "For example things like the mutual information curve have this client but at the end you know these are the really common words which appear equally sort of frequently in both classes, both the positive and negative class.",
                    "label": 0
                },
                {
                    "sent": "And for example in the case of odds ratio at the end you have all the words which are common.",
                    "label": 1
                },
                {
                    "sent": "In the negative class but not in the positive class.",
                    "label": 0
                },
                {
                    "sent": "And that again is quite a lot of very common words, and that also gives you a very low odds ratio and put them at the end of the ranking.",
                    "label": 0
                },
                {
                    "sent": "So apparently in the in the case of in the case of SVM, it is kind of apparently reasonably good at putting many of the low frequency words towards the end of the ranking, and that's why it has a slow ascent here towards the end rather than a steep one like like the others did.",
                    "label": 0
                },
                {
                    "sent": "So I guess this means that SVM.",
                    "label": 0
                },
                {
                    "sent": "Sort of.",
                    "label": 0
                },
                {
                    "sent": "Managed to put many of the less frequent words which are not useful for predicting the class.",
                    "label": 0
                },
                {
                    "sent": "It put them at the end and not maybe in the middle like some of the others did, which have a slow slow part here in the middle.",
                    "label": 0
                },
                {
                    "sent": "Have you checked whether like the best F1 is when this transition happens this?",
                    "label": 0
                },
                {
                    "sent": "Well I haven't really checked that, but sort of my if you look at the information gain curves for example, we saw that the best F1 is the relatively smaller number of features and.",
                    "label": 1
                },
                {
                    "sent": "Well, probably not as late as here when the when the transition occurs.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}