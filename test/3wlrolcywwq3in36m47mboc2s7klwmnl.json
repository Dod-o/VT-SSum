{
    "id": "3wlrolcywwq3in36m47mboc2s7klwmnl",
    "title": "Learning the dependence structure of rare events: a non-asymptotic study",
    "info": {
        "author": [
            "Nicolas Goix, Telecom ParisTech"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_goix_rare_events/",
    "segmentation": [
        [
            "Hi everyone, I'm Nicaragua and I'm going to present to my work which is joint work with my page advisors and Subway and Steven came on soon and it's about learning the dependence structure of where events."
        ],
        [
            "So the purpose of this talk is twofold.",
            "Firstly, to introduce a well known statistical theory which is precisely designed for the study of where events, unlike phenomenon whose behavior hard world by an averaging effect or mean behavior.",
            "An this story.",
            "This material extreme value theory is actually widely used in fields involving risk management like finance, insurance or environmental Sciences, but it is not used for know in machine learning or anomaly detection in general, at least in its multivariate framework, an this can actually be surprising because it seems to be a link between anomalies and extreme value.",
            "So that's why I will show you some application to anomaly detection and the second purpose of this talk is to show you how statistical learning theory can be applied to mutilate AVT in part, in particular for deriving bones on the estimation of what is called the asymptotic dependence structure.",
            "So."
        ],
        [
            "Let me introduce what materia ticks in value theory is an in particular extreme dependence structure it involves."
        ],
        [
            "So the general context of mutual activity is that you have a random vector X with margins, EG following continuous CDF FG and.",
            "If you if you want to study the difference between each features first natural step is simply to standard standardize each margins and this can be done, for instance by taking this transform here.",
            "So 1 /, 1 minus FG, FG to have some standard pirate, two variables, meaning that the survey function is of the Form 1 / X.",
            "So here of course we don't know the FG and we will have to estimate it.",
            "But let us assume for now that we know them and.",
            "In order to precise problematic so."
        ],
        [
            "The problem is we want to characterize and estimate the probability for V standardized variable to be in a given that a is extreme.",
            "So by extreme I mean far from zero with respect to any norm of RG.",
            "So of course this cannot be done without doing assumption on the tail of the distribution.",
            "And in new."
        ],
        [
            "Theoretics in value theory, the standard assumption is to assume that when you're far from zero, then you have some regularity.",
            "For instance, kind of modulo marginality, an formally.",
            "This is called regular valuation assumption.",
            "So here I speak about standardized variable V. And it means that you have this conversions here to an exponent measure knew an as soon as you have these conversions, then mu has necessary original majority property and this this property allows us to define its angular part, which is the angular measure of extremes which defined on the sphere by 5 of B.",
            "Simply mu of the induced cone.",
            "Baby, actually the truncated current because it is better than one."
        ],
        [
            "So now with this assume soon what you have is general model for extreme meaning that we can able to characterize the priority for V to be in A and this is almost knew of a an.",
            "Equivalently, you can formalize it using polar coordinate by simply decomposing you into it's Roger, Part 1, / R An its angular part, which is the angular measure Phi of B.",
            "So we can see here that Phi or mu.",
            "Worse, completely, the joint distribution of extreme.",
            "But as soon as you know the margins because I will tell you that the V here is defined using the FG.",
            "The marginal CDF."
        ],
        [
            "So the fact that Phi Mu words to join this joint distribution of extremes can be illustrated by by this figure here.",
            "So in the first case you can see that five charges social face of the simplex, and even as density on it but not the other surface.",
            "And this code in this, in this case view and V2 can be large can be extreme together and we call it a septic dependence.",
            "On the other case file only charge the two sub phase of the simplex.",
            "So here.",
            "In Dimension 2, you only have two sub phase, but in greater dimension, dimension D in dimension D you have two at the party minus one faces under some plates which correspond to any groups of coordinates an in this case for only charge, the two subphase of the simplex, meaning that V1 and V2 cannot be lost together in this case is called aseptic independence an this for more can actually be used in anomaly detection because if you have a set of normal data.",
            "Then you are able to estimate mu or Phi Phi, which represents the normal behavior in extreme urgency, and this region's extreme regions are very important in anomaly detection because it concerns less than 1% of the data, but with high percentage of anomalies, so that if you are good on such region you are able to increase the precision of your algorithm, in particular to decrease of US alarm rate."
        ],
        [
            "So now the problem is if you want to estimate fi or mu, it is very hard to attach the quality of this estimate in dimension greater than two existing existing work actually also only for dimension 2, so that the IG is to consider the restriction of mu too convenient VC class, which is the class of complementary effect on yours.",
            "And this is done by considering the stable take dependance function which is simply defined as mu of the complementary.",
            "Corresponding to X -- 1 and.",
            "So yesterday this table to deepen instruction is actually an analytic tool in the sense that if you know L, then you know mu and you know the structure of extremes and the trick is that it allows us to works on rectangles instead on any measurable set.",
            "If you do the same with you or Phi.",
            "So."
        ],
        [
            "It's not only actually analytic tools, yesterday has also some intuition behind it.",
            "In particular, if you take 2 where events with small probability P1P2, and if you assume if you assume that you know P1 and P2 or if you have estimated using for instance univariate DVT, and suppose that you that you want to investigate the joint probability P12 and actually it can be approximated by L estate F evaluated.",
            "In P1P2, as soon as pure and P2R smiling us."
        ],
        [
            "So to summarize, the link between you an the STD FL, the spectral measure mu or is simply simply defined as an aseptic limit probability limit.",
            "An L is just the restriction of Mewtwo convenient VC class.",
            "So that L can be actually expressed as.",
            "This limits here and which is actually an equivalent definition of the SDF.",
            "We will use later for defining its empirical counterpart an.",
            "Considering this definition we can also can define a bias term, which is simply the difference between what is in the limit and the limit itself, namely the SDF.",
            "So here we consider as a Superman on compact set because we will need it later to have bonded VC class."
        ],
        [
            "So now for the estimation part, will related work."
        ],
        [
            "It consists in some tragic results an under smoothness assumption, but our goal is to derive known as synthetic bones and with no assumption accepting the existence of the SDF, which is actually equivalent to the regular valuation assumption.",
            "I have told you before."
        ],
        [
            "So using the previous alternative definition of the STD FL, we are able to define its empirical counterpart by simply replacing V by its empirical counterpart had V&T, which goes to Infinity by N / K, where K depends on N and go also goes to Infinity but slower than N and actually K corresponds to the number of data considered as extreme.",
            "In other words, K is the number of data you really used in the.",
            "Approximation in the estimation process.",
            "So here we simply estimate the merge the marginal CDF by the rank of the corresponding feature."
        ],
        [
            "So the main issue you can see here that you have such scaling factor which goes to Infinity just before the probability.",
            "And if you want to use standard concentration inequality, it will it ways problem.",
            "Actually, to be more precisely, we are in such a case here where P&PN corresponds to almost uniform variable, and so the bad news is this factor here.",
            "But there is a good news that K / N goes to zero an if the set hey is bounded, then the limbic measures will also goes to zero and then the probability so.",
            "If we apply simply standard VIS inequality, we obtain bone like this, which can even goes to Infinity so that we have to take into account that we have to use the fact we have to use the good news where to use the fact that came over in terms a as RBC measure which goes to 0."
        ],
        [
            "And this is done by using adaptive VC inequality which allow you to add in the bond square of P factor.",
            "Which is actually the probability to be in the union of your VC classes.",
            "That is why we want to have bonded VC class and by chance this probability is bounded by ATM in K / N which yields bond which should stand out visible but with K instead of North which was actually expected because K is a real number of observation you use in the estimation process."
        ],
        [
            "So after other issue you have this final results which is bound on the maximal deviation for the estimation of the SDF and the first term.",
            "Here is simply a classical visitor, but with K instead of N and we also have the bias term here, which is the price to pay to do no assumption Anet measure how far are we in the tail, and this bias team is simply.",
            "The difference between zested F and the quantity which goes to the SDF, which is an accepted quantity.",
            "And so here we need a super mom on a compact set.",
            "Becausw yes we need each XG to be between zero and T. 'cause then you have bonded VC class and you are able to bound the probability to be in the union of this VC class.",
            "So what is the bias term goes to 0 by assumption, as soon as N goes to Infinity."
        ],
        [
            "So an idea for applications to anomaly detection will be too.",
            "So firstly, I recall that the structure of five can be very complex in dimension greater than two, meaning that you have to a deeper D minus one faces on the simplex.",
            "And actually each.",
            "Each face on the simplex corresponds to a group of features and to each group of features you are facing the simplex, and if I does charge certain face of the simplex, it means that the corresponding features can be large together.",
            "But what we hope is that file as sparse structure, meaning that file doesn't charge every face in the simplex, meaning that you cannot have every possible combination of features which can be large together.",
            "And in practice it is verify an.",
            "No, if we have such a sparse structure, we would like to estimate the face which have mass, because if we are able to estimate that face or normal data then the anomalies will be the point which violate this dependence structure which are not in the in the good faces.",
            "And to do this.",
            "So.",
            "Here, each phase, excepting the central one, each phase corresponds to Kern, which has zero LA bag measure.",
            "So you cannot just count the data on it too.",
            "It will be no data on it, and so you are.",
            "You want to add a 30 once parameter epsilon in order to capture the points which are close to the face.",
            "And then if you do this, what you obtain is you obtain scones, which can actually be approximated by rectangles and so that you can.",
            "What produces the result before?",
            "You can reproduce the reasoning to obtain bones on.",
            "To obtain bones on the the sparse dependent structure estimation.",
            "And.",
            "So.",
            "So it is actually very.",
            "They go to to use mathematics and value theory in detection because actually standout anomaly detection only use univariate extreme value theory even to handle multibyte observations and what the loses the dependence structures University you don't have any dependence structure, and it is particularly important when you are when you have in you have large dimensional data."
        ],
        [
            "And as a conclusion, learning theory can be adapted to meteoritics and value theory too.",
            "22 yields new assertions in this field, and we also have seen more generally as a tools for study low priority regions in general.",
            "And this works, paves the way to the use of material ticks in value theory in machine learning.",
            "Anomaly detection, which is an ongoing work, the."
        ],
        [
            "Thank you for your attention and here are some references."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hi everyone, I'm Nicaragua and I'm going to present to my work which is joint work with my page advisors and Subway and Steven came on soon and it's about learning the dependence structure of where events.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the purpose of this talk is twofold.",
                    "label": 0
                },
                {
                    "sent": "Firstly, to introduce a well known statistical theory which is precisely designed for the study of where events, unlike phenomenon whose behavior hard world by an averaging effect or mean behavior.",
                    "label": 0
                },
                {
                    "sent": "An this story.",
                    "label": 0
                },
                {
                    "sent": "This material extreme value theory is actually widely used in fields involving risk management like finance, insurance or environmental Sciences, but it is not used for know in machine learning or anomaly detection in general, at least in its multivariate framework, an this can actually be surprising because it seems to be a link between anomalies and extreme value.",
                    "label": 0
                },
                {
                    "sent": "So that's why I will show you some application to anomaly detection and the second purpose of this talk is to show you how statistical learning theory can be applied to mutilate AVT in part, in particular for deriving bones on the estimation of what is called the asymptotic dependence structure.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me introduce what materia ticks in value theory is an in particular extreme dependence structure it involves.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the general context of mutual activity is that you have a random vector X with margins, EG following continuous CDF FG and.",
                    "label": 1
                },
                {
                    "sent": "If you if you want to study the difference between each features first natural step is simply to standard standardize each margins and this can be done, for instance by taking this transform here.",
                    "label": 0
                },
                {
                    "sent": "So 1 /, 1 minus FG, FG to have some standard pirate, two variables, meaning that the survey function is of the Form 1 / X.",
                    "label": 0
                },
                {
                    "sent": "So here of course we don't know the FG and we will have to estimate it.",
                    "label": 0
                },
                {
                    "sent": "But let us assume for now that we know them and.",
                    "label": 0
                },
                {
                    "sent": "In order to precise problematic so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem is we want to characterize and estimate the probability for V standardized variable to be in a given that a is extreme.",
                    "label": 0
                },
                {
                    "sent": "So by extreme I mean far from zero with respect to any norm of RG.",
                    "label": 1
                },
                {
                    "sent": "So of course this cannot be done without doing assumption on the tail of the distribution.",
                    "label": 0
                },
                {
                    "sent": "And in new.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Theoretics in value theory, the standard assumption is to assume that when you're far from zero, then you have some regularity.",
                    "label": 0
                },
                {
                    "sent": "For instance, kind of modulo marginality, an formally.",
                    "label": 0
                },
                {
                    "sent": "This is called regular valuation assumption.",
                    "label": 0
                },
                {
                    "sent": "So here I speak about standardized variable V. And it means that you have this conversions here to an exponent measure knew an as soon as you have these conversions, then mu has necessary original majority property and this this property allows us to define its angular part, which is the angular measure of extremes which defined on the sphere by 5 of B.",
                    "label": 0
                },
                {
                    "sent": "Simply mu of the induced cone.",
                    "label": 0
                },
                {
                    "sent": "Baby, actually the truncated current because it is better than one.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now with this assume soon what you have is general model for extreme meaning that we can able to characterize the priority for V to be in A and this is almost knew of a an.",
                    "label": 1
                },
                {
                    "sent": "Equivalently, you can formalize it using polar coordinate by simply decomposing you into it's Roger, Part 1, / R An its angular part, which is the angular measure Phi of B.",
                    "label": 0
                },
                {
                    "sent": "So we can see here that Phi or mu.",
                    "label": 0
                },
                {
                    "sent": "Worse, completely, the joint distribution of extreme.",
                    "label": 1
                },
                {
                    "sent": "But as soon as you know the margins because I will tell you that the V here is defined using the FG.",
                    "label": 0
                },
                {
                    "sent": "The marginal CDF.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the fact that Phi Mu words to join this joint distribution of extremes can be illustrated by by this figure here.",
                    "label": 1
                },
                {
                    "sent": "So in the first case you can see that five charges social face of the simplex, and even as density on it but not the other surface.",
                    "label": 0
                },
                {
                    "sent": "And this code in this, in this case view and V2 can be large can be extreme together and we call it a septic dependence.",
                    "label": 0
                },
                {
                    "sent": "On the other case file only charge the two sub phase of the simplex.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "In Dimension 2, you only have two sub phase, but in greater dimension, dimension D in dimension D you have two at the party minus one faces under some plates which correspond to any groups of coordinates an in this case for only charge, the two subphase of the simplex, meaning that V1 and V2 cannot be lost together in this case is called aseptic independence an this for more can actually be used in anomaly detection because if you have a set of normal data.",
                    "label": 1
                },
                {
                    "sent": "Then you are able to estimate mu or Phi Phi, which represents the normal behavior in extreme urgency, and this region's extreme regions are very important in anomaly detection because it concerns less than 1% of the data, but with high percentage of anomalies, so that if you are good on such region you are able to increase the precision of your algorithm, in particular to decrease of US alarm rate.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now the problem is if you want to estimate fi or mu, it is very hard to attach the quality of this estimate in dimension greater than two existing existing work actually also only for dimension 2, so that the IG is to consider the restriction of mu too convenient VC class, which is the class of complementary effect on yours.",
                    "label": 1
                },
                {
                    "sent": "And this is done by considering the stable take dependance function which is simply defined as mu of the complementary.",
                    "label": 0
                },
                {
                    "sent": "Corresponding to X -- 1 and.",
                    "label": 0
                },
                {
                    "sent": "So yesterday this table to deepen instruction is actually an analytic tool in the sense that if you know L, then you know mu and you know the structure of extremes and the trick is that it allows us to works on rectangles instead on any measurable set.",
                    "label": 1
                },
                {
                    "sent": "If you do the same with you or Phi.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's not only actually analytic tools, yesterday has also some intuition behind it.",
                    "label": 0
                },
                {
                    "sent": "In particular, if you take 2 where events with small probability P1P2, and if you assume if you assume that you know P1 and P2 or if you have estimated using for instance univariate DVT, and suppose that you that you want to investigate the joint probability P12 and actually it can be approximated by L estate F evaluated.",
                    "label": 1
                },
                {
                    "sent": "In P1P2, as soon as pure and P2R smiling us.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to summarize, the link between you an the STD FL, the spectral measure mu or is simply simply defined as an aseptic limit probability limit.",
                    "label": 0
                },
                {
                    "sent": "An L is just the restriction of Mewtwo convenient VC class.",
                    "label": 0
                },
                {
                    "sent": "So that L can be actually expressed as.",
                    "label": 0
                },
                {
                    "sent": "This limits here and which is actually an equivalent definition of the SDF.",
                    "label": 1
                },
                {
                    "sent": "We will use later for defining its empirical counterpart an.",
                    "label": 0
                },
                {
                    "sent": "Considering this definition we can also can define a bias term, which is simply the difference between what is in the limit and the limit itself, namely the SDF.",
                    "label": 0
                },
                {
                    "sent": "So here we consider as a Superman on compact set because we will need it later to have bonded VC class.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now for the estimation part, will related work.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It consists in some tragic results an under smoothness assumption, but our goal is to derive known as synthetic bones and with no assumption accepting the existence of the SDF, which is actually equivalent to the regular valuation assumption.",
                    "label": 0
                },
                {
                    "sent": "I have told you before.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So using the previous alternative definition of the STD FL, we are able to define its empirical counterpart by simply replacing V by its empirical counterpart had V&T, which goes to Infinity by N / K, where K depends on N and go also goes to Infinity but slower than N and actually K corresponds to the number of data considered as extreme.",
                    "label": 0
                },
                {
                    "sent": "In other words, K is the number of data you really used in the.",
                    "label": 0
                },
                {
                    "sent": "Approximation in the estimation process.",
                    "label": 0
                },
                {
                    "sent": "So here we simply estimate the merge the marginal CDF by the rank of the corresponding feature.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the main issue you can see here that you have such scaling factor which goes to Infinity just before the probability.",
                    "label": 0
                },
                {
                    "sent": "And if you want to use standard concentration inequality, it will it ways problem.",
                    "label": 1
                },
                {
                    "sent": "Actually, to be more precisely, we are in such a case here where P&PN corresponds to almost uniform variable, and so the bad news is this factor here.",
                    "label": 0
                },
                {
                    "sent": "But there is a good news that K / N goes to zero an if the set hey is bounded, then the limbic measures will also goes to zero and then the probability so.",
                    "label": 0
                },
                {
                    "sent": "If we apply simply standard VIS inequality, we obtain bone like this, which can even goes to Infinity so that we have to take into account that we have to use the fact we have to use the good news where to use the fact that came over in terms a as RBC measure which goes to 0.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this is done by using adaptive VC inequality which allow you to add in the bond square of P factor.",
                    "label": 0
                },
                {
                    "sent": "Which is actually the probability to be in the union of your VC classes.",
                    "label": 1
                },
                {
                    "sent": "That is why we want to have bonded VC class and by chance this probability is bounded by ATM in K / N which yields bond which should stand out visible but with K instead of North which was actually expected because K is a real number of observation you use in the estimation process.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So after other issue you have this final results which is bound on the maximal deviation for the estimation of the SDF and the first term.",
                    "label": 0
                },
                {
                    "sent": "Here is simply a classical visitor, but with K instead of N and we also have the bias term here, which is the price to pay to do no assumption Anet measure how far are we in the tail, and this bias team is simply.",
                    "label": 1
                },
                {
                    "sent": "The difference between zested F and the quantity which goes to the SDF, which is an accepted quantity.",
                    "label": 0
                },
                {
                    "sent": "And so here we need a super mom on a compact set.",
                    "label": 0
                },
                {
                    "sent": "Becausw yes we need each XG to be between zero and T. 'cause then you have bonded VC class and you are able to bound the probability to be in the union of this VC class.",
                    "label": 1
                },
                {
                    "sent": "So what is the bias term goes to 0 by assumption, as soon as N goes to Infinity.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So an idea for applications to anomaly detection will be too.",
                    "label": 1
                },
                {
                    "sent": "So firstly, I recall that the structure of five can be very complex in dimension greater than two, meaning that you have to a deeper D minus one faces on the simplex.",
                    "label": 0
                },
                {
                    "sent": "And actually each.",
                    "label": 0
                },
                {
                    "sent": "Each face on the simplex corresponds to a group of features and to each group of features you are facing the simplex, and if I does charge certain face of the simplex, it means that the corresponding features can be large together.",
                    "label": 0
                },
                {
                    "sent": "But what we hope is that file as sparse structure, meaning that file doesn't charge every face in the simplex, meaning that you cannot have every possible combination of features which can be large together.",
                    "label": 0
                },
                {
                    "sent": "And in practice it is verify an.",
                    "label": 0
                },
                {
                    "sent": "No, if we have such a sparse structure, we would like to estimate the face which have mass, because if we are able to estimate that face or normal data then the anomalies will be the point which violate this dependence structure which are not in the in the good faces.",
                    "label": 0
                },
                {
                    "sent": "And to do this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here, each phase, excepting the central one, each phase corresponds to Kern, which has zero LA bag measure.",
                    "label": 0
                },
                {
                    "sent": "So you cannot just count the data on it too.",
                    "label": 0
                },
                {
                    "sent": "It will be no data on it, and so you are.",
                    "label": 0
                },
                {
                    "sent": "You want to add a 30 once parameter epsilon in order to capture the points which are close to the face.",
                    "label": 0
                },
                {
                    "sent": "And then if you do this, what you obtain is you obtain scones, which can actually be approximated by rectangles and so that you can.",
                    "label": 0
                },
                {
                    "sent": "What produces the result before?",
                    "label": 0
                },
                {
                    "sent": "You can reproduce the reasoning to obtain bones on.",
                    "label": 0
                },
                {
                    "sent": "To obtain bones on the the sparse dependent structure estimation.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So it is actually very.",
                    "label": 0
                },
                {
                    "sent": "They go to to use mathematics and value theory in detection because actually standout anomaly detection only use univariate extreme value theory even to handle multibyte observations and what the loses the dependence structures University you don't have any dependence structure, and it is particularly important when you are when you have in you have large dimensional data.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And as a conclusion, learning theory can be adapted to meteoritics and value theory too.",
                    "label": 1
                },
                {
                    "sent": "22 yields new assertions in this field, and we also have seen more generally as a tools for study low priority regions in general.",
                    "label": 0
                },
                {
                    "sent": "And this works, paves the way to the use of material ticks in value theory in machine learning.",
                    "label": 1
                },
                {
                    "sent": "Anomaly detection, which is an ongoing work, the.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you for your attention and here are some references.",
                    "label": 0
                }
            ]
        }
    }
}