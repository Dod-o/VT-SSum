{
    "id": "lfnh6dhfu7xvqo46v5xiznchmd5t3gam",
    "title": "Object Localization with Global and Local Context Kernels",
    "info": {
        "author": [
            "Matthew B. Blaschko, Max Planck Institute for Biological Cybernetics, Max Planck Institute"
        ],
        "published": "Dec. 1, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/bmvc09_blaschko_olgl/",
    "segmentation": [
        [
            "So I'm going to talk about sliding window."
        ],
        [
            "In general, and then I'm going to talk about formalism for doing regression from one input space to some sort of arbitrary output space, which is a joint kernel nap.",
            "Then I'm going to talk about how to incorporate contextual cues into a joint kernel map for object localization.",
            "Then I'll show how one can do branch and bound localization with this, typically getting localization speeds of 10s of milliseconds, and then I'll give some."
        ],
        [
            "Results in terms of accuracy on VLC.",
            "So let's talk about sliding window localization."
        ],
        [
            "This is a standard localization.",
            "You're probably all familiar with from the OC competition, so we have a bunch of objects in an image and we'd like to have classifiers or some sort of discriminant function for each of these objects, and then we'd like to find those objects in the image."
        ],
        [
            "By putting a bounding box around them.",
            "We can parameterize this bounding box, and we're going to use this parametrisation throughout the talk by specifying the left, top, right, and bottom of the bounding box, so that's going to be in terms of coordinates of the pixels in the image.",
            "OK, so we can think of this left top right."
        ],
        [
            "And bottom of the bounding box as a space of objects that we'd like to predict, so there's only going to be certain ones of these that are valid.",
            "For example, we need to have that the right is greater than the left.",
            "At the bottom is greater than the top.",
            "And X is going to be our notation for images and will use math calcs and math Kelway to denote the space of images and the space of bounding boxes.",
            "And then we're also going to assume that we have a procedure for training and combat compatibility function.",
            "This can be support vector machine, structured output regression, any sort of compatibility function that you want.",
            "It is going to take in an image and a bounding box and then return a score for that.",
            "So this is a standard sliding window framework.",
            "And then when we do our localization, what we want to do is find the maximally scored bounding box of all possible ones in R."
        ],
        [
            "Space Y."
        ],
        [
            "So what is a joint kernel map?",
            "Well, first of all, let's say what we're going to assume about our compatibility function F. We're going to say that it's going to be linear with respect to some feature function.",
            "Phi Phi is going to take X&Y, and it's going to then return some feature vector, and then we're going to have parameters W. An R score is simply going to be the inner product of those parameters, and the vector resulting from the feature function.",
            "Now the feature function we can make that implicit in a joint kernel, so this is different from kernels that you might have been familiar with before, where you only take in an X and another ex.",
            "So for example 2 images, and then you have some sort of comparison of the similarity of the images.",
            "Now what we want actually is a function that's going to take an image and a bounding box as well as another image in a bounding box and then compare somehow the things that are present in those bounding boxes.",
            "OK, an using the Representer theorem.",
            "Then in the end, we end up with the compatibility function at the bottom, which is going to be the sum over weighted joint kernel."
        ],
        [
            "Evaluations.",
            "OK, So what is an example of a joint kernel that we can use for localization?",
            "The restriction kernel we can define by taking X restricted to why that's what this notation X vertical bar Y means an.",
            "You can just think of that you take the image, you take the bounding box and you crop that image to the bounding box, and then we can use any standard image kernel.",
            "We want to compare these so you can use a bag of words, image, kernel.",
            "You can use a chi square kernel pyramid, match kernel.",
            "Anything really that you've seen in the literature to compare these regions you can use.",
            "And the resulting joint kernel is going to be positive definite.",
            "Over this SpaceX cross Y."
        ],
        [
            "So here's some examples of restriction kernel on the top we have two images where we have cows present in each of these images and if we have a combination of an image with a bounding box that specifies the region around the cow, another image with the bounding box specifying the region around the cow, then we see that the similarity between these two Cal regions is large and so our kernel value is going to be large.",
            "If we take the same 2 images but have different bounding boxes, then those regions are going to be dissimilar and we're going to get a small score."
        ],
        [
            "Now there's a problem using the restriction.",
            "Kernel is a little bit like having tunnel vision.",
            "You can't actually see really what's going on here.",
            "This could be a bus, a boat, or."
        ],
        [
            "Nothing turns out it's an airplane, but really, we need to look around the entire scene to have higher conf."
        ],
        [
            "And so what's going on?",
            "So how do we incorporate contextual?"
        ],
        [
            "Is into joint kernel framework.",
            "We can use global context an this has been something that people use all the time.",
            "You just take the entire image.",
            "You ignore the bounding box information and that's still going to be a positive definite kernel over the space of X cross Y."
        ],
        [
            "We can do local context kernels where we have some parameter Phi or PSI Theta which is going to give us then a region Theta of Y.",
            "That is going to give us a region in the image relative to a current bounding box, why?",
            "So putting it all together, we."
        ],
        [
            "And actually just take weighted sum of all of these and we can learn those weights with multiple kernel learning."
        ],
        [
            "So what does the local context kernel look like?",
            "What we've chosen here is to have a region directly around current bounding box, so we have our parameter Theta.",
            "So yeah, data and.",
            "Theta over two.",
            "You can think of as a multiplicative factor.",
            "That's going to give the size of this outer box of the local context.",
            "The local context is indicated here by the shaded region, so inside the local context we have the actual bounding box.",
            "This is where we want to have the object end up, and then we have some strip of image around that.",
            "And we can parameterize that with one single parameter.",
            "And.",
            "We've chosen this definition because we can model the statistics of an objects neighborhood, but we're not going to pollute those statistics with the statistics of the object itself."
        ],
        [
            "OK, so this so far actually is not that different from what a lot of people have already done for including context into localization an I would say probably the main contribution of the work, then is how to incorporate that into a branch and bound framework so that we can get very high."
        ],
        [
            "Speed using this exact same model.",
            "So if we want to do sliding windows, then exhaustive evaluation is going to be very expensive.",
            "If we want to really evaluate the window at every single possible location, but we don't have to because similar boxes are going to have similar scores.",
            "We have some geometric information about the structure of the image, so if we see that there's no car in the upper right hand corner of the image, then we don't need to evaluate each of these bounding boxes.",
            "We can come up with a bound for that entire region and just discard that, because that's not going to have a very high.",
            "Bound for the scores of boxes that can occur there."
        ],
        [
            "So how do we achieve that?",
            "It's quite simple, given an upper bound for a an objective function over sets of rectangles.",
            "So the upper bound is going to provide.",
            "Upper bound for any possible rectangle that can occur in that set of rectangles.",
            "We put our set of rectangles as a state into a priority queue.",
            "We pull the state that has the highest upper bound off the priority queue, split that into two and recalculate our upper bounds, and then once we've gotten to a single element that is a state where there's only one rectangle in that state, then we know we found the optimum."
        ],
        [
            "So to do that, we need to specify sets of rectangles.",
            "We can do this compactly by specifying ranges over the left, top, right and bottom, instead of having a single rectangle, we're going to have intervals for each of these.",
            "And this gives us the notion that for these intervals where we have a lower bound and upper bound for the location of each of these, the Gray region now is specifying all of the possible rectangles that can be in the set of bounding boxes."
        ],
        [
            "The split it's quite simple.",
            "We take the side that has the most uncertainty.",
            "In this case it's the right side and we split it into two intervals.",
            "The left interval which is shown on the left side and the right interval on the right."
        ],
        [
            "So how are we going to then provide a bound for these sets of rectangles?",
            "So what we do is we bound the weighted sum of kernels.",
            "Remember we have a kernel for the restriction kernel.",
            "The local context in the global context we're going to bound that by the sum of bounds for each of these individual kernels.",
            "So the restriction Colonel we've shown in CPR and Pammy this year.",
            "How to come up with bounds for that?",
            "The global context kernel doesn't even depend on why, so it's going to be just a constant value for all possible bounding boxes.",
            "So the only thing we really need to worry about is how to specify a bound for the local context.",
            "What we can do is actually propagate uncertainty about the position of the bounding box through our function Theta, and if we do that, that gives us ranges for the value Theta of Y.",
            "This is actually just standard interval arithmetic.",
            "If you're familiar with that.",
            "And using these ranges, that gives us the range of uncertainty that we have for the location of the boundaries of the context kernel.",
            "For a linear kernel, that means that we can actually come up with an upper bound for the local context.",
            "Using only four numbers, so this notation at the bottom Theta hat of capital Y, we have a superscript plus in the first term, indicating that we're going to be summing all positive features and an_Max over the maximum possible size box.",
            "Then we want to sum all negative features over the minimum possible box of this outer bound, and from that we're going to subtract all of the positive features from the minimum possible bounding box and all the negative features from the maximum possible bounding box.",
            "And this is going to give us an upper bound of the score of all of those features that are going to end up in this strip around the bounding box, and it will converge to the true score as the uncertainty in those intervals goes to 0."
        ],
        [
            "So let's look at some results.",
            "Our parameter settings."
        ],
        [
            "Are as follows.",
            "We have Theta set to root 2 / 2.",
            "Now what that actually gives us is that the contextual region is going to be the same number of pixels as the bounding box.",
            "It's just something we chose kind of out of a hat we use VLC datasets, surf descriptors that were vector quantized to 3000 features Chi Square kernel for global context and then we use linear bag of word features.",
            "Linear backwards kernels for the restriction in local context and we chose that because it's very fast to specify the bound.",
            "Using.",
            "Using integral images, we can actually do the total bound for both the restriction and the local context kernels, and only I think 4 for look ups."
        ],
        [
            "So there's just a few example results.",
            "The full results tables are available in the paper.",
            "And this is pretty typical for all of the results.",
            "What we see is if we use no context, we get significantly worse than using context, either with a fixed waiting or with the learned waiting.",
            "So with the fixed waiting, what we've done is we've actually traced normalized our kernel matrices and then just sum them up.",
            "So you can just think of that as kind of normalizing the range of our features and then taking just an average of them so.",
            "So we haven't done any multiple kernel learning tricks there and then with the learned weights.",
            "That's the result from multiple kernel learning.",
            "So in this case we actually do get a pretty consistent improvement from using multiple kernel learning over the fixed weights, and that's because different types of contexts are going to be important for different object categories.",
            "In the bottom curves, what we have is 2 example classes where we compared to our work from CPR in 2008 and you can see the black curve at the bottom is just going to be the average weights where the blue curve is the restriction kernel.",
            "Qualitatively, there's not much of a difference between those.",
            "We haven't seen much of an improvement.",
            "The pink curve, which is from our CVR paper.",
            "We actually used something like a global context to re wait the.",
            "The order of the ranking and then the red curve at the top is the learned weights.",
            "So you can see that the effect, both global and local context is important."
        ],
        [
            "So to conclude, we have a joint kernel framework where you can just throw in global and local context."
        ],
        [
            "Context kernels are actually very helpful if they're learned appropriately.",
            "If you just wait them arbitrarily, then you probably won't get that much of an improvement.",
            "Ah."
        ],
        [
            "And by defining local context kernels relative to the bounding box, we can actually still have a very efficient solution because we don't have a very high dimensional parametrization of our branch and bound, we only need to still optimize over 4 variables.",
            "The top left, bottom, right of the bounding box.",
            "So I'm ready to answer any questions and I want to give a plug to our special."
        ],
        [
            "Issue of AJC that's coming out over well the deadline is going to be in February so please submit if you want.",
            "OK thanks.",
            "Well, well, I think that has to do with how we've defined the local context.",
            "So because the local context gives us the background, whereas the restriction kernel gives us the actual statistics of the foreground of the object that we're interested in, we're able to say if you have some sort of consistent local context, then we can use that.",
            "And actually the multiple kernel learning will will determine how much to rely on that local context.",
            "So if the background statistics aren't somehow consistent around the object of interest, then then we'll just down weight that and not use that at all.",
            "Right, yeah?",
            "So if you have, you know a random background then then it will essentially learn not to use it at all.",
            "But if you have a car that's always on the road as opposed to in the Bush is, then it will be able to make use of that.",
            "Yeah, so actually, that was one of the pieces of work that got me thinking about doing this and what they if it's the paper I'm thinking of, then what they did is actually apply.",
            "Fixed segmentation ahead of time as a preprocessing step and then based on that segmentation actually pull out kind of relative regions is that the one you're talking about?",
            "Yeah, so if you do this work then.",
            "I think that it really probably doesn't make that big of a difference in terms of the actual statistics.",
            "If you use a rectangular region versus if you use the actual segmentation and so.",
            "I thought that this would be a way where you could get a lot of those same effects, but then still be able to do a very quick localization.",
            "So I would say that's really the relationship between the two.",
            "You mean like a polynomial kernel or something like that, right?",
            "So in general it's very easy to come up with a bound for many types of kernels.",
            "So for a polynomial kernel or anything else more or less, you can use what's called interval arithmetic, and this is a standard library of mathematical routines where you say that if you input interval that has an upper and lower bound for a certain value, and then you can provide a lot of mathematical operations.",
            "Then those mathematical operations will return back an interval itself, which gives an upper and lower bound, and so if you pipe this through your normal code for computing the kernel, then in the end you'll end up with a valid upper bound.",
            "So there is an automated automatic way to do it.",
            "Usually if you're more clever about it, you can come up with tighter bounds than what you would get from that, and then you'll therefore get faster convergence.",
            "So it is.",
            "There's a bit of art to it still, but it is pretty generally applicable.",
            "Yes, absolutely, so.",
            "Your when we computed Theta.",
            "Actually here we go, yeah, so we would just have a different definition for this function Theta an if you were to actually use the say I just fixed offset so instead of Theta times are minus L you just have L minus Theta T plus Theta.",
            "Then you'd have a fixed region a fixed number of pixels.",
            "Then of course you still have your range of uncertainty from your bounding box during the branch and bound scheme.",
            "And you can propagate that uncertainty through this more simple function, and then that would give you again ranges for that outer boundary.",
            "And you could use that again.",
            "I've not done empirical results on whether that works better.",
            "I suppose it's going to be probably class dependent.",
            "Which one works better?"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to talk about sliding window.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In general, and then I'm going to talk about formalism for doing regression from one input space to some sort of arbitrary output space, which is a joint kernel nap.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to talk about how to incorporate contextual cues into a joint kernel map for object localization.",
                    "label": 1
                },
                {
                    "sent": "Then I'll show how one can do branch and bound localization with this, typically getting localization speeds of 10s of milliseconds, and then I'll give some.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results in terms of accuracy on VLC.",
                    "label": 0
                },
                {
                    "sent": "So let's talk about sliding window localization.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a standard localization.",
                    "label": 0
                },
                {
                    "sent": "You're probably all familiar with from the OC competition, so we have a bunch of objects in an image and we'd like to have classifiers or some sort of discriminant function for each of these objects, and then we'd like to find those objects in the image.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By putting a bounding box around them.",
                    "label": 1
                },
                {
                    "sent": "We can parameterize this bounding box, and we're going to use this parametrisation throughout the talk by specifying the left, top, right, and bottom of the bounding box, so that's going to be in terms of coordinates of the pixels in the image.",
                    "label": 0
                },
                {
                    "sent": "OK, so we can think of this left top right.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And bottom of the bounding box as a space of objects that we'd like to predict, so there's only going to be certain ones of these that are valid.",
                    "label": 0
                },
                {
                    "sent": "For example, we need to have that the right is greater than the left.",
                    "label": 0
                },
                {
                    "sent": "At the bottom is greater than the top.",
                    "label": 0
                },
                {
                    "sent": "And X is going to be our notation for images and will use math calcs and math Kelway to denote the space of images and the space of bounding boxes.",
                    "label": 1
                },
                {
                    "sent": "And then we're also going to assume that we have a procedure for training and combat compatibility function.",
                    "label": 0
                },
                {
                    "sent": "This can be support vector machine, structured output regression, any sort of compatibility function that you want.",
                    "label": 1
                },
                {
                    "sent": "It is going to take in an image and a bounding box and then return a score for that.",
                    "label": 0
                },
                {
                    "sent": "So this is a standard sliding window framework.",
                    "label": 0
                },
                {
                    "sent": "And then when we do our localization, what we want to do is find the maximally scored bounding box of all possible ones in R.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Space Y.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is a joint kernel map?",
                    "label": 1
                },
                {
                    "sent": "Well, first of all, let's say what we're going to assume about our compatibility function F. We're going to say that it's going to be linear with respect to some feature function.",
                    "label": 1
                },
                {
                    "sent": "Phi Phi is going to take X&Y, and it's going to then return some feature vector, and then we're going to have parameters W. An R score is simply going to be the inner product of those parameters, and the vector resulting from the feature function.",
                    "label": 0
                },
                {
                    "sent": "Now the feature function we can make that implicit in a joint kernel, so this is different from kernels that you might have been familiar with before, where you only take in an X and another ex.",
                    "label": 1
                },
                {
                    "sent": "So for example 2 images, and then you have some sort of comparison of the similarity of the images.",
                    "label": 0
                },
                {
                    "sent": "Now what we want actually is a function that's going to take an image and a bounding box as well as another image in a bounding box and then compare somehow the things that are present in those bounding boxes.",
                    "label": 0
                },
                {
                    "sent": "OK, an using the Representer theorem.",
                    "label": 0
                },
                {
                    "sent": "Then in the end, we end up with the compatibility function at the bottom, which is going to be the sum over weighted joint kernel.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Evaluations.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is an example of a joint kernel that we can use for localization?",
                    "label": 0
                },
                {
                    "sent": "The restriction kernel we can define by taking X restricted to why that's what this notation X vertical bar Y means an.",
                    "label": 1
                },
                {
                    "sent": "You can just think of that you take the image, you take the bounding box and you crop that image to the bounding box, and then we can use any standard image kernel.",
                    "label": 1
                },
                {
                    "sent": "We want to compare these so you can use a bag of words, image, kernel.",
                    "label": 0
                },
                {
                    "sent": "You can use a chi square kernel pyramid, match kernel.",
                    "label": 0
                },
                {
                    "sent": "Anything really that you've seen in the literature to compare these regions you can use.",
                    "label": 0
                },
                {
                    "sent": "And the resulting joint kernel is going to be positive definite.",
                    "label": 1
                },
                {
                    "sent": "Over this SpaceX cross Y.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's some examples of restriction kernel on the top we have two images where we have cows present in each of these images and if we have a combination of an image with a bounding box that specifies the region around the cow, another image with the bounding box specifying the region around the cow, then we see that the similarity between these two Cal regions is large and so our kernel value is going to be large.",
                    "label": 0
                },
                {
                    "sent": "If we take the same 2 images but have different bounding boxes, then those regions are going to be dissimilar and we're going to get a small score.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now there's a problem using the restriction.",
                    "label": 0
                },
                {
                    "sent": "Kernel is a little bit like having tunnel vision.",
                    "label": 1
                },
                {
                    "sent": "You can't actually see really what's going on here.",
                    "label": 0
                },
                {
                    "sent": "This could be a bus, a boat, or.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nothing turns out it's an airplane, but really, we need to look around the entire scene to have higher conf.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so what's going on?",
                    "label": 0
                },
                {
                    "sent": "So how do we incorporate contextual?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is into joint kernel framework.",
                    "label": 0
                },
                {
                    "sent": "We can use global context an this has been something that people use all the time.",
                    "label": 0
                },
                {
                    "sent": "You just take the entire image.",
                    "label": 0
                },
                {
                    "sent": "You ignore the bounding box information and that's still going to be a positive definite kernel over the space of X cross Y.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can do local context kernels where we have some parameter Phi or PSI Theta which is going to give us then a region Theta of Y.",
                    "label": 1
                },
                {
                    "sent": "That is going to give us a region in the image relative to a current bounding box, why?",
                    "label": 0
                },
                {
                    "sent": "So putting it all together, we.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And actually just take weighted sum of all of these and we can learn those weights with multiple kernel learning.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what does the local context kernel look like?",
                    "label": 1
                },
                {
                    "sent": "What we've chosen here is to have a region directly around current bounding box, so we have our parameter Theta.",
                    "label": 0
                },
                {
                    "sent": "So yeah, data and.",
                    "label": 0
                },
                {
                    "sent": "Theta over two.",
                    "label": 0
                },
                {
                    "sent": "You can think of as a multiplicative factor.",
                    "label": 0
                },
                {
                    "sent": "That's going to give the size of this outer box of the local context.",
                    "label": 0
                },
                {
                    "sent": "The local context is indicated here by the shaded region, so inside the local context we have the actual bounding box.",
                    "label": 1
                },
                {
                    "sent": "This is where we want to have the object end up, and then we have some strip of image around that.",
                    "label": 0
                },
                {
                    "sent": "And we can parameterize that with one single parameter.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We've chosen this definition because we can model the statistics of an objects neighborhood, but we're not going to pollute those statistics with the statistics of the object itself.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this so far actually is not that different from what a lot of people have already done for including context into localization an I would say probably the main contribution of the work, then is how to incorporate that into a branch and bound framework so that we can get very high.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Speed using this exact same model.",
                    "label": 0
                },
                {
                    "sent": "So if we want to do sliding windows, then exhaustive evaluation is going to be very expensive.",
                    "label": 1
                },
                {
                    "sent": "If we want to really evaluate the window at every single possible location, but we don't have to because similar boxes are going to have similar scores.",
                    "label": 1
                },
                {
                    "sent": "We have some geometric information about the structure of the image, so if we see that there's no car in the upper right hand corner of the image, then we don't need to evaluate each of these bounding boxes.",
                    "label": 0
                },
                {
                    "sent": "We can come up with a bound for that entire region and just discard that, because that's not going to have a very high.",
                    "label": 1
                },
                {
                    "sent": "Bound for the scores of boxes that can occur there.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we achieve that?",
                    "label": 0
                },
                {
                    "sent": "It's quite simple, given an upper bound for a an objective function over sets of rectangles.",
                    "label": 1
                },
                {
                    "sent": "So the upper bound is going to provide.",
                    "label": 0
                },
                {
                    "sent": "Upper bound for any possible rectangle that can occur in that set of rectangles.",
                    "label": 0
                },
                {
                    "sent": "We put our set of rectangles as a state into a priority queue.",
                    "label": 1
                },
                {
                    "sent": "We pull the state that has the highest upper bound off the priority queue, split that into two and recalculate our upper bounds, and then once we've gotten to a single element that is a state where there's only one rectangle in that state, then we know we found the optimum.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to do that, we need to specify sets of rectangles.",
                    "label": 1
                },
                {
                    "sent": "We can do this compactly by specifying ranges over the left, top, right and bottom, instead of having a single rectangle, we're going to have intervals for each of these.",
                    "label": 0
                },
                {
                    "sent": "And this gives us the notion that for these intervals where we have a lower bound and upper bound for the location of each of these, the Gray region now is specifying all of the possible rectangles that can be in the set of bounding boxes.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The split it's quite simple.",
                    "label": 0
                },
                {
                    "sent": "We take the side that has the most uncertainty.",
                    "label": 0
                },
                {
                    "sent": "In this case it's the right side and we split it into two intervals.",
                    "label": 0
                },
                {
                    "sent": "The left interval which is shown on the left side and the right interval on the right.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how are we going to then provide a bound for these sets of rectangles?",
                    "label": 0
                },
                {
                    "sent": "So what we do is we bound the weighted sum of kernels.",
                    "label": 1
                },
                {
                    "sent": "Remember we have a kernel for the restriction kernel.",
                    "label": 1
                },
                {
                    "sent": "The local context in the global context we're going to bound that by the sum of bounds for each of these individual kernels.",
                    "label": 1
                },
                {
                    "sent": "So the restriction Colonel we've shown in CPR and Pammy this year.",
                    "label": 0
                },
                {
                    "sent": "How to come up with bounds for that?",
                    "label": 0
                },
                {
                    "sent": "The global context kernel doesn't even depend on why, so it's going to be just a constant value for all possible bounding boxes.",
                    "label": 1
                },
                {
                    "sent": "So the only thing we really need to worry about is how to specify a bound for the local context.",
                    "label": 0
                },
                {
                    "sent": "What we can do is actually propagate uncertainty about the position of the bounding box through our function Theta, and if we do that, that gives us ranges for the value Theta of Y.",
                    "label": 0
                },
                {
                    "sent": "This is actually just standard interval arithmetic.",
                    "label": 1
                },
                {
                    "sent": "If you're familiar with that.",
                    "label": 0
                },
                {
                    "sent": "And using these ranges, that gives us the range of uncertainty that we have for the location of the boundaries of the context kernel.",
                    "label": 0
                },
                {
                    "sent": "For a linear kernel, that means that we can actually come up with an upper bound for the local context.",
                    "label": 0
                },
                {
                    "sent": "Using only four numbers, so this notation at the bottom Theta hat of capital Y, we have a superscript plus in the first term, indicating that we're going to be summing all positive features and an_Max over the maximum possible size box.",
                    "label": 0
                },
                {
                    "sent": "Then we want to sum all negative features over the minimum possible box of this outer bound, and from that we're going to subtract all of the positive features from the minimum possible bounding box and all the negative features from the maximum possible bounding box.",
                    "label": 0
                },
                {
                    "sent": "And this is going to give us an upper bound of the score of all of those features that are going to end up in this strip around the bounding box, and it will converge to the true score as the uncertainty in those intervals goes to 0.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's look at some results.",
                    "label": 0
                },
                {
                    "sent": "Our parameter settings.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are as follows.",
                    "label": 0
                },
                {
                    "sent": "We have Theta set to root 2 / 2.",
                    "label": 1
                },
                {
                    "sent": "Now what that actually gives us is that the contextual region is going to be the same number of pixels as the bounding box.",
                    "label": 0
                },
                {
                    "sent": "It's just something we chose kind of out of a hat we use VLC datasets, surf descriptors that were vector quantized to 3000 features Chi Square kernel for global context and then we use linear bag of word features.",
                    "label": 1
                },
                {
                    "sent": "Linear backwards kernels for the restriction in local context and we chose that because it's very fast to specify the bound.",
                    "label": 0
                },
                {
                    "sent": "Using.",
                    "label": 0
                },
                {
                    "sent": "Using integral images, we can actually do the total bound for both the restriction and the local context kernels, and only I think 4 for look ups.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there's just a few example results.",
                    "label": 0
                },
                {
                    "sent": "The full results tables are available in the paper.",
                    "label": 0
                },
                {
                    "sent": "And this is pretty typical for all of the results.",
                    "label": 0
                },
                {
                    "sent": "What we see is if we use no context, we get significantly worse than using context, either with a fixed waiting or with the learned waiting.",
                    "label": 0
                },
                {
                    "sent": "So with the fixed waiting, what we've done is we've actually traced normalized our kernel matrices and then just sum them up.",
                    "label": 0
                },
                {
                    "sent": "So you can just think of that as kind of normalizing the range of our features and then taking just an average of them so.",
                    "label": 0
                },
                {
                    "sent": "So we haven't done any multiple kernel learning tricks there and then with the learned weights.",
                    "label": 0
                },
                {
                    "sent": "That's the result from multiple kernel learning.",
                    "label": 0
                },
                {
                    "sent": "So in this case we actually do get a pretty consistent improvement from using multiple kernel learning over the fixed weights, and that's because different types of contexts are going to be important for different object categories.",
                    "label": 0
                },
                {
                    "sent": "In the bottom curves, what we have is 2 example classes where we compared to our work from CPR in 2008 and you can see the black curve at the bottom is just going to be the average weights where the blue curve is the restriction kernel.",
                    "label": 0
                },
                {
                    "sent": "Qualitatively, there's not much of a difference between those.",
                    "label": 0
                },
                {
                    "sent": "We haven't seen much of an improvement.",
                    "label": 0
                },
                {
                    "sent": "The pink curve, which is from our CVR paper.",
                    "label": 0
                },
                {
                    "sent": "We actually used something like a global context to re wait the.",
                    "label": 0
                },
                {
                    "sent": "The order of the ranking and then the red curve at the top is the learned weights.",
                    "label": 0
                },
                {
                    "sent": "So you can see that the effect, both global and local context is important.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to conclude, we have a joint kernel framework where you can just throw in global and local context.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Context kernels are actually very helpful if they're learned appropriately.",
                    "label": 1
                },
                {
                    "sent": "If you just wait them arbitrarily, then you probably won't get that much of an improvement.",
                    "label": 0
                },
                {
                    "sent": "Ah.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And by defining local context kernels relative to the bounding box, we can actually still have a very efficient solution because we don't have a very high dimensional parametrization of our branch and bound, we only need to still optimize over 4 variables.",
                    "label": 1
                },
                {
                    "sent": "The top left, bottom, right of the bounding box.",
                    "label": 0
                },
                {
                    "sent": "So I'm ready to answer any questions and I want to give a plug to our special.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Issue of AJC that's coming out over well the deadline is going to be in February so please submit if you want.",
                    "label": 1
                },
                {
                    "sent": "OK thanks.",
                    "label": 0
                },
                {
                    "sent": "Well, well, I think that has to do with how we've defined the local context.",
                    "label": 0
                },
                {
                    "sent": "So because the local context gives us the background, whereas the restriction kernel gives us the actual statistics of the foreground of the object that we're interested in, we're able to say if you have some sort of consistent local context, then we can use that.",
                    "label": 0
                },
                {
                    "sent": "And actually the multiple kernel learning will will determine how much to rely on that local context.",
                    "label": 0
                },
                {
                    "sent": "So if the background statistics aren't somehow consistent around the object of interest, then then we'll just down weight that and not use that at all.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah?",
                    "label": 0
                },
                {
                    "sent": "So if you have, you know a random background then then it will essentially learn not to use it at all.",
                    "label": 0
                },
                {
                    "sent": "But if you have a car that's always on the road as opposed to in the Bush is, then it will be able to make use of that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so actually, that was one of the pieces of work that got me thinking about doing this and what they if it's the paper I'm thinking of, then what they did is actually apply.",
                    "label": 0
                },
                {
                    "sent": "Fixed segmentation ahead of time as a preprocessing step and then based on that segmentation actually pull out kind of relative regions is that the one you're talking about?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so if you do this work then.",
                    "label": 0
                },
                {
                    "sent": "I think that it really probably doesn't make that big of a difference in terms of the actual statistics.",
                    "label": 1
                },
                {
                    "sent": "If you use a rectangular region versus if you use the actual segmentation and so.",
                    "label": 0
                },
                {
                    "sent": "I thought that this would be a way where you could get a lot of those same effects, but then still be able to do a very quick localization.",
                    "label": 0
                },
                {
                    "sent": "So I would say that's really the relationship between the two.",
                    "label": 0
                },
                {
                    "sent": "You mean like a polynomial kernel or something like that, right?",
                    "label": 0
                },
                {
                    "sent": "So in general it's very easy to come up with a bound for many types of kernels.",
                    "label": 0
                },
                {
                    "sent": "So for a polynomial kernel or anything else more or less, you can use what's called interval arithmetic, and this is a standard library of mathematical routines where you say that if you input interval that has an upper and lower bound for a certain value, and then you can provide a lot of mathematical operations.",
                    "label": 0
                },
                {
                    "sent": "Then those mathematical operations will return back an interval itself, which gives an upper and lower bound, and so if you pipe this through your normal code for computing the kernel, then in the end you'll end up with a valid upper bound.",
                    "label": 0
                },
                {
                    "sent": "So there is an automated automatic way to do it.",
                    "label": 0
                },
                {
                    "sent": "Usually if you're more clever about it, you can come up with tighter bounds than what you would get from that, and then you'll therefore get faster convergence.",
                    "label": 0
                },
                {
                    "sent": "So it is.",
                    "label": 0
                },
                {
                    "sent": "There's a bit of art to it still, but it is pretty generally applicable.",
                    "label": 0
                },
                {
                    "sent": "Yes, absolutely, so.",
                    "label": 0
                },
                {
                    "sent": "Your when we computed Theta.",
                    "label": 0
                },
                {
                    "sent": "Actually here we go, yeah, so we would just have a different definition for this function Theta an if you were to actually use the say I just fixed offset so instead of Theta times are minus L you just have L minus Theta T plus Theta.",
                    "label": 0
                },
                {
                    "sent": "Then you'd have a fixed region a fixed number of pixels.",
                    "label": 0
                },
                {
                    "sent": "Then of course you still have your range of uncertainty from your bounding box during the branch and bound scheme.",
                    "label": 0
                },
                {
                    "sent": "And you can propagate that uncertainty through this more simple function, and then that would give you again ranges for that outer boundary.",
                    "label": 0
                },
                {
                    "sent": "And you could use that again.",
                    "label": 0
                },
                {
                    "sent": "I've not done empirical results on whether that works better.",
                    "label": 0
                },
                {
                    "sent": "I suppose it's going to be probably class dependent.",
                    "label": 0
                },
                {
                    "sent": "Which one works better?",
                    "label": 0
                }
            ]
        }
    }
}