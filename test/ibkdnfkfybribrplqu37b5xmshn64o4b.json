{
    "id": "ibkdnfkfybribrplqu37b5xmshn64o4b",
    "title": "An Accelerated Gradient Method for Trace Norm Minimization",
    "info": {
        "author": [
            "Shuiwang Ji, School of Electrical Engineering and Computer Science, Washington State University"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Multi-Task Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_ji_agmt/",
    "segmentation": [
        [
            "Observation.",
            "Fix should be low rank and penalizes rank over Dublin.",
            "And this is controlled by the plant lamp.",
            "Since we know that the rank function is difficult to optimize later, commentarial function so typical."
        ],
        [
            "Approaches to.",
            "Consider the metric W as a product of two matrices, you and we.",
            "And we optimize you and we iteratively.",
            "So this is."
        ],
        [
            "Solution obtained by this way is a locally opting because this is not a convex problem.",
            "So another approach is to relax."
        ],
        [
            "The rank function to other functions, while commonly used relaxation, is dressed now.",
            "So the case number is basically our convex relaxation of the rank function.",
            "So what's that rational for metrics W breaking through the singular value decomposition and the trace norm is just the summation of the singular values.",
            "So did you consider the if you put on the singular values in a vector, this is basically the one of the vector or singular value, so that's why the justification why this paper is printed this decision.",
            "Ella chess now.",
            "To customize the rock function with address now is equivalent to a user open up to approximately 0 now.",
            "Tell you this is Walker."
        ],
        [
            "Consider the general formulation of.",
            "We have our loss and we have our regularization which would be penalized, not doubling.",
            "So in this case, the W is metrics variable and we also make assumptions the F is differentiable and is a great general effect is limited continuous, which is this definition?",
            "Unfortunate thing is that right now is not smooth, that it is not differentiable, so we can not compute the gradient.",
            "So if this will investigate the developed method to.",
            "How to avoid this nothingness?"
        ],
        [
            "So before we start, we consider why other settings rather than metric completion, so we already discussed this metric conversion.",
            "We have this M is observing metric is a low rank matrix.",
            "We want to consider and weigh.",
            "Replace the rank function with face.",
            "Now this is a metric conversion problem, consider the.",
            "Version is the single paper and recently by counties in order to show that if the metrics I am satisfying certain conditions and if it is a low rank by minimizing this function, the exact solution can be recovered by minimizing the personal instead of the rank function and the case number can be used in other settings.",
            "Such as a multitask learning.",
            "We have a multiple tasks and W is a metrics.",
            "Each column correspond to a task.",
            "So for tasks we have uncovered metrics and.",
            "My population for my task is to penalize the rest of doubling.",
            "Another interesting problems is from metric conversion considerably.",
            "To Mecca and back.",
            "In this case, each data point is basically a matrix instead of vector and we.",
            "Apply a weight matrix.",
            "Are the data points and we also want to penalize the train of Dublin.",
            "These are the sample problem which involves not that many others."
        ],
        [
            "So unfortunate thing is Crystal is not smooth so far not most problem.",
            "Well, natural algorithm to solve this problem is apply the subgradient and we know that the.",
            "Have a great day, can be completely lost.",
            "Most functions and this is a basic after step with.",
            "We basically there.",
            "This is similar to gradient descent for smooth problems, only difference is we replace the gradient with the subgrade.",
            "Which we did not have the F drive.",
            "So the convergence rate of the subgradient descent is.",
            "Big ol one over square root which.",
            "Except for this one, in this case, the W star is the optimal solution, so they said if you go on step, the function will be guaranteed to close the optimal solution by a certain amount."
        ],
        [
            "So some remarks are something that isn't there.",
            "Someone theoretical proofs that if you work on.",
            "If you use a blender for not most problems.",
            "This is already the optimal convicted right.",
            "You connect.",
            "This is one over square root OK. Is under the assumption that it is a first order black box.",
            "If you only use a greater information and then you do not exploit the structure of the particular function.",
            "So that means that if you do not.",
            "The structure of this is already the best you can get, so in our work we basically use the special structure of the personal tool.",
            "Actually, the convergence rate."
        ],
        [
            "So in particular, in this work we first develop an extended gradient method which can work as 1 / K. And if I start this algorithm, we further user access acceleration technique to accelerate this algorithm to our place where.",
            "And another room."
        ],
        [
            "Talk about our algorithm is.",
            "The lower case, where is the optimal convergence rate for even for smooth problem.",
            "So by saying This is why we basically said not motion effect of that is not have been totally removed.",
            "OK, probably you get into the translator.",
            "Consider the gradient decent for smooth problems.",
            "So we only we just do not consider the translator and we can."
        ],
        [
            "That's most lost.",
            "We minimize the FWF is smooth.",
            "And we use a.",
            "We can use a gradient descent.",
            "And here we use a gradient instead of subway.",
            "Another view of gradient descent is we can.",
            "Rewrite this.",
            "This turn into this one, so basically.",
            "Then some applications are this a reformulation that reformulation basically considered to pass.",
            "The first one is the linear approximation at the W, K -- 1.",
            "This is the last point.",
            "So let's give us some nutrition why we need to graduate?",
            "Because we at a time point kW K -- 1, we compute the gradient and we do linear approximations that point and we.",
            "Have another tool called regularization.",
            "Basically wanted the WK to be not too far away from W K -- 1 because the linear approximation is only accurate in the local neighborhood or W K -- 1.",
            "So by looking at the greater dissent in another different view, we can basically incorporate the trace now."
        ],
        [
            "So how about her?",
            "How do we deal with the?"
        ],
        [
            "Because the reason why we need the grinder is we want to compute the linear approximation, But if the last most term is not very complicated like that right now, while we just did it directly, so that's our basic intuition.",
            "We just.",
            "Father's master because the producer, linear approximation and the regularization and for the northmost we just put it into directly without any approximation.",
            "So by doing that way, we basically want to exploit the special structure of traditional.",
            "If you can see the other three terms together and that that can be relating as this problem and a is defined basically at the, this is basically if you ignore the translator and this is you go one step by following the gradient of the smoother.",
            "So we basically doing well this kind of problem, so we imagine that if we can solve this problem exactly, the convergence rate should be the same as the graduation because we put the Northwestern directly without any approximation, it won't affect the convergence rate.",
            "The capital that this is true.",
            "It turns out that that some of these are this problem can be solved.",
            "The inexpensive form is electric."
        ],
        [
            "So this is considered.",
            "It is our age is the previous slide.",
            "The this basically said that if the.",
            "Optimal.",
            "See for this problem is we just compute the the.",
            "Optimal doublets, or even see the order fixed.",
            "So we basically compute the singular value decomposition of the matrix tablet and we apply thresholding other similar values.",
            "So previously the.",
            "At this point, the only difference with the.",
            "W is we apply some thresholding, other singular values of the Sigma Lambda which is expressed this way.",
            "So by doing that we can solve the on each step.",
            "We just compute the SVD outfit and we apply social."
        ],
        [
            "So this is our extended gradient method.",
            "We basically we choose.",
            "Hopefully the stepsize SK and we do Granger dissent or the smooth part of the function.",
            "And after that we do some thresholding on the resulting W~ key.",
            "Then some little bit of complications are the.",
            "So started to choose the step size, but I choose to not to discuss the very detailed here because this thing is quite involved and difficult to understand."
        ],
        [
            "So by that time I I just give you some applications.",
            "We basically start from an initial value which is supplied by the user.",
            "How do we decrease it by multiplicative factor, which is less than one until he is satisfied?",
            "And we can prove that if SK is smaller than one hour till sometime.",
            "This is this is the 1 / L L is the latest concept of the function smooth function.",
            "This kid is a smaller than Ceylon.",
            "Our condition is guaranteed to be satisfied.",
            "So at each step we use a.",
            "The step size over the last step as our initial way.",
            "This is basically reduce the number of files and each iteration.",
            "So the reason why we need kind of this step size strategy is."
        ],
        [
            "Approval conversions we need this.",
            "So we have we drink can prove that if we generate the sequence WK by our extended gradient algorithm, we can prove that our convergence rate is basically 1 / K. So the next step is we used."
        ],
        [
            "Acceleration techniques developed by natural.",
            "To accelerate our gradual method.",
            "So the basic idea is the convergence rate of greater distance, which is 1 / K is not the optimal convergence rate for smooth problems.",
            "And next often others developer the extrapolation techniques to accelerate this.",
            "That idea is in great condition we.",
            "WK it doesn't compute the gradient and WK and we do go.",
            "This supposes direction with Richard WK password, either acceleration technique.",
            "We do not do the gradual distant at the current point.",
            "We do interpolation at W K -- 1 and WK and we do gradient descent and some point along this line there are several methods.",
            "To determine the exact point on this line, and we use wild, consider.",
            "This is the point we want to use, denoted as the calculus one, and we do great.",
            "You're decent at this point, we get W. So this is the gradient descent and this is acceleration technique.",
            "So this is our."
        ],
        [
            "Or accelerated gradient method.",
            "The difference with our extended method is.",
            "At this gradient decent step originally we did at the W K -- 1 point and at least in this algorithm we do this at the decay.",
            "They say it is the.",
            "Is a.",
            "The interpolated part.",
            "And I did just that.",
            "We also need to update the coefficient or do the interpolation and we obtain the the K plus one, which is a is a linear combination of the last two approximate solutions, which is WK&W came in as well.",
            "So this is only the.",
            "This is the extra work we need to do compared with the our first algorithm.",
            "Basically we need to maintain the last two histories.",
            "So weak."
        ],
        [
            "Prove that if we generated two sequences instead of 1, which we denoted WK&DKZK is interpolation sequence and WK is approximate solutions.",
            "And if we generate the sequence by following our accelerated gradient method, we can prove the convergence rate our our algorithm is 1 / K ^2.",
            "So we have some remarks about the.",
            "This convergence rate it is approved is a natural book that if the optimal convergence rate of a greater distance for smooth problem is 1 / K ^2.",
            "So for smooth problems if you use black Box 1st order assumption, this is the best you can get."
        ],
        [
            "Power in valuation with Rama.",
            "Multitask formulations and we compared our two algorithm with the multi task feature learning algorithms.",
            "Which is a convex multitask formulation.",
            "We run.",
            "Our ads are.",
            "For the headset with a different percentage is training and it is literal.",
            "Is running time over each of the methods.",
            "We can see that the accelerated gradient method is.",
            "Much more efficient than the extended growth, greater method, and it is also more efficient than the multi task feature learning algorithm."
        ],
        [
            "We also want to see how the acceleration techniques works precisely, so the this is for two datasets.",
            "We brought the convergence of the extended gradient method and the accelerated gradient method.",
            "We can see that.",
            "Especially at the beginning part, the acceleration techniques is quite effective, or both of the datasets."
        ],
        [
            "So in conclusion, in this work we proposed two algorithm to Southwest Normal regularize problem that is extended gradient method and the accelerated version the.",
            "Basically improve the convergence rate of the separator from our case square square root OK to run over, and further accelerate that to our case group.",
            "In future work we want to consider using some approximate SVD techniques to reduce the computational cost, because currently at each iteration we still need to compute one activity, which could be computationally expensive.",
            "And another feature work is currently we consider the traditional regularize problem.",
            "We basically have a smooth last term and present at other classes of problems involved with involved.",
            "Minimize the case number W subject to certain constraints, and we want to adapt our algorithm to this kind of problem.",
            "With that I."
        ],
        [
            "Question."
        ],
        [
            "I.",
            "So this is essentially using nested.",
            "Baby squid.",
            "Animation.",
            "Recently based on this, time is forever.",
            "Normalizations is exactly like that, so we watch the special happening.",
            "Hey, I think you mentioned that is the second part of the paper which deal with how to accelerate the mass of the.",
            "Greater Mesa part is that only works for the standard form.",
            "That only works for smooth problem that our first party to deal with.",
            "How to deal with the non smooth part and the second part is to accelerate that to do the optimal convergence rate.",
            "Yeah yeah yeah yeah.",
            "I also I also aware with the work by little recently that developer how to deal with the Northmost plan, but in his work the differences there are two key differences and like that more details in the paper.",
            "The key differences in his paper he used entire history over the like the WORKW K -- 1.",
            "But in our metrics case if you keep the entire history that could be.",
            "So in our case we already need to keep lost to history as we move on with through our history, except the last two and another one is his method.",
            "He uses 2 projections at each iteration.",
            "In that case we need to compute device with each iteration.",
            "In our method we only use one projection and it's one next week.",
            "Avoiding STD's.",
            "What is pensive?",
            "Whale the point.",
            "As far as I know, it's very difficult to avoid SVD completely in the trace now because the transform involved some of the singular values.",
            "Metrics and many cases, you can just render.",
            "The probabilities in the sparse in the missing value estimation case.",
            "If the original matrix is sparse, we could extend our method to the case that we only compute SVD.",
            "Other sparse work instead of the full version if the matrix is sparse enough, that should work.",
            "I often see.",
            "Singer.",
            "Say that again.",
            "Would be better.",
            "Avoid the SVD.",
            "At the end of the day, you come to it like 1000, STD's also substring singular values, and that's very expensive.",
            "Is.",
            "Before all the desktops that user experience at the inner loop.",
            "Incremental.",
            "Stop stop CCP.",
            "Looking for awhile and take nights or Matlab.",
            "They don't do it.",
            "Go to let it be.",
            "So are there any positions?",
            "OK, so let's think this."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Observation.",
                    "label": 0
                },
                {
                    "sent": "Fix should be low rank and penalizes rank over Dublin.",
                    "label": 0
                },
                {
                    "sent": "And this is controlled by the plant lamp.",
                    "label": 0
                },
                {
                    "sent": "Since we know that the rank function is difficult to optimize later, commentarial function so typical.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approaches to.",
                    "label": 0
                },
                {
                    "sent": "Consider the metric W as a product of two matrices, you and we.",
                    "label": 0
                },
                {
                    "sent": "And we optimize you and we iteratively.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solution obtained by this way is a locally opting because this is not a convex problem.",
                    "label": 0
                },
                {
                    "sent": "So another approach is to relax.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The rank function to other functions, while commonly used relaxation, is dressed now.",
                    "label": 0
                },
                {
                    "sent": "So the case number is basically our convex relaxation of the rank function.",
                    "label": 1
                },
                {
                    "sent": "So what's that rational for metrics W breaking through the singular value decomposition and the trace norm is just the summation of the singular values.",
                    "label": 0
                },
                {
                    "sent": "So did you consider the if you put on the singular values in a vector, this is basically the one of the vector or singular value, so that's why the justification why this paper is printed this decision.",
                    "label": 0
                },
                {
                    "sent": "Ella chess now.",
                    "label": 0
                },
                {
                    "sent": "To customize the rock function with address now is equivalent to a user open up to approximately 0 now.",
                    "label": 0
                },
                {
                    "sent": "Tell you this is Walker.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Consider the general formulation of.",
                    "label": 0
                },
                {
                    "sent": "We have our loss and we have our regularization which would be penalized, not doubling.",
                    "label": 0
                },
                {
                    "sent": "So in this case, the W is metrics variable and we also make assumptions the F is differentiable and is a great general effect is limited continuous, which is this definition?",
                    "label": 1
                },
                {
                    "sent": "Unfortunate thing is that right now is not smooth, that it is not differentiable, so we can not compute the gradient.",
                    "label": 1
                },
                {
                    "sent": "So if this will investigate the developed method to.",
                    "label": 0
                },
                {
                    "sent": "How to avoid this nothingness?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So before we start, we consider why other settings rather than metric completion, so we already discussed this metric conversion.",
                    "label": 0
                },
                {
                    "sent": "We have this M is observing metric is a low rank matrix.",
                    "label": 0
                },
                {
                    "sent": "We want to consider and weigh.",
                    "label": 0
                },
                {
                    "sent": "Replace the rank function with face.",
                    "label": 0
                },
                {
                    "sent": "Now this is a metric conversion problem, consider the.",
                    "label": 0
                },
                {
                    "sent": "Version is the single paper and recently by counties in order to show that if the metrics I am satisfying certain conditions and if it is a low rank by minimizing this function, the exact solution can be recovered by minimizing the personal instead of the rank function and the case number can be used in other settings.",
                    "label": 0
                },
                {
                    "sent": "Such as a multitask learning.",
                    "label": 0
                },
                {
                    "sent": "We have a multiple tasks and W is a metrics.",
                    "label": 0
                },
                {
                    "sent": "Each column correspond to a task.",
                    "label": 0
                },
                {
                    "sent": "So for tasks we have uncovered metrics and.",
                    "label": 0
                },
                {
                    "sent": "My population for my task is to penalize the rest of doubling.",
                    "label": 0
                },
                {
                    "sent": "Another interesting problems is from metric conversion considerably.",
                    "label": 0
                },
                {
                    "sent": "To Mecca and back.",
                    "label": 0
                },
                {
                    "sent": "In this case, each data point is basically a matrix instead of vector and we.",
                    "label": 0
                },
                {
                    "sent": "Apply a weight matrix.",
                    "label": 0
                },
                {
                    "sent": "Are the data points and we also want to penalize the train of Dublin.",
                    "label": 0
                },
                {
                    "sent": "These are the sample problem which involves not that many others.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So unfortunate thing is Crystal is not smooth so far not most problem.",
                    "label": 0
                },
                {
                    "sent": "Well, natural algorithm to solve this problem is apply the subgradient and we know that the.",
                    "label": 1
                },
                {
                    "sent": "Have a great day, can be completely lost.",
                    "label": 0
                },
                {
                    "sent": "Most functions and this is a basic after step with.",
                    "label": 0
                },
                {
                    "sent": "We basically there.",
                    "label": 0
                },
                {
                    "sent": "This is similar to gradient descent for smooth problems, only difference is we replace the gradient with the subgrade.",
                    "label": 0
                },
                {
                    "sent": "Which we did not have the F drive.",
                    "label": 0
                },
                {
                    "sent": "So the convergence rate of the subgradient descent is.",
                    "label": 1
                },
                {
                    "sent": "Big ol one over square root which.",
                    "label": 0
                },
                {
                    "sent": "Except for this one, in this case, the W star is the optimal solution, so they said if you go on step, the function will be guaranteed to close the optimal solution by a certain amount.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So some remarks are something that isn't there.",
                    "label": 0
                },
                {
                    "sent": "Someone theoretical proofs that if you work on.",
                    "label": 0
                },
                {
                    "sent": "If you use a blender for not most problems.",
                    "label": 0
                },
                {
                    "sent": "This is already the optimal convicted right.",
                    "label": 0
                },
                {
                    "sent": "You connect.",
                    "label": 0
                },
                {
                    "sent": "This is one over square root OK. Is under the assumption that it is a first order black box.",
                    "label": 1
                },
                {
                    "sent": "If you only use a greater information and then you do not exploit the structure of the particular function.",
                    "label": 0
                },
                {
                    "sent": "So that means that if you do not.",
                    "label": 0
                },
                {
                    "sent": "The structure of this is already the best you can get, so in our work we basically use the special structure of the personal tool.",
                    "label": 1
                },
                {
                    "sent": "Actually, the convergence rate.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in particular, in this work we first develop an extended gradient method which can work as 1 / K. And if I start this algorithm, we further user access acceleration technique to accelerate this algorithm to our place where.",
                    "label": 0
                },
                {
                    "sent": "And another room.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talk about our algorithm is.",
                    "label": 0
                },
                {
                    "sent": "The lower case, where is the optimal convergence rate for even for smooth problem.",
                    "label": 1
                },
                {
                    "sent": "So by saying This is why we basically said not motion effect of that is not have been totally removed.",
                    "label": 0
                },
                {
                    "sent": "OK, probably you get into the translator.",
                    "label": 1
                },
                {
                    "sent": "Consider the gradient decent for smooth problems.",
                    "label": 0
                },
                {
                    "sent": "So we only we just do not consider the translator and we can.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's most lost.",
                    "label": 0
                },
                {
                    "sent": "We minimize the FWF is smooth.",
                    "label": 0
                },
                {
                    "sent": "And we use a.",
                    "label": 0
                },
                {
                    "sent": "We can use a gradient descent.",
                    "label": 1
                },
                {
                    "sent": "And here we use a gradient instead of subway.",
                    "label": 0
                },
                {
                    "sent": "Another view of gradient descent is we can.",
                    "label": 1
                },
                {
                    "sent": "Rewrite this.",
                    "label": 0
                },
                {
                    "sent": "This turn into this one, so basically.",
                    "label": 0
                },
                {
                    "sent": "Then some applications are this a reformulation that reformulation basically considered to pass.",
                    "label": 1
                },
                {
                    "sent": "The first one is the linear approximation at the W, K -- 1.",
                    "label": 0
                },
                {
                    "sent": "This is the last point.",
                    "label": 0
                },
                {
                    "sent": "So let's give us some nutrition why we need to graduate?",
                    "label": 0
                },
                {
                    "sent": "Because we at a time point kW K -- 1, we compute the gradient and we do linear approximations that point and we.",
                    "label": 0
                },
                {
                    "sent": "Have another tool called regularization.",
                    "label": 0
                },
                {
                    "sent": "Basically wanted the WK to be not too far away from W K -- 1 because the linear approximation is only accurate in the local neighborhood or W K -- 1.",
                    "label": 0
                },
                {
                    "sent": "So by looking at the greater dissent in another different view, we can basically incorporate the trace now.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how about her?",
                    "label": 0
                },
                {
                    "sent": "How do we deal with the?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because the reason why we need the grinder is we want to compute the linear approximation, But if the last most term is not very complicated like that right now, while we just did it directly, so that's our basic intuition.",
                    "label": 0
                },
                {
                    "sent": "We just.",
                    "label": 0
                },
                {
                    "sent": "Father's master because the producer, linear approximation and the regularization and for the northmost we just put it into directly without any approximation.",
                    "label": 1
                },
                {
                    "sent": "So by doing that way, we basically want to exploit the special structure of traditional.",
                    "label": 0
                },
                {
                    "sent": "If you can see the other three terms together and that that can be relating as this problem and a is defined basically at the, this is basically if you ignore the translator and this is you go one step by following the gradient of the smoother.",
                    "label": 0
                },
                {
                    "sent": "So we basically doing well this kind of problem, so we imagine that if we can solve this problem exactly, the convergence rate should be the same as the graduation because we put the Northwestern directly without any approximation, it won't affect the convergence rate.",
                    "label": 0
                },
                {
                    "sent": "The capital that this is true.",
                    "label": 0
                },
                {
                    "sent": "It turns out that that some of these are this problem can be solved.",
                    "label": 1
                },
                {
                    "sent": "The inexpensive form is electric.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is considered.",
                    "label": 0
                },
                {
                    "sent": "It is our age is the previous slide.",
                    "label": 0
                },
                {
                    "sent": "The this basically said that if the.",
                    "label": 0
                },
                {
                    "sent": "Optimal.",
                    "label": 0
                },
                {
                    "sent": "See for this problem is we just compute the the.",
                    "label": 0
                },
                {
                    "sent": "Optimal doublets, or even see the order fixed.",
                    "label": 0
                },
                {
                    "sent": "So we basically compute the singular value decomposition of the matrix tablet and we apply thresholding other similar values.",
                    "label": 0
                },
                {
                    "sent": "So previously the.",
                    "label": 0
                },
                {
                    "sent": "At this point, the only difference with the.",
                    "label": 0
                },
                {
                    "sent": "W is we apply some thresholding, other singular values of the Sigma Lambda which is expressed this way.",
                    "label": 0
                },
                {
                    "sent": "So by doing that we can solve the on each step.",
                    "label": 0
                },
                {
                    "sent": "We just compute the SVD outfit and we apply social.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is our extended gradient method.",
                    "label": 1
                },
                {
                    "sent": "We basically we choose.",
                    "label": 0
                },
                {
                    "sent": "Hopefully the stepsize SK and we do Granger dissent or the smooth part of the function.",
                    "label": 0
                },
                {
                    "sent": "And after that we do some thresholding on the resulting W~ key.",
                    "label": 0
                },
                {
                    "sent": "Then some little bit of complications are the.",
                    "label": 1
                },
                {
                    "sent": "So started to choose the step size, but I choose to not to discuss the very detailed here because this thing is quite involved and difficult to understand.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So by that time I I just give you some applications.",
                    "label": 0
                },
                {
                    "sent": "We basically start from an initial value which is supplied by the user.",
                    "label": 1
                },
                {
                    "sent": "How do we decrease it by multiplicative factor, which is less than one until he is satisfied?",
                    "label": 0
                },
                {
                    "sent": "And we can prove that if SK is smaller than one hour till sometime.",
                    "label": 0
                },
                {
                    "sent": "This is this is the 1 / L L is the latest concept of the function smooth function.",
                    "label": 0
                },
                {
                    "sent": "This kid is a smaller than Ceylon.",
                    "label": 0
                },
                {
                    "sent": "Our condition is guaranteed to be satisfied.",
                    "label": 0
                },
                {
                    "sent": "So at each step we use a.",
                    "label": 1
                },
                {
                    "sent": "The step size over the last step as our initial way.",
                    "label": 0
                },
                {
                    "sent": "This is basically reduce the number of files and each iteration.",
                    "label": 0
                },
                {
                    "sent": "So the reason why we need kind of this step size strategy is.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approval conversions we need this.",
                    "label": 0
                },
                {
                    "sent": "So we have we drink can prove that if we generate the sequence WK by our extended gradient algorithm, we can prove that our convergence rate is basically 1 / K. So the next step is we used.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Acceleration techniques developed by natural.",
                    "label": 0
                },
                {
                    "sent": "To accelerate our gradual method.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea is the convergence rate of greater distance, which is 1 / K is not the optimal convergence rate for smooth problems.",
                    "label": 1
                },
                {
                    "sent": "And next often others developer the extrapolation techniques to accelerate this.",
                    "label": 0
                },
                {
                    "sent": "That idea is in great condition we.",
                    "label": 0
                },
                {
                    "sent": "WK it doesn't compute the gradient and WK and we do go.",
                    "label": 0
                },
                {
                    "sent": "This supposes direction with Richard WK password, either acceleration technique.",
                    "label": 0
                },
                {
                    "sent": "We do not do the gradual distant at the current point.",
                    "label": 0
                },
                {
                    "sent": "We do interpolation at W K -- 1 and WK and we do gradient descent and some point along this line there are several methods.",
                    "label": 0
                },
                {
                    "sent": "To determine the exact point on this line, and we use wild, consider.",
                    "label": 0
                },
                {
                    "sent": "This is the point we want to use, denoted as the calculus one, and we do great.",
                    "label": 0
                },
                {
                    "sent": "You're decent at this point, we get W. So this is the gradient descent and this is acceleration technique.",
                    "label": 0
                },
                {
                    "sent": "So this is our.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or accelerated gradient method.",
                    "label": 0
                },
                {
                    "sent": "The difference with our extended method is.",
                    "label": 0
                },
                {
                    "sent": "At this gradient decent step originally we did at the W K -- 1 point and at least in this algorithm we do this at the decay.",
                    "label": 0
                },
                {
                    "sent": "They say it is the.",
                    "label": 0
                },
                {
                    "sent": "Is a.",
                    "label": 0
                },
                {
                    "sent": "The interpolated part.",
                    "label": 0
                },
                {
                    "sent": "And I did just that.",
                    "label": 0
                },
                {
                    "sent": "We also need to update the coefficient or do the interpolation and we obtain the the K plus one, which is a is a linear combination of the last two approximate solutions, which is WK&W came in as well.",
                    "label": 0
                },
                {
                    "sent": "So this is only the.",
                    "label": 0
                },
                {
                    "sent": "This is the extra work we need to do compared with the our first algorithm.",
                    "label": 0
                },
                {
                    "sent": "Basically we need to maintain the last two histories.",
                    "label": 0
                },
                {
                    "sent": "So weak.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Prove that if we generated two sequences instead of 1, which we denoted WK&DKZK is interpolation sequence and WK is approximate solutions.",
                    "label": 0
                },
                {
                    "sent": "And if we generate the sequence by following our accelerated gradient method, we can prove the convergence rate our our algorithm is 1 / K ^2.",
                    "label": 1
                },
                {
                    "sent": "So we have some remarks about the.",
                    "label": 1
                },
                {
                    "sent": "This convergence rate it is approved is a natural book that if the optimal convergence rate of a greater distance for smooth problem is 1 / K ^2.",
                    "label": 0
                },
                {
                    "sent": "So for smooth problems if you use black Box 1st order assumption, this is the best you can get.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Power in valuation with Rama.",
                    "label": 0
                },
                {
                    "sent": "Multitask formulations and we compared our two algorithm with the multi task feature learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "Which is a convex multitask formulation.",
                    "label": 1
                },
                {
                    "sent": "We run.",
                    "label": 0
                },
                {
                    "sent": "Our ads are.",
                    "label": 1
                },
                {
                    "sent": "For the headset with a different percentage is training and it is literal.",
                    "label": 0
                },
                {
                    "sent": "Is running time over each of the methods.",
                    "label": 0
                },
                {
                    "sent": "We can see that the accelerated gradient method is.",
                    "label": 1
                },
                {
                    "sent": "Much more efficient than the extended growth, greater method, and it is also more efficient than the multi task feature learning algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also want to see how the acceleration techniques works precisely, so the this is for two datasets.",
                    "label": 0
                },
                {
                    "sent": "We brought the convergence of the extended gradient method and the accelerated gradient method.",
                    "label": 0
                },
                {
                    "sent": "We can see that.",
                    "label": 0
                },
                {
                    "sent": "Especially at the beginning part, the acceleration techniques is quite effective, or both of the datasets.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in conclusion, in this work we proposed two algorithm to Southwest Normal regularize problem that is extended gradient method and the accelerated version the.",
                    "label": 0
                },
                {
                    "sent": "Basically improve the convergence rate of the separator from our case square square root OK to run over, and further accelerate that to our case group.",
                    "label": 0
                },
                {
                    "sent": "In future work we want to consider using some approximate SVD techniques to reduce the computational cost, because currently at each iteration we still need to compute one activity, which could be computationally expensive.",
                    "label": 1
                },
                {
                    "sent": "And another feature work is currently we consider the traditional regularize problem.",
                    "label": 0
                },
                {
                    "sent": "We basically have a smooth last term and present at other classes of problems involved with involved.",
                    "label": 0
                },
                {
                    "sent": "Minimize the case number W subject to certain constraints, and we want to adapt our algorithm to this kind of problem.",
                    "label": 0
                },
                {
                    "sent": "With that I.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Question.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially using nested.",
                    "label": 0
                },
                {
                    "sent": "Baby squid.",
                    "label": 0
                },
                {
                    "sent": "Animation.",
                    "label": 0
                },
                {
                    "sent": "Recently based on this, time is forever.",
                    "label": 0
                },
                {
                    "sent": "Normalizations is exactly like that, so we watch the special happening.",
                    "label": 0
                },
                {
                    "sent": "Hey, I think you mentioned that is the second part of the paper which deal with how to accelerate the mass of the.",
                    "label": 0
                },
                {
                    "sent": "Greater Mesa part is that only works for the standard form.",
                    "label": 0
                },
                {
                    "sent": "That only works for smooth problem that our first party to deal with.",
                    "label": 0
                },
                {
                    "sent": "How to deal with the non smooth part and the second part is to accelerate that to do the optimal convergence rate.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "I also I also aware with the work by little recently that developer how to deal with the Northmost plan, but in his work the differences there are two key differences and like that more details in the paper.",
                    "label": 0
                },
                {
                    "sent": "The key differences in his paper he used entire history over the like the WORKW K -- 1.",
                    "label": 0
                },
                {
                    "sent": "But in our metrics case if you keep the entire history that could be.",
                    "label": 0
                },
                {
                    "sent": "So in our case we already need to keep lost to history as we move on with through our history, except the last two and another one is his method.",
                    "label": 0
                },
                {
                    "sent": "He uses 2 projections at each iteration.",
                    "label": 0
                },
                {
                    "sent": "In that case we need to compute device with each iteration.",
                    "label": 0
                },
                {
                    "sent": "In our method we only use one projection and it's one next week.",
                    "label": 0
                },
                {
                    "sent": "Avoiding STD's.",
                    "label": 0
                },
                {
                    "sent": "What is pensive?",
                    "label": 0
                },
                {
                    "sent": "Whale the point.",
                    "label": 0
                },
                {
                    "sent": "As far as I know, it's very difficult to avoid SVD completely in the trace now because the transform involved some of the singular values.",
                    "label": 0
                },
                {
                    "sent": "Metrics and many cases, you can just render.",
                    "label": 0
                },
                {
                    "sent": "The probabilities in the sparse in the missing value estimation case.",
                    "label": 0
                },
                {
                    "sent": "If the original matrix is sparse, we could extend our method to the case that we only compute SVD.",
                    "label": 0
                },
                {
                    "sent": "Other sparse work instead of the full version if the matrix is sparse enough, that should work.",
                    "label": 0
                },
                {
                    "sent": "I often see.",
                    "label": 0
                },
                {
                    "sent": "Singer.",
                    "label": 0
                },
                {
                    "sent": "Say that again.",
                    "label": 0
                },
                {
                    "sent": "Would be better.",
                    "label": 0
                },
                {
                    "sent": "Avoid the SVD.",
                    "label": 0
                },
                {
                    "sent": "At the end of the day, you come to it like 1000, STD's also substring singular values, and that's very expensive.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Before all the desktops that user experience at the inner loop.",
                    "label": 0
                },
                {
                    "sent": "Incremental.",
                    "label": 0
                },
                {
                    "sent": "Stop stop CCP.",
                    "label": 0
                },
                {
                    "sent": "Looking for awhile and take nights or Matlab.",
                    "label": 0
                },
                {
                    "sent": "They don't do it.",
                    "label": 0
                },
                {
                    "sent": "Go to let it be.",
                    "label": 0
                },
                {
                    "sent": "So are there any positions?",
                    "label": 0
                },
                {
                    "sent": "OK, so let's think this.",
                    "label": 0
                }
            ]
        }
    }
}