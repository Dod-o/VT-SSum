{
    "id": "v2wokmluhe4oofkyki5ynrzbjo5tlzoo",
    "title": "Active, Semi-Supervised Learning for Textual Information Access",
    "info": {
        "author": [
            "Anastasia Krithara, Xerox Research Centre Europe, Xerox"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Text Mining",
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning",
            "Top->Computer Science->Information Retrieval"
        ]
    },
    "url": "http://videolectures.net/iiia06_krithara_asslt/",
    "segmentation": [
        [
            "So hello, I'm going to present you the work we have done with this really good must series.",
            "Armenians and surrenders incentive provides an active learning and we're mostly focusing on a text classification.",
            "So at the beginner."
        ],
        [
            "Represent the motivation of our work so.",
            "A lot of research have been made in text classification and manage provides learning methods have been present where the general idea is we take our data set with unlabeled examples.",
            "We annotate a part of them in order to use them as a training set to have a model and then do the classification producer classifier.",
            "But the problem is that during the annotation process it's often costly and time consuming because we need human experts spend time to do it.",
            "And also often we don't have graduate students available to do it so.",
            "So, um.",
            "So I research have been made in order to find solution."
        ],
        [
            "This problem and two main techniques have been introduced with.",
            "This image provides learning and active learning and both tried to solve this problem.",
            "So to reduce the annotation cost but from two different perspectives I'll say just now."
        ],
        [
            "What are the details of this?",
            "Just an outline.",
            "So if my talk I just present the problem will say I give some details of the solutions, then I'll present our methods, which is actually some supervised probabilistic latent semantic analysis.",
            "The experiments we have done till now and then conclusion and future work."
        ],
        [
            "So the idea for those who are not aware of the idea of semi supervised learning is we start with a small small lalabel data set and the huge amount of unlabeled data and we turn our model on the combination of both.",
            "The motivation behind this is to give the unlabeled data can give an idea of the of the distribution of the data and this can help us in many cases, not always."
        ],
        [
            "On the other hand, active learning start within a the same initial settings.",
            "So if you label data set, if you label data and a large amount of labeled datasets an, But then we turn our model or models.",
            "It depends on the method we're using only on the labels set.",
            "When we use this model to test to test with our model, in the unlabeled data.",
            "Then we select the most beautiful example.",
            "Here is the long story of how we select their different methods.",
            "One of them is to select the most ambiguous example or another one is to do use different models on the training.",
            "Different classifiers for example, and then choose the example with the way we have the bigger disagreement between them among them.",
            "Then after choosing so the example we give you give it to the human expert, global it if we put it again the labels.",
            "Said then we restart the procedure.",
            "An and then we stop by there when we reach the performance we want, or after a certain number of queries."
        ],
        [
            "So researchers, they can also step forward, and the combination of both have been having tried out with the first work.",
            "One of the 1st order is Mcaloon Ligon in 98, if I'm not mistaken.",
            "So the setting always the same few labels, a lot of unlabeled.",
            "And now we train our model on the combination of both.",
            "And here's the assembly supervised step.",
            "But from then on is like the active learning part.",
            "So we test our model only to be labeled test set.",
            "We used the example we want the most useful.",
            "We put it in the reliability we put it in the label set and we will recommend the process.",
            "This is this is what we're trying to do.",
            "Also we have take this direction.",
            "But we're trying to apply it in the framework of PSA algorithm Now.",
            "So why you use PSA algorithm?"
        ],
        [
            "So I'll I'll give a few details of how we represent our data to explain why we choose, people say algorithm.",
            "So as the most typical representation would take our documents which active work, and we create a table will have the documents and the terms and we have the occurrences of each term in its document.",
            "So we are entering the currencies of the documents in the world."
        ],
        [
            "The problem we have is the words because words have synonyms.",
            "Many words are politicians.",
            "And this can create a disconnection between topics of the documents and words.",
            "What I mean an example, if we take the world.",
            "If document has the work April inside, we don't know if the topic is fruits or it's computers Max.",
            "So here's where people say comes in and he will say try to give something behind the words something about the topic of the of the document and how he does that."
        ],
        [
            "He does that by introducing Lantern variable, which is the components.",
            "Actually the topics so.",
            "And then we can model.",
            "We're going to model our algorithm using the equation there where actually the we have the profile of the component, the profile of the topic, which is actually the words from which can consist the topic and the topics which are in a document from from which topic segment talks.",
            "But as we said at the beginning, we are interesting what we incentivized by let's say so want to.",
            "Toplice PSA in Sims provides learning.",
            "We will use the framework.",
            "This provides framework with reduced by Gaussian and good last year 2005."
        ],
        [
            "When so really say normally can.",
            "Can be used as we can extend, will say in the supervisor supervised framework easily, but there's a problem that occurs is that as a ratio of labeling, unlabeled data is really low, then many components with no label data appears and that creates the problem of.",
            "That's a problem because then arbitrary probabilities will be assigned to the this component.",
            "And this will also influence our our classification because we have a component with only a label data, then even really uncertain probability will will assign the particular label component and then we cannot see that if it cetera cetera.",
            "So the idea is to introduce another variable with this effect label, let's say 00, and the idea is to.",
            "The label data will keep their label, the real label, and they're labeled.",
            "Data will all get the new label.",
            "So if we see in the binary case, for example, the label examples keep their own labels and all the labels gets into into the new label.",
            "And using these metrics, we can say we can train our model.",
            "Of course, after the training we have on some how to assign these probabilities which are paying for the fake label in the true label, because this is what we are interested in.",
            "So using this equation what we're doing is done, waiting the unlabeled example, which is actually our goal and.",
            "Of course, the influence, but they're not influence.",
            "Are they labeled example?",
            "Which are the more center certain?"
        ],
        [
            "So using this this new variable, our model becomes like this and in order to train all model we use AM algorithm as we have London variables.",
            "They look like a cover of our model is actually the sum of the documents.",
            "Some of the document of all documents are full words of the currency of the occurrences of a word in a document.",
            "At times, the probability that the log of the joint probability of the document, the word and the label of this work, including the fake label."
        ],
        [
            "Here, the EM algorithm and they get in detail who run our EM algorithm and using using the equation I gently before.",
            "We obtain them.",
            "Probabilities of the classification probabilities.",
            "So here is where active learning."
        ],
        [
            "Appears we want to apply active learning on the top of the semi supervised learning, so now all available data have a probability of being in a class.",
            "So which is the most ambiguous example where ambiguous?",
            "Which is the most in the intuitive method with this example, which is highest entropy in the multiclass case in the binary?",
            "This can be simplified the document with probability closes to 05.",
            "With which is all this document will label it.",
            "We put it in the in the set of label examples and we train our model.",
            "So this."
        ],
        [
            "The idea is simple as that.",
            "So we experiment.",
            "We need in order to to test our methods, and it's an ongoing work, so we haven't done much.",
            "We used three binary problems.",
            "Of the 20 newsgroups we try to find 3 examples.",
            "The one more difficult than the other.",
            "So the first one is baseball.",
            "Muskogee PC versus Mac and resume with sophism.",
            "So the first is quite easy, the other moderate, moderate, moderate and the other hard.",
            "In the parenthesis the number of examples of fits.",
            "Office category we use the 80% of the training set as a 80% of the data's training set, from which we used to label examples as initial training.",
            "One of its class and all the others unlabeled and 20% just to test our accuracy of our system."
        ],
        [
            "Hey, the comparison would have done is our method with this supervised learning essential revised policy plus active learning and we test it with the same supervised well Sir plus random query where the idea is we perform the same which provides PSA and then instead of doing active learning we just buy random document from your label set and we label it and as a baseline with the SVM plus the active learning where we choose the example closest to the merge.",
            "But we are mostly interested in the comparison of the two, first becauses the active learning part that we want to see if it helps or not."
        ],
        [
            "So I hear something, some results from the first experiment from the first data set, the black line, the 1st on the top.",
            "It's our algorithm, which is the same supervised prospective learning PSA.",
            "The second is the random one, and the third is the SVM, which is quite normal to start quite low because it uses only two examples at the beginning.",
            "In order to to train.",
            "Aiden."
        ],
        [
            "The second in the second was a bit harder.",
            "We can see a very small difference difference between the difference a bit bigger between the activating and the random."
        ],
        [
            "But it's in the third exam that we can really see.",
            "That the harder the problem the problem is, the more the active learning can help us to the classification, because here the random doesn't perform that well as our method."
        ],
        [
            "Of course.",
            "So of course we have to say that they will propose this method from the first experiments have done, it seems that our method outperformed the semi supervised by let's say alone.",
            "And say something about retain?",
            "Is that the harder the problem is, the more the active learning helps, at least from the experiments we have till now."
        ],
        [
            "So one.",
            "So of course more experiments have to be made different datasets to compare with different methods.",
            "For example, the SVM or different methods of combine also semi supervised and active learning.",
            "And also another idea is to improve our method by using different active learning methods or using our own combined.",
            "More and also have thought of.",
            "Can we talk on different costs?",
            "What I mean is, instead of taking just the ambiguity of an example and what we can do if 2 examples of the same ambiguity, we can choose the shorter one 'cause it would be easier for the for the human to label it than if you have a document of 10 pages.",
            "And it's very straightforward to apply our method on multiclass problems, But this will have to do some experiments, and of course anymore ideas are welcome."
        ],
        [
            "OK, I think that's it.",
            "I don't think much time try to be quick.",
            "Thank you.",
            "OK, we have time for a few questions.",
            "That was the balance between positive and negative glass.",
            "In this diagram switch shown.",
            "And so, in the interest of the number of examples, so they're quite balanced the classes.",
            "OK, One moment slide further.",
            "So the bottom is it says the N plus active learning.",
            "It starts quite bad because it's true that we are using also the unlabeled data is VM doesn't.",
            "It's not TSV.",
            "So starting with just two label data, it's quite hard to start.",
            "Of course you can see that the.",
            "It's not, it's not just random, it's semisupervised PSA blood shadow.",
            "So another experience.",
            "Well intended for very unbalanced data classes.",
            "Sex women you really want to isolate minority minority class very small class this.",
            "This is something that is a disaster.",
            "Measures works very well.",
            "Efficient computation verification.",
            "OK, we haven't tried to unbalanced yet, but probably yes, it's goes was better.",
            "However, formula method, if you would run on 100 thousand 100,000.",
            "A.",
            "This I'm not sure, but the.",
            "Probably it won't change.",
            "Matter probably depends on the data.",
            "If they're quite helpful, because as we know, some supervised learning doesn't help always.",
            "We must have a big big latter assumption and and so it depends if the data set it said quite well, probably will have more.",
            "You have better results otherwise if the data are not good, probably may hurt also.",
            "So it really depends.",
            "The semi supervised learning on all the data also.",
            "The red line?",
            "Yeah, just forget again.",
            "Yeah, you only you don't have not using any unlabeled data.",
            "No, no.",
            "But we want to try also TSM with him but didn't have time for that.",
            "The effect is.",
            "You have been using the unlabeled data.",
            "Even when you have very little label date.",
            "So the kind of the fair comparison this year will be something like this Belkin and all.",
            "Sorry, I think so.",
            "We are this schemes where you basically use galangal data, create A to create a regularizer, sensually that order to modify the kernel.",
            "To to create business essential between instances that they did.",
            "Date.",
            "Yes, very much.",
            "This site comparison.",
            "Yeah, that's true.",
            "Yeah, that's a very good remark.",
            "OK, thank you for their mark.",
            "Yeah, yeah, I think we should try that.",
            "Thank you.",
            "Anymore questions.",
            "Make regular assumption about the test sets will be answering beta being having the same distribution of examples of the unlabeled data that you have.",
            "Hey, yes in general it's a the same from the same data set we just start the 20%, then we take it that as a test.",
            "So yes, the same distribution.",
            "It's a.",
            "Is it effective for it's different?",
            "Yes, in the in general does I think it doesn't work quite well if the distribution on different you need different methods.",
            "Actually do the capture the.",
            "Now find the distribution of the data.",
            "Sorry.",
            "10 minutes to go."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So hello, I'm going to present you the work we have done with this really good must series.",
                    "label": 0
                },
                {
                    "sent": "Armenians and surrenders incentive provides an active learning and we're mostly focusing on a text classification.",
                    "label": 0
                },
                {
                    "sent": "So at the beginner.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Represent the motivation of our work so.",
                    "label": 0
                },
                {
                    "sent": "A lot of research have been made in text classification and manage provides learning methods have been present where the general idea is we take our data set with unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "We annotate a part of them in order to use them as a training set to have a model and then do the classification producer classifier.",
                    "label": 0
                },
                {
                    "sent": "But the problem is that during the annotation process it's often costly and time consuming because we need human experts spend time to do it.",
                    "label": 1
                },
                {
                    "sent": "And also often we don't have graduate students available to do it so.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "So I research have been made in order to find solution.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This problem and two main techniques have been introduced with.",
                    "label": 0
                },
                {
                    "sent": "This image provides learning and active learning and both tried to solve this problem.",
                    "label": 1
                },
                {
                    "sent": "So to reduce the annotation cost but from two different perspectives I'll say just now.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What are the details of this?",
                    "label": 0
                },
                {
                    "sent": "Just an outline.",
                    "label": 0
                },
                {
                    "sent": "So if my talk I just present the problem will say I give some details of the solutions, then I'll present our methods, which is actually some supervised probabilistic latent semantic analysis.",
                    "label": 1
                },
                {
                    "sent": "The experiments we have done till now and then conclusion and future work.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the idea for those who are not aware of the idea of semi supervised learning is we start with a small small lalabel data set and the huge amount of unlabeled data and we turn our model on the combination of both.",
                    "label": 0
                },
                {
                    "sent": "The motivation behind this is to give the unlabeled data can give an idea of the of the distribution of the data and this can help us in many cases, not always.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the other hand, active learning start within a the same initial settings.",
                    "label": 1
                },
                {
                    "sent": "So if you label data set, if you label data and a large amount of labeled datasets an, But then we turn our model or models.",
                    "label": 1
                },
                {
                    "sent": "It depends on the method we're using only on the labels set.",
                    "label": 0
                },
                {
                    "sent": "When we use this model to test to test with our model, in the unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "Then we select the most beautiful example.",
                    "label": 1
                },
                {
                    "sent": "Here is the long story of how we select their different methods.",
                    "label": 0
                },
                {
                    "sent": "One of them is to select the most ambiguous example or another one is to do use different models on the training.",
                    "label": 1
                },
                {
                    "sent": "Different classifiers for example, and then choose the example with the way we have the bigger disagreement between them among them.",
                    "label": 0
                },
                {
                    "sent": "Then after choosing so the example we give you give it to the human expert, global it if we put it again the labels.",
                    "label": 0
                },
                {
                    "sent": "Said then we restart the procedure.",
                    "label": 0
                },
                {
                    "sent": "An and then we stop by there when we reach the performance we want, or after a certain number of queries.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So researchers, they can also step forward, and the combination of both have been having tried out with the first work.",
                    "label": 0
                },
                {
                    "sent": "One of the 1st order is Mcaloon Ligon in 98, if I'm not mistaken.",
                    "label": 0
                },
                {
                    "sent": "So the setting always the same few labels, a lot of unlabeled.",
                    "label": 1
                },
                {
                    "sent": "And now we train our model on the combination of both.",
                    "label": 1
                },
                {
                    "sent": "And here's the assembly supervised step.",
                    "label": 0
                },
                {
                    "sent": "But from then on is like the active learning part.",
                    "label": 1
                },
                {
                    "sent": "So we test our model only to be labeled test set.",
                    "label": 0
                },
                {
                    "sent": "We used the example we want the most useful.",
                    "label": 1
                },
                {
                    "sent": "We put it in the reliability we put it in the label set and we will recommend the process.",
                    "label": 0
                },
                {
                    "sent": "This is this is what we're trying to do.",
                    "label": 0
                },
                {
                    "sent": "Also we have take this direction.",
                    "label": 0
                },
                {
                    "sent": "But we're trying to apply it in the framework of PSA algorithm Now.",
                    "label": 0
                },
                {
                    "sent": "So why you use PSA algorithm?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll I'll give a few details of how we represent our data to explain why we choose, people say algorithm.",
                    "label": 1
                },
                {
                    "sent": "So as the most typical representation would take our documents which active work, and we create a table will have the documents and the terms and we have the occurrences of each term in its document.",
                    "label": 0
                },
                {
                    "sent": "So we are entering the currencies of the documents in the world.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The problem we have is the words because words have synonyms.",
                    "label": 0
                },
                {
                    "sent": "Many words are politicians.",
                    "label": 0
                },
                {
                    "sent": "And this can create a disconnection between topics of the documents and words.",
                    "label": 1
                },
                {
                    "sent": "What I mean an example, if we take the world.",
                    "label": 0
                },
                {
                    "sent": "If document has the work April inside, we don't know if the topic is fruits or it's computers Max.",
                    "label": 1
                },
                {
                    "sent": "So here's where people say comes in and he will say try to give something behind the words something about the topic of the of the document and how he does that.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "He does that by introducing Lantern variable, which is the components.",
                    "label": 1
                },
                {
                    "sent": "Actually the topics so.",
                    "label": 0
                },
                {
                    "sent": "And then we can model.",
                    "label": 0
                },
                {
                    "sent": "We're going to model our algorithm using the equation there where actually the we have the profile of the component, the profile of the topic, which is actually the words from which can consist the topic and the topics which are in a document from from which topic segment talks.",
                    "label": 1
                },
                {
                    "sent": "But as we said at the beginning, we are interesting what we incentivized by let's say so want to.",
                    "label": 0
                },
                {
                    "sent": "Toplice PSA in Sims provides learning.",
                    "label": 0
                },
                {
                    "sent": "We will use the framework.",
                    "label": 0
                },
                {
                    "sent": "This provides framework with reduced by Gaussian and good last year 2005.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "When so really say normally can.",
                    "label": 0
                },
                {
                    "sent": "Can be used as we can extend, will say in the supervisor supervised framework easily, but there's a problem that occurs is that as a ratio of labeling, unlabeled data is really low, then many components with no label data appears and that creates the problem of.",
                    "label": 0
                },
                {
                    "sent": "That's a problem because then arbitrary probabilities will be assigned to the this component.",
                    "label": 1
                },
                {
                    "sent": "And this will also influence our our classification because we have a component with only a label data, then even really uncertain probability will will assign the particular label component and then we cannot see that if it cetera cetera.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to introduce another variable with this effect label, let's say 00, and the idea is to.",
                    "label": 1
                },
                {
                    "sent": "The label data will keep their label, the real label, and they're labeled.",
                    "label": 0
                },
                {
                    "sent": "Data will all get the new label.",
                    "label": 0
                },
                {
                    "sent": "So if we see in the binary case, for example, the label examples keep their own labels and all the labels gets into into the new label.",
                    "label": 1
                },
                {
                    "sent": "And using these metrics, we can say we can train our model.",
                    "label": 0
                },
                {
                    "sent": "Of course, after the training we have on some how to assign these probabilities which are paying for the fake label in the true label, because this is what we are interested in.",
                    "label": 0
                },
                {
                    "sent": "So using this equation what we're doing is done, waiting the unlabeled example, which is actually our goal and.",
                    "label": 0
                },
                {
                    "sent": "Of course, the influence, but they're not influence.",
                    "label": 0
                },
                {
                    "sent": "Are they labeled example?",
                    "label": 0
                },
                {
                    "sent": "Which are the more center certain?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So using this this new variable, our model becomes like this and in order to train all model we use AM algorithm as we have London variables.",
                    "label": 1
                },
                {
                    "sent": "They look like a cover of our model is actually the sum of the documents.",
                    "label": 1
                },
                {
                    "sent": "Some of the document of all documents are full words of the currency of the occurrences of a word in a document.",
                    "label": 0
                },
                {
                    "sent": "At times, the probability that the log of the joint probability of the document, the word and the label of this work, including the fake label.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, the EM algorithm and they get in detail who run our EM algorithm and using using the equation I gently before.",
                    "label": 0
                },
                {
                    "sent": "We obtain them.",
                    "label": 0
                },
                {
                    "sent": "Probabilities of the classification probabilities.",
                    "label": 0
                },
                {
                    "sent": "So here is where active learning.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Appears we want to apply active learning on the top of the semi supervised learning, so now all available data have a probability of being in a class.",
                    "label": 1
                },
                {
                    "sent": "So which is the most ambiguous example where ambiguous?",
                    "label": 1
                },
                {
                    "sent": "Which is the most in the intuitive method with this example, which is highest entropy in the multiclass case in the binary?",
                    "label": 0
                },
                {
                    "sent": "This can be simplified the document with probability closes to 05.",
                    "label": 0
                },
                {
                    "sent": "With which is all this document will label it.",
                    "label": 0
                },
                {
                    "sent": "We put it in the in the set of label examples and we train our model.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The idea is simple as that.",
                    "label": 0
                },
                {
                    "sent": "So we experiment.",
                    "label": 0
                },
                {
                    "sent": "We need in order to to test our methods, and it's an ongoing work, so we haven't done much.",
                    "label": 0
                },
                {
                    "sent": "We used three binary problems.",
                    "label": 1
                },
                {
                    "sent": "Of the 20 newsgroups we try to find 3 examples.",
                    "label": 0
                },
                {
                    "sent": "The one more difficult than the other.",
                    "label": 0
                },
                {
                    "sent": "So the first one is baseball.",
                    "label": 0
                },
                {
                    "sent": "Muskogee PC versus Mac and resume with sophism.",
                    "label": 1
                },
                {
                    "sent": "So the first is quite easy, the other moderate, moderate, moderate and the other hard.",
                    "label": 1
                },
                {
                    "sent": "In the parenthesis the number of examples of fits.",
                    "label": 0
                },
                {
                    "sent": "Office category we use the 80% of the training set as a 80% of the data's training set, from which we used to label examples as initial training.",
                    "label": 1
                },
                {
                    "sent": "One of its class and all the others unlabeled and 20% just to test our accuracy of our system.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hey, the comparison would have done is our method with this supervised learning essential revised policy plus active learning and we test it with the same supervised well Sir plus random query where the idea is we perform the same which provides PSA and then instead of doing active learning we just buy random document from your label set and we label it and as a baseline with the SVM plus the active learning where we choose the example closest to the merge.",
                    "label": 0
                },
                {
                    "sent": "But we are mostly interested in the comparison of the two, first becauses the active learning part that we want to see if it helps or not.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I hear something, some results from the first experiment from the first data set, the black line, the 1st on the top.",
                    "label": 0
                },
                {
                    "sent": "It's our algorithm, which is the same supervised prospective learning PSA.",
                    "label": 0
                },
                {
                    "sent": "The second is the random one, and the third is the SVM, which is quite normal to start quite low because it uses only two examples at the beginning.",
                    "label": 0
                },
                {
                    "sent": "In order to to train.",
                    "label": 0
                },
                {
                    "sent": "Aiden.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second in the second was a bit harder.",
                    "label": 0
                },
                {
                    "sent": "We can see a very small difference difference between the difference a bit bigger between the activating and the random.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But it's in the third exam that we can really see.",
                    "label": 0
                },
                {
                    "sent": "That the harder the problem the problem is, the more the active learning can help us to the classification, because here the random doesn't perform that well as our method.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "So of course we have to say that they will propose this method from the first experiments have done, it seems that our method outperformed the semi supervised by let's say alone.",
                    "label": 0
                },
                {
                    "sent": "And say something about retain?",
                    "label": 0
                },
                {
                    "sent": "Is that the harder the problem is, the more the active learning helps, at least from the experiments we have till now.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one.",
                    "label": 0
                },
                {
                    "sent": "So of course more experiments have to be made different datasets to compare with different methods.",
                    "label": 1
                },
                {
                    "sent": "For example, the SVM or different methods of combine also semi supervised and active learning.",
                    "label": 1
                },
                {
                    "sent": "And also another idea is to improve our method by using different active learning methods or using our own combined.",
                    "label": 1
                },
                {
                    "sent": "More and also have thought of.",
                    "label": 0
                },
                {
                    "sent": "Can we talk on different costs?",
                    "label": 0
                },
                {
                    "sent": "What I mean is, instead of taking just the ambiguity of an example and what we can do if 2 examples of the same ambiguity, we can choose the shorter one 'cause it would be easier for the for the human to label it than if you have a document of 10 pages.",
                    "label": 0
                },
                {
                    "sent": "And it's very straightforward to apply our method on multiclass problems, But this will have to do some experiments, and of course anymore ideas are welcome.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I think that's it.",
                    "label": 0
                },
                {
                    "sent": "I don't think much time try to be quick.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, we have time for a few questions.",
                    "label": 0
                },
                {
                    "sent": "That was the balance between positive and negative glass.",
                    "label": 0
                },
                {
                    "sent": "In this diagram switch shown.",
                    "label": 0
                },
                {
                    "sent": "And so, in the interest of the number of examples, so they're quite balanced the classes.",
                    "label": 0
                },
                {
                    "sent": "OK, One moment slide further.",
                    "label": 0
                },
                {
                    "sent": "So the bottom is it says the N plus active learning.",
                    "label": 0
                },
                {
                    "sent": "It starts quite bad because it's true that we are using also the unlabeled data is VM doesn't.",
                    "label": 0
                },
                {
                    "sent": "It's not TSV.",
                    "label": 0
                },
                {
                    "sent": "So starting with just two label data, it's quite hard to start.",
                    "label": 0
                },
                {
                    "sent": "Of course you can see that the.",
                    "label": 0
                },
                {
                    "sent": "It's not, it's not just random, it's semisupervised PSA blood shadow.",
                    "label": 0
                },
                {
                    "sent": "So another experience.",
                    "label": 0
                },
                {
                    "sent": "Well intended for very unbalanced data classes.",
                    "label": 0
                },
                {
                    "sent": "Sex women you really want to isolate minority minority class very small class this.",
                    "label": 0
                },
                {
                    "sent": "This is something that is a disaster.",
                    "label": 0
                },
                {
                    "sent": "Measures works very well.",
                    "label": 0
                },
                {
                    "sent": "Efficient computation verification.",
                    "label": 0
                },
                {
                    "sent": "OK, we haven't tried to unbalanced yet, but probably yes, it's goes was better.",
                    "label": 0
                },
                {
                    "sent": "However, formula method, if you would run on 100 thousand 100,000.",
                    "label": 0
                },
                {
                    "sent": "A.",
                    "label": 0
                },
                {
                    "sent": "This I'm not sure, but the.",
                    "label": 0
                },
                {
                    "sent": "Probably it won't change.",
                    "label": 0
                },
                {
                    "sent": "Matter probably depends on the data.",
                    "label": 0
                },
                {
                    "sent": "If they're quite helpful, because as we know, some supervised learning doesn't help always.",
                    "label": 0
                },
                {
                    "sent": "We must have a big big latter assumption and and so it depends if the data set it said quite well, probably will have more.",
                    "label": 0
                },
                {
                    "sent": "You have better results otherwise if the data are not good, probably may hurt also.",
                    "label": 0
                },
                {
                    "sent": "So it really depends.",
                    "label": 0
                },
                {
                    "sent": "The semi supervised learning on all the data also.",
                    "label": 0
                },
                {
                    "sent": "The red line?",
                    "label": 0
                },
                {
                    "sent": "Yeah, just forget again.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you only you don't have not using any unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "No, no.",
                    "label": 0
                },
                {
                    "sent": "But we want to try also TSM with him but didn't have time for that.",
                    "label": 0
                },
                {
                    "sent": "The effect is.",
                    "label": 0
                },
                {
                    "sent": "You have been using the unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Even when you have very little label date.",
                    "label": 0
                },
                {
                    "sent": "So the kind of the fair comparison this year will be something like this Belkin and all.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I think so.",
                    "label": 0
                },
                {
                    "sent": "We are this schemes where you basically use galangal data, create A to create a regularizer, sensually that order to modify the kernel.",
                    "label": 0
                },
                {
                    "sent": "To to create business essential between instances that they did.",
                    "label": 0
                },
                {
                    "sent": "Date.",
                    "label": 0
                },
                {
                    "sent": "Yes, very much.",
                    "label": 0
                },
                {
                    "sent": "This site comparison.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's true.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a very good remark.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you for their mark.",
                    "label": 1
                },
                {
                    "sent": "Yeah, yeah, I think we should try that.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "Make regular assumption about the test sets will be answering beta being having the same distribution of examples of the unlabeled data that you have.",
                    "label": 0
                },
                {
                    "sent": "Hey, yes in general it's a the same from the same data set we just start the 20%, then we take it that as a test.",
                    "label": 0
                },
                {
                    "sent": "So yes, the same distribution.",
                    "label": 0
                },
                {
                    "sent": "It's a.",
                    "label": 0
                },
                {
                    "sent": "Is it effective for it's different?",
                    "label": 0
                },
                {
                    "sent": "Yes, in the in general does I think it doesn't work quite well if the distribution on different you need different methods.",
                    "label": 0
                },
                {
                    "sent": "Actually do the capture the.",
                    "label": 0
                },
                {
                    "sent": "Now find the distribution of the data.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "10 minutes to go.",
                    "label": 0
                }
            ]
        }
    }
}