{
    "id": "avsflcg6msxt7wj5x44rblauvflkml5k",
    "title": "GADGET SVM: a Gossip-bAseD sub-GradiEnT SVM solver",
    "info": {
        "author": [
            "Chase Hensel, Department of Computer Science, Columbia University"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/icml09_hensel_gbg/",
    "segmentation": [
        [
            "So I'm going to be talking about a distributed SVM solver called Gadget SVM."
        ],
        [
            "And, uh, I guess basically what?",
            "There's a few."
        ],
        [
            "Let's see."
        ],
        [
            "I'm not going to go too quickly through what SVM, just 'cause of lack of time and everything else, and you pretty much know it.",
            "But SVM will give us the maximum."
        ],
        [
            "Hyperplane, and typically we have this setting where it's just kind of regularization term and a sum of loss parameters in typical SVM loss."
        ],
        [
            "But now we're looking at a distributed setting which actually isn't master slave like other things have been mentioned early."
        ],
        [
            "Today, but is in more of a, you know it's uncoordinated.",
            "So basically now we have a bunch of nodes where each node has a different set of data right?",
            "And so basically you can see it's similar to the traditional SVM setting except now that we just have a sum of all the local node data and we're trying together to solve group problem where all the notes of their different data.",
            "This is different than the standard standard formulation because we don't have a bias parameter and we don't work with kernels.",
            "There's two reasons for that, which basically sorry what ends up happening is the bias parameter ruins strong convexity, which will be very important for the algorithm, and also kernels will talk about later are pretty much the bane of anyone's existence.",
            "Who's trying to do this kind of distributed SVM solving?",
            "Because it effectively sent they effectively centralized your data."
        ],
        [
            "OK, so why is this setting interesting?",
            "It's just it's common we have, you know, this type of distributed setting comes up in Federated databases, PDP networks, cell phones, anything, any sort of wireless network.",
            "It's this type of thing."
        ],
        [
            "Come up the C and it's also pretty hard because we want to minimize communication between nodes.",
            "We want to work on different types of graphs that show up, you know, and then also graphs can change overtime, which is interesting in learning because we have all of this problems of graphs can be unstable and we don't have these kind of average guarantees, especially with hardware."
        ],
        [
            "Failures and everything else.",
            "OK, so there's a few solutions to distributed SPM right now.",
            "But they're not too great, so one of the main ideas is to basically compute local SVM and all your different nodes and then send your support vectors back to a central machine, which then computes SVM again and then send your support vectors back to the nodes, and so on and so forth.",
            "And the problem with that is you can think you can construct trivial examples without centralized your data, or say every example is a support vector, then all of a sudden what you're effectively doing is sending all of your data to a central computer, which isn't that good if you're assuming your data spread out in the first place.",
            "And you didn't have to send it out.",
            "And also the problem with that is you have you have master slave Sir.",
            "Inherently imposing some sort of structure which doesn't really work if you're going to work on a robust network and then the other idea is take a specific topology and then design an algorithm for that.",
            "These two one of them works and kind of one of them works in a tree, which is this cascade SVM which I guess came out a couple years ago and any other one works on Akowski graph, which are fine, but they kind of suffer from the same problems that if you want something that works in practice you don't want to work on specific.",
            "Topology, you don't want it to work on something that's very structured."
        ],
        [
            "So what we're going to use, we decided to use gossip, which is kind of an interesting idea for these algorithms, because basically what we're doing is very simple message passing protocols, where each node only talks to their neighbors.",
            "So what you're going to do is from a high level, what will do is it's going to be a conjugate gradient method, but during each step we're going to use.",
            "During each iteration, will use a little bit of gossip, and will use that gossip between neighbors and then will converge together as a global solution, which very much is a different flavor from everything else that's out there in terms of distributed.",
            "EM algorithms."
        ],
        [
            "OK so I."
        ],
        [
            "Now now we have this distributed SPM equation again and you can see it strongly convex because basically the loss terms are there Hessian their generalization because it's not differentiable is never negative.",
            "It's at most zero and then the passion of the first part is going to be Lambda.",
            "So this is nice because it basically ensures that the equations well behaved and it will be useful in a couple of seconds.",
            "And what you should just take out of looking at the fact that it's strongly convex and it's actually pretty simple because the only constraint is if the project is."
        ],
        [
            "The constraint that will use is that you you can project onto a ball, but it's simple, so solving as a black box is kind of a bad idea, and especially in the distributed setting where you really want to do things as simple as possible and you really want to send the messages as possible, it's bad idea to look at this black box."
        ],
        [
            "So instead we."
        ],
        [
            "Have these type this."
        ],
        [
            "Of algorithm."
        ],
        [
            "Jesse."
        ],
        [
            "This is."
        ],
        [
            "Totally different than say.",
            "Here's a black box.",
            "It's like this is your conjugate gradient and it's just it's built into the SVM equation.",
            "So what we do is the first thing is figure out at each.",
            "This is this is what each node does.",
            "So you start out each node, figures out which examples you start out with a basic margin, which is going to be just the zero vector, and then each node figures out which examples they screwed up on, and then they compute some sort of local subgradient.",
            "And I kind of glossed over that fact, but that's the way this is, so it's not a strictly conjugate gradient method, it's a subgradient projected method which is kind of the advancing of this, which is the idea of a subgradient, which I don't really have time to get into, but it's the it's the tangent line.",
            "As opposed to your gradient.",
            "All that really means for us is that it's these methods end up being more robust and they have interesting convergence properties, but they're just as simple to work with any sort of straightforward conjuring gradient method.",
            "Anyway, so each local node computes a local account gradient and then or subgrade.",
            "Excuse me and then they send that out using a gossip protocol to each of their other neighbors and everything else and basically what ends up everyone does is they converge to an approximate network wide subgradient step which is.",
            "So if you can see this right here.",
            "Basically what it does is if you remove this and said hey, we're just in a centralized location, this would be a standard subgradient method.",
            "But now all of a sudden all the nodes are working together during.",
            "Nation of Conjur gradient.",
            "So then after they each node gets kind of an approximate subgradient, if you will.",
            "They then use that with the.",
            "Use that to update their local weight vectors, which is this next step, which is kind of very much akin with what would be a what would be a subgradient method, except now this is all being done in distributed setting.",
            "And then afterwards they have to have this another call to push some which is again our message passing protocol which we'll talk about in a few minutes.",
            "But that is I'll talk about why that's done in a few minutes also, but basically from a high level all it does is it just ensures stability of the algorithm.",
            "And then finally, this is the final step.",
            "Here really is the fact this is our only constraint which what we're doing is we're projecting everything onto this.",
            "We're projecting W onto this ball, which is the ball of radius square root, one over Lambda, or one over square root of Lambda.",
            "Excuse me, and the reason why we do that, is it because we can?",
            "Basically, if you look at the SVM equation you can prove it and what it ends up doing, it ends up giving you nice numerical properties."
        ],
        [
            "OK, so now the kind of the."
        ],
        [
            "Trick hidden in this is does this work right?",
            "'cause all of a sudden we're having we're not using true subgradients or not, you know, and everything is kind of approximate and hand WAVY and you have that see."
        ],
        [
            "This is going to converge and will work with gossip because gossip itself is, well, it's robust, you know it's giving you error that has gives you close relative errors for everything, but it doesn't give you exact solutions.",
            "I'm not going to get into the convergence, but it does end up working, but basically so this is a push.",
            "Some protocol is.",
            "This is on the next page, but basically what it's going to do is it's going to simulate in the wake of supports.",
            "It simulates a random walk, deterministic random walk on network.",
            "So basically what you would say is that if you have if you look at this instead as a Markov chain as a Markov model, what you're looking at is you're looking at your mixing time for till when it comes with steady state distribution, because if each node is talking to its neighbors you know it's talking to 1/3 of the time it's talking to one neighbor, another neighbor, so a for example.",
            "Talking to EB&C so it's.",
            "It's waiting, you know, three times what it should in terms of its contribution to overall system.",
            "So if you divide its contributions by a third, everything ends up normal, everything is."
        ],
        [
            "Converging OK, so this is just this is our.",
            "Very simple gossip protocol which is used for a specific number of rounds.",
            "It's very it's fixed when you actually there's a way to check for the end, but basically to your neighbors you send out your local weight vectors."
        ],
        [
            "Which in this case you can see was your loss in this case, and in this case just your current solution."
        ],
        [
            "And then you send those out to your neighbors.",
            "You get some information from your neighbors and you repeat.",
            "And then all of a sudden you have."
        ],
        [
            "Updates great, and as I mentioned before, there's these two calls to push some.",
            "First one gives you your approximate global subgradient and the second one ensures that all of your local weight vectors are close.",
            "Now this is important, 'cause when you think about it, if you didn't have the second one, you could start diverging during each step and by the time you got the time T all of your local weight vectors could be very far apart.",
            "You need some sort of way of ensuring that everyone's on the same track."
        ],
        [
            "Just, uh, this is kind of.",
            "This is part of the convergence ends up being kind of messy, and there's also this."
        ],
        [
            "Hidden is this hidden thing here, which you don't really notice I guess, which is.",
            "We're not.",
            "The output solution isn't one of these WS, it's actually.",
            "It's actually what's it called their average because of the fact that you use convexity properties.",
            "The trick of subgradient methods is you always want to update output.",
            "The best of your WS at any iteration you don't want output, you don't want to output the last one because you're not guaranteed to continually converge to the best solution.",
            "So, but using what's it called using convexity?",
            "And why am I blanking on this using convexity and?",
            "What's the yeah, just the convexity condition?",
            "You know that there at the average is better than any of the individual values.",
            "It's a it's yeah, but basic."
        ],
        [
            "Really, it's a it's a cheap way without ever checking OK. Is this W hat the best one so far out?",
            "Putting your average will do the same thing.",
            "Yeah, I'm gonna remember."
        ],
        [
            "Name in a second it's but anyway, so This is why this works.",
            "So if you imagine here your average if you divide everything over here by T and then you get your get your average.",
            "Basically this all converge is nice and this all becomes an epsilon on the right hand side."
        ],
        [
            "You get nice convergence."
        ],
        [
            "But what I was going to highlight is just this is that you kind of have these two additional in the standard projected subgradient methods.",
            "For SVM you would get this first loss term.",
            "Yeah, OK, you get, you get this first term, but now you have these two additional terms due to network communication and they really do break down to this is your loss due to the first call.",
            "This is your lawsuit is the 2nd call and it becomes actually nice."
        ],
        [
            "When you do the analysis.",
            "OK, so then I guess here's just a brief you kind of the bottom line of the running time in the message complexity, which is basically that Emacs is the mixing time of your of your graph.",
            "And but then it runs, you know, proportional to one over Lambda epsilon, which is kind of interesting, because what that means is.",
            "It means that it depends on your regularization parameter as opposed to and only linearly."
        ],
        [
            "On your examples.",
            "Which is.",
            "Which is kind of I don't know.",
            "It's just it's unique characteristic."
        ],
        [
            "The steps, algorithms and the other kind of interesting thing about this is that it only runs on.",
            "It only runs in time, worst, compareable to the worst.",
            "The number the biggest number examples of any node, which is kind of interesting as well because it means it's pretty highly parallel when you."
        ],
        [
            "We break everything down."
        ],
        [
            "OK, so then we ran experimental results on the Enron email data set."
        ],
        [
            "And we compared it to a bunch of centralized SVM algorithm.",
            "A distributed SVM algorithm which is in the vein of compute local support vectors, send it back to a master and then repeat and basically what you end up seeing is that."
        ],
        [
            "This is just, you know, we did CPU time, which isn't that accurate 'cause it was on a simulated network, but the message complexity is kind of interesting 'cause it's just very well defined in terms of something where you know these heuristic methods of computing local support vectors aren't necessarily straightforward as to what you're going to get.",
            "You know it's not like a there's no nice."
        ],
        [
            "Analysis of it.",
            "Where is this?"
        ],
        [
            "Everything."
        ],
        [
            "Works out really well and I."
        ],
        [
            "Probably the most telling experiment was probably this one, which was if you fix network size and then increase the number of examples at any node.",
            "Message complexity is constant right?",
            "Because it doesn't depend.",
            "It depends on the number of iterations in your accuracy.",
            "It doesn't depend on the number of examples at any node.",
            "Which to me was kind of interesting because effectively what you could do is you could keep if your number examples at each node increased, the problem would just increase for running time in terms of how much each node had to spend, but your message complexities fix."
        ],
        [
            "OK, so I guess so some.",
            "I guess some future challenges with all this is, you know incorporating bias parameter, which is kind of a hard thing which it's just 'cause it ruins the strong convexity with strong convexity, which is so necessary and then another another few things are dynamic networks like what if nodes are entering and leaving which seems to be a very hard challenge and.",
            "Yeah, and then I guess different subgradient steps, because this one, for example, runs in one over Lambda epsilon.",
            "But then there are other steps which you could do which conceivably don't matter on strong convexity but run in one over epsilon squared.",
            "And there's all sorts of different types of which are actually small changes to algorithms but have drastic effects on basically runtime and everything else."
        ],
        [
            "And you know another thing which I mentioned is just.",
            "Mercer kernels are absolutely impossible because in this type of algorithm, in order to compute the Mercer kernel you need to take your data times its transpose, which effectively centralizes everything, because all of the nodes have to talk to everyone else, which which you know I don't have a good solution for, but it seems like a very interesting problem, because it seems like we would have to do some sort of approximation for the kernel during each step, and I just I don't really know how you would do it."
        ],
        [
            "Well as questions.",
            "Some kind of stuff so that.",
            "Or does it actually attacked the nice thing about these stored sorts of gossip algorithms is that ultimately you can use unstructured networks.",
            "So you could do something like this that you don't need Masters.",
            "The problem is a lot of networks aren't built this way.",
            "Networks of structure clearly, but this could be something that you could do that like an ad hoc network, for example, right with this type of algorithm, and I think.",
            "You can apply this to something like Gnutella or something like that.",
            "You know we're using that sort of standard peer to peer network architecture and that would work.",
            "I haven't done it yet, but I think that's at least in terms of practical implementation.",
            "That seems like the next kind of way to go because.",
            "You, I think you would have to do something that's probably more peer to peer and you know not necessarily fix structure, but has.",
            "Yeah, I guess supernode is probably the right.",
            "Yeah.",
            "Other questions.",
            "Normal."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm going to be talking about a distributed SVM solver called Gadget SVM.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And, uh, I guess basically what?",
                    "label": 0
                },
                {
                    "sent": "There's a few.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's see.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm not going to go too quickly through what SVM, just 'cause of lack of time and everything else, and you pretty much know it.",
                    "label": 0
                },
                {
                    "sent": "But SVM will give us the maximum.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hyperplane, and typically we have this setting where it's just kind of regularization term and a sum of loss parameters in typical SVM loss.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But now we're looking at a distributed setting which actually isn't master slave like other things have been mentioned early.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Today, but is in more of a, you know it's uncoordinated.",
                    "label": 0
                },
                {
                    "sent": "So basically now we have a bunch of nodes where each node has a different set of data right?",
                    "label": 1
                },
                {
                    "sent": "And so basically you can see it's similar to the traditional SVM setting except now that we just have a sum of all the local node data and we're trying together to solve group problem where all the notes of their different data.",
                    "label": 0
                },
                {
                    "sent": "This is different than the standard standard formulation because we don't have a bias parameter and we don't work with kernels.",
                    "label": 0
                },
                {
                    "sent": "There's two reasons for that, which basically sorry what ends up happening is the bias parameter ruins strong convexity, which will be very important for the algorithm, and also kernels will talk about later are pretty much the bane of anyone's existence.",
                    "label": 1
                },
                {
                    "sent": "Who's trying to do this kind of distributed SVM solving?",
                    "label": 0
                },
                {
                    "sent": "Because it effectively sent they effectively centralized your data.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so why is this setting interesting?",
                    "label": 1
                },
                {
                    "sent": "It's just it's common we have, you know, this type of distributed setting comes up in Federated databases, PDP networks, cell phones, anything, any sort of wireless network.",
                    "label": 1
                },
                {
                    "sent": "It's this type of thing.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Come up the C and it's also pretty hard because we want to minimize communication between nodes.",
                    "label": 0
                },
                {
                    "sent": "We want to work on different types of graphs that show up, you know, and then also graphs can change overtime, which is interesting in learning because we have all of this problems of graphs can be unstable and we don't have these kind of average guarantees, especially with hardware.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Failures and everything else.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a few solutions to distributed SPM right now.",
                    "label": 1
                },
                {
                    "sent": "But they're not too great, so one of the main ideas is to basically compute local SVM and all your different nodes and then send your support vectors back to a central machine, which then computes SVM again and then send your support vectors back to the nodes, and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "And the problem with that is you can think you can construct trivial examples without centralized your data, or say every example is a support vector, then all of a sudden what you're effectively doing is sending all of your data to a central computer, which isn't that good if you're assuming your data spread out in the first place.",
                    "label": 0
                },
                {
                    "sent": "And you didn't have to send it out.",
                    "label": 0
                },
                {
                    "sent": "And also the problem with that is you have you have master slave Sir.",
                    "label": 0
                },
                {
                    "sent": "Inherently imposing some sort of structure which doesn't really work if you're going to work on a robust network and then the other idea is take a specific topology and then design an algorithm for that.",
                    "label": 1
                },
                {
                    "sent": "These two one of them works and kind of one of them works in a tree, which is this cascade SVM which I guess came out a couple years ago and any other one works on Akowski graph, which are fine, but they kind of suffer from the same problems that if you want something that works in practice you don't want to work on specific.",
                    "label": 0
                },
                {
                    "sent": "Topology, you don't want it to work on something that's very structured.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we're going to use, we decided to use gossip, which is kind of an interesting idea for these algorithms, because basically what we're doing is very simple message passing protocols, where each node only talks to their neighbors.",
                    "label": 1
                },
                {
                    "sent": "So what you're going to do is from a high level, what will do is it's going to be a conjugate gradient method, but during each step we're going to use.",
                    "label": 0
                },
                {
                    "sent": "During each iteration, will use a little bit of gossip, and will use that gossip between neighbors and then will converge together as a global solution, which very much is a different flavor from everything else that's out there in terms of distributed.",
                    "label": 0
                },
                {
                    "sent": "EM algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so I.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now now we have this distributed SPM equation again and you can see it strongly convex because basically the loss terms are there Hessian their generalization because it's not differentiable is never negative.",
                    "label": 0
                },
                {
                    "sent": "It's at most zero and then the passion of the first part is going to be Lambda.",
                    "label": 0
                },
                {
                    "sent": "So this is nice because it basically ensures that the equations well behaved and it will be useful in a couple of seconds.",
                    "label": 0
                },
                {
                    "sent": "And what you should just take out of looking at the fact that it's strongly convex and it's actually pretty simple because the only constraint is if the project is.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The constraint that will use is that you you can project onto a ball, but it's simple, so solving as a black box is kind of a bad idea, and especially in the distributed setting where you really want to do things as simple as possible and you really want to send the messages as possible, it's bad idea to look at this black box.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So instead we.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have these type this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Jesse.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Totally different than say.",
                    "label": 0
                },
                {
                    "sent": "Here's a black box.",
                    "label": 0
                },
                {
                    "sent": "It's like this is your conjugate gradient and it's just it's built into the SVM equation.",
                    "label": 0
                },
                {
                    "sent": "So what we do is the first thing is figure out at each.",
                    "label": 0
                },
                {
                    "sent": "This is this is what each node does.",
                    "label": 0
                },
                {
                    "sent": "So you start out each node, figures out which examples you start out with a basic margin, which is going to be just the zero vector, and then each node figures out which examples they screwed up on, and then they compute some sort of local subgradient.",
                    "label": 0
                },
                {
                    "sent": "And I kind of glossed over that fact, but that's the way this is, so it's not a strictly conjugate gradient method, it's a subgradient projected method which is kind of the advancing of this, which is the idea of a subgradient, which I don't really have time to get into, but it's the it's the tangent line.",
                    "label": 0
                },
                {
                    "sent": "As opposed to your gradient.",
                    "label": 0
                },
                {
                    "sent": "All that really means for us is that it's these methods end up being more robust and they have interesting convergence properties, but they're just as simple to work with any sort of straightforward conjuring gradient method.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so each local node computes a local account gradient and then or subgrade.",
                    "label": 0
                },
                {
                    "sent": "Excuse me and then they send that out using a gossip protocol to each of their other neighbors and everything else and basically what ends up everyone does is they converge to an approximate network wide subgradient step which is.",
                    "label": 0
                },
                {
                    "sent": "So if you can see this right here.",
                    "label": 0
                },
                {
                    "sent": "Basically what it does is if you remove this and said hey, we're just in a centralized location, this would be a standard subgradient method.",
                    "label": 0
                },
                {
                    "sent": "But now all of a sudden all the nodes are working together during.",
                    "label": 0
                },
                {
                    "sent": "Nation of Conjur gradient.",
                    "label": 0
                },
                {
                    "sent": "So then after they each node gets kind of an approximate subgradient, if you will.",
                    "label": 0
                },
                {
                    "sent": "They then use that with the.",
                    "label": 0
                },
                {
                    "sent": "Use that to update their local weight vectors, which is this next step, which is kind of very much akin with what would be a what would be a subgradient method, except now this is all being done in distributed setting.",
                    "label": 0
                },
                {
                    "sent": "And then afterwards they have to have this another call to push some which is again our message passing protocol which we'll talk about in a few minutes.",
                    "label": 0
                },
                {
                    "sent": "But that is I'll talk about why that's done in a few minutes also, but basically from a high level all it does is it just ensures stability of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "And then finally, this is the final step.",
                    "label": 0
                },
                {
                    "sent": "Here really is the fact this is our only constraint which what we're doing is we're projecting everything onto this.",
                    "label": 0
                },
                {
                    "sent": "We're projecting W onto this ball, which is the ball of radius square root, one over Lambda, or one over square root of Lambda.",
                    "label": 0
                },
                {
                    "sent": "Excuse me, and the reason why we do that, is it because we can?",
                    "label": 0
                },
                {
                    "sent": "Basically, if you look at the SVM equation you can prove it and what it ends up doing, it ends up giving you nice numerical properties.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now the kind of the.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trick hidden in this is does this work right?",
                    "label": 0
                },
                {
                    "sent": "'cause all of a sudden we're having we're not using true subgradients or not, you know, and everything is kind of approximate and hand WAVY and you have that see.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is going to converge and will work with gossip because gossip itself is, well, it's robust, you know it's giving you error that has gives you close relative errors for everything, but it doesn't give you exact solutions.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to get into the convergence, but it does end up working, but basically so this is a push.",
                    "label": 0
                },
                {
                    "sent": "Some protocol is.",
                    "label": 0
                },
                {
                    "sent": "This is on the next page, but basically what it's going to do is it's going to simulate in the wake of supports.",
                    "label": 0
                },
                {
                    "sent": "It simulates a random walk, deterministic random walk on network.",
                    "label": 0
                },
                {
                    "sent": "So basically what you would say is that if you have if you look at this instead as a Markov chain as a Markov model, what you're looking at is you're looking at your mixing time for till when it comes with steady state distribution, because if each node is talking to its neighbors you know it's talking to 1/3 of the time it's talking to one neighbor, another neighbor, so a for example.",
                    "label": 0
                },
                {
                    "sent": "Talking to EB&C so it's.",
                    "label": 0
                },
                {
                    "sent": "It's waiting, you know, three times what it should in terms of its contribution to overall system.",
                    "label": 0
                },
                {
                    "sent": "So if you divide its contributions by a third, everything ends up normal, everything is.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Converging OK, so this is just this is our.",
                    "label": 0
                },
                {
                    "sent": "Very simple gossip protocol which is used for a specific number of rounds.",
                    "label": 0
                },
                {
                    "sent": "It's very it's fixed when you actually there's a way to check for the end, but basically to your neighbors you send out your local weight vectors.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which in this case you can see was your loss in this case, and in this case just your current solution.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then you send those out to your neighbors.",
                    "label": 0
                },
                {
                    "sent": "You get some information from your neighbors and you repeat.",
                    "label": 0
                },
                {
                    "sent": "And then all of a sudden you have.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Updates great, and as I mentioned before, there's these two calls to push some.",
                    "label": 0
                },
                {
                    "sent": "First one gives you your approximate global subgradient and the second one ensures that all of your local weight vectors are close.",
                    "label": 0
                },
                {
                    "sent": "Now this is important, 'cause when you think about it, if you didn't have the second one, you could start diverging during each step and by the time you got the time T all of your local weight vectors could be very far apart.",
                    "label": 0
                },
                {
                    "sent": "You need some sort of way of ensuring that everyone's on the same track.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just, uh, this is kind of.",
                    "label": 0
                },
                {
                    "sent": "This is part of the convergence ends up being kind of messy, and there's also this.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hidden is this hidden thing here, which you don't really notice I guess, which is.",
                    "label": 0
                },
                {
                    "sent": "We're not.",
                    "label": 0
                },
                {
                    "sent": "The output solution isn't one of these WS, it's actually.",
                    "label": 0
                },
                {
                    "sent": "It's actually what's it called their average because of the fact that you use convexity properties.",
                    "label": 0
                },
                {
                    "sent": "The trick of subgradient methods is you always want to update output.",
                    "label": 0
                },
                {
                    "sent": "The best of your WS at any iteration you don't want output, you don't want to output the last one because you're not guaranteed to continually converge to the best solution.",
                    "label": 0
                },
                {
                    "sent": "So, but using what's it called using convexity?",
                    "label": 0
                },
                {
                    "sent": "And why am I blanking on this using convexity and?",
                    "label": 0
                },
                {
                    "sent": "What's the yeah, just the convexity condition?",
                    "label": 0
                },
                {
                    "sent": "You know that there at the average is better than any of the individual values.",
                    "label": 0
                },
                {
                    "sent": "It's a it's yeah, but basic.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really, it's a it's a cheap way without ever checking OK. Is this W hat the best one so far out?",
                    "label": 0
                },
                {
                    "sent": "Putting your average will do the same thing.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'm gonna remember.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Name in a second it's but anyway, so This is why this works.",
                    "label": 0
                },
                {
                    "sent": "So if you imagine here your average if you divide everything over here by T and then you get your get your average.",
                    "label": 0
                },
                {
                    "sent": "Basically this all converge is nice and this all becomes an epsilon on the right hand side.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You get nice convergence.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But what I was going to highlight is just this is that you kind of have these two additional in the standard projected subgradient methods.",
                    "label": 0
                },
                {
                    "sent": "For SVM you would get this first loss term.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, you get, you get this first term, but now you have these two additional terms due to network communication and they really do break down to this is your loss due to the first call.",
                    "label": 0
                },
                {
                    "sent": "This is your lawsuit is the 2nd call and it becomes actually nice.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When you do the analysis.",
                    "label": 0
                },
                {
                    "sent": "OK, so then I guess here's just a brief you kind of the bottom line of the running time in the message complexity, which is basically that Emacs is the mixing time of your of your graph.",
                    "label": 0
                },
                {
                    "sent": "And but then it runs, you know, proportional to one over Lambda epsilon, which is kind of interesting, because what that means is.",
                    "label": 0
                },
                {
                    "sent": "It means that it depends on your regularization parameter as opposed to and only linearly.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On your examples.",
                    "label": 0
                },
                {
                    "sent": "Which is.",
                    "label": 0
                },
                {
                    "sent": "Which is kind of I don't know.",
                    "label": 0
                },
                {
                    "sent": "It's just it's unique characteristic.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The steps, algorithms and the other kind of interesting thing about this is that it only runs on.",
                    "label": 0
                },
                {
                    "sent": "It only runs in time, worst, compareable to the worst.",
                    "label": 0
                },
                {
                    "sent": "The number the biggest number examples of any node, which is kind of interesting as well because it means it's pretty highly parallel when you.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We break everything down.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so then we ran experimental results on the Enron email data set.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we compared it to a bunch of centralized SVM algorithm.",
                    "label": 0
                },
                {
                    "sent": "A distributed SVM algorithm which is in the vein of compute local support vectors, send it back to a master and then repeat and basically what you end up seeing is that.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is just, you know, we did CPU time, which isn't that accurate 'cause it was on a simulated network, but the message complexity is kind of interesting 'cause it's just very well defined in terms of something where you know these heuristic methods of computing local support vectors aren't necessarily straightforward as to what you're going to get.",
                    "label": 0
                },
                {
                    "sent": "You know it's not like a there's no nice.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Analysis of it.",
                    "label": 0
                },
                {
                    "sent": "Where is this?",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Everything.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Works out really well and I.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Probably the most telling experiment was probably this one, which was if you fix network size and then increase the number of examples at any node.",
                    "label": 0
                },
                {
                    "sent": "Message complexity is constant right?",
                    "label": 0
                },
                {
                    "sent": "Because it doesn't depend.",
                    "label": 0
                },
                {
                    "sent": "It depends on the number of iterations in your accuracy.",
                    "label": 0
                },
                {
                    "sent": "It doesn't depend on the number of examples at any node.",
                    "label": 0
                },
                {
                    "sent": "Which to me was kind of interesting because effectively what you could do is you could keep if your number examples at each node increased, the problem would just increase for running time in terms of how much each node had to spend, but your message complexities fix.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I guess so some.",
                    "label": 0
                },
                {
                    "sent": "I guess some future challenges with all this is, you know incorporating bias parameter, which is kind of a hard thing which it's just 'cause it ruins the strong convexity with strong convexity, which is so necessary and then another another few things are dynamic networks like what if nodes are entering and leaving which seems to be a very hard challenge and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and then I guess different subgradient steps, because this one, for example, runs in one over Lambda epsilon.",
                    "label": 0
                },
                {
                    "sent": "But then there are other steps which you could do which conceivably don't matter on strong convexity but run in one over epsilon squared.",
                    "label": 0
                },
                {
                    "sent": "And there's all sorts of different types of which are actually small changes to algorithms but have drastic effects on basically runtime and everything else.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you know another thing which I mentioned is just.",
                    "label": 0
                },
                {
                    "sent": "Mercer kernels are absolutely impossible because in this type of algorithm, in order to compute the Mercer kernel you need to take your data times its transpose, which effectively centralizes everything, because all of the nodes have to talk to everyone else, which which you know I don't have a good solution for, but it seems like a very interesting problem, because it seems like we would have to do some sort of approximation for the kernel during each step, and I just I don't really know how you would do it.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well as questions.",
                    "label": 0
                },
                {
                    "sent": "Some kind of stuff so that.",
                    "label": 0
                },
                {
                    "sent": "Or does it actually attacked the nice thing about these stored sorts of gossip algorithms is that ultimately you can use unstructured networks.",
                    "label": 0
                },
                {
                    "sent": "So you could do something like this that you don't need Masters.",
                    "label": 0
                },
                {
                    "sent": "The problem is a lot of networks aren't built this way.",
                    "label": 0
                },
                {
                    "sent": "Networks of structure clearly, but this could be something that you could do that like an ad hoc network, for example, right with this type of algorithm, and I think.",
                    "label": 0
                },
                {
                    "sent": "You can apply this to something like Gnutella or something like that.",
                    "label": 0
                },
                {
                    "sent": "You know we're using that sort of standard peer to peer network architecture and that would work.",
                    "label": 0
                },
                {
                    "sent": "I haven't done it yet, but I think that's at least in terms of practical implementation.",
                    "label": 0
                },
                {
                    "sent": "That seems like the next kind of way to go because.",
                    "label": 0
                },
                {
                    "sent": "You, I think you would have to do something that's probably more peer to peer and you know not necessarily fix structure, but has.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I guess supernode is probably the right.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "Normal.",
                    "label": 0
                }
            ]
        }
    }
}