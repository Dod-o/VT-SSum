{
    "id": "2rdgfdukeigil43bzuycejrzapscrpvl",
    "title": "Representation and Reasoning with Universal Schema Embeddings",
    "info": {
        "author": [
            "Andrew McCallum, Department of Computer Science, University of Massachusetts Amherst"
        ],
        "published": "Jan. 11, 2016",
        "recorded": "October 2015",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2015_mccallum_universal_schema/",
    "segmentation": [
        [
            "So much for the warm welcome, so I'm new to your conference into your community where neighbors, but I haven't been to visit before.",
            "My apologies, so I arrive with a bit of ignorance about your house and its history and its plans, so I hope you will be patient with me and I hope that you'll find a little something useful in what I what I have to say.",
            "Let me also say from the start that I welcome your questions during my talk.",
            "I'd be very happy to have those.",
            "A few weeks ago I gave an invited talk at an information retrieval conference.",
            "Extreme of about this size, but we still managed to have a dialogue during the talk and I enjoyed it very much, so please don't hesitate to speak up.",
            "Alright, so."
        ],
        [
            "Let's get started.",
            "So some companies, which in their early years focused on web search.",
            "Now."
        ],
        [
            "That people are spending less time.",
            "In front of their desktops and more."
        ],
        [
            "Time in front of screens like this are instead."
        ],
        [
            "Or not instead, but in addition."
        ],
        [
            "Focused quite a bit on dialogue and on question, answering from knowledge bases so that rather than returning a list."
        ],
        [
            "Links an answer to a question we can use this structured knowledge of."
        ],
        [
            "To give more precise and concise answers."
        ],
        [
            "Like this?"
        ],
        [
            "And also like this.",
            "So this is a great thing for your community, right?",
            "There is a very large body of people interested in building knowledge bases and the semantic web like you.",
            "And.",
            "Freebase is one such example.",
            "Very big.",
            "It's a bit of an odd thing in a way for a company used to doing web search today."
        ],
        [
            "Because web search is well known for having a very long tail of queries and have interests.",
            "And if we define.",
            "In a small narrow schema then.",
            "We may not have good coverage and so this graph shows the long tail of queries were really the most of the mass of queries are are out in that in the long tail of quite detailed or unusual things, and so this incense.",
            "Them to build Freebase to be very large and to have as high coverage as possible, but this is a difficult thing to do and to keep it manageable.",
            "And it's both hard to manage and hard."
        ],
        [
            "Have good coverage.",
            "So for example, we might be interested in who did Jeff Bezos criticize?",
            "Well, Freebase has no criticized relationship.",
            "So this talk is asking the question, how can we preserve the great diversity and openness?",
            "We get from textual search on the web."
        ],
        [
            "Kind of open schema and by open I really mean almost like driven by raw text and yet at the same time have a sense of an entity relational structure and and do reasoning on top of this structure."
        ],
        [
            "But this talk is about.",
            "So first let's launch in by talking."
        ],
        [
            "Bit about.",
            "How to build knowledge basis from text, which is something that I've been thinking about and working on for quite some time.",
            "Back in the mid 90s with some grass."
        ],
        [
            "Two things at Carnegie Mellon.",
            "We built this system which spider the web for research papers and information extraction to get the title of the authors.",
            "The abstracts built the reference graph essentially was than earlier versions of Google Scholar.",
            "Before Google Scholar existed, and this is one small example of a knowledge base that we built early on."
        ],
        [
            "When I arrived at UMass, we built this system, which also in addition to people."
        ],
        [
            "In papers, knows about institutions and conferences and journals and grants and advisors.",
            "All this first class to cross referenced objects, and this is a step along the way towards one of our."
        ],
        [
            "Location goals, which is to build a knowledge base of all scientists in the world by extracting and integrating information from papers and patents webpages.",
            "Newswire press releases.",
            "Even tweets and blogs and to get not only established scientists but also the newly arrived first year graduate students and to understand their career trajectories and so on.",
            "We'd like to build these better tools to accelerate the progress of science.",
            "To help people find papers to read and to site to help people find reviewers, collaborators, Hieros and also to understand the trends and landscape of science.",
            "And also, incidentally, in this space, we've become interested in building platforms for a new model of publishing and peer review, and gotten quite interested in open peer review.",
            "I learned recently that your own semantic web Journal practices open peer review, in which the public at large gets to see these reviews.",
            "We've been building systems for some new conf."
        ],
        [
            "This is that.",
            "In addition, also allow the public at large to contribute reviews in addition to the assigned reviewers, and have had some great experiences and if any of you are interested in talking with me about this, or about using a system like this and some of your own new venues, I would love to talk about that.",
            "But"
        ],
        [
            "But I digress.",
            "So here are some examples of knowledge bases that we've been working on.",
            "I want to talk."
        ],
        [
            "Now a little bit about.",
            "What it looks like to build a knowledge base from text and some of the steps involved in some of the research that we've been doing along the way."
        ],
        [
            "So we start with some text."
        ],
        [
            "And the first thing we want to do is find some mentions of the entities involved, like Whaley or Xinhua University by by finding the boundaries within text, extracting these and also knowing what the types are we wanted."
        ],
        [
            "Relation extraction again from the evidence in the text by looking at the text that surrounds the appearance of these mentions to extract relations like Whaleys affiliated machine while University.",
            "Then, in addition to the."
        ],
        [
            "Textual input data.",
            "We may also have some additional structured data input and in both."
        ],
        [
            "Cases there may be instances of the same entities that are described in different places in slightly different ways, so we need to resolve these or disambiguate these against each other so we know walian W via the same person.",
            "And this then gives us what we think of is the truth."
        ],
        [
            "About entities and relations, which we then inject into the knowledge base from which we can answer questions.",
            "And we've been using machine learning for all of these things so.",
            "One among multiple issues here."
        ],
        [
            "Is that well?",
            "Information extraction components aren't perfect and they make mistakes and they always will, and these errors tend to snowball, right?",
            "You make an error early in the pipeline and it's very difficult to recover from it later and.",
            "And even moderately high accuracy accuracy for each one of the components when they're strung together.",
            "It turns into low accuracy for the pipeline as a whole.",
            "So with this in mind."
        ],
        [
            "We and others have become interested in.",
            "Reasoning with."
        ],
        [
            "Certainty through this pipeline, probabilities on all of these things, multiple hypothesis reasoning about the confidence of them and."
        ],
        [
            "Performing joint inference to allow both top down and bottom up reasoning to come into play here to help us.",
            "Apply reasoning from constraints from later parts of the tasks to earlier parts of the pipeline in order to help us resolve some of these uncertainties, and we and others have shown that this kind of joint inference or probabilistic reasoning and large scale graphical model can indeed absolutely help accuracy quite a."
        ],
        [
            "Here are a few examples.",
            "Some of these, the majority of these are doing joint inference on what I think of as a budding tasks early in the information extraction pipeline joint part of speech tagging and shallow parsing.",
            "For example, reasoning together about the data that appears together in just one sentence.",
            "But I think that there is even more.",
            "Juice.",
            "More benefit to be gained by reasoning jointly about things that happen at the end of the processing pipeline, because it's there that we bring together more evidence, more data from.",
            "Bring to bear that we can integrate together.",
            "And Furthermore, that we'd like to inject this uncertainty all the way into the knowledge base as well, and use the knowledge base to help us do a more accurate job of interpreting the text.",
            "And this brings up lots of really interesting.",
            "Difficult questions."
        ],
        [
            "So like how do we represent uncertainty and inject that from extraction into the knowledge base?",
            "And how should we use the knowledge base contents to help information extraction?",
            "And then Furthermore, this last one seems a little bit mechanical, but in some ways it's I feel the most important information extraction isn't one shot.",
            "It's not like I'm going to receive.",
            "The 500 million documents in which are going to run once up front and then I'm finished.",
            "I'm going to have all those documents, but then tomorrow I'm going to receive another 10 million documents and.",
            "The evidence of those new documents maybe sometimes should cause me to change my mind about some of the conclusions that I came to about how to interpret the previous text, and I would like to be able to go back and visit those decisions and make them more accurate, but I want to do that without having to redo inference across the entire input data.",
            "From the beginning.",
            "So how can I?",
            "Store those previous decisions and go back and revisit them in an efficient way.",
            "So in a way I'd like the knowledge base infrastructure, not just to store the end result of this extraction pipeline, but also help me to manage its intermediate steps along the way.",
            "So with that in mind."
        ],
        [
            "We have been thinking about enlarging the purview of the knowledge base to look something more like."
        ],
        [
            "Yes.",
            "In which the Knowledge Base Store is not just the truth, but all the intermediate results and the only thing you're allowed to inject into the knowledge base is the original raw evidence.",
            "And there's some sense in which the truth discovery happens inside the knowledge base."
        ],
        [
            "I think of this as very similar to epistemological."
        ],
        [
            "Philosophy which says you don't get to observe the truth right.",
            "You just observe the evidence and it's up to your own internal reasoning to discover the truth.",
            "So we've taken to calling this."
        ],
        [
            "Analogical knowledge basis.",
            "And we might think that.",
            "In a way."
        ],
        [
            "What have I really done here?",
            "But I sort of moved some boundaries on my boxes and arrows diagram.",
            "And what does this mean really?",
            "And that's true in a way, but thinking about the problem in this way has given us some new insights and lent itself to having us think about solving some problems in some new ways, and I'd like to give you a brief tour of some of those right now before getting into some more technical details on the main body of my talk.",
            "So."
        ],
        [
            "One example is how we deal with human edits.",
            "So rather than a human edit consisting of reaching into the knowledge base or racing the old notion of the truth and writing a new version with maybe some edit history for Providence instead, we treat a human evidence as another piece of."
        ],
        [
            "Evidence.",
            "It's like a new mini document that says well on this state.",
            "Natasha said this thing was true and we can reason about that evidence alongside the rest of the evidence and reason simultaneously."
        ],
        [
            "Out.",
            "Truth discovery, as well as Natasha's reliability and her expertise on this particular topic and this can help us manage cases in which humans are wrong or they disagree.",
            "Or maybe human makes a correct edit.",
            "But then later this data the world changes.",
            "The humans don't notice, but extraction does notice and extraction should be allowed to overwrite the human as well.",
            "So we've been doing some work in this area."
        ],
        [
            "Example is never ending inference or rather than doing extraction once up front, we can think of inferences constantly bub."
        ],
        [
            "Thing in the background inside this knowledge base and.",
            "And the truth is, always reconsidered with more evidence and more time."
        ],
        [
            "And, uh."
        ],
        [
            "That we've been thinking a lot about is that is about resolution and not just for coreference of entity mentions, but also for aligning values and ontologies and schemas and relations and events.",
            "Actually, the kind of thing that exactly this community I know I think about a lot.",
            "And yeah, we've found that machine learning methods for doing this and methods for doing looking at all of these alignments jointly together at the same time can really vary.",
            "Be very helpful."
        ],
        [
            "We"
        ],
        [
            "Also, I've been thinking about resource bounded information gathering.",
            "You have a knowledge base, it has some wholesome missing blanks.",
            "You know that that information is out there on the web somewhere.",
            "We can't afford to process the entire web.",
            "Maybe Google could, but we can't, so we'd like in some targeted way to go out and issue some queries to get us just exactly those pages we think are most likely to help us fill these blanks."
        ],
        [
            "And.",
            "And to reason about this as a resource constrained task, overtime it would be using reinforcement learning techniques to do this."
        ],
        [
            "And then Lastly, if we."
        ],
        [
            "To do this at large scale, we have to think about this with parallel distributed resources, and so we've been doing so."
        ],
        [
            "Work on smart parallelism rather than using black box kind of techniques like map reduce methods that reason in a very intertwined way about.",
            "Parallel distribution and about the particular inference problem to be solved, and I don't have time to get into detail here, but one example is a method that does large scale entity resolution that reason simultaneously about solving the resolution task and also how to distribute the data across parallel distributed resources for maximum efficiency where these inference tasks are very intertwined."
        ],
        [
            "Alright, so."
        ],
        [
            "That's a brief tour and."
        ],
        [
            "Here's my plan for the rest of the talk.",
            "I know we've talked about motivation and open schema and epistemological knowledge bases, and now I want to move on to the."
        ],
        [
            "The title here.",
            "This thing that we call Universal Schema.",
            "And."
        ],
        [
            "To get into this, I want to focus first on the relation extraction aspect that we talked about earlier.",
            "And I want to start by stepping back and talking about how is this task done.",
            "Traditionally, usually in natural language processing."
        ],
        [
            "So the traditional method is a supervised learning task, right?",
            "Some humans."
        ],
        [
            "Down and design some schema of relation types that we care about and then goes to some."
        ],
        [
            "Unlabeled text and has a human do the task of labeling this text to find instances of where these relations are expressed in text.",
            "And we train some machine learning model on this label date."
        ],
        [
            "Such that on test data, hopefully the automated."
        ],
        [
            "Nothing can predict successfully where these relations exist."
        ],
        [
            "Don't get good accuracy.",
            "Labeling all this data is a really big pain.",
            "It takes a lot of labeled data to do a good job, and so it would be nice to be a bit more clever.",
            "So kind of method that's been doing."
        ],
        [
            "Best in recent NIST Tech CBP knowledge based population competition tasks is distant supervision in which we assume that we're."
        ],
        [
            "Given a knowledge base that already has a schema, that's like the one that we want, and we're also given a bunch of."
        ],
        [
            "Labeled text.",
            "Whose contents overlap with the contents of the knowledge base we have already, and we."
        ],
        [
            "For cases where entries in the knowledge base have been rendered in unstructured text.",
            "Align those with each other.",
            "That alignment effectively labels the text, and then we then treat those labelings as truth.",
            "Train some machine."
        ],
        [
            "Learning method on that pseudo labeled text.",
            "Run that on some."
        ],
        [
            "Some additional new text.",
            "Find new instance."
        ],
        [
            "Simulations to further populate the knowledge base.",
            "And this is very nice and it works well.",
            "Except that finding these alignments can be brittle and can wander.",
            "Into bad patterns.",
            "Um?",
            "And it also requires that we already have a knowledge base with the schema that we care about.",
            "So with this in mind, there have been some people have been thinking about a radically different approach, which is, well, let's."
        ],
        [
            "Let's just throw away the schema completely an we have no schema and use an unsupervised method.",
            "And here I'm referring to kind of work that's been done quite a bit of University of Washington, which they call open information extraction."
        ],
        [
            "And here we take the unlabeled text we run."
        ],
        [
            "A dependency parser on it, or some cheap approximation to a dependency parser to find the verb."
        ],
        [
            "And."
        ],
        [
            "Their arguments and we take the verb string and we'll say."
        ],
        [
            "That is the relation and then here are its arguments, and then we're done.",
            "This is the relation, and so the string itself is the relation type.",
            "This is very nice and has grand diversity over everything that you could say in text.",
            "And the issue is that we start to know how this generalizes, right?",
            "So here we see the attends relation.",
            "We might."
        ],
        [
            "Also see the affiliated relation, but we have no idea that these two relations are somehow there's some implicature that should be between these, because there are different strings and doesn't know how they relate.",
            "So this might make us think about continuing."
        ],
        [
            "In this unsupervised vein, but.",
            "Doing some schema discovery automatically, perhaps by some clustering, and so we and others."
        ],
        [
            "Done work in this area as well and you get clusters that look something like this, so we see things like relation 3.",
            "Somebody authored a paper.",
            "They wrote a paper.",
            "They published a paper.",
            "They were the Co author of a paper and so on.",
            "So yes, these mean the same things.",
            "This is a certain type of relation and there's some mutual implicature here.",
            "And that's very nice.",
            "But all too often there are also things like relation one.",
            "Where someone is affiliated with the University they attend the University they study at a University.",
            "There, a professor at a University.",
            "And you can see how these might have gotten clustered together.",
            "They're somewhat similar, but they don't mean the same thing.",
            "There's not a mutual implicature there, and.",
            "And this happens too often, and so the cluster is."
        ],
        [
            "That one gets are often very little bit arbitrary.",
            "They're hard to evaluate their kind of incomplete.",
            "There are a lot of semantic boundary cases between them, and I would argue."
        ],
        [
            "But in just about any schema it's it's going to be incomplete.",
            "They're going to boundary cases.",
            "At a recent."
        ],
        [
            "Natural Language processing conference.",
            "There was a big discussion about, well, you know, where did these relation types come from really?",
            "And do we really know what they mean?",
            "Crisply?"
        ],
        [
            "And Ed Hovy, who maybe many of you know.",
            "We're saying, well, you know, even simple relations that we think we may understand.",
            "Like is a we don't really know what that means or what really the boundaries of that are.",
            "And in."
        ],
        [
            "Being incomplete, I've already talked about even large efforts like Freebase don't have the full coverage that we might want.",
            "So with this in mind."
        ],
        [
            "We've been thinking about this thing that we call Universal Schema, and I apologize somewhat for its heuristic name, but."
        ],
        [
            "All we really mean by universal scheme by Universal is that we will operate on the union of all the input schemas.",
            "Both come from multiple structured sources as well as from natural language, so I mean I think.",
            "This kind of approach is, I think, very familiar to this community.",
            "From what I understand right off and working with many schemas and trying to work with them all together.",
            "And so he will embrace the diversity in the ambiguity of all of the original inputs, and we won't try to force.",
            "These semantics are these given schemas into some predefined boxes, or force them all into one schema to rule them all to which everything will be mapped.",
            "We'll keep around all of this diversity.",
            "And.",
            "Maybe somewhat like open IE when we think about the natural language side of things, but unlike open IE will learn implicature among these entity relations an be able to fill in an observed relation types based on other kinds of evidence and.",
            "As they say here, combine both.",
            "Given structured schemas as well as natural schemas that come from natural language.",
            "So let me be more specific, explain better what I'm talking about by.",
            "By showing some pictures that will then lead directly into the kind of machine learning models that we're using to do some of this work."
        ],
        [
            "So here we have a matrix in which in the Rose we have entity pairs, resolved entity pairs like Obama United States and Merkel in Germany and in the columns we have relation types and some of these come from structured sources like Freebase is head of relation and some of them come from other structured sources like tech Peas, top member and many of them will come from text phrases from from text like President Oven, Prime Minister, Oven, leader of and so on.",
            "From our raw data, either structured or unstructured, some of these webs."
        ],
        [
            "Directly.",
            "And so we can fill them in.",
            "Um?",
            "But what if somebody asks us, well, is Obama the leader of the United States?",
            "We have a column for leader of, but we never observed that directly.",
            "We just don't know what we would like to do is complete this matrix to be able to answer."
        ],
        [
            "Ah yes.",
            "Here."
        ],
        [
            "The leader, and to infer this automatically.",
            "And the way that we'll do this is with.",
            "Kind of generalized principle components analysis that is very similar to the kinds of methods used to win the Netflix Prize.",
            "Alright, because this is very much like the Netflix problem, in which there are movies in the Rose.",
            "Or maybe the other way around, so maybe we should say that the movie watchers are in the rows and movies are in the columns and you tell Netflix a few examples of movies you like and don't like.",
            "And it's a large matrix that sparsely populated would like to predict or fill in this matrix for the many entries that you said nothing about.",
            "So I'm going to say more about how that process works and how we do this specifically.",
            "In a moment, the particular parameterisation and underlying learning mechanism and model before I get there, I want to give you some.",
            "Examples that emerge from real data from running our model on real data to give you more intuition for what this looks at looks like, so here's one example."
        ],
        [
            "We observed in one of these middle rows that Steven Forbes denounced George Bush and then we inferred from our model that Forbes criticized Bush.",
            "Here is just like learning in a way, a synonym."
        ],
        [
            "Here's another slightly more of each example.",
            "We observed that Volvo bought a stake in Scanio, another car company, and we inferred that Volvo owns a percentage of Kenya.",
            "Here's my favorite example because."
        ],
        [
            "Shows an example of our models ability to capture some asymmetry.",
            "We observe that Kevin Boyle is a historian at Ohio State, and we infer that he's a professor at Ohio State.",
            "But when we observed that Freeman is a professor at Harvard, we didn't necessarily infer that he's a historian.",
            "So."
        ],
        [
            "Are examples of.",
            "I'm running on relation types, but we've also."
        ],
        [
            "I've been running this on entity types as well, in which we put single entities in the Rose and entity types in the column.",
            "So, for example, we observe that Bill Gates is a person, and he's a film subject.",
            "I'm coming from CBP and Freebase, and in text we observe the Chairman and after we do our."
        ],
        [
            "Completion we we infer that he's an executive and a leader, and so.",
            "This allows us to reason about a very large number.",
            "Of types at least, maybe not large from your point of view in this community, but from the natural language processing POV where we used to using, say, five or six entity types or maybe 40 or 100 instead, real reason about 40,000 entity types altogether."
        ],
        [
            "About plumbers and ballerinas and musical trios.",
            "And all of these things.",
            "So here's some exam."
        ],
        [
            "Again, that come from actually running our model on real text.",
            "We observed that Peter Norton is an entrepreneur, philanthropist, and magnatone be inferred that he's a mogul alright.",
            "Here's an example of where we also predicted a structured kind of type.",
            "We had observed that Sonic used Youth has an album.",
            "It's a band performed at, and we inferred that it belongs to the Freebase type musical ensemble.",
            "Alright, so in."
        ],
        [
            "Summary In Universal Schema, we're going to embrace the diversity and ambiguity of all of our original inputs and not try to force them into predefined boxes or a single.",
            "Single schema we're going to reason about entities and relations together.",
            "It's just not an abstract relation to relation mapping.",
            "We're not saying that whenever I observe this relation type, that always implies this other relation type.",
            "That kind of implicature is always done in the context of a particular entity pair.",
            "And the user can one way that we think about this.",
            "One thing we like about there is this result here is that a user can then query our knowledge base without really understanding some particular design schema.",
            "For example, if you want to use Freebase, it's pretty hard because you have to study the Freebase schema and really understand it in order to know how to query it.",
            "Here our schema includes vast quantities of just natural language, textual phrases, and you can probably just ask the question in whatever way it comes naturally to you in natural with natural language names for these relations and we'll be able to answer it because that will be one of the columns.",
            "In our model.",
            "And Furthermore, we're following this philosophy in a way that we're using the model to predict original expressions or the original data in text or in the observed.",
            "Given structured knowledge base, we're not trying to create models of semantic equivalence, which I believe are elusive."
        ],
        [
            "Alright, so now let's talk about how this is actually done mechanically, and to do this I want to step back again to provide a bit of context and draw some different pictures for you."
        ],
        [
            "So again, remember, we're given some text."
        ],
        [
            "We find some entity mentions."
        ],
        [
            "We resolve these into entities."
        ],
        [
            "We want to predict what there."
        ],
        [
            "The types are."
        ],
        [
            "Antop"
        ],
        [
            "Predict what their release."
        ],
        [
            "Types are."
        ],
        [
            "And.",
            "Let's start by focusing on entity types."
        ],
        [
            "And how we typically do this, at least in the natural language processing community, we would build some classifier with parameter."
        ],
        [
            "For each one of these entity types that looks at.",
            "Play a couple of 1,000,000 features.",
            "In the neighborhood of the appearance of these mentions in text.",
            "Learn those parameters and then given any particular intervention, use this classifier to predict what the type is and.",
            "You know, as I said earlier, I want to use not just a few entity types that would be given in one particular."
        ],
        [
            "But also use some from multiple schemas and use some from."
        ],
        [
            "A positives that come from text which provide us a lot of other entity types, and so to do."
        ],
        [
            "This on like I was saying about 50,000 different entity types or many more and this is what I mean."
        ],
        [
            "Universal schema.",
            "Someone quickly apparent issue here is that if we have 2 million parameters in each, one of these stripes, and we're trying to do this on the 50,000 types or more, this is a lot of parameters, and it gets pretty big.",
            "So we'd like to have."
        ],
        [
            "Other parameters, so we're going to do that and these these parameters say maybe just one hundred, 200 or maybe 500 of them instead of a couple of million won't be directly in raw feature space.",
            "They're going to be in some new.",
            "Latent space that we're going to make up through structure.",
            "We're going to discover.",
            "And we are.",
            "Then going to take the kind of features that we had before and embed them in the same space or here.",
            "Actually what I'll talk about."
        ],
        [
            "That we're going to take the entities themselves and have learned parameters associated with the entities which will be of the same dimension of vector into the similar latent space, and we're going to learn those also, but in a way you can think about these parameters is being like.",
            "Like an observed feature vector and we're like like a logistic regression classifier, we're going to take a dot product between a vector here and a vector here in order to make some prediction.",
            "But we're going to be usually will just learn the parameters of the classes were trying to predict, and now we're going to learn both those parameters and the parameters of how will represent our inputs."
        ],
        [
            "And here we are."
        ],
        [
            "This matrix, and as I was just saying, will make a prediction anywhere at some intersection in this matrix by doing a dot product between the vector if each row and column and putting that through some logistic function in order to turn it to a value between zero and one."
        ],
        [
            "And again, we will observe someday."
        ],
        [
            "And we'll do some matrix completion in order to do this prediction, and I'll get into how we do this.",
            "Learning in just a minute."
        ],
        [
            "Um?"
        ],
        [
            "So.",
            "So we also can do this for relate."
        ],
        [
            "Types right in which we have some of these types again embedded in some."
        ],
        [
            "Or dimensional space for multiple given relation."
        ],
        [
            "Schema as well As for relations observed in."
        ],
        [
            "And as before, for the Rosewill will have entity pair?"
        ],
        [
            "And we'll do it."
        ],
        [
            "Back to the same kind of."
        ],
        [
            "Completion in the same way, by doing a dot product between the representation for the entity pair and a relation for."
        ],
        [
            "An entity type.",
            "Sorry for relation type.",
            "Alright, so how do we do learning?",
            "How do we learn all of these purple parameters?",
            "Um?",
            "In Netflix, Skype problems, this matrix is filled with a movie scores.",
            "How much did you like this movie from one to five and the loss function that they'll use to drive learning will?"
        ],
        [
            "Will typically be squared error.",
            "The model predicted something between one to five.",
            "The human said it should have been something one to five.",
            "How far are you off?",
            "But here we're making a binary prediction, and so instead will replace this.",
            "By taking this sum is just representing essentially the mechanics of doing a dot product right and will put this through a logistic function.",
            "I'm just like logistic regression.",
            "This two number between zero and one and.",
            "We can get a gradient from this either by log likelihood or by some ranking based objective, which I'll describe in a minute, and by this gradient some parameters we can then use some stochastic gradient descent optimized with various methods, including if it's like at a grad to nudge all of our parameters in the right direction and train until we get some kind of convergence."
        ],
        [
            "So.",
            "Let's talk about this more concrete in terms of some pictures instead of equations.",
            "What does this look like mechanically, and how is it that we can?",
            "Run this in a very scalable fashion.",
            "They make a comment about scalability, so we've been running this with saying.",
            "More than 300,000 rows and you know maybe 50,000 columns and we train these parameters on one machine in less than half an hour, so we're not that concerned about scalability.",
            "So what does this consist of?",
            "We will randomly sample sumrow.",
            "And some entry within that row that's labeled.",
            "That was observed that we think is positive."
        ],
        [
            "Will randomly pick some UN observed cell in the same row anwil."
        ],
        [
            "Assume for the moment that that should.",
            "That's a negative example, which note could be wrong, but that's just like some training noise that we can be robust to.",
            "And we'll take the dot product of each for this row in this column, yielding a score for this cell.",
            "Take the dot product for this row in this column.",
            "Building on something here."
        ],
        [
            "And the rank objective says, well, I would like this dot product to be larger than this dot product.",
            "The positives should score higher than the negative and the extent to which that's not true.",
            "We can turn into a gradient which changes the parameters for this row as well As for these two columns.",
            "Essentially, bringing this row closer to this column and further away from this column and likewise the parameters for this column A little bit closer to this row and the opposite here as well, and so we just step through the matrix so this sort of sparse matrix training as we go.",
            "We never need to fully populate the matrix, were only.",
            "Learning these purple parameters.",
            "So let's talk about."
        ],
        [
            "Results on real data.",
            "We took the full text of 20 years of New York Times news articles, extracted entity mentions, perform density resolution.",
            "Got about 350 entity pairs that appeared within close enough proximity to each other that we could claim that there was some textually expressed relation between them.",
            "Here we found about 23,000 unique relational surface forms.",
            "I mean unique relational textual patterns that describe relations.",
            "We also went to Freebase and got about 6000 entity pairs that were resolved against these entity.",
            "These New York Times entities and about we just chose about 160 relations from Freebase."
        ],
        [
            "And then did some training and held out some separate test data to do some to assess our accuracy.",
            "And here are some results that we got.",
            "So actually on this data set UMass and University of Washington and Stanford and others have been leapfrogging each other over the years here.",
            "And so here's what our results looked like in 2001 by the distance supervision method that I described earlier, we're getting about an average of 48.",
            "Percent this unsupervised clustering method brought things up a bit.",
            "Stanford had been working with a fancy distance supervision method that also got some nice results, but Universal Schema made a big jump that we're very proud of.",
            "We're very, very happy with these results.",
            "We've."
        ],
        [
            "I have been applying this to entity types as I described earlier as well and a straight up traditional entity type classifier on a particular fine grained entity type prediction task was getting results like this around 55 method from University of Washington was doing better like this and universal schema did did better still.",
            "So I think there's a lot more work to be done.",
            "But I think there are some promising early signs.",
            "Alright, so let's take a breath.",
            "I."
        ],
        [
            "We're about to go into the next major section.",
            "Let me let me pause.",
            "Let's see I have to say that.",
            "To wag my finger at an issue so that the information retrieval community did a little better at asking questions along the way, maybe I've just been speaking too fast and not letting you get a word in edgewise.",
            "Let me pause to see if you have some questions before I continue.",
            "We can also take questions at the end, but.",
            "Yes, thank you.",
            "Good OK yeah.",
            "So the question is how is it that Universal Schema learns to do this asymmetric asymmetrical implicature?",
            "No historian implies professor, but professor doesn't necessarily imply historian.",
            "Our I'm going to talk more about this in a bit when I talk about the this second bullet in this triplet here with Gaussians, but.",
            "I believe that it has to do with vectors of different magnitudes and essentially the overall magnitude of the vector.",
            "For Professor is larger, which represents the notion that it's a more general concept.",
            "And.",
            "But actually after I talk about Gaussians, ask your question again, if if you'd still like to know more.",
            "Wonderful, yes thank you.",
            "What is the contribution of your prior structure knowledge versus when you learn from language in a war against unorganized but yeah.",
            "Good, so the question is noting that we're training on both the raw text as well as this pre structured data and what's the contribution of training on the pre structured data we could train with either one and not the other and still get some good and useful results.",
            "We and others have done a lot of training on actually just the structured data and we can essentially do knowledge base completion and see cases where relation should have been there but were left out accidentally.",
            "You can also train on just the text and say well which textual phrases.",
            "Predict which others.",
            "What I like about trading them both together is that we then affectively seeing how text and these structured schema align with each other and get mutually implicature among those as well.",
            "But the training method will work fine either way.",
            "Let me take now there are questions.",
            "This is fantastic.",
            "I'm so happy.",
            "I think I'll take two more questions and then continue and then I'll take more questions at the end and I think I first saw question back in this table and then will come forward.",
            "Yes please.",
            "OK, so the question is how would we deal with conflicting information in our knowledge base?",
            "Let's see here.",
            "I think I could answer that in a number of ways, jumping again if I've taken your question in the wrong sense.",
            "When we observe raw data in green squares that I was drawing in the matrix earlier, yes, it's actually the case that there could be errors there.",
            "Or there could be conflicts and to us from a machine learning perspective, this just looks like noisy training data, which many machine learning algorithms are used to dealing with, and the hope is that.",
            "Right, the learning capacity of the of the model and its capacity for regularization helps.",
            "Smooth over these errors and discover the kinds of regularity's that actually should be reflected in the truth and recognize when something was noisy.",
            "The model's ability to actually do that successfully?",
            "Uh, is an empirical question, and it certainly doesn't do it all always perfectly, but that's my sense.",
            "OK, last question right here, thank you.",
            "Knowledge.",
            "Right, yeah?",
            "So the question is, do we consider the original knowledge base to be the truth and or do we also consider that to have uncertainty?",
            "And the answer is.",
            "Yes, through training perspective anyway, we consider it to be the truth and you know similarly to what answer before they can deal with noise.",
            "If there were, if we were presented with uncertainties or confidence is in that, we could absolutely leverage that in these training methods, and that would be an exciting direction to take absolutely.",
            "Alright, I'm so glad to see so many hands.",
            "I'm looking forward to more questions at the end.",
            "Let me jump ahead so."
        ],
        [
            "In this picture, we've been exploring some new avenues recently that I'm excited about, and I want to give you a brief tour of three of those and the first is multi sense representation.",
            "So sometimes you could observe a piece of text like Microsoft beat novel.",
            "And the word beat can have multiple senses, though, right?",
            "It could mean to dominate in a business sense.",
            "Come to fisticuffs or maybe even one politician beating another politician has different implications than one company beating another company and we'd like to represent these separately.",
            "So how can we?",
            "How can we do this?",
            "We've been.",
            "We started to think about this problem.",
            "Thank you not in.",
            "Not yet with Universal Schema.",
            "Entity types in relations, but thinking about word embeddings and then we're eventually will get back to."
        ],
        [
            "The types and two recently very popular methods for learning wording meetings are called SIBO and Skip gram, and these work by assigning to each word in the dictionary some unique latent vector in space.",
            "And we would say, you know Sebo would take the.",
            "The average of.",
            "The words in the context of some central word in a running stream of text.",
            "Here the automobile plant manufacturing said ends take the average of the context and considered the dot product between that average and a central word.",
            "An ask that that rank higher than the dot product between the central word in some other randomly chosen dictionary word that's not properly in the context and use that to get some gradient signal to learn.",
            "Parameters here skip gram does something similar without bothering to do with the average."
        ],
        [
            "And when we you know, we can really efficiently train models like this, say I'm 2 billion words of text in about 8 hours on machine.",
            "And when you do this, you."
        ],
        [
            "Just some lovely.",
            "You look at where these words are positioned in vector space.",
            "Here I'm taking say 200 dimensional space, projecting it down to do and you."
        ],
        [
            "Some very nice semantic relations in which cash, stock and share are near."
        ],
        [
            "Each other, well, service operations and products are near each other, and Furthermore we can notice."
        ],
        [
            "Only that.",
            "Names of countries tend to cluster together and names of cities clustered together, but even the vector distance between a country and its capital.",
            "That vector displacement seems to be similar for different countries and their capital so that we could, say take this vector from China to Beijing and add that vector to Portugal and end up in the neighborhood of Lisbon and so."
        ],
        [
            "We can get results like noticing that that we can do this vector arithmetic in this space take."
        ],
        [
            "Take Paris, subtract France ad Rome asked what am I closest to now and get Italy."
        ],
        [
            "Or take sushi, subtract Japan at Germany as one of my closest to now and you get Bratwurst.",
            "So this is fun and we have."
        ],
        [
            "Using.",
            "Both kinds of models to create a model that helps us reason about multiple senses.",
            "By putting these two things that are usually run separately."
        ],
        [
            "Together, so here we'll take a word like plant and capture the notion that we could have multiple different senses, and each will have multiple different vector representations.",
            "We will take the context to take the average, find the sense that is closest to that average.",
            "And then.",
            "Train that sense but not the others according to context.",
            "Using skip Gram and Furthermore if."
        ],
        [
            "We look at all the sensors, find that none of them are particularly close.",
            "We will go ahead and."
        ],
        [
            "Create a brand new sense so that in a nonparametric sense, we can create as many senses as the data suggests, and some words may have just one since some may have two so may have 17."
        ],
        [
            "And he."
        ],
        [
            "Some examples we run this Sunday to a traditional method that asks what are the nearest neighbors of the word cell with only one sense, we get things like this."
        ],
        [
            "But when we allow the method to have two senses, we get both.",
            "Cell as well as the spreadsheet version of cell."
        ],
        [
            "Another example for sin, and here are words that we can anybody guess.",
            "What another sense of sin might be?",
            "It's the."
        ],
        [
            "Mathematical sense of sin pronounced differently.",
            "And then."
        ],
        [
            "Here are some examples that show the nonparametric version when we actually run it on real data for plants actually decides.",
            "To to have a four senses of plant one.",
            "The.",
            "Common usage in conversational sense of how we talk about plants as well as the scientific biological sense, as well as manufacturing as well as.",
            "Electricity generation."
        ],
        [
            "Here are some other examples, like for Fox, it discovers two senses, both the animal as well as the television.",
            "So we."
        ],
        [
            "Ran this on some data and compared it to some previous work from Stanford.",
            "In a way that runs more quickly and that is able to do this in a non parametric way and."
        ],
        [
            "Got some very nice results on some data actually produced by the by the Stanford folks on a data set called Contextual Word similarities where the previous results like this and then we're getting something close to 60.",
            "So this is."
        ],
        [
            "One piece of fun that we're having and like I say one of the next things we want to do is then apply this to relations in entity types.",
            "Alright, next in my."
        ],
        [
            "Of three things, Gaussian embeddings, so we."
        ],
        [
            "These word embeddings that we've been talking about, and they've been really useful.",
            "Actually, there's been a blossoming of interest in these in natural language processing.",
            "The extent to which, at the most recent conference, empirical methods in natural language processing MLP.",
            "The common running joke through the conference was that E was standing now for embeddings.",
            "And so they've been using low level NLP and we and others have used them to get new state of the art results in named entity extraction.",
            "They've been used for machine translation and very exciting ways.",
            "Been used for question answering and so on."
        ],
        [
            "But what's missing?",
            "So one thing I would claim as a notion of breath, like here we have a person and a musical composer and they have positions in space.",
            "But a person is a more broad concept, more general concept in a musical composer, something more specific.",
            "It would be nice to somehow represent that.",
            "And we're also missing a notion of asymmetry in ways that will describe in a minute.",
            "So we would like it to be."
        ],
        [
            "Represent concepts like this right with instead of a vector point in space.",
            "A Gaussian region in space.",
            "So that we can capture the notion that a person is a broad concept and the composer is more narrow and sits inside of it.",
            "And actually, these drawings that are about to show you actually come from real data when we trained on the raw text of Wikipedia."
        ],
        [
            "We find that their words like famous or actually or classical also appears similarly to this that crosscuts these concepts and."
        ],
        [
            "Johann Sebastian Bach appears right at the intersection in a very nice way, just the way that I would like.",
            "So this just emerges unsupervised from training on raw text.",
            "So what does this look like?"
        ],
        [
            "Mechanically, for each word we have a vector representing the mean of the."
        ],
        [
            "And then."
        ],
        [
            "Also another vector."
        ],
        [
            "Representing the diagonal of the covariance.",
            "We're currently using a diagonal, but we could do something more expressively."
        ],
        [
            "And this covariance matrix captures the breadth and four.",
            "You know we can write down with this what this means."
        ],
        [
            "Way that captures both allowed rhythmic penalty for the volume due to normalization as well as the Mahalanobis distance that sort of obeys the covariances as it considers measuring distances and for a simile."
        ],
        [
            "Maybe we can use collect Leibler divergent switches and asymmetric distance measure, which were often used to in natural language processing information retrieval running on discrete distributions.",
            "But you can absolutely run on continue."
        ],
        [
            "Distributions, oops, sorry.",
            "Can just continuous distributions like Gaussians as well to get an asymmetric distance?"
        ],
        [
            "So how do we train this?"
        ],
        [
            "By method very much like Skip grammar will start with some central word in some sequence, like composer.",
            "Will compare."
        ],
        [
            "It's.",
            "Compatibility with some true word word that it cures it.",
            "It's true local."
        ],
        [
            "As well as its compatibility with some random dictionary."
        ],
        [
            "There's a negative example and we want."
        ],
        [
            "Sort of like the energy of the positive pair to be greater than the energy of the negative pair."
        ],
        [
            "And we can calculate that energy well before we've been doing it by a dot product written here.",
            "And we can do something very similar."
        ],
        [
            "With Gaussians where we'll do an integral instead of a discrete sum."
        ],
        [
            "And this integral."
        ],
        [
            "With."
        ],
        [
            "Gaussians."
        ],
        [
            "Looks like this set has a very nice clothes for."
        ],
        [
            "That's easy to calculate and even have pieces that are pretty interpretable."
        ],
        [
            "So then."
        ],
        [
            "Loss function is to say, well, we want to maximize.",
            "Since they want the positive energy to be larger than the negative energy, and again we can turn this into a gradient to learn parameters of both the mean and the variance of these Gaussians.",
            "So let me show you give you a sense of what kind of results this gives us in some real."
        ],
        [
            "Is that in which we ran some benchmark results against the previous state of the art?",
            "There has been some work by Barone and others in natural language processing to capture some sense of entailment.",
            "Among words that express categories where positive examples in this data set are things like adrenaline is a neurotransmitter, Archbishop is a type of clergymen, horses, a type of mammal, and so on and then negative cases or some random."
        ],
        [
            "There's like an air crew is not a playlist as well."
        ],
        [
            "As reversed negative cases like, although pizza is a food, food is not a pizza.",
            "It's not always necessarily a pizza.",
            "Anne."
        ],
        [
            "And we experimented with both his agonal covariance version of our model as well as a spherical covariance version we trained on about a billion words of raw text from Wikipedia, in addition to 3 billion tokens of newswire, and we evaluated on the optimal F1 operating points."
        ],
        [
            "Average position and Barone, which was the previous state of the art, was getting numbers like this and we were getting numbers almost 80, so it's not a huge leap, but a step in the right direction thing we're happy with."
        ],
        [
            "Alright, now take to close let me."
        ],
        [
            "Talk about something that I think might be especially relevant to this Community because I know.",
            "I think I want to be able to talk about not just representation but reasoning, and in a way that kind of entailment I just described as a simple kind of reasoning, but I'd like to do something a little bit more rich, and I think it's not going to count as truly rich by your standards, but it's a small step by from an NLP persons POV.",
            "I'm.",
            "I'd like to."
        ],
        [
            "Be able to say well what's the nature of the relationship between Melinda Gates here in Seattle?",
            "And I observed no raw direct evidence of what that relationship would be.",
            "But I have that I have a spouse relation to Bill and the Chairman relation to Microsoft and headquartered in relation to Seattle.",
            "Maybe somehow I can use that to infer."
        ],
        [
            "What this relationship should be here by some sort of multihop reasoning is through this path.",
            "So this should be very familiar to you, right?",
            "So we can absolutely we could write down some rules."
        ],
        [
            "It says well, if we have spell Sabian Chairman and headquarters in all in the chain that I can infer lives in relationship between the two endpoints.",
            "That's good."
        ],
        [
            "And."
        ],
        [
            "This is exactly the kind of work that's been done in NLP by Neil out to Carnegie Mellon who's now at Google.",
            "Now, of course, that's just one particular chain."
        ],
        [
            "So I.",
            "Not I may observe CEO instead of CEO, and so maybe I need a rule for that also.",
            "Or sorry, CEO or."
        ],
        [
            "Maybe observe CEO also will need another rule for that.",
            "And well, maybe not."
        ],
        [
            "Spell so, but also a child of could I could make some inferences from that as well."
        ],
        [
            "And well, maybe not just even lives in, but also other kinds of relations.",
            "I might end up predicting, and this is starting to get painful, and so I'd like to do something more general and.",
            "No, I'm noting that."
        ],
        [
            "Already we've been talking about vector representations for all of these things, and in fact we've been thinking quite a bit about.",
            "A representation of knowledge base that actually doesn't really store symbols at all.",
            "It just associated with each node.",
            "Instead of storing, say that you know the list of entity types that belong to this node as a list of symbols representing those entity types will just store a vector representation along with that node.",
            "That representation is responsible for answering all questions about what entity types this belongs to.",
            "By Gaussian containment or by something else, and similarly for these kinds of.",
            "Relations each edge won't have a symbol on it.",
            "It will have a vector on it instead.",
            "And now I'd like to be able to reason.",
            "Not about symbols, but about vectors with something that looks like logic.",
            "But what would that be?",
            "Or how can we do that?",
            "And so we've taken some initial baby steps into what that might look like with."
        ],
        [
            "Following mechanism, we've been using a recurrent neural network.",
            "Which consumes the vectors, the vector representation of the semantics of the relations along this path consumes them all in order and at the end out pops from the network.",
            "A vector representation of.",
            "Of the semantics of the relation between the two endpoints of that path, and indeed this network produces a vector who that is very close to the vector representation of the symbol lives in.",
            "And so in case we don't know too much about these recurrent neural networks, how do they work right, you take.",
            "The vector representation of whatever context you had so far concatenated with the vector representation for the next link in the chain that you're about to consume.",
            "It goes through one or multiple layers of interpretation and think about, well, how should I compose these two things together produces a new representation of the composition of these things so far, and that composition then gets fed back into the bottom, saying, well, here's my context so far from the chain, the arbitrary length chain so far, and now I'm going to check on this one additional piece of evidence, and I'll produce a new step in the composition."
        ],
        [
            "So this was published at just this most recent ACL conference."
        ],
        [
            "And so there's related work as I said by Neil OW, using rules on symbols.",
            "Gardner also at CMU's been doing some work.",
            "In addition, on relational vectors, where the relational vectors are alert learned offline that also has some nice results."
        ],
        [
            "So we've been running this on about 18 million entities, about 40 million Freebase triples, as well as 12 million triples that come from the text of the clue web text collection, which is a broad web crawl produced by Carnegie Mellon.",
            "With about 25,000 different relation types.",
            "And here I just want to get a sense of what this produces.",
            "Some examples of some predictive paths that emerged."
        ],
        [
            "From our model."
        ],
        [
            "So we'd like to predict the place of birth relation between a person and a location."
        ],
        [
            "And we found that.",
            "Observing the text was born in with the mailing address.",
            "These are all free based relations.",
            "I don't know how use you are reading Freebase, but the mailing address, city, town relation and then that's that that occurs in some province.",
            "Region captures that this person was born in that province."
        ],
        [
            "Or if user the text from and then that some location contains another location, that's another kind."
        ],
        [
            "Difference you can make.",
            "These are paths that were.",
            "Seen in Arad training data right, they weren't labeled cousin away.",
            "All of its unlabeled, but but they were seen.",
            "Here's another path that wasn't seen in the training data all, but was discovered by generalization and just neighborhood of these.",
            "Vector positions that the text born in and near can be chained together together to infer this relation.",
            "OK, this is, I think of this simple example for me."
        ],
        [
            "Show you something just slightly more."
        ],
        [
            "Complicated, what if we're trying to predict?",
            "Books, original language.",
            "The original language in which a book was written."
        ],
        [
            "So here we have a book was.",
            "Here's something in the previous series to which this book belong, and the author of that book has this nationality and another person who has the same nationality speaks this language."
        ],
        [
            "This book was written by this author who has this ethnicity and people who have this ethnicity speak this language."
        ],
        [
            "This book was written by this author who addressed sort of spoke to people of this nationality who speak this."
        ],
        [
            "English.",
            "Where this book takes place in a certain location, this location has people of this nationality who speak this language, so all these were automatically discovered from a combination of text and structured inferences."
        ],
        [
            "And."
        ],
        [
            "When we ran on some data, here are some results that we got, so the former original method that was doing it."
        ],
        [
            "On top of symbols.",
            "I was getting some results like this."
        ],
        [
            "Aurora cursive Neural Network did a little better."
        ],
        [
            "About 7% better, not hugely, but again a little something."
        ],
        [
            "We were using this as a baseline, but then we actually made a change to the baseline that's trained it more the way we would a classifier and and the new baseline we created was actually significantly better.",
            "And then we combine."
        ],
        [
            "Our new baseline with RNN."
        ],
        [
            "We did better still about 15% above the original estimate."
        ],
        [
            "This.",
            "I think this is a little bit hard to explain, but I just briefly as well as a near final comment.",
            "This method can also make predictions about how to chain or compose relations that it never saw at all at training time what's often called zero shot learning.",
            "So we will have learned.",
            "Not like an embedding for this relation, but never at training time did it even try to reason about how this relation would be composed with others, but because we had learned how to compose other relations, we also sort of in the smooth continuous space, figured out how to compose this new relation.",
            "We have never seen before, and you know, it's 20% is not great accuracy, but it's better than random and I just find that interesting.",
            "Alright."
        ],
        [
            "So I'm let me just wrap up by saying some things that we're now looking at in the future.",
            "We would like to also think about events."
        ],
        [
            "Which we've been thinking about in."
        ],
        [
            "So have a neo davidsonian."
        ],
        [
            "Kind of way, which I think is the same way that you often do in this community.",
            "Create a node to represent that event and."
        ],
        [
            "It's many arguments off of that."
        ],
        [
            "Single node and we can do inferences and use embeddings in similar ways here."
        ],
        [
            "And we would also like to reason about multilingual data, right?",
            "So far we've just been talking about English, but in a way these columns are already learning synonyms.",
            "In both symbols.",
            "We can't just stick."
        ],
        [
            "A whole bunch of language in those columns and learn that.",
            "Here's a Spanish phrase that it's a synonym for an English phrase, and we're actually already getting some very nice good results here."
        ],
        [
            "Alright."
        ],
        [
            "So with that close, here are the things that we've been talking about here."
        ],
        [
            "Summary, and I've been very happy to take your questions.",
            "Thank you.",
            "Hi, this is Michael, a shield from semantic arts consultant in industry.",
            "Just wondering how the research pipeline goes from this kind of good research to product says you know information extraction is already a cottage industry.",
            "So is this already in a product?",
            "Or when might we see something like this?",
            "Thank you yeah.",
            "So there's been let me step back a little bit from the specifics here to just generally learning embeddings and deep neural networks have are.",
            "Widely used in places like Google and there are multiple startup companies including Richard Socher, whose previous work I described earlier as a startup company that he just started within the last year or so.",
            "And these methods have been highly successful in making real word world impact in products.",
            "As for Universal Schema, I don't know of this being used in industry.",
            "At least in a shipping product yet.",
            "But if you're interested in exploring this, I'd be happy to talk with you.",
            "Yeah, I don't think I have much more insight there.",
            "And yes, so let me say that actually, so Michael mentioned very kindly at the beginning.",
            "This mallet toolkit that I started producing in 2002 when I first arrived at UMass, that I'm very happy has been broadly useful and is widely used.",
            "About 2009, we found that our own research was.",
            "Needing more flexibility than the kind of classification and linear chain sequence prediction and a little bit of topic modeling that Mallett was producing, and so I embarked on building a new tool kit which I call factory ending in IE instead of Y, which is a general graphical model toolkit with a full stack of NLP.",
            "Components sitting on top of it and yeah, so it's now been in development since 2009 is released under Apache Open Source Toolkit and license.",
            "And yeah, it is being broadly used, actually multiple places in industry and in shipping products.",
            "And embedding, learning, and not all but multiple of the components of what I described today have been implemented in factory.",
            "Yeah, further questions.",
            "Hi thanks a lot.",
            "My name is Abby Bernstein from University of Zurich.",
            "Thanks a lot for this wonderful glimpse into a wonderfully interesting world world.",
            "Sorry I had two thoughts until questions that I wanted to ask.",
            "First of all, you showed us this big kind of integrated pipeline of many things in this kind of huge knowledge base where you feed a lot of information back and forth and things bubble in the in the in.",
            "The reasoning.",
            "I was wondering, since you have such an integrated system where things go back and forth, how do you prevent the system from oscillate?",
            "And kind of continuously reasoning and oscillating around some stable point.",
            "There was one question and the other question I had is it seems to me that you did not take into regard the questions that get ends as to the knowledge base.",
            "Can't you kind of extract information from the questions that you could actually use to inform you?",
            "Know the reasoning an actually extract you know an informed the reasoning and informed the building of the models inside of this whole bubbly thing, yeah?",
            "Yeah, good.",
            "Let's see.",
            "So the second question first.",
            "Maybe I'm not sure what you mean by extracting.",
            "Maybe hold on to the microphone, 'cause I'm not sure what you mean by you thinking that we would get new facts from the questions themselves.",
            "Yeah, that's true.",
            "That's not something that we had considered so much.",
            "Yes, this time in this relation in the resource bounded information extraction, we have thought a little bit about.",
            "However, in response to a certain question that would seem to rely on some missing holes in our knowledge base, how we might on the fly go out to the web to try to fill those holes in response to those questions.",
            "But yeah, I think there's.",
            "There's a lot of work to be done, instruction, and when we barely touched on it, then first question was about oscillations in our inference process.",
            "Yes, I have a few things to say about that.",
            "One is that a lot of the grand picture that I described, we have built some early versions of this and we've been working on many components, but should clarify that we're far from having a full, robust version of this entire thing working all together.",
            "Mostly the concrete that we work that we've done on never ending constantly bubbling inference in knowledge base that the mechanisms that we've been doing to do this inference have been Markov chain Monte Carlo processes.",
            "So we consider the knowledge base to be storing some current estimate of what the truth is, and we are constantly visiting subsets of the random variables in this knowledge base, and considering making some random or some some proposed change to our estimate of the truth.",
            "There are learned parameters involved in scoring this.",
            "Proposed leap into another version of the truth.",
            "We score this and decide to accept or reject this change.",
            "And.",
            "Actually, the ability to extremely rapidly evaluate these leaps of truth and undo them if they don't seem to be good was one of the big motivators in some of the original design of factory actually, and and it can be.",
            "Done very, very quickly.",
            "So could Markov chain Monte Carlo kind of processes oscillate?",
            "Yes, and in fact they I mean that's.",
            "One of the goals is that you want Markov chain processes to mix well.",
            "How might?",
            "Someone feel happy, especially this community that thinks about storing Chris notions of the truth.",
            "Feel about the notion that, well, the truth is sort of shifting underneath you, and possibly random ways.",
            "Yeah, that could be problematic.",
            "I see.",
            "And we've also worked on methods that would store.",
            "In the midst of that exploration.",
            "The highest scoring version of the truth so far as the stable thing that we answer questions from and then underneath there's some wandering thing that might replace that that new maximum.",
            "And beyond that, I think I think your question actually deserves a much.",
            "Richer and more multifaceted answer, and I think all I have to say is that there's a lot more interesting stuff to be done and I'm glad you asked it.",
            "There are many questions.",
            "It's bit hard to oversee.",
            "Who's next?",
            "Please line up at the microphone that's easiest to oversee.",
            "Ha.",
            "Natasha, I go ahead and then we'll take thank you.",
            "Thank you for the wonderful talk.",
            "The question that I had so earlier on when you talked about annotating and it is essentially figuring out the learning, the types and learning the relation types.",
            "I was wondering if it would make sense to try and marry those and figure out the and learn that you know certain types.",
            "Social learning ontologies would say here, right?",
            "Yeah, the certain type tends to have this relation types, and maybe this relation types the sort of the range over other types and sort of take it in.",
            "One level up.",
            "Thank you so much for that question, I would.",
            "Yeah, I know that I can rapidly enough find a particular slide.",
            "It would.",
            "Help me answer that.",
            "If I can't find it, I'll move on quickly.",
            "I think I'm not going to find it in time.",
            "Um?",
            "OK, so.",
            "Go back.",
            "Yes, in a way this is.",
            "This is partially captured in a version of our model in a detail that I wasn't able to get into, which is that.",
            "The vector for representing a particular entity pair is divided into 3 parts and part of that is unique to this particular entity pair, but the other two parts are tide or repeated, or it's a single set of parameters that are shared across say, all rows in which the first entity is the same.",
            "So all places where Obama is the first entity have the same first section of their vector representation and.",
            "And all places where the United States is the 2nd argument.",
            "There are some vector that in a way represents the United States in this relational pattern, and so in a sense we can think of these as representing.",
            "Vector representation for Obama embedded inside the relation vector represents something about the types of relations that Obama and that the person and that politicians can participate in in a soft vector kind of way.",
            "And you and another results that it didn't show.",
            "You can actually look at these vectors and see what it says about which entity types can go with each other in relation types.",
            "And this is something that we're interested in exploring further.",
            "One thing that we have done a little bit of, but not really successfully enough yet to publish.",
            "Is to jointly train.",
            "Full ending type representations of the type that I did describe in this talk, and the relation representations at the same time.",
            "Because I have this intuition that training them together should yield even better results than we're getting now.",
            "Thank you for the Christmas.",
            "The results of your techniques are really impressive, but you know we're never happy, yes?",
            "So how good are your?",
            "Techniques in extracting negative information.",
            "The examples where appropriate techniques for extracting negative.",
            "Yeah, the negative examples that you had where about known implications.",
            "OK, so one set is not included in another set, for example, but it is.",
            "Are they able to extract facts such as cars and people are disjoint?",
            "Aurora Banana is definitely not the car.",
            "Yeah, yeah, strong engagement.",
            "So sure yeah.",
            "Two things, so the first briefly as I described how we're training these vectors for Universal schema, you know there are at least in the data we have, we aren't given negative training examples, and so we're just off.",
            "And assuming that the UN observed ones randomly sampled will on average be negative, and that does cause some regret.",
            "If we were given negative training examples, we would absolutely use those, and that would improve our accuracy.",
            "With the first statement, the second statement is some other results for training the Gaussian embeddings that are in the paper, but I didn't get to describe in the talk.",
            "Positive exactly this question of what if we were given.",
            "Hierarchy that containment hierarchy can we train these embeddings using that hierarchy and then do do the Gaussians then correctly reflect this kind of containment?",
            "Do they have the ability to express that into the training method, find those and we found that yes, they did we?",
            "We took some hierarchies as training data.",
            "We trained them both on positive examples of parents and children that were indeed connected as well as negative examples that were not connected.",
            "And found that we got the kind of Gaussian containment that we wanted, which I mean this was actually on some synthetic data that I think if it's just an early positive indicator.",
            "But to do this at large scale at some real ontologies, like the kind that you build, I think would be an exciting next step.",
            "Thank you.",
            "Yeah, so this is just a clarification question for your entities, are you doing into any entity resolution for that?",
            "Or those are just nouns?",
            "Yeah, thank you.",
            "I'm glad you asked this.",
            "Can you deal with that?",
            "Important point, they must be resolved entities and so we did the resolution in advance.",
            "So maybe I can?",
            "I'm sorry, let me also answer this in two parts.",
            "Continue with this first answer and.",
            "So let's see here.",
            "Like in the 2nd results that I described, we grabbed free based entities which are obviously resolved and we extracted entities from New York Times and resolve them against three days.",
            "And this is.",
            "And this is a key point.",
            "And again I didn't have time to talk about it all here, but we've been doing actually a lot of research on large scale entity resolution.",
            "One thing, if you permanently to briefly toot my own horn, we're very excited that just recently actually participated in a competition run by the US Patent Office to do resolution of authors or inventors on the patents and so on.",
            "A very large set of patent data.",
            "I think there's a few hundreds of millions of author mentions doing resolution of these things, and there was an international competition with many participants and we came in first place and I'm so proud of my students in the kimono who did that work.",
            "And and they're going to use our software inside the Patent Office for some next steps.",
            "Yeah, so so resolution is a key component.",
            "The second part is is early in our work on Universal Schema.",
            "Before we really bothered to run our large scale entity resolution of this we just took string equivalence as a very cheap kind of entity resolution and ran Universal Schema anyway.",
            "In a way in which I consider some of the rows would have been noisy, they should have gone together but they didn't and also maybe some things where it might have been confounded also.",
            "And we also got reasonably good results from Universal Schema, so it can be.",
            "Somewhat robust to noise, it doesn't fall apart completely when there's noise.",
            "So thank you for answering all these questions and wonderful talk.",
            "We have to cut out further questions that are here, but I hope question is can still catch you in a coffee break.",
            "Thanks a lot Andrew very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So much for the warm welcome, so I'm new to your conference into your community where neighbors, but I haven't been to visit before.",
                    "label": 0
                },
                {
                    "sent": "My apologies, so I arrive with a bit of ignorance about your house and its history and its plans, so I hope you will be patient with me and I hope that you'll find a little something useful in what I what I have to say.",
                    "label": 0
                },
                {
                    "sent": "Let me also say from the start that I welcome your questions during my talk.",
                    "label": 0
                },
                {
                    "sent": "I'd be very happy to have those.",
                    "label": 0
                },
                {
                    "sent": "A few weeks ago I gave an invited talk at an information retrieval conference.",
                    "label": 0
                },
                {
                    "sent": "Extreme of about this size, but we still managed to have a dialogue during the talk and I enjoyed it very much, so please don't hesitate to speak up.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's get started.",
                    "label": 0
                },
                {
                    "sent": "So some companies, which in their early years focused on web search.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That people are spending less time.",
                    "label": 0
                },
                {
                    "sent": "In front of their desktops and more.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time in front of screens like this are instead.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or not instead, but in addition.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Focused quite a bit on dialogue and on question, answering from knowledge bases so that rather than returning a list.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Links an answer to a question we can use this structured knowledge of.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To give more precise and concise answers.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also like this.",
                    "label": 0
                },
                {
                    "sent": "So this is a great thing for your community, right?",
                    "label": 0
                },
                {
                    "sent": "There is a very large body of people interested in building knowledge bases and the semantic web like you.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Freebase is one such example.",
                    "label": 0
                },
                {
                    "sent": "Very big.",
                    "label": 0
                },
                {
                    "sent": "It's a bit of an odd thing in a way for a company used to doing web search today.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because web search is well known for having a very long tail of queries and have interests.",
                    "label": 0
                },
                {
                    "sent": "And if we define.",
                    "label": 0
                },
                {
                    "sent": "In a small narrow schema then.",
                    "label": 0
                },
                {
                    "sent": "We may not have good coverage and so this graph shows the long tail of queries were really the most of the mass of queries are are out in that in the long tail of quite detailed or unusual things, and so this incense.",
                    "label": 0
                },
                {
                    "sent": "Them to build Freebase to be very large and to have as high coverage as possible, but this is a difficult thing to do and to keep it manageable.",
                    "label": 0
                },
                {
                    "sent": "And it's both hard to manage and hard.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have good coverage.",
                    "label": 0
                },
                {
                    "sent": "So for example, we might be interested in who did Jeff Bezos criticize?",
                    "label": 1
                },
                {
                    "sent": "Well, Freebase has no criticized relationship.",
                    "label": 0
                },
                {
                    "sent": "So this talk is asking the question, how can we preserve the great diversity and openness?",
                    "label": 0
                },
                {
                    "sent": "We get from textual search on the web.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of open schema and by open I really mean almost like driven by raw text and yet at the same time have a sense of an entity relational structure and and do reasoning on top of this structure.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this talk is about.",
                    "label": 0
                },
                {
                    "sent": "So first let's launch in by talking.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bit about.",
                    "label": 0
                },
                {
                    "sent": "How to build knowledge basis from text, which is something that I've been thinking about and working on for quite some time.",
                    "label": 0
                },
                {
                    "sent": "Back in the mid 90s with some grass.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two things at Carnegie Mellon.",
                    "label": 0
                },
                {
                    "sent": "We built this system which spider the web for research papers and information extraction to get the title of the authors.",
                    "label": 1
                },
                {
                    "sent": "The abstracts built the reference graph essentially was than earlier versions of Google Scholar.",
                    "label": 0
                },
                {
                    "sent": "Before Google Scholar existed, and this is one small example of a knowledge base that we built early on.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When I arrived at UMass, we built this system, which also in addition to people.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In papers, knows about institutions and conferences and journals and grants and advisors.",
                    "label": 0
                },
                {
                    "sent": "All this first class to cross referenced objects, and this is a step along the way towards one of our.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Location goals, which is to build a knowledge base of all scientists in the world by extracting and integrating information from papers and patents webpages.",
                    "label": 1
                },
                {
                    "sent": "Newswire press releases.",
                    "label": 0
                },
                {
                    "sent": "Even tweets and blogs and to get not only established scientists but also the newly arrived first year graduate students and to understand their career trajectories and so on.",
                    "label": 1
                },
                {
                    "sent": "We'd like to build these better tools to accelerate the progress of science.",
                    "label": 0
                },
                {
                    "sent": "To help people find papers to read and to site to help people find reviewers, collaborators, Hieros and also to understand the trends and landscape of science.",
                    "label": 1
                },
                {
                    "sent": "And also, incidentally, in this space, we've become interested in building platforms for a new model of publishing and peer review, and gotten quite interested in open peer review.",
                    "label": 0
                },
                {
                    "sent": "I learned recently that your own semantic web Journal practices open peer review, in which the public at large gets to see these reviews.",
                    "label": 0
                },
                {
                    "sent": "We've been building systems for some new conf.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is that.",
                    "label": 0
                },
                {
                    "sent": "In addition, also allow the public at large to contribute reviews in addition to the assigned reviewers, and have had some great experiences and if any of you are interested in talking with me about this, or about using a system like this and some of your own new venues, I would love to talk about that.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But I digress.",
                    "label": 0
                },
                {
                    "sent": "So here are some examples of knowledge bases that we've been working on.",
                    "label": 0
                },
                {
                    "sent": "I want to talk.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now a little bit about.",
                    "label": 0
                },
                {
                    "sent": "What it looks like to build a knowledge base from text and some of the steps involved in some of the research that we've been doing along the way.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we start with some text.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the first thing we want to do is find some mentions of the entities involved, like Whaley or Xinhua University by by finding the boundaries within text, extracting these and also knowing what the types are we wanted.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Relation extraction again from the evidence in the text by looking at the text that surrounds the appearance of these mentions to extract relations like Whaleys affiliated machine while University.",
                    "label": 0
                },
                {
                    "sent": "Then, in addition to the.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Textual input data.",
                    "label": 0
                },
                {
                    "sent": "We may also have some additional structured data input and in both.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cases there may be instances of the same entities that are described in different places in slightly different ways, so we need to resolve these or disambiguate these against each other so we know walian W via the same person.",
                    "label": 0
                },
                {
                    "sent": "And this then gives us what we think of is the truth.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About entities and relations, which we then inject into the knowledge base from which we can answer questions.",
                    "label": 0
                },
                {
                    "sent": "And we've been using machine learning for all of these things so.",
                    "label": 0
                },
                {
                    "sent": "One among multiple issues here.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that well?",
                    "label": 0
                },
                {
                    "sent": "Information extraction components aren't perfect and they make mistakes and they always will, and these errors tend to snowball, right?",
                    "label": 0
                },
                {
                    "sent": "You make an error early in the pipeline and it's very difficult to recover from it later and.",
                    "label": 0
                },
                {
                    "sent": "And even moderately high accuracy accuracy for each one of the components when they're strung together.",
                    "label": 0
                },
                {
                    "sent": "It turns into low accuracy for the pipeline as a whole.",
                    "label": 0
                },
                {
                    "sent": "So with this in mind.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We and others have become interested in.",
                    "label": 0
                },
                {
                    "sent": "Reasoning with.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Certainty through this pipeline, probabilities on all of these things, multiple hypothesis reasoning about the confidence of them and.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Performing joint inference to allow both top down and bottom up reasoning to come into play here to help us.",
                    "label": 0
                },
                {
                    "sent": "Apply reasoning from constraints from later parts of the tasks to earlier parts of the pipeline in order to help us resolve some of these uncertainties, and we and others have shown that this kind of joint inference or probabilistic reasoning and large scale graphical model can indeed absolutely help accuracy quite a.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here are a few examples.",
                    "label": 0
                },
                {
                    "sent": "Some of these, the majority of these are doing joint inference on what I think of as a budding tasks early in the information extraction pipeline joint part of speech tagging and shallow parsing.",
                    "label": 1
                },
                {
                    "sent": "For example, reasoning together about the data that appears together in just one sentence.",
                    "label": 0
                },
                {
                    "sent": "But I think that there is even more.",
                    "label": 0
                },
                {
                    "sent": "Juice.",
                    "label": 0
                },
                {
                    "sent": "More benefit to be gained by reasoning jointly about things that happen at the end of the processing pipeline, because it's there that we bring together more evidence, more data from.",
                    "label": 0
                },
                {
                    "sent": "Bring to bear that we can integrate together.",
                    "label": 1
                },
                {
                    "sent": "And Furthermore, that we'd like to inject this uncertainty all the way into the knowledge base as well, and use the knowledge base to help us do a more accurate job of interpreting the text.",
                    "label": 0
                },
                {
                    "sent": "And this brings up lots of really interesting.",
                    "label": 0
                },
                {
                    "sent": "Difficult questions.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So like how do we represent uncertainty and inject that from extraction into the knowledge base?",
                    "label": 0
                },
                {
                    "sent": "And how should we use the knowledge base contents to help information extraction?",
                    "label": 1
                },
                {
                    "sent": "And then Furthermore, this last one seems a little bit mechanical, but in some ways it's I feel the most important information extraction isn't one shot.",
                    "label": 0
                },
                {
                    "sent": "It's not like I'm going to receive.",
                    "label": 0
                },
                {
                    "sent": "The 500 million documents in which are going to run once up front and then I'm finished.",
                    "label": 0
                },
                {
                    "sent": "I'm going to have all those documents, but then tomorrow I'm going to receive another 10 million documents and.",
                    "label": 1
                },
                {
                    "sent": "The evidence of those new documents maybe sometimes should cause me to change my mind about some of the conclusions that I came to about how to interpret the previous text, and I would like to be able to go back and visit those decisions and make them more accurate, but I want to do that without having to redo inference across the entire input data.",
                    "label": 0
                },
                {
                    "sent": "From the beginning.",
                    "label": 0
                },
                {
                    "sent": "So how can I?",
                    "label": 1
                },
                {
                    "sent": "Store those previous decisions and go back and revisit them in an efficient way.",
                    "label": 0
                },
                {
                    "sent": "So in a way I'd like the knowledge base infrastructure, not just to store the end result of this extraction pipeline, but also help me to manage its intermediate steps along the way.",
                    "label": 0
                },
                {
                    "sent": "So with that in mind.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have been thinking about enlarging the purview of the knowledge base to look something more like.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "In which the Knowledge Base Store is not just the truth, but all the intermediate results and the only thing you're allowed to inject into the knowledge base is the original raw evidence.",
                    "label": 0
                },
                {
                    "sent": "And there's some sense in which the truth discovery happens inside the knowledge base.",
                    "label": 1
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think of this as very similar to epistemological.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Philosophy which says you don't get to observe the truth right.",
                    "label": 0
                },
                {
                    "sent": "You just observe the evidence and it's up to your own internal reasoning to discover the truth.",
                    "label": 0
                },
                {
                    "sent": "So we've taken to calling this.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Analogical knowledge basis.",
                    "label": 0
                },
                {
                    "sent": "And we might think that.",
                    "label": 0
                },
                {
                    "sent": "In a way.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What have I really done here?",
                    "label": 0
                },
                {
                    "sent": "But I sort of moved some boundaries on my boxes and arrows diagram.",
                    "label": 0
                },
                {
                    "sent": "And what does this mean really?",
                    "label": 0
                },
                {
                    "sent": "And that's true in a way, but thinking about the problem in this way has given us some new insights and lent itself to having us think about solving some problems in some new ways, and I'd like to give you a brief tour of some of those right now before getting into some more technical details on the main body of my talk.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One example is how we deal with human edits.",
                    "label": 0
                },
                {
                    "sent": "So rather than a human edit consisting of reaching into the knowledge base or racing the old notion of the truth and writing a new version with maybe some edit history for Providence instead, we treat a human evidence as another piece of.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Evidence.",
                    "label": 0
                },
                {
                    "sent": "It's like a new mini document that says well on this state.",
                    "label": 0
                },
                {
                    "sent": "Natasha said this thing was true and we can reason about that evidence alongside the rest of the evidence and reason simultaneously.",
                    "label": 1
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Out.",
                    "label": 0
                },
                {
                    "sent": "Truth discovery, as well as Natasha's reliability and her expertise on this particular topic and this can help us manage cases in which humans are wrong or they disagree.",
                    "label": 0
                },
                {
                    "sent": "Or maybe human makes a correct edit.",
                    "label": 0
                },
                {
                    "sent": "But then later this data the world changes.",
                    "label": 0
                },
                {
                    "sent": "The humans don't notice, but extraction does notice and extraction should be allowed to overwrite the human as well.",
                    "label": 0
                },
                {
                    "sent": "So we've been doing some work in this area.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example is never ending inference or rather than doing extraction once up front, we can think of inferences constantly bub.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing in the background inside this knowledge base and.",
                    "label": 0
                },
                {
                    "sent": "And the truth is, always reconsidered with more evidence and more time.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And, uh.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we've been thinking a lot about is that is about resolution and not just for coreference of entity mentions, but also for aligning values and ontologies and schemas and relations and events.",
                    "label": 0
                },
                {
                    "sent": "Actually, the kind of thing that exactly this community I know I think about a lot.",
                    "label": 0
                },
                {
                    "sent": "And yeah, we've found that machine learning methods for doing this and methods for doing looking at all of these alignments jointly together at the same time can really vary.",
                    "label": 0
                },
                {
                    "sent": "Be very helpful.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, I've been thinking about resource bounded information gathering.",
                    "label": 0
                },
                {
                    "sent": "You have a knowledge base, it has some wholesome missing blanks.",
                    "label": 0
                },
                {
                    "sent": "You know that that information is out there on the web somewhere.",
                    "label": 0
                },
                {
                    "sent": "We can't afford to process the entire web.",
                    "label": 0
                },
                {
                    "sent": "Maybe Google could, but we can't, so we'd like in some targeted way to go out and issue some queries to get us just exactly those pages we think are most likely to help us fill these blanks.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And to reason about this as a resource constrained task, overtime it would be using reinforcement learning techniques to do this.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then Lastly, if we.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do this at large scale, we have to think about this with parallel distributed resources, and so we've been doing so.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Work on smart parallelism rather than using black box kind of techniques like map reduce methods that reason in a very intertwined way about.",
                    "label": 0
                },
                {
                    "sent": "Parallel distribution and about the particular inference problem to be solved, and I don't have time to get into detail here, but one example is a method that does large scale entity resolution that reason simultaneously about solving the resolution task and also how to distribute the data across parallel distributed resources for maximum efficiency where these inference tasks are very intertwined.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's a brief tour and.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's my plan for the rest of the talk.",
                    "label": 0
                },
                {
                    "sent": "I know we've talked about motivation and open schema and epistemological knowledge bases, and now I want to move on to the.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The title here.",
                    "label": 0
                },
                {
                    "sent": "This thing that we call Universal Schema.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To get into this, I want to focus first on the relation extraction aspect that we talked about earlier.",
                    "label": 0
                },
                {
                    "sent": "And I want to start by stepping back and talking about how is this task done.",
                    "label": 0
                },
                {
                    "sent": "Traditionally, usually in natural language processing.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the traditional method is a supervised learning task, right?",
                    "label": 0
                },
                {
                    "sent": "Some humans.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Down and design some schema of relation types that we care about and then goes to some.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unlabeled text and has a human do the task of labeling this text to find instances of where these relations are expressed in text.",
                    "label": 0
                },
                {
                    "sent": "And we train some machine learning model on this label date.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Such that on test data, hopefully the automated.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nothing can predict successfully where these relations exist.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Don't get good accuracy.",
                    "label": 0
                },
                {
                    "sent": "Labeling all this data is a really big pain.",
                    "label": 0
                },
                {
                    "sent": "It takes a lot of labeled data to do a good job, and so it would be nice to be a bit more clever.",
                    "label": 0
                },
                {
                    "sent": "So kind of method that's been doing.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Best in recent NIST Tech CBP knowledge based population competition tasks is distant supervision in which we assume that we're.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Given a knowledge base that already has a schema, that's like the one that we want, and we're also given a bunch of.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Labeled text.",
                    "label": 0
                },
                {
                    "sent": "Whose contents overlap with the contents of the knowledge base we have already, and we.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For cases where entries in the knowledge base have been rendered in unstructured text.",
                    "label": 0
                },
                {
                    "sent": "Align those with each other.",
                    "label": 0
                },
                {
                    "sent": "That alignment effectively labels the text, and then we then treat those labelings as truth.",
                    "label": 0
                },
                {
                    "sent": "Train some machine.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning method on that pseudo labeled text.",
                    "label": 0
                },
                {
                    "sent": "Run that on some.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some additional new text.",
                    "label": 0
                },
                {
                    "sent": "Find new instance.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simulations to further populate the knowledge base.",
                    "label": 0
                },
                {
                    "sent": "And this is very nice and it works well.",
                    "label": 0
                },
                {
                    "sent": "Except that finding these alignments can be brittle and can wander.",
                    "label": 0
                },
                {
                    "sent": "Into bad patterns.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And it also requires that we already have a knowledge base with the schema that we care about.",
                    "label": 0
                },
                {
                    "sent": "So with this in mind, there have been some people have been thinking about a radically different approach, which is, well, let's.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's just throw away the schema completely an we have no schema and use an unsupervised method.",
                    "label": 0
                },
                {
                    "sent": "And here I'm referring to kind of work that's been done quite a bit of University of Washington, which they call open information extraction.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here we take the unlabeled text we run.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A dependency parser on it, or some cheap approximation to a dependency parser to find the verb.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Their arguments and we take the verb string and we'll say.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is the relation and then here are its arguments, and then we're done.",
                    "label": 0
                },
                {
                    "sent": "This is the relation, and so the string itself is the relation type.",
                    "label": 0
                },
                {
                    "sent": "This is very nice and has grand diversity over everything that you could say in text.",
                    "label": 0
                },
                {
                    "sent": "And the issue is that we start to know how this generalizes, right?",
                    "label": 0
                },
                {
                    "sent": "So here we see the attends relation.",
                    "label": 0
                },
                {
                    "sent": "We might.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also see the affiliated relation, but we have no idea that these two relations are somehow there's some implicature that should be between these, because there are different strings and doesn't know how they relate.",
                    "label": 0
                },
                {
                    "sent": "So this might make us think about continuing.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this unsupervised vein, but.",
                    "label": 0
                },
                {
                    "sent": "Doing some schema discovery automatically, perhaps by some clustering, and so we and others.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Done work in this area as well and you get clusters that look something like this, so we see things like relation 3.",
                    "label": 1
                },
                {
                    "sent": "Somebody authored a paper.",
                    "label": 0
                },
                {
                    "sent": "They wrote a paper.",
                    "label": 0
                },
                {
                    "sent": "They published a paper.",
                    "label": 0
                },
                {
                    "sent": "They were the Co author of a paper and so on.",
                    "label": 0
                },
                {
                    "sent": "So yes, these mean the same things.",
                    "label": 0
                },
                {
                    "sent": "This is a certain type of relation and there's some mutual implicature here.",
                    "label": 1
                },
                {
                    "sent": "And that's very nice.",
                    "label": 0
                },
                {
                    "sent": "But all too often there are also things like relation one.",
                    "label": 0
                },
                {
                    "sent": "Where someone is affiliated with the University they attend the University they study at a University.",
                    "label": 0
                },
                {
                    "sent": "There, a professor at a University.",
                    "label": 1
                },
                {
                    "sent": "And you can see how these might have gotten clustered together.",
                    "label": 0
                },
                {
                    "sent": "They're somewhat similar, but they don't mean the same thing.",
                    "label": 0
                },
                {
                    "sent": "There's not a mutual implicature there, and.",
                    "label": 0
                },
                {
                    "sent": "And this happens too often, and so the cluster is.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That one gets are often very little bit arbitrary.",
                    "label": 0
                },
                {
                    "sent": "They're hard to evaluate their kind of incomplete.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of semantic boundary cases between them, and I would argue.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But in just about any schema it's it's going to be incomplete.",
                    "label": 1
                },
                {
                    "sent": "They're going to boundary cases.",
                    "label": 1
                },
                {
                    "sent": "At a recent.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Natural Language processing conference.",
                    "label": 0
                },
                {
                    "sent": "There was a big discussion about, well, you know, where did these relation types come from really?",
                    "label": 0
                },
                {
                    "sent": "And do we really know what they mean?",
                    "label": 0
                },
                {
                    "sent": "Crisply?",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Ed Hovy, who maybe many of you know.",
                    "label": 0
                },
                {
                    "sent": "We're saying, well, you know, even simple relations that we think we may understand.",
                    "label": 0
                },
                {
                    "sent": "Like is a we don't really know what that means or what really the boundaries of that are.",
                    "label": 0
                },
                {
                    "sent": "And in.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Being incomplete, I've already talked about even large efforts like Freebase don't have the full coverage that we might want.",
                    "label": 0
                },
                {
                    "sent": "So with this in mind.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've been thinking about this thing that we call Universal Schema, and I apologize somewhat for its heuristic name, but.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All we really mean by universal scheme by Universal is that we will operate on the union of all the input schemas.",
                    "label": 0
                },
                {
                    "sent": "Both come from multiple structured sources as well as from natural language, so I mean I think.",
                    "label": 0
                },
                {
                    "sent": "This kind of approach is, I think, very familiar to this community.",
                    "label": 0
                },
                {
                    "sent": "From what I understand right off and working with many schemas and trying to work with them all together.",
                    "label": 0
                },
                {
                    "sent": "And so he will embrace the diversity in the ambiguity of all of the original inputs, and we won't try to force.",
                    "label": 0
                },
                {
                    "sent": "These semantics are these given schemas into some predefined boxes, or force them all into one schema to rule them all to which everything will be mapped.",
                    "label": 0
                },
                {
                    "sent": "We'll keep around all of this diversity.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Maybe somewhat like open IE when we think about the natural language side of things, but unlike open IE will learn implicature among these entity relations an be able to fill in an observed relation types based on other kinds of evidence and.",
                    "label": 0
                },
                {
                    "sent": "As they say here, combine both.",
                    "label": 0
                },
                {
                    "sent": "Given structured schemas as well as natural schemas that come from natural language.",
                    "label": 0
                },
                {
                    "sent": "So let me be more specific, explain better what I'm talking about by.",
                    "label": 0
                },
                {
                    "sent": "By showing some pictures that will then lead directly into the kind of machine learning models that we're using to do some of this work.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we have a matrix in which in the Rose we have entity pairs, resolved entity pairs like Obama United States and Merkel in Germany and in the columns we have relation types and some of these come from structured sources like Freebase is head of relation and some of them come from other structured sources like tech Peas, top member and many of them will come from text phrases from from text like President Oven, Prime Minister, Oven, leader of and so on.",
                    "label": 0
                },
                {
                    "sent": "From our raw data, either structured or unstructured, some of these webs.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Directly.",
                    "label": 0
                },
                {
                    "sent": "And so we can fill them in.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But what if somebody asks us, well, is Obama the leader of the United States?",
                    "label": 0
                },
                {
                    "sent": "We have a column for leader of, but we never observed that directly.",
                    "label": 0
                },
                {
                    "sent": "We just don't know what we would like to do is complete this matrix to be able to answer.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ah yes.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The leader, and to infer this automatically.",
                    "label": 0
                },
                {
                    "sent": "And the way that we'll do this is with.",
                    "label": 0
                },
                {
                    "sent": "Kind of generalized principle components analysis that is very similar to the kinds of methods used to win the Netflix Prize.",
                    "label": 1
                },
                {
                    "sent": "Alright, because this is very much like the Netflix problem, in which there are movies in the Rose.",
                    "label": 0
                },
                {
                    "sent": "Or maybe the other way around, so maybe we should say that the movie watchers are in the rows and movies are in the columns and you tell Netflix a few examples of movies you like and don't like.",
                    "label": 1
                },
                {
                    "sent": "And it's a large matrix that sparsely populated would like to predict or fill in this matrix for the many entries that you said nothing about.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to say more about how that process works and how we do this specifically.",
                    "label": 0
                },
                {
                    "sent": "In a moment, the particular parameterisation and underlying learning mechanism and model before I get there, I want to give you some.",
                    "label": 0
                },
                {
                    "sent": "Examples that emerge from real data from running our model on real data to give you more intuition for what this looks at looks like, so here's one example.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We observed in one of these middle rows that Steven Forbes denounced George Bush and then we inferred from our model that Forbes criticized Bush.",
                    "label": 0
                },
                {
                    "sent": "Here is just like learning in a way, a synonym.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's another slightly more of each example.",
                    "label": 0
                },
                {
                    "sent": "We observed that Volvo bought a stake in Scanio, another car company, and we inferred that Volvo owns a percentage of Kenya.",
                    "label": 1
                },
                {
                    "sent": "Here's my favorite example because.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Shows an example of our models ability to capture some asymmetry.",
                    "label": 0
                },
                {
                    "sent": "We observe that Kevin Boyle is a historian at Ohio State, and we infer that he's a professor at Ohio State.",
                    "label": 1
                },
                {
                    "sent": "But when we observed that Freeman is a professor at Harvard, we didn't necessarily infer that he's a historian.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Are examples of.",
                    "label": 0
                },
                {
                    "sent": "I'm running on relation types, but we've also.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I've been running this on entity types as well, in which we put single entities in the Rose and entity types in the column.",
                    "label": 1
                },
                {
                    "sent": "So, for example, we observe that Bill Gates is a person, and he's a film subject.",
                    "label": 1
                },
                {
                    "sent": "I'm coming from CBP and Freebase, and in text we observe the Chairman and after we do our.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Completion we we infer that he's an executive and a leader, and so.",
                    "label": 0
                },
                {
                    "sent": "This allows us to reason about a very large number.",
                    "label": 0
                },
                {
                    "sent": "Of types at least, maybe not large from your point of view in this community, but from the natural language processing POV where we used to using, say, five or six entity types or maybe 40 or 100 instead, real reason about 40,000 entity types altogether.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About plumbers and ballerinas and musical trios.",
                    "label": 0
                },
                {
                    "sent": "And all of these things.",
                    "label": 0
                },
                {
                    "sent": "So here's some exam.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, that come from actually running our model on real text.",
                    "label": 0
                },
                {
                    "sent": "We observed that Peter Norton is an entrepreneur, philanthropist, and magnatone be inferred that he's a mogul alright.",
                    "label": 1
                },
                {
                    "sent": "Here's an example of where we also predicted a structured kind of type.",
                    "label": 0
                },
                {
                    "sent": "We had observed that Sonic used Youth has an album.",
                    "label": 0
                },
                {
                    "sent": "It's a band performed at, and we inferred that it belongs to the Freebase type musical ensemble.",
                    "label": 0
                },
                {
                    "sent": "Alright, so in.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Summary In Universal Schema, we're going to embrace the diversity and ambiguity of all of our original inputs and not try to force them into predefined boxes or a single.",
                    "label": 1
                },
                {
                    "sent": "Single schema we're going to reason about entities and relations together.",
                    "label": 0
                },
                {
                    "sent": "It's just not an abstract relation to relation mapping.",
                    "label": 0
                },
                {
                    "sent": "We're not saying that whenever I observe this relation type, that always implies this other relation type.",
                    "label": 0
                },
                {
                    "sent": "That kind of implicature is always done in the context of a particular entity pair.",
                    "label": 0
                },
                {
                    "sent": "And the user can one way that we think about this.",
                    "label": 0
                },
                {
                    "sent": "One thing we like about there is this result here is that a user can then query our knowledge base without really understanding some particular design schema.",
                    "label": 0
                },
                {
                    "sent": "For example, if you want to use Freebase, it's pretty hard because you have to study the Freebase schema and really understand it in order to know how to query it.",
                    "label": 1
                },
                {
                    "sent": "Here our schema includes vast quantities of just natural language, textual phrases, and you can probably just ask the question in whatever way it comes naturally to you in natural with natural language names for these relations and we'll be able to answer it because that will be one of the columns.",
                    "label": 0
                },
                {
                    "sent": "In our model.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, we're following this philosophy in a way that we're using the model to predict original expressions or the original data in text or in the observed.",
                    "label": 1
                },
                {
                    "sent": "Given structured knowledge base, we're not trying to create models of semantic equivalence, which I believe are elusive.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so now let's talk about how this is actually done mechanically, and to do this I want to step back again to provide a bit of context and draw some different pictures for you.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, remember, we're given some text.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We find some entity mentions.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We resolve these into entities.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We want to predict what there.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The types are.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Antop",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Predict what their release.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Types are.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Let's start by focusing on entity types.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And how we typically do this, at least in the natural language processing community, we would build some classifier with parameter.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For each one of these entity types that looks at.",
                    "label": 1
                },
                {
                    "sent": "Play a couple of 1,000,000 features.",
                    "label": 0
                },
                {
                    "sent": "In the neighborhood of the appearance of these mentions in text.",
                    "label": 0
                },
                {
                    "sent": "Learn those parameters and then given any particular intervention, use this classifier to predict what the type is and.",
                    "label": 1
                },
                {
                    "sent": "You know, as I said earlier, I want to use not just a few entity types that would be given in one particular.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But also use some from multiple schemas and use some from.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A positives that come from text which provide us a lot of other entity types, and so to do.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This on like I was saying about 50,000 different entity types or many more and this is what I mean.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Universal schema.",
                    "label": 0
                },
                {
                    "sent": "Someone quickly apparent issue here is that if we have 2 million parameters in each, one of these stripes, and we're trying to do this on the 50,000 types or more, this is a lot of parameters, and it gets pretty big.",
                    "label": 0
                },
                {
                    "sent": "So we'd like to have.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other parameters, so we're going to do that and these these parameters say maybe just one hundred, 200 or maybe 500 of them instead of a couple of million won't be directly in raw feature space.",
                    "label": 0
                },
                {
                    "sent": "They're going to be in some new.",
                    "label": 0
                },
                {
                    "sent": "Latent space that we're going to make up through structure.",
                    "label": 0
                },
                {
                    "sent": "We're going to discover.",
                    "label": 0
                },
                {
                    "sent": "And we are.",
                    "label": 0
                },
                {
                    "sent": "Then going to take the kind of features that we had before and embed them in the same space or here.",
                    "label": 0
                },
                {
                    "sent": "Actually what I'll talk about.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That we're going to take the entities themselves and have learned parameters associated with the entities which will be of the same dimension of vector into the similar latent space, and we're going to learn those also, but in a way you can think about these parameters is being like.",
                    "label": 0
                },
                {
                    "sent": "Like an observed feature vector and we're like like a logistic regression classifier, we're going to take a dot product between a vector here and a vector here in order to make some prediction.",
                    "label": 0
                },
                {
                    "sent": "But we're going to be usually will just learn the parameters of the classes were trying to predict, and now we're going to learn both those parameters and the parameters of how will represent our inputs.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here we are.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This matrix, and as I was just saying, will make a prediction anywhere at some intersection in this matrix by doing a dot product between the vector if each row and column and putting that through some logistic function in order to turn it to a value between zero and one.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again, we will observe someday.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we'll do some matrix completion in order to do this prediction, and I'll get into how we do this.",
                    "label": 0
                },
                {
                    "sent": "Learning in just a minute.",
                    "label": 0
                }
            ]
        },
        "clip_123": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So we also can do this for relate.",
                    "label": 0
                }
            ]
        },
        "clip_125": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Types right in which we have some of these types again embedded in some.",
                    "label": 0
                }
            ]
        },
        "clip_126": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or dimensional space for multiple given relation.",
                    "label": 0
                }
            ]
        },
        "clip_127": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Schema as well As for relations observed in.",
                    "label": 0
                }
            ]
        },
        "clip_128": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as before, for the Rosewill will have entity pair?",
                    "label": 0
                }
            ]
        },
        "clip_129": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we'll do it.",
                    "label": 0
                }
            ]
        },
        "clip_130": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Back to the same kind of.",
                    "label": 0
                }
            ]
        },
        "clip_131": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Completion in the same way, by doing a dot product between the representation for the entity pair and a relation for.",
                    "label": 0
                }
            ]
        },
        "clip_132": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An entity type.",
                    "label": 0
                },
                {
                    "sent": "Sorry for relation type.",
                    "label": 0
                },
                {
                    "sent": "Alright, so how do we do learning?",
                    "label": 0
                },
                {
                    "sent": "How do we learn all of these purple parameters?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "In Netflix, Skype problems, this matrix is filled with a movie scores.",
                    "label": 0
                },
                {
                    "sent": "How much did you like this movie from one to five and the loss function that they'll use to drive learning will?",
                    "label": 0
                }
            ]
        },
        "clip_133": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will typically be squared error.",
                    "label": 0
                },
                {
                    "sent": "The model predicted something between one to five.",
                    "label": 0
                },
                {
                    "sent": "The human said it should have been something one to five.",
                    "label": 0
                },
                {
                    "sent": "How far are you off?",
                    "label": 0
                },
                {
                    "sent": "But here we're making a binary prediction, and so instead will replace this.",
                    "label": 1
                },
                {
                    "sent": "By taking this sum is just representing essentially the mechanics of doing a dot product right and will put this through a logistic function.",
                    "label": 0
                },
                {
                    "sent": "I'm just like logistic regression.",
                    "label": 0
                },
                {
                    "sent": "This two number between zero and one and.",
                    "label": 0
                },
                {
                    "sent": "We can get a gradient from this either by log likelihood or by some ranking based objective, which I'll describe in a minute, and by this gradient some parameters we can then use some stochastic gradient descent optimized with various methods, including if it's like at a grad to nudge all of our parameters in the right direction and train until we get some kind of convergence.",
                    "label": 1
                }
            ]
        },
        "clip_134": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's talk about this more concrete in terms of some pictures instead of equations.",
                    "label": 0
                },
                {
                    "sent": "What does this look like mechanically, and how is it that we can?",
                    "label": 0
                },
                {
                    "sent": "Run this in a very scalable fashion.",
                    "label": 0
                },
                {
                    "sent": "They make a comment about scalability, so we've been running this with saying.",
                    "label": 0
                },
                {
                    "sent": "More than 300,000 rows and you know maybe 50,000 columns and we train these parameters on one machine in less than half an hour, so we're not that concerned about scalability.",
                    "label": 0
                },
                {
                    "sent": "So what does this consist of?",
                    "label": 0
                },
                {
                    "sent": "We will randomly sample sumrow.",
                    "label": 0
                },
                {
                    "sent": "And some entry within that row that's labeled.",
                    "label": 0
                },
                {
                    "sent": "That was observed that we think is positive.",
                    "label": 0
                }
            ]
        },
        "clip_135": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will randomly pick some UN observed cell in the same row anwil.",
                    "label": 0
                }
            ]
        },
        "clip_136": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Assume for the moment that that should.",
                    "label": 0
                },
                {
                    "sent": "That's a negative example, which note could be wrong, but that's just like some training noise that we can be robust to.",
                    "label": 0
                },
                {
                    "sent": "And we'll take the dot product of each for this row in this column, yielding a score for this cell.",
                    "label": 0
                },
                {
                    "sent": "Take the dot product for this row in this column.",
                    "label": 0
                },
                {
                    "sent": "Building on something here.",
                    "label": 0
                }
            ]
        },
        "clip_137": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the rank objective says, well, I would like this dot product to be larger than this dot product.",
                    "label": 0
                },
                {
                    "sent": "The positives should score higher than the negative and the extent to which that's not true.",
                    "label": 0
                },
                {
                    "sent": "We can turn into a gradient which changes the parameters for this row as well As for these two columns.",
                    "label": 0
                },
                {
                    "sent": "Essentially, bringing this row closer to this column and further away from this column and likewise the parameters for this column A little bit closer to this row and the opposite here as well, and so we just step through the matrix so this sort of sparse matrix training as we go.",
                    "label": 0
                },
                {
                    "sent": "We never need to fully populate the matrix, were only.",
                    "label": 0
                },
                {
                    "sent": "Learning these purple parameters.",
                    "label": 0
                },
                {
                    "sent": "So let's talk about.",
                    "label": 0
                }
            ]
        },
        "clip_138": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Results on real data.",
                    "label": 0
                },
                {
                    "sent": "We took the full text of 20 years of New York Times news articles, extracted entity mentions, perform density resolution.",
                    "label": 1
                },
                {
                    "sent": "Got about 350 entity pairs that appeared within close enough proximity to each other that we could claim that there was some textually expressed relation between them.",
                    "label": 1
                },
                {
                    "sent": "Here we found about 23,000 unique relational surface forms.",
                    "label": 0
                },
                {
                    "sent": "I mean unique relational textual patterns that describe relations.",
                    "label": 0
                },
                {
                    "sent": "We also went to Freebase and got about 6000 entity pairs that were resolved against these entity.",
                    "label": 0
                },
                {
                    "sent": "These New York Times entities and about we just chose about 160 relations from Freebase.",
                    "label": 0
                }
            ]
        },
        "clip_139": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then did some training and held out some separate test data to do some to assess our accuracy.",
                    "label": 0
                },
                {
                    "sent": "And here are some results that we got.",
                    "label": 0
                },
                {
                    "sent": "So actually on this data set UMass and University of Washington and Stanford and others have been leapfrogging each other over the years here.",
                    "label": 0
                },
                {
                    "sent": "And so here's what our results looked like in 2001 by the distance supervision method that I described earlier, we're getting about an average of 48.",
                    "label": 0
                },
                {
                    "sent": "Percent this unsupervised clustering method brought things up a bit.",
                    "label": 0
                },
                {
                    "sent": "Stanford had been working with a fancy distance supervision method that also got some nice results, but Universal Schema made a big jump that we're very proud of.",
                    "label": 0
                },
                {
                    "sent": "We're very, very happy with these results.",
                    "label": 0
                },
                {
                    "sent": "We've.",
                    "label": 0
                }
            ]
        },
        "clip_140": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have been applying this to entity types as I described earlier as well and a straight up traditional entity type classifier on a particular fine grained entity type prediction task was getting results like this around 55 method from University of Washington was doing better like this and universal schema did did better still.",
                    "label": 0
                },
                {
                    "sent": "So I think there's a lot more work to be done.",
                    "label": 0
                },
                {
                    "sent": "But I think there are some promising early signs.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let's take a breath.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_141": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're about to go into the next major section.",
                    "label": 0
                },
                {
                    "sent": "Let me let me pause.",
                    "label": 0
                },
                {
                    "sent": "Let's see I have to say that.",
                    "label": 0
                },
                {
                    "sent": "To wag my finger at an issue so that the information retrieval community did a little better at asking questions along the way, maybe I've just been speaking too fast and not letting you get a word in edgewise.",
                    "label": 0
                },
                {
                    "sent": "Let me pause to see if you have some questions before I continue.",
                    "label": 0
                },
                {
                    "sent": "We can also take questions at the end, but.",
                    "label": 0
                },
                {
                    "sent": "Yes, thank you.",
                    "label": 0
                },
                {
                    "sent": "Good OK yeah.",
                    "label": 0
                },
                {
                    "sent": "So the question is how is it that Universal Schema learns to do this asymmetric asymmetrical implicature?",
                    "label": 0
                },
                {
                    "sent": "No historian implies professor, but professor doesn't necessarily imply historian.",
                    "label": 0
                },
                {
                    "sent": "Our I'm going to talk more about this in a bit when I talk about the this second bullet in this triplet here with Gaussians, but.",
                    "label": 0
                },
                {
                    "sent": "I believe that it has to do with vectors of different magnitudes and essentially the overall magnitude of the vector.",
                    "label": 0
                },
                {
                    "sent": "For Professor is larger, which represents the notion that it's a more general concept.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "But actually after I talk about Gaussians, ask your question again, if if you'd still like to know more.",
                    "label": 0
                },
                {
                    "sent": "Wonderful, yes thank you.",
                    "label": 0
                },
                {
                    "sent": "What is the contribution of your prior structure knowledge versus when you learn from language in a war against unorganized but yeah.",
                    "label": 0
                },
                {
                    "sent": "Good, so the question is noting that we're training on both the raw text as well as this pre structured data and what's the contribution of training on the pre structured data we could train with either one and not the other and still get some good and useful results.",
                    "label": 0
                },
                {
                    "sent": "We and others have done a lot of training on actually just the structured data and we can essentially do knowledge base completion and see cases where relation should have been there but were left out accidentally.",
                    "label": 0
                },
                {
                    "sent": "You can also train on just the text and say well which textual phrases.",
                    "label": 0
                },
                {
                    "sent": "Predict which others.",
                    "label": 0
                },
                {
                    "sent": "What I like about trading them both together is that we then affectively seeing how text and these structured schema align with each other and get mutually implicature among those as well.",
                    "label": 0
                },
                {
                    "sent": "But the training method will work fine either way.",
                    "label": 0
                },
                {
                    "sent": "Let me take now there are questions.",
                    "label": 0
                },
                {
                    "sent": "This is fantastic.",
                    "label": 0
                },
                {
                    "sent": "I'm so happy.",
                    "label": 0
                },
                {
                    "sent": "I think I'll take two more questions and then continue and then I'll take more questions at the end and I think I first saw question back in this table and then will come forward.",
                    "label": 0
                },
                {
                    "sent": "Yes please.",
                    "label": 0
                },
                {
                    "sent": "OK, so the question is how would we deal with conflicting information in our knowledge base?",
                    "label": 0
                },
                {
                    "sent": "Let's see here.",
                    "label": 0
                },
                {
                    "sent": "I think I could answer that in a number of ways, jumping again if I've taken your question in the wrong sense.",
                    "label": 0
                },
                {
                    "sent": "When we observe raw data in green squares that I was drawing in the matrix earlier, yes, it's actually the case that there could be errors there.",
                    "label": 0
                },
                {
                    "sent": "Or there could be conflicts and to us from a machine learning perspective, this just looks like noisy training data, which many machine learning algorithms are used to dealing with, and the hope is that.",
                    "label": 0
                },
                {
                    "sent": "Right, the learning capacity of the of the model and its capacity for regularization helps.",
                    "label": 0
                },
                {
                    "sent": "Smooth over these errors and discover the kinds of regularity's that actually should be reflected in the truth and recognize when something was noisy.",
                    "label": 0
                },
                {
                    "sent": "The model's ability to actually do that successfully?",
                    "label": 0
                },
                {
                    "sent": "Uh, is an empirical question, and it certainly doesn't do it all always perfectly, but that's my sense.",
                    "label": 0
                },
                {
                    "sent": "OK, last question right here, thank you.",
                    "label": 0
                },
                {
                    "sent": "Knowledge.",
                    "label": 0
                },
                {
                    "sent": "Right, yeah?",
                    "label": 0
                },
                {
                    "sent": "So the question is, do we consider the original knowledge base to be the truth and or do we also consider that to have uncertainty?",
                    "label": 0
                },
                {
                    "sent": "And the answer is.",
                    "label": 0
                },
                {
                    "sent": "Yes, through training perspective anyway, we consider it to be the truth and you know similarly to what answer before they can deal with noise.",
                    "label": 0
                },
                {
                    "sent": "If there were, if we were presented with uncertainties or confidence is in that, we could absolutely leverage that in these training methods, and that would be an exciting direction to take absolutely.",
                    "label": 0
                },
                {
                    "sent": "Alright, I'm so glad to see so many hands.",
                    "label": 0
                },
                {
                    "sent": "I'm looking forward to more questions at the end.",
                    "label": 0
                },
                {
                    "sent": "Let me jump ahead so.",
                    "label": 0
                }
            ]
        },
        "clip_142": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this picture, we've been exploring some new avenues recently that I'm excited about, and I want to give you a brief tour of three of those and the first is multi sense representation.",
                    "label": 0
                },
                {
                    "sent": "So sometimes you could observe a piece of text like Microsoft beat novel.",
                    "label": 0
                },
                {
                    "sent": "And the word beat can have multiple senses, though, right?",
                    "label": 0
                },
                {
                    "sent": "It could mean to dominate in a business sense.",
                    "label": 0
                },
                {
                    "sent": "Come to fisticuffs or maybe even one politician beating another politician has different implications than one company beating another company and we'd like to represent these separately.",
                    "label": 0
                },
                {
                    "sent": "So how can we?",
                    "label": 0
                },
                {
                    "sent": "How can we do this?",
                    "label": 0
                },
                {
                    "sent": "We've been.",
                    "label": 0
                },
                {
                    "sent": "We started to think about this problem.",
                    "label": 0
                },
                {
                    "sent": "Thank you not in.",
                    "label": 0
                },
                {
                    "sent": "Not yet with Universal Schema.",
                    "label": 0
                },
                {
                    "sent": "Entity types in relations, but thinking about word embeddings and then we're eventually will get back to.",
                    "label": 0
                }
            ]
        },
        "clip_143": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The types and two recently very popular methods for learning wording meetings are called SIBO and Skip gram, and these work by assigning to each word in the dictionary some unique latent vector in space.",
                    "label": 0
                },
                {
                    "sent": "And we would say, you know Sebo would take the.",
                    "label": 0
                },
                {
                    "sent": "The average of.",
                    "label": 0
                },
                {
                    "sent": "The words in the context of some central word in a running stream of text.",
                    "label": 0
                },
                {
                    "sent": "Here the automobile plant manufacturing said ends take the average of the context and considered the dot product between that average and a central word.",
                    "label": 0
                },
                {
                    "sent": "An ask that that rank higher than the dot product between the central word in some other randomly chosen dictionary word that's not properly in the context and use that to get some gradient signal to learn.",
                    "label": 0
                },
                {
                    "sent": "Parameters here skip gram does something similar without bothering to do with the average.",
                    "label": 0
                }
            ]
        },
        "clip_144": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when we you know, we can really efficiently train models like this, say I'm 2 billion words of text in about 8 hours on machine.",
                    "label": 0
                },
                {
                    "sent": "And when you do this, you.",
                    "label": 0
                }
            ]
        },
        "clip_145": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just some lovely.",
                    "label": 0
                },
                {
                    "sent": "You look at where these words are positioned in vector space.",
                    "label": 0
                },
                {
                    "sent": "Here I'm taking say 200 dimensional space, projecting it down to do and you.",
                    "label": 0
                }
            ]
        },
        "clip_146": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some very nice semantic relations in which cash, stock and share are near.",
                    "label": 0
                }
            ]
        },
        "clip_147": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each other, well, service operations and products are near each other, and Furthermore we can notice.",
                    "label": 0
                }
            ]
        },
        "clip_148": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Only that.",
                    "label": 0
                },
                {
                    "sent": "Names of countries tend to cluster together and names of cities clustered together, but even the vector distance between a country and its capital.",
                    "label": 1
                },
                {
                    "sent": "That vector displacement seems to be similar for different countries and their capital so that we could, say take this vector from China to Beijing and add that vector to Portugal and end up in the neighborhood of Lisbon and so.",
                    "label": 0
                }
            ]
        },
        "clip_149": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can get results like noticing that that we can do this vector arithmetic in this space take.",
                    "label": 0
                }
            ]
        },
        "clip_150": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take Paris, subtract France ad Rome asked what am I closest to now and get Italy.",
                    "label": 0
                }
            ]
        },
        "clip_151": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or take sushi, subtract Japan at Germany as one of my closest to now and you get Bratwurst.",
                    "label": 0
                },
                {
                    "sent": "So this is fun and we have.",
                    "label": 0
                }
            ]
        },
        "clip_152": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using.",
                    "label": 0
                },
                {
                    "sent": "Both kinds of models to create a model that helps us reason about multiple senses.",
                    "label": 0
                },
                {
                    "sent": "By putting these two things that are usually run separately.",
                    "label": 0
                }
            ]
        },
        "clip_153": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Together, so here we'll take a word like plant and capture the notion that we could have multiple different senses, and each will have multiple different vector representations.",
                    "label": 0
                },
                {
                    "sent": "We will take the context to take the average, find the sense that is closest to that average.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Train that sense but not the others according to context.",
                    "label": 0
                },
                {
                    "sent": "Using skip Gram and Furthermore if.",
                    "label": 0
                }
            ]
        },
        "clip_154": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We look at all the sensors, find that none of them are particularly close.",
                    "label": 0
                },
                {
                    "sent": "We will go ahead and.",
                    "label": 0
                }
            ]
        },
        "clip_155": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Create a brand new sense so that in a nonparametric sense, we can create as many senses as the data suggests, and some words may have just one since some may have two so may have 17.",
                    "label": 0
                }
            ]
        },
        "clip_156": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And he.",
                    "label": 0
                }
            ]
        },
        "clip_157": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some examples we run this Sunday to a traditional method that asks what are the nearest neighbors of the word cell with only one sense, we get things like this.",
                    "label": 0
                }
            ]
        },
        "clip_158": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But when we allow the method to have two senses, we get both.",
                    "label": 0
                },
                {
                    "sent": "Cell as well as the spreadsheet version of cell.",
                    "label": 0
                }
            ]
        },
        "clip_159": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another example for sin, and here are words that we can anybody guess.",
                    "label": 0
                },
                {
                    "sent": "What another sense of sin might be?",
                    "label": 0
                },
                {
                    "sent": "It's the.",
                    "label": 0
                }
            ]
        },
        "clip_160": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mathematical sense of sin pronounced differently.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_161": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are some examples that show the nonparametric version when we actually run it on real data for plants actually decides.",
                    "label": 0
                },
                {
                    "sent": "To to have a four senses of plant one.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Common usage in conversational sense of how we talk about plants as well as the scientific biological sense, as well as manufacturing as well as.",
                    "label": 0
                },
                {
                    "sent": "Electricity generation.",
                    "label": 0
                }
            ]
        },
        "clip_162": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are some other examples, like for Fox, it discovers two senses, both the animal as well as the television.",
                    "label": 0
                },
                {
                    "sent": "So we.",
                    "label": 0
                }
            ]
        },
        "clip_163": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ran this on some data and compared it to some previous work from Stanford.",
                    "label": 0
                },
                {
                    "sent": "In a way that runs more quickly and that is able to do this in a non parametric way and.",
                    "label": 0
                }
            ]
        },
        "clip_164": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Got some very nice results on some data actually produced by the by the Stanford folks on a data set called Contextual Word similarities where the previous results like this and then we're getting something close to 60.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_165": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One piece of fun that we're having and like I say one of the next things we want to do is then apply this to relations in entity types.",
                    "label": 0
                },
                {
                    "sent": "Alright, next in my.",
                    "label": 0
                }
            ]
        },
        "clip_166": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of three things, Gaussian embeddings, so we.",
                    "label": 0
                }
            ]
        },
        "clip_167": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These word embeddings that we've been talking about, and they've been really useful.",
                    "label": 1
                },
                {
                    "sent": "Actually, there's been a blossoming of interest in these in natural language processing.",
                    "label": 0
                },
                {
                    "sent": "The extent to which, at the most recent conference, empirical methods in natural language processing MLP.",
                    "label": 0
                },
                {
                    "sent": "The common running joke through the conference was that E was standing now for embeddings.",
                    "label": 0
                },
                {
                    "sent": "And so they've been using low level NLP and we and others have used them to get new state of the art results in named entity extraction.",
                    "label": 1
                },
                {
                    "sent": "They've been used for machine translation and very exciting ways.",
                    "label": 1
                },
                {
                    "sent": "Been used for question answering and so on.",
                    "label": 0
                }
            ]
        },
        "clip_168": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But what's missing?",
                    "label": 0
                },
                {
                    "sent": "So one thing I would claim as a notion of breath, like here we have a person and a musical composer and they have positions in space.",
                    "label": 0
                },
                {
                    "sent": "But a person is a more broad concept, more general concept in a musical composer, something more specific.",
                    "label": 0
                },
                {
                    "sent": "It would be nice to somehow represent that.",
                    "label": 0
                },
                {
                    "sent": "And we're also missing a notion of asymmetry in ways that will describe in a minute.",
                    "label": 0
                },
                {
                    "sent": "So we would like it to be.",
                    "label": 0
                }
            ]
        },
        "clip_169": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Represent concepts like this right with instead of a vector point in space.",
                    "label": 0
                },
                {
                    "sent": "A Gaussian region in space.",
                    "label": 0
                },
                {
                    "sent": "So that we can capture the notion that a person is a broad concept and the composer is more narrow and sits inside of it.",
                    "label": 0
                },
                {
                    "sent": "And actually, these drawings that are about to show you actually come from real data when we trained on the raw text of Wikipedia.",
                    "label": 0
                }
            ]
        },
        "clip_170": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We find that their words like famous or actually or classical also appears similarly to this that crosscuts these concepts and.",
                    "label": 0
                }
            ]
        },
        "clip_171": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Johann Sebastian Bach appears right at the intersection in a very nice way, just the way that I would like.",
                    "label": 0
                },
                {
                    "sent": "So this just emerges unsupervised from training on raw text.",
                    "label": 0
                },
                {
                    "sent": "So what does this look like?",
                    "label": 0
                }
            ]
        },
        "clip_172": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mechanically, for each word we have a vector representing the mean of the.",
                    "label": 0
                }
            ]
        },
        "clip_173": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_174": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also another vector.",
                    "label": 0
                }
            ]
        },
        "clip_175": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Representing the diagonal of the covariance.",
                    "label": 0
                },
                {
                    "sent": "We're currently using a diagonal, but we could do something more expressively.",
                    "label": 0
                }
            ]
        },
        "clip_176": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this covariance matrix captures the breadth and four.",
                    "label": 0
                },
                {
                    "sent": "You know we can write down with this what this means.",
                    "label": 0
                }
            ]
        },
        "clip_177": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Way that captures both allowed rhythmic penalty for the volume due to normalization as well as the Mahalanobis distance that sort of obeys the covariances as it considers measuring distances and for a simile.",
                    "label": 0
                }
            ]
        },
        "clip_178": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe we can use collect Leibler divergent switches and asymmetric distance measure, which were often used to in natural language processing information retrieval running on discrete distributions.",
                    "label": 0
                },
                {
                    "sent": "But you can absolutely run on continue.",
                    "label": 0
                }
            ]
        },
        "clip_179": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distributions, oops, sorry.",
                    "label": 0
                },
                {
                    "sent": "Can just continuous distributions like Gaussians as well to get an asymmetric distance?",
                    "label": 0
                }
            ]
        },
        "clip_180": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how do we train this?",
                    "label": 0
                }
            ]
        },
        "clip_181": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By method very much like Skip grammar will start with some central word in some sequence, like composer.",
                    "label": 0
                },
                {
                    "sent": "Will compare.",
                    "label": 0
                }
            ]
        },
        "clip_182": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "Compatibility with some true word word that it cures it.",
                    "label": 0
                },
                {
                    "sent": "It's true local.",
                    "label": 0
                }
            ]
        },
        "clip_183": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As well as its compatibility with some random dictionary.",
                    "label": 0
                }
            ]
        },
        "clip_184": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a negative example and we want.",
                    "label": 0
                }
            ]
        },
        "clip_185": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sort of like the energy of the positive pair to be greater than the energy of the negative pair.",
                    "label": 0
                }
            ]
        },
        "clip_186": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can calculate that energy well before we've been doing it by a dot product written here.",
                    "label": 0
                },
                {
                    "sent": "And we can do something very similar.",
                    "label": 0
                }
            ]
        },
        "clip_187": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With Gaussians where we'll do an integral instead of a discrete sum.",
                    "label": 0
                }
            ]
        },
        "clip_188": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this integral.",
                    "label": 0
                }
            ]
        },
        "clip_189": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With.",
                    "label": 0
                }
            ]
        },
        "clip_190": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gaussians.",
                    "label": 0
                }
            ]
        },
        "clip_191": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looks like this set has a very nice clothes for.",
                    "label": 0
                }
            ]
        },
        "clip_192": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_193": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's easy to calculate and even have pieces that are pretty interpretable.",
                    "label": 0
                }
            ]
        },
        "clip_194": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So then.",
                    "label": 0
                }
            ]
        },
        "clip_195": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Loss function is to say, well, we want to maximize.",
                    "label": 0
                },
                {
                    "sent": "Since they want the positive energy to be larger than the negative energy, and again we can turn this into a gradient to learn parameters of both the mean and the variance of these Gaussians.",
                    "label": 0
                },
                {
                    "sent": "So let me show you give you a sense of what kind of results this gives us in some real.",
                    "label": 0
                }
            ]
        },
        "clip_196": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that in which we ran some benchmark results against the previous state of the art?",
                    "label": 0
                },
                {
                    "sent": "There has been some work by Barone and others in natural language processing to capture some sense of entailment.",
                    "label": 0
                },
                {
                    "sent": "Among words that express categories where positive examples in this data set are things like adrenaline is a neurotransmitter, Archbishop is a type of clergymen, horses, a type of mammal, and so on and then negative cases or some random.",
                    "label": 0
                }
            ]
        },
        "clip_197": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's like an air crew is not a playlist as well.",
                    "label": 0
                }
            ]
        },
        "clip_198": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As reversed negative cases like, although pizza is a food, food is not a pizza.",
                    "label": 0
                },
                {
                    "sent": "It's not always necessarily a pizza.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_199": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we experimented with both his agonal covariance version of our model as well as a spherical covariance version we trained on about a billion words of raw text from Wikipedia, in addition to 3 billion tokens of newswire, and we evaluated on the optimal F1 operating points.",
                    "label": 0
                }
            ]
        },
        "clip_200": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Average position and Barone, which was the previous state of the art, was getting numbers like this and we were getting numbers almost 80, so it's not a huge leap, but a step in the right direction thing we're happy with.",
                    "label": 0
                }
            ]
        },
        "clip_201": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, now take to close let me.",
                    "label": 0
                }
            ]
        },
        "clip_202": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk about something that I think might be especially relevant to this Community because I know.",
                    "label": 0
                },
                {
                    "sent": "I think I want to be able to talk about not just representation but reasoning, and in a way that kind of entailment I just described as a simple kind of reasoning, but I'd like to do something a little bit more rich, and I think it's not going to count as truly rich by your standards, but it's a small step by from an NLP persons POV.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "I'd like to.",
                    "label": 0
                }
            ]
        },
        "clip_203": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be able to say well what's the nature of the relationship between Melinda Gates here in Seattle?",
                    "label": 0
                },
                {
                    "sent": "And I observed no raw direct evidence of what that relationship would be.",
                    "label": 0
                },
                {
                    "sent": "But I have that I have a spouse relation to Bill and the Chairman relation to Microsoft and headquartered in relation to Seattle.",
                    "label": 0
                },
                {
                    "sent": "Maybe somehow I can use that to infer.",
                    "label": 0
                }
            ]
        },
        "clip_204": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What this relationship should be here by some sort of multihop reasoning is through this path.",
                    "label": 0
                },
                {
                    "sent": "So this should be very familiar to you, right?",
                    "label": 0
                },
                {
                    "sent": "So we can absolutely we could write down some rules.",
                    "label": 0
                }
            ]
        },
        "clip_205": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It says well, if we have spell Sabian Chairman and headquarters in all in the chain that I can infer lives in relationship between the two endpoints.",
                    "label": 0
                },
                {
                    "sent": "That's good.",
                    "label": 0
                }
            ]
        },
        "clip_206": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_207": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is exactly the kind of work that's been done in NLP by Neil out to Carnegie Mellon who's now at Google.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, that's just one particular chain.",
                    "label": 0
                }
            ]
        },
        "clip_208": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I.",
                    "label": 0
                },
                {
                    "sent": "Not I may observe CEO instead of CEO, and so maybe I need a rule for that also.",
                    "label": 0
                },
                {
                    "sent": "Or sorry, CEO or.",
                    "label": 0
                }
            ]
        },
        "clip_209": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe observe CEO also will need another rule for that.",
                    "label": 0
                },
                {
                    "sent": "And well, maybe not.",
                    "label": 0
                }
            ]
        },
        "clip_210": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spell so, but also a child of could I could make some inferences from that as well.",
                    "label": 0
                }
            ]
        },
        "clip_211": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And well, maybe not just even lives in, but also other kinds of relations.",
                    "label": 0
                },
                {
                    "sent": "I might end up predicting, and this is starting to get painful, and so I'd like to do something more general and.",
                    "label": 0
                },
                {
                    "sent": "No, I'm noting that.",
                    "label": 0
                }
            ]
        },
        "clip_212": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Already we've been talking about vector representations for all of these things, and in fact we've been thinking quite a bit about.",
                    "label": 0
                },
                {
                    "sent": "A representation of knowledge base that actually doesn't really store symbols at all.",
                    "label": 0
                },
                {
                    "sent": "It just associated with each node.",
                    "label": 0
                },
                {
                    "sent": "Instead of storing, say that you know the list of entity types that belong to this node as a list of symbols representing those entity types will just store a vector representation along with that node.",
                    "label": 0
                },
                {
                    "sent": "That representation is responsible for answering all questions about what entity types this belongs to.",
                    "label": 0
                },
                {
                    "sent": "By Gaussian containment or by something else, and similarly for these kinds of.",
                    "label": 0
                },
                {
                    "sent": "Relations each edge won't have a symbol on it.",
                    "label": 0
                },
                {
                    "sent": "It will have a vector on it instead.",
                    "label": 0
                },
                {
                    "sent": "And now I'd like to be able to reason.",
                    "label": 0
                },
                {
                    "sent": "Not about symbols, but about vectors with something that looks like logic.",
                    "label": 0
                },
                {
                    "sent": "But what would that be?",
                    "label": 0
                },
                {
                    "sent": "Or how can we do that?",
                    "label": 0
                },
                {
                    "sent": "And so we've taken some initial baby steps into what that might look like with.",
                    "label": 0
                }
            ]
        },
        "clip_213": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Following mechanism, we've been using a recurrent neural network.",
                    "label": 0
                },
                {
                    "sent": "Which consumes the vectors, the vector representation of the semantics of the relations along this path consumes them all in order and at the end out pops from the network.",
                    "label": 0
                },
                {
                    "sent": "A vector representation of.",
                    "label": 0
                },
                {
                    "sent": "Of the semantics of the relation between the two endpoints of that path, and indeed this network produces a vector who that is very close to the vector representation of the symbol lives in.",
                    "label": 0
                },
                {
                    "sent": "And so in case we don't know too much about these recurrent neural networks, how do they work right, you take.",
                    "label": 0
                },
                {
                    "sent": "The vector representation of whatever context you had so far concatenated with the vector representation for the next link in the chain that you're about to consume.",
                    "label": 0
                },
                {
                    "sent": "It goes through one or multiple layers of interpretation and think about, well, how should I compose these two things together produces a new representation of the composition of these things so far, and that composition then gets fed back into the bottom, saying, well, here's my context so far from the chain, the arbitrary length chain so far, and now I'm going to check on this one additional piece of evidence, and I'll produce a new step in the composition.",
                    "label": 0
                }
            ]
        },
        "clip_214": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this was published at just this most recent ACL conference.",
                    "label": 0
                }
            ]
        },
        "clip_215": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so there's related work as I said by Neil OW, using rules on symbols.",
                    "label": 1
                },
                {
                    "sent": "Gardner also at CMU's been doing some work.",
                    "label": 0
                },
                {
                    "sent": "In addition, on relational vectors, where the relational vectors are alert learned offline that also has some nice results.",
                    "label": 0
                }
            ]
        },
        "clip_216": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we've been running this on about 18 million entities, about 40 million Freebase triples, as well as 12 million triples that come from the text of the clue web text collection, which is a broad web crawl produced by Carnegie Mellon.",
                    "label": 0
                },
                {
                    "sent": "With about 25,000 different relation types.",
                    "label": 1
                },
                {
                    "sent": "And here I just want to get a sense of what this produces.",
                    "label": 0
                },
                {
                    "sent": "Some examples of some predictive paths that emerged.",
                    "label": 0
                }
            ]
        },
        "clip_217": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From our model.",
                    "label": 0
                }
            ]
        },
        "clip_218": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we'd like to predict the place of birth relation between a person and a location.",
                    "label": 0
                }
            ]
        },
        "clip_219": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we found that.",
                    "label": 0
                },
                {
                    "sent": "Observing the text was born in with the mailing address.",
                    "label": 1
                },
                {
                    "sent": "These are all free based relations.",
                    "label": 0
                },
                {
                    "sent": "I don't know how use you are reading Freebase, but the mailing address, city, town relation and then that's that that occurs in some province.",
                    "label": 0
                },
                {
                    "sent": "Region captures that this person was born in that province.",
                    "label": 0
                }
            ]
        },
        "clip_220": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or if user the text from and then that some location contains another location, that's another kind.",
                    "label": 0
                }
            ]
        },
        "clip_221": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Difference you can make.",
                    "label": 0
                },
                {
                    "sent": "These are paths that were.",
                    "label": 0
                },
                {
                    "sent": "Seen in Arad training data right, they weren't labeled cousin away.",
                    "label": 0
                },
                {
                    "sent": "All of its unlabeled, but but they were seen.",
                    "label": 0
                },
                {
                    "sent": "Here's another path that wasn't seen in the training data all, but was discovered by generalization and just neighborhood of these.",
                    "label": 0
                },
                {
                    "sent": "Vector positions that the text born in and near can be chained together together to infer this relation.",
                    "label": 0
                },
                {
                    "sent": "OK, this is, I think of this simple example for me.",
                    "label": 0
                }
            ]
        },
        "clip_222": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Show you something just slightly more.",
                    "label": 0
                }
            ]
        },
        "clip_223": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Complicated, what if we're trying to predict?",
                    "label": 0
                },
                {
                    "sent": "Books, original language.",
                    "label": 0
                },
                {
                    "sent": "The original language in which a book was written.",
                    "label": 0
                }
            ]
        },
        "clip_224": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we have a book was.",
                    "label": 0
                },
                {
                    "sent": "Here's something in the previous series to which this book belong, and the author of that book has this nationality and another person who has the same nationality speaks this language.",
                    "label": 0
                }
            ]
        },
        "clip_225": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This book was written by this author who has this ethnicity and people who have this ethnicity speak this language.",
                    "label": 0
                }
            ]
        },
        "clip_226": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This book was written by this author who addressed sort of spoke to people of this nationality who speak this.",
                    "label": 0
                }
            ]
        },
        "clip_227": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "English.",
                    "label": 0
                },
                {
                    "sent": "Where this book takes place in a certain location, this location has people of this nationality who speak this language, so all these were automatically discovered from a combination of text and structured inferences.",
                    "label": 0
                }
            ]
        },
        "clip_228": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_229": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we ran on some data, here are some results that we got, so the former original method that was doing it.",
                    "label": 0
                }
            ]
        },
        "clip_230": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On top of symbols.",
                    "label": 0
                },
                {
                    "sent": "I was getting some results like this.",
                    "label": 0
                }
            ]
        },
        "clip_231": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Aurora cursive Neural Network did a little better.",
                    "label": 0
                }
            ]
        },
        "clip_232": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About 7% better, not hugely, but again a little something.",
                    "label": 0
                }
            ]
        },
        "clip_233": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We were using this as a baseline, but then we actually made a change to the baseline that's trained it more the way we would a classifier and and the new baseline we created was actually significantly better.",
                    "label": 0
                },
                {
                    "sent": "And then we combine.",
                    "label": 0
                }
            ]
        },
        "clip_234": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our new baseline with RNN.",
                    "label": 0
                }
            ]
        },
        "clip_235": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We did better still about 15% above the original estimate.",
                    "label": 0
                }
            ]
        },
        "clip_236": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "I think this is a little bit hard to explain, but I just briefly as well as a near final comment.",
                    "label": 0
                },
                {
                    "sent": "This method can also make predictions about how to chain or compose relations that it never saw at all at training time what's often called zero shot learning.",
                    "label": 0
                },
                {
                    "sent": "So we will have learned.",
                    "label": 0
                },
                {
                    "sent": "Not like an embedding for this relation, but never at training time did it even try to reason about how this relation would be composed with others, but because we had learned how to compose other relations, we also sort of in the smooth continuous space, figured out how to compose this new relation.",
                    "label": 0
                },
                {
                    "sent": "We have never seen before, and you know, it's 20% is not great accuracy, but it's better than random and I just find that interesting.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_237": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm let me just wrap up by saying some things that we're now looking at in the future.",
                    "label": 0
                },
                {
                    "sent": "We would like to also think about events.",
                    "label": 0
                }
            ]
        },
        "clip_238": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which we've been thinking about in.",
                    "label": 0
                }
            ]
        },
        "clip_239": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So have a neo davidsonian.",
                    "label": 0
                }
            ]
        },
        "clip_240": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of way, which I think is the same way that you often do in this community.",
                    "label": 0
                },
                {
                    "sent": "Create a node to represent that event and.",
                    "label": 0
                }
            ]
        },
        "clip_241": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's many arguments off of that.",
                    "label": 0
                }
            ]
        },
        "clip_242": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Single node and we can do inferences and use embeddings in similar ways here.",
                    "label": 0
                }
            ]
        },
        "clip_243": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we would also like to reason about multilingual data, right?",
                    "label": 0
                },
                {
                    "sent": "So far we've just been talking about English, but in a way these columns are already learning synonyms.",
                    "label": 0
                },
                {
                    "sent": "In both symbols.",
                    "label": 0
                },
                {
                    "sent": "We can't just stick.",
                    "label": 0
                }
            ]
        },
        "clip_244": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A whole bunch of language in those columns and learn that.",
                    "label": 0
                },
                {
                    "sent": "Here's a Spanish phrase that it's a synonym for an English phrase, and we're actually already getting some very nice good results here.",
                    "label": 0
                }
            ]
        },
        "clip_245": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_246": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So with that close, here are the things that we've been talking about here.",
                    "label": 0
                }
            ]
        },
        "clip_247": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Summary, and I've been very happy to take your questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Hi, this is Michael, a shield from semantic arts consultant in industry.",
                    "label": 0
                },
                {
                    "sent": "Just wondering how the research pipeline goes from this kind of good research to product says you know information extraction is already a cottage industry.",
                    "label": 0
                },
                {
                    "sent": "So is this already in a product?",
                    "label": 0
                },
                {
                    "sent": "Or when might we see something like this?",
                    "label": 0
                },
                {
                    "sent": "Thank you yeah.",
                    "label": 0
                },
                {
                    "sent": "So there's been let me step back a little bit from the specifics here to just generally learning embeddings and deep neural networks have are.",
                    "label": 0
                },
                {
                    "sent": "Widely used in places like Google and there are multiple startup companies including Richard Socher, whose previous work I described earlier as a startup company that he just started within the last year or so.",
                    "label": 0
                },
                {
                    "sent": "And these methods have been highly successful in making real word world impact in products.",
                    "label": 0
                },
                {
                    "sent": "As for Universal Schema, I don't know of this being used in industry.",
                    "label": 0
                },
                {
                    "sent": "At least in a shipping product yet.",
                    "label": 0
                },
                {
                    "sent": "But if you're interested in exploring this, I'd be happy to talk with you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't think I have much more insight there.",
                    "label": 0
                },
                {
                    "sent": "And yes, so let me say that actually, so Michael mentioned very kindly at the beginning.",
                    "label": 0
                },
                {
                    "sent": "This mallet toolkit that I started producing in 2002 when I first arrived at UMass, that I'm very happy has been broadly useful and is widely used.",
                    "label": 0
                },
                {
                    "sent": "About 2009, we found that our own research was.",
                    "label": 0
                },
                {
                    "sent": "Needing more flexibility than the kind of classification and linear chain sequence prediction and a little bit of topic modeling that Mallett was producing, and so I embarked on building a new tool kit which I call factory ending in IE instead of Y, which is a general graphical model toolkit with a full stack of NLP.",
                    "label": 0
                },
                {
                    "sent": "Components sitting on top of it and yeah, so it's now been in development since 2009 is released under Apache Open Source Toolkit and license.",
                    "label": 0
                },
                {
                    "sent": "And yeah, it is being broadly used, actually multiple places in industry and in shipping products.",
                    "label": 0
                },
                {
                    "sent": "And embedding, learning, and not all but multiple of the components of what I described today have been implemented in factory.",
                    "label": 0
                },
                {
                    "sent": "Yeah, further questions.",
                    "label": 0
                },
                {
                    "sent": "Hi thanks a lot.",
                    "label": 0
                },
                {
                    "sent": "My name is Abby Bernstein from University of Zurich.",
                    "label": 0
                },
                {
                    "sent": "Thanks a lot for this wonderful glimpse into a wonderfully interesting world world.",
                    "label": 0
                },
                {
                    "sent": "Sorry I had two thoughts until questions that I wanted to ask.",
                    "label": 0
                },
                {
                    "sent": "First of all, you showed us this big kind of integrated pipeline of many things in this kind of huge knowledge base where you feed a lot of information back and forth and things bubble in the in the in.",
                    "label": 0
                },
                {
                    "sent": "The reasoning.",
                    "label": 0
                },
                {
                    "sent": "I was wondering, since you have such an integrated system where things go back and forth, how do you prevent the system from oscillate?",
                    "label": 0
                },
                {
                    "sent": "And kind of continuously reasoning and oscillating around some stable point.",
                    "label": 0
                },
                {
                    "sent": "There was one question and the other question I had is it seems to me that you did not take into regard the questions that get ends as to the knowledge base.",
                    "label": 0
                },
                {
                    "sent": "Can't you kind of extract information from the questions that you could actually use to inform you?",
                    "label": 0
                },
                {
                    "sent": "Know the reasoning an actually extract you know an informed the reasoning and informed the building of the models inside of this whole bubbly thing, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah, good.",
                    "label": 0
                },
                {
                    "sent": "Let's see.",
                    "label": 0
                },
                {
                    "sent": "So the second question first.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'm not sure what you mean by extracting.",
                    "label": 0
                },
                {
                    "sent": "Maybe hold on to the microphone, 'cause I'm not sure what you mean by you thinking that we would get new facts from the questions themselves.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's true.",
                    "label": 0
                },
                {
                    "sent": "That's not something that we had considered so much.",
                    "label": 0
                },
                {
                    "sent": "Yes, this time in this relation in the resource bounded information extraction, we have thought a little bit about.",
                    "label": 0
                },
                {
                    "sent": "However, in response to a certain question that would seem to rely on some missing holes in our knowledge base, how we might on the fly go out to the web to try to fill those holes in response to those questions.",
                    "label": 0
                },
                {
                    "sent": "But yeah, I think there's.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of work to be done, instruction, and when we barely touched on it, then first question was about oscillations in our inference process.",
                    "label": 0
                },
                {
                    "sent": "Yes, I have a few things to say about that.",
                    "label": 0
                },
                {
                    "sent": "One is that a lot of the grand picture that I described, we have built some early versions of this and we've been working on many components, but should clarify that we're far from having a full, robust version of this entire thing working all together.",
                    "label": 0
                },
                {
                    "sent": "Mostly the concrete that we work that we've done on never ending constantly bubbling inference in knowledge base that the mechanisms that we've been doing to do this inference have been Markov chain Monte Carlo processes.",
                    "label": 0
                },
                {
                    "sent": "So we consider the knowledge base to be storing some current estimate of what the truth is, and we are constantly visiting subsets of the random variables in this knowledge base, and considering making some random or some some proposed change to our estimate of the truth.",
                    "label": 0
                },
                {
                    "sent": "There are learned parameters involved in scoring this.",
                    "label": 0
                },
                {
                    "sent": "Proposed leap into another version of the truth.",
                    "label": 0
                },
                {
                    "sent": "We score this and decide to accept or reject this change.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Actually, the ability to extremely rapidly evaluate these leaps of truth and undo them if they don't seem to be good was one of the big motivators in some of the original design of factory actually, and and it can be.",
                    "label": 0
                },
                {
                    "sent": "Done very, very quickly.",
                    "label": 0
                },
                {
                    "sent": "So could Markov chain Monte Carlo kind of processes oscillate?",
                    "label": 0
                },
                {
                    "sent": "Yes, and in fact they I mean that's.",
                    "label": 0
                },
                {
                    "sent": "One of the goals is that you want Markov chain processes to mix well.",
                    "label": 0
                },
                {
                    "sent": "How might?",
                    "label": 0
                },
                {
                    "sent": "Someone feel happy, especially this community that thinks about storing Chris notions of the truth.",
                    "label": 0
                },
                {
                    "sent": "Feel about the notion that, well, the truth is sort of shifting underneath you, and possibly random ways.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that could be problematic.",
                    "label": 0
                },
                {
                    "sent": "I see.",
                    "label": 0
                },
                {
                    "sent": "And we've also worked on methods that would store.",
                    "label": 0
                },
                {
                    "sent": "In the midst of that exploration.",
                    "label": 0
                },
                {
                    "sent": "The highest scoring version of the truth so far as the stable thing that we answer questions from and then underneath there's some wandering thing that might replace that that new maximum.",
                    "label": 0
                },
                {
                    "sent": "And beyond that, I think I think your question actually deserves a much.",
                    "label": 0
                },
                {
                    "sent": "Richer and more multifaceted answer, and I think all I have to say is that there's a lot more interesting stuff to be done and I'm glad you asked it.",
                    "label": 0
                },
                {
                    "sent": "There are many questions.",
                    "label": 0
                },
                {
                    "sent": "It's bit hard to oversee.",
                    "label": 0
                },
                {
                    "sent": "Who's next?",
                    "label": 0
                },
                {
                    "sent": "Please line up at the microphone that's easiest to oversee.",
                    "label": 0
                },
                {
                    "sent": "Ha.",
                    "label": 0
                },
                {
                    "sent": "Natasha, I go ahead and then we'll take thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you for the wonderful talk.",
                    "label": 0
                },
                {
                    "sent": "The question that I had so earlier on when you talked about annotating and it is essentially figuring out the learning, the types and learning the relation types.",
                    "label": 0
                },
                {
                    "sent": "I was wondering if it would make sense to try and marry those and figure out the and learn that you know certain types.",
                    "label": 0
                },
                {
                    "sent": "Social learning ontologies would say here, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, the certain type tends to have this relation types, and maybe this relation types the sort of the range over other types and sort of take it in.",
                    "label": 0
                },
                {
                    "sent": "One level up.",
                    "label": 0
                },
                {
                    "sent": "Thank you so much for that question, I would.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I know that I can rapidly enough find a particular slide.",
                    "label": 0
                },
                {
                    "sent": "It would.",
                    "label": 0
                },
                {
                    "sent": "Help me answer that.",
                    "label": 0
                },
                {
                    "sent": "If I can't find it, I'll move on quickly.",
                    "label": 0
                },
                {
                    "sent": "I think I'm not going to find it in time.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Go back.",
                    "label": 0
                },
                {
                    "sent": "Yes, in a way this is.",
                    "label": 0
                },
                {
                    "sent": "This is partially captured in a version of our model in a detail that I wasn't able to get into, which is that.",
                    "label": 0
                },
                {
                    "sent": "The vector for representing a particular entity pair is divided into 3 parts and part of that is unique to this particular entity pair, but the other two parts are tide or repeated, or it's a single set of parameters that are shared across say, all rows in which the first entity is the same.",
                    "label": 0
                },
                {
                    "sent": "So all places where Obama is the first entity have the same first section of their vector representation and.",
                    "label": 0
                },
                {
                    "sent": "And all places where the United States is the 2nd argument.",
                    "label": 0
                },
                {
                    "sent": "There are some vector that in a way represents the United States in this relational pattern, and so in a sense we can think of these as representing.",
                    "label": 0
                },
                {
                    "sent": "Vector representation for Obama embedded inside the relation vector represents something about the types of relations that Obama and that the person and that politicians can participate in in a soft vector kind of way.",
                    "label": 0
                },
                {
                    "sent": "And you and another results that it didn't show.",
                    "label": 0
                },
                {
                    "sent": "You can actually look at these vectors and see what it says about which entity types can go with each other in relation types.",
                    "label": 0
                },
                {
                    "sent": "And this is something that we're interested in exploring further.",
                    "label": 0
                },
                {
                    "sent": "One thing that we have done a little bit of, but not really successfully enough yet to publish.",
                    "label": 0
                },
                {
                    "sent": "Is to jointly train.",
                    "label": 0
                },
                {
                    "sent": "Full ending type representations of the type that I did describe in this talk, and the relation representations at the same time.",
                    "label": 0
                },
                {
                    "sent": "Because I have this intuition that training them together should yield even better results than we're getting now.",
                    "label": 0
                },
                {
                    "sent": "Thank you for the Christmas.",
                    "label": 0
                },
                {
                    "sent": "The results of your techniques are really impressive, but you know we're never happy, yes?",
                    "label": 0
                },
                {
                    "sent": "So how good are your?",
                    "label": 0
                },
                {
                    "sent": "Techniques in extracting negative information.",
                    "label": 0
                },
                {
                    "sent": "The examples where appropriate techniques for extracting negative.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the negative examples that you had where about known implications.",
                    "label": 0
                },
                {
                    "sent": "OK, so one set is not included in another set, for example, but it is.",
                    "label": 0
                },
                {
                    "sent": "Are they able to extract facts such as cars and people are disjoint?",
                    "label": 0
                },
                {
                    "sent": "Aurora Banana is definitely not the car.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, strong engagement.",
                    "label": 0
                },
                {
                    "sent": "So sure yeah.",
                    "label": 0
                },
                {
                    "sent": "Two things, so the first briefly as I described how we're training these vectors for Universal schema, you know there are at least in the data we have, we aren't given negative training examples, and so we're just off.",
                    "label": 0
                },
                {
                    "sent": "And assuming that the UN observed ones randomly sampled will on average be negative, and that does cause some regret.",
                    "label": 0
                },
                {
                    "sent": "If we were given negative training examples, we would absolutely use those, and that would improve our accuracy.",
                    "label": 0
                },
                {
                    "sent": "With the first statement, the second statement is some other results for training the Gaussian embeddings that are in the paper, but I didn't get to describe in the talk.",
                    "label": 0
                },
                {
                    "sent": "Positive exactly this question of what if we were given.",
                    "label": 0
                },
                {
                    "sent": "Hierarchy that containment hierarchy can we train these embeddings using that hierarchy and then do do the Gaussians then correctly reflect this kind of containment?",
                    "label": 0
                },
                {
                    "sent": "Do they have the ability to express that into the training method, find those and we found that yes, they did we?",
                    "label": 0
                },
                {
                    "sent": "We took some hierarchies as training data.",
                    "label": 0
                },
                {
                    "sent": "We trained them both on positive examples of parents and children that were indeed connected as well as negative examples that were not connected.",
                    "label": 0
                },
                {
                    "sent": "And found that we got the kind of Gaussian containment that we wanted, which I mean this was actually on some synthetic data that I think if it's just an early positive indicator.",
                    "label": 0
                },
                {
                    "sent": "But to do this at large scale at some real ontologies, like the kind that you build, I think would be an exciting next step.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is just a clarification question for your entities, are you doing into any entity resolution for that?",
                    "label": 0
                },
                {
                    "sent": "Or those are just nouns?",
                    "label": 0
                },
                {
                    "sent": "Yeah, thank you.",
                    "label": 0
                },
                {
                    "sent": "I'm glad you asked this.",
                    "label": 0
                },
                {
                    "sent": "Can you deal with that?",
                    "label": 0
                },
                {
                    "sent": "Important point, they must be resolved entities and so we did the resolution in advance.",
                    "label": 0
                },
                {
                    "sent": "So maybe I can?",
                    "label": 0
                },
                {
                    "sent": "I'm sorry, let me also answer this in two parts.",
                    "label": 0
                },
                {
                    "sent": "Continue with this first answer and.",
                    "label": 0
                },
                {
                    "sent": "So let's see here.",
                    "label": 0
                },
                {
                    "sent": "Like in the 2nd results that I described, we grabbed free based entities which are obviously resolved and we extracted entities from New York Times and resolve them against three days.",
                    "label": 0
                },
                {
                    "sent": "And this is.",
                    "label": 0
                },
                {
                    "sent": "And this is a key point.",
                    "label": 0
                },
                {
                    "sent": "And again I didn't have time to talk about it all here, but we've been doing actually a lot of research on large scale entity resolution.",
                    "label": 0
                },
                {
                    "sent": "One thing, if you permanently to briefly toot my own horn, we're very excited that just recently actually participated in a competition run by the US Patent Office to do resolution of authors or inventors on the patents and so on.",
                    "label": 0
                },
                {
                    "sent": "A very large set of patent data.",
                    "label": 0
                },
                {
                    "sent": "I think there's a few hundreds of millions of author mentions doing resolution of these things, and there was an international competition with many participants and we came in first place and I'm so proud of my students in the kimono who did that work.",
                    "label": 0
                },
                {
                    "sent": "And and they're going to use our software inside the Patent Office for some next steps.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so resolution is a key component.",
                    "label": 0
                },
                {
                    "sent": "The second part is is early in our work on Universal Schema.",
                    "label": 0
                },
                {
                    "sent": "Before we really bothered to run our large scale entity resolution of this we just took string equivalence as a very cheap kind of entity resolution and ran Universal Schema anyway.",
                    "label": 0
                },
                {
                    "sent": "In a way in which I consider some of the rows would have been noisy, they should have gone together but they didn't and also maybe some things where it might have been confounded also.",
                    "label": 0
                },
                {
                    "sent": "And we also got reasonably good results from Universal Schema, so it can be.",
                    "label": 0
                },
                {
                    "sent": "Somewhat robust to noise, it doesn't fall apart completely when there's noise.",
                    "label": 0
                },
                {
                    "sent": "So thank you for answering all these questions and wonderful talk.",
                    "label": 0
                },
                {
                    "sent": "We have to cut out further questions that are here, but I hope question is can still catch you in a coffee break.",
                    "label": 0
                },
                {
                    "sent": "Thanks a lot Andrew very much.",
                    "label": 0
                }
            ]
        }
    }
}