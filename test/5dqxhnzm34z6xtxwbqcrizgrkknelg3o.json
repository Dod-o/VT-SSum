{
    "id": "5dqxhnzm34z6xtxwbqcrizgrkknelg3o",
    "title": "Visualizing Cauchy\u2019s Interlacing Property for Line Distance Matrices",
    "info": {
        "author": [
            "Ga\u0161per Jakli\u010d, In\u0161titut za matematiko, fiziko in mehaniko (IMFM)"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "November 2005",
        "category": [
            "Top->Mathematics"
        ]
    },
    "url": "http://videolectures.net/cov05_jaklic_vcipl/",
    "segmentation": [
        [
            "Sorry about the noise from border size, but we cannot do anything about that.",
            "So let me introduce cash for the average from Institute of mathematics, physics and mechanics.",
            "You gonna and please.",
            "This is joint work with professor to Mashpee, Science can Professor Millen, riding around it.",
            "I will introduce a special class of matrices so called line distance matrices and study properties of their eigenvalues.",
            "And we will use kushis interlacing property to study properties of eigenvalues and then visualize this interesting property of the eigenvalues.",
            "The motivation for this talk."
        ],
        [
            "Comes from bioinformatics.",
            "As we have seen from in the previous talk.",
            "There is a very interesting problem of how to compare DNA sequences.",
            "How to find some similarities and differences between sequences and it is well known that finding optimal structural alignment between multiple DNA sequences is NP hard problem.",
            "So there are only some heuristics exist.",
            "Based on computer search and compare methods.",
            "And we will look at one of such methods, namely, will consider relative positions of characters AC G&T in the given DNA sequences and then study the properties of such.",
            "Of such dispositions and properties of sequences.",
            "Based on the eigenvalues of special matrices.",
            "Now let us be."
        ],
        [
            "Find line distance metrics.",
            "First, let T be a vector of increasing elements T1T2TT N. And then we can define a line distance metrics as a metrics of differences between vector elements.",
            "TI minus DJ for if J is less or equal to I and the matrix is symmetric.",
            "So in the similar way we define also element Ji.",
            "Here General general form of line distance matrix of size 4 is presented.",
            "Now we can."
        ],
        [
            "Go back to the study of DNA sequences.",
            "If we take, for example, the first exon of the human beta globin gene, it starts as ATG GTG and so on.",
            "And for example, let us consider positions of G in this sequence.",
            "We see that G appears in the third, 4th, 6th, 12th, 19th, and 21st place, and we write this positions in the vector T. And with the help of this vector, we construct the corresponding client distance metrics in the following query, the first we will construct the first row of the matrix.",
            "We take the difference 3 -- 3, we get zero.",
            "4 -- 3 is one 6 -- 3 is 3 and so on.",
            "And then for the next for the second, throw vce abstract element for from the from the rest of the elements 4 -- 4 is zero, 6 -- 4 is 2 performance foresight and so on.",
            "So we get the upper triangle of this metrics.",
            "And because the metric is symmetric, so also the lower triangle is defined by this.",
            "This is the first way of how we can look at this matrix.",
            "But there is also another way.",
            "This matrix is fully determined by only its first row.",
            "If if now we take only the first row of the matrix and subtract the first non zero element.",
            "From every other element of the first row, we get the 2nd row.",
            "For example, here 1 -- 1 is zero, 3 -- 1 is two 9 -- 1 is 8, and so on.",
            "And then we continue.",
            "With this procedure we take the element of the 2nd row just from the diagonal on and subtract 2.",
            "Here 8 -- 2, six 15 -- 2, thirteen, 17 -- 215.",
            "And then we can continue.",
            "From this"
        ],
        [
            "So we can tell something about the obvious properties of line distance matrices.",
            "We have already seen that they are fully determined by their first row.",
            "Then from the definition we have seen that the elements of the metrics are non negative.",
            "In fact, the elements that are not on the diagonal are strictly positive, and the elements on the diagonal are or 0.",
            "Then the matrix is symmetric and that implies that.",
            "Its eigenvalues are real.",
            "Becausw on the diagonal, there are only zeros then.",
            "That means that the trace of the matrix is also zero and that is very important property becausw that means also that the sum of the eigenvalues of this matrix is 0."
        ],
        [
            "Now our our goal is to start the eigenvalues of line distance matrices and for this we will use cautious interlacing theorem.",
            "Let us recall this well known theorem first for symmetric tridiagonal matrices.",
            "Let a be symmetric, tridiagonal irreducible matrix and let A1A two til AN denote its principle.",
            "Sub matrices of dimension 1, two and so on till N. Then the eigenvalues of.",
            "Any principle sub metrics are distinct.",
            "And they strictly interlace with eigenvalues of the following principle submatrix.",
            "So between every every two eigenvalues on the level K + 1, there must be one eigenvalue.",
            "Or from the previous level?",
            "From the level K. And this is very useful fact.",
            "Used for example for bisection algorithm for the eigenvalue computation for symmetric matrices."
        ],
        [
            "Um?",
            "The.",
            "This theorem tells us the following thing.",
            "If, for example, we want to determine the number of negative eigenvalues of the symmetric met symmetric tridiagonal matrix, then we can.",
            "Just easily count some sign changes in the given sequence, and this sequence is the sequence one determinant of the first principle submatrix determinant of the second principle submatrix, til determinant of the last principle submatrix that is in fact equal to the original metric A.",
            "So we just have to construct the sequence and count the number of sign changes and this gives us the number of negative eigenvalues.",
            "And if we shift the original matrix by multiple of identity matrix, then we can determine the number of eigenvalues on air any given interval AB.",
            "For example, let us go."
        ],
        [
            "See the such a simple 3 diagonal matrix.",
            "If we calculate the corresponding diagonal determinant, we get 1 -- 1 -- 3 and four, and now we count the number of sign interchanges.",
            "Here we get from plus to minus and here the sign is the same.",
            "And here is another sign interchange from minus to plus.",
            "So we have two sign interchanges and that means that the metrics has two negative eigenvalues.",
            "If we plot those."
        ],
        [
            "Eigen values in the first level.",
            "There is the only eigenvalue of principle submatrix A1.",
            "Here on the second level RI R2 eigenvalues of the second principle submatrix here are the eigenvalues of the 3rd and the last step dragon values of the our original metrics.",
            "And we clearly see that there are really just two negative eigenvalues and also from this we can see the.",
            "This interlacing property of eigenvalues between every two eigenvalues.",
            "There is one eigenvalue from the previous step.",
            "You can also tell what are the committee members?",
            "Yes, because you just count the size changes.",
            "Just think the smaller for the sickness.",
            "Now what can be said about the general symmetric method?"
        ],
        [
            "Here something similar holds.",
            "Again, we have pushed interlacing property.",
            "We again denote with A1A two dilay and principle sub matrices.",
            "And now it is known that the eigenvalues of two consecutive principal sub matrices interlace.",
            "But here are some important differences, and the one thing is that the eigenvalues of one of the principle sub matrices can be the same.",
            "They are not necessarily distinct.",
            "And the next thing is that here we do not have strict inequalities.",
            "So we do not have strict interlacing.",
            "And here the problem of determining the number of eigenvalues on a given interval is much is harder than before.",
            "Of course, we know that the general symmetric matrix can be.",
            "Can be transformed with well known operations to tridiagonal symmetric matrix and then calculated the number of eigenvalues, but some work has to be done in the structure of the metrics changes.",
            "Now we would like to you."
        ],
        [
            "Is this properties to crisis interesting property?",
            "Two study eigenvalues of flying distance metrics.",
            "The following theorem can be proved.",
            "Let's now LB align distance metrics defined by a vector T. And let again Li denote its principal submatrix is, and if we sort the eigen values so that the Lambda one is the largest and then Lambda two, Lambda, three and so on, the Lambda I.",
            "If there are the eigenvalues of this slide distance metrics, then we can prove that the largest eigenvalue of the matrix is strictly positive.",
            "And that the second eigenvalue.",
            "Is negative, that means also that all the rest of the eigenvalues are negative?",
            "Except for the first principle, submatrix that is just zero from the definition.",
            "So we've got the following result.",
            "The line distance metrics has exactly 1 positive eigenvalue.",
            "And if we study, if we look at the question when the second largest eigenvalue can be 0.",
            "Then we can prove that the second largest eigenvalue is 0 if and only if two consecutive elements of vector T are the same.",
            "Do you have a key?",
            "One of the music, yes.",
            "So you live in this area.",
            "OK, now let us look at some."
        ],
        [
            "Observations that enable us to prove this theorem.",
            "1st letter, Shorten the notation and denote with Lambda Ji the eigenvalues of the metrics.",
            "The Jade eigenvalues of the principal submatrix Li.",
            "And let P sub I denote the characteristic polynomial of the metrics Ally.",
            "Then we can prove that the determinant of the matrix Li has a very nice form.",
            "It can be factorized in the following way.",
            "Only there, of course elements of vector T appear, but from this we can quickly conclude that that this matrices are nonsingular.",
            "Because this cannot be 0 if the element of vector T are different.",
            "And this can be 0 only if one of those factor is zero.",
            "That means that two consecutive elements must be 0.",
            "This is this property.",
            "When the second largest eigenvalue is 0.",
            "From this this can be seen.",
            "And now we know that the trace of the eigenvector trace of the Matrix ally is 0.",
            "And this is also some of its eigenvalues, and the determinant of the matrix.",
            "Is non 0 from this from this expression this means that all the eigenvalues are non 0.",
            "And also because we know that the element of the metrics are non negative and the elements that are not on the diagonal are positive.",
            "From this we can conclude that for all the matrices Ally, the largest eigenvalue must be positive and the smallest eigenvalue must be negative.",
            "Except for the case when I is 1 then we have just the eigenvalue 0 because the matrix is just one times one matrix with elements 0.",
            "OK."
        ],
        [
            "Now we use the cautious interlacing theorem on the first step.",
            "On the first level and the second level.",
            "We obtain that the eigenvalues, of course, must internalize.",
            "And from this we can conclude.",
            "That the second largest eigenvalue of the principal submatrix two is negative.",
            "Then we use Kushis interlacing theorem again.",
            "For in the Level 3 and two we get the following interlacing property, and now if we study their characteristic polynomial P3, we can see easily that the leading term of this characteristic polynomial is minus X to the power 3.",
            "And if we calculate the value of this characteristic polynomial at zero, we can see because this is just the term the previously evaluates the term."
        ],
        [
            "Went with I is equal 3."
        ],
        [
            "We can see that this is this is positive and from the previous step we have seen the properties of eigenvalues, Lambda two and Lambda one.",
            "From this we can conclude that the second eigenvalue in the Level 3.",
            "Must be negative.",
            "Now we use inductive approach."
        ],
        [
            "Ouch.",
            "We presume that at the level I -- 1, the second largest eigenvalue is negative.",
            "Use cautious interlacing theorem.",
            "We need to adjust the eigenvalues till Lambda, one Lambda two and Lambda 3.",
            "We recall that the largest eigenvalue is positive.",
            "And all the eigenvalues 3, four and so forth are negative.",
            "Again, study the characteristic polynomial.",
            "We see that the leading term of this characteristic polynomial P sub I is minus a -- X to the power I.",
            "And we can determine the sign of the value of the characteristic polynomial at 0.",
            "Becausw this science of this leading term and value at zero are different from all this.",
            "We can finally conclude that the second largest eigenvalue at the level I is negative.",
            "So this finishes the idea of proof.",
            "And."
        ],
        [
            "Now if we consider some examples.",
            "If we take vector T123, four and six, then the corresponding client distance matrix of size 5 is this matrix.",
            "And if we calculate the eigenvalues of its principal submatrix, we get first from the for the metrics zero of size 1 * 1.",
            "We have eigenvalue zero, then for the next principle submatrix 0110 we get eigenvalues minus one and one then from the next step we get the 3rd row IV role and at last the five.",
            "The 5th row.",
            "These are the eigenvalues of our line distance matrix L. And from this we see that only the largest eigenvalues are positive.",
            "The rest of the eigenvalues are negative.",
            "Now we can construct the graph from this from kushies interesting property and eigenvalues of the matrices.",
            "And if we do this.",
            "We"
        ],
        [
            "Add the following visualization of this kuchis interlacing property for our line distance metrics.",
            "Those edges of this graph denoted just this crisis, interlacing property.",
            "This vertex on the higher level is connected with two vertices at the lower level.",
            "In such way that here we have this intricacies interlacing property.",
            "And this continues at every level of our graph.",
            "From this we can see that the first eigenvalue is positive.",
            "Here is 0.",
            "The largest angle value is something larger than then, then also it every previous step.",
            "The largest eigenvalue is positive, but all the rest of the eigenvalues are negative at every step except the first step.",
            "When the eigenvalue is 0.",
            "If we do the same thing on."
        ],
        [
            "The original example on the first part of human beta globin.",
            "We get similar graph again.",
            "It can be seen that the largest eigenvalue is positive, the rest of the eigenvalues are negative on every step.",
            "Graphs are.",
            "Similar, but here this interlacing property can be seen more clearly, because the eigenvalues are more apart than in this real real life data example.",
            "Now let us do."
        ],
        [
            "Discuss in the further work on the subject.",
            "Now of course, interesting question is how and how to determine and to determine the eigenvalues positions.",
            "More exactly, not just one is positive and the rest are negative.",
            "The first answer can be obtained.",
            "Answer That give us bound bounds for the eigen values can be obtained from Gregorians theorem that gives us the interval on which the eigenvalues must lie from this.",
            "Now from minus this number to this number.",
            "This gives us the interval and from numerical examples it can be seen that the largest eigenvalue.",
            "His."
        ],
        [
            "Near this upper bound.",
            "And the rest of the eigenvalues are more nearer than zero because there are of course N minus one negative eigenvalues, and all of the eigenvalues must sum up to 0."
        ],
        [
            "This property that the sum of the eigenvalues is 0 lead us to.",
            "Another thing that could be studied.",
            "We can study partial sums of eigenvalues because we know the largest eigenvalue is positive and then if we sum up.",
            "The 1st and the 2nd eigenvalue then first second and the 3rd and so on.",
            "We get decreasing sequence of positive OK.",
            "In fact non negative elements and we can perhaps compare the DNA sequences for a given character in the sequence.",
            "We can compare those sequences.",
            "And get something from the such from such characterizations.",
            "Then it is interesting, perhaps interesting question, what happens if we use this method to?",
            "Two on the protein sequences.",
            "And.",
            "This also has have to be done to compare the existing methods and our method on some real life data and see how this results compare.",
            "Thank you.",
            "Any questions or remarks?",
            "Go to this picture number, not a loss on once, before and now when I look at this, obviously leading."
        ],
        [
            "I'm here baby.",
            "I never get to be division.",
            "Also, the second eigenvector comes bigger, bigger and bigger.",
            "Can you prove that it means that eventually limit is 0 because it can't go over 0?",
            "Not be necessary.",
            "Is this this property?",
            "This follows from this cautious, interlacing property.",
            "Not here."
        ],
        [
            "It follows from this interlacing the eigenvalues every eigenvalue of in the next step must be larger or equal than the previous.",
            "Inducing.",
            "Maybe maybe if you got him.",
            "Because you have this, if you take the sum of elements in one element and they can use about the internal elements that contain spec."
        ],
        [
            "So Windows for menus.",
            "Mm-hmm, it's all because of."
        ],
        [
            "You have these in a larger matters.",
            "Then it's this large office.",
            "Then when you have when you couple you take the smaller number system.",
            "Sorry I didn't make it out this morning.",
            "If you would like, we get those pictures there."
        ],
        [
            "So the median value is increasing, the second one is also losing, but then the last one is decreasing.",
            "So which one starts with using?",
            "Very rude, yeah.",
            "All others are busy 'cause there's only one more every.",
            "It is always increasing.",
            "My dearest.",
            "Oh the second one, yeah, but it's a little lower.",
            "Start slowing down.",
            "This course is stupid.",
            "Another question, because such kind of girls, you can't.",
            "It's not necessary to look at like this.",
            "You can do for me for appointees.",
            "It can be adjusted.",
            "I don't think he said no, but.",
            "Yes, ma'am.",
            "This month to month or something, but I mean such kind of got that reservation can be done.",
            "Majestic campusvue the Matrix is actually very large, So what happens with the convergence or whatever?",
            "And there's something about?",
            "Would you like the other people Nachi sequence so beautiful to see what happens there.",
            "Definition sequence, But then you came to divide probably the variable by some number and then what is that number?",
            "Which design fiddle with square or value again or whatever?",
            "What is the right pictures like?",
            "Don't think so.",
            "So I suggested we start the discussion here and we have lunch later.",
            "Open Facebook please."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry about the noise from border size, but we cannot do anything about that.",
                    "label": 0
                },
                {
                    "sent": "So let me introduce cash for the average from Institute of mathematics, physics and mechanics.",
                    "label": 0
                },
                {
                    "sent": "You gonna and please.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with professor to Mashpee, Science can Professor Millen, riding around it.",
                    "label": 0
                },
                {
                    "sent": "I will introduce a special class of matrices so called line distance matrices and study properties of their eigenvalues.",
                    "label": 1
                },
                {
                    "sent": "And we will use kushis interlacing property to study properties of eigenvalues and then visualize this interesting property of the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "The motivation for this talk.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Comes from bioinformatics.",
                    "label": 0
                },
                {
                    "sent": "As we have seen from in the previous talk.",
                    "label": 0
                },
                {
                    "sent": "There is a very interesting problem of how to compare DNA sequences.",
                    "label": 0
                },
                {
                    "sent": "How to find some similarities and differences between sequences and it is well known that finding optimal structural alignment between multiple DNA sequences is NP hard problem.",
                    "label": 1
                },
                {
                    "sent": "So there are only some heuristics exist.",
                    "label": 0
                },
                {
                    "sent": "Based on computer search and compare methods.",
                    "label": 0
                },
                {
                    "sent": "And we will look at one of such methods, namely, will consider relative positions of characters AC G&T in the given DNA sequences and then study the properties of such.",
                    "label": 0
                },
                {
                    "sent": "Of such dispositions and properties of sequences.",
                    "label": 0
                },
                {
                    "sent": "Based on the eigenvalues of special matrices.",
                    "label": 0
                },
                {
                    "sent": "Now let us be.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Find line distance metrics.",
                    "label": 0
                },
                {
                    "sent": "First, let T be a vector of increasing elements T1T2TT N. And then we can define a line distance metrics as a metrics of differences between vector elements.",
                    "label": 1
                },
                {
                    "sent": "TI minus DJ for if J is less or equal to I and the matrix is symmetric.",
                    "label": 0
                },
                {
                    "sent": "So in the similar way we define also element Ji.",
                    "label": 1
                },
                {
                    "sent": "Here General general form of line distance matrix of size 4 is presented.",
                    "label": 0
                },
                {
                    "sent": "Now we can.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Go back to the study of DNA sequences.",
                    "label": 0
                },
                {
                    "sent": "If we take, for example, the first exon of the human beta globin gene, it starts as ATG GTG and so on.",
                    "label": 1
                },
                {
                    "sent": "And for example, let us consider positions of G in this sequence.",
                    "label": 1
                },
                {
                    "sent": "We see that G appears in the third, 4th, 6th, 12th, 19th, and 21st place, and we write this positions in the vector T. And with the help of this vector, we construct the corresponding client distance metrics in the following query, the first we will construct the first row of the matrix.",
                    "label": 0
                },
                {
                    "sent": "We take the difference 3 -- 3, we get zero.",
                    "label": 0
                },
                {
                    "sent": "4 -- 3 is one 6 -- 3 is 3 and so on.",
                    "label": 0
                },
                {
                    "sent": "And then for the next for the second, throw vce abstract element for from the from the rest of the elements 4 -- 4 is zero, 6 -- 4 is 2 performance foresight and so on.",
                    "label": 0
                },
                {
                    "sent": "So we get the upper triangle of this metrics.",
                    "label": 0
                },
                {
                    "sent": "And because the metric is symmetric, so also the lower triangle is defined by this.",
                    "label": 0
                },
                {
                    "sent": "This is the first way of how we can look at this matrix.",
                    "label": 0
                },
                {
                    "sent": "But there is also another way.",
                    "label": 0
                },
                {
                    "sent": "This matrix is fully determined by only its first row.",
                    "label": 0
                },
                {
                    "sent": "If if now we take only the first row of the matrix and subtract the first non zero element.",
                    "label": 0
                },
                {
                    "sent": "From every other element of the first row, we get the 2nd row.",
                    "label": 0
                },
                {
                    "sent": "For example, here 1 -- 1 is zero, 3 -- 1 is two 9 -- 1 is 8, and so on.",
                    "label": 0
                },
                {
                    "sent": "And then we continue.",
                    "label": 0
                },
                {
                    "sent": "With this procedure we take the element of the 2nd row just from the diagonal on and subtract 2.",
                    "label": 0
                },
                {
                    "sent": "Here 8 -- 2, six 15 -- 2, thirteen, 17 -- 215.",
                    "label": 0
                },
                {
                    "sent": "And then we can continue.",
                    "label": 0
                },
                {
                    "sent": "From this",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can tell something about the obvious properties of line distance matrices.",
                    "label": 1
                },
                {
                    "sent": "We have already seen that they are fully determined by their first row.",
                    "label": 0
                },
                {
                    "sent": "Then from the definition we have seen that the elements of the metrics are non negative.",
                    "label": 0
                },
                {
                    "sent": "In fact, the elements that are not on the diagonal are strictly positive, and the elements on the diagonal are or 0.",
                    "label": 0
                },
                {
                    "sent": "Then the matrix is symmetric and that implies that.",
                    "label": 0
                },
                {
                    "sent": "Its eigenvalues are real.",
                    "label": 0
                },
                {
                    "sent": "Becausw on the diagonal, there are only zeros then.",
                    "label": 0
                },
                {
                    "sent": "That means that the trace of the matrix is also zero and that is very important property becausw that means also that the sum of the eigenvalues of this matrix is 0.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now our our goal is to start the eigenvalues of line distance matrices and for this we will use cautious interlacing theorem.",
                    "label": 0
                },
                {
                    "sent": "Let us recall this well known theorem first for symmetric tridiagonal matrices.",
                    "label": 0
                },
                {
                    "sent": "Let a be symmetric, tridiagonal irreducible matrix and let A1A two til AN denote its principle.",
                    "label": 1
                },
                {
                    "sent": "Sub matrices of dimension 1, two and so on till N. Then the eigenvalues of.",
                    "label": 1
                },
                {
                    "sent": "Any principle sub metrics are distinct.",
                    "label": 0
                },
                {
                    "sent": "And they strictly interlace with eigenvalues of the following principle submatrix.",
                    "label": 0
                },
                {
                    "sent": "So between every every two eigenvalues on the level K + 1, there must be one eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "Or from the previous level?",
                    "label": 1
                },
                {
                    "sent": "From the level K. And this is very useful fact.",
                    "label": 0
                },
                {
                    "sent": "Used for example for bisection algorithm for the eigenvalue computation for symmetric matrices.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "This theorem tells us the following thing.",
                    "label": 0
                },
                {
                    "sent": "If, for example, we want to determine the number of negative eigenvalues of the symmetric met symmetric tridiagonal matrix, then we can.",
                    "label": 1
                },
                {
                    "sent": "Just easily count some sign changes in the given sequence, and this sequence is the sequence one determinant of the first principle submatrix determinant of the second principle submatrix, til determinant of the last principle submatrix that is in fact equal to the original metric A.",
                    "label": 1
                },
                {
                    "sent": "So we just have to construct the sequence and count the number of sign changes and this gives us the number of negative eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "And if we shift the original matrix by multiple of identity matrix, then we can determine the number of eigenvalues on air any given interval AB.",
                    "label": 0
                },
                {
                    "sent": "For example, let us go.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See the such a simple 3 diagonal matrix.",
                    "label": 0
                },
                {
                    "sent": "If we calculate the corresponding diagonal determinant, we get 1 -- 1 -- 3 and four, and now we count the number of sign interchanges.",
                    "label": 1
                },
                {
                    "sent": "Here we get from plus to minus and here the sign is the same.",
                    "label": 0
                },
                {
                    "sent": "And here is another sign interchange from minus to plus.",
                    "label": 0
                },
                {
                    "sent": "So we have two sign interchanges and that means that the metrics has two negative eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "If we plot those.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eigen values in the first level.",
                    "label": 0
                },
                {
                    "sent": "There is the only eigenvalue of principle submatrix A1.",
                    "label": 0
                },
                {
                    "sent": "Here on the second level RI R2 eigenvalues of the second principle submatrix here are the eigenvalues of the 3rd and the last step dragon values of the our original metrics.",
                    "label": 0
                },
                {
                    "sent": "And we clearly see that there are really just two negative eigenvalues and also from this we can see the.",
                    "label": 0
                },
                {
                    "sent": "This interlacing property of eigenvalues between every two eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "There is one eigenvalue from the previous step.",
                    "label": 0
                },
                {
                    "sent": "You can also tell what are the committee members?",
                    "label": 0
                },
                {
                    "sent": "Yes, because you just count the size changes.",
                    "label": 0
                },
                {
                    "sent": "Just think the smaller for the sickness.",
                    "label": 0
                },
                {
                    "sent": "Now what can be said about the general symmetric method?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here something similar holds.",
                    "label": 0
                },
                {
                    "sent": "Again, we have pushed interlacing property.",
                    "label": 1
                },
                {
                    "sent": "We again denote with A1A two dilay and principle sub matrices.",
                    "label": 0
                },
                {
                    "sent": "And now it is known that the eigenvalues of two consecutive principal sub matrices interlace.",
                    "label": 1
                },
                {
                    "sent": "But here are some important differences, and the one thing is that the eigenvalues of one of the principle sub matrices can be the same.",
                    "label": 0
                },
                {
                    "sent": "They are not necessarily distinct.",
                    "label": 0
                },
                {
                    "sent": "And the next thing is that here we do not have strict inequalities.",
                    "label": 0
                },
                {
                    "sent": "So we do not have strict interlacing.",
                    "label": 1
                },
                {
                    "sent": "And here the problem of determining the number of eigenvalues on a given interval is much is harder than before.",
                    "label": 0
                },
                {
                    "sent": "Of course, we know that the general symmetric matrix can be.",
                    "label": 0
                },
                {
                    "sent": "Can be transformed with well known operations to tridiagonal symmetric matrix and then calculated the number of eigenvalues, but some work has to be done in the structure of the metrics changes.",
                    "label": 0
                },
                {
                    "sent": "Now we would like to you.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is this properties to crisis interesting property?",
                    "label": 0
                },
                {
                    "sent": "Two study eigenvalues of flying distance metrics.",
                    "label": 0
                },
                {
                    "sent": "The following theorem can be proved.",
                    "label": 0
                },
                {
                    "sent": "Let's now LB align distance metrics defined by a vector T. And let again Li denote its principal submatrix is, and if we sort the eigen values so that the Lambda one is the largest and then Lambda two, Lambda, three and so on, the Lambda I.",
                    "label": 1
                },
                {
                    "sent": "If there are the eigenvalues of this slide distance metrics, then we can prove that the largest eigenvalue of the matrix is strictly positive.",
                    "label": 0
                },
                {
                    "sent": "And that the second eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "Is negative, that means also that all the rest of the eigenvalues are negative?",
                    "label": 0
                },
                {
                    "sent": "Except for the first principle, submatrix that is just zero from the definition.",
                    "label": 0
                },
                {
                    "sent": "So we've got the following result.",
                    "label": 0
                },
                {
                    "sent": "The line distance metrics has exactly 1 positive eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "And if we study, if we look at the question when the second largest eigenvalue can be 0.",
                    "label": 0
                },
                {
                    "sent": "Then we can prove that the second largest eigenvalue is 0 if and only if two consecutive elements of vector T are the same.",
                    "label": 0
                },
                {
                    "sent": "Do you have a key?",
                    "label": 0
                },
                {
                    "sent": "One of the music, yes.",
                    "label": 0
                },
                {
                    "sent": "So you live in this area.",
                    "label": 0
                },
                {
                    "sent": "OK, now let us look at some.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Observations that enable us to prove this theorem.",
                    "label": 0
                },
                {
                    "sent": "1st letter, Shorten the notation and denote with Lambda Ji the eigenvalues of the metrics.",
                    "label": 0
                },
                {
                    "sent": "The Jade eigenvalues of the principal submatrix Li.",
                    "label": 0
                },
                {
                    "sent": "And let P sub I denote the characteristic polynomial of the metrics Ally.",
                    "label": 0
                },
                {
                    "sent": "Then we can prove that the determinant of the matrix Li has a very nice form.",
                    "label": 0
                },
                {
                    "sent": "It can be factorized in the following way.",
                    "label": 0
                },
                {
                    "sent": "Only there, of course elements of vector T appear, but from this we can quickly conclude that that this matrices are nonsingular.",
                    "label": 0
                },
                {
                    "sent": "Because this cannot be 0 if the element of vector T are different.",
                    "label": 0
                },
                {
                    "sent": "And this can be 0 only if one of those factor is zero.",
                    "label": 0
                },
                {
                    "sent": "That means that two consecutive elements must be 0.",
                    "label": 0
                },
                {
                    "sent": "This is this property.",
                    "label": 0
                },
                {
                    "sent": "When the second largest eigenvalue is 0.",
                    "label": 0
                },
                {
                    "sent": "From this this can be seen.",
                    "label": 0
                },
                {
                    "sent": "And now we know that the trace of the eigenvector trace of the Matrix ally is 0.",
                    "label": 0
                },
                {
                    "sent": "And this is also some of its eigenvalues, and the determinant of the matrix.",
                    "label": 0
                },
                {
                    "sent": "Is non 0 from this from this expression this means that all the eigenvalues are non 0.",
                    "label": 0
                },
                {
                    "sent": "And also because we know that the element of the metrics are non negative and the elements that are not on the diagonal are positive.",
                    "label": 0
                },
                {
                    "sent": "From this we can conclude that for all the matrices Ally, the largest eigenvalue must be positive and the smallest eigenvalue must be negative.",
                    "label": 0
                },
                {
                    "sent": "Except for the case when I is 1 then we have just the eigenvalue 0 because the matrix is just one times one matrix with elements 0.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we use the cautious interlacing theorem on the first step.",
                    "label": 0
                },
                {
                    "sent": "On the first level and the second level.",
                    "label": 0
                },
                {
                    "sent": "We obtain that the eigenvalues, of course, must internalize.",
                    "label": 0
                },
                {
                    "sent": "And from this we can conclude.",
                    "label": 0
                },
                {
                    "sent": "That the second largest eigenvalue of the principal submatrix two is negative.",
                    "label": 0
                },
                {
                    "sent": "Then we use Kushis interlacing theorem again.",
                    "label": 0
                },
                {
                    "sent": "For in the Level 3 and two we get the following interlacing property, and now if we study their characteristic polynomial P3, we can see easily that the leading term of this characteristic polynomial is minus X to the power 3.",
                    "label": 0
                },
                {
                    "sent": "And if we calculate the value of this characteristic polynomial at zero, we can see because this is just the term the previously evaluates the term.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Went with I is equal 3.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can see that this is this is positive and from the previous step we have seen the properties of eigenvalues, Lambda two and Lambda one.",
                    "label": 0
                },
                {
                    "sent": "From this we can conclude that the second eigenvalue in the Level 3.",
                    "label": 0
                },
                {
                    "sent": "Must be negative.",
                    "label": 0
                },
                {
                    "sent": "Now we use inductive approach.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ouch.",
                    "label": 0
                },
                {
                    "sent": "We presume that at the level I -- 1, the second largest eigenvalue is negative.",
                    "label": 0
                },
                {
                    "sent": "Use cautious interlacing theorem.",
                    "label": 0
                },
                {
                    "sent": "We need to adjust the eigenvalues till Lambda, one Lambda two and Lambda 3.",
                    "label": 0
                },
                {
                    "sent": "We recall that the largest eigenvalue is positive.",
                    "label": 0
                },
                {
                    "sent": "And all the eigenvalues 3, four and so forth are negative.",
                    "label": 0
                },
                {
                    "sent": "Again, study the characteristic polynomial.",
                    "label": 0
                },
                {
                    "sent": "We see that the leading term of this characteristic polynomial P sub I is minus a -- X to the power I.",
                    "label": 0
                },
                {
                    "sent": "And we can determine the sign of the value of the characteristic polynomial at 0.",
                    "label": 0
                },
                {
                    "sent": "Becausw this science of this leading term and value at zero are different from all this.",
                    "label": 0
                },
                {
                    "sent": "We can finally conclude that the second largest eigenvalue at the level I is negative.",
                    "label": 0
                },
                {
                    "sent": "So this finishes the idea of proof.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now if we consider some examples.",
                    "label": 0
                },
                {
                    "sent": "If we take vector T123, four and six, then the corresponding client distance matrix of size 5 is this matrix.",
                    "label": 1
                },
                {
                    "sent": "And if we calculate the eigenvalues of its principal submatrix, we get first from the for the metrics zero of size 1 * 1.",
                    "label": 1
                },
                {
                    "sent": "We have eigenvalue zero, then for the next principle submatrix 0110 we get eigenvalues minus one and one then from the next step we get the 3rd row IV role and at last the five.",
                    "label": 0
                },
                {
                    "sent": "The 5th row.",
                    "label": 0
                },
                {
                    "sent": "These are the eigenvalues of our line distance matrix L. And from this we see that only the largest eigenvalues are positive.",
                    "label": 0
                },
                {
                    "sent": "The rest of the eigenvalues are negative.",
                    "label": 0
                },
                {
                    "sent": "Now we can construct the graph from this from kushies interesting property and eigenvalues of the matrices.",
                    "label": 0
                },
                {
                    "sent": "And if we do this.",
                    "label": 0
                },
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Add the following visualization of this kuchis interlacing property for our line distance metrics.",
                    "label": 0
                },
                {
                    "sent": "Those edges of this graph denoted just this crisis, interlacing property.",
                    "label": 0
                },
                {
                    "sent": "This vertex on the higher level is connected with two vertices at the lower level.",
                    "label": 0
                },
                {
                    "sent": "In such way that here we have this intricacies interlacing property.",
                    "label": 0
                },
                {
                    "sent": "And this continues at every level of our graph.",
                    "label": 0
                },
                {
                    "sent": "From this we can see that the first eigenvalue is positive.",
                    "label": 0
                },
                {
                    "sent": "Here is 0.",
                    "label": 0
                },
                {
                    "sent": "The largest angle value is something larger than then, then also it every previous step.",
                    "label": 0
                },
                {
                    "sent": "The largest eigenvalue is positive, but all the rest of the eigenvalues are negative at every step except the first step.",
                    "label": 0
                },
                {
                    "sent": "When the eigenvalue is 0.",
                    "label": 0
                },
                {
                    "sent": "If we do the same thing on.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The original example on the first part of human beta globin.",
                    "label": 0
                },
                {
                    "sent": "We get similar graph again.",
                    "label": 0
                },
                {
                    "sent": "It can be seen that the largest eigenvalue is positive, the rest of the eigenvalues are negative on every step.",
                    "label": 0
                },
                {
                    "sent": "Graphs are.",
                    "label": 0
                },
                {
                    "sent": "Similar, but here this interlacing property can be seen more clearly, because the eigenvalues are more apart than in this real real life data example.",
                    "label": 0
                },
                {
                    "sent": "Now let us do.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Discuss in the further work on the subject.",
                    "label": 1
                },
                {
                    "sent": "Now of course, interesting question is how and how to determine and to determine the eigenvalues positions.",
                    "label": 0
                },
                {
                    "sent": "More exactly, not just one is positive and the rest are negative.",
                    "label": 0
                },
                {
                    "sent": "The first answer can be obtained.",
                    "label": 0
                },
                {
                    "sent": "Answer That give us bound bounds for the eigen values can be obtained from Gregorians theorem that gives us the interval on which the eigenvalues must lie from this.",
                    "label": 0
                },
                {
                    "sent": "Now from minus this number to this number.",
                    "label": 0
                },
                {
                    "sent": "This gives us the interval and from numerical examples it can be seen that the largest eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "His.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Near this upper bound.",
                    "label": 0
                },
                {
                    "sent": "And the rest of the eigenvalues are more nearer than zero because there are of course N minus one negative eigenvalues, and all of the eigenvalues must sum up to 0.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This property that the sum of the eigenvalues is 0 lead us to.",
                    "label": 0
                },
                {
                    "sent": "Another thing that could be studied.",
                    "label": 0
                },
                {
                    "sent": "We can study partial sums of eigenvalues because we know the largest eigenvalue is positive and then if we sum up.",
                    "label": 1
                },
                {
                    "sent": "The 1st and the 2nd eigenvalue then first second and the 3rd and so on.",
                    "label": 0
                },
                {
                    "sent": "We get decreasing sequence of positive OK.",
                    "label": 0
                },
                {
                    "sent": "In fact non negative elements and we can perhaps compare the DNA sequences for a given character in the sequence.",
                    "label": 0
                },
                {
                    "sent": "We can compare those sequences.",
                    "label": 0
                },
                {
                    "sent": "And get something from the such from such characterizations.",
                    "label": 0
                },
                {
                    "sent": "Then it is interesting, perhaps interesting question, what happens if we use this method to?",
                    "label": 0
                },
                {
                    "sent": "Two on the protein sequences.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This also has have to be done to compare the existing methods and our method on some real life data and see how this results compare.",
                    "label": 1
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Any questions or remarks?",
                    "label": 0
                },
                {
                    "sent": "Go to this picture number, not a loss on once, before and now when I look at this, obviously leading.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm here baby.",
                    "label": 0
                },
                {
                    "sent": "I never get to be division.",
                    "label": 0
                },
                {
                    "sent": "Also, the second eigenvector comes bigger, bigger and bigger.",
                    "label": 0
                },
                {
                    "sent": "Can you prove that it means that eventually limit is 0 because it can't go over 0?",
                    "label": 0
                },
                {
                    "sent": "Not be necessary.",
                    "label": 0
                },
                {
                    "sent": "Is this this property?",
                    "label": 0
                },
                {
                    "sent": "This follows from this cautious, interlacing property.",
                    "label": 0
                },
                {
                    "sent": "Not here.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It follows from this interlacing the eigenvalues every eigenvalue of in the next step must be larger or equal than the previous.",
                    "label": 0
                },
                {
                    "sent": "Inducing.",
                    "label": 0
                },
                {
                    "sent": "Maybe maybe if you got him.",
                    "label": 0
                },
                {
                    "sent": "Because you have this, if you take the sum of elements in one element and they can use about the internal elements that contain spec.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Windows for menus.",
                    "label": 0
                },
                {
                    "sent": "Mm-hmm, it's all because of.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have these in a larger matters.",
                    "label": 0
                },
                {
                    "sent": "Then it's this large office.",
                    "label": 0
                },
                {
                    "sent": "Then when you have when you couple you take the smaller number system.",
                    "label": 0
                },
                {
                    "sent": "Sorry I didn't make it out this morning.",
                    "label": 0
                },
                {
                    "sent": "If you would like, we get those pictures there.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the median value is increasing, the second one is also losing, but then the last one is decreasing.",
                    "label": 0
                },
                {
                    "sent": "So which one starts with using?",
                    "label": 0
                },
                {
                    "sent": "Very rude, yeah.",
                    "label": 0
                },
                {
                    "sent": "All others are busy 'cause there's only one more every.",
                    "label": 0
                },
                {
                    "sent": "It is always increasing.",
                    "label": 0
                },
                {
                    "sent": "My dearest.",
                    "label": 0
                },
                {
                    "sent": "Oh the second one, yeah, but it's a little lower.",
                    "label": 0
                },
                {
                    "sent": "Start slowing down.",
                    "label": 0
                },
                {
                    "sent": "This course is stupid.",
                    "label": 0
                },
                {
                    "sent": "Another question, because such kind of girls, you can't.",
                    "label": 0
                },
                {
                    "sent": "It's not necessary to look at like this.",
                    "label": 0
                },
                {
                    "sent": "You can do for me for appointees.",
                    "label": 0
                },
                {
                    "sent": "It can be adjusted.",
                    "label": 0
                },
                {
                    "sent": "I don't think he said no, but.",
                    "label": 0
                },
                {
                    "sent": "Yes, ma'am.",
                    "label": 0
                },
                {
                    "sent": "This month to month or something, but I mean such kind of got that reservation can be done.",
                    "label": 0
                },
                {
                    "sent": "Majestic campusvue the Matrix is actually very large, So what happens with the convergence or whatever?",
                    "label": 0
                },
                {
                    "sent": "And there's something about?",
                    "label": 0
                },
                {
                    "sent": "Would you like the other people Nachi sequence so beautiful to see what happens there.",
                    "label": 0
                },
                {
                    "sent": "Definition sequence, But then you came to divide probably the variable by some number and then what is that number?",
                    "label": 0
                },
                {
                    "sent": "Which design fiddle with square or value again or whatever?",
                    "label": 0
                },
                {
                    "sent": "What is the right pictures like?",
                    "label": 0
                },
                {
                    "sent": "Don't think so.",
                    "label": 0
                },
                {
                    "sent": "So I suggested we start the discussion here and we have lunch later.",
                    "label": 0
                },
                {
                    "sent": "Open Facebook please.",
                    "label": 0
                }
            ]
        }
    }
}