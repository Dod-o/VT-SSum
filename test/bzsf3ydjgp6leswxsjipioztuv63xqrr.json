{
    "id": "bzsf3ydjgp6leswxsjipioztuv63xqrr",
    "title": "Multi-Task Learning and Matrix Regularization",
    "info": {
        "author": [
            "Andreas Argyriou, \u00c9cole Centrale Paris"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_argyriou_mtlmr/",
    "segmentation": [
        [
            "So."
        ],
        [
            "I'm going to talk about several multi Tasker methods from the perspective of a regularization.",
            "There are some connections, clearly with some Gaussian process methods that have been.",
            "I will talk a little bit about the interpretations, so some of these methods have multiple interpretations, so you can view them in.",
            "From my regularization perspective, Gaussian processes or another Bayesian perspective and so on.",
            "And I will so it will be a little bit theoretical.",
            "I will.",
            "I will not give many results in experimental results about these methods, but you can.",
            "Maybe you can research this afterwards.",
            "So finally I will give some theoretical results called represented theorems, which are probably you know, that we are very interested in them in."
        ],
        [
            "Whenever we work with Karen methods.",
            "And.",
            "Let me start by presenting the.",
            "Like the framework General framework we have in this learning, so let me repeat that we learn from.",
            "From a number of samples.",
            "So we have N examples, but ask for simplicity, but this can be different numbers of examples.",
            "And we just want to find a number of N functions FT that relate the input to the output space.",
            "So we want to jointly find these functions because we believe we have some prior assumptions that these tasks are related.",
            "And this is especially important when we have a few data points, but task.",
            "So with a small sample we cannot do good prediction or classification per task and we need to exploit all of all of the tasks together."
        ],
        [
            "Another reason why we want to do this is what?",
            "We call transfer transfer learning, which means that we get some kind of model which I will describe later on, but we get some some knowledge from the task we have.",
            "And we want to transfer this knowledge to a new task and this new task has to be of course related to the previous ones.",
            "But the question of course is what to transfer to this new task so that we get good performance.",
            "Good prediction on this new task, especially when we have a few examples from this task."
        ],
        [
            "So let me briefly repeat some review.",
            "Some of the of the applications of multiple learning and why it's important.",
            "One very famous one is what is called collaborative filtering or recommendation systems.",
            "An example is the Netflix competition.",
            "It's also found in many marketing databases.",
            "Marketing problems.",
            "So here you have.",
            "A number of products and the attributes and.",
            "This is given to some.",
            "Some customer or user on the Internet and they have to assign a rating of how much they like the product.",
            "The question, of course, is that you want to predict over on new products, and its task corresponds to its user."
        ],
        [
            "This is related to the matrix completion problem, which has been recently studied a lot in in other areas like compressed sensing and.",
            "Essentially here the tasks are the columns correspond to the columns of a matrix.",
            "You have a number of observed entries, but this number is usually small compared to the full matrix you need about.",
            "It's usually order N log or something like that.",
            "You can see this as a special case of multitask learning.",
            "And they have one assumption underlying assumption here is you want to minimize.",
            "The rank of the matrix.",
            "Becaused this means that you have an underlying low dimensional or low rank factorization."
        ],
        [
            "Also, there are other related problems like multiview problems or multi label problems, domain adaptation or domain transfer.",
            "So these are closely related.",
            "Not exactly the same as mild.",
            "Ask the mildest problems.",
            "But of course, my task learning is a very broad question.",
            "Very broad problem.",
            "We cannot hope a single method finds and solves everything."
        ],
        [
            "OK, so let me start with the formulation I propose for this problem or one formulation.",
            "Um?",
            "So again, we have a sample of xti, so we have a double index be 'cause there is the index for the task and the index for for the example.",
            "So there are N tasks here.",
            "And we have some convex loss function here.",
            "Which penalizes in the usual way like squared loss or logistic loss or loss.",
            "And we propose to use this regularizer.",
            "So the rationale behind this is that.",
            "We would like to have a.",
            "Solve an SVM Aurora kernel problem in general, but using a common kernel for for all the tasks.",
            "So here the common kernel is this linear kernel with parameterized by the covariance matrix or positive semi definite matrix D. Of course we need to restrict this matrix because we cannot.",
            "We do not want to overfit on the regularizer, and the way we restrict it is we restrict the trace so the trace of this matrix has to be one.",
            "Of course there are other other constraints you could add.",
            "You could use there, but this one is important and.",
            "I will show why.",
            "Um but.",
            "Overall, this problem says that if you fix the.",
            "If you fix the kernel then it would be like a with the same like doing independent regularization problems like solving independence VM's for all the tasks for every task separately.",
            "But if you learn the kernel together with with the parameters for the tasks.",
            "Then it's the.",
            "Induces relations.",
            "Among the tasks.",
            "And.",
            "This prob."
        ],
        [
            "And can be seen to be jointly convex in both the variables.",
            "So it is easy to see this.",
            "It's also important to use this trace constraint.",
            "Or some other constraint that penalizes the number of features.",
            "So this is a similar constraint to the L1 constraint in in methods like lasso, L1 regularization etc.",
            "Um?",
            "So by penalize the number of the trace of D. And we implicitly ensure that our solution will have a low rank.",
            "Under conditions, so we tried.",
            "We don't try.",
            "We don't see it from this motivation, but we using this.",
            "Using this formulation we expect under in some situations at least to get a low rank solution.",
            "Now transfer learning doing transfer using this formulation is quite easy becausw after we have trained.",
            "So after training and finding the correct D, The correct kernel.",
            "Transferring means that you transferred the kernel to the new task.",
            "So you have a new sample from from task D prime.",
            "And you solve a regularization problem like an SVM using this new learned linear kernel."
        ],
        [
            "So it's a.",
            "This problem is convex and there are of course different ways to approach the problem.",
            "One way we tried and which is quite efficient in some experiments, is to alternately minimize between the two variables.",
            "The variable of the matrix variables WND.",
            "So you first.",
            "Each step.",
            "It's iteration you first fix your your D, your kernel and you can.",
            "This means that you can learn each of the tasks independently.",
            "So this means that you learn you do an.",
            "Problems.",
            "Um?",
            "And then you fix it.",
            "You fix your parameters.",
            "And you have to learn the, but this is given by a closed form.",
            "So basically you need to compute this matrix square root.",
            "Or equivalently you need to compute the SVD of the matrix W."
        ],
        [
            "So this is the costly step is basically usually it's the computing D 'cause you need to compute an SVD.",
            "And it turns out that most of these approaches cannot avoid.",
            "For solving this problem cannot avoid the SVD.",
            "So this is a big bottleneck usually.",
            "But if you look at the number of iterations this takes to convert, it's quite small becausw.",
            "Because you have an alternate imagination that.",
            "I act on these two big matrices of the problem.",
            "What I have here is I have compared this.",
            "Dotted dashed curves are correspond to the gradient descent.",
            "On the on the Matrix W with different learner learning rates.",
            "So you see that.",
            "Using the alternating approach, you convert in much fewer iterations.",
            "Um?"
        ],
        [
            "So there are other methods for this problem, and this problem has been studied by other people in in other areas like compressed sensing statistics and also in machine learning.",
            "As I mentioned.",
            "Most of these methods use need this singular value decomposition, but you can see there are other methods also with.",
            "Actor on on some equivalent formulation with.",
            "You did not use the SVD, but.",
            "They're also like this.",
            "This is a.",
            "A problem with many variables, so it's a matrix problem.",
            "And.",
            "I don't think asymptotically you cannot say you can avoid the same rate of this convergence rate.",
            "There are also, of course the SoC PRSD methods which are.",
            "Coming with those is that after a certain amount of variables you cannot really do anything.",
            "They take up all of your memory and it is really impossible to.",
            "To use them.",
            "At least in the current state with."
        ],
        [
            "And software methods.",
            "So this method.",
            "This method is equivalent essentially to the trace norm regularization, so it's equivalent to using this regularizer.",
            "Um?",
            "And solving solving the standard regularization problem so.",
            "Having an error 10 plus address normal regularizer.",
            "The trace norm or nuclear norm is a sum of the singular values of a matrix.",
            "So if you take the singular value, you penalize an L1 type of penalty on on the singular values.",
            "Intuitively, this says that under certain assumptions."
        ],
        [
            "Conditions.",
            "It would be similar to penalizing the rank of the Matrix so it's an effect similar to the L1L0 effect relation, which is has been studied recently quite a lot.",
            "So the argument of people from compressed sensing is that you you solve this problem.",
            "It's an NP hard problem.",
            "In fact, you cannot really solve it even for small sizes of the matrix.",
            "And you and the convex the best convex relaxation.",
            "We we can they came up with and people have come up with is to use the trace norm.",
            "So this is 1 rationale that under conditions you will get a low dimensional matrix for.",
            "As a solution."
        ],
        [
            "But there are many machine learning deputations why you want to do this.",
            "So regardless of whether you get a low rank matrix or not.",
            "So one interpretation is you want to learn a common kernel for all tasks, and this is a more general approach.",
            "You can apply to multitask learning.",
            "You can learn a common kernel and transfer it to new tasks.",
            "So this is what I discussed initially.",
            "There are other interpretations like it's immediate to see that this is the same as learning doing maximum likelihood and learning your covariance.",
            "Anne.",
            "And there is an alternative interpretation which comes from an alternative way to write the trace norm.",
            "So this is.",
            "Ann made this factorization essentially, which says that you minimize.",
            "Some norm on the factors of a matrix factorization.",
            "So there have been also some.",
            "There has been work that has shown has used this as a in a maximum posteriori context.",
            "There has also been a Gaussian process interpretation of this."
        ],
        [
            "But there is also another interesting connection or interpretation of this which has to do with.",
            "Regularizer used in statistics in particular and which is called the group lasso so the group lasso which I denote by this group, last last one or more mixed 21 norm.",
            "Which I denote by this.",
            "Is there is a sum of all the norms of the rows of a matrix?",
            "So if you take the L2 norms of its row and you apply essentially an L1 norm.",
            "That is, adding them up.",
            "Then you get this mixed number.",
            "And why you want to do this?",
            "You want to use this norm in many problems, 'cause when you want to do feature selection and you know that the features are common for all the tasks.",
            "So intuitively, you want all this.",
            "Most of this matrix to be to be 0, but a few of these rows to be to have non zero coefficients.",
            "So by penalizing using the L1 norm.",
            "There will be a tendency to get many zeros here.",
            "So the trace norm can be seen also as.",
            "As a rotation invariant form of group lasso, in the sense that if you minimize.",
            "This group lasso norm subject to all rotations in your in your space.",
            "You get the trace Norm as a minimal value.",
            "So it's like intuitively.",
            "Also, it's obvious 'cause you get get one of the two factors of the matrix."
        ],
        [
            "So to show how this works, briefly, this is just an experiment for it's a marketing type of experiment where there is a survey and.",
            "Um?",
            "A number of tasks, which is 100 tasks, 180 tasks and one it's person was was given eight PC models.",
            "So there are eight training examples that it's a training example consists of 13 attributes and that people have rated.",
            "Visa.",
            "These computers."
        ],
        [
            "So you see that.",
            "This algorithm in this method performs better than.",
            "Multi task, then separately independent length tasks also aggregate learning.",
            "If you put all of them in one vector.",
            "Also, the group lasso approach and there is an interesting characterization that you get.",
            "One feature which is important and has.",
            "An effect of Wayne.",
            "The different relevant characteristics.",
            "So in this problem you need to somehow mix features of your data.",
            "Into the variables of your data into one one feature.",
            "That the ways variables against one against the other."
        ],
        [
            "So this is this was regularization regularization formulation which can be easily generalized more more complicated or more general situations.",
            "So one clear one obvious first obvious thing is that if in the regularizer you said you take the Frobenius norm.",
            "Then you get the complete independence of the task.",
            "So it is a complete absence of multis, clearly.",
            "So one might think that why not.",
            "Am having a tradeoff between the L1 and L2.",
            "Type of effect.",
            "So L1 corresponds to the trace norm and L2 to three business norm.",
            "So in general you could have a PIN norm here and LP norm.",
            "Which is a certain LP norm.",
            "So this means you have an LP norm on the singular values of the matrix.",
            "In general, you can solve this with the same alternating algorithm.",
            "It's a convex problem of course, and you can generalize this also to bigger family of spectral functions.",
            "So you."
        ],
        [
            "This can give you a tradeoff between the.",
            "At the independence of the tasks and the multitask, strong multitask assumptions another another set up you could think of is.",
            "Maybe the tasks are not really one group of related tasks, but they have.",
            "Consist of two groups, so in one group you need one kernel.",
            "You need to learn one kernel in another group.",
            "You need to learn another one.",
            "So you need to learn.",
            "All these decay.",
            "So if you have K groups, you need all these kernels.",
            "And within this group you need to just minimize the previous regularization problem.",
            "But you further need to minimize over the groups and over the partitions of these of these tasks.",
            "And this is what makes this problem a nonconvex problem, because you have to take into account all the partitions of tasks into K groups.",
            "However you can you can.",
            "Attack this problem at least.",
            "Not at least local in a local way.",
            "Using the using some stochastic gradient descent and you can use the first method so you can use the K = 1 method, which was the trace norm regularization.",
            "As a starting point, then you can improve using two groups and so on.",
            "So this gives you an improvement over.",
            "Over the previous the simple case of all the tasks being assumed to be in the same group."
        ],
        [
            "So finally, all these methods there is an observation that all these methods satisfy some represented theorem of some sort.",
            "So I will describe what this represented theorem is.",
            "And this differs from the standard represented theorem, but this enables what this does is that it means that you can canalize all these methods.",
            "So instead of the.",
            "Of the input, you could have any feature map and you and your gram matrix could could be could come from any kernel, like a Gaussian kernel."
        ],
        [
            "So let me go back to the what.",
            "Represented theorems are or how they are defined, stated.",
            "In in the kernel literature.",
            "So in general, if you have a regularization problem and you have a regularizer on your on your W, where W is in some Hilbert space more generally.",
            "Then you know that the order your solution will be in a linear combination of your inputs.",
            "If this regularizer is a function of the V L2 norm.",
            "In fact, an increasing function of the L2 norm.",
            "So this is a standard single task represent."
        ],
        [
            "Theorem.",
            "And what we know is that it has to be the regularizer has to be of this form, where AIDS is an increasing function.",
            "For the represented theorem to hold in all these problems.",
            "So here this is a known result from some time ago, but we have also shown that this is a necessary and sufficient condition.",
            "So the necessary part means that only these regularizers are.",
            "I can give you the represented theater."
        ],
        [
            "Reason just."
        ],
        [
            "Protection.",
            "But what happens in Malta's case?",
            "So in the Multis case the previous result translates only in regularizers which are functions of the Frobenius norm if the Frobenius norm is if you have the Frobenius norm it's in the mall task setting, it means that you do independent regularizations, so this is useless and we don't want this.",
            "What we want though is we want some represented theorems, attacks that uses these regularizers.",
            "Where this can be things like the certain LP norms?",
            "And in particular, the trace node.",
            "So what happens is that you have another type of theorem.",
            "In which now it's task is a linear combination of all the inputs for all the tasks.",
            "And this makes intuitive sense.",
            "Becausw in the Multis case you need, it's of your your functions to depend on all the on the sample for all the tasks and not only on its own sample.",
            "Anne.",
            "This holds as a theorem, as I said, for all the sudden LP norms.",
            "Blue demonstration on.",
            "So in this matrix optimization problem.",
            "Where regularizer is a function of matrix, this Omega can be any of these certain LP."
        ],
        [
            "But more generally.",
            "It can be any any function of this form where age is decreasing and decreasing in a matrix sense.",
            "So it's it's increasing if you consider the the order of the ordering of matrices.",
            "In this sense.",
            "And this is an if and only if.",
            "So this is a necessary condition as well.",
            "Fast 5.",
            "Then well, I'm"
        ],
        [
            "OK, So what does this mean?",
            "That is this theorem.",
            "Tell us when you can catalyze regularization problem in the multitask case or in general, when you can kernelized this type of matrix optimization problem.",
            "In single task learning we essentially.",
            "This expression included only regularizers which were.",
            "Functions of the L2 norm.",
            "Or the Hilbert space norm more generally?",
            "So in a sense.",
            "This choice of the function did not really matter cause any increasing function would simply have.",
            "AFN affect univariate effect.",
            "In the in the in the Matrix case, however, we have this order which is.",
            "Which means that no, not, it's not true that any two matrices can be ordered like this.",
            "And this means that.",
            "This aids is important.",
            "In fact, choosing AIDS can give you different regularizers.",
            "So it can give you all these certain LP norms.",
            "It can even give you the rank, because this is also nondecreasing.",
            "But of course this is an NP hard problem.",
            "It can give you all orthogonally invariant norms which are certain there is a certain characterization for them.",
            "And other types of nodes so.",
            "It also includes the non convex problem I formulated.",
            "So we can give you all sorts of convex or nonconvex regularizer."
        ],
        [
            "So finally there is some some other things you can you can say about this represented theorem.",
            "Um?",
            "So before we saw that we saw this form.",
            "We saw.",
            "We show this type of represented theorem.",
            "So another way to write this is in matrix notation.",
            "Is.",
            "Right, right like this, so it's column of W is in the span of all these inputs, which are the.",
            "The columns of this matrix, so I put the total sample.",
            "In one matrix and then I write it represent theorem in this form.",
            "And you see why this makes the problem a kernel problem.",
            "A problem in terms of the gram matrix becausw.",
            "If you have a regularizer of the appropriate form it will.",
            "It means that the regularizer will be a function of X transpose X, which involves only the gram entries.",
            "The current the current values.",
            "Another question though, is what is the number of the degrees of freedom or the number of variables?",
            "In this in this problem, so you have now sees your variable.",
            "If you put the kernel the gram matrix.",
            "And the number of variables is.",
            "The total size times N, where N is the number of tasks.",
            "Becauses X has dimension of X is the total size.",
            "Of course, in some cases, in certain situations, this dimension might be small, comparatively.",
            "One situations when you have the same sample for its task.",
            "For example, in the experiment, for example, I showed there was the same the same sample becausw all the users rated the same product.",
            "But in general it's not true.",
            "So in general you have different.",
            "Different examples from different parts of your input space.",
            "So there is a natural question.",
            "Can we exist?",
            "And this number of degrees of freedom.",
            "Can this be reduced?",
            "And how do we relate this to do each of these representations where we're here?",
            "Each of the X?",
            "Assain corresponds to the inputs for all for each of the tasks.",
            "So this type of.",
            "Representation is the original.",
            "I.",
            "Like the the representation you would get from the particular task S."
        ],
        [
            "And it turns out that you get this type of form.",
            "So you you can combine these per task, let's say representations with a matrix R. These are these are positive definite matrix.",
            "But this means that.",
            "Intuitively, that input sample for this particular task.",
            "Appears everywhere with the same coefficients Alpha as.",
            "Up to some normalization that that that you obtained from this R. So the columns the corresponding rows of R give you this normalization, but.",
            "Um?",
            "But these are these coefficients are in all the tasks are multiples of these vectors.",
            "So another way to see it is that the Matrix see we had that it consists of blocks of rank one matrix, so it's.",
            "Vertical blocks of rank one matrices.",
            "Where it's rank one matrix corresponds to these alphas.",
            "To 1 task.",
            "And I end intuitively this makes sense, why becausw?",
            "Um?",
            "It's the effect of the influence of its task sample.",
            "Should be like a module, so it's not really differ.",
            "From from in the solution from between across the task vectors.",
            "So you might have different dependencies between tasks, like some tasks might be related in a strong way, so other tasks in in a week, your way, but.",
            "This does not affect this is one layer which does not affect.",
            "The sample within the tasks, so the sample within the tasks is is something like a black box."
        ],
        [
            "And also this decreases the number of degrees of freedom because now you have an addition.",
            "So some of these of the total sample size.",
            "And in order N square and this is this is."
        ],
        [
            "Not smaller than.",
            "Then what you had here.",
            "In some cases can be.",
            "An insignificant factor."
        ],
        [
            "Smaller.",
            "Um?",
            "And OK, it calls for all for all of signally invariant norms in a certain family, but it more specifically calls for all certain LP norms.",
            "And except the case of the spectral norm where you it calls for one of the solutions of your problem.",
            "So if you want to choose this solution, you can.",
            "In fact, it's the lowest rank solution.",
            "So if you want to choose the lowest rank solution, it also satisfies it in the spectrum."
        ],
        [
            "Old case.",
            "So.",
            "It's a I'm basically over, so I just described.",
            "I just gave an interesting why.",
            "Multitask learning is is a problem we would like to solve and it appears everywhere.",
            "Transfer learning is also a problem.",
            "We would like to solve.",
            "It is in many cases you can enhance performance by exploiting task relatedness.",
            "I proposed the specific way to attack the problem in many cases.",
            "More generally it's an instance of learning a common kernel for the tasks and transferring the current to new tasks.",
            "And the instances of this problem are the trace normalization, which has been studied recently by many people.",
            "The spectral norms, which are quite a relatively new thing.",
            "And some non convex regularizers for a task grouping.",
            "So finally I finally I gave necessary and sufficient conditions for the represented theorems, which is a more general problem in in kernel methods both in single task learning and multi task learning.",
            "But it turns out that many of these methods can be paralyzed.",
            "And you know you have a theorem that tells you when this can happen, for which regularization problems.",
            "Thanks."
        ],
        [
            "In one of your year end for me for dinner, he forwarded represent that they weren't expecting you or something like and there are different ability assumption.",
            "Yeah, it's there's no.",
            "Sorry.",
            "Yeah, it means that your your Omega.",
            "Your function has to be differentiable so.",
            "So this function Omega.",
            "Um?",
            "So this theorem the theorem holds for for differentiable functions, but this is a technicality I think, and it can be lifted.",
            "Easily, although I didn't haven't done it yet.",
            "But One Direction this theorem holds for 94 every function aids.",
            "Which is this for matrix nondecreasing?",
            "So if your Omega is of this form.",
            "When AIDS is matrix increasing, you get the representative theorem.",
            "But to get the nationality of the condition.",
            "You need some technical assumptions.",
            "I think that you need weaker assumptions than this, but.",
            "I saw it.",
            "I have signed need for defensible functions.",
            "But I suspect it can be shown for continuous functions.",
            "Bait"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to talk about several multi Tasker methods from the perspective of a regularization.",
                    "label": 0
                },
                {
                    "sent": "There are some connections, clearly with some Gaussian process methods that have been.",
                    "label": 0
                },
                {
                    "sent": "I will talk a little bit about the interpretations, so some of these methods have multiple interpretations, so you can view them in.",
                    "label": 0
                },
                {
                    "sent": "From my regularization perspective, Gaussian processes or another Bayesian perspective and so on.",
                    "label": 0
                },
                {
                    "sent": "And I will so it will be a little bit theoretical.",
                    "label": 0
                },
                {
                    "sent": "I will.",
                    "label": 0
                },
                {
                    "sent": "I will not give many results in experimental results about these methods, but you can.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can research this afterwards.",
                    "label": 0
                },
                {
                    "sent": "So finally I will give some theoretical results called represented theorems, which are probably you know, that we are very interested in them in.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Whenever we work with Karen methods.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Let me start by presenting the.",
                    "label": 0
                },
                {
                    "sent": "Like the framework General framework we have in this learning, so let me repeat that we learn from.",
                    "label": 0
                },
                {
                    "sent": "From a number of samples.",
                    "label": 0
                },
                {
                    "sent": "So we have N examples, but ask for simplicity, but this can be different numbers of examples.",
                    "label": 0
                },
                {
                    "sent": "And we just want to find a number of N functions FT that relate the input to the output space.",
                    "label": 0
                },
                {
                    "sent": "So we want to jointly find these functions because we believe we have some prior assumptions that these tasks are related.",
                    "label": 1
                },
                {
                    "sent": "And this is especially important when we have a few data points, but task.",
                    "label": 1
                },
                {
                    "sent": "So with a small sample we cannot do good prediction or classification per task and we need to exploit all of all of the tasks together.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another reason why we want to do this is what?",
                    "label": 1
                },
                {
                    "sent": "We call transfer transfer learning, which means that we get some kind of model which I will describe later on, but we get some some knowledge from the task we have.",
                    "label": 0
                },
                {
                    "sent": "And we want to transfer this knowledge to a new task and this new task has to be of course related to the previous ones.",
                    "label": 0
                },
                {
                    "sent": "But the question of course is what to transfer to this new task so that we get good performance.",
                    "label": 0
                },
                {
                    "sent": "Good prediction on this new task, especially when we have a few examples from this task.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me briefly repeat some review.",
                    "label": 0
                },
                {
                    "sent": "Some of the of the applications of multiple learning and why it's important.",
                    "label": 0
                },
                {
                    "sent": "One very famous one is what is called collaborative filtering or recommendation systems.",
                    "label": 1
                },
                {
                    "sent": "An example is the Netflix competition.",
                    "label": 1
                },
                {
                    "sent": "It's also found in many marketing databases.",
                    "label": 0
                },
                {
                    "sent": "Marketing problems.",
                    "label": 0
                },
                {
                    "sent": "So here you have.",
                    "label": 0
                },
                {
                    "sent": "A number of products and the attributes and.",
                    "label": 0
                },
                {
                    "sent": "This is given to some.",
                    "label": 0
                },
                {
                    "sent": "Some customer or user on the Internet and they have to assign a rating of how much they like the product.",
                    "label": 0
                },
                {
                    "sent": "The question, of course, is that you want to predict over on new products, and its task corresponds to its user.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is related to the matrix completion problem, which has been recently studied a lot in in other areas like compressed sensing and.",
                    "label": 1
                },
                {
                    "sent": "Essentially here the tasks are the columns correspond to the columns of a matrix.",
                    "label": 0
                },
                {
                    "sent": "You have a number of observed entries, but this number is usually small compared to the full matrix you need about.",
                    "label": 0
                },
                {
                    "sent": "It's usually order N log or something like that.",
                    "label": 0
                },
                {
                    "sent": "You can see this as a special case of multitask learning.",
                    "label": 1
                },
                {
                    "sent": "And they have one assumption underlying assumption here is you want to minimize.",
                    "label": 1
                },
                {
                    "sent": "The rank of the matrix.",
                    "label": 0
                },
                {
                    "sent": "Becaused this means that you have an underlying low dimensional or low rank factorization.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also, there are other related problems like multiview problems or multi label problems, domain adaptation or domain transfer.",
                    "label": 1
                },
                {
                    "sent": "So these are closely related.",
                    "label": 0
                },
                {
                    "sent": "Not exactly the same as mild.",
                    "label": 0
                },
                {
                    "sent": "Ask the mildest problems.",
                    "label": 1
                },
                {
                    "sent": "But of course, my task learning is a very broad question.",
                    "label": 0
                },
                {
                    "sent": "Very broad problem.",
                    "label": 0
                },
                {
                    "sent": "We cannot hope a single method finds and solves everything.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me start with the formulation I propose for this problem or one formulation.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So again, we have a sample of xti, so we have a double index be 'cause there is the index for the task and the index for for the example.",
                    "label": 0
                },
                {
                    "sent": "So there are N tasks here.",
                    "label": 0
                },
                {
                    "sent": "And we have some convex loss function here.",
                    "label": 0
                },
                {
                    "sent": "Which penalizes in the usual way like squared loss or logistic loss or loss.",
                    "label": 0
                },
                {
                    "sent": "And we propose to use this regularizer.",
                    "label": 0
                },
                {
                    "sent": "So the rationale behind this is that.",
                    "label": 0
                },
                {
                    "sent": "We would like to have a.",
                    "label": 0
                },
                {
                    "sent": "Solve an SVM Aurora kernel problem in general, but using a common kernel for for all the tasks.",
                    "label": 1
                },
                {
                    "sent": "So here the common kernel is this linear kernel with parameterized by the covariance matrix or positive semi definite matrix D. Of course we need to restrict this matrix because we cannot.",
                    "label": 0
                },
                {
                    "sent": "We do not want to overfit on the regularizer, and the way we restrict it is we restrict the trace so the trace of this matrix has to be one.",
                    "label": 0
                },
                {
                    "sent": "Of course there are other other constraints you could add.",
                    "label": 0
                },
                {
                    "sent": "You could use there, but this one is important and.",
                    "label": 0
                },
                {
                    "sent": "I will show why.",
                    "label": 0
                },
                {
                    "sent": "Um but.",
                    "label": 0
                },
                {
                    "sent": "Overall, this problem says that if you fix the.",
                    "label": 0
                },
                {
                    "sent": "If you fix the kernel then it would be like a with the same like doing independent regularization problems like solving independence VM's for all the tasks for every task separately.",
                    "label": 0
                },
                {
                    "sent": "But if you learn the kernel together with with the parameters for the tasks.",
                    "label": 0
                },
                {
                    "sent": "Then it's the.",
                    "label": 0
                },
                {
                    "sent": "Induces relations.",
                    "label": 0
                },
                {
                    "sent": "Among the tasks.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This prob.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And can be seen to be jointly convex in both the variables.",
                    "label": 0
                },
                {
                    "sent": "So it is easy to see this.",
                    "label": 0
                },
                {
                    "sent": "It's also important to use this trace constraint.",
                    "label": 0
                },
                {
                    "sent": "Or some other constraint that penalizes the number of features.",
                    "label": 0
                },
                {
                    "sent": "So this is a similar constraint to the L1 constraint in in methods like lasso, L1 regularization etc.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So by penalize the number of the trace of D. And we implicitly ensure that our solution will have a low rank.",
                    "label": 0
                },
                {
                    "sent": "Under conditions, so we tried.",
                    "label": 0
                },
                {
                    "sent": "We don't try.",
                    "label": 0
                },
                {
                    "sent": "We don't see it from this motivation, but we using this.",
                    "label": 0
                },
                {
                    "sent": "Using this formulation we expect under in some situations at least to get a low rank solution.",
                    "label": 0
                },
                {
                    "sent": "Now transfer learning doing transfer using this formulation is quite easy becausw after we have trained.",
                    "label": 0
                },
                {
                    "sent": "So after training and finding the correct D, The correct kernel.",
                    "label": 0
                },
                {
                    "sent": "Transferring means that you transferred the kernel to the new task.",
                    "label": 0
                },
                {
                    "sent": "So you have a new sample from from task D prime.",
                    "label": 0
                },
                {
                    "sent": "And you solve a regularization problem like an SVM using this new learned linear kernel.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's a.",
                    "label": 0
                },
                {
                    "sent": "This problem is convex and there are of course different ways to approach the problem.",
                    "label": 0
                },
                {
                    "sent": "One way we tried and which is quite efficient in some experiments, is to alternately minimize between the two variables.",
                    "label": 0
                },
                {
                    "sent": "The variable of the matrix variables WND.",
                    "label": 0
                },
                {
                    "sent": "So you first.",
                    "label": 0
                },
                {
                    "sent": "Each step.",
                    "label": 0
                },
                {
                    "sent": "It's iteration you first fix your your D, your kernel and you can.",
                    "label": 0
                },
                {
                    "sent": "This means that you can learn each of the tasks independently.",
                    "label": 0
                },
                {
                    "sent": "So this means that you learn you do an.",
                    "label": 0
                },
                {
                    "sent": "Problems.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And then you fix it.",
                    "label": 0
                },
                {
                    "sent": "You fix your parameters.",
                    "label": 0
                },
                {
                    "sent": "And you have to learn the, but this is given by a closed form.",
                    "label": 0
                },
                {
                    "sent": "So basically you need to compute this matrix square root.",
                    "label": 0
                },
                {
                    "sent": "Or equivalently you need to compute the SVD of the matrix W.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the costly step is basically usually it's the computing D 'cause you need to compute an SVD.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that most of these approaches cannot avoid.",
                    "label": 0
                },
                {
                    "sent": "For solving this problem cannot avoid the SVD.",
                    "label": 0
                },
                {
                    "sent": "So this is a big bottleneck usually.",
                    "label": 0
                },
                {
                    "sent": "But if you look at the number of iterations this takes to convert, it's quite small becausw.",
                    "label": 0
                },
                {
                    "sent": "Because you have an alternate imagination that.",
                    "label": 0
                },
                {
                    "sent": "I act on these two big matrices of the problem.",
                    "label": 0
                },
                {
                    "sent": "What I have here is I have compared this.",
                    "label": 0
                },
                {
                    "sent": "Dotted dashed curves are correspond to the gradient descent.",
                    "label": 1
                },
                {
                    "sent": "On the on the Matrix W with different learner learning rates.",
                    "label": 0
                },
                {
                    "sent": "So you see that.",
                    "label": 0
                },
                {
                    "sent": "Using the alternating approach, you convert in much fewer iterations.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are other methods for this problem, and this problem has been studied by other people in in other areas like compressed sensing statistics and also in machine learning.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned.",
                    "label": 0
                },
                {
                    "sent": "Most of these methods use need this singular value decomposition, but you can see there are other methods also with.",
                    "label": 0
                },
                {
                    "sent": "Actor on on some equivalent formulation with.",
                    "label": 0
                },
                {
                    "sent": "You did not use the SVD, but.",
                    "label": 0
                },
                {
                    "sent": "They're also like this.",
                    "label": 0
                },
                {
                    "sent": "This is a.",
                    "label": 0
                },
                {
                    "sent": "A problem with many variables, so it's a matrix problem.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I don't think asymptotically you cannot say you can avoid the same rate of this convergence rate.",
                    "label": 0
                },
                {
                    "sent": "There are also, of course the SoC PRSD methods which are.",
                    "label": 0
                },
                {
                    "sent": "Coming with those is that after a certain amount of variables you cannot really do anything.",
                    "label": 0
                },
                {
                    "sent": "They take up all of your memory and it is really impossible to.",
                    "label": 0
                },
                {
                    "sent": "To use them.",
                    "label": 0
                },
                {
                    "sent": "At least in the current state with.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And software methods.",
                    "label": 0
                },
                {
                    "sent": "So this method.",
                    "label": 0
                },
                {
                    "sent": "This method is equivalent essentially to the trace norm regularization, so it's equivalent to using this regularizer.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And solving solving the standard regularization problem so.",
                    "label": 0
                },
                {
                    "sent": "Having an error 10 plus address normal regularizer.",
                    "label": 0
                },
                {
                    "sent": "The trace norm or nuclear norm is a sum of the singular values of a matrix.",
                    "label": 1
                },
                {
                    "sent": "So if you take the singular value, you penalize an L1 type of penalty on on the singular values.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, this says that under certain assumptions.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conditions.",
                    "label": 0
                },
                {
                    "sent": "It would be similar to penalizing the rank of the Matrix so it's an effect similar to the L1L0 effect relation, which is has been studied recently quite a lot.",
                    "label": 0
                },
                {
                    "sent": "So the argument of people from compressed sensing is that you you solve this problem.",
                    "label": 0
                },
                {
                    "sent": "It's an NP hard problem.",
                    "label": 0
                },
                {
                    "sent": "In fact, you cannot really solve it even for small sizes of the matrix.",
                    "label": 1
                },
                {
                    "sent": "And you and the convex the best convex relaxation.",
                    "label": 1
                },
                {
                    "sent": "We we can they came up with and people have come up with is to use the trace norm.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 rationale that under conditions you will get a low dimensional matrix for.",
                    "label": 1
                },
                {
                    "sent": "As a solution.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But there are many machine learning deputations why you want to do this.",
                    "label": 0
                },
                {
                    "sent": "So regardless of whether you get a low rank matrix or not.",
                    "label": 0
                },
                {
                    "sent": "So one interpretation is you want to learn a common kernel for all tasks, and this is a more general approach.",
                    "label": 1
                },
                {
                    "sent": "You can apply to multitask learning.",
                    "label": 0
                },
                {
                    "sent": "You can learn a common kernel and transfer it to new tasks.",
                    "label": 0
                },
                {
                    "sent": "So this is what I discussed initially.",
                    "label": 0
                },
                {
                    "sent": "There are other interpretations like it's immediate to see that this is the same as learning doing maximum likelihood and learning your covariance.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And there is an alternative interpretation which comes from an alternative way to write the trace norm.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "Ann made this factorization essentially, which says that you minimize.",
                    "label": 0
                },
                {
                    "sent": "Some norm on the factors of a matrix factorization.",
                    "label": 0
                },
                {
                    "sent": "So there have been also some.",
                    "label": 0
                },
                {
                    "sent": "There has been work that has shown has used this as a in a maximum posteriori context.",
                    "label": 1
                },
                {
                    "sent": "There has also been a Gaussian process interpretation of this.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But there is also another interesting connection or interpretation of this which has to do with.",
                    "label": 0
                },
                {
                    "sent": "Regularizer used in statistics in particular and which is called the group lasso so the group lasso which I denote by this group, last last one or more mixed 21 norm.",
                    "label": 0
                },
                {
                    "sent": "Which I denote by this.",
                    "label": 0
                },
                {
                    "sent": "Is there is a sum of all the norms of the rows of a matrix?",
                    "label": 0
                },
                {
                    "sent": "So if you take the L2 norms of its row and you apply essentially an L1 norm.",
                    "label": 0
                },
                {
                    "sent": "That is, adding them up.",
                    "label": 0
                },
                {
                    "sent": "Then you get this mixed number.",
                    "label": 0
                },
                {
                    "sent": "And why you want to do this?",
                    "label": 0
                },
                {
                    "sent": "You want to use this norm in many problems, 'cause when you want to do feature selection and you know that the features are common for all the tasks.",
                    "label": 0
                },
                {
                    "sent": "So intuitively, you want all this.",
                    "label": 0
                },
                {
                    "sent": "Most of this matrix to be to be 0, but a few of these rows to be to have non zero coefficients.",
                    "label": 0
                },
                {
                    "sent": "So by penalizing using the L1 norm.",
                    "label": 0
                },
                {
                    "sent": "There will be a tendency to get many zeros here.",
                    "label": 0
                },
                {
                    "sent": "So the trace norm can be seen also as.",
                    "label": 0
                },
                {
                    "sent": "As a rotation invariant form of group lasso, in the sense that if you minimize.",
                    "label": 1
                },
                {
                    "sent": "This group lasso norm subject to all rotations in your in your space.",
                    "label": 0
                },
                {
                    "sent": "You get the trace Norm as a minimal value.",
                    "label": 0
                },
                {
                    "sent": "So it's like intuitively.",
                    "label": 0
                },
                {
                    "sent": "Also, it's obvious 'cause you get get one of the two factors of the matrix.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to show how this works, briefly, this is just an experiment for it's a marketing type of experiment where there is a survey and.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "A number of tasks, which is 100 tasks, 180 tasks and one it's person was was given eight PC models.",
                    "label": 0
                },
                {
                    "sent": "So there are eight training examples that it's a training example consists of 13 attributes and that people have rated.",
                    "label": 0
                },
                {
                    "sent": "Visa.",
                    "label": 0
                },
                {
                    "sent": "These computers.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you see that.",
                    "label": 0
                },
                {
                    "sent": "This algorithm in this method performs better than.",
                    "label": 0
                },
                {
                    "sent": "Multi task, then separately independent length tasks also aggregate learning.",
                    "label": 0
                },
                {
                    "sent": "If you put all of them in one vector.",
                    "label": 0
                },
                {
                    "sent": "Also, the group lasso approach and there is an interesting characterization that you get.",
                    "label": 0
                },
                {
                    "sent": "One feature which is important and has.",
                    "label": 0
                },
                {
                    "sent": "An effect of Wayne.",
                    "label": 0
                },
                {
                    "sent": "The different relevant characteristics.",
                    "label": 0
                },
                {
                    "sent": "So in this problem you need to somehow mix features of your data.",
                    "label": 0
                },
                {
                    "sent": "Into the variables of your data into one one feature.",
                    "label": 0
                },
                {
                    "sent": "That the ways variables against one against the other.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is this was regularization regularization formulation which can be easily generalized more more complicated or more general situations.",
                    "label": 0
                },
                {
                    "sent": "So one clear one obvious first obvious thing is that if in the regularizer you said you take the Frobenius norm.",
                    "label": 0
                },
                {
                    "sent": "Then you get the complete independence of the task.",
                    "label": 0
                },
                {
                    "sent": "So it is a complete absence of multis, clearly.",
                    "label": 0
                },
                {
                    "sent": "So one might think that why not.",
                    "label": 0
                },
                {
                    "sent": "Am having a tradeoff between the L1 and L2.",
                    "label": 0
                },
                {
                    "sent": "Type of effect.",
                    "label": 0
                },
                {
                    "sent": "So L1 corresponds to the trace norm and L2 to three business norm.",
                    "label": 0
                },
                {
                    "sent": "So in general you could have a PIN norm here and LP norm.",
                    "label": 0
                },
                {
                    "sent": "Which is a certain LP norm.",
                    "label": 1
                },
                {
                    "sent": "So this means you have an LP norm on the singular values of the matrix.",
                    "label": 1
                },
                {
                    "sent": "In general, you can solve this with the same alternating algorithm.",
                    "label": 1
                },
                {
                    "sent": "It's a convex problem of course, and you can generalize this also to bigger family of spectral functions.",
                    "label": 0
                },
                {
                    "sent": "So you.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This can give you a tradeoff between the.",
                    "label": 0
                },
                {
                    "sent": "At the independence of the tasks and the multitask, strong multitask assumptions another another set up you could think of is.",
                    "label": 0
                },
                {
                    "sent": "Maybe the tasks are not really one group of related tasks, but they have.",
                    "label": 0
                },
                {
                    "sent": "Consist of two groups, so in one group you need one kernel.",
                    "label": 0
                },
                {
                    "sent": "You need to learn one kernel in another group.",
                    "label": 0
                },
                {
                    "sent": "You need to learn another one.",
                    "label": 0
                },
                {
                    "sent": "So you need to learn.",
                    "label": 0
                },
                {
                    "sent": "All these decay.",
                    "label": 0
                },
                {
                    "sent": "So if you have K groups, you need all these kernels.",
                    "label": 1
                },
                {
                    "sent": "And within this group you need to just minimize the previous regularization problem.",
                    "label": 0
                },
                {
                    "sent": "But you further need to minimize over the groups and over the partitions of these of these tasks.",
                    "label": 0
                },
                {
                    "sent": "And this is what makes this problem a nonconvex problem, because you have to take into account all the partitions of tasks into K groups.",
                    "label": 1
                },
                {
                    "sent": "However you can you can.",
                    "label": 0
                },
                {
                    "sent": "Attack this problem at least.",
                    "label": 0
                },
                {
                    "sent": "Not at least local in a local way.",
                    "label": 1
                },
                {
                    "sent": "Using the using some stochastic gradient descent and you can use the first method so you can use the K = 1 method, which was the trace norm regularization.",
                    "label": 0
                },
                {
                    "sent": "As a starting point, then you can improve using two groups and so on.",
                    "label": 0
                },
                {
                    "sent": "So this gives you an improvement over.",
                    "label": 0
                },
                {
                    "sent": "Over the previous the simple case of all the tasks being assumed to be in the same group.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So finally, all these methods there is an observation that all these methods satisfy some represented theorem of some sort.",
                    "label": 0
                },
                {
                    "sent": "So I will describe what this represented theorem is.",
                    "label": 0
                },
                {
                    "sent": "And this differs from the standard represented theorem, but this enables what this does is that it means that you can canalize all these methods.",
                    "label": 0
                },
                {
                    "sent": "So instead of the.",
                    "label": 0
                },
                {
                    "sent": "Of the input, you could have any feature map and you and your gram matrix could could be could come from any kernel, like a Gaussian kernel.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me go back to the what.",
                    "label": 0
                },
                {
                    "sent": "Represented theorems are or how they are defined, stated.",
                    "label": 0
                },
                {
                    "sent": "In in the kernel literature.",
                    "label": 0
                },
                {
                    "sent": "So in general, if you have a regularization problem and you have a regularizer on your on your W, where W is in some Hilbert space more generally.",
                    "label": 0
                },
                {
                    "sent": "Then you know that the order your solution will be in a linear combination of your inputs.",
                    "label": 0
                },
                {
                    "sent": "If this regularizer is a function of the V L2 norm.",
                    "label": 0
                },
                {
                    "sent": "In fact, an increasing function of the L2 norm.",
                    "label": 0
                },
                {
                    "sent": "So this is a standard single task represent.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Theorem.",
                    "label": 0
                },
                {
                    "sent": "And what we know is that it has to be the regularizer has to be of this form, where AIDS is an increasing function.",
                    "label": 0
                },
                {
                    "sent": "For the represented theorem to hold in all these problems.",
                    "label": 0
                },
                {
                    "sent": "So here this is a known result from some time ago, but we have also shown that this is a necessary and sufficient condition.",
                    "label": 0
                },
                {
                    "sent": "So the necessary part means that only these regularizers are.",
                    "label": 0
                },
                {
                    "sent": "I can give you the represented theater.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reason just.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Protection.",
                    "label": 0
                },
                {
                    "sent": "But what happens in Malta's case?",
                    "label": 0
                },
                {
                    "sent": "So in the Multis case the previous result translates only in regularizers which are functions of the Frobenius norm if the Frobenius norm is if you have the Frobenius norm it's in the mall task setting, it means that you do independent regularizations, so this is useless and we don't want this.",
                    "label": 0
                },
                {
                    "sent": "What we want though is we want some represented theorems, attacks that uses these regularizers.",
                    "label": 0
                },
                {
                    "sent": "Where this can be things like the certain LP norms?",
                    "label": 0
                },
                {
                    "sent": "And in particular, the trace node.",
                    "label": 0
                },
                {
                    "sent": "So what happens is that you have another type of theorem.",
                    "label": 0
                },
                {
                    "sent": "In which now it's task is a linear combination of all the inputs for all the tasks.",
                    "label": 0
                },
                {
                    "sent": "And this makes intuitive sense.",
                    "label": 0
                },
                {
                    "sent": "Becausw in the Multis case you need, it's of your your functions to depend on all the on the sample for all the tasks and not only on its own sample.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "This holds as a theorem, as I said, for all the sudden LP norms.",
                    "label": 0
                },
                {
                    "sent": "Blue demonstration on.",
                    "label": 0
                },
                {
                    "sent": "So in this matrix optimization problem.",
                    "label": 1
                },
                {
                    "sent": "Where regularizer is a function of matrix, this Omega can be any of these certain LP.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But more generally.",
                    "label": 0
                },
                {
                    "sent": "It can be any any function of this form where age is decreasing and decreasing in a matrix sense.",
                    "label": 1
                },
                {
                    "sent": "So it's it's increasing if you consider the the order of the ordering of matrices.",
                    "label": 0
                },
                {
                    "sent": "In this sense.",
                    "label": 0
                },
                {
                    "sent": "And this is an if and only if.",
                    "label": 1
                },
                {
                    "sent": "So this is a necessary condition as well.",
                    "label": 0
                },
                {
                    "sent": "Fast 5.",
                    "label": 0
                },
                {
                    "sent": "Then well, I'm",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what does this mean?",
                    "label": 0
                },
                {
                    "sent": "That is this theorem.",
                    "label": 0
                },
                {
                    "sent": "Tell us when you can catalyze regularization problem in the multitask case or in general, when you can kernelized this type of matrix optimization problem.",
                    "label": 0
                },
                {
                    "sent": "In single task learning we essentially.",
                    "label": 0
                },
                {
                    "sent": "This expression included only regularizers which were.",
                    "label": 0
                },
                {
                    "sent": "Functions of the L2 norm.",
                    "label": 0
                },
                {
                    "sent": "Or the Hilbert space norm more generally?",
                    "label": 0
                },
                {
                    "sent": "So in a sense.",
                    "label": 0
                },
                {
                    "sent": "This choice of the function did not really matter cause any increasing function would simply have.",
                    "label": 0
                },
                {
                    "sent": "AFN affect univariate effect.",
                    "label": 0
                },
                {
                    "sent": "In the in the in the Matrix case, however, we have this order which is.",
                    "label": 0
                },
                {
                    "sent": "Which means that no, not, it's not true that any two matrices can be ordered like this.",
                    "label": 0
                },
                {
                    "sent": "And this means that.",
                    "label": 0
                },
                {
                    "sent": "This aids is important.",
                    "label": 0
                },
                {
                    "sent": "In fact, choosing AIDS can give you different regularizers.",
                    "label": 0
                },
                {
                    "sent": "So it can give you all these certain LP norms.",
                    "label": 1
                },
                {
                    "sent": "It can even give you the rank, because this is also nondecreasing.",
                    "label": 0
                },
                {
                    "sent": "But of course this is an NP hard problem.",
                    "label": 0
                },
                {
                    "sent": "It can give you all orthogonally invariant norms which are certain there is a certain characterization for them.",
                    "label": 1
                },
                {
                    "sent": "And other types of nodes so.",
                    "label": 0
                },
                {
                    "sent": "It also includes the non convex problem I formulated.",
                    "label": 0
                },
                {
                    "sent": "So we can give you all sorts of convex or nonconvex regularizer.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So finally there is some some other things you can you can say about this represented theorem.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So before we saw that we saw this form.",
                    "label": 0
                },
                {
                    "sent": "We saw.",
                    "label": 0
                },
                {
                    "sent": "We show this type of represented theorem.",
                    "label": 0
                },
                {
                    "sent": "So another way to write this is in matrix notation.",
                    "label": 1
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Right, right like this, so it's column of W is in the span of all these inputs, which are the.",
                    "label": 1
                },
                {
                    "sent": "The columns of this matrix, so I put the total sample.",
                    "label": 0
                },
                {
                    "sent": "In one matrix and then I write it represent theorem in this form.",
                    "label": 0
                },
                {
                    "sent": "And you see why this makes the problem a kernel problem.",
                    "label": 1
                },
                {
                    "sent": "A problem in terms of the gram matrix becausw.",
                    "label": 0
                },
                {
                    "sent": "If you have a regularizer of the appropriate form it will.",
                    "label": 0
                },
                {
                    "sent": "It means that the regularizer will be a function of X transpose X, which involves only the gram entries.",
                    "label": 0
                },
                {
                    "sent": "The current the current values.",
                    "label": 0
                },
                {
                    "sent": "Another question though, is what is the number of the degrees of freedom or the number of variables?",
                    "label": 0
                },
                {
                    "sent": "In this in this problem, so you have now sees your variable.",
                    "label": 0
                },
                {
                    "sent": "If you put the kernel the gram matrix.",
                    "label": 0
                },
                {
                    "sent": "And the number of variables is.",
                    "label": 0
                },
                {
                    "sent": "The total size times N, where N is the number of tasks.",
                    "label": 0
                },
                {
                    "sent": "Becauses X has dimension of X is the total size.",
                    "label": 0
                },
                {
                    "sent": "Of course, in some cases, in certain situations, this dimension might be small, comparatively.",
                    "label": 0
                },
                {
                    "sent": "One situations when you have the same sample for its task.",
                    "label": 0
                },
                {
                    "sent": "For example, in the experiment, for example, I showed there was the same the same sample becausw all the users rated the same product.",
                    "label": 0
                },
                {
                    "sent": "But in general it's not true.",
                    "label": 0
                },
                {
                    "sent": "So in general you have different.",
                    "label": 0
                },
                {
                    "sent": "Different examples from different parts of your input space.",
                    "label": 0
                },
                {
                    "sent": "So there is a natural question.",
                    "label": 0
                },
                {
                    "sent": "Can we exist?",
                    "label": 0
                },
                {
                    "sent": "And this number of degrees of freedom.",
                    "label": 0
                },
                {
                    "sent": "Can this be reduced?",
                    "label": 0
                },
                {
                    "sent": "And how do we relate this to do each of these representations where we're here?",
                    "label": 0
                },
                {
                    "sent": "Each of the X?",
                    "label": 0
                },
                {
                    "sent": "Assain corresponds to the inputs for all for each of the tasks.",
                    "label": 1
                },
                {
                    "sent": "So this type of.",
                    "label": 0
                },
                {
                    "sent": "Representation is the original.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Like the the representation you would get from the particular task S.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it turns out that you get this type of form.",
                    "label": 0
                },
                {
                    "sent": "So you you can combine these per task, let's say representations with a matrix R. These are these are positive definite matrix.",
                    "label": 0
                },
                {
                    "sent": "But this means that.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, that input sample for this particular task.",
                    "label": 1
                },
                {
                    "sent": "Appears everywhere with the same coefficients Alpha as.",
                    "label": 1
                },
                {
                    "sent": "Up to some normalization that that that you obtained from this R. So the columns the corresponding rows of R give you this normalization, but.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "But these are these coefficients are in all the tasks are multiples of these vectors.",
                    "label": 0
                },
                {
                    "sent": "So another way to see it is that the Matrix see we had that it consists of blocks of rank one matrix, so it's.",
                    "label": 0
                },
                {
                    "sent": "Vertical blocks of rank one matrices.",
                    "label": 1
                },
                {
                    "sent": "Where it's rank one matrix corresponds to these alphas.",
                    "label": 0
                },
                {
                    "sent": "To 1 task.",
                    "label": 0
                },
                {
                    "sent": "And I end intuitively this makes sense, why becausw?",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "It's the effect of the influence of its task sample.",
                    "label": 0
                },
                {
                    "sent": "Should be like a module, so it's not really differ.",
                    "label": 0
                },
                {
                    "sent": "From from in the solution from between across the task vectors.",
                    "label": 0
                },
                {
                    "sent": "So you might have different dependencies between tasks, like some tasks might be related in a strong way, so other tasks in in a week, your way, but.",
                    "label": 0
                },
                {
                    "sent": "This does not affect this is one layer which does not affect.",
                    "label": 0
                },
                {
                    "sent": "The sample within the tasks, so the sample within the tasks is is something like a black box.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And also this decreases the number of degrees of freedom because now you have an addition.",
                    "label": 0
                },
                {
                    "sent": "So some of these of the total sample size.",
                    "label": 1
                },
                {
                    "sent": "And in order N square and this is this is.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not smaller than.",
                    "label": 0
                },
                {
                    "sent": "Then what you had here.",
                    "label": 0
                },
                {
                    "sent": "In some cases can be.",
                    "label": 0
                },
                {
                    "sent": "An insignificant factor.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Smaller.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "And OK, it calls for all for all of signally invariant norms in a certain family, but it more specifically calls for all certain LP norms.",
                    "label": 1
                },
                {
                    "sent": "And except the case of the spectral norm where you it calls for one of the solutions of your problem.",
                    "label": 1
                },
                {
                    "sent": "So if you want to choose this solution, you can.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's the lowest rank solution.",
                    "label": 0
                },
                {
                    "sent": "So if you want to choose the lowest rank solution, it also satisfies it in the spectrum.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Old case.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It's a I'm basically over, so I just described.",
                    "label": 0
                },
                {
                    "sent": "I just gave an interesting why.",
                    "label": 0
                },
                {
                    "sent": "Multitask learning is is a problem we would like to solve and it appears everywhere.",
                    "label": 1
                },
                {
                    "sent": "Transfer learning is also a problem.",
                    "label": 0
                },
                {
                    "sent": "We would like to solve.",
                    "label": 0
                },
                {
                    "sent": "It is in many cases you can enhance performance by exploiting task relatedness.",
                    "label": 1
                },
                {
                    "sent": "I proposed the specific way to attack the problem in many cases.",
                    "label": 0
                },
                {
                    "sent": "More generally it's an instance of learning a common kernel for the tasks and transferring the current to new tasks.",
                    "label": 0
                },
                {
                    "sent": "And the instances of this problem are the trace normalization, which has been studied recently by many people.",
                    "label": 0
                },
                {
                    "sent": "The spectral norms, which are quite a relatively new thing.",
                    "label": 0
                },
                {
                    "sent": "And some non convex regularizers for a task grouping.",
                    "label": 0
                },
                {
                    "sent": "So finally I finally I gave necessary and sufficient conditions for the represented theorems, which is a more general problem in in kernel methods both in single task learning and multi task learning.",
                    "label": 1
                },
                {
                    "sent": "But it turns out that many of these methods can be paralyzed.",
                    "label": 0
                },
                {
                    "sent": "And you know you have a theorem that tells you when this can happen, for which regularization problems.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In one of your year end for me for dinner, he forwarded represent that they weren't expecting you or something like and there are different ability assumption.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's there's no.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it means that your your Omega.",
                    "label": 0
                },
                {
                    "sent": "Your function has to be differentiable so.",
                    "label": 0
                },
                {
                    "sent": "So this function Omega.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this theorem the theorem holds for for differentiable functions, but this is a technicality I think, and it can be lifted.",
                    "label": 0
                },
                {
                    "sent": "Easily, although I didn't haven't done it yet.",
                    "label": 0
                },
                {
                    "sent": "But One Direction this theorem holds for 94 every function aids.",
                    "label": 0
                },
                {
                    "sent": "Which is this for matrix nondecreasing?",
                    "label": 1
                },
                {
                    "sent": "So if your Omega is of this form.",
                    "label": 1
                },
                {
                    "sent": "When AIDS is matrix increasing, you get the representative theorem.",
                    "label": 0
                },
                {
                    "sent": "But to get the nationality of the condition.",
                    "label": 0
                },
                {
                    "sent": "You need some technical assumptions.",
                    "label": 0
                },
                {
                    "sent": "I think that you need weaker assumptions than this, but.",
                    "label": 0
                },
                {
                    "sent": "I saw it.",
                    "label": 0
                },
                {
                    "sent": "I have signed need for defensible functions.",
                    "label": 0
                },
                {
                    "sent": "But I suspect it can be shown for continuous functions.",
                    "label": 0
                },
                {
                    "sent": "Bait",
                    "label": 0
                }
            ]
        }
    }
}