{
    "id": "zqok4xsjncgyuadxf6ddsljuh2n3tr4i",
    "title": "Deep Learning via Semi-Supervised Embedding",
    "info": {
        "author": [
            "Jason Weston, Facebook"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_weston_dlss/",
    "segmentation": [
        [
            "Hello, sorry for the.",
            "Slight disruption, so yeah, this is joint work with Ronald Koehler, Bear Frederic, Ratle Hossain, Mohammad Mobile, he Pavel Cook, certain Corey.",
            "I'm not going to try and pronounce that maybe maybe so he's in Jan's group.",
            "OK thanks."
        ],
        [
            "This is I.",
            "Isn't good.",
            "So this talk is about looking at deep learning, semi supervised deep learning in a multi task learning context so you can see the Hinton, Lacon, Bengio, groups work in this kind of context where they use an autoencoder as to train an unsupervised task as the auxiliary task and then uh.",
            "You know, train their supervised task as the other task, but they do it not at the same time but pre training one and then the other.",
            "But still you could see it as multitasking.",
            "And but in this work we're going to propose instead of an encoder decoder model like in the Hinton, Lakanen, Bengio approaches.",
            "We're going to just use an encoder only method which is just to use.",
            "Embedding algorithms like dimensionality reduction algorithm to do the semi supervised learning, which is just what people are doing in like with support vector machines with like the Laplacian SVM.",
            "So we're going to use that kind of regularizer and show that that has some nice results.",
            "Yeah, it's easy to implement and think about, it's simple.",
            "Faster train and seems to work well.",
            "And then we're going to.",
            "We're going to show some first experiments on small datasets that show that works and then apply it to video and text as well.",
            "How do I OK so my first slides are just like?"
        ],
        [
            "A recap of the nice tutorials of Jan and Joshua.",
            "Neural networks, they seem really cool, but standard backdrop might not always give good results.",
            "So one argument is that.",
            "When you do the backpropagation with a supervised target, you're going all the way from the output backwards, so the gradients get very small on these earlier layers, or these earlier layers.",
            "This is a convolutional net, and this is just a normal normal net and yeah, and so those those layers, the earlier ones don't get trained.",
            "So if you multitask with the task that isn't as deep.",
            "And it's sharing these earlier layers.",
            "Then you could think it's going to train those layers more, right?",
            "So that's why you're greedily."
        ],
        [
            "You're greedily starting from the beginning with these kind of encoder decoder.",
            "Methods.",
            "So I don't really need to talk about that, more so if you look at it from.",
            "From this perspective."
        ],
        [
            "Of what deep learning type researchers are doing and what people doing like kernel methods are doing.",
            "Then you could see these two sort of points of view.",
            "So the deep researchers could be saying that and this was, you know these.",
            "Some of these points are in the tutorial.",
            "What we want to use deep networks because you can learn some tasks in the layers and you know you can like in the previous talk you can share share these things that you learn with multiple tasks and this should be essential for hard tasks and you can't do that with this shallow system so.",
            "And also that's what you were saying.",
            "The nonlinear nonlinearity is sufficient.",
            "So like yeah, like in kernel methods.",
            "If you train on a lot of points and you just don't do any smart tricks, you have a huge number of support vectors or something and it's really slow.",
            "But on the other hand, people doing like kernel methods might say are these neural Nets?",
            "They look really complicated messy.",
            "Then you deep methods like these ones might look even more complicated and messy.",
            "And shallow methods give clean, valuable insights to what works.",
            "And so my point of view is like, let's try and take all the nice things that shallow researchers do and try and put it into the deep algorithm."
        ],
        [
            "So.",
            "We're just going to do that so very high level.",
            "And then Zang proposed a kind of in this.",
            "In German law 2005 framework for multi task learning, and they did it just with like linear classifiers.",
            "I think where you propose like pseudo supervised tasks on unlabeled data, and you multitask with those to try and improve your error rate.",
            "Semi supervised.",
            "So you have a large number of unlabeled data, presumably in the small number of supervised.",
            "So they do things like predict the middle word.",
            "Given a window of words and then perform really well on like NLP tasks.",
            "And so in convex learning you have to multitask.",
            "You have to train the two tasks at the same time, right?",
            "'cause otherwise if you train the unsupervised 1 first and then train the other one is convex and there's a unique solution.",
            "So you're just going to forget what you had.",
            "But with non convex systems you could see why doing one and then the other.",
            "I mean has been said.",
            "You can still retain some of the memory of what you did before, like you're only going to move the weights so far with the supervised learning.",
            "So for me that sort of while you could see it as multi task learning, but you sort of you didn't do them at the same time, but it still has a similar effect.",
            "But for us, in this work, we're going to sort of look at this very general framework and just choose a particular kind of super pseudo supervised task which is pairwise embedding average."
        ],
        [
            "Circles gone one too many, so there's lots of like kernel method type, dimensionality reduction embedding algorithms like Laplacian, Eigen Maps, Isomap, multidimensional scaling and they all have the kind of flavor where you take pairs of inputs and you have some you have some prior on which pairs are similar in which are dissimilar and typically this is done very naively by just saying.",
            "The nearest neighbors are a point I have are the ones that are similar in the high dimensional space and then you learn a low dimensional embedding which tries to retain that.",
            "With this kind of loss function on the pairs.",
            "And yeah, so it's Laplacian eigen map does exactly that.",
            "And.",
            "It gets very nice embeddings and then just cutting forward so people have used that with support vector machines to add as a regularizer to.",
            "This is a normal SVM objective function so you add this term which is like the Laplacian eigen map.",
            "Objective function and what that does is if the cluster assumption is true.",
            "So like the points that you identified as WI J equals one.",
            "A similar do indeed tend to have the same label.",
            "Then this thing is going to push the outputs of the SVM for the unlabeled points that you believe have the same label to be the same.",
            "So it tends to.",
            "Make these clusters of points agree Ann, you end up getting better error rates than if you just learned with a small amount of supervised data, so.",
            "There's no."
        ],
        [
            "Other embeddings embedding set of embedding algorithms for neural networks, which are the analog of those embedding algorithms which are Siamese networks, right?",
            "It's the same type of loss function.",
            "You have a pair of examples and you push them together, or if they're similar and Pullman parties are dissimilar and like one particular loss function which is the Doctor Lim algorithm from Jan's Group has looks like this for the similar pairs you push them together.",
            "Squared distance an for the other dissimilar you pulling parts have a margin of at least M, So this sort of looks just like the Laplacian eigen map thing.",
            "I so you can think of it as the neural network analogue of that.",
            "I mean, there's some differences like this.",
            "I mean this function you're learning now is a neural network.",
            "It can be any general neural network, whereas in Laplacian eigen map is the just learning vectors, one vector for every example.",
            "Actually like completely nonlinear.",
            "It's not actually a mapping that you can apply to new Point scored the out of sample extension problem that people have tried to address.",
            "But here there's no problem like that is learning a function so you can just apply.",
            "To any point, you can also include prior knowledge in the function, because it's in your network, so there's lots of."
        ],
        [
            "Nice things about it, but then there's basically that's that's our trick.",
            "So you've got the SVM plus that regularizer right?",
            "For Laplacian SVM we can do the same thing.",
            "We're just going to put neural net here.",
            "And instead of passing Eigen map objective here, we're going to put the Siamese net objective.",
            "The.",
            "Difference is we don't have to just put it on the output like they did on the SM.",
            "So the."
        ],
        [
            "Yeah, mine has one layers.",
            "You can only.",
            "There's only one place to put it on the regularizer on the output, right?",
            "But we can regularize the output of any of the layers of the neural network, so the output of this layer is the input features to the next layer, right?",
            "So this this output.",
            "Here we can say for two examples that we believe should have a similar representation, we push them together.",
            "That point there.",
            "So if you want so essentially you can see three different where we see three different ways of doing it.",
            "You can either put it the output, put it somewhere in one of the internal layers, or maybe a little bit more tricky.",
            "You could make some extra layer that's not actually shared with the with the rest of the network.",
            "That's that you're using for your supervised task, and then do the.",
            "Do the loss function on that and the reason why that might be good is because you've got one more linear map that's rescaling the data so that when you're asking for this margin, M are pushing the points apart you.",
            "You don't force them apart too much for the supervised task."
        ],
        [
            "So that's that's the algorithm.",
            "So we just literally do that with stochastic gradient descent.",
            "And we don't do that pretraining, by the way, we just do it like in SVM, both objectives at the same time.",
            "So it's very simple.",
            "Just make the grade."
        ],
        [
            "Updates.",
            "Now.",
            "The.",
            "The thing is, what people did with Laplacian Eigen Maps mostly just using K nearest neighbor.",
            "This idea is much more general than that you can take if you have any kind of prior on pairs, you can use it for example in videos which is what we did in our ICML talk that Renan gave yesterday's.",
            "You could say 2 frames of video next to each other that you know might have the same objects and things of the same labels.",
            "You can use those as the pairs for embedding or if you have text you could take a word in its neighbors and use that.",
            "Or, you know, you could use stuff from the web as well.",
            "Lots of possibilities.",
            "Some."
        ],
        [
            "She's fired and have really got time to go into.",
            "The more you could see this thing as an encoder without a decoder and compared to the encoder decoder methods of other other groups that might be nice that you don't have to do the decoding step.",
            "For example, you have a very sparse representation is input when you did, when you decode it again, you would be dense within normal methods so.",
            "This could be really slow.",
            "With this method.",
            "You don't need to do that."
        ],
        [
            "So now just some simple experiments on typical semi supervised datasets that that people use to measure things like Laplacian SVM.",
            "This is this is a normal neural net and this is adding this regularizer on three of these small datasets."
        ],
        [
            "And this is just a two layer neural Nets.",
            "Essentially you get the same kind of improvement from SFMTA Laplacian SVM as you get from neural net to this nor net with the embedding so OK."
        ],
        [
            "Doing the same thing.",
            "But now we can look at.",
            "This is on three different sides of emnace, with 100 training points 600 and 1000.",
            "You can do the same thing, but embedding not on the output layer but on one of the internal layers or one of these auxiliary layer that I proposed and all of those seem to work.",
            "So this is the normal neural net.",
            "So this is the difference.",
            "Once you add the semi supervised learning and this is where the convolutional net any kind of network you can do this, there's no restriction.",
            "You're also going to see improvements."
        ],
        [
            "So that was only with two layer Nets and now this is the same kind of results where I go from 2 layers for later 6, eight 10:15 and with a normal neural net you're going to see the test error here.",
            "Getting bigger and bigger the training area you can easily get its zero.",
            "In this case actually, but it's just over 15 an with the embedding on the output, just on the output layer you are seeing improvement actually, and this is where you do the embedding regularization.",
            "All the layers at the same time.",
            "You see a further improvement.",
            "And that's that's better than SVM and T SVM in that case.",
            "So."
        ],
        [
            "Yeah, that that please conclusion so far it's simple method, easy to train.",
            "We didn't do any pre training.",
            "There's no decoding step.",
            "And yeah now how long have I got like?",
            "5 minutes, yeah, now we just show quickly some experiments on video and text so."
        ],
        [
            "Deep learning for video, so this this is just a little summary of Reynolds talk."
        ],
        [
            "Yesterday is basically we take the the frames consecutive frames of video as the.",
            "Regularizer.",
            "OK, so so presumably you could do things like learning variance, suppose illumination background clutter will kind of things with that."
        ],
        [
            "We actually did a very simple experiment.",
            "We just took oil 100, which is 100 objects 70 two different poses.",
            "We recorded our own video of objects that look similar, so we call it coil like and we took these plastic animals like 60 of those and took videos as well and use these two as unsupervised video.",
            "We also like pretended that was video as well."
        ],
        [
            "In another experiment and just quickly go for that, there's two setups, so anyway, one where you have 30 objects and one where you have a."
        ],
        [
            "And so if you use the fake video from coil, you get this like ridiculous jumping error rate from a standard CNN.",
            "But that's kind of cheating 'cause you have, like the tests images.",
            "Actually in that video.",
            "In this setup you don't get semi supervised.",
            "We have 30 objects in the other 70 we put in the video and you still get a big jump an with these other videos that we made which have completely different set illumination settings and so on.",
            "Or different objects in this case.",
            "You still get an improvement.",
            "Not as much.",
            "Yeah so."
        ],
        [
            "So basically, yeah, then you're up to's to state of the art without feature engineering.",
            "And.",
            "Yes, and we did the same thing in text.",
            "This is probably our biggest experiment, so we took different tiles, part of speech tagging, chunking.",
            "Any R and SRL.",
            "In"
        ],
        [
            "NOP is really quite striking what people do because they make a cascade of classifiers like they do a part of speech."
        ],
        [
            "Tiger then a pause are in the each time they hang code what the outputs to each of these tasks are going labeled data and their hand code.",
            "The input features that based on those predictions.",
            "The next layer and they do that and it really sort of looks like a neural network, but like nothing is trained between.",
            "It's all independent.",
            "Mind you can set them up as convex problems then, but we just try and train them."
        ],
        [
            "Thing and we use the CNN and the each so we only have words as input and the words are represented as like 50 dimensional features and we're going to do an embedding."
        ],
        [
            "Some of the words just like before, so we're going to take like some neighborhood of words plus or word in unsupervised data.",
            "So we take Wikipedia, which is 630 million examples of such such pairs.",
            "And we do the embedding algorithm the same time as we train these."
        ],
        [
            "Tasks this was kind of what the embeddings look like, so afterwards, so you get like the country France is close to these other countries in the embedding space, so these things look nice, but.",
            "The error rates we get big improvements."
        ],
        [
            "So over not doing the semi supervised learning.",
            "So this is the normal CNN is most striking here.",
            "This is F1 score on the semantic role labeling task.",
            "Go from 51 to 72 and after we've done that we're at the state of the art of the shallow and methods that don't use all the feature engineering.",
            "Yeah, that's that's it."
        ],
        [
            "So when I do preach, I don't do pretraining.",
            "But yeah, when I'm training, I backdrop through all the layers.",
            "Yeah, so if the embedding is on the output, that's really going through all of them.",
            "You could actually try it.",
            "Not doing that, and I suspect it might still do something good.",
            "CNN.",
            "Yeah.",
            "Convolutional net without the semi supervised learning.",
            "So when we add the embedding algorithm we call it in bed scene and.",
            "So.",
            "How is what trained?",
            "Specific background.",
            "Oh, it's like the same, the same network, but just on the supervised data.",
            "It's not semi supervised.",
            "Right, right, right, yeah."
        ],
        [
            "So it's all these results were analogous every in each table.",
            "I basically showed the supervised network and the semi supervised network using the embedding algorithm and every time I was getting an improvement in this.",
            "Basically.",
            "Yeah, maybe one more question.",
            "So we have for this consultation CLU training separately.",
            "We actually we try both, and because, yeah, that's another kind of multitasking right where you multitask with supervised task.",
            "And actually the improvement you get from multitasking.",
            "The other supervised tasks are not that much is something, but not that much.",
            "I don't have the graphs here, but the unsupervised task makes a much bigger difference.",
            "But there's two reasons why there might be.",
            "One is there's.",
            "There's only.",
            "A million training examples supervised and the other.",
            "The other problem is that it's actually the same data.",
            "It's all Wall Street Journal, so it's not like you're even seeing anything knew in the inputs, only the output is new.",
            "Speak again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello, sorry for the.",
                    "label": 0
                },
                {
                    "sent": "Slight disruption, so yeah, this is joint work with Ronald Koehler, Bear Frederic, Ratle Hossain, Mohammad Mobile, he Pavel Cook, certain Corey.",
                    "label": 1
                },
                {
                    "sent": "I'm not going to try and pronounce that maybe maybe so he's in Jan's group.",
                    "label": 0
                },
                {
                    "sent": "OK thanks.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is I.",
                    "label": 0
                },
                {
                    "sent": "Isn't good.",
                    "label": 0
                },
                {
                    "sent": "So this talk is about looking at deep learning, semi supervised deep learning in a multi task learning context so you can see the Hinton, Lacon, Bengio, groups work in this kind of context where they use an autoencoder as to train an unsupervised task as the auxiliary task and then uh.",
                    "label": 1
                },
                {
                    "sent": "You know, train their supervised task as the other task, but they do it not at the same time but pre training one and then the other.",
                    "label": 1
                },
                {
                    "sent": "But still you could see it as multitasking.",
                    "label": 1
                },
                {
                    "sent": "And but in this work we're going to propose instead of an encoder decoder model like in the Hinton, Lakanen, Bengio approaches.",
                    "label": 0
                },
                {
                    "sent": "We're going to just use an encoder only method which is just to use.",
                    "label": 0
                },
                {
                    "sent": "Embedding algorithms like dimensionality reduction algorithm to do the semi supervised learning, which is just what people are doing in like with support vector machines with like the Laplacian SVM.",
                    "label": 0
                },
                {
                    "sent": "So we're going to use that kind of regularizer and show that that has some nice results.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's easy to implement and think about, it's simple.",
                    "label": 0
                },
                {
                    "sent": "Faster train and seems to work well.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to show some first experiments on small datasets that show that works and then apply it to video and text as well.",
                    "label": 0
                },
                {
                    "sent": "How do I OK so my first slides are just like?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A recap of the nice tutorials of Jan and Joshua.",
                    "label": 0
                },
                {
                    "sent": "Neural networks, they seem really cool, but standard backdrop might not always give good results.",
                    "label": 1
                },
                {
                    "sent": "So one argument is that.",
                    "label": 0
                },
                {
                    "sent": "When you do the backpropagation with a supervised target, you're going all the way from the output backwards, so the gradients get very small on these earlier layers, or these earlier layers.",
                    "label": 0
                },
                {
                    "sent": "This is a convolutional net, and this is just a normal normal net and yeah, and so those those layers, the earlier ones don't get trained.",
                    "label": 0
                },
                {
                    "sent": "So if you multitask with the task that isn't as deep.",
                    "label": 0
                },
                {
                    "sent": "And it's sharing these earlier layers.",
                    "label": 0
                },
                {
                    "sent": "Then you could think it's going to train those layers more, right?",
                    "label": 0
                },
                {
                    "sent": "So that's why you're greedily.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're greedily starting from the beginning with these kind of encoder decoder.",
                    "label": 0
                },
                {
                    "sent": "Methods.",
                    "label": 0
                },
                {
                    "sent": "So I don't really need to talk about that, more so if you look at it from.",
                    "label": 0
                },
                {
                    "sent": "From this perspective.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of what deep learning type researchers are doing and what people doing like kernel methods are doing.",
                    "label": 0
                },
                {
                    "sent": "Then you could see these two sort of points of view.",
                    "label": 0
                },
                {
                    "sent": "So the deep researchers could be saying that and this was, you know these.",
                    "label": 0
                },
                {
                    "sent": "Some of these points are in the tutorial.",
                    "label": 0
                },
                {
                    "sent": "What we want to use deep networks because you can learn some tasks in the layers and you know you can like in the previous talk you can share share these things that you learn with multiple tasks and this should be essential for hard tasks and you can't do that with this shallow system so.",
                    "label": 0
                },
                {
                    "sent": "And also that's what you were saying.",
                    "label": 0
                },
                {
                    "sent": "The nonlinear nonlinearity is sufficient.",
                    "label": 0
                },
                {
                    "sent": "So like yeah, like in kernel methods.",
                    "label": 0
                },
                {
                    "sent": "If you train on a lot of points and you just don't do any smart tricks, you have a huge number of support vectors or something and it's really slow.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, people doing like kernel methods might say are these neural Nets?",
                    "label": 0
                },
                {
                    "sent": "They look really complicated messy.",
                    "label": 0
                },
                {
                    "sent": "Then you deep methods like these ones might look even more complicated and messy.",
                    "label": 1
                },
                {
                    "sent": "And shallow methods give clean, valuable insights to what works.",
                    "label": 0
                },
                {
                    "sent": "And so my point of view is like, let's try and take all the nice things that shallow researchers do and try and put it into the deep algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We're just going to do that so very high level.",
                    "label": 0
                },
                {
                    "sent": "And then Zang proposed a kind of in this.",
                    "label": 0
                },
                {
                    "sent": "In German law 2005 framework for multi task learning, and they did it just with like linear classifiers.",
                    "label": 0
                },
                {
                    "sent": "I think where you propose like pseudo supervised tasks on unlabeled data, and you multitask with those to try and improve your error rate.",
                    "label": 1
                },
                {
                    "sent": "Semi supervised.",
                    "label": 0
                },
                {
                    "sent": "So you have a large number of unlabeled data, presumably in the small number of supervised.",
                    "label": 1
                },
                {
                    "sent": "So they do things like predict the middle word.",
                    "label": 0
                },
                {
                    "sent": "Given a window of words and then perform really well on like NLP tasks.",
                    "label": 1
                },
                {
                    "sent": "And so in convex learning you have to multitask.",
                    "label": 0
                },
                {
                    "sent": "You have to train the two tasks at the same time, right?",
                    "label": 0
                },
                {
                    "sent": "'cause otherwise if you train the unsupervised 1 first and then train the other one is convex and there's a unique solution.",
                    "label": 0
                },
                {
                    "sent": "So you're just going to forget what you had.",
                    "label": 0
                },
                {
                    "sent": "But with non convex systems you could see why doing one and then the other.",
                    "label": 0
                },
                {
                    "sent": "I mean has been said.",
                    "label": 0
                },
                {
                    "sent": "You can still retain some of the memory of what you did before, like you're only going to move the weights so far with the supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So for me that sort of while you could see it as multi task learning, but you sort of you didn't do them at the same time, but it still has a similar effect.",
                    "label": 0
                },
                {
                    "sent": "But for us, in this work, we're going to sort of look at this very general framework and just choose a particular kind of super pseudo supervised task which is pairwise embedding average.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Circles gone one too many, so there's lots of like kernel method type, dimensionality reduction embedding algorithms like Laplacian, Eigen Maps, Isomap, multidimensional scaling and they all have the kind of flavor where you take pairs of inputs and you have some you have some prior on which pairs are similar in which are dissimilar and typically this is done very naively by just saying.",
                    "label": 0
                },
                {
                    "sent": "The nearest neighbors are a point I have are the ones that are similar in the high dimensional space and then you learn a low dimensional embedding which tries to retain that.",
                    "label": 0
                },
                {
                    "sent": "With this kind of loss function on the pairs.",
                    "label": 0
                },
                {
                    "sent": "And yeah, so it's Laplacian eigen map does exactly that.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It gets very nice embeddings and then just cutting forward so people have used that with support vector machines to add as a regularizer to.",
                    "label": 0
                },
                {
                    "sent": "This is a normal SVM objective function so you add this term which is like the Laplacian eigen map.",
                    "label": 0
                },
                {
                    "sent": "Objective function and what that does is if the cluster assumption is true.",
                    "label": 0
                },
                {
                    "sent": "So like the points that you identified as WI J equals one.",
                    "label": 0
                },
                {
                    "sent": "A similar do indeed tend to have the same label.",
                    "label": 0
                },
                {
                    "sent": "Then this thing is going to push the outputs of the SVM for the unlabeled points that you believe have the same label to be the same.",
                    "label": 0
                },
                {
                    "sent": "So it tends to.",
                    "label": 0
                },
                {
                    "sent": "Make these clusters of points agree Ann, you end up getting better error rates than if you just learned with a small amount of supervised data, so.",
                    "label": 0
                },
                {
                    "sent": "There's no.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other embeddings embedding set of embedding algorithms for neural networks, which are the analog of those embedding algorithms which are Siamese networks, right?",
                    "label": 0
                },
                {
                    "sent": "It's the same type of loss function.",
                    "label": 0
                },
                {
                    "sent": "You have a pair of examples and you push them together, or if they're similar and Pullman parties are dissimilar and like one particular loss function which is the Doctor Lim algorithm from Jan's Group has looks like this for the similar pairs you push them together.",
                    "label": 0
                },
                {
                    "sent": "Squared distance an for the other dissimilar you pulling parts have a margin of at least M, So this sort of looks just like the Laplacian eigen map thing.",
                    "label": 1
                },
                {
                    "sent": "I so you can think of it as the neural network analogue of that.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's some differences like this.",
                    "label": 0
                },
                {
                    "sent": "I mean this function you're learning now is a neural network.",
                    "label": 0
                },
                {
                    "sent": "It can be any general neural network, whereas in Laplacian eigen map is the just learning vectors, one vector for every example.",
                    "label": 0
                },
                {
                    "sent": "Actually like completely nonlinear.",
                    "label": 0
                },
                {
                    "sent": "It's not actually a mapping that you can apply to new Point scored the out of sample extension problem that people have tried to address.",
                    "label": 0
                },
                {
                    "sent": "But here there's no problem like that is learning a function so you can just apply.",
                    "label": 0
                },
                {
                    "sent": "To any point, you can also include prior knowledge in the function, because it's in your network, so there's lots of.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nice things about it, but then there's basically that's that's our trick.",
                    "label": 0
                },
                {
                    "sent": "So you've got the SVM plus that regularizer right?",
                    "label": 0
                },
                {
                    "sent": "For Laplacian SVM we can do the same thing.",
                    "label": 0
                },
                {
                    "sent": "We're just going to put neural net here.",
                    "label": 0
                },
                {
                    "sent": "And instead of passing Eigen map objective here, we're going to put the Siamese net objective.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Difference is we don't have to just put it on the output like they did on the SM.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, mine has one layers.",
                    "label": 0
                },
                {
                    "sent": "You can only.",
                    "label": 0
                },
                {
                    "sent": "There's only one place to put it on the regularizer on the output, right?",
                    "label": 0
                },
                {
                    "sent": "But we can regularize the output of any of the layers of the neural network, so the output of this layer is the input features to the next layer, right?",
                    "label": 0
                },
                {
                    "sent": "So this this output.",
                    "label": 0
                },
                {
                    "sent": "Here we can say for two examples that we believe should have a similar representation, we push them together.",
                    "label": 0
                },
                {
                    "sent": "That point there.",
                    "label": 0
                },
                {
                    "sent": "So if you want so essentially you can see three different where we see three different ways of doing it.",
                    "label": 0
                },
                {
                    "sent": "You can either put it the output, put it somewhere in one of the internal layers, or maybe a little bit more tricky.",
                    "label": 0
                },
                {
                    "sent": "You could make some extra layer that's not actually shared with the with the rest of the network.",
                    "label": 0
                },
                {
                    "sent": "That's that you're using for your supervised task, and then do the.",
                    "label": 0
                },
                {
                    "sent": "Do the loss function on that and the reason why that might be good is because you've got one more linear map that's rescaling the data so that when you're asking for this margin, M are pushing the points apart you.",
                    "label": 0
                },
                {
                    "sent": "You don't force them apart too much for the supervised task.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's that's the algorithm.",
                    "label": 0
                },
                {
                    "sent": "So we just literally do that with stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "And we don't do that pretraining, by the way, we just do it like in SVM, both objectives at the same time.",
                    "label": 0
                },
                {
                    "sent": "So it's very simple.",
                    "label": 0
                },
                {
                    "sent": "Just make the grade.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Updates.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The thing is, what people did with Laplacian Eigen Maps mostly just using K nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "This idea is much more general than that you can take if you have any kind of prior on pairs, you can use it for example in videos which is what we did in our ICML talk that Renan gave yesterday's.",
                    "label": 0
                },
                {
                    "sent": "You could say 2 frames of video next to each other that you know might have the same objects and things of the same labels.",
                    "label": 0
                },
                {
                    "sent": "You can use those as the pairs for embedding or if you have text you could take a word in its neighbors and use that.",
                    "label": 0
                },
                {
                    "sent": "Or, you know, you could use stuff from the web as well.",
                    "label": 0
                },
                {
                    "sent": "Lots of possibilities.",
                    "label": 0
                },
                {
                    "sent": "Some.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "She's fired and have really got time to go into.",
                    "label": 0
                },
                {
                    "sent": "The more you could see this thing as an encoder without a decoder and compared to the encoder decoder methods of other other groups that might be nice that you don't have to do the decoding step.",
                    "label": 0
                },
                {
                    "sent": "For example, you have a very sparse representation is input when you did, when you decode it again, you would be dense within normal methods so.",
                    "label": 0
                },
                {
                    "sent": "This could be really slow.",
                    "label": 0
                },
                {
                    "sent": "With this method.",
                    "label": 0
                },
                {
                    "sent": "You don't need to do that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now just some simple experiments on typical semi supervised datasets that that people use to measure things like Laplacian SVM.",
                    "label": 0
                },
                {
                    "sent": "This is this is a normal neural net and this is adding this regularizer on three of these small datasets.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is just a two layer neural Nets.",
                    "label": 0
                },
                {
                    "sent": "Essentially you get the same kind of improvement from SFMTA Laplacian SVM as you get from neural net to this nor net with the embedding so OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doing the same thing.",
                    "label": 0
                },
                {
                    "sent": "But now we can look at.",
                    "label": 0
                },
                {
                    "sent": "This is on three different sides of emnace, with 100 training points 600 and 1000.",
                    "label": 0
                },
                {
                    "sent": "You can do the same thing, but embedding not on the output layer but on one of the internal layers or one of these auxiliary layer that I proposed and all of those seem to work.",
                    "label": 0
                },
                {
                    "sent": "So this is the normal neural net.",
                    "label": 0
                },
                {
                    "sent": "So this is the difference.",
                    "label": 0
                },
                {
                    "sent": "Once you add the semi supervised learning and this is where the convolutional net any kind of network you can do this, there's no restriction.",
                    "label": 0
                },
                {
                    "sent": "You're also going to see improvements.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that was only with two layer Nets and now this is the same kind of results where I go from 2 layers for later 6, eight 10:15 and with a normal neural net you're going to see the test error here.",
                    "label": 0
                },
                {
                    "sent": "Getting bigger and bigger the training area you can easily get its zero.",
                    "label": 0
                },
                {
                    "sent": "In this case actually, but it's just over 15 an with the embedding on the output, just on the output layer you are seeing improvement actually, and this is where you do the embedding regularization.",
                    "label": 0
                },
                {
                    "sent": "All the layers at the same time.",
                    "label": 0
                },
                {
                    "sent": "You see a further improvement.",
                    "label": 0
                },
                {
                    "sent": "And that's that's better than SVM and T SVM in that case.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, that that please conclusion so far it's simple method, easy to train.",
                    "label": 1
                },
                {
                    "sent": "We didn't do any pre training.",
                    "label": 0
                },
                {
                    "sent": "There's no decoding step.",
                    "label": 0
                },
                {
                    "sent": "And yeah now how long have I got like?",
                    "label": 1
                },
                {
                    "sent": "5 minutes, yeah, now we just show quickly some experiments on video and text so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Deep learning for video, so this this is just a little summary of Reynolds talk.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yesterday is basically we take the the frames consecutive frames of video as the.",
                    "label": 0
                },
                {
                    "sent": "Regularizer.",
                    "label": 0
                },
                {
                    "sent": "OK, so so presumably you could do things like learning variance, suppose illumination background clutter will kind of things with that.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We actually did a very simple experiment.",
                    "label": 0
                },
                {
                    "sent": "We just took oil 100, which is 100 objects 70 two different poses.",
                    "label": 1
                },
                {
                    "sent": "We recorded our own video of objects that look similar, so we call it coil like and we took these plastic animals like 60 of those and took videos as well and use these two as unsupervised video.",
                    "label": 0
                },
                {
                    "sent": "We also like pretended that was video as well.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In another experiment and just quickly go for that, there's two setups, so anyway, one where you have 30 objects and one where you have a.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so if you use the fake video from coil, you get this like ridiculous jumping error rate from a standard CNN.",
                    "label": 0
                },
                {
                    "sent": "But that's kind of cheating 'cause you have, like the tests images.",
                    "label": 0
                },
                {
                    "sent": "Actually in that video.",
                    "label": 0
                },
                {
                    "sent": "In this setup you don't get semi supervised.",
                    "label": 0
                },
                {
                    "sent": "We have 30 objects in the other 70 we put in the video and you still get a big jump an with these other videos that we made which have completely different set illumination settings and so on.",
                    "label": 0
                },
                {
                    "sent": "Or different objects in this case.",
                    "label": 0
                },
                {
                    "sent": "You still get an improvement.",
                    "label": 0
                },
                {
                    "sent": "Not as much.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically, yeah, then you're up to's to state of the art without feature engineering.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yes, and we did the same thing in text.",
                    "label": 0
                },
                {
                    "sent": "This is probably our biggest experiment, so we took different tiles, part of speech tagging, chunking.",
                    "label": 0
                },
                {
                    "sent": "Any R and SRL.",
                    "label": 0
                },
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "NOP is really quite striking what people do because they make a cascade of classifiers like they do a part of speech.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tiger then a pause are in the each time they hang code what the outputs to each of these tasks are going labeled data and their hand code.",
                    "label": 0
                },
                {
                    "sent": "The input features that based on those predictions.",
                    "label": 0
                },
                {
                    "sent": "The next layer and they do that and it really sort of looks like a neural network, but like nothing is trained between.",
                    "label": 0
                },
                {
                    "sent": "It's all independent.",
                    "label": 0
                },
                {
                    "sent": "Mind you can set them up as convex problems then, but we just try and train them.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing and we use the CNN and the each so we only have words as input and the words are represented as like 50 dimensional features and we're going to do an embedding.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some of the words just like before, so we're going to take like some neighborhood of words plus or word in unsupervised data.",
                    "label": 0
                },
                {
                    "sent": "So we take Wikipedia, which is 630 million examples of such such pairs.",
                    "label": 0
                },
                {
                    "sent": "And we do the embedding algorithm the same time as we train these.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tasks this was kind of what the embeddings look like, so afterwards, so you get like the country France is close to these other countries in the embedding space, so these things look nice, but.",
                    "label": 0
                },
                {
                    "sent": "The error rates we get big improvements.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So over not doing the semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So this is the normal CNN is most striking here.",
                    "label": 0
                },
                {
                    "sent": "This is F1 score on the semantic role labeling task.",
                    "label": 0
                },
                {
                    "sent": "Go from 51 to 72 and after we've done that we're at the state of the art of the shallow and methods that don't use all the feature engineering.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that's it.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when I do preach, I don't do pretraining.",
                    "label": 0
                },
                {
                    "sent": "But yeah, when I'm training, I backdrop through all the layers.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so if the embedding is on the output, that's really going through all of them.",
                    "label": 0
                },
                {
                    "sent": "You could actually try it.",
                    "label": 0
                },
                {
                    "sent": "Not doing that, and I suspect it might still do something good.",
                    "label": 0
                },
                {
                    "sent": "CNN.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Convolutional net without the semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "So when we add the embedding algorithm we call it in bed scene and.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "How is what trained?",
                    "label": 0
                },
                {
                    "sent": "Specific background.",
                    "label": 0
                },
                {
                    "sent": "Oh, it's like the same, the same network, but just on the supervised data.",
                    "label": 0
                },
                {
                    "sent": "It's not semi supervised.",
                    "label": 0
                },
                {
                    "sent": "Right, right, right, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's all these results were analogous every in each table.",
                    "label": 0
                },
                {
                    "sent": "I basically showed the supervised network and the semi supervised network using the embedding algorithm and every time I was getting an improvement in this.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "Yeah, maybe one more question.",
                    "label": 0
                },
                {
                    "sent": "So we have for this consultation CLU training separately.",
                    "label": 0
                },
                {
                    "sent": "We actually we try both, and because, yeah, that's another kind of multitasking right where you multitask with supervised task.",
                    "label": 0
                },
                {
                    "sent": "And actually the improvement you get from multitasking.",
                    "label": 0
                },
                {
                    "sent": "The other supervised tasks are not that much is something, but not that much.",
                    "label": 0
                },
                {
                    "sent": "I don't have the graphs here, but the unsupervised task makes a much bigger difference.",
                    "label": 0
                },
                {
                    "sent": "But there's two reasons why there might be.",
                    "label": 0
                },
                {
                    "sent": "One is there's.",
                    "label": 0
                },
                {
                    "sent": "There's only.",
                    "label": 0
                },
                {
                    "sent": "A million training examples supervised and the other.",
                    "label": 0
                },
                {
                    "sent": "The other problem is that it's actually the same data.",
                    "label": 0
                },
                {
                    "sent": "It's all Wall Street Journal, so it's not like you're even seeing anything knew in the inputs, only the output is new.",
                    "label": 0
                },
                {
                    "sent": "Speak again.",
                    "label": 0
                }
            ]
        }
    }
}