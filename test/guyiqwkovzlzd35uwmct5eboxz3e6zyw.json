{
    "id": "guyiqwkovzlzd35uwmct5eboxz3e6zyw",
    "title": "Complexity Theoretic Lower Bounds for Sparse Principal Component Detection",
    "info": {
        "author": [
            "Quentin Berthet, Department of Operations Research and Financial Engineering, Princeton University"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_berthet_sparse/",
    "segmentation": [
        [
            "Hello everyone, thank you for being here today.",
            "I'm here to talk about complexity theoretic, lower bounds forms, past principle, component detection.",
            "This is joint work with my advisor, Philip Wrigley.",
            "So first of all, what is sparse principle?"
        ],
        [
            "Component detection while the model is quite simple, we have N random vectors in dimension D that are independent standard Gaussian and they had an unknown covariance.",
            "So if this covariance matrix is the identity, then the distribution is completely isotropic.",
            "There's no direction with more violence than any other.",
            "Otherwise, if there is a sparse principal component, then there exists a sparse vector V with just a little bit more violence, one plus Theta.",
            "This is how we model it, so this is a very active topic.",
            "Often it's presented in the form of an estimation problem.",
            "People are trying to find the.",
            "Vector V This perspective, which an today or question is more simple, is their response principle component.",
            "We have any of these vectors and we want to know if there is one or if it's completely isotropic.",
            "So formally we presented as."
        ],
        [
            "Testing problem between two hypothesis so that no other than all, it's just the identity completely isotropic and under the alternative there exists a unit vector V with sparsity K such that such that the volumes along that direction is 1 plus Theta.",
            "So now that we have a testing problem, we want some measure of success."
        ],
        [
            "On for any testing that we're going to try, so this is very classical and statistics is the minimax setting, so we're going to say that rate fit a star is the optimal rate of detection, when basically it's possible to detect above and impossible to detect for failure, smaller up to up to constant.",
            "So the way we say is that we want to bound the testing error by some probability Delta.",
            "For for specific test when we're above festival and undefeated stomp, all tests are going to fail, so this is the this is the big picture.",
            "So we have the knowledge we have the NYPA thesis with the distribution P0 completely isotropic and all the alternative.",
            "We have a lot of different distributions.",
            "PV and Theta is a measure of distance between those sets of distribution.",
            "If it is equal to 0, it's just the identity matrix all the time.",
            "The sets are the same, Ann's fear becomes greater and greater.",
            "There's going to be more and more violence against at least one sparse direction, and then it's going to be easier to distinguish.",
            "So when you're in the Theta star, it's going to be impossible and above it, or start, it's going to be possible.",
            "So the problem there is with this setting is that.",
            "Well, first of all, it's Gaussian and nothing is really Gaussian in the data sometimes.",
            "Things are bounded and we don't want a situation where we would be using either in the upper bound or in the lower bound.",
            "Some very specific, just very specific property of the Gaussian distribution.",
            "We want to be very robust to the model, and so we're going to put ourselves in a robust set."
        ],
        [
            "So all sets are going to be just a little bigger.",
            "We're going to have a D0.",
            "It's basically the sets of distribution that are isotropic, so with variance one along every axis and that have deviations that behave a little bit like Gaussians and saying we're going to have a set D1K or failure, that's really just a little bit bigger and same idea, it's the set.",
            "It's a set of distribution such that there's at least one sponse direction, V. For which the violence is of the order one plus fitter.",
            "And so we've seen deviations that are similar to captions.",
            "So this is the very classical statistical statistical setting.",
            "But the problem is that it doesn't really describe what happens in practice, so.",
            "A test.",
            "It's done by human computer machine might not be, might not achieve the optimal rate, and we're going to be interested at the sets that we can, you know, in practice do so we're going to.",
            "Lou"
        ],
        [
            "At the rate of detection over the Class T, so the class he is going to be a class of tests, and we're going to.",
            "We're going to look at whether it's possible not to test for test that belong in this class.",
            "However, it's going to be impossible for all the tests and for the definition to work well, because sometimes you know there.",
            "If you were looking at tests that are in polynomial time, it's a it has a symbolic nature.",
            "We're going to have to allow the constant here to depend on depend on the test.",
            "Just a detail.",
            "So this is this is a picture.",
            "This is what we're going to do, and this is what we're going to look at.",
            "So first of all, we don't have."
        ],
        [
            "Any restriction on testing methods when we're looking at the purely statistical point of view, it's well known there's a lot of work on that, whether for estimation or for detection.",
            "The optimal rate is square root of K, log D / N, so everything behaves as you would expect it to.",
            "The more data points that you have, the smaller this morning preservation needs to be so that you can detect it.",
            "And the higher the dimension is, the harder it is, but only with the log term, and so in this case there are a lot of theorems and we have an explicit test that tells us that we can.",
            "Actually test one way above that rate and there's some results in information theory that tell that tell us well, know the when you're under that rate, the sets are going to be too close and you're not going to be able to make the difference between them, and so that would be.",
            "That would be fine, but the problem is that.",
            "It's not, you know, because the test side is, space is based on the eigenvalue statistic with some spicy and it's NP hard to compute, so this isn't actually the picture, and this isn't actually what happens in practice.",
            "So we need to look at what a computer could do.",
            "What you know, what someone can do actually, when he has he or she has a real data set.",
            "So we're going to look 1st at the test in polynomial time.",
            "If this still works, yes."
        ],
        [
            "So what happens in polynomial time?",
            "Well, we have some tests that have sub optimal rate, so now it's not kids K square log divided by and so a lot of a lot of tests work at exactly this rate.",
            "Not not anything smaller, and so there's a diagonal method, works sometimes by Johnson semidefinite programming by the promotion is coauthors and something that we call minimum drop activation, and so this is kind of.",
            "What's going on under the optimal rate?",
            "There is no detection at all above.",
            "It's possible for combinatorial method an above fitil.",
            "It's possible right now to prove that some methods.",
            "I'm sorry.",
            "So Theta is the I'm going to go back to the this line so Theta."
        ],
        [
            "Is the is the added eigenvalue along this path along this path direction so you know it measures how elongated the shape is.",
            "So it's as I said it measures also then."
        ],
        [
            "The distance between the sets."
        ],
        [
            "So when it is very small."
        ],
        [
            "Detection is possible at all, so this is the information theoretical limit and this is what people are able to do now in polynomial time.",
            "So once you once you have this then it might suggest a lot of different things so."
        ],
        [
            "So something that you know if you're if you're an optimist, if you think that you know the test you've been doing so far, good.",
            "If you think that the proofs you've been making are not too bad, then you think that because all these tests in polynomial time that by a lot of different people seem to stop at this rate, then maybe it's just that when you can't do polynomial time detection under this rate, but actually you know we don't know.",
            "So the situation could be."
        ],
        [
            "Very different.",
            "Nothing tells us that another test could not detect at a smaller rate.",
            "Theater Alpha of the order square root of K of divided by N for Alpha between one and two.",
            "So there is a need for a complexity theoretic lower bound that would be similar to the information theoretic lower bound here, and we tell us, know, know your Fairchild.",
            "You can't go behind for polynomial time method.",
            "So the way we're going to do is prove that Theta Alpha.",
            "Is it possible for methods in polynomial time and the way we're going to do it is we're going to draw a link between our problem and another problem where the bottleneck for detection or estimation comes eventually from problems in computational complexity.",
            "So this problem is."
        ],
        [
            "I did click problem.",
            "So what is the plan ticket column?"
        ],
        [
            "Well, first of all we need to define adult radiographs.",
            "So GM 1/2 is the distribution of graph with N vertices where each edge is randomly connected or not with probability 1/2 independent.",
            "So you know this is a graph, this is the corresponding adjacency matrix that those are instances.",
            "For this distribution.",
            "And you can see that you know above the diagonal.",
            "You know it's sure it's symmetric, but above the diagonal everything is independent.",
            "This is just pure noise.",
            "And the expectation of."
        ],
        [
            "Adjacency matrix is completely and is completely constant equal to 1/2.",
            "On the other hand, so there's going to be another."
        ],
        [
            "Native distribution it's the plenty click distribution, so we take a graph from GM 1/2 and we put in it a clique of size camper.",
            "So here Kappa is equal to four.",
            "We take this click and we artificially connect everyone in it so you can see it in the adjacency matrix an we hide it in a graph.",
            "Now people have to detect whether there is a planted Cleek or not.",
            "Choose the K points for the clicked uniformly channel.",
            "Yes, yes.",
            "So right now I'm going to show it later.",
            "Right now they're showing, as you know, being adjacent India.",
            "Just that you have in the bank.",
            "So first you choose your Kappa valances.",
            "You connect them and then you draw the other edges randomly independently.",
            "Everything is independent.",
            "And so."
        ],
        [
            "We expect."
        ],
        [
            "And of the adjacency matrix has a sparse signal structure.",
            "So here there is always going to be.",
            "There's always going to be once and everywhere else it's going to be a expectation of 1/2 and so."
        ],
        [
            "The picture, so this is first the simple picture.",
            "We tried to distinguish between those two situations, and of course, because you know the variances could be chosen everywhere you."
        ],
        [
            "I have to look for it.",
            "It's a combinatorial problem, so this is the distribution P0 for the graph, and this distribution P1 for the graphs.",
            "And so."
        ],
        [
            "Formally, the problem becomes also a testing problem where we test between those two hypothesis on plenty click and so similarly to our problem, there's a rate described by information theory that tells us well above 2 log M above for cliques of size above to log in you can detect it.",
            "It's going to be an NTR method, you just look for the largest clique.",
            "And there are a lot of polynomial time methods for detection or estimation of the order square root of M, and so there are strong reasons to believe impossibility for polynomial time methods under that under that range.",
            "So the few reasons as a lot of very smart people, many of them, many of them, are in this room right now.",
            "Try and could not do it under square root of M. And also some people prove prove that under certain computational models it's actually impossible to do go.",
            "What is K messed up?",
            "Little demo came over today.",
            "Came more than two again, so it's possible to detect non in polynomial time.",
            "But you know, just looking at the.",
            "Then so there is an NPR method that works above to log in.",
            "Forecasters often login.",
            "There is an algorithm which is very noisy, so there is an algorithm which is looking at the maximum click and looking looking, looking at its size horse.",
            "Vote for him.",
            "Causing problem right there.",
            "That's also OK, sorry, sorry.",
            "So it's a it's a brute force method.",
            "That's what I meant."
        ],
        [
            "So the hypothesis we're going to make is that it's actually impossible for random randomized polynomial time test to detect which with non trivial nontrivial probability under under square root of M. So if you have a lower power than 1/2, it's not possible, so that's going to be working at the office, so she remarks first of all, so formalisation of computational hardness of this ticket.",
            "Of the statistical problem.",
            "So we're saying that for all and under this, it's going to be impossible and justice, as I said, for description of optimal rates of a certain computational class.",
            "Here we allow the constant gamma to depend on the on those on the test on everything.",
            "It's just the synthetic nature of the class P. And so we have this problem now.",
            "The distance between those two is Keppra, and we want to create a randomized.",
            "Polynomial time function from graph to random vectors.",
            "We're going to take.",
            "Those graphs will create random vectors from from those graphs.",
            "An random vectors are going to be linked to original problem, and now we're going to be able to say if we were able to detect underrate Theta Alpha in our original problem, then we would be able to detect clicks.",
            "Of size smaller than the beta over 2, so this is the."
        ],
        [
            "Again for now complicated picture.",
            "So we have a function DL that's going to take a random graph.",
            "An going to give a distribution so the image of this distribution through this function is a new distribution of an independent.",
            "And identically distributed random vectors.",
            "With the same distribution P0 that belongs to the 0.",
            "An under the alternative hypothesis, we're going to have a distribution for the random vectors that we create.",
            "That's not going to be independent and identically distributed, but it's going to be very close in total valuation to a distribution that identically distributed independent, and that you know to do 1K of Theta, and so the idea then, is that if we were able to detect at this rate, then we would also contradict.",
            "The Kontiki Kaipan Physis that we make.",
            "Now how this?"
        ],
        [
            "How does this reduction work?",
            "Well, we call it the bottom left function because we take the bottom left corner of the adjacency matrix.",
            "So we have N vectors of size M that we extract, and now there's no symmetry structure anymore.",
            "You don't have this dependency because we're just taking a bottom left corner, and so we have N vectors with once, here and here.",
            "So this is just the intersection of the click in the.",
            "Random vectors that we have, we we make them in the dimension D. And then shuffle them and multiply them columnwise by minus one one, so that they are centered and that they look like random vectors under hypothesis.",
            "The idea then, is that when we're going to take this bottom left extraction, we're going to have something of other key here.",
            "And here we're going to have a Theta that's going to be that's going to represent the correlation between those vectors of all the Theta Alpha.",
            "And so we have when we take, when we take another training graphs and vectors that are independent that belong that have a distribution that belongs to the 0 and under the alternative, we're going to be in D1K of Alpha.",
            "So the key observation, the one the total valuation bound between the two distribution, just comes from the fact that sampling without without replacement is closed to sampling with replacement for very large for very large graph.",
            "So the idea is that when we're taking.",
            "And extraction of this graph was sampling without replacement.",
            "We're taking elements that are going to be in the kitchen or not in the click without replacement.",
            "And if they were independent, there will be taking without replacement.",
            "So that's just that's just a general idea.",
            "That's what we're able to do.",
            "That's how we're able to."
        ],
        [
            "To proof and so.",
            "In the end, the rate is that the final idea is the detection rate.",
            "Third Alpha would contradict the hypothesis on the planted Cleek, and so SDP MVP methods like that optimal among polynomial time efforts you can significantly improve the rate that we have.",
            "We had in case square log D / N and we have this picture.",
            "You know, combinatorial methods work above the above the information.",
            "Critic rates, but then there's a.",
            "There's a complexity theoretic rate under which no polynomial time detection works, so the take home message is that there is a gap of square root K for this problem, at least for methods in polynomial time, so."
        ],
        [
            "A simple a simple conclusion now, so in this work we what we did was a theoretical formulation of computational lower bounds.",
            "You know, we often we often read.",
            "It's hard to do this problem in this situation in this in this situation, so the the theoretical formulation is what does this mean from a statistical testing point of view, the link between sparse PCA and plenty key column.",
            "So this is something that exploited now also by people who want well interested at the plane ticket problem, and we use sparse PCA heuristics for it and show, and we show that there's optimal rates in polynomial time for.",
            "Four methods like this for this past PCA column.",
            "So now there are a few open questions are left.",
            "First of all, can we do computational lower bounds for other problems in the same way, linking them to some apophysis and also can we strengthen the complexity assumption?",
            "Not not say that it would contradict something on plenty kyxy that we contradict something on random preset or link it to NP harness directly.",
            "This is the reference for work and thank you very much for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello everyone, thank you for being here today.",
                    "label": 0
                },
                {
                    "sent": "I'm here to talk about complexity theoretic, lower bounds forms, past principle, component detection.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with my advisor, Philip Wrigley.",
                    "label": 0
                },
                {
                    "sent": "So first of all, what is sparse principle?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Component detection while the model is quite simple, we have N random vectors in dimension D that are independent standard Gaussian and they had an unknown covariance.",
                    "label": 1
                },
                {
                    "sent": "So if this covariance matrix is the identity, then the distribution is completely isotropic.",
                    "label": 0
                },
                {
                    "sent": "There's no direction with more violence than any other.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, if there is a sparse principal component, then there exists a sparse vector V with just a little bit more violence, one plus Theta.",
                    "label": 1
                },
                {
                    "sent": "This is how we model it, so this is a very active topic.",
                    "label": 0
                },
                {
                    "sent": "Often it's presented in the form of an estimation problem.",
                    "label": 0
                },
                {
                    "sent": "People are trying to find the.",
                    "label": 0
                },
                {
                    "sent": "Vector V This perspective, which an today or question is more simple, is their response principle component.",
                    "label": 0
                },
                {
                    "sent": "We have any of these vectors and we want to know if there is one or if it's completely isotropic.",
                    "label": 0
                },
                {
                    "sent": "So formally we presented as.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Testing problem between two hypothesis so that no other than all, it's just the identity completely isotropic and under the alternative there exists a unit vector V with sparsity K such that such that the volumes along that direction is 1 plus Theta.",
                    "label": 0
                },
                {
                    "sent": "So now that we have a testing problem, we want some measure of success.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On for any testing that we're going to try, so this is very classical and statistics is the minimax setting, so we're going to say that rate fit a star is the optimal rate of detection, when basically it's possible to detect above and impossible to detect for failure, smaller up to up to constant.",
                    "label": 1
                },
                {
                    "sent": "So the way we say is that we want to bound the testing error by some probability Delta.",
                    "label": 0
                },
                {
                    "sent": "For for specific test when we're above festival and undefeated stomp, all tests are going to fail, so this is the this is the big picture.",
                    "label": 0
                },
                {
                    "sent": "So we have the knowledge we have the NYPA thesis with the distribution P0 completely isotropic and all the alternative.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of different distributions.",
                    "label": 0
                },
                {
                    "sent": "PV and Theta is a measure of distance between those sets of distribution.",
                    "label": 0
                },
                {
                    "sent": "If it is equal to 0, it's just the identity matrix all the time.",
                    "label": 0
                },
                {
                    "sent": "The sets are the same, Ann's fear becomes greater and greater.",
                    "label": 0
                },
                {
                    "sent": "There's going to be more and more violence against at least one sparse direction, and then it's going to be easier to distinguish.",
                    "label": 0
                },
                {
                    "sent": "So when you're in the Theta star, it's going to be impossible and above it, or start, it's going to be possible.",
                    "label": 0
                },
                {
                    "sent": "So the problem there is with this setting is that.",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, it's Gaussian and nothing is really Gaussian in the data sometimes.",
                    "label": 0
                },
                {
                    "sent": "Things are bounded and we don't want a situation where we would be using either in the upper bound or in the lower bound.",
                    "label": 0
                },
                {
                    "sent": "Some very specific, just very specific property of the Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "We want to be very robust to the model, and so we're going to put ourselves in a robust set.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So all sets are going to be just a little bigger.",
                    "label": 0
                },
                {
                    "sent": "We're going to have a D0.",
                    "label": 0
                },
                {
                    "sent": "It's basically the sets of distribution that are isotropic, so with variance one along every axis and that have deviations that behave a little bit like Gaussians and saying we're going to have a set D1K or failure, that's really just a little bit bigger and same idea, it's the set.",
                    "label": 0
                },
                {
                    "sent": "It's a set of distribution such that there's at least one sponse direction, V. For which the violence is of the order one plus fitter.",
                    "label": 0
                },
                {
                    "sent": "And so we've seen deviations that are similar to captions.",
                    "label": 0
                },
                {
                    "sent": "So this is the very classical statistical statistical setting.",
                    "label": 1
                },
                {
                    "sent": "But the problem is that it doesn't really describe what happens in practice, so.",
                    "label": 0
                },
                {
                    "sent": "A test.",
                    "label": 0
                },
                {
                    "sent": "It's done by human computer machine might not be, might not achieve the optimal rate, and we're going to be interested at the sets that we can, you know, in practice do so we're going to.",
                    "label": 1
                },
                {
                    "sent": "Lou",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At the rate of detection over the Class T, so the class he is going to be a class of tests, and we're going to.",
                    "label": 1
                },
                {
                    "sent": "We're going to look at whether it's possible not to test for test that belong in this class.",
                    "label": 0
                },
                {
                    "sent": "However, it's going to be impossible for all the tests and for the definition to work well, because sometimes you know there.",
                    "label": 0
                },
                {
                    "sent": "If you were looking at tests that are in polynomial time, it's a it has a symbolic nature.",
                    "label": 0
                },
                {
                    "sent": "We're going to have to allow the constant here to depend on depend on the test.",
                    "label": 0
                },
                {
                    "sent": "Just a detail.",
                    "label": 0
                },
                {
                    "sent": "So this is this is a picture.",
                    "label": 0
                },
                {
                    "sent": "This is what we're going to do, and this is what we're going to look at.",
                    "label": 0
                },
                {
                    "sent": "So first of all, we don't have.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Any restriction on testing methods when we're looking at the purely statistical point of view, it's well known there's a lot of work on that, whether for estimation or for detection.",
                    "label": 1
                },
                {
                    "sent": "The optimal rate is square root of K, log D / N, so everything behaves as you would expect it to.",
                    "label": 0
                },
                {
                    "sent": "The more data points that you have, the smaller this morning preservation needs to be so that you can detect it.",
                    "label": 1
                },
                {
                    "sent": "And the higher the dimension is, the harder it is, but only with the log term, and so in this case there are a lot of theorems and we have an explicit test that tells us that we can.",
                    "label": 0
                },
                {
                    "sent": "Actually test one way above that rate and there's some results in information theory that tell that tell us well, know the when you're under that rate, the sets are going to be too close and you're not going to be able to make the difference between them, and so that would be.",
                    "label": 0
                },
                {
                    "sent": "That would be fine, but the problem is that.",
                    "label": 0
                },
                {
                    "sent": "It's not, you know, because the test side is, space is based on the eigenvalue statistic with some spicy and it's NP hard to compute, so this isn't actually the picture, and this isn't actually what happens in practice.",
                    "label": 1
                },
                {
                    "sent": "So we need to look at what a computer could do.",
                    "label": 0
                },
                {
                    "sent": "What you know, what someone can do actually, when he has he or she has a real data set.",
                    "label": 0
                },
                {
                    "sent": "So we're going to look 1st at the test in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "If this still works, yes.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what happens in polynomial time?",
                    "label": 1
                },
                {
                    "sent": "Well, we have some tests that have sub optimal rate, so now it's not kids K square log divided by and so a lot of a lot of tests work at exactly this rate.",
                    "label": 0
                },
                {
                    "sent": "Not not anything smaller, and so there's a diagonal method, works sometimes by Johnson semidefinite programming by the promotion is coauthors and something that we call minimum drop activation, and so this is kind of.",
                    "label": 0
                },
                {
                    "sent": "What's going on under the optimal rate?",
                    "label": 1
                },
                {
                    "sent": "There is no detection at all above.",
                    "label": 0
                },
                {
                    "sent": "It's possible for combinatorial method an above fitil.",
                    "label": 0
                },
                {
                    "sent": "It's possible right now to prove that some methods.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "So Theta is the I'm going to go back to the this line so Theta.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the is the added eigenvalue along this path along this path direction so you know it measures how elongated the shape is.",
                    "label": 0
                },
                {
                    "sent": "So it's as I said it measures also then.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The distance between the sets.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when it is very small.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Detection is possible at all, so this is the information theoretical limit and this is what people are able to do now in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "So once you once you have this then it might suggest a lot of different things so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So something that you know if you're if you're an optimist, if you think that you know the test you've been doing so far, good.",
                    "label": 0
                },
                {
                    "sent": "If you think that the proofs you've been making are not too bad, then you think that because all these tests in polynomial time that by a lot of different people seem to stop at this rate, then maybe it's just that when you can't do polynomial time detection under this rate, but actually you know we don't know.",
                    "label": 0
                },
                {
                    "sent": "So the situation could be.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very different.",
                    "label": 0
                },
                {
                    "sent": "Nothing tells us that another test could not detect at a smaller rate.",
                    "label": 0
                },
                {
                    "sent": "Theater Alpha of the order square root of K of divided by N for Alpha between one and two.",
                    "label": 0
                },
                {
                    "sent": "So there is a need for a complexity theoretic lower bound that would be similar to the information theoretic lower bound here, and we tell us, know, know your Fairchild.",
                    "label": 1
                },
                {
                    "sent": "You can't go behind for polynomial time method.",
                    "label": 0
                },
                {
                    "sent": "So the way we're going to do is prove that Theta Alpha.",
                    "label": 0
                },
                {
                    "sent": "Is it possible for methods in polynomial time and the way we're going to do it is we're going to draw a link between our problem and another problem where the bottleneck for detection or estimation comes eventually from problems in computational complexity.",
                    "label": 0
                },
                {
                    "sent": "So this problem is.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I did click problem.",
                    "label": 0
                },
                {
                    "sent": "So what is the plan ticket column?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, first of all we need to define adult radiographs.",
                    "label": 0
                },
                {
                    "sent": "So GM 1/2 is the distribution of graph with N vertices where each edge is randomly connected or not with probability 1/2 independent.",
                    "label": 1
                },
                {
                    "sent": "So you know this is a graph, this is the corresponding adjacency matrix that those are instances.",
                    "label": 0
                },
                {
                    "sent": "For this distribution.",
                    "label": 0
                },
                {
                    "sent": "And you can see that you know above the diagonal.",
                    "label": 0
                },
                {
                    "sent": "You know it's sure it's symmetric, but above the diagonal everything is independent.",
                    "label": 0
                },
                {
                    "sent": "This is just pure noise.",
                    "label": 0
                },
                {
                    "sent": "And the expectation of.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Adjacency matrix is completely and is completely constant equal to 1/2.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, so there's going to be another.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Native distribution it's the plenty click distribution, so we take a graph from GM 1/2 and we put in it a clique of size camper.",
                    "label": 1
                },
                {
                    "sent": "So here Kappa is equal to four.",
                    "label": 0
                },
                {
                    "sent": "We take this click and we artificially connect everyone in it so you can see it in the adjacency matrix an we hide it in a graph.",
                    "label": 0
                },
                {
                    "sent": "Now people have to detect whether there is a planted Cleek or not.",
                    "label": 0
                },
                {
                    "sent": "Choose the K points for the clicked uniformly channel.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "So right now I'm going to show it later.",
                    "label": 0
                },
                {
                    "sent": "Right now they're showing, as you know, being adjacent India.",
                    "label": 0
                },
                {
                    "sent": "Just that you have in the bank.",
                    "label": 0
                },
                {
                    "sent": "So first you choose your Kappa valances.",
                    "label": 0
                },
                {
                    "sent": "You connect them and then you draw the other edges randomly independently.",
                    "label": 0
                },
                {
                    "sent": "Everything is independent.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We expect.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And of the adjacency matrix has a sparse signal structure.",
                    "label": 1
                },
                {
                    "sent": "So here there is always going to be.",
                    "label": 0
                },
                {
                    "sent": "There's always going to be once and everywhere else it's going to be a expectation of 1/2 and so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The picture, so this is first the simple picture.",
                    "label": 0
                },
                {
                    "sent": "We tried to distinguish between those two situations, and of course, because you know the variances could be chosen everywhere you.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I have to look for it.",
                    "label": 0
                },
                {
                    "sent": "It's a combinatorial problem, so this is the distribution P0 for the graph, and this distribution P1 for the graphs.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Formally, the problem becomes also a testing problem where we test between those two hypothesis on plenty click and so similarly to our problem, there's a rate described by information theory that tells us well above 2 log M above for cliques of size above to log in you can detect it.",
                    "label": 0
                },
                {
                    "sent": "It's going to be an NTR method, you just look for the largest clique.",
                    "label": 0
                },
                {
                    "sent": "And there are a lot of polynomial time methods for detection or estimation of the order square root of M, and so there are strong reasons to believe impossibility for polynomial time methods under that under that range.",
                    "label": 1
                },
                {
                    "sent": "So the few reasons as a lot of very smart people, many of them, many of them, are in this room right now.",
                    "label": 0
                },
                {
                    "sent": "Try and could not do it under square root of M. And also some people prove prove that under certain computational models it's actually impossible to do go.",
                    "label": 0
                },
                {
                    "sent": "What is K messed up?",
                    "label": 0
                },
                {
                    "sent": "Little demo came over today.",
                    "label": 1
                },
                {
                    "sent": "Came more than two again, so it's possible to detect non in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "But you know, just looking at the.",
                    "label": 0
                },
                {
                    "sent": "Then so there is an NPR method that works above to log in.",
                    "label": 0
                },
                {
                    "sent": "Forecasters often login.",
                    "label": 0
                },
                {
                    "sent": "There is an algorithm which is very noisy, so there is an algorithm which is looking at the maximum click and looking looking, looking at its size horse.",
                    "label": 0
                },
                {
                    "sent": "Vote for him.",
                    "label": 0
                },
                {
                    "sent": "Causing problem right there.",
                    "label": 0
                },
                {
                    "sent": "That's also OK, sorry, sorry.",
                    "label": 0
                },
                {
                    "sent": "So it's a it's a brute force method.",
                    "label": 0
                },
                {
                    "sent": "That's what I meant.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the hypothesis we're going to make is that it's actually impossible for random randomized polynomial time test to detect which with non trivial nontrivial probability under under square root of M. So if you have a lower power than 1/2, it's not possible, so that's going to be working at the office, so she remarks first of all, so formalisation of computational hardness of this ticket.",
                    "label": 0
                },
                {
                    "sent": "Of the statistical problem.",
                    "label": 0
                },
                {
                    "sent": "So we're saying that for all and under this, it's going to be impossible and justice, as I said, for description of optimal rates of a certain computational class.",
                    "label": 0
                },
                {
                    "sent": "Here we allow the constant gamma to depend on the on those on the test on everything.",
                    "label": 0
                },
                {
                    "sent": "It's just the synthetic nature of the class P. And so we have this problem now.",
                    "label": 1
                },
                {
                    "sent": "The distance between those two is Keppra, and we want to create a randomized.",
                    "label": 1
                },
                {
                    "sent": "Polynomial time function from graph to random vectors.",
                    "label": 0
                },
                {
                    "sent": "We're going to take.",
                    "label": 0
                },
                {
                    "sent": "Those graphs will create random vectors from from those graphs.",
                    "label": 0
                },
                {
                    "sent": "An random vectors are going to be linked to original problem, and now we're going to be able to say if we were able to detect underrate Theta Alpha in our original problem, then we would be able to detect clicks.",
                    "label": 0
                },
                {
                    "sent": "Of size smaller than the beta over 2, so this is the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again for now complicated picture.",
                    "label": 0
                },
                {
                    "sent": "So we have a function DL that's going to take a random graph.",
                    "label": 0
                },
                {
                    "sent": "An going to give a distribution so the image of this distribution through this function is a new distribution of an independent.",
                    "label": 0
                },
                {
                    "sent": "And identically distributed random vectors.",
                    "label": 0
                },
                {
                    "sent": "With the same distribution P0 that belongs to the 0.",
                    "label": 0
                },
                {
                    "sent": "An under the alternative hypothesis, we're going to have a distribution for the random vectors that we create.",
                    "label": 0
                },
                {
                    "sent": "That's not going to be independent and identically distributed, but it's going to be very close in total valuation to a distribution that identically distributed independent, and that you know to do 1K of Theta, and so the idea then, is that if we were able to detect at this rate, then we would also contradict.",
                    "label": 0
                },
                {
                    "sent": "The Kontiki Kaipan Physis that we make.",
                    "label": 0
                },
                {
                    "sent": "Now how this?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How does this reduction work?",
                    "label": 0
                },
                {
                    "sent": "Well, we call it the bottom left function because we take the bottom left corner of the adjacency matrix.",
                    "label": 1
                },
                {
                    "sent": "So we have N vectors of size M that we extract, and now there's no symmetry structure anymore.",
                    "label": 0
                },
                {
                    "sent": "You don't have this dependency because we're just taking a bottom left corner, and so we have N vectors with once, here and here.",
                    "label": 0
                },
                {
                    "sent": "So this is just the intersection of the click in the.",
                    "label": 0
                },
                {
                    "sent": "Random vectors that we have, we we make them in the dimension D. And then shuffle them and multiply them columnwise by minus one one, so that they are centered and that they look like random vectors under hypothesis.",
                    "label": 0
                },
                {
                    "sent": "The idea then, is that when we're going to take this bottom left extraction, we're going to have something of other key here.",
                    "label": 0
                },
                {
                    "sent": "And here we're going to have a Theta that's going to be that's going to represent the correlation between those vectors of all the Theta Alpha.",
                    "label": 0
                },
                {
                    "sent": "And so we have when we take, when we take another training graphs and vectors that are independent that belong that have a distribution that belongs to the 0 and under the alternative, we're going to be in D1K of Alpha.",
                    "label": 1
                },
                {
                    "sent": "So the key observation, the one the total valuation bound between the two distribution, just comes from the fact that sampling without without replacement is closed to sampling with replacement for very large for very large graph.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that when we're taking.",
                    "label": 0
                },
                {
                    "sent": "And extraction of this graph was sampling without replacement.",
                    "label": 0
                },
                {
                    "sent": "We're taking elements that are going to be in the kitchen or not in the click without replacement.",
                    "label": 0
                },
                {
                    "sent": "And if they were independent, there will be taking without replacement.",
                    "label": 0
                },
                {
                    "sent": "So that's just that's just a general idea.",
                    "label": 0
                },
                {
                    "sent": "That's what we're able to do.",
                    "label": 0
                },
                {
                    "sent": "That's how we're able to.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To proof and so.",
                    "label": 0
                },
                {
                    "sent": "In the end, the rate is that the final idea is the detection rate.",
                    "label": 0
                },
                {
                    "sent": "Third Alpha would contradict the hypothesis on the planted Cleek, and so SDP MVP methods like that optimal among polynomial time efforts you can significantly improve the rate that we have.",
                    "label": 1
                },
                {
                    "sent": "We had in case square log D / N and we have this picture.",
                    "label": 0
                },
                {
                    "sent": "You know, combinatorial methods work above the above the information.",
                    "label": 0
                },
                {
                    "sent": "Critic rates, but then there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a complexity theoretic rate under which no polynomial time detection works, so the take home message is that there is a gap of square root K for this problem, at least for methods in polynomial time, so.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A simple a simple conclusion now, so in this work we what we did was a theoretical formulation of computational lower bounds.",
                    "label": 1
                },
                {
                    "sent": "You know, we often we often read.",
                    "label": 0
                },
                {
                    "sent": "It's hard to do this problem in this situation in this in this situation, so the the theoretical formulation is what does this mean from a statistical testing point of view, the link between sparse PCA and plenty key column.",
                    "label": 0
                },
                {
                    "sent": "So this is something that exploited now also by people who want well interested at the plane ticket problem, and we use sparse PCA heuristics for it and show, and we show that there's optimal rates in polynomial time for.",
                    "label": 0
                },
                {
                    "sent": "Four methods like this for this past PCA column.",
                    "label": 0
                },
                {
                    "sent": "So now there are a few open questions are left.",
                    "label": 1
                },
                {
                    "sent": "First of all, can we do computational lower bounds for other problems in the same way, linking them to some apophysis and also can we strengthen the complexity assumption?",
                    "label": 0
                },
                {
                    "sent": "Not not say that it would contradict something on plenty kyxy that we contradict something on random preset or link it to NP harness directly.",
                    "label": 0
                },
                {
                    "sent": "This is the reference for work and thank you very much for your attention.",
                    "label": 0
                }
            ]
        }
    }
}