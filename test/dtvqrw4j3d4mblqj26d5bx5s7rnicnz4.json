{
    "id": "dtvqrw4j3d4mblqj26d5bx5s7rnicnz4",
    "title": "An Information Theoretic Approach to Learning Generative Graph Prototypes",
    "info": {
        "author": [
            "Edwin Hancock, Department of Computer Science, University of York"
        ],
        "published": "Oct. 17, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Mathematics->Graph Theory"
        ]
    },
    "url": "http://videolectures.net/simbad2011_hancock_generative/",
    "segmentation": [
        [
            "So I'd like to talk about some work that I've been doing with Linhan Ann Richard Wilson.",
            "This is of course supported by the Sinbad project and also by my Royal Society Wolfson Research Merit Award.",
            "So the talk is is closely related to the talk that you have just heard and what I want to talk about here is a way of computing the infinite information criterion that is used to learn a generative model of a set of graphs, and so there are many similarities.",
            "Many absences between the pieces of work, and so much so that."
        ],
        [
            "My introduction, I think, is already largely been covered, so graph representations.",
            "What are the advantages and disadvantages of using them to represent image data while they drive to do is that they can capture structure in a manner that is invariant to changes in viewpoint.",
            "And the efficient abstractions of information in the scene and the arrangement of features or objects.",
            "But on the other hand, there are disadvantages.",
            "And the main disadvantage is that graphs, representations of image structure can be very fragile.",
            "There sensitive to noise and segmentation error.",
            "And as pointed out in the last talk, if we're trying to learn in the graph domain, the methodology is limited due to the non vectorial nature of graph data."
        ],
        [
            "Sir.",
            "So in this talk what I've done is and slide.",
            "Sorry, I've tried to illustrate some of the difficulties, so on the left and the right hand side I've shown a synthetic and a real world image sequence.",
            "What's happening here is the camera is panning around the object and features are changing in their spatial arrangement and also in terms of their visibility as the camera pans around the object and on the what you see, is it as the?",
            "As the features change their arrangement then so the graph structure representing them changes its structure in terms of connectivity.",
            "And really, the aim in this talk is to try to learn the way in which a set of features change their connectivity arrangement over a set of views of a particular object.",
            "So we're trying to learn variability in edge and potentially no structure when an object is viewed under different viewing conditions on those viewing conditions might be, for instance, viewpoint, or they may be due to inherent variations in the shape.",
            "That object."
        ],
        [
            "So welcome on do it once trying to learn in the graph domain welcome.",
            "Try to cluster similar objects together and represent them using a class prototype and this class prototype for instance might be a median and this prototype could be learned using a dissimilarity measurements between graph structures.",
            "One also might try to extract features and graphs and perform central clustering.",
            "So in a sense, once trying to opt out here from the the graph representation and to find a proxy vector representation that can be used to capture the graph structures.",
            "In a sense, both of these two alternatives are really full.",
            "Well short of the full blown problem of trying to construct a generative model which can account for variations in graph structure and which we can use both to classify graphs and also to generate if necessary new instances of graphs from a particular class using an underlying probability distribution."
        ],
        [
            "So why is this difficult?",
            "Well, this is all been summarized in the previous talk.",
            "It's difficult because graphs are not vectors, they don't naturally reside in a vector space.",
            "There's no natural ordering of the nodes and edges in a graph.",
            "It's difficult because we're trying to capture structural variations in graphs.",
            "That is to say, we're trying to model the way in which the numbers of nodes and edges may vary."
        ],
        [
            "So what's available in terms of generative models in the in the structural domain?",
            "Well, like the previous talk, what I'm trying to do here is build on a piece of previous work that work was done met by Andrea Torsello and myself, and we published a paper on it in Pammy in 2007.",
            "The idea here was to try to define the probability distribution over a prototype structure.",
            "In this case a limited type of structure tree.",
            "And then to try to learn the parameters of this probability distribution unit using a description length criterion.",
            "So in this store I'm trying to do is to take this world and expand both on the information theoretic representation of it and move from trees to graphs and So what I'm attempting to do is to pull in ideas that we've developed in the analysis and learning of graphs using graph spectral information and graph Spectra simply is information conveyed by the eigenvalues and eigenvectors of the adjacency matrix or Laplacian matrix."
        ],
        [
            "Now before I go on to describe the main contributions here, I just think it's worth pausing to actually think how this work impinges on current work, in a sort of a wider sense.",
            "At the moment there's a great deal of interest in the problem of deep learning, both in the neural computation and also the computer vision literature.",
            "So for instance, Hinton and Bengio have developed deep belief networks to try to learn hierarchical descriptions of image data.",
            "Their compositional networks developed by originally by Imagine Guimond, and our recently extended by Fergus.",
            "There, Markov models of the type developed by LEONARDIS and then of course I think there are also links with stochastic image image grammars as described by Zoo Mumford.",
            "And in fact, very closely related to this work, and in fact borrowing some of the ideas from it, is the work of Todd Rich and Ahuja who have developed tree based methods for taxonomy and category learning."
        ],
        [
            "OK, so just to give some names when I'm trying to do is to combine spectral and structural methods.",
            "I want to do this in a description length criterion and I want to apply this to the learning of generative models for graphs rather than trees."
        ],
        [
            "OK, that's just."
        ],
        [
            "A list of prior work, so let's not try to set up the description length framework for the for the present work and to show you how ideas of complexity can be imported in order to enhance what Andrea and I did previously in our family 2007 paper and and why this makes it amenable to graph."
        ],
        [
            "Structures.",
            "So if you look at the literature on description length, it's a complicated one.",
            "A little bit fractional.",
            "I mean there are two approaches.",
            "There is the approach adopted by Wallace and Freeman, which is minimum message length and the approach adopted by rising and which is the minimum description length.",
            "So let me just try to control."
        ],
        [
            "What these these two approaches have done because they do differ quite fundamentally in terms of their philosophy.",
            "So in minimum description language.",
            "It's the selection of the model that is the aim.",
            "Model parameters are simply a means to end, and the parameters are usually estimated in a maximum likelihood sense, and most often the prior on the parameters is taken as flat.",
            "Minimum message length, on the other hand, puts the recovery of model parameters at the center of the picture and the parameter prior may be more complicated."
        ],
        [
            "So if you're going to apply a minimum message length or minimum description length approach to a learning problem, then you need a coding scheme.",
            "And in most of the existing coding schemes, these are usually assumed to be exponential.",
            "Their alternatives, such as universal codes and predictive codes, and if you then try to contrast ML&MDL in terms of the coding scheme adopted MML uses two part codes models plus parameters, and MGL.",
            "The codes may be either one or."
        ],
        [
            "Two part.",
            "So let me try to know in general terms cost what I'm attempting to do into this into this sort of message length framework.",
            "So what I want to do is to learn a model which is a super graph and this graph is going to be formed through a union of all the graphs in my training set.",
            "And in order to try to capture this probabilistically, what I'm going to do is to assume that there's a sample, they're observation model, and that is based on a Bernoulli distribution over the nodes and edges of the graphs appearing in my training sample.",
            "Then I'm going to attempt to model the complexity of the of the model that I'm using and hearing this or novel ingredient of this talk is to use the Von Neumann entropy of the Super graph as a measure of its complexity.",
            "So then I am going to construct a code length criterion.",
            "It falls it doesn't fall precisely into either the MGL all the MML.",
            "A framework, it's it's somewhere between and, so it's MDL like because we make maximum likelihood estimates of the Bernoulli parameters and it's MML like because we have a two part code for the data link data fit to the data and the complexity of the Super graph model."
        ],
        [
            "So now to try to put that into a little bit more sort of Chris mathematical focus.",
            "What I'm trying to do is to optimize the description.",
            "Next criterion the code length is the sum of the negative log likelihood of the data given our Bernoulli model, and then I'm going to answer that a model code language criterion which is going to be the Von Neumann entropy of the learned super graph.",
            "And so the ingredients here are going to be a set of graphs G from which I'm going to try to learn the structure and the model is going to be a super Graph prototype graph and correspondences with it.",
            "And then I'm going to rather than using the the solving approach in the previous tool.",
            "What I'm going to do is I'm going to use a variant of expectation maximization to learn the model, graph adjacency matrix and.",
            "The correspondence is with it."
        ],
        [
            "So this is how the learning of the Super graph proceeds were going to follow to settle in Hancock and pose the problem as learning in a description.",
            "Next criterion, I'm going to borrow a probability distributions from my earlier work with been low, we're going to use this on Norman entropy, which is a new ingredient to control super graph complexity and then develop the EM algorithm to find node correspondances and the Super graph adjacency matrix."
        ],
        [
            "So this just is.",
            "It strikes the idea of representative graph using energy."
        ],
        [
            "Since you matrix here in the formal ingredients of my my my scheme, for each sample graph, I'm going to danger adjacency matrix with elements 01 depending on whether there's a connecting edge.",
            "I'm going to try to learn a super graph model that captures the distribution of edges, and that model is going to be represented using a super graph adjacency matrix.",
            "And then I'm going to have a correspondences between each sample graph.",
            "And the adjacency matrix and the learned model graph adjacency matrix.",
            "So according to the work that's been roided, the Bernoulli distribution leads to the probability distribution at the bottom of the page for the data sample given the model gamma and the correspondence with the correspondence with it S. So what you see is it's an exponential function of the correlation.",
            "Between the data graph and model graph adjacency matrices and that correlation is controlled by the correspondence indicators S."
        ],
        [
            "So what we want to say first of all, the data part of the description, next criterion and that's just going to be the negative log likelihood of the set of data graphs given the current model.",
            "So I give the log likelihood function here."
        ],
        [
            "And then the next ingredient is to try to write down the critic criterion of complexity for the learned adjacency matrix for the model graph and the idea here is to draw ideas from entropic measures of complexity.",
            "In particular, I'm going to use the Von Neumann entropy, which is a concept introduced in quantum mechanics to measure the entropy associated with density matrix.",
            "And then I'm going to use this as the modeling coding criterion in my description like framework and briefly at the end of the talk show your relationship with kernelized representation of dissimilar."
        ],
        [
            "Writing.",
            "So this is a definition of the Von Neumann entropy two.",
            "To compute it, you commence by computing the normalized.",
            "Laplacian matrix that's the degree matrix minus adjacency matrix.",
            "Pre and post Volt applied by one over the square root of the degree matrix.",
            "And if I have the eigenvalues lambda's for the normalized classy and volume and entropy is just mindless Lander, I / 2 times log man drive to some Dover or Laplacian eigenvalues I.",
            "Now the trouble is, if I if I turn to optimize, this logarithm is going to make life difficult for me.",
            "So what I want to do is to commence by approximating this form."
        ],
        [
            "Entropy and what I do is I take the Shannon term P dot P appearing in the in the definition of entropy and replace it by its quadratic counterpart P * 1 -- P. So if I do that with a normalized class in eigenvalues, what happens is that the Von Neumann entropy just boils down to half the traces of organizing classy and minuses.",
            "Quarter of the traits of a normalized classroom squared.",
            "So what I want to do is to try to avoid computing that entropy using the eigen values and all I want to do is to see whether I can compute it using simple statistics over the graph."
        ],
        [
            "And that turns out to be quite straightforward.",
            "The traits of a normalized class and is just a number of nodes, and the trace of a normalized, classy and squared is just given by the expression at the bottom of the slide in terms of the number of nodes and the distribution of degrees T view and TNV over pairs of nodes connected by edges in the graph.",
            "So the idea is to use this entropy measure."
        ],
        [
            "Which I've collected together here as a measure of complexity of the current learned model graph adjacency matrix an.",
            "OK."
        ],
        [
            "So let's just run through now the optimization steps.",
            "I add the entropy to the negative likelihood of the data graphs given the model based on our Bernoulli Bernoulli."
        ],
        [
            "Process and it turns out that the quantity I have to optimize if I go to pose this problem in terms of EN framework where I have posterior probabilities that one iteration and parameters that another iteration is given on the current slide.",
            "So here the end indicates the current posterior probabilities and the N + 1.",
            "Updated browsers"
        ],
        [
            "And then on this slide about going to the details is other two steps of an EM algorithm and optimizes that criterion.",
            "I have two updates, one for the correspondence indicators between the data graphs and the the model graph, and then I have updates for the elements of the model, graph adjacency matrix, and then finally there is an update step.",
            "The step where I update the posterior probabilities.",
            "So that's the pretty well."
        ],
        [
            "The algorithm now I can go onto the experiments we we tested this on both the coil data set, the one which previous speaker talked about and also a data set that we have developed in House and made available.",
            "This is images of toys captured at different viewpoints."
        ],
        [
            "So what I've shown on this slide are the what's happened.",
            "Has the EM algorithm, iterates on our information criterion so the there are three plots for the two datasets I've got that oil kernel data set at the top.",
            "The toys data set at the bottom, and the first plot shows the model complexity.",
            "Second, plot the data log likelihood.",
            "And the third plot shows the combined information criterion.",
            "Each of these plots show the relevant quantity as a function of iteration number, and So what you see is that.",
            "Although the.",
            "Model complexity initially increases after iterations it it decreases.",
            "The log likelihood increases and the overall coding lens criterion decreases with iteration number and so from this you can see that probably after about 10 iterations there is reasonable convergence of the scheme."
        ],
        [
            "So this slide shows.",
            "Experiments using the learned model graph adjacency matrix to classify objects according to class.",
            "So we've trained these models up separately on the different objects in our data set and then tried to find the nearest class prototype and so here is some experiments we compare here.",
            "The results obtained using the Learn super graph using our.",
            "MDL criterion with complexity, median graph and then learn super graph where we exclude the complexity term in the expectation maximization algorithm and you see it all the results.",
            "The complexity.",
            "Moderated MDL algorithm outperforms the alternatives."
        ],
        [
            "Finally, what we've done is we have experimented with this technique to try to compute kernelized representation of the similarities between graphs.",
            "The idea here is that we've used Jensen Shannon Divergent.",
            "So what we do is we learn a model graph pairwise.",
            "Paragraphs and that will give us an entropy H of GI, cross GJ, and then we subtract the average of the Von Neumann entropy of the two graphs.",
            "So that's the Jensen Shannon Divergent with then in bed."
        ],
        [
            "The graphs into a 2 dimensional vector space using the Jensen Shannon Divergent and compared the results that we obtained simply using edit distances between graphs.",
            "And you see that they generally using the learned model graph and the Jensen Shannon Divergent.",
            "We get better class separation for the example objects here."
        ],
        [
            "And this just illustrates some of the adjacency matrices that we we learn for sets of sets of graphs using different algorithms and.",
            "I think the main features to note here are the fact that the adjacency structures are generally similar, and then what I've done here is generated to sample graphs from the generative model, one with high probability and one with low probability.",
            "Those are shown in the right hand column and you see that the high probability adjacency matrix resembles very closely to learn structure, whereas the low probability one.",
            "Does not track is it really as you did."
        ],
        [
            "Correct, OK, so just to conclude, we've shown how super graph or generative model of graph structure can be learned using description length.",
            "In particular, using a Von Neumann entropy to represent complexity, we presented the EM algorithm to do the the learning, and in our experiments we discussed we discovered that by using this complexity measure, we're actually able to learn a rather better structure than we would without using it.",
            "So thank you very much indeed.",
            "So.",
            "Entropy corresponds to the bottom and some coding player.",
            "Some coding scheme photographs that I'm going through.",
            "I mean, I think we we haven't really looked at the we we borrowed the idea from from quantum mechanics, it where it's defined as the effectively the entropy associated with the density matrix, and so we're sort of intent interpreting here that are passing matrix as if it's a density matrix.",
            "There's a paper in Phys Rev.",
            "Recently by Simone Severini, who motivates the.",
            "The use of this complexity measure in that context.",
            "I don't.",
            "I mean, we have not investigated in a sense, the formal link between the complexity measure and the information content or code ability of graphs.",
            "And I mean, I think that that would be a next step.",
            "I mean, we've mainly explored issues of whether it can be used to control a generative model, whether it can be used in a kernel.",
            "So we haven't really looked at it in the compressive sense to see whether it tells us anything about how much information is lossed.",
            "When you code a graph in a particular way.",
            "I would like to follow up on this question, so can you maybe give us some insight on why you actually came to this choice?",
            "But why does entropy measure well?",
            "I mean, I think because we like Simone Severini's work and also if you haven't shown it here.",
            "Write down the complexity measure.",
            "Then it looks very similar to the heterogeneity measure that is been introduced by an S to Estrada in the complex systems literature.",
            "What he does in that literature is to effectively compute variances over no degree over sets of edges in the graph.",
            "Now we've we've taken his complexity measure and our.",
            "Home for the Von Neumann entropy.",
            "It turns out there are sort of some interesting relationships.",
            "So in the first case, if you take estradas index and you make the graph large, then what happens is that the heterogeneity index is measuring the difference between twice the adjacency matrix and the commute time between a pair of nodes.",
            "So what it's doing is telling you how different the commute time is from the shortest possible distance between a pair of nodes on the graph, and then if you analyze our complexity, measure.",
            "It appears I mean astronomers index has this sort of global property of using the commute time distribution.",
            "Our method seems to be somewhat more local in the way in which it accumulates information.",
            "So you are essentially estimating a graph generator and it would be interesting in the following sense if you are interested in some some estimators on graph properties like capital or tightly connected components and so on.",
            "You want to give guarantees in this estimation.",
            "For that one, we have two less you have to investigate the asymptotical behavior of any goes to Infinity.",
            "So is the way you generate your graph.",
            "I quit for its projectively.",
            "I mean the.",
            "With a Cup, for instance dependence on the second, the eigenvalue gap, and from this the.",
            "We could calculate that using the degree distribution, so it ought to be in the degree distribution and so by investigating the distribution of degrees that we get out of this, one ought to be able to say something about weather.",
            "For instance, the accountability of the graph has been preserved, so I mean we haven't looked in depth at whether the.",
            "Um?",
            "The generative model reproduces, for instance, the degree distribution or the the eigenvalue distribution of the of the sample.",
            "But I mean there ought to be some tests that we could do there to to see for instance, I mean if we if we were taking samples of graphs that were all.",
            "You know of a sort of dumbbell type where you had structures connected by relatively few bridging edges.",
            "We ought to be able to compute whether the eigenvalue distributions generated by the model are consistent with that kind of structure, but we haven't done that.",
            "I mean, it's a it's a.",
            "It's clearly, you know, an important thing to do in order to generate the veracity of the samples being generated by the generative model.",
            "OK, thank you very much.",
            "Yes thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'd like to talk about some work that I've been doing with Linhan Ann Richard Wilson.",
                    "label": 0
                },
                {
                    "sent": "This is of course supported by the Sinbad project and also by my Royal Society Wolfson Research Merit Award.",
                    "label": 0
                },
                {
                    "sent": "So the talk is is closely related to the talk that you have just heard and what I want to talk about here is a way of computing the infinite information criterion that is used to learn a generative model of a set of graphs, and so there are many similarities.",
                    "label": 0
                },
                {
                    "sent": "Many absences between the pieces of work, and so much so that.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My introduction, I think, is already largely been covered, so graph representations.",
                    "label": 0
                },
                {
                    "sent": "What are the advantages and disadvantages of using them to represent image data while they drive to do is that they can capture structure in a manner that is invariant to changes in viewpoint.",
                    "label": 1
                },
                {
                    "sent": "And the efficient abstractions of information in the scene and the arrangement of features or objects.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand, there are disadvantages.",
                    "label": 0
                },
                {
                    "sent": "And the main disadvantage is that graphs, representations of image structure can be very fragile.",
                    "label": 1
                },
                {
                    "sent": "There sensitive to noise and segmentation error.",
                    "label": 0
                },
                {
                    "sent": "And as pointed out in the last talk, if we're trying to learn in the graph domain, the methodology is limited due to the non vectorial nature of graph data.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sir.",
                    "label": 0
                },
                {
                    "sent": "So in this talk what I've done is and slide.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I've tried to illustrate some of the difficulties, so on the left and the right hand side I've shown a synthetic and a real world image sequence.",
                    "label": 0
                },
                {
                    "sent": "What's happening here is the camera is panning around the object and features are changing in their spatial arrangement and also in terms of their visibility as the camera pans around the object and on the what you see, is it as the?",
                    "label": 0
                },
                {
                    "sent": "As the features change their arrangement then so the graph structure representing them changes its structure in terms of connectivity.",
                    "label": 0
                },
                {
                    "sent": "And really, the aim in this talk is to try to learn the way in which a set of features change their connectivity arrangement over a set of views of a particular object.",
                    "label": 0
                },
                {
                    "sent": "So we're trying to learn variability in edge and potentially no structure when an object is viewed under different viewing conditions on those viewing conditions might be, for instance, viewpoint, or they may be due to inherent variations in the shape.",
                    "label": 0
                },
                {
                    "sent": "That object.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So welcome on do it once trying to learn in the graph domain welcome.",
                    "label": 0
                },
                {
                    "sent": "Try to cluster similar objects together and represent them using a class prototype and this class prototype for instance might be a median and this prototype could be learned using a dissimilarity measurements between graph structures.",
                    "label": 1
                },
                {
                    "sent": "One also might try to extract features and graphs and perform central clustering.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, once trying to opt out here from the the graph representation and to find a proxy vector representation that can be used to capture the graph structures.",
                    "label": 0
                },
                {
                    "sent": "In a sense, both of these two alternatives are really full.",
                    "label": 0
                },
                {
                    "sent": "Well short of the full blown problem of trying to construct a generative model which can account for variations in graph structure and which we can use both to classify graphs and also to generate if necessary new instances of graphs from a particular class using an underlying probability distribution.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why is this difficult?",
                    "label": 0
                },
                {
                    "sent": "Well, this is all been summarized in the previous talk.",
                    "label": 0
                },
                {
                    "sent": "It's difficult because graphs are not vectors, they don't naturally reside in a vector space.",
                    "label": 1
                },
                {
                    "sent": "There's no natural ordering of the nodes and edges in a graph.",
                    "label": 1
                },
                {
                    "sent": "It's difficult because we're trying to capture structural variations in graphs.",
                    "label": 0
                },
                {
                    "sent": "That is to say, we're trying to model the way in which the numbers of nodes and edges may vary.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's available in terms of generative models in the in the structural domain?",
                    "label": 1
                },
                {
                    "sent": "Well, like the previous talk, what I'm trying to do here is build on a piece of previous work that work was done met by Andrea Torsello and myself, and we published a paper on it in Pammy in 2007.",
                    "label": 0
                },
                {
                    "sent": "The idea here was to try to define the probability distribution over a prototype structure.",
                    "label": 1
                },
                {
                    "sent": "In this case a limited type of structure tree.",
                    "label": 0
                },
                {
                    "sent": "And then to try to learn the parameters of this probability distribution unit using a description length criterion.",
                    "label": 0
                },
                {
                    "sent": "So in this store I'm trying to do is to take this world and expand both on the information theoretic representation of it and move from trees to graphs and So what I'm attempting to do is to pull in ideas that we've developed in the analysis and learning of graphs using graph spectral information and graph Spectra simply is information conveyed by the eigenvalues and eigenvectors of the adjacency matrix or Laplacian matrix.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now before I go on to describe the main contributions here, I just think it's worth pausing to actually think how this work impinges on current work, in a sort of a wider sense.",
                    "label": 0
                },
                {
                    "sent": "At the moment there's a great deal of interest in the problem of deep learning, both in the neural computation and also the computer vision literature.",
                    "label": 0
                },
                {
                    "sent": "So for instance, Hinton and Bengio have developed deep belief networks to try to learn hierarchical descriptions of image data.",
                    "label": 0
                },
                {
                    "sent": "Their compositional networks developed by originally by Imagine Guimond, and our recently extended by Fergus.",
                    "label": 0
                },
                {
                    "sent": "There, Markov models of the type developed by LEONARDIS and then of course I think there are also links with stochastic image image grammars as described by Zoo Mumford.",
                    "label": 1
                },
                {
                    "sent": "And in fact, very closely related to this work, and in fact borrowing some of the ideas from it, is the work of Todd Rich and Ahuja who have developed tree based methods for taxonomy and category learning.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just to give some names when I'm trying to do is to combine spectral and structural methods.",
                    "label": 0
                },
                {
                    "sent": "I want to do this in a description length criterion and I want to apply this to the learning of generative models for graphs rather than trees.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, that's just.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A list of prior work, so let's not try to set up the description length framework for the for the present work and to show you how ideas of complexity can be imported in order to enhance what Andrea and I did previously in our family 2007 paper and and why this makes it amenable to graph.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Structures.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the literature on description length, it's a complicated one.",
                    "label": 0
                },
                {
                    "sent": "A little bit fractional.",
                    "label": 0
                },
                {
                    "sent": "I mean there are two approaches.",
                    "label": 0
                },
                {
                    "sent": "There is the approach adopted by Wallace and Freeman, which is minimum message length and the approach adopted by rising and which is the minimum description length.",
                    "label": 1
                },
                {
                    "sent": "So let me just try to control.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What these these two approaches have done because they do differ quite fundamentally in terms of their philosophy.",
                    "label": 0
                },
                {
                    "sent": "So in minimum description language.",
                    "label": 0
                },
                {
                    "sent": "It's the selection of the model that is the aim.",
                    "label": 0
                },
                {
                    "sent": "Model parameters are simply a means to end, and the parameters are usually estimated in a maximum likelihood sense, and most often the prior on the parameters is taken as flat.",
                    "label": 1
                },
                {
                    "sent": "Minimum message length, on the other hand, puts the recovery of model parameters at the center of the picture and the parameter prior may be more complicated.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you're going to apply a minimum message length or minimum description length approach to a learning problem, then you need a coding scheme.",
                    "label": 0
                },
                {
                    "sent": "And in most of the existing coding schemes, these are usually assumed to be exponential.",
                    "label": 0
                },
                {
                    "sent": "Their alternatives, such as universal codes and predictive codes, and if you then try to contrast ML&MDL in terms of the coding scheme adopted MML uses two part codes models plus parameters, and MGL.",
                    "label": 1
                },
                {
                    "sent": "The codes may be either one or.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Two part.",
                    "label": 0
                },
                {
                    "sent": "So let me try to know in general terms cost what I'm attempting to do into this into this sort of message length framework.",
                    "label": 0
                },
                {
                    "sent": "So what I want to do is to learn a model which is a super graph and this graph is going to be formed through a union of all the graphs in my training set.",
                    "label": 0
                },
                {
                    "sent": "And in order to try to capture this probabilistically, what I'm going to do is to assume that there's a sample, they're observation model, and that is based on a Bernoulli distribution over the nodes and edges of the graphs appearing in my training sample.",
                    "label": 1
                },
                {
                    "sent": "Then I'm going to attempt to model the complexity of the of the model that I'm using and hearing this or novel ingredient of this talk is to use the Von Neumann entropy of the Super graph as a measure of its complexity.",
                    "label": 0
                },
                {
                    "sent": "So then I am going to construct a code length criterion.",
                    "label": 0
                },
                {
                    "sent": "It falls it doesn't fall precisely into either the MGL all the MML.",
                    "label": 0
                },
                {
                    "sent": "A framework, it's it's somewhere between and, so it's MDL like because we make maximum likelihood estimates of the Bernoulli parameters and it's MML like because we have a two part code for the data link data fit to the data and the complexity of the Super graph model.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now to try to put that into a little bit more sort of Chris mathematical focus.",
                    "label": 0
                },
                {
                    "sent": "What I'm trying to do is to optimize the description.",
                    "label": 0
                },
                {
                    "sent": "Next criterion the code length is the sum of the negative log likelihood of the data given our Bernoulli model, and then I'm going to answer that a model code language criterion which is going to be the Von Neumann entropy of the learned super graph.",
                    "label": 0
                },
                {
                    "sent": "And so the ingredients here are going to be a set of graphs G from which I'm going to try to learn the structure and the model is going to be a super Graph prototype graph and correspondences with it.",
                    "label": 1
                },
                {
                    "sent": "And then I'm going to rather than using the the solving approach in the previous tool.",
                    "label": 1
                },
                {
                    "sent": "What I'm going to do is I'm going to use a variant of expectation maximization to learn the model, graph adjacency matrix and.",
                    "label": 0
                },
                {
                    "sent": "The correspondence is with it.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is how the learning of the Super graph proceeds were going to follow to settle in Hancock and pose the problem as learning in a description.",
                    "label": 0
                },
                {
                    "sent": "Next criterion, I'm going to borrow a probability distributions from my earlier work with been low, we're going to use this on Norman entropy, which is a new ingredient to control super graph complexity and then develop the EM algorithm to find node correspondances and the Super graph adjacency matrix.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this just is.",
                    "label": 0
                },
                {
                    "sent": "It strikes the idea of representative graph using energy.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Since you matrix here in the formal ingredients of my my my scheme, for each sample graph, I'm going to danger adjacency matrix with elements 01 depending on whether there's a connecting edge.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to learn a super graph model that captures the distribution of edges, and that model is going to be represented using a super graph adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to have a correspondences between each sample graph.",
                    "label": 1
                },
                {
                    "sent": "And the adjacency matrix and the learned model graph adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "So according to the work that's been roided, the Bernoulli distribution leads to the probability distribution at the bottom of the page for the data sample given the model gamma and the correspondence with the correspondence with it S. So what you see is it's an exponential function of the correlation.",
                    "label": 1
                },
                {
                    "sent": "Between the data graph and model graph adjacency matrices and that correlation is controlled by the correspondence indicators S.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we want to say first of all, the data part of the description, next criterion and that's just going to be the negative log likelihood of the set of data graphs given the current model.",
                    "label": 0
                },
                {
                    "sent": "So I give the log likelihood function here.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then the next ingredient is to try to write down the critic criterion of complexity for the learned adjacency matrix for the model graph and the idea here is to draw ideas from entropic measures of complexity.",
                    "label": 1
                },
                {
                    "sent": "In particular, I'm going to use the Von Neumann entropy, which is a concept introduced in quantum mechanics to measure the entropy associated with density matrix.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to use this as the modeling coding criterion in my description like framework and briefly at the end of the talk show your relationship with kernelized representation of dissimilar.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Writing.",
                    "label": 0
                },
                {
                    "sent": "So this is a definition of the Von Neumann entropy two.",
                    "label": 0
                },
                {
                    "sent": "To compute it, you commence by computing the normalized.",
                    "label": 0
                },
                {
                    "sent": "Laplacian matrix that's the degree matrix minus adjacency matrix.",
                    "label": 0
                },
                {
                    "sent": "Pre and post Volt applied by one over the square root of the degree matrix.",
                    "label": 0
                },
                {
                    "sent": "And if I have the eigenvalues lambda's for the normalized classy and volume and entropy is just mindless Lander, I / 2 times log man drive to some Dover or Laplacian eigenvalues I.",
                    "label": 0
                },
                {
                    "sent": "Now the trouble is, if I if I turn to optimize, this logarithm is going to make life difficult for me.",
                    "label": 0
                },
                {
                    "sent": "So what I want to do is to commence by approximating this form.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Entropy and what I do is I take the Shannon term P dot P appearing in the in the definition of entropy and replace it by its quadratic counterpart P * 1 -- P. So if I do that with a normalized class in eigenvalues, what happens is that the Von Neumann entropy just boils down to half the traces of organizing classy and minuses.",
                    "label": 0
                },
                {
                    "sent": "Quarter of the traits of a normalized classroom squared.",
                    "label": 0
                },
                {
                    "sent": "So what I want to do is to try to avoid computing that entropy using the eigen values and all I want to do is to see whether I can compute it using simple statistics over the graph.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that turns out to be quite straightforward.",
                    "label": 0
                },
                {
                    "sent": "The traits of a normalized class and is just a number of nodes, and the trace of a normalized, classy and squared is just given by the expression at the bottom of the slide in terms of the number of nodes and the distribution of degrees T view and TNV over pairs of nodes connected by edges in the graph.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to use this entropy measure.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which I've collected together here as a measure of complexity of the current learned model graph adjacency matrix an.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's just run through now the optimization steps.",
                    "label": 0
                },
                {
                    "sent": "I add the entropy to the negative likelihood of the data graphs given the model based on our Bernoulli Bernoulli.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Process and it turns out that the quantity I have to optimize if I go to pose this problem in terms of EN framework where I have posterior probabilities that one iteration and parameters that another iteration is given on the current slide.",
                    "label": 0
                },
                {
                    "sent": "So here the end indicates the current posterior probabilities and the N + 1.",
                    "label": 0
                },
                {
                    "sent": "Updated browsers",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then on this slide about going to the details is other two steps of an EM algorithm and optimizes that criterion.",
                    "label": 0
                },
                {
                    "sent": "I have two updates, one for the correspondence indicators between the data graphs and the the model graph, and then I have updates for the elements of the model, graph adjacency matrix, and then finally there is an update step.",
                    "label": 0
                },
                {
                    "sent": "The step where I update the posterior probabilities.",
                    "label": 0
                },
                {
                    "sent": "So that's the pretty well.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The algorithm now I can go onto the experiments we we tested this on both the coil data set, the one which previous speaker talked about and also a data set that we have developed in House and made available.",
                    "label": 0
                },
                {
                    "sent": "This is images of toys captured at different viewpoints.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what I've shown on this slide are the what's happened.",
                    "label": 0
                },
                {
                    "sent": "Has the EM algorithm, iterates on our information criterion so the there are three plots for the two datasets I've got that oil kernel data set at the top.",
                    "label": 0
                },
                {
                    "sent": "The toys data set at the bottom, and the first plot shows the model complexity.",
                    "label": 0
                },
                {
                    "sent": "Second, plot the data log likelihood.",
                    "label": 0
                },
                {
                    "sent": "And the third plot shows the combined information criterion.",
                    "label": 0
                },
                {
                    "sent": "Each of these plots show the relevant quantity as a function of iteration number, and So what you see is that.",
                    "label": 0
                },
                {
                    "sent": "Although the.",
                    "label": 0
                },
                {
                    "sent": "Model complexity initially increases after iterations it it decreases.",
                    "label": 1
                },
                {
                    "sent": "The log likelihood increases and the overall coding lens criterion decreases with iteration number and so from this you can see that probably after about 10 iterations there is reasonable convergence of the scheme.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this slide shows.",
                    "label": 0
                },
                {
                    "sent": "Experiments using the learned model graph adjacency matrix to classify objects according to class.",
                    "label": 0
                },
                {
                    "sent": "So we've trained these models up separately on the different objects in our data set and then tried to find the nearest class prototype and so here is some experiments we compare here.",
                    "label": 1
                },
                {
                    "sent": "The results obtained using the Learn super graph using our.",
                    "label": 0
                },
                {
                    "sent": "MDL criterion with complexity, median graph and then learn super graph where we exclude the complexity term in the expectation maximization algorithm and you see it all the results.",
                    "label": 1
                },
                {
                    "sent": "The complexity.",
                    "label": 0
                },
                {
                    "sent": "Moderated MDL algorithm outperforms the alternatives.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finally, what we've done is we have experimented with this technique to try to compute kernelized representation of the similarities between graphs.",
                    "label": 0
                },
                {
                    "sent": "The idea here is that we've used Jensen Shannon Divergent.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we learn a model graph pairwise.",
                    "label": 0
                },
                {
                    "sent": "Paragraphs and that will give us an entropy H of GI, cross GJ, and then we subtract the average of the Von Neumann entropy of the two graphs.",
                    "label": 1
                },
                {
                    "sent": "So that's the Jensen Shannon Divergent with then in bed.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The graphs into a 2 dimensional vector space using the Jensen Shannon Divergent and compared the results that we obtained simply using edit distances between graphs.",
                    "label": 0
                },
                {
                    "sent": "And you see that they generally using the learned model graph and the Jensen Shannon Divergent.",
                    "label": 0
                },
                {
                    "sent": "We get better class separation for the example objects here.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this just illustrates some of the adjacency matrices that we we learn for sets of sets of graphs using different algorithms and.",
                    "label": 0
                },
                {
                    "sent": "I think the main features to note here are the fact that the adjacency structures are generally similar, and then what I've done here is generated to sample graphs from the generative model, one with high probability and one with low probability.",
                    "label": 0
                },
                {
                    "sent": "Those are shown in the right hand column and you see that the high probability adjacency matrix resembles very closely to learn structure, whereas the low probability one.",
                    "label": 0
                },
                {
                    "sent": "Does not track is it really as you did.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Correct, OK, so just to conclude, we've shown how super graph or generative model of graph structure can be learned using description length.",
                    "label": 1
                },
                {
                    "sent": "In particular, using a Von Neumann entropy to represent complexity, we presented the EM algorithm to do the the learning, and in our experiments we discussed we discovered that by using this complexity measure, we're actually able to learn a rather better structure than we would without using it.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much indeed.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Entropy corresponds to the bottom and some coding player.",
                    "label": 0
                },
                {
                    "sent": "Some coding scheme photographs that I'm going through.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think we we haven't really looked at the we we borrowed the idea from from quantum mechanics, it where it's defined as the effectively the entropy associated with the density matrix, and so we're sort of intent interpreting here that are passing matrix as if it's a density matrix.",
                    "label": 0
                },
                {
                    "sent": "There's a paper in Phys Rev.",
                    "label": 0
                },
                {
                    "sent": "Recently by Simone Severini, who motivates the.",
                    "label": 0
                },
                {
                    "sent": "The use of this complexity measure in that context.",
                    "label": 0
                },
                {
                    "sent": "I don't.",
                    "label": 0
                },
                {
                    "sent": "I mean, we have not investigated in a sense, the formal link between the complexity measure and the information content or code ability of graphs.",
                    "label": 0
                },
                {
                    "sent": "And I mean, I think that that would be a next step.",
                    "label": 0
                },
                {
                    "sent": "I mean, we've mainly explored issues of whether it can be used to control a generative model, whether it can be used in a kernel.",
                    "label": 0
                },
                {
                    "sent": "So we haven't really looked at it in the compressive sense to see whether it tells us anything about how much information is lossed.",
                    "label": 0
                },
                {
                    "sent": "When you code a graph in a particular way.",
                    "label": 0
                },
                {
                    "sent": "I would like to follow up on this question, so can you maybe give us some insight on why you actually came to this choice?",
                    "label": 0
                },
                {
                    "sent": "But why does entropy measure well?",
                    "label": 0
                },
                {
                    "sent": "I mean, I think because we like Simone Severini's work and also if you haven't shown it here.",
                    "label": 0
                },
                {
                    "sent": "Write down the complexity measure.",
                    "label": 0
                },
                {
                    "sent": "Then it looks very similar to the heterogeneity measure that is been introduced by an S to Estrada in the complex systems literature.",
                    "label": 0
                },
                {
                    "sent": "What he does in that literature is to effectively compute variances over no degree over sets of edges in the graph.",
                    "label": 0
                },
                {
                    "sent": "Now we've we've taken his complexity measure and our.",
                    "label": 0
                },
                {
                    "sent": "Home for the Von Neumann entropy.",
                    "label": 0
                },
                {
                    "sent": "It turns out there are sort of some interesting relationships.",
                    "label": 0
                },
                {
                    "sent": "So in the first case, if you take estradas index and you make the graph large, then what happens is that the heterogeneity index is measuring the difference between twice the adjacency matrix and the commute time between a pair of nodes.",
                    "label": 0
                },
                {
                    "sent": "So what it's doing is telling you how different the commute time is from the shortest possible distance between a pair of nodes on the graph, and then if you analyze our complexity, measure.",
                    "label": 0
                },
                {
                    "sent": "It appears I mean astronomers index has this sort of global property of using the commute time distribution.",
                    "label": 0
                },
                {
                    "sent": "Our method seems to be somewhat more local in the way in which it accumulates information.",
                    "label": 0
                },
                {
                    "sent": "So you are essentially estimating a graph generator and it would be interesting in the following sense if you are interested in some some estimators on graph properties like capital or tightly connected components and so on.",
                    "label": 0
                },
                {
                    "sent": "You want to give guarantees in this estimation.",
                    "label": 0
                },
                {
                    "sent": "For that one, we have two less you have to investigate the asymptotical behavior of any goes to Infinity.",
                    "label": 0
                },
                {
                    "sent": "So is the way you generate your graph.",
                    "label": 0
                },
                {
                    "sent": "I quit for its projectively.",
                    "label": 0
                },
                {
                    "sent": "I mean the.",
                    "label": 0
                },
                {
                    "sent": "With a Cup, for instance dependence on the second, the eigenvalue gap, and from this the.",
                    "label": 0
                },
                {
                    "sent": "We could calculate that using the degree distribution, so it ought to be in the degree distribution and so by investigating the distribution of degrees that we get out of this, one ought to be able to say something about weather.",
                    "label": 0
                },
                {
                    "sent": "For instance, the accountability of the graph has been preserved, so I mean we haven't looked in depth at whether the.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The generative model reproduces, for instance, the degree distribution or the the eigenvalue distribution of the of the sample.",
                    "label": 0
                },
                {
                    "sent": "But I mean there ought to be some tests that we could do there to to see for instance, I mean if we if we were taking samples of graphs that were all.",
                    "label": 0
                },
                {
                    "sent": "You know of a sort of dumbbell type where you had structures connected by relatively few bridging edges.",
                    "label": 0
                },
                {
                    "sent": "We ought to be able to compute whether the eigenvalue distributions generated by the model are consistent with that kind of structure, but we haven't done that.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's a it's a.",
                    "label": 0
                },
                {
                    "sent": "It's clearly, you know, an important thing to do in order to generate the veracity of the samples being generated by the generative model.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Yes thanks.",
                    "label": 0
                }
            ]
        }
    }
}