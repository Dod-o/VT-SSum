{
    "id": "ezs37necyvithhqdljr7gxyok2vpnwh3",
    "title": "A Framework for Probability Density Estimation",
    "info": {
        "author": [
            "Shai Ben-David, School of Computer Science, University of Waterloo"
        ],
        "published": "Feb. 25, 2008",
        "recorded": "December 2007",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/ripd07_david_fbd/",
    "segmentation": [
        [
            "I'm speaking for John and Dan.",
            "My name here on the slide is just the cause when he explained this to me so that I will talk about it.",
            "I had some comments, so we decided that they should put my name too, but this is an outcome of agreeing to talk about it, not the reason that I was I'm talking about.",
            "OK, so the the idea is that it relates to the."
        ],
        [
            "Talk about two that we had earlier this morning.",
            "The problem is that there are many, many tasks in which you would like to learn a probability distribution.",
            "But as we know from the work that but we talked about this is impossible.",
            "I mean, it's too hard.",
            "You cannot learn distributions from samples.",
            "So the question is, what aspects of the distributions?",
            "And such that on one hand, so we want if you just want to do rather than learning a distribution, just learn some to predict some function.",
            "Then we know that we can do it.",
            "We have all the theory of classification to do it.",
            "So the question is, can we find some kind of a compromise?",
            "We don't want to learn the distribution fully, but we don't want just to concentrate on one task wants to learn some aspects of the distribution that will allow us to perform.",
            "A collection of tasks, so somewhere between just learning classification function and learning the complete distribution, which this is impossible, this is doable.",
            "We want something in between.",
            "So."
        ],
        [
            "The idea.",
            "OK so here is.",
            "There is an example here which I don't know how useful it is.",
            "It mate looks a bit more complicated than the issue that it comes to demonstrate, but here you are trying to learn one tasks distributions by one class SVM and you can say that if you require constraint that the sum of coefficients of your kernel is 1 then you can view the kernel as a probability distribution and the.",
            "Task of learning distribution can be translated into the task of learning account."
        ],
        [
            "And then it turns out this problem of one class SVM was addressed by.",
            "Imagery and and vapnik and they were trying to learn the full class.",
            "They were trying to.",
            "An and put constraints that.",
            "Imposed on the kernel to represent the full distribution over the sample, and it turns out that if we put less and less, if we look at the number of constraints that they put in distribution, we're trying to make the kernel reflect exactly distribution, and in order to do that we have to put on the kernel and number of constraints which corresponds to the number of sample points, and that if we look at the error of the kernel as a presentation of the distribution as the number of constraints go grow, we see that.",
            "Rather than needing the full number of constraints with the sample size, we can get pretty good approximation to distribution, which much less constraints and in some sense that was the I think the motivation of John for looking at this notion of just approximating the probability distribution rather than learning it precisely, but that's just."
        ],
        [
            "Example, and I think that the definition is more clear so.",
            "Definition The key notion is this notion of attached.",
            "Touchstone, Touchstone, class so attached or class is.",
            "A class of you have a collection of functions.",
            "And these are the functions that you are interested in being able to estimate.",
            "So rather than trying to learn a probability distribution, I say I want to learn a collection of functions over this space.",
            "I want to be able to estimate all of those functions, and I want to learn the probability distribution just to the extent that I can carry out all the tasks, estimating all the functions in F. On top of it, what he also adds is a probability solution over F. In some sense, it tells you how important are the different functions.",
            "Maybe I don't need to know to approximate in the same quality all of the functions, but some functions are more important than others and I want to be able to approximate them better.",
            "Others I don't care about having else, so I have a class of functions.",
            "This is what I want to conclude to know about the distribution and have a distribution about over this class of functions, which is some kind of kind of.",
            "Prior, what task from F is going to occur and now?",
            "If we are, we have an unknown probability distribution.",
            "And we have some approximation by say, a final sample.",
            "We will say that the error of this approximation is the expected over this version of the functions of the error that we make on each of the functions.",
            "So again, we want to learn a probability distribution, but we say this is too difficult.",
            "Rather than learning is a distribution, we have a class of tasks that we want to perform and we want to take a sample P head and say this sample performs well with respect to this collection of functions.",
            "If the expectation this is over the probability, I'll pick each function of the error that I make in estimating F from the sample.",
            "This is estimated from the sample this is.",
            "If itself, if this this.",
            "Expectation is my arrow.",
            "And with respect to this sample, so this error is with respect to the function class F and to the probability over these functions PF.",
            "And now what we want to show is that when we what conditions over the class of functions F suffice to guarantee that we can have low error in this sense.",
            "So we know that if the function F is the class of all measurable functions of our space, this is impossible.",
            "These are the results of not being able to learn a probability function.",
            "We know that if the class F contains just one function, then we can do it.",
            "So we want to find conditions on F that will allow us to approximate it from."
        ],
        [
            "Spinal samples here is an example that if I have here a distribution, that's my probability distribution and what the class of functions that I'm trying to learn is the probabilities of the corners.",
            "So I take a corner, which is what it is.",
            "In the first quadrant, I take such a corner and I want to be able.",
            "My class is the class of all possible such corners.",
            "And it turns out that in this case I can from finite sample estimate, uniformly the probability of all of those corners, although it will not be possible to estimate the probability distribution itself.",
            "So my hour.",
            "Estimation for a corner will be just the number of points here in this corner.",
            "I have three points, so my estimated for the weight of this corner in this distribution would be 3 over the number of sample points ahead."
        ],
        [
            "An another, so this is a reference from John to the talk that I'll have in the afternoon.",
            "So we define we were looking at the problem of trying to do change detection in streaming data.",
            "So you have a stream of real values and it's generated by some distribution and you want to be able to detect when does this underlying distribution that generates the data.",
            "When does it change?",
            "For example, you see prices in the stock market.",
            "You want to know when does the trend in the stock market change and you're just watching those numbers as they come so.",
            "4222 solve this problem.",
            "We developed this notion of some restricted distance between two distributions that I'll talk about in the afternoon and this is we have some kind of attached own class.",
            "The class of subsets that interest us and we just look at the Super moon of the difference between the two distributions over members of this class.",
            "And this could be viewed as a particular instantiation of this general framework of touchstone class and probability over it.",
            "And similarly, the given quantity of ambiguity and is a famous theorem tells you that you can estimate the probability of all intervals on the line uniformly for every every probability solution.",
            "For every interval of the line, if you count the number of points that fell in this interval and divided by the sample size, you get a pretty good estimate of the probability of this interval, and you can bound the.",
            "The quality of the approximation, regardless of what the distribution is, that even clarity remain.",
            "This could also be seen as a particular instantiation of this general approach."
        ],
        [
            "Yeah, and we can talk about so another one is you take you have functions which are we have class of functions over with the same binary vectors of values and we want to know what are the marginals when we project them to a subset of the variables and fix the value in the variables.",
            "So it's like you have a set of each function is you can think of it as a string and now you only want to estimate the probability of all the strings that.",
            "I hold my I hold a bunch of strings like this.",
            "All the strings that on those coordinates path through my fist and anywhere else they go anyway they like and I want to estimate the probability of this set of strings.",
            "So these are the kind of things that that you can do with this touchdown class.",
            "How well I haven't started yet, OK?"
        ],
        [
            "And so the role of so basically what I'd have would be able to say, just describe the problem to you and maybe the main result so.",
            "But the one of the distribution is a set is somehow reflects which function we think we are more likely to need to approximate well, and the we can take.",
            "The uniform distribution is just as if we don't have any information and we can take the more knowledgeable distributions if we know which functions are more likely to occur in our tasks."
        ],
        [
            "And now we can define the learnability of such a touchstone class.",
            "So we have a class of functions, and we have a distribution over it, and we want to define a notion of learnability an we say that touched on class is learnable.",
            "If we can have a uniform convergence rate for the approximation of all the functions of all the functions in this class."
        ],
        [
            "And then.",
            "The 1st result is that one can characterize this learnability in terms of the Rademacher complexity of this touchtone class, so this is an object that we can well define, and it's not difficult to see that if we do have abounded random complexitiy, then we can learn a class of functions in this respect."
        ],
        [
            "And then."
        ],
        [
            "The 2nd result is that.",
            "Right, so the 2nd result gives you a condition on when can you bound this random complexity and this is for the case that our functions are kernels.",
            "So if you have a family of kernels and we want to, this is called the support vector density estimation and we want to use them to estimate a probability distribution, then we have here abound on the router market complexity in terms of the bound on the norm of the kernels.",
            "So putting those two results together tells us that in this case we can learn a class of kernels.",
            "So if we have a bound with rather more complexity, 1st result said that we can learn it in terms of a bound on the other one capacity and the 2nd result says that for this type of functions we can bound the Rademacher complexity and therefore we can learn classes of support vector machine, estimations of probability distribution and let me just jump to the conclusion."
        ],
        [
            "So the comparison is equivalent.",
            "Classes are classes over which we have uniform convergence, and this is what we use.",
            "The fat shattering dimension for.",
            "That's the work of.",
            "Alone at all, which myself included in there at all.",
            "And here there is a comparison which tells you that these two approaches, kind of independent.",
            "None of them includes the other glyphicon teleclasses contain things which cannot be learned, as in this framework.",
            "In this framework can capture things which cannot reach an article, then cantelli classes, but I don't have time to explain what given container class."
        ],
        [
            "So.",
            "And they are."
        ],
        [
            "Paraments that showed."
        ],
        [
            "This works very nice in practice, so in summary, we introduced a new framework for learning family of task over distribution and there is the theoretical justification for this approach and experiments that show that it really works in practice.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm speaking for John and Dan.",
                    "label": 0
                },
                {
                    "sent": "My name here on the slide is just the cause when he explained this to me so that I will talk about it.",
                    "label": 0
                },
                {
                    "sent": "I had some comments, so we decided that they should put my name too, but this is an outcome of agreeing to talk about it, not the reason that I was I'm talking about.",
                    "label": 0
                },
                {
                    "sent": "OK, so the the idea is that it relates to the.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Talk about two that we had earlier this morning.",
                    "label": 0
                },
                {
                    "sent": "The problem is that there are many, many tasks in which you would like to learn a probability distribution.",
                    "label": 0
                },
                {
                    "sent": "But as we know from the work that but we talked about this is impossible.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's too hard.",
                    "label": 0
                },
                {
                    "sent": "You cannot learn distributions from samples.",
                    "label": 0
                },
                {
                    "sent": "So the question is, what aspects of the distributions?",
                    "label": 0
                },
                {
                    "sent": "And such that on one hand, so we want if you just want to do rather than learning a distribution, just learn some to predict some function.",
                    "label": 0
                },
                {
                    "sent": "Then we know that we can do it.",
                    "label": 0
                },
                {
                    "sent": "We have all the theory of classification to do it.",
                    "label": 0
                },
                {
                    "sent": "So the question is, can we find some kind of a compromise?",
                    "label": 0
                },
                {
                    "sent": "We don't want to learn the distribution fully, but we don't want just to concentrate on one task wants to learn some aspects of the distribution that will allow us to perform.",
                    "label": 0
                },
                {
                    "sent": "A collection of tasks, so somewhere between just learning classification function and learning the complete distribution, which this is impossible, this is doable.",
                    "label": 0
                },
                {
                    "sent": "We want something in between.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The idea.",
                    "label": 0
                },
                {
                    "sent": "OK so here is.",
                    "label": 0
                },
                {
                    "sent": "There is an example here which I don't know how useful it is.",
                    "label": 0
                },
                {
                    "sent": "It mate looks a bit more complicated than the issue that it comes to demonstrate, but here you are trying to learn one tasks distributions by one class SVM and you can say that if you require constraint that the sum of coefficients of your kernel is 1 then you can view the kernel as a probability distribution and the.",
                    "label": 0
                },
                {
                    "sent": "Task of learning distribution can be translated into the task of learning account.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then it turns out this problem of one class SVM was addressed by.",
                    "label": 0
                },
                {
                    "sent": "Imagery and and vapnik and they were trying to learn the full class.",
                    "label": 0
                },
                {
                    "sent": "They were trying to.",
                    "label": 0
                },
                {
                    "sent": "An and put constraints that.",
                    "label": 0
                },
                {
                    "sent": "Imposed on the kernel to represent the full distribution over the sample, and it turns out that if we put less and less, if we look at the number of constraints that they put in distribution, we're trying to make the kernel reflect exactly distribution, and in order to do that we have to put on the kernel and number of constraints which corresponds to the number of sample points, and that if we look at the error of the kernel as a presentation of the distribution as the number of constraints go grow, we see that.",
                    "label": 0
                },
                {
                    "sent": "Rather than needing the full number of constraints with the sample size, we can get pretty good approximation to distribution, which much less constraints and in some sense that was the I think the motivation of John for looking at this notion of just approximating the probability distribution rather than learning it precisely, but that's just.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example, and I think that the definition is more clear so.",
                    "label": 0
                },
                {
                    "sent": "Definition The key notion is this notion of attached.",
                    "label": 0
                },
                {
                    "sent": "Touchstone, Touchstone, class so attached or class is.",
                    "label": 0
                },
                {
                    "sent": "A class of you have a collection of functions.",
                    "label": 0
                },
                {
                    "sent": "And these are the functions that you are interested in being able to estimate.",
                    "label": 0
                },
                {
                    "sent": "So rather than trying to learn a probability distribution, I say I want to learn a collection of functions over this space.",
                    "label": 0
                },
                {
                    "sent": "I want to be able to estimate all of those functions, and I want to learn the probability distribution just to the extent that I can carry out all the tasks, estimating all the functions in F. On top of it, what he also adds is a probability solution over F. In some sense, it tells you how important are the different functions.",
                    "label": 0
                },
                {
                    "sent": "Maybe I don't need to know to approximate in the same quality all of the functions, but some functions are more important than others and I want to be able to approximate them better.",
                    "label": 0
                },
                {
                    "sent": "Others I don't care about having else, so I have a class of functions.",
                    "label": 0
                },
                {
                    "sent": "This is what I want to conclude to know about the distribution and have a distribution about over this class of functions, which is some kind of kind of.",
                    "label": 0
                },
                {
                    "sent": "Prior, what task from F is going to occur and now?",
                    "label": 0
                },
                {
                    "sent": "If we are, we have an unknown probability distribution.",
                    "label": 0
                },
                {
                    "sent": "And we have some approximation by say, a final sample.",
                    "label": 0
                },
                {
                    "sent": "We will say that the error of this approximation is the expected over this version of the functions of the error that we make on each of the functions.",
                    "label": 0
                },
                {
                    "sent": "So again, we want to learn a probability distribution, but we say this is too difficult.",
                    "label": 0
                },
                {
                    "sent": "Rather than learning is a distribution, we have a class of tasks that we want to perform and we want to take a sample P head and say this sample performs well with respect to this collection of functions.",
                    "label": 0
                },
                {
                    "sent": "If the expectation this is over the probability, I'll pick each function of the error that I make in estimating F from the sample.",
                    "label": 0
                },
                {
                    "sent": "This is estimated from the sample this is.",
                    "label": 0
                },
                {
                    "sent": "If itself, if this this.",
                    "label": 0
                },
                {
                    "sent": "Expectation is my arrow.",
                    "label": 0
                },
                {
                    "sent": "And with respect to this sample, so this error is with respect to the function class F and to the probability over these functions PF.",
                    "label": 0
                },
                {
                    "sent": "And now what we want to show is that when we what conditions over the class of functions F suffice to guarantee that we can have low error in this sense.",
                    "label": 0
                },
                {
                    "sent": "So we know that if the function F is the class of all measurable functions of our space, this is impossible.",
                    "label": 0
                },
                {
                    "sent": "These are the results of not being able to learn a probability function.",
                    "label": 0
                },
                {
                    "sent": "We know that if the class F contains just one function, then we can do it.",
                    "label": 0
                },
                {
                    "sent": "So we want to find conditions on F that will allow us to approximate it from.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Spinal samples here is an example that if I have here a distribution, that's my probability distribution and what the class of functions that I'm trying to learn is the probabilities of the corners.",
                    "label": 0
                },
                {
                    "sent": "So I take a corner, which is what it is.",
                    "label": 0
                },
                {
                    "sent": "In the first quadrant, I take such a corner and I want to be able.",
                    "label": 0
                },
                {
                    "sent": "My class is the class of all possible such corners.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that in this case I can from finite sample estimate, uniformly the probability of all of those corners, although it will not be possible to estimate the probability distribution itself.",
                    "label": 0
                },
                {
                    "sent": "So my hour.",
                    "label": 0
                },
                {
                    "sent": "Estimation for a corner will be just the number of points here in this corner.",
                    "label": 0
                },
                {
                    "sent": "I have three points, so my estimated for the weight of this corner in this distribution would be 3 over the number of sample points ahead.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An another, so this is a reference from John to the talk that I'll have in the afternoon.",
                    "label": 0
                },
                {
                    "sent": "So we define we were looking at the problem of trying to do change detection in streaming data.",
                    "label": 0
                },
                {
                    "sent": "So you have a stream of real values and it's generated by some distribution and you want to be able to detect when does this underlying distribution that generates the data.",
                    "label": 0
                },
                {
                    "sent": "When does it change?",
                    "label": 0
                },
                {
                    "sent": "For example, you see prices in the stock market.",
                    "label": 0
                },
                {
                    "sent": "You want to know when does the trend in the stock market change and you're just watching those numbers as they come so.",
                    "label": 0
                },
                {
                    "sent": "4222 solve this problem.",
                    "label": 0
                },
                {
                    "sent": "We developed this notion of some restricted distance between two distributions that I'll talk about in the afternoon and this is we have some kind of attached own class.",
                    "label": 1
                },
                {
                    "sent": "The class of subsets that interest us and we just look at the Super moon of the difference between the two distributions over members of this class.",
                    "label": 1
                },
                {
                    "sent": "And this could be viewed as a particular instantiation of this general framework of touchstone class and probability over it.",
                    "label": 0
                },
                {
                    "sent": "And similarly, the given quantity of ambiguity and is a famous theorem tells you that you can estimate the probability of all intervals on the line uniformly for every every probability solution.",
                    "label": 0
                },
                {
                    "sent": "For every interval of the line, if you count the number of points that fell in this interval and divided by the sample size, you get a pretty good estimate of the probability of this interval, and you can bound the.",
                    "label": 0
                },
                {
                    "sent": "The quality of the approximation, regardless of what the distribution is, that even clarity remain.",
                    "label": 0
                },
                {
                    "sent": "This could also be seen as a particular instantiation of this general approach.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, and we can talk about so another one is you take you have functions which are we have class of functions over with the same binary vectors of values and we want to know what are the marginals when we project them to a subset of the variables and fix the value in the variables.",
                    "label": 0
                },
                {
                    "sent": "So it's like you have a set of each function is you can think of it as a string and now you only want to estimate the probability of all the strings that.",
                    "label": 0
                },
                {
                    "sent": "I hold my I hold a bunch of strings like this.",
                    "label": 0
                },
                {
                    "sent": "All the strings that on those coordinates path through my fist and anywhere else they go anyway they like and I want to estimate the probability of this set of strings.",
                    "label": 0
                },
                {
                    "sent": "So these are the kind of things that that you can do with this touchdown class.",
                    "label": 0
                },
                {
                    "sent": "How well I haven't started yet, OK?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so the role of so basically what I'd have would be able to say, just describe the problem to you and maybe the main result so.",
                    "label": 0
                },
                {
                    "sent": "But the one of the distribution is a set is somehow reflects which function we think we are more likely to need to approximate well, and the we can take.",
                    "label": 0
                },
                {
                    "sent": "The uniform distribution is just as if we don't have any information and we can take the more knowledgeable distributions if we know which functions are more likely to occur in our tasks.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we can define the learnability of such a touchstone class.",
                    "label": 0
                },
                {
                    "sent": "So we have a class of functions, and we have a distribution over it, and we want to define a notion of learnability an we say that touched on class is learnable.",
                    "label": 0
                },
                {
                    "sent": "If we can have a uniform convergence rate for the approximation of all the functions of all the functions in this class.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "The 1st result is that one can characterize this learnability in terms of the Rademacher complexity of this touchtone class, so this is an object that we can well define, and it's not difficult to see that if we do have abounded random complexitiy, then we can learn a class of functions in this respect.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The 2nd result is that.",
                    "label": 0
                },
                {
                    "sent": "Right, so the 2nd result gives you a condition on when can you bound this random complexity and this is for the case that our functions are kernels.",
                    "label": 0
                },
                {
                    "sent": "So if you have a family of kernels and we want to, this is called the support vector density estimation and we want to use them to estimate a probability distribution, then we have here abound on the router market complexity in terms of the bound on the norm of the kernels.",
                    "label": 0
                },
                {
                    "sent": "So putting those two results together tells us that in this case we can learn a class of kernels.",
                    "label": 0
                },
                {
                    "sent": "So if we have a bound with rather more complexity, 1st result said that we can learn it in terms of a bound on the other one capacity and the 2nd result says that for this type of functions we can bound the Rademacher complexity and therefore we can learn classes of support vector machine, estimations of probability distribution and let me just jump to the conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the comparison is equivalent.",
                    "label": 0
                },
                {
                    "sent": "Classes are classes over which we have uniform convergence, and this is what we use.",
                    "label": 0
                },
                {
                    "sent": "The fat shattering dimension for.",
                    "label": 0
                },
                {
                    "sent": "That's the work of.",
                    "label": 0
                },
                {
                    "sent": "Alone at all, which myself included in there at all.",
                    "label": 0
                },
                {
                    "sent": "And here there is a comparison which tells you that these two approaches, kind of independent.",
                    "label": 0
                },
                {
                    "sent": "None of them includes the other glyphicon teleclasses contain things which cannot be learned, as in this framework.",
                    "label": 0
                },
                {
                    "sent": "In this framework can capture things which cannot reach an article, then cantelli classes, but I don't have time to explain what given container class.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And they are.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Paraments that showed.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This works very nice in practice, so in summary, we introduced a new framework for learning family of task over distribution and there is the theoretical justification for this approach and experiments that show that it really works in practice.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}