{
    "id": "no6y5widcdktau2zy5obsmv4643byr5h",
    "title": "Solving Person Re-identification in Non-overlapping Camera using Efficient Gibbs Sampling",
    "info": {
        "author": [
            "Gwenn Englebienne, University of Amsterdam"
        ],
        "published": "April 3, 2014",
        "recorded": "September 2013",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/bmvc2013_englebienne_gibbs_sampling/",
    "segmentation": [
        [
            "Good afternoon, Ann.",
            "Indeed the talk I want to present is about solving the identification of persons in networks of cameras and focusing on re identifying tracks of people.",
            "So we're not focusing on tracking people inside cameras but across cameras."
        ],
        [
            "I'm in this problem can come in many forms.",
            "You can either have a single camera, track, people working under the camera screen view and wanting to detect whether the person that you see at some point corresponds to the same person bit later, or it can be with many cameras in a network of cameras and then a distinction can be made between cameras that overlap and cameras that don't overlap, and when cameras overlap things are easier because then you can use.",
            "The extrinsic calibration of the cameras to recover that the person that you see in both cameras is actually standing at the same location and therefore the same person.",
            "When the cameras don't overlap, you don't have this luxury and therefore you have to rely on other information such as appearance."
        ],
        [
            "Obviously this kind of personal identification has many applications.",
            "It's used in the analysis of people's behavior, is used a lot in or it has lots of applications in surveillance and also in building logistics.",
            "People like or do owners of big buildings like to know what routes people take true their buildings so that they know where to put the advertisements and this kind of rent ification can help them for this.",
            "And so how do you solve the problem?",
            "Well, we have to look at appearance, features, features that are person specific that allow you to recover the identification, the identity of the person and temporal information that allows you to say how likely it is to go from 1 camera to the next in a certain amount of time.",
            "Now the issues that we run run up against our manifold.",
            "One is that the appearance of a person will vary overtime and will vary across cameras.",
            "Different cameras have different sensors which are not color constant and therefore dependence of the people will vary and people can just change clothes or whatever, which will also make our problem harder.",
            "Moreover, we don't have all of the temporal information because we don't have overlapping things of you and therefore all possible combinations of label identifying the people.",
            "Are possible and will result in different scores or different likelihoods.",
            "So therefore we have an exponential increase of the complexity with the number of observations and the number of people."
        ],
        [
            "So what are we trying to do?",
            "We're trying to do unsupervised person trajectory identification and networks of non overlapping cameras where we do explicitly take into account the variation in illumination variation in camera parameters and taking care of the high computational complexity of this kind of problem.",
            "And our solution relies on using a probabilistic graphical model where we infer the person labels using.",
            "Sampling algorithm, so we're not doing this exactly, but we have an efficient Gibbs sampling that makes it possible.",
            "And we use a full Bayesian model where we have closed form updates for the absolute appearance, the person's appearance, and the camera gain, and the camera illumination boundaries or noise."
        ],
        [
            "This.",
            "So what does this look like?",
            "Well, when you have a single person, you get a very nice sparse graphical model where you have a set of observations X one to extend and a set of identifiers that want to depend which corresponds to the person labels.",
            "And if you know these labels, then for one person the set of observations which consists of some appearance features.",
            "In our case just the RGB components of the average person appearance label that identifies the camera that.",
            "Record that track a timestamp of when a person enters a cameras film review.",
            "And the timestamp of when the person left the cameras interview and we combine these into this graphical model where.",
            "Our where time goes down.",
            "Each row corresponds to one track and we have say at time 2G appearance at time two depends on the identity of the person, but also the camera that's records to track the camera that recorded the track depends on which camera recorded the previous track.",
            "So we have a transition probability between the cameras, the time at which the person entered the cameras field of view depends on which cameras.",
            "In the view it entered which cameras field of view at last left and what time at last left.",
            "This camera.",
            "Swing view ads.",
            "If that makes sense and this goes on for all observations that correspond with that person's identity."
        ],
        [
            "Right now, how do we model all of this?",
            "Well, again for, one person will have this this model.",
            "The appearance is modeled in a nice and fancy way where we have two additive components.",
            "One is the absolute persons appearance and one is camera specific noise and one is the camera specific gain.",
            "So we have.",
            "So the two additional components are then multiplied by the camera specific game and.",
            "We define distributions over this for the gain, which is always positive.",
            "We'd like to have a gamma distribution, but we will approximate that with the Gaussian distribution.",
            "To keep things analytically closed form and then two Gaussian distributions, 3 dimensional Gaussian distributions over the appearance parameters.",
            "I'm we Additionally have transition probabilities across cameras which are multinomial distributions and distributions.",
            "Overtime for transition between camera pairs and which are modeled with a gamma distribution.",
            "These two distributions are provided before hands.",
            "There are specific to the network and do not depend on the people that exist in the system.",
            "This is not a limitation of the system, it could.",
            "We could well learn all of the parameters together, but it's nice to be able to provide these parameters because it allows you to avoid being trapped in local Optima, and it allows you to specify, for example, that by not allowing somebody to enter cameras field of view before that same person has left that same camera field of view when you see two people in the same camera, these two people are necessarily different and have different identities.",
            "I'm.",
            "Then for the other parameters, we do learn these these distributions an we provide conjugate gradient conjugate priors over the parameters of these distributions for the gain at just a normal gamma distribution and for the additive RGB components.",
            "These are normal Wishart distributions.",
            "And this allows us to have a closed form analytical solution for the posterior, which we can then integrate out and compute the probability that a certain uncertain trajectory actually corresponds to a certain person."
        ],
        [
            "Now the problem that we have is that when we know the labels, things are nice, intractable.",
            "But when we don't know the labels which you see here on my left.",
            "And the model becomes a complete mess, basically because the appearances depends on the price over the parameters, so the camera specific game parameters, the camera specific noise parameters and person specific appearance branches, and then each appearance depends on the identity of the person and the camera by which disappearance has been recorded and each camera depends on the last camera that that person was seen, but that could have been any of the previous cameras.",
            "So it depends on all previous cameras.",
            "And all previous labels and the same holds for the entrance and exit times.",
            "So that means that basically the probability of 1 observation which previously had this nice first order Markov property Now depends on all other observations and the Markov blanket of any observation is all other observations.",
            "So basically simply doing inference is completely intractable here.",
            "So now our solution is to approximate the distributions over the person labels by doing sampling."
        ],
        [
            "And we can do this by doing simple, very simple Gibbs sampling for each.",
            "Each observation we can compute the probability given all other labels given some say initial distribution over the labels and probability that that observation belongs to any of the possible identities that we have around system.",
            "And then we can just use Bayes rule to compute the probability of any label and sample the table.",
            "The problem is that to compute this Bayes rule we need to compute the probability of all observations given our sets are new sets of person labels and that requires us to go for each each observation through all through all observations again and results in quadratic complexity.",
            "So this is still quite slow.",
            "But with a nice and clever bookkeeping system, we can actually reduce the complexity of the sampling to linear complexity by making the computation of the probability of all observations constants for each new change in set of tables."
        ],
        [
            "And this works as follows.",
            "What we do is we have a vector that represents the set of identities for all observations, and for each of these observations, each index.",
            "In these observations we keep a forward and backwards pointer or set of pointers that will just identify the next time that we see that same person again and the previous time that we have seen that person in the sequence of observations.",
            "And that makes sense.",
            "Then we compute the probability of all observations given some initial value for our identity vector.",
            "And when we try to change when we change one label, we take that original probability distribution.",
            "The probability that P of X given that and we multiply in those terms, that becomes true under the new label and divide out the terms that become false with the new label.",
            "So we multiply in the probability of the current observation given the new label and the new previous time that we saw that that person we multiply the probability.",
            "Of the next time we saw the old label, and given that old label and the new previous time that we saw the old table, which is not the same time as where we're changing it now and we multiply in the probability of the next time that we see in a person with the new label given the current observation, and we divide out the probability of the current observation given the old label and probability of the next time that we see that person given your label given the current.",
            "Observation and so on.",
            "I hope that makes sense.",
            "I'll try to show that graphically in 2nd and then we update the pointers and so we can go through our whole sequence of observations, compute the probability of any new identity at anytime in constant time, and so therefore our sampling becomes tractable and things become infeasible."
        ],
        [
            "So just graphically we have a set of labels that sends it.",
            "Primes are shown here for each that prime we show, we keep a pointer that keeps track of when the next time is that we see that the prime.",
            "Same for that and then we change the label.",
            "We use these pointers to compute efficiently the probability of all observations given the new vector and we updated the pointers."
        ],
        [
            "Right?",
            "So then this brings me to the experiments.",
            "We tried this out on two datasets that were recorded at Philips.",
            "Basically cameras.",
            "The datasets were recorded with networks of five and 13 cameras recording people 5 to 10 people working under these cameras in random.",
            "Random trajectories.",
            "The cameras were all ceiling mounted and looking straight down.",
            "We did this because when when cameras are ceiling mounted and looking straight down when you have many people in a camera's field of view there much more unlikely to include each other.",
            "Then when you have a sideways or elevated side view cameras and therefore the tracking is easier.",
            "Moreover these were stereo cameras which allows us to do very accurate tracking without having false.",
            "Those tracking switches and so on, which meant that our appearance features were actually quite clean and pure.",
            "Now we compare this to a state of the art method that's the same thing.",
            "The Pezula sampler, where instead of keeping track for each observation of which identity this observation belong to, it keeps track for each identity of a sequence of observations that are assigned to that identity.",
            "This means that the sampling becomes much harder because you have to modify these sequences, making sure that the modification will not assign any observation to two people or will not drop any observations and the way they do that is by doing crossover between sequences of observations at random.",
            "So without any informative prior distribution and therefore the sampling is quite inefficient.",
            "Moreover, the parameters of the distributions are not as well.",
            "They learn by maximum likelihood, so they're more likely to overfit and they do not model any camera specific parameters in their model.",
            "I'm so the next set of experiments that we did was to try to see whether this fancy model of the camera specific parameters and added in addition to the person specific parameters actually bought us anything.",
            "And we will see that indeed they do."
        ],
        [
            "So here we see all of our results nicely summarized in one page on the right for me on the left for you, and we have a comparative results with the state of the art.",
            "And we see that for different for our datasets an with five or ten subjects, we consistently significantly outperform the baseline.",
            "And we also see that the computational complexity of the original Pezula sampler is indeed quadratic and actually quadratic in the number of people with exponential in the number of of observations.",
            "So this is actually.",
            "Not tractable, and while our skills much better and then when we compare our fully Bayesian methods with the fancy modeling of the camera specific parameters with a naive fully Bayesian model of only the person specific parameters, we still see that the fancy model significantly outperforms the naive model.",
            "The last thing that we did was to say how are or was to check how our system scales when more data is gathered.",
            "What we did was just cuts bits of the data where we had 200 frames, 16 frames and so on.",
            "With fewer frames we have fewer people in the system and therefore things are easier.",
            "But we also have less information about those people.",
            "It turns out that actually this extra information that we get by having more frames actually improves the performance, and so that's a nice result because it shows that our approach actually scales with more data, which we're very happy about."
        ],
        [
            "So to summarize, we proposed an unsupervised methods for person trajectory identification in networks of non overlapping cameras where we explicitly model variations that are due to cameras and the location of cameras which explicitly model the cameras transitions and which deals with the high computational complexity of the full exact inference in the model.",
            "We use the probabilistic graphical model to do this.",
            "And we proposed an efficient way to do Gibbs sampling in this model, where we also we also provide closed form analytical updates for the parameters over appearance gain and illumination variance, and we show that this leads to significantly improved performance over the state of the art.",
            "So with."
        ],
        [
            "So thank you, and I'm open for questions.",
            "So.",
            "This is something that particular sampler seems to do, though, is switch the identity of multiple time steps at the same time.",
            "So I can imagine that Gibbs sampler gets stuck in a local minimum because.",
            "You know you have to change the future in the past at the same time, or it won't.",
            "It won't be.",
            "It won't look as good, right?",
            "That's a very good point.",
            "That is true.",
            "Actually.",
            "It turns out that by using this fully Bayesian model and you learn specific distributions over the appearance of people slowly, and therefore you have, you basically get the sort of simulated annealing kind of thing where you still converge to a good optimum, despite the fact that absolutely.",
            "By changing a single single person you know you may have that you get into into situation where changing second person is impossible while actually changing as a set of people who should be better.",
            "And we did experiment a little bit with blog, Ipswich probably would have improved it as well, but it turned out not to make a big difference.",
            "So have you considered solving the problem using integer programming rather than MCMC for optimization?",
            "No, we didn't.",
            "I don't quite see why you say that our data set is so small.",
            "We do have lots of different trajectories and it says the number of trajectories that actually matters.",
            "It's not so much the number of people, so it's true that we have few people, but we have.",
            "I don't remember the exact numbers, but tends close to hundreds of different trajectories and so the number of possible combinations is actually exponential in that.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good afternoon, Ann.",
                    "label": 0
                },
                {
                    "sent": "Indeed the talk I want to present is about solving the identification of persons in networks of cameras and focusing on re identifying tracks of people.",
                    "label": 0
                },
                {
                    "sent": "So we're not focusing on tracking people inside cameras but across cameras.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm in this problem can come in many forms.",
                    "label": 0
                },
                {
                    "sent": "You can either have a single camera, track, people working under the camera screen view and wanting to detect whether the person that you see at some point corresponds to the same person bit later, or it can be with many cameras in a network of cameras and then a distinction can be made between cameras that overlap and cameras that don't overlap, and when cameras overlap things are easier because then you can use.",
                    "label": 0
                },
                {
                    "sent": "The extrinsic calibration of the cameras to recover that the person that you see in both cameras is actually standing at the same location and therefore the same person.",
                    "label": 0
                },
                {
                    "sent": "When the cameras don't overlap, you don't have this luxury and therefore you have to rely on other information such as appearance.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Obviously this kind of personal identification has many applications.",
                    "label": 0
                },
                {
                    "sent": "It's used in the analysis of people's behavior, is used a lot in or it has lots of applications in surveillance and also in building logistics.",
                    "label": 0
                },
                {
                    "sent": "People like or do owners of big buildings like to know what routes people take true their buildings so that they know where to put the advertisements and this kind of rent ification can help them for this.",
                    "label": 0
                },
                {
                    "sent": "And so how do you solve the problem?",
                    "label": 1
                },
                {
                    "sent": "Well, we have to look at appearance, features, features that are person specific that allow you to recover the identification, the identity of the person and temporal information that allows you to say how likely it is to go from 1 camera to the next in a certain amount of time.",
                    "label": 1
                },
                {
                    "sent": "Now the issues that we run run up against our manifold.",
                    "label": 0
                },
                {
                    "sent": "One is that the appearance of a person will vary overtime and will vary across cameras.",
                    "label": 0
                },
                {
                    "sent": "Different cameras have different sensors which are not color constant and therefore dependence of the people will vary and people can just change clothes or whatever, which will also make our problem harder.",
                    "label": 0
                },
                {
                    "sent": "Moreover, we don't have all of the temporal information because we don't have overlapping things of you and therefore all possible combinations of label identifying the people.",
                    "label": 0
                },
                {
                    "sent": "Are possible and will result in different scores or different likelihoods.",
                    "label": 0
                },
                {
                    "sent": "So therefore we have an exponential increase of the complexity with the number of observations and the number of people.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are we trying to do?",
                    "label": 0
                },
                {
                    "sent": "We're trying to do unsupervised person trajectory identification and networks of non overlapping cameras where we do explicitly take into account the variation in illumination variation in camera parameters and taking care of the high computational complexity of this kind of problem.",
                    "label": 1
                },
                {
                    "sent": "And our solution relies on using a probabilistic graphical model where we infer the person labels using.",
                    "label": 1
                },
                {
                    "sent": "Sampling algorithm, so we're not doing this exactly, but we have an efficient Gibbs sampling that makes it possible.",
                    "label": 0
                },
                {
                    "sent": "And we use a full Bayesian model where we have closed form updates for the absolute appearance, the person's appearance, and the camera gain, and the camera illumination boundaries or noise.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "So what does this look like?",
                    "label": 0
                },
                {
                    "sent": "Well, when you have a single person, you get a very nice sparse graphical model where you have a set of observations X one to extend and a set of identifiers that want to depend which corresponds to the person labels.",
                    "label": 1
                },
                {
                    "sent": "And if you know these labels, then for one person the set of observations which consists of some appearance features.",
                    "label": 0
                },
                {
                    "sent": "In our case just the RGB components of the average person appearance label that identifies the camera that.",
                    "label": 0
                },
                {
                    "sent": "Record that track a timestamp of when a person enters a cameras film review.",
                    "label": 0
                },
                {
                    "sent": "And the timestamp of when the person left the cameras interview and we combine these into this graphical model where.",
                    "label": 0
                },
                {
                    "sent": "Our where time goes down.",
                    "label": 0
                },
                {
                    "sent": "Each row corresponds to one track and we have say at time 2G appearance at time two depends on the identity of the person, but also the camera that's records to track the camera that recorded the track depends on which camera recorded the previous track.",
                    "label": 0
                },
                {
                    "sent": "So we have a transition probability between the cameras, the time at which the person entered the cameras field of view depends on which cameras.",
                    "label": 0
                },
                {
                    "sent": "In the view it entered which cameras field of view at last left and what time at last left.",
                    "label": 0
                },
                {
                    "sent": "This camera.",
                    "label": 0
                },
                {
                    "sent": "Swing view ads.",
                    "label": 0
                },
                {
                    "sent": "If that makes sense and this goes on for all observations that correspond with that person's identity.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right now, how do we model all of this?",
                    "label": 0
                },
                {
                    "sent": "Well, again for, one person will have this this model.",
                    "label": 0
                },
                {
                    "sent": "The appearance is modeled in a nice and fancy way where we have two additive components.",
                    "label": 0
                },
                {
                    "sent": "One is the absolute persons appearance and one is camera specific noise and one is the camera specific gain.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "So the two additional components are then multiplied by the camera specific game and.",
                    "label": 0
                },
                {
                    "sent": "We define distributions over this for the gain, which is always positive.",
                    "label": 0
                },
                {
                    "sent": "We'd like to have a gamma distribution, but we will approximate that with the Gaussian distribution.",
                    "label": 1
                },
                {
                    "sent": "To keep things analytically closed form and then two Gaussian distributions, 3 dimensional Gaussian distributions over the appearance parameters.",
                    "label": 0
                },
                {
                    "sent": "I'm we Additionally have transition probabilities across cameras which are multinomial distributions and distributions.",
                    "label": 0
                },
                {
                    "sent": "Overtime for transition between camera pairs and which are modeled with a gamma distribution.",
                    "label": 0
                },
                {
                    "sent": "These two distributions are provided before hands.",
                    "label": 0
                },
                {
                    "sent": "There are specific to the network and do not depend on the people that exist in the system.",
                    "label": 0
                },
                {
                    "sent": "This is not a limitation of the system, it could.",
                    "label": 0
                },
                {
                    "sent": "We could well learn all of the parameters together, but it's nice to be able to provide these parameters because it allows you to avoid being trapped in local Optima, and it allows you to specify, for example, that by not allowing somebody to enter cameras field of view before that same person has left that same camera field of view when you see two people in the same camera, these two people are necessarily different and have different identities.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "Then for the other parameters, we do learn these these distributions an we provide conjugate gradient conjugate priors over the parameters of these distributions for the gain at just a normal gamma distribution and for the additive RGB components.",
                    "label": 0
                },
                {
                    "sent": "These are normal Wishart distributions.",
                    "label": 0
                },
                {
                    "sent": "And this allows us to have a closed form analytical solution for the posterior, which we can then integrate out and compute the probability that a certain uncertain trajectory actually corresponds to a certain person.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the problem that we have is that when we know the labels, things are nice, intractable.",
                    "label": 0
                },
                {
                    "sent": "But when we don't know the labels which you see here on my left.",
                    "label": 0
                },
                {
                    "sent": "And the model becomes a complete mess, basically because the appearances depends on the price over the parameters, so the camera specific game parameters, the camera specific noise parameters and person specific appearance branches, and then each appearance depends on the identity of the person and the camera by which disappearance has been recorded and each camera depends on the last camera that that person was seen, but that could have been any of the previous cameras.",
                    "label": 0
                },
                {
                    "sent": "So it depends on all previous cameras.",
                    "label": 0
                },
                {
                    "sent": "And all previous labels and the same holds for the entrance and exit times.",
                    "label": 0
                },
                {
                    "sent": "So that means that basically the probability of 1 observation which previously had this nice first order Markov property Now depends on all other observations and the Markov blanket of any observation is all other observations.",
                    "label": 0
                },
                {
                    "sent": "So basically simply doing inference is completely intractable here.",
                    "label": 0
                },
                {
                    "sent": "So now our solution is to approximate the distributions over the person labels by doing sampling.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can do this by doing simple, very simple Gibbs sampling for each.",
                    "label": 0
                },
                {
                    "sent": "Each observation we can compute the probability given all other labels given some say initial distribution over the labels and probability that that observation belongs to any of the possible identities that we have around system.",
                    "label": 0
                },
                {
                    "sent": "And then we can just use Bayes rule to compute the probability of any label and sample the table.",
                    "label": 0
                },
                {
                    "sent": "The problem is that to compute this Bayes rule we need to compute the probability of all observations given our sets are new sets of person labels and that requires us to go for each each observation through all through all observations again and results in quadratic complexity.",
                    "label": 0
                },
                {
                    "sent": "So this is still quite slow.",
                    "label": 0
                },
                {
                    "sent": "But with a nice and clever bookkeeping system, we can actually reduce the complexity of the sampling to linear complexity by making the computation of the probability of all observations constants for each new change in set of tables.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this works as follows.",
                    "label": 0
                },
                {
                    "sent": "What we do is we have a vector that represents the set of identities for all observations, and for each of these observations, each index.",
                    "label": 1
                },
                {
                    "sent": "In these observations we keep a forward and backwards pointer or set of pointers that will just identify the next time that we see that same person again and the previous time that we have seen that person in the sequence of observations.",
                    "label": 0
                },
                {
                    "sent": "And that makes sense.",
                    "label": 0
                },
                {
                    "sent": "Then we compute the probability of all observations given some initial value for our identity vector.",
                    "label": 0
                },
                {
                    "sent": "And when we try to change when we change one label, we take that original probability distribution.",
                    "label": 0
                },
                {
                    "sent": "The probability that P of X given that and we multiply in those terms, that becomes true under the new label and divide out the terms that become false with the new label.",
                    "label": 1
                },
                {
                    "sent": "So we multiply in the probability of the current observation given the new label and the new previous time that we saw that that person we multiply the probability.",
                    "label": 0
                },
                {
                    "sent": "Of the next time we saw the old label, and given that old label and the new previous time that we saw the old table, which is not the same time as where we're changing it now and we multiply in the probability of the next time that we see in a person with the new label given the current observation, and we divide out the probability of the current observation given the old label and probability of the next time that we see that person given your label given the current.",
                    "label": 0
                },
                {
                    "sent": "Observation and so on.",
                    "label": 0
                },
                {
                    "sent": "I hope that makes sense.",
                    "label": 1
                },
                {
                    "sent": "I'll try to show that graphically in 2nd and then we update the pointers and so we can go through our whole sequence of observations, compute the probability of any new identity at anytime in constant time, and so therefore our sampling becomes tractable and things become infeasible.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just graphically we have a set of labels that sends it.",
                    "label": 0
                },
                {
                    "sent": "Primes are shown here for each that prime we show, we keep a pointer that keeps track of when the next time is that we see that the prime.",
                    "label": 0
                },
                {
                    "sent": "Same for that and then we change the label.",
                    "label": 0
                },
                {
                    "sent": "We use these pointers to compute efficiently the probability of all observations given the new vector and we updated the pointers.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So then this brings me to the experiments.",
                    "label": 0
                },
                {
                    "sent": "We tried this out on two datasets that were recorded at Philips.",
                    "label": 0
                },
                {
                    "sent": "Basically cameras.",
                    "label": 0
                },
                {
                    "sent": "The datasets were recorded with networks of five and 13 cameras recording people 5 to 10 people working under these cameras in random.",
                    "label": 0
                },
                {
                    "sent": "Random trajectories.",
                    "label": 0
                },
                {
                    "sent": "The cameras were all ceiling mounted and looking straight down.",
                    "label": 0
                },
                {
                    "sent": "We did this because when when cameras are ceiling mounted and looking straight down when you have many people in a camera's field of view there much more unlikely to include each other.",
                    "label": 0
                },
                {
                    "sent": "Then when you have a sideways or elevated side view cameras and therefore the tracking is easier.",
                    "label": 0
                },
                {
                    "sent": "Moreover these were stereo cameras which allows us to do very accurate tracking without having false.",
                    "label": 0
                },
                {
                    "sent": "Those tracking switches and so on, which meant that our appearance features were actually quite clean and pure.",
                    "label": 0
                },
                {
                    "sent": "Now we compare this to a state of the art method that's the same thing.",
                    "label": 0
                },
                {
                    "sent": "The Pezula sampler, where instead of keeping track for each observation of which identity this observation belong to, it keeps track for each identity of a sequence of observations that are assigned to that identity.",
                    "label": 0
                },
                {
                    "sent": "This means that the sampling becomes much harder because you have to modify these sequences, making sure that the modification will not assign any observation to two people or will not drop any observations and the way they do that is by doing crossover between sequences of observations at random.",
                    "label": 0
                },
                {
                    "sent": "So without any informative prior distribution and therefore the sampling is quite inefficient.",
                    "label": 0
                },
                {
                    "sent": "Moreover, the parameters of the distributions are not as well.",
                    "label": 0
                },
                {
                    "sent": "They learn by maximum likelihood, so they're more likely to overfit and they do not model any camera specific parameters in their model.",
                    "label": 0
                },
                {
                    "sent": "I'm so the next set of experiments that we did was to try to see whether this fancy model of the camera specific parameters and added in addition to the person specific parameters actually bought us anything.",
                    "label": 0
                },
                {
                    "sent": "And we will see that indeed they do.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we see all of our results nicely summarized in one page on the right for me on the left for you, and we have a comparative results with the state of the art.",
                    "label": 0
                },
                {
                    "sent": "And we see that for different for our datasets an with five or ten subjects, we consistently significantly outperform the baseline.",
                    "label": 0
                },
                {
                    "sent": "And we also see that the computational complexity of the original Pezula sampler is indeed quadratic and actually quadratic in the number of people with exponential in the number of of observations.",
                    "label": 0
                },
                {
                    "sent": "So this is actually.",
                    "label": 0
                },
                {
                    "sent": "Not tractable, and while our skills much better and then when we compare our fully Bayesian methods with the fancy modeling of the camera specific parameters with a naive fully Bayesian model of only the person specific parameters, we still see that the fancy model significantly outperforms the naive model.",
                    "label": 0
                },
                {
                    "sent": "The last thing that we did was to say how are or was to check how our system scales when more data is gathered.",
                    "label": 0
                },
                {
                    "sent": "What we did was just cuts bits of the data where we had 200 frames, 16 frames and so on.",
                    "label": 0
                },
                {
                    "sent": "With fewer frames we have fewer people in the system and therefore things are easier.",
                    "label": 0
                },
                {
                    "sent": "But we also have less information about those people.",
                    "label": 0
                },
                {
                    "sent": "It turns out that actually this extra information that we get by having more frames actually improves the performance, and so that's a nice result because it shows that our approach actually scales with more data, which we're very happy about.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to summarize, we proposed an unsupervised methods for person trajectory identification in networks of non overlapping cameras where we explicitly model variations that are due to cameras and the location of cameras which explicitly model the cameras transitions and which deals with the high computational complexity of the full exact inference in the model.",
                    "label": 0
                },
                {
                    "sent": "We use the probabilistic graphical model to do this.",
                    "label": 1
                },
                {
                    "sent": "And we proposed an efficient way to do Gibbs sampling in this model, where we also we also provide closed form analytical updates for the parameters over appearance gain and illumination variance, and we show that this leads to significantly improved performance over the state of the art.",
                    "label": 1
                },
                {
                    "sent": "So with.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you, and I'm open for questions.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is something that particular sampler seems to do, though, is switch the identity of multiple time steps at the same time.",
                    "label": 0
                },
                {
                    "sent": "So I can imagine that Gibbs sampler gets stuck in a local minimum because.",
                    "label": 0
                },
                {
                    "sent": "You know you have to change the future in the past at the same time, or it won't.",
                    "label": 0
                },
                {
                    "sent": "It won't be.",
                    "label": 0
                },
                {
                    "sent": "It won't look as good, right?",
                    "label": 0
                },
                {
                    "sent": "That's a very good point.",
                    "label": 0
                },
                {
                    "sent": "That is true.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "It turns out that by using this fully Bayesian model and you learn specific distributions over the appearance of people slowly, and therefore you have, you basically get the sort of simulated annealing kind of thing where you still converge to a good optimum, despite the fact that absolutely.",
                    "label": 0
                },
                {
                    "sent": "By changing a single single person you know you may have that you get into into situation where changing second person is impossible while actually changing as a set of people who should be better.",
                    "label": 0
                },
                {
                    "sent": "And we did experiment a little bit with blog, Ipswich probably would have improved it as well, but it turned out not to make a big difference.",
                    "label": 0
                },
                {
                    "sent": "So have you considered solving the problem using integer programming rather than MCMC for optimization?",
                    "label": 0
                },
                {
                    "sent": "No, we didn't.",
                    "label": 0
                },
                {
                    "sent": "I don't quite see why you say that our data set is so small.",
                    "label": 0
                },
                {
                    "sent": "We do have lots of different trajectories and it says the number of trajectories that actually matters.",
                    "label": 0
                },
                {
                    "sent": "It's not so much the number of people, so it's true that we have few people, but we have.",
                    "label": 0
                },
                {
                    "sent": "I don't remember the exact numbers, but tends close to hundreds of different trajectories and so the number of possible combinations is actually exponential in that.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}