{
    "id": "6dyi34ngslc6wz5nlafcmge4k263gqx6",
    "title": "Spotlights 4",
    "info": {
        "published": "Dec. 3, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/nips09_spotlights2/",
    "segmentation": [
        [
            "I kinda looks at the question of predicting where malicious traffic on the Internet will originate from current measurements and networking research indicates that malicious traffic tends to cluster in a way that aligns with the structure of the IP address space and from the machine learning perspective, the structure allows us to model a problem as one of adaptively learning.",
            "A good pruning of a decision tree over the IP address space, the IP address space may be naturally interpreted as a binary tree.",
            "A very large one, IPV 4 Cree has 2 to 32 leaves.",
            "The IPV 6 Street is 2 to the 128 leaves.",
            "And our problem is to predict the labels on the individual IP addresses at the leaves of this tree.",
            "And we have a number of additional challenges that arise in our context, such as a changing target function.",
            "Since malicious activity is dynamic and can change its origin overtime and severe space requirements in our work, we design online learning algorithms that can address these challenges by combining a number of experts algorithms.",
            "Anna Tree based version of paging we prove guarantees in our algorithms predictions as a function of the best pruning of a similar size and we show experimental results on email datasets with over 100 million.",
            "Addresses and we can also generate Maps that show the Internet's dynamic malicious activity.",
            "So come see our poster at W. 47, thank you."
        ],
        [
            "OK, next is skill discovery in continuous reinforcement learning domains using skill chaining.",
            "Hi everyone, so this research is about developing reinforcement learning agents that we can drop into new continuous domains and have them acquire their own high level macro actions or skills autonomously without any prior knowledge of the domain.",
            "So this is accomplished in this paper by building a skill tree and what that means is that from anywhere in the domain, the agent can chain together a sequence of learn skills to go from where it is to the goal.",
            "So in the upper right corner of my slide you'll see a sample picture from the paper where there is a solution trajectory and it's broken up into skills by color.",
            "It turns out that when we do this, we can gain both initial initial performance improvements in learning, but also we were able to represent and obtain far better or far superior policies.",
            "And this occurs because we're taking a large complex problem.",
            "We're breaking it up into smaller problems.",
            "We're solving them more precisely because we represent them independently, and then we're training them back together.",
            "So if that sounds interesting, it would be great to meet you downstairs.",
            "I'm at post W. 39.",
            "Thank you."
        ],
        [
            "OK, next we have efficient match kernel between sets of features for visual recognition.",
            "So investor recognition, the images are frequently modeled as set of local features.",
            "We show that the combination or bagger words plus linear classifier can be viewed as a special match kernel.",
            "Which comes one if two local feature for into the same region partitioned by visual words and there's no otherwise, but this conversation is too 'cause it can be better to design match kernels that more accurately measure the similarity between local features.",
            "However, these are impractical to use in large datasets due to the computational cost.",
            "In this work, we propose efficient match kernels to learn low dimension features so the inner products.",
            "Preserves the original kernel values.",
            "We present two methods.",
            "One based on Lambda low dimensional projection that applies to any kernel the other based on random projections.",
            "In this way only linear classifier are required.",
            "We show that the methodology matches the current state of the art in three challenging image data science, some 15 Caltech 101 and Caltech 256.",
            "If you are interested, please.",
            "Come by our post W 22 thank you."
        ],
        [
            "OK, the next spotlight is orthogonal matching pursuit from noisy random measurements and new analysis.",
            "Orthogonal matching pursuit is a widely used greedy algorithm for finding sparse, approximate solutions to sets of linear equations.",
            "It has been recently applied to detect sparse signals from their randomized measurements as in compressed sensing.",
            "Although orthogonal matching pursuit performs very well in practice and it is quite simple to implement, performance is very difficult to analyze to characterize analytically due to the inherent.",
            "Nonlinear nonlinearities in the algorithm.",
            "Previous work by trapping.",
            "Gilbert had showed that the number of measurements needed to recover a sparse signal from it's not from its noise.",
            "Free measurements was 2K log NR result cuts that by half.",
            "Additionally we allow for will offer not to not allow for lack of knowledge and K at the beginning.",
            "So usually it's fine.",
            "There was also known K. Furthermore, our result also provides.",
            "Also provides a single sufficient condition for support recovery of a sparse vector X under noisy random measurements, and we're excited about this because this OMP result matches the previous results of Lasso by Martin Wainwright.",
            "So this shows that under certain asymptotic regimes that Lasso and OMP match in behavior.",
            "Additionally, our analysis requires or shows that certain things in the algorithm become Brownian motion, and that might be of independent interest for.",
            "For analysis of algorithms such as Co. Samp, another iterative greedy algorithms."
        ],
        [
            "OK, next we have learning to hash with binary reconstructive embeddings.",
            "This paper looks at scaling nearest neighbor searches to very excuse me.",
            "Very large datasets with a focus on using his little memory as possible to store the data.",
            "How many approaches for scaling the nearest neighbor problem and we focus on hashing techniques so these can be viewed as taking the data and embedding it into a low dimensional binary.",
            "Embedding there are several existing methods like locality sensitive hashing, semantic hashing, spectral hashing, and others.",
            "Our contribution is a new parameterisation for constructing for hash functions along with a new objective for measuring the quality of a hashing embedding.",
            "Then we analyze this and come up with a coordinate descent scheme that efficiently learns or constructs the hash functions.",
            "There's no assumptions that are required about the data, and it can be applied in a variety of settings.",
            "So For more information, please come to the poster, thank you."
        ],
        [
            "The next spotlight is perceptual multistability as Markov chain Monte Carlo inference.",
            "So on the left here you see the Necker cube which is the line drawing of a cube whose 3 dimensional depth configuration is ambiguous.",
            "So two different configurations are equally possible.",
            "Given the sensory information, this results in a multistable percept that rapidly alternates between the different configurations.",
            "Now the standard Bayesian story about ambiguous images is that the posterior over image interpretations is multimodal, and the different rivalrous percepts correspond to different modes of the posterior.",
            "However, this can't be the whole story because it doesn't explain.",
            "Why perception actually alternates stands for that.",
            "You have to look to algorithm.",
            "Our proposal is that humans approximate posterior with a set of samples generated from a sequential sampling process, namely a Markov chain Monte Carlo algorithm operating on a simple image model.",
            "Under this account, multistability arises from random walk on the posterior energy surface that bounces between the different modes.",
            "This idea can actually count for a number of psycho physical phenomena and.",
            "Opens a new page in this emerging emerging issue of what sort of algorithms humans use to approximate Bayesian inference.",
            "Visit us at poster W 36."
        ],
        [
            "OK, next we have information theoretic lower bounds on the Oracle complexity of convex optimization.",
            "So as the title suggests, this paper is about the hardness of convex optimization and the motivation to do this comes from trying to understand the computational hardness of statistical estimation.",
            "So for specific learning algorithms for several learning problems, it's rather well understood what is the amount of sufficient computation to achieve a certain test error epsilon, but there is little understanding of what is the minimum computation that any method from a broad class of learning method.",
            "Will have to perform to achieve the test error of epsilon.",
            "In our work.",
            "We argued that if we are trying to minimize the convex loss function under a distribution where we have access to only samples, then this is easily seen as a stochastic convex optimization problem and hence it suffices to study the computational hardness of stochastic convex optimization.",
            "We formulate an article model of complexity in which the optimization algorithm makes repeated queries to an Oracle and each query reveals some information about the function being optimized.",
            "We are able to lower bound the number of queries that any optimization method in this model will need to make to the article, and the bounds can be shown to be minimax optimal.",
            "For more details and implications to statistical estimation and variety of other things, please come to our poster at W 49."
        ],
        [
            "The next spotlight is hierarchical learning of dimensional biases in human categorisation.",
            "Given only a few examples of objects from a category, people have strong expectations about which other objects also belong to that category.",
            "People generally expect that objects with similar observed properties belong to the same category, and that expectation decreases with Euclidean distance.",
            "But people also tend to generalize along particular dimensions.",
            "For example, objects in the categories would or ice can come in a wide range of sizes.",
            "These dimensions are called separable dimensions, and his children develop into adults.",
            "They tend to generalize along separable dimensions more.",
            "And rely on Euclidian distance less.",
            "We model categories with the Dursley process mixture model and the learning of dimensional bias by learning the Dursley process based distribution, we show that we can correctly model children's learning of dimensional bias, and unlike previous models for categorization, we can learn the basis for separable dimensions to find out more, come to our poster 86 tonight, thanks."
        ],
        [
            "Next we have lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness.",
            "So sparse additive models I generalization of sparse linear models.",
            "The response denoted here as Y depends on each of the features or covariates through some possibly nonlinear transformation function.",
            "Each of these transformation functions lies in some smooth space, and many of them are zero, but you don't know which ones which enforces the sparsity constraint.",
            "So given this this model, we derive our statistical lower bounds for function estimation.",
            "So that is we find the minimum lower bound for the minimum number of samples that are required to reliably estimate the function given that it lies given that it takes the form of a spice additive model.",
            "So to prove lower bounds we use an information theoretic approach involving Fano's inequality.",
            "Come and the resulting lower bounds capture the intuition that estimating sparse additive models decomposed into two well known subproblems.",
            "The sparse recovery or subset selection problem, and then low dimensional nonparametric regression problem.",
            "So to find out more about how this is done, please come to our poster at W. 50, thank you."
        ],
        [
            "The next ballot is neural implementation of hierarchical Bayesian inference by important sampling.",
            "So the human brain can solve many problems which still pose a great challenge for modern computers such as pattern recognization in natural images, and many of those problems can be can be interpreted in terms of basic inference.",
            "Recent study in neuroscience and psychologists are just many human behaviors is often consistent with basic inference which make it critical for us to understand.",
            "How the nervous system can carry out such extensive computations?",
            "In particular, many perception tasks involves multiple layers of abstractions.",
            "This results in a hierarchical structure for the perception tasks in our model.",
            "In this work, we first introduce a simple neural networks that implement important sampling that we show.",
            "We can use these circuits as building block to construct multiple layer neural network.",
            "So perform hierarchical base and this approach produce results that highly consistent with human behaviors in a variety of perceptual tasks such as Q combination and the public effects in orientation detection.",
            "So welcome to our post W studies one, thank you."
        ],
        [
            "Next is posterior versus parameter sparsity in latent variable models.",
            "This work is about sparsity.",
            "In supervised learning of grammatical structure, specifically power, speech, induction.",
            "Open class grammatical types like noun and verb can be assigned to many words.",
            "On the other hand, the type ability of most words is limited.",
            "In this work, we propose a novel method to ensure this kind of sparsity induced model.",
            "Previous studies have tried to solve this problem by imposing parameter sparsity, but our experiments show that parameter sparsity hurts the accuracy of the induced models.",
            "Our approach enforces group sparsity directly on the distribution of types assigned towards.",
            "We do this by penalizing ambiguous posterior distributions using the posterior regularization framework.",
            "This kind of sparsity leads to significant gains in performance across several languages and scenarios.",
            "The distributions our approach produces have significant larger mutual information with the true types.",
            "Most important, these distributions are in fact useful.",
            "In a semi supervised setting where use has features for a supervised learner, come to our poster double 64 to learn how to Curb Your ambiguity in the right way, thanks."
        ],
        [
            "The next spotlight is structural inference affects a depth perception in the context of potential occlusion.",
            "So in the past few decades, there's been a bit of evidence that people seem to combine perceptual information in a way that's pretty close to statistically optimal.",
            "So if you have noisy visual cue and noisy auditory cue, you combine them linearly, weighted by the precision of each of those cues, and more recently, people have been working on these causal inference models where you allow for the possibility that the information shouldn't be combined, that they're from 2 separate causes.",
            "And what we present here is a new ideal observer model for ordinal information.",
            "So where one of the causes is strictly or greater than the other and it also has this feature like causal inference where it's a mixture model.",
            "So you allow for the possibility that you should really be estimating them independently, because they're from 2 separate causes.",
            "And this has this nice relationship to depth perception, where if you have a background queue, it really puts a constraint on your estimate of the some object that's closer to you.",
            "So we use this model to fit.",
            "Some old psychophysics experiments, and we also do a new experiment where we vary the uncertainty that.",
            "People have on these objects and see how it affects their their perception.",
            "So if you're interested, I hope you'll come by where it W 24.",
            "OK."
        ],
        [
            "The final spotlight is by linear classifiers for visual recognition.",
            "So we're motivated by approaches to object detection in which you scan over an image and consider for each possible window whether it contains an object and typically score such a window with a linear classifier.",
            "Now, often when you look at the features you extract from that window, they have some interesting sort of spatial structure.",
            "So, for example, you'll divide the window up into blocks, and for each block, extract a histogram of orientations and then score your classifier against those.",
            "But when you.",
            "Say training SVM.",
            "You simply string out these features into a big long vector in sort of throw away this interesting spatial structure.",
            "So we basically are motivated by sort of two simple ideas here.",
            "So one is that we should keep around some of that structure, that is, say, keep that feature not as a vector, but in some matrix, and then once we've done that, we also think of our template, the W, that we're trying to learn.",
            "Also is a matrix, and this leads to the second idea, which is that we can think about constraining the rank of that W matrix.",
            "In learning so rather than writing W is 1 big matrix, we factor it into a two components which have inner dimensions that are smaller in order to constrain the rank.",
            "Now when we look at this in SVM training, we now have problem which has these two different components, so it's no longer convex.",
            "But if we fix one we end up with an SVM problem in the remaining.",
            "So it's sort of a biconvex problem, which we can think of optimizing and fashion by just alternating.",
            "And so this has a few advantages.",
            "One is that this rank acts is some sort of regularization on the problem.",
            "The second is that we can use this as a way to sort of share information between problems by sharing a subspace between those problems, and in practice it gives us a big speedup because we're sort of reducing the size of that feature space.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I kinda looks at the question of predicting where malicious traffic on the Internet will originate from current measurements and networking research indicates that malicious traffic tends to cluster in a way that aligns with the structure of the IP address space and from the machine learning perspective, the structure allows us to model a problem as one of adaptively learning.",
                    "label": 1
                },
                {
                    "sent": "A good pruning of a decision tree over the IP address space, the IP address space may be naturally interpreted as a binary tree.",
                    "label": 1
                },
                {
                    "sent": "A very large one, IPV 4 Cree has 2 to 32 leaves.",
                    "label": 0
                },
                {
                    "sent": "The IPV 6 Street is 2 to the 128 leaves.",
                    "label": 0
                },
                {
                    "sent": "And our problem is to predict the labels on the individual IP addresses at the leaves of this tree.",
                    "label": 0
                },
                {
                    "sent": "And we have a number of additional challenges that arise in our context, such as a changing target function.",
                    "label": 0
                },
                {
                    "sent": "Since malicious activity is dynamic and can change its origin overtime and severe space requirements in our work, we design online learning algorithms that can address these challenges by combining a number of experts algorithms.",
                    "label": 0
                },
                {
                    "sent": "Anna Tree based version of paging we prove guarantees in our algorithms predictions as a function of the best pruning of a similar size and we show experimental results on email datasets with over 100 million.",
                    "label": 0
                },
                {
                    "sent": "Addresses and we can also generate Maps that show the Internet's dynamic malicious activity.",
                    "label": 0
                },
                {
                    "sent": "So come see our poster at W. 47, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, next is skill discovery in continuous reinforcement learning domains using skill chaining.",
                    "label": 1
                },
                {
                    "sent": "Hi everyone, so this research is about developing reinforcement learning agents that we can drop into new continuous domains and have them acquire their own high level macro actions or skills autonomously without any prior knowledge of the domain.",
                    "label": 0
                },
                {
                    "sent": "So this is accomplished in this paper by building a skill tree and what that means is that from anywhere in the domain, the agent can chain together a sequence of learn skills to go from where it is to the goal.",
                    "label": 0
                },
                {
                    "sent": "So in the upper right corner of my slide you'll see a sample picture from the paper where there is a solution trajectory and it's broken up into skills by color.",
                    "label": 0
                },
                {
                    "sent": "It turns out that when we do this, we can gain both initial initial performance improvements in learning, but also we were able to represent and obtain far better or far superior policies.",
                    "label": 0
                },
                {
                    "sent": "And this occurs because we're taking a large complex problem.",
                    "label": 0
                },
                {
                    "sent": "We're breaking it up into smaller problems.",
                    "label": 0
                },
                {
                    "sent": "We're solving them more precisely because we represent them independently, and then we're training them back together.",
                    "label": 0
                },
                {
                    "sent": "So if that sounds interesting, it would be great to meet you downstairs.",
                    "label": 0
                },
                {
                    "sent": "I'm at post W. 39.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, next we have efficient match kernel between sets of features for visual recognition.",
                    "label": 1
                },
                {
                    "sent": "So investor recognition, the images are frequently modeled as set of local features.",
                    "label": 1
                },
                {
                    "sent": "We show that the combination or bagger words plus linear classifier can be viewed as a special match kernel.",
                    "label": 0
                },
                {
                    "sent": "Which comes one if two local feature for into the same region partitioned by visual words and there's no otherwise, but this conversation is too 'cause it can be better to design match kernels that more accurately measure the similarity between local features.",
                    "label": 0
                },
                {
                    "sent": "However, these are impractical to use in large datasets due to the computational cost.",
                    "label": 0
                },
                {
                    "sent": "In this work, we propose efficient match kernels to learn low dimension features so the inner products.",
                    "label": 0
                },
                {
                    "sent": "Preserves the original kernel values.",
                    "label": 0
                },
                {
                    "sent": "We present two methods.",
                    "label": 0
                },
                {
                    "sent": "One based on Lambda low dimensional projection that applies to any kernel the other based on random projections.",
                    "label": 0
                },
                {
                    "sent": "In this way only linear classifier are required.",
                    "label": 0
                },
                {
                    "sent": "We show that the methodology matches the current state of the art in three challenging image data science, some 15 Caltech 101 and Caltech 256.",
                    "label": 0
                },
                {
                    "sent": "If you are interested, please.",
                    "label": 0
                },
                {
                    "sent": "Come by our post W 22 thank you.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, the next spotlight is orthogonal matching pursuit from noisy random measurements and new analysis.",
                    "label": 1
                },
                {
                    "sent": "Orthogonal matching pursuit is a widely used greedy algorithm for finding sparse, approximate solutions to sets of linear equations.",
                    "label": 0
                },
                {
                    "sent": "It has been recently applied to detect sparse signals from their randomized measurements as in compressed sensing.",
                    "label": 0
                },
                {
                    "sent": "Although orthogonal matching pursuit performs very well in practice and it is quite simple to implement, performance is very difficult to analyze to characterize analytically due to the inherent.",
                    "label": 0
                },
                {
                    "sent": "Nonlinear nonlinearities in the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Previous work by trapping.",
                    "label": 0
                },
                {
                    "sent": "Gilbert had showed that the number of measurements needed to recover a sparse signal from it's not from its noise.",
                    "label": 0
                },
                {
                    "sent": "Free measurements was 2K log NR result cuts that by half.",
                    "label": 0
                },
                {
                    "sent": "Additionally we allow for will offer not to not allow for lack of knowledge and K at the beginning.",
                    "label": 0
                },
                {
                    "sent": "So usually it's fine.",
                    "label": 0
                },
                {
                    "sent": "There was also known K. Furthermore, our result also provides.",
                    "label": 0
                },
                {
                    "sent": "Also provides a single sufficient condition for support recovery of a sparse vector X under noisy random measurements, and we're excited about this because this OMP result matches the previous results of Lasso by Martin Wainwright.",
                    "label": 1
                },
                {
                    "sent": "So this shows that under certain asymptotic regimes that Lasso and OMP match in behavior.",
                    "label": 0
                },
                {
                    "sent": "Additionally, our analysis requires or shows that certain things in the algorithm become Brownian motion, and that might be of independent interest for.",
                    "label": 0
                },
                {
                    "sent": "For analysis of algorithms such as Co. Samp, another iterative greedy algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, next we have learning to hash with binary reconstructive embeddings.",
                    "label": 1
                },
                {
                    "sent": "This paper looks at scaling nearest neighbor searches to very excuse me.",
                    "label": 0
                },
                {
                    "sent": "Very large datasets with a focus on using his little memory as possible to store the data.",
                    "label": 0
                },
                {
                    "sent": "How many approaches for scaling the nearest neighbor problem and we focus on hashing techniques so these can be viewed as taking the data and embedding it into a low dimensional binary.",
                    "label": 1
                },
                {
                    "sent": "Embedding there are several existing methods like locality sensitive hashing, semantic hashing, spectral hashing, and others.",
                    "label": 0
                },
                {
                    "sent": "Our contribution is a new parameterisation for constructing for hash functions along with a new objective for measuring the quality of a hashing embedding.",
                    "label": 1
                },
                {
                    "sent": "Then we analyze this and come up with a coordinate descent scheme that efficiently learns or constructs the hash functions.",
                    "label": 0
                },
                {
                    "sent": "There's no assumptions that are required about the data, and it can be applied in a variety of settings.",
                    "label": 0
                },
                {
                    "sent": "So For more information, please come to the poster, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next spotlight is perceptual multistability as Markov chain Monte Carlo inference.",
                    "label": 0
                },
                {
                    "sent": "So on the left here you see the Necker cube which is the line drawing of a cube whose 3 dimensional depth configuration is ambiguous.",
                    "label": 0
                },
                {
                    "sent": "So two different configurations are equally possible.",
                    "label": 0
                },
                {
                    "sent": "Given the sensory information, this results in a multistable percept that rapidly alternates between the different configurations.",
                    "label": 0
                },
                {
                    "sent": "Now the standard Bayesian story about ambiguous images is that the posterior over image interpretations is multimodal, and the different rivalrous percepts correspond to different modes of the posterior.",
                    "label": 0
                },
                {
                    "sent": "However, this can't be the whole story because it doesn't explain.",
                    "label": 0
                },
                {
                    "sent": "Why perception actually alternates stands for that.",
                    "label": 0
                },
                {
                    "sent": "You have to look to algorithm.",
                    "label": 0
                },
                {
                    "sent": "Our proposal is that humans approximate posterior with a set of samples generated from a sequential sampling process, namely a Markov chain Monte Carlo algorithm operating on a simple image model.",
                    "label": 0
                },
                {
                    "sent": "Under this account, multistability arises from random walk on the posterior energy surface that bounces between the different modes.",
                    "label": 0
                },
                {
                    "sent": "This idea can actually count for a number of psycho physical phenomena and.",
                    "label": 0
                },
                {
                    "sent": "Opens a new page in this emerging emerging issue of what sort of algorithms humans use to approximate Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "Visit us at poster W 36.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, next we have information theoretic lower bounds on the Oracle complexity of convex optimization.",
                    "label": 0
                },
                {
                    "sent": "So as the title suggests, this paper is about the hardness of convex optimization and the motivation to do this comes from trying to understand the computational hardness of statistical estimation.",
                    "label": 0
                },
                {
                    "sent": "So for specific learning algorithms for several learning problems, it's rather well understood what is the amount of sufficient computation to achieve a certain test error epsilon, but there is little understanding of what is the minimum computation that any method from a broad class of learning method.",
                    "label": 0
                },
                {
                    "sent": "Will have to perform to achieve the test error of epsilon.",
                    "label": 0
                },
                {
                    "sent": "In our work.",
                    "label": 0
                },
                {
                    "sent": "We argued that if we are trying to minimize the convex loss function under a distribution where we have access to only samples, then this is easily seen as a stochastic convex optimization problem and hence it suffices to study the computational hardness of stochastic convex optimization.",
                    "label": 0
                },
                {
                    "sent": "We formulate an article model of complexity in which the optimization algorithm makes repeated queries to an Oracle and each query reveals some information about the function being optimized.",
                    "label": 0
                },
                {
                    "sent": "We are able to lower bound the number of queries that any optimization method in this model will need to make to the article, and the bounds can be shown to be minimax optimal.",
                    "label": 0
                },
                {
                    "sent": "For more details and implications to statistical estimation and variety of other things, please come to our poster at W 49.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next spotlight is hierarchical learning of dimensional biases in human categorisation.",
                    "label": 1
                },
                {
                    "sent": "Given only a few examples of objects from a category, people have strong expectations about which other objects also belong to that category.",
                    "label": 0
                },
                {
                    "sent": "People generally expect that objects with similar observed properties belong to the same category, and that expectation decreases with Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "But people also tend to generalize along particular dimensions.",
                    "label": 0
                },
                {
                    "sent": "For example, objects in the categories would or ice can come in a wide range of sizes.",
                    "label": 0
                },
                {
                    "sent": "These dimensions are called separable dimensions, and his children develop into adults.",
                    "label": 0
                },
                {
                    "sent": "They tend to generalize along separable dimensions more.",
                    "label": 0
                },
                {
                    "sent": "And rely on Euclidian distance less.",
                    "label": 0
                },
                {
                    "sent": "We model categories with the Dursley process mixture model and the learning of dimensional bias by learning the Dursley process based distribution, we show that we can correctly model children's learning of dimensional bias, and unlike previous models for categorization, we can learn the basis for separable dimensions to find out more, come to our poster 86 tonight, thanks.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next we have lower bounds on minimax rates for nonparametric regression with additive sparsity and smoothness.",
                    "label": 1
                },
                {
                    "sent": "So sparse additive models I generalization of sparse linear models.",
                    "label": 0
                },
                {
                    "sent": "The response denoted here as Y depends on each of the features or covariates through some possibly nonlinear transformation function.",
                    "label": 0
                },
                {
                    "sent": "Each of these transformation functions lies in some smooth space, and many of them are zero, but you don't know which ones which enforces the sparsity constraint.",
                    "label": 0
                },
                {
                    "sent": "So given this this model, we derive our statistical lower bounds for function estimation.",
                    "label": 0
                },
                {
                    "sent": "So that is we find the minimum lower bound for the minimum number of samples that are required to reliably estimate the function given that it lies given that it takes the form of a spice additive model.",
                    "label": 0
                },
                {
                    "sent": "So to prove lower bounds we use an information theoretic approach involving Fano's inequality.",
                    "label": 0
                },
                {
                    "sent": "Come and the resulting lower bounds capture the intuition that estimating sparse additive models decomposed into two well known subproblems.",
                    "label": 0
                },
                {
                    "sent": "The sparse recovery or subset selection problem, and then low dimensional nonparametric regression problem.",
                    "label": 0
                },
                {
                    "sent": "So to find out more about how this is done, please come to our poster at W. 50, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next ballot is neural implementation of hierarchical Bayesian inference by important sampling.",
                    "label": 1
                },
                {
                    "sent": "So the human brain can solve many problems which still pose a great challenge for modern computers such as pattern recognization in natural images, and many of those problems can be can be interpreted in terms of basic inference.",
                    "label": 0
                },
                {
                    "sent": "Recent study in neuroscience and psychologists are just many human behaviors is often consistent with basic inference which make it critical for us to understand.",
                    "label": 0
                },
                {
                    "sent": "How the nervous system can carry out such extensive computations?",
                    "label": 0
                },
                {
                    "sent": "In particular, many perception tasks involves multiple layers of abstractions.",
                    "label": 0
                },
                {
                    "sent": "This results in a hierarchical structure for the perception tasks in our model.",
                    "label": 0
                },
                {
                    "sent": "In this work, we first introduce a simple neural networks that implement important sampling that we show.",
                    "label": 0
                },
                {
                    "sent": "We can use these circuits as building block to construct multiple layer neural network.",
                    "label": 0
                },
                {
                    "sent": "So perform hierarchical base and this approach produce results that highly consistent with human behaviors in a variety of perceptual tasks such as Q combination and the public effects in orientation detection.",
                    "label": 0
                },
                {
                    "sent": "So welcome to our post W studies one, thank you.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next is posterior versus parameter sparsity in latent variable models.",
                    "label": 0
                },
                {
                    "sent": "This work is about sparsity.",
                    "label": 0
                },
                {
                    "sent": "In supervised learning of grammatical structure, specifically power, speech, induction.",
                    "label": 0
                },
                {
                    "sent": "Open class grammatical types like noun and verb can be assigned to many words.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, the type ability of most words is limited.",
                    "label": 0
                },
                {
                    "sent": "In this work, we propose a novel method to ensure this kind of sparsity induced model.",
                    "label": 0
                },
                {
                    "sent": "Previous studies have tried to solve this problem by imposing parameter sparsity, but our experiments show that parameter sparsity hurts the accuracy of the induced models.",
                    "label": 0
                },
                {
                    "sent": "Our approach enforces group sparsity directly on the distribution of types assigned towards.",
                    "label": 0
                },
                {
                    "sent": "We do this by penalizing ambiguous posterior distributions using the posterior regularization framework.",
                    "label": 0
                },
                {
                    "sent": "This kind of sparsity leads to significant gains in performance across several languages and scenarios.",
                    "label": 0
                },
                {
                    "sent": "The distributions our approach produces have significant larger mutual information with the true types.",
                    "label": 0
                },
                {
                    "sent": "Most important, these distributions are in fact useful.",
                    "label": 0
                },
                {
                    "sent": "In a semi supervised setting where use has features for a supervised learner, come to our poster double 64 to learn how to Curb Your ambiguity in the right way, thanks.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next spotlight is structural inference affects a depth perception in the context of potential occlusion.",
                    "label": 1
                },
                {
                    "sent": "So in the past few decades, there's been a bit of evidence that people seem to combine perceptual information in a way that's pretty close to statistically optimal.",
                    "label": 0
                },
                {
                    "sent": "So if you have noisy visual cue and noisy auditory cue, you combine them linearly, weighted by the precision of each of those cues, and more recently, people have been working on these causal inference models where you allow for the possibility that the information shouldn't be combined, that they're from 2 separate causes.",
                    "label": 0
                },
                {
                    "sent": "And what we present here is a new ideal observer model for ordinal information.",
                    "label": 0
                },
                {
                    "sent": "So where one of the causes is strictly or greater than the other and it also has this feature like causal inference where it's a mixture model.",
                    "label": 0
                },
                {
                    "sent": "So you allow for the possibility that you should really be estimating them independently, because they're from 2 separate causes.",
                    "label": 0
                },
                {
                    "sent": "And this has this nice relationship to depth perception, where if you have a background queue, it really puts a constraint on your estimate of the some object that's closer to you.",
                    "label": 0
                },
                {
                    "sent": "So we use this model to fit.",
                    "label": 0
                },
                {
                    "sent": "Some old psychophysics experiments, and we also do a new experiment where we vary the uncertainty that.",
                    "label": 0
                },
                {
                    "sent": "People have on these objects and see how it affects their their perception.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested, I hope you'll come by where it W 24.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The final spotlight is by linear classifiers for visual recognition.",
                    "label": 1
                },
                {
                    "sent": "So we're motivated by approaches to object detection in which you scan over an image and consider for each possible window whether it contains an object and typically score such a window with a linear classifier.",
                    "label": 0
                },
                {
                    "sent": "Now, often when you look at the features you extract from that window, they have some interesting sort of spatial structure.",
                    "label": 0
                },
                {
                    "sent": "So, for example, you'll divide the window up into blocks, and for each block, extract a histogram of orientations and then score your classifier against those.",
                    "label": 0
                },
                {
                    "sent": "But when you.",
                    "label": 0
                },
                {
                    "sent": "Say training SVM.",
                    "label": 0
                },
                {
                    "sent": "You simply string out these features into a big long vector in sort of throw away this interesting spatial structure.",
                    "label": 0
                },
                {
                    "sent": "So we basically are motivated by sort of two simple ideas here.",
                    "label": 0
                },
                {
                    "sent": "So one is that we should keep around some of that structure, that is, say, keep that feature not as a vector, but in some matrix, and then once we've done that, we also think of our template, the W, that we're trying to learn.",
                    "label": 0
                },
                {
                    "sent": "Also is a matrix, and this leads to the second idea, which is that we can think about constraining the rank of that W matrix.",
                    "label": 0
                },
                {
                    "sent": "In learning so rather than writing W is 1 big matrix, we factor it into a two components which have inner dimensions that are smaller in order to constrain the rank.",
                    "label": 0
                },
                {
                    "sent": "Now when we look at this in SVM training, we now have problem which has these two different components, so it's no longer convex.",
                    "label": 0
                },
                {
                    "sent": "But if we fix one we end up with an SVM problem in the remaining.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of a biconvex problem, which we can think of optimizing and fashion by just alternating.",
                    "label": 0
                },
                {
                    "sent": "And so this has a few advantages.",
                    "label": 0
                },
                {
                    "sent": "One is that this rank acts is some sort of regularization on the problem.",
                    "label": 0
                },
                {
                    "sent": "The second is that we can use this as a way to sort of share information between problems by sharing a subspace between those problems, and in practice it gives us a big speedup because we're sort of reducing the size of that feature space.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}