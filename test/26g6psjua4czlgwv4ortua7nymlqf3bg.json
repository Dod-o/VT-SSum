{
    "id": "26g6psjua4czlgwv4ortua7nymlqf3bg",
    "title": "Scalable Graph Clustering Using Stochastic Flows: Applications to Community Discovery",
    "info": {
        "author": [
            "Venu Satuluri, Ohio State University"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "July 2009",
        "category": [
            "Top->Computer Science->Web Mining"
        ]
    },
    "url": "http://videolectures.net/kdd09_satuluri_sgcusfacd/",
    "segmentation": [
        [
            "Good morning, thanks a lot.",
            "So I she just said I'm very sorry I'm a member of the data Mining Research Lab at the High State University.",
            "This is joint work with my advisor Srinivasan party Saturday."
        ],
        [
            "So just a quick outline of the talk I'll be introducing our problem statement and then I'll be discussing in somewhat more detail, particularly relevant piece of previous work, Markov clustering that will allow me to naturally segue into our own contributions, which are regularised MCL and multi level regularised and so then I'll talk about some experiments and some conclusions."
        ],
        [
            "So the problem statement is graph clustering.",
            "It's a longstanding classical problem we're given.",
            "On directed possibly weighted graph and we want to partition the vertices of the graph into into disjoint sets so that each partition is a coherent group.",
            "So this has a number of applications.",
            "I just list a few of these discovery of protein complexes from protein interaction networks and then discovering communities in social networks and segmenting images.",
            "So given the importance of the problem, it's not surprising that the number of solutions have been proposed.",
            "Two of the sort of classical methods are spectral methods and edge based agglomerative slash tools of methods.",
            "These give good results, especially spectral methods, which is very widely used, but the sort of the one limitation is that it tends to be not very scalable.",
            "The first 2 methods, so Dylan at all in 2007, proposed an equivalent, an algorithm that.",
            "It was shown to optimize an equivalent objective to spectral methods that work by weighted kernel K means that will be one of the algorithms that will be comparing against later and and the key feature of this algorithm is that it's much faster than the usual spectral methods.",
            "And then there's also metrics algorithm which is which is proposed around 12 years ago by Cara Pesan Weapon Kumar.",
            "And that's also one of the fastest algorithms for graph clustering around.",
            "And then there is Markov clustering which is proposed by Steinwand.",
            "And in 2000, and I'll be discussing that in more detail, so."
        ],
        [
            "This was the original algorithm for clustering graphs using stochastic flows.",
            "I'll be explaining in just a little bit how exactly the algorithm works, but the reason we chose this algorithm is that it's in fact very widely used in bioinformatics because it's been found to be very noise tolerant and very effective at at retreiving the right clusters and and so it has a very simple and elegant formulation, but on the other hand it's very slow.",
            "For example, it took around 1.2 hours to cluster 76,000 known social network and it's also prone to output too many clusters.",
            "So on a 4741 node, protein protein interaction network output around 1416 clusters, it's roughly around three nodes per cluster.",
            "So the goal of our work is to try to redress the disadvantages of Markov clustering while retaining its advantages."
        ],
        [
            "So just a brief bit of terminology we will call the transition probability from a node to another node in the graph, as the flow from that node to the other node, and we can.",
            "We can put all the flows from between all pairs of nodes in a matrix and we can do this just the usual Markov transition matrix.",
            "We call this flow matrix for conciseness and for intuition.",
            "We have a simple example graph of a simple line graph of three nodes.",
            "You can see that once you construct the transition probabilities, the nodes at the end flow with probability one to the node in the middle and the and the node in the middle flows with probability have to either of the nodes at the end and you can put that in a flow matrix and we put the flows out of out of each node in the in the column corresponding to that node.",
            "So the sum of each column is 1.",
            "The other thing to note about this matrix is not it need not be symmetric, even though the original matrix."
        ],
        [
            "Is symmetric.",
            "So the MCL algorithm?",
            "How does it work?",
            "It basically manipulates the flow matrix such that after a series of manipulations, the flow matrix represents the cluster structure of the of the underlying graph.",
            "So how does it do this?",
            "You initialize the flow matrix to what we call the Canonical transition matrix or the Canonical flow matrix, MG, which is simply the normalized column normalized adjacency matrix.",
            "We add the identity matrix to avoid certain pathological conditions, and then there are these two operations.",
            "The expand and inflate operation which which are which are applied in automation until convergence.",
            "What they expand operation does is it simply computes the final distribution of a random walk of length 2 from each node, so that is in matrix notation.",
            "Simply M = M * M. So what this does is it enhances the flow too well connected nodes in the graph.",
            "As well as gives flow to new notes, excuse me.",
            "And the next operation is the inflate operation, which increases the, which essentially, in case you didn't understand the math lab notation, it takes each entry in the matrix and raises it to the power R, which we call the inflation parameter.",
            "And what this?",
            "And then we re normalize each column to have someone.",
            "So what this essentially does is it increases the inequality in each column.",
            "So what we refer to as the rich getting richer and the poor getting poorer.",
            "Then we have this simple prune operation which would save memory by removing those entries close to 0 and we do this until convergence, so this very intuitive sort of algorithm turns out to have one disadvantage to disadvantage is the first of which will be discussing how to reduce first, so MCL outputs too many clusters.",
            "And why does this happen?",
            "We believe that it happens."
        ],
        [
            "Do over fitting, in particular because it does not explicitly penalize divergance of flows between neighbors.",
            "In other words, that the probability distributions that correspond to two neighbors in the original graph diverge a little too much, so our remedy is to explicitly penalized this divergent and flows between neighbors.",
            "So how do we?",
            "How do we express the penalty at each node?",
            "We say that the penalty at each node is simply the sum of the weighted KL divergences.",
            "From between that node and all of its neighbors.",
            "And if you remember, the MG Ji represents the flow in the original.",
            "The flow in the original graph from the node I to its neighbor J.",
            "So this has, uh, this we think is a pretty intuitive expression of the roughness at a particular node.",
            "So Fortunately for us, it turns out that this penalty has a simple closed form solution.",
            "It's simply the linear combination of the columns of of the probability distributions of each of your neighbors, the.",
            "The weights again turn out to be the flows to that particular to the particular neighbor from the original graph, so this particular update defines the regularize operator, and it can again be conveniently for us expressed in matrix notation as M times MG, where MG is again the Canonical flow matrix."
        ],
        [
            "So the regular is MCL algorithm is very simple.",
            "It's we simply replace the expand step from the MCL algorithm and replace it with the regularize step.",
            "So the regular step takes into account the flows of the neighbors as well when updating the flows of the node itself.",
            "So, but it turns out that this actually improves the cluster quality of the algorithm quite significantly, as we'll see later and then.",
            "But we still want to reduce the second disadvantage, which is that it might be slow, which is that the algorithm is slow, so we embed regularised."
        ],
        [
            "MCL in this multi level framework.",
            "So the basic idea behind the multi level framework is as follows.",
            "You're given this input graph which might be quite big and then you you perform this coarsening step to produce a smaller graph which is in which nevertheless represents the structure of the original graph.",
            "Somehow you continue doing this until you.",
            "Come up with a graph that is sufficiently small.",
            "Say a few 100 nodes or 1000 nodes depending on how how coarse grained or fine grained you want your final clustering to be.",
            "So once you have this course is graph you you run your MCL, you run your algorithm on the coarsest graph and then you use the state of the algorithm on the coarsest graph to initialize the state of the algorithm on the next bigger graph.",
            "You continue doing this until you arrive at the final graph and then you run our MCL to convergence and output the clusters.",
            "So one of the critical challenges here is how do you project the flow that preserves the sanctity of the algorithm.",
            "Unfortunately, the details are a little complicated, so I encourage you to read the paper if you're interested.",
            "And the advantages that we derive from using this framework that the course is running on the courses craft first helps us capture the global topology of the graph and the second benefit is that it's faster to run on the courses on the smaller on the smaller steps first.",
            "So let me."
        ],
        [
            "Go to the evaluation first.",
            "We compare with MCL and we here we show the results on for real world datasets.",
            "We've got some other results in the paper as well.",
            "The evaluation metric we use in the is the normalized cut metric, which is pretty, which is pretty standard quality metric for graph clustering.",
            "An lower the lower the metric, the better so you can, so the so the blue bars are represent MLR MCL, whereas the modern bars represent MCL.",
            "So you can see that in all four datasets there is quite a significant improvement in and cut scores, ANAN with respect to speed, MLR MCL is on average 96 times faster, and on the 76,000 Epinions graph, MLM sales runtime is 26 seconds compared to MCS 1.2 hours."
        ],
        [
            "We also compared with crack Listen Matters, Witcher 2, State of the art graph clustering algorithms.",
            "So this is the on the left.",
            "We have the clustering quality represented by the average and cut.",
            "We can see that the blue curve represents MLR MCL, whereas grad classes the red curve.",
            "We can see that UMSL dominates both crack listen matters quite significantly, and when it comes to speed, which is represented in the graph on the right, you can see that.",
            "MLRMCL is is competitive with metals.",
            "It's almost along the same trend and it's it's quite faster than reckless."
        ],
        [
            "So I also have some evaluation on PPI networks, but I think I'll try to leave some time for questions, so I encourage you to read the paper if you're interested in that."
        ],
        [
            "So to conclude.",
            "Regularised MCL overcomes the fragmentation problem of MCL and multi level regularised MCL further improves the quality and the speed of MCL and so in the end we have achieved an algorithm that often outperforms the state of the art algorithms, both quality and speed wise on a wide variety of real datasets.",
            "Future directions and number of people were mentioning to me yesterday.",
            "So with this work doesn't is still not.",
            "Perfectly applicable to directed graphs because the semantics of the directed graphs are slightly different, we'd like to extend this work to director and bipartite graphs, and we'd also like to look at novel coarsening strategies.",
            "We currently use the standard coarsening strategy that is used in both crank listen mentors.",
            "So I would like to acknowledge NSF for funding our work."
        ],
        [
            "These are differences I can take any questions.",
            "The next speaker right there is why questions.",
            "Did you compare this newer result?",
            "Resume data faster result.",
            "Some worker that works at the host community structures and well before and compare the effectiveness of both the algorithms on which well known networks did you mean?",
            "Right, right?",
            "True, so with respect to domain specific evaluation that we had, we did some domain specific evaluation on the biological networks, but we were primarily constantly trying to look at graphs that were of some reasonable size, so we did not.",
            "Actually, I think there's a characteristic clubs around 100 people or so, but yeah, that's a good idea.",
            "I think that's one thing I'll go ahead and review and actually do that.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good morning, thanks a lot.",
                    "label": 0
                },
                {
                    "sent": "So I she just said I'm very sorry I'm a member of the data Mining Research Lab at the High State University.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with my advisor Srinivasan party Saturday.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just a quick outline of the talk I'll be introducing our problem statement and then I'll be discussing in somewhat more detail, particularly relevant piece of previous work, Markov clustering that will allow me to naturally segue into our own contributions, which are regularised MCL and multi level regularised and so then I'll talk about some experiments and some conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem statement is graph clustering.",
                    "label": 1
                },
                {
                    "sent": "It's a longstanding classical problem we're given.",
                    "label": 0
                },
                {
                    "sent": "On directed possibly weighted graph and we want to partition the vertices of the graph into into disjoint sets so that each partition is a coherent group.",
                    "label": 1
                },
                {
                    "sent": "So this has a number of applications.",
                    "label": 1
                },
                {
                    "sent": "I just list a few of these discovery of protein complexes from protein interaction networks and then discovering communities in social networks and segmenting images.",
                    "label": 0
                },
                {
                    "sent": "So given the importance of the problem, it's not surprising that the number of solutions have been proposed.",
                    "label": 0
                },
                {
                    "sent": "Two of the sort of classical methods are spectral methods and edge based agglomerative slash tools of methods.",
                    "label": 0
                },
                {
                    "sent": "These give good results, especially spectral methods, which is very widely used, but the sort of the one limitation is that it tends to be not very scalable.",
                    "label": 0
                },
                {
                    "sent": "The first 2 methods, so Dylan at all in 2007, proposed an equivalent, an algorithm that.",
                    "label": 0
                },
                {
                    "sent": "It was shown to optimize an equivalent objective to spectral methods that work by weighted kernel K means that will be one of the algorithms that will be comparing against later and and the key feature of this algorithm is that it's much faster than the usual spectral methods.",
                    "label": 0
                },
                {
                    "sent": "And then there's also metrics algorithm which is which is proposed around 12 years ago by Cara Pesan Weapon Kumar.",
                    "label": 0
                },
                {
                    "sent": "And that's also one of the fastest algorithms for graph clustering around.",
                    "label": 0
                },
                {
                    "sent": "And then there is Markov clustering which is proposed by Steinwand.",
                    "label": 0
                },
                {
                    "sent": "And in 2000, and I'll be discussing that in more detail, so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This was the original algorithm for clustering graphs using stochastic flows.",
                    "label": 1
                },
                {
                    "sent": "I'll be explaining in just a little bit how exactly the algorithm works, but the reason we chose this algorithm is that it's in fact very widely used in bioinformatics because it's been found to be very noise tolerant and very effective at at retreiving the right clusters and and so it has a very simple and elegant formulation, but on the other hand it's very slow.",
                    "label": 0
                },
                {
                    "sent": "For example, it took around 1.2 hours to cluster 76,000 known social network and it's also prone to output too many clusters.",
                    "label": 1
                },
                {
                    "sent": "So on a 4741 node, protein protein interaction network output around 1416 clusters, it's roughly around three nodes per cluster.",
                    "label": 0
                },
                {
                    "sent": "So the goal of our work is to try to redress the disadvantages of Markov clustering while retaining its advantages.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just a brief bit of terminology we will call the transition probability from a node to another node in the graph, as the flow from that node to the other node, and we can.",
                    "label": 1
                },
                {
                    "sent": "We can put all the flows from between all pairs of nodes in a matrix and we can do this just the usual Markov transition matrix.",
                    "label": 1
                },
                {
                    "sent": "We call this flow matrix for conciseness and for intuition.",
                    "label": 0
                },
                {
                    "sent": "We have a simple example graph of a simple line graph of three nodes.",
                    "label": 0
                },
                {
                    "sent": "You can see that once you construct the transition probabilities, the nodes at the end flow with probability one to the node in the middle and the and the node in the middle flows with probability have to either of the nodes at the end and you can put that in a flow matrix and we put the flows out of out of each node in the in the column corresponding to that node.",
                    "label": 1
                },
                {
                    "sent": "So the sum of each column is 1.",
                    "label": 0
                },
                {
                    "sent": "The other thing to note about this matrix is not it need not be symmetric, even though the original matrix.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is symmetric.",
                    "label": 0
                },
                {
                    "sent": "So the MCL algorithm?",
                    "label": 0
                },
                {
                    "sent": "How does it work?",
                    "label": 0
                },
                {
                    "sent": "It basically manipulates the flow matrix such that after a series of manipulations, the flow matrix represents the cluster structure of the of the underlying graph.",
                    "label": 0
                },
                {
                    "sent": "So how does it do this?",
                    "label": 0
                },
                {
                    "sent": "You initialize the flow matrix to what we call the Canonical transition matrix or the Canonical flow matrix, MG, which is simply the normalized column normalized adjacency matrix.",
                    "label": 1
                },
                {
                    "sent": "We add the identity matrix to avoid certain pathological conditions, and then there are these two operations.",
                    "label": 0
                },
                {
                    "sent": "The expand and inflate operation which which are which are applied in automation until convergence.",
                    "label": 0
                },
                {
                    "sent": "What they expand operation does is it simply computes the final distribution of a random walk of length 2 from each node, so that is in matrix notation.",
                    "label": 0
                },
                {
                    "sent": "Simply M = M * M. So what this does is it enhances the flow too well connected nodes in the graph.",
                    "label": 0
                },
                {
                    "sent": "As well as gives flow to new notes, excuse me.",
                    "label": 1
                },
                {
                    "sent": "And the next operation is the inflate operation, which increases the, which essentially, in case you didn't understand the math lab notation, it takes each entry in the matrix and raises it to the power R, which we call the inflation parameter.",
                    "label": 0
                },
                {
                    "sent": "And what this?",
                    "label": 1
                },
                {
                    "sent": "And then we re normalize each column to have someone.",
                    "label": 1
                },
                {
                    "sent": "So what this essentially does is it increases the inequality in each column.",
                    "label": 0
                },
                {
                    "sent": "So what we refer to as the rich getting richer and the poor getting poorer.",
                    "label": 0
                },
                {
                    "sent": "Then we have this simple prune operation which would save memory by removing those entries close to 0 and we do this until convergence, so this very intuitive sort of algorithm turns out to have one disadvantage to disadvantage is the first of which will be discussing how to reduce first, so MCL outputs too many clusters.",
                    "label": 0
                },
                {
                    "sent": "And why does this happen?",
                    "label": 0
                },
                {
                    "sent": "We believe that it happens.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do over fitting, in particular because it does not explicitly penalize divergance of flows between neighbors.",
                    "label": 1
                },
                {
                    "sent": "In other words, that the probability distributions that correspond to two neighbors in the original graph diverge a little too much, so our remedy is to explicitly penalized this divergent and flows between neighbors.",
                    "label": 0
                },
                {
                    "sent": "So how do we?",
                    "label": 1
                },
                {
                    "sent": "How do we express the penalty at each node?",
                    "label": 0
                },
                {
                    "sent": "We say that the penalty at each node is simply the sum of the weighted KL divergences.",
                    "label": 0
                },
                {
                    "sent": "From between that node and all of its neighbors.",
                    "label": 0
                },
                {
                    "sent": "And if you remember, the MG Ji represents the flow in the original.",
                    "label": 0
                },
                {
                    "sent": "The flow in the original graph from the node I to its neighbor J.",
                    "label": 1
                },
                {
                    "sent": "So this has, uh, this we think is a pretty intuitive expression of the roughness at a particular node.",
                    "label": 0
                },
                {
                    "sent": "So Fortunately for us, it turns out that this penalty has a simple closed form solution.",
                    "label": 1
                },
                {
                    "sent": "It's simply the linear combination of the columns of of the probability distributions of each of your neighbors, the.",
                    "label": 0
                },
                {
                    "sent": "The weights again turn out to be the flows to that particular to the particular neighbor from the original graph, so this particular update defines the regularize operator, and it can again be conveniently for us expressed in matrix notation as M times MG, where MG is again the Canonical flow matrix.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the regular is MCL algorithm is very simple.",
                    "label": 0
                },
                {
                    "sent": "It's we simply replace the expand step from the MCL algorithm and replace it with the regularize step.",
                    "label": 0
                },
                {
                    "sent": "So the regular step takes into account the flows of the neighbors as well when updating the flows of the node itself.",
                    "label": 1
                },
                {
                    "sent": "So, but it turns out that this actually improves the cluster quality of the algorithm quite significantly, as we'll see later and then.",
                    "label": 0
                },
                {
                    "sent": "But we still want to reduce the second disadvantage, which is that it might be slow, which is that the algorithm is slow, so we embed regularised.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "MCL in this multi level framework.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea behind the multi level framework is as follows.",
                    "label": 0
                },
                {
                    "sent": "You're given this input graph which might be quite big and then you you perform this coarsening step to produce a smaller graph which is in which nevertheless represents the structure of the original graph.",
                    "label": 0
                },
                {
                    "sent": "Somehow you continue doing this until you.",
                    "label": 0
                },
                {
                    "sent": "Come up with a graph that is sufficiently small.",
                    "label": 0
                },
                {
                    "sent": "Say a few 100 nodes or 1000 nodes depending on how how coarse grained or fine grained you want your final clustering to be.",
                    "label": 0
                },
                {
                    "sent": "So once you have this course is graph you you run your MCL, you run your algorithm on the coarsest graph and then you use the state of the algorithm on the coarsest graph to initialize the state of the algorithm on the next bigger graph.",
                    "label": 0
                },
                {
                    "sent": "You continue doing this until you arrive at the final graph and then you run our MCL to convergence and output the clusters.",
                    "label": 0
                },
                {
                    "sent": "So one of the critical challenges here is how do you project the flow that preserves the sanctity of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, the details are a little complicated, so I encourage you to read the paper if you're interested.",
                    "label": 0
                },
                {
                    "sent": "And the advantages that we derive from using this framework that the course is running on the courses craft first helps us capture the global topology of the graph and the second benefit is that it's faster to run on the courses on the smaller on the smaller steps first.",
                    "label": 1
                },
                {
                    "sent": "So let me.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Go to the evaluation first.",
                    "label": 0
                },
                {
                    "sent": "We compare with MCL and we here we show the results on for real world datasets.",
                    "label": 0
                },
                {
                    "sent": "We've got some other results in the paper as well.",
                    "label": 0
                },
                {
                    "sent": "The evaluation metric we use in the is the normalized cut metric, which is pretty, which is pretty standard quality metric for graph clustering.",
                    "label": 0
                },
                {
                    "sent": "An lower the lower the metric, the better so you can, so the so the blue bars are represent MLR MCL, whereas the modern bars represent MCL.",
                    "label": 0
                },
                {
                    "sent": "So you can see that in all four datasets there is quite a significant improvement in and cut scores, ANAN with respect to speed, MLR MCL is on average 96 times faster, and on the 76,000 Epinions graph, MLM sales runtime is 26 seconds compared to MCS 1.2 hours.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also compared with crack Listen Matters, Witcher 2, State of the art graph clustering algorithms.",
                    "label": 0
                },
                {
                    "sent": "So this is the on the left.",
                    "label": 0
                },
                {
                    "sent": "We have the clustering quality represented by the average and cut.",
                    "label": 0
                },
                {
                    "sent": "We can see that the blue curve represents MLR MCL, whereas grad classes the red curve.",
                    "label": 0
                },
                {
                    "sent": "We can see that UMSL dominates both crack listen matters quite significantly, and when it comes to speed, which is represented in the graph on the right, you can see that.",
                    "label": 0
                },
                {
                    "sent": "MLRMCL is is competitive with metals.",
                    "label": 1
                },
                {
                    "sent": "It's almost along the same trend and it's it's quite faster than reckless.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I also have some evaluation on PPI networks, but I think I'll try to leave some time for questions, so I encourage you to read the paper if you're interested in that.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to conclude.",
                    "label": 0
                },
                {
                    "sent": "Regularised MCL overcomes the fragmentation problem of MCL and multi level regularised MCL further improves the quality and the speed of MCL and so in the end we have achieved an algorithm that often outperforms the state of the art algorithms, both quality and speed wise on a wide variety of real datasets.",
                    "label": 1
                },
                {
                    "sent": "Future directions and number of people were mentioning to me yesterday.",
                    "label": 0
                },
                {
                    "sent": "So with this work doesn't is still not.",
                    "label": 0
                },
                {
                    "sent": "Perfectly applicable to directed graphs because the semantics of the directed graphs are slightly different, we'd like to extend this work to director and bipartite graphs, and we'd also like to look at novel coarsening strategies.",
                    "label": 0
                },
                {
                    "sent": "We currently use the standard coarsening strategy that is used in both crank listen mentors.",
                    "label": 0
                },
                {
                    "sent": "So I would like to acknowledge NSF for funding our work.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are differences I can take any questions.",
                    "label": 0
                },
                {
                    "sent": "The next speaker right there is why questions.",
                    "label": 0
                },
                {
                    "sent": "Did you compare this newer result?",
                    "label": 0
                },
                {
                    "sent": "Resume data faster result.",
                    "label": 0
                },
                {
                    "sent": "Some worker that works at the host community structures and well before and compare the effectiveness of both the algorithms on which well known networks did you mean?",
                    "label": 0
                },
                {
                    "sent": "Right, right?",
                    "label": 0
                },
                {
                    "sent": "True, so with respect to domain specific evaluation that we had, we did some domain specific evaluation on the biological networks, but we were primarily constantly trying to look at graphs that were of some reasonable size, so we did not.",
                    "label": 0
                },
                {
                    "sent": "Actually, I think there's a characteristic clubs around 100 people or so, but yeah, that's a good idea.",
                    "label": 0
                },
                {
                    "sent": "I think that's one thing I'll go ahead and review and actually do that.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}