{
    "id": "du7kk4fkjpzzftornlegykv2w74kt5dy",
    "title": "Deep Learning (hopefully faster)",
    "info": {
        "author": [
            "Adam Coates, Baidu, Inc."
        ],
        "published": "Sept. 13, 2015",
        "recorded": "August 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2015_coates_deep_learning/",
    "segmentation": [
        [
            "Very hard to figure out what was going to be useful for the crowd here."
        ],
        [
            "Which is systems and parallel the parallelization for deep learning.",
            "Basically talking about how do you make your your deep learning experiments run faster than the reason that I think this is a challenging topic.",
            "That's kind of difficult to get our arms around.",
            "Is that AI and deep learning of course depends very heavily on good systems knowledge, right?",
            "We need our experiments to run fast and deep learning especially really likes really likes these big dense operations that run great on hardware.",
            "But to actually build a system, if you want to build a bleeding edge, highly scalable system, or maybe you have a job just running on your own GPU at home that you wish would run in less than a month.",
            "There are all these tools out there to solve the problem, and so if you're a systems expert and you've been studying systems your whole life, you know all of the tools in the store you've been working with them constantly.",
            "You know how they what.",
            "Therefore, when to use them, how they can break.",
            "When they're not appropriate, and even if you're not really sure exactly what the right tool is, you at least know which part of the story you should go looking in.",
            "Whereas me when I end up at like Home Depot, I spend most of the time just kind of like wandering around, 'cause I'm not really sure.",
            "And so when you start as a sort of a machine learning researcher or deep learning researcher, and you want to figure out how do you make your your systems faster, I think there's this sort of same sensation that it's just not clear where to look because it's so complex."
        ],
        [
            "So.",
            "What I wanted to do instead is.",
            "Is think about what are sort of the simple tools that can stand in for this level of experience 'cause most of the experts on this topic that I talked to I think they have this sense of where to look not so much from from training per say from having read a whole bunch of textbooks that say do this.",
            "Do this do this, but because they've solved lots and lots and lots of systems problems, they've seen all kinds of failures before.",
            "And so the best way to actually learn this stuff and really get it down path is to see a huge number of examples overtime to wind up building lots and lots of deep learning systems.",
            "HPC Systems, cloud systems and just kind of knowing building up that Sixth sense for what is fast and what is slow.",
            "But since we don't have time for that.",
            "Instead, going to sort of narrow the scope of this.",
            "So what we're going to do instead is talk about some simple tools.",
            "What I want to try to get at are some conceptual tools that you can use to understand or strategize about how to make your deep learning systems run faster or be more scalable so you can think of this is sort of starting out with your measuring stick for getting some information and some tools for planning and deciding what you're going to do next as you try to make your deep learning system faster."
        ],
        [
            "So I guess break this into a few pieces and actually this this should be pretty short and I haven't taught this specific material before, so if you have questions along the way about the systems aspect or how to how these things relate, definitely just kind of keep the questions coming.",
            "So I'm going to talk for a minute about sort of the basic motivations, like why do we care about any of this?",
            "So just in case you needed a reason to make your jobs run faster.",
            "And then I'll also talk a bit about how do we talk about performance on a single node.",
            "So if you just have one GPU in your workstation, or heaven forbid you guys are trying to develop deep learning software on your laptops.",
            "So you have some idea for what are the bottlenecks and what are the performance tradeoffs you have to make when you're just talking about a single node.",
            "And then we want to talk about."
        ],
        [
            "Multiple nodes where things kind of get a little bit more interesting, but in some ways simpler because you have a lot more control over network traffic and then the sort of things happening on your machine.",
            "Whereas when it comes to a piece of hardware that's very complex and has a lot of stuff going on underneath it can be very opaque when you're trying to make things run faster.",
            "So the first reason actually, maybe even the most important reason to try to make your jobs run faster in deep learning, is the fact that our workflow is researchers is very much a kind of guided exploration that we don't really know, like.",
            "What is the best model for image NET?",
            "We don't really know what is the best model for speech recognition, and So what we do is we start out with something that kind of feels right.",
            "That has the right structure in the right parts, and then we launch an experiment.",
            "And then we wait and we hang out.",
            "We read papers.",
            "We maybe try coding up some other experiments in parallel, but mostly we're just hanging out waiting for this to finish to see which things worked in which things failed, and my underfitting, like my baby, my model is too small and my overfitting 'cause my model is now too big.",
            "did I hit some kind of nasty optimization problem that I now want to go back and visualize?",
            "We're sort of sitting around waiting for all of this feedback in order to decide what to do next.",
            "And so if we could just get that end to end experiment time down, that would be really great, because in our experience, one of the things that.",
            "Really, Gates research progress that determines how quickly we can move toward our goal is the speed with which we can get around this cycle from the from the moment we have some idea for what we're going to do to the time we can get it into some code and then get results to sort of jog the next idea an the sort of sad reality is that this little innocent PowerPoint arrow here is something like weeks or months, so you quickly jump here 'cause you coded it up in Pilar and and then you're stuck for about a month.",
            "And then you get to go back around quickly."
        ],
        [
            "So, so that's one big reason, but the other one that I think is maybe a more sort of high level or strategic reason to worry about whether your model scale is that scaling is really an argument for deep learning itself.",
            "One of the reasons I think that deep learning has been so successful is partly because there's been genuine progress on the problems, right?",
            "We're much better at debugging optimization issues and sort of dealing with all the little exceptional cases of deep learning now than we used to be, we've discovered.",
            "Like going around this cycle many times we've discovered a lot of different models that in fact are very good for things like computer vision and speech and language.",
            "So we've made a lot of good discoveries.",
            "But I think the reason a lot of the results today are so eye popping is actually because of scalability.",
            "The fact that with previous algorithms like if you did a K nearest neighbor or support vector machine, or maybe you did boosting with trees or something like that.",
            "If I gave you a lot more data and a lot more computing power, you could make a bigger model, but at some point your machine learning algorithm would just start to see diminishing returns.",
            "You'll get more accurate for a little while, but then it would slow down.",
            "And So what seems to be different about deep learning is that we've found ourselves on this different trajectory that for a very long time in the past I mean the 80s nineties and so on.",
            "Deep learning was down here, we just didn't have enough data.",
            "We didn't have enough computing power to make progress and build state of the art systems.",
            "And so all these sort of hand engineered things or these more traditional machine learning systems were always better and you just couldn't beat them no matter what you did.",
            "And so I think what we're seeing in more recent history is that as time and effort have gone along into this problem and we've added more data in computing power, we are slowly working our way up this curve.",
            "And you're now seeing deep learning kind of overtake all of these other systems.",
            "And so in the past deep learning maybe hasn't been a great competitor.",
            "I think right now we're seeing deep learning sort of crossover to be maybe a close second and a lot of applications to suddenly being the top performer in a lot of applications and in the future think we will be up here.",
            "So the reason that you should worry about making your machine learning job more scalable is that for one, if you put some effort into scalability, you can push your way down this axis and hopefully get even better performance.",
            "That's one reason.",
            "But the more sort of future oriented thought is that the models that you choose today and that you're working on.",
            "If they match the hardware very well and they're very scalable, these are models that are going to get better almost for free as we capture more data and more computing power, and so one of the things that's been nice about deep learning is that a deep, dense Relu network has just been getting better and better and better.",
            "Even though you haven't had to do a whole lot in terms of fiddling with the structure.",
            "So the question, though is without actually running that experiment, how do you know which models are going to work?",
            "How do you have a sense for which models are likely to scale and which ones are not?",
            "So.",
            "In any event, so I think we'd like to try to find models apriori that are likely to benefit from this trend overtime, but it's tough to figure out if you maybe haven't built up this instinct yet.",
            "For for what is going to be faster, slow?",
            "Maybe, and so if you're going to build hardware.",
            "Hopefully you have a very good argument for why the hardware will make things better, and some of the tools we'll talk about it will be helpful for making that decision.",
            "So maybe the more common case that a lot of the folks in this room will have to deal with is you'll see a new GPU come out and you're trying to decide, like should I upgrade, it has like 20% more memory bandwidth, but it's twice as many teraflops.",
            "Is that a good deal?",
            "And if I have to decide, should I buy 2 GPU's which doubles the cost?",
            "Is that a good idea?",
            "And so some of the tools will talk about help you make that decision?"
        ],
        [
            "So there are a bunch of different ways to try to make things faster, so certainly we can change the software, right?",
            "Fifth, running slow, we can dig into our code, go look for bottlenecks and try to make the software faster and to go through this point.",
            "Certainly we could change the hardware for using CPU's.",
            "Maybe we should be switching to GPU's.",
            "I don't think I'm totally agnostic to this, but I hold out the possibility that there are many jobs in their world that will actually run just fine on a CPU, and to know whether you should have a GPU for your particular job.",
            "Is in some sense a design decision, and then there is the even further question that if a GPU is not enough, even many GPS is not enough.",
            "Do you want to do something like start looking at in FGA or you want to go even further in terms of capital investment and time and go for something like an ASIC?",
            "Go actually design A chip to do deep learning.",
            "So those kinds of decisions are pretty tough and you can't just dive into them unaware.",
            "And then finally, maybe the one we're most familiar with actually as researchers is that we can change the model in the algorithm, and this is.",
            "In a way, the hardest solution because it creates this sort of code design problem that on the one hand you're chasing accuracy.",
            "You're trying to keep your model very accurate and get the best performance while also dealing with this other constraint that you want it to run fast on hardware.",
            "So there are lots of things that we might like to do, for instance like attentional mechanism.",
            "We were having a conversation yesterday about attentional mechanisms, where you might really like to have this attention scheme that looks around in a big video.",
            "But unfortunately, these sorts of things that access data in unusual or unpredictable ways also don't fit well with the hardware, and so trying to make that tradeoff of getting good performance and building an innovative system will also matching up with the hardware becomes very challenging, so some of the tools we're going to talk about actually help a little bit with this, so that while you're busy optimizing for accuracy, you kind of have a sort of soft prior about what to do for the system side.",
            "So the way that we're going to do this is we're actually going to talk about performance modeling, so.",
            "In some sense, what I'm going to talk about for the entire remainder of the time here is really easy.",
            "It's sort of deceptively simple.",
            "There's going to be a whole bunch of plug and chug later on.",
            "That's going to feel really pedantic.",
            "And so when I first saw this was like, OK, that's nice.",
            "And kind of like sort of ignored it, but the more I started playing with this in some ways.",
            "Just to put these slides together more, start to realize like this will embody a lot of the intuition in the rules of thumb that very experienced systems designers are using without knowing it.",
            "They're making a lot of the same judgments.",
            "That this tool will let you make.",
            "So.",
            "So we'll work some examples through and so you forgive me.",
            "A little bit of the plug and chug that we're going to go through.",
            "But if you get a chance to do this on like a cocktail napkin with your own GPU, for example, and your own models, think you'll find that kind of can bring you some insights about what's happening."
        ],
        [
            "OK.",
            "So just as a preliminary, I want to point out that one of the reasons that optimizing software is maybe not the thing we have a lot of experience with is deep learning researchers is because deep learning has become very popular and so all of the things that we like to use for building neural networks have been picked up by NVIDIA and Intel and so on.",
            "And people are building beautiful libraries for us and doing all of the really nasty low level optimization to make things fast.",
            "So I won't talk in a lot of depth.",
            "About all of the sort of nitty gritty hardware aspects.",
            "If you want to make things fast.",
            "So instead.",
            "Just want you to keep in mind the fact that deep learning is this really nice architecture where we sort of have Legos.",
            "We can put together and a lot of those Legos are really fast, and so we'll try to analyze performance in terms of how fast these different operations run.",
            "Sort of small collections of these operations are likely to run."
        ],
        [
            "So how do we actually measure performance?",
            "So it's just sort of a definition here.",
            "So if I give you a fixed problem size, let's say I want to do a convolution in a convolutional neural network.",
            "I need to give you a bunch of hyperparameters, right?",
            "I need to give you the filter size.",
            "I need to tell you how many input Maps there are.",
            "I need to tell you how many output Maps you're going to need to compute.",
            "You need to decide is it double precision or fluent, single precision, or even like 16 bit precision floating point.",
            "And the question is, is what are we actually trying to optimize when we go to make this system faster?",
            "So to try to do that, let's assume much of the time that we're going to hold the number of operations fixed.",
            "If we could do that, we're just talking about a single instance of running convolution, then what we're going to try to maximize is the throughput, the number of operations that we can get done every 2nd, and so not surprisingly, right.",
            "If the number of operations is held fixed, then making this large means you're making your running time really small.",
            "So we're going to talk a lot about throughput for the rest of this, and it just means how many operations I'm pushing through per second and no matter what I'm doing, obviously finishing more of my operations is better."
        ],
        [
            "Alright, so one caveat with this, though, that I'll warn you about, as I did say.",
            "This goes against all of the things we normally do right.",
            "The one thing that we do a lot is fiddle with the model, change the hyperparameters, and so on.",
            "This is like part of what deep learning research has been doing that this actually made a lot of progress, and so you should keep in mind as you're doing some of these analysis that changes you make to the system that alter the number of operations or that alter things like the convergence time will have an effect.",
            "And so I'll go into this in a little bit more detail later, as a sort of illustrative example, but just keep in mind that throughput in terms of the number of operations I get done per second doesn't necessarily consider things like convergence rate, so I might have wonderful throughput, But if my algorithm is taking longer to converge now because of whatever I changed, it really doesn't matter.",
            "So in a sense, throughput is a sort of gameable metric.",
            "So, so it's something that you can actually see.",
            "If you want to maximize throughput, the answer is to get an enormous number of GPU's and make your mini batch something like a million.",
            "And then and then your your throughput will be incredible.",
            "It will be something you know like 1000 GPU's running at peak and the number of operations you're finishing will be awesome.",
            "But the wall time for your whole experiment will be the same as if you just had a couple of GPU's, so keep that in mind.",
            "Don't don't get fooled by by what we're optimizing here."
        ],
        [
            "So let's start talking about the principles for how we do some of this analysis on one machine or one node.",
            "So when I talk about a node, actually I'm sort of referring to like one CPU socket or one GPU, one FGA in some sense is like one enclosed unit of computation with its own address space and local memory."
        ],
        [
            "So the best question to ask yourself when you start this out and this will motivate why we're we're doing.",
            "Some analysis here is to keep asking how much is there that could be gained.",
            "So if it turns out that my system is already very well optimized, which it will be, for instance, if you're using like packages to do, very large matrix multiplies that are already well optimized, then there's some question about whether it's worth it.",
            "And so one of the important things about doing the back of the envelope analysis.",
            "I'm going to teach you to do here is that you keep rechecking it as you move along and make improvements to ask yourself is it really worth spending another week reading about optimization techniques.",
            "When my job only takes 4 days to run now, so there's going to be some question of whether you want to stop optimizing and go back to deep learning or not.",
            "So to answer this, we're going to talk about ways to sort of assess how much potential gain you can get, and then we'll go back and we say we're going to look for the biggest potential gains and will start optimizing those things until we get tired.",
            "So."
        ],
        [
            "So.",
            "One thing that sort of irks the systems people I know is how we measure how fast our machine learning work is going, because we often go to conferences like Man, I spent like a month learning CUDA and I made this really amazing kernel and it's like 4 times faster than the previous code I was using and this drives all of my systems friends crazy because they say your baseline is not how slow your code runs.",
            "If you really terrible code to begin with and now your new code is running four times faster, that is just not interesting to a systems researcher 'cause they don't, they don't know how to interpret this.",
            "There's no way to compare this to all the work that's been done before, so if you ran your code in Theano and someone else ran it in torch, and someone else ran it in cafe, I don't know how to compare you.",
            "If you just tell me how much faster you're going today.",
            "So on the one hand, telling people that you got a 10X speedup or a 5X speedup.",
            "This is awesome for you.",
            "This is really great right?",
            "Because it totally changes how you do you work.",
            "If you can achieve that speed up.",
            "However, this is also a totally non actionable metric that changes how you do your work today, but it doesn't tell you what to do next, so you don't know.",
            "Can you get another 10 X?",
            "So how do you know if there's actually more to be done?",
            "So instead."
        ],
        [
            "Add the baseline that we want to measure everything against.",
            "When we're asking what's the next thing to do.",
            "Should I buy another GPU?",
            "Should I spend more time optimizing?",
            "Should I be investigating novel hardware?",
            "What should I do?",
            "Our baseline is the fastest sarcodes could ever possibly run, so we sort of call this the speed of light measurement.",
            "It's the it's the speed limit.",
            "The cosmic speed limit for your neural network, and the speed limit is going to be set by the hardware platform that you're running on, and so if you knew how to calculate this if you knew what the maximum potential throughput was, then you would know that as you started to get close to that, you should just give up because you're getting close to the speed of light.",
            "And you're not going to be able to go faster than that.",
            "And so just is like rules of thumb to get a sense for what people will be happy with if you're running at about half the speed of light, you're doing pretty good like you're doing a good job.",
            "If you're going to see because there's about a potential 2X speedup left, which means there you could buy another GPU or double your cluster and get that potentially.",
            "So 2X is not nothing, but it's also not something you should feel bad about, 'cause you're certainly not going to get all the way till 1.0 C. And then if you're getting something like .8 C, right, you're getting 80% of the speed of light throughput right then?",
            "Then you're doing great.",
            "'cause that means there's only about 25% speedup left on the table an as in physics.",
            "Usually it's going to cost you more to go faster if you're already close to the speed of light.",
            "So if you keep track of this metric for yourself and always benchmark yourself against the fastest you could possibly run, then you'll know how much effort it's going to take you to go even faster.",
            "And if it's time to start looking at something else.",
            "So."
        ],
        [
            "So.",
            "Yeah, so I want to put this in a box here just to remind you so, so don't worry about how slow your code was before.",
            "Try to run an analysis to figure out what's the fastest you could run and benchmark your progress against that.",
            "So our goal here, the tools that we're going to start cranking through is to try to figure out for a single node, like your GPU for example.",
            "How do you estimate the speed of light?",
            "How do I know in advance without trying to run tons and tons and tons of benchmarks?",
            "What is the fastest I could possibly go?"
        ],
        [
            "So let's talk now about performance modeling that will help us answer this question.",
            "So let's suppose that I give you a fixed amount of computation that you have to perform.",
            "So I tell you, please do this convolution or this matrix multiply as fast as you can, and I'm going to give you all the hyperparameters so you know exactly how much work you have to accomplish.",
            "What we want to do is estimate the maximum potential throughput.",
            "We want to know if I did everything right if my hardware were perfect, what's the fastest I could possibly go?",
            "And this is incredibly hard to do in general, so if you have an X86 processor there, there's all kinds of incredible stuff going on in there to make your programming faster and more convenient, and so figuring out what it's going to do and when and when you're going to run afoul of it is really, really hard.",
            "So nevertheless, we're going to use a really simple scheme here.",
            "That's very quick.",
            "You can do it on a cocktail napkin an it'll give you intuition, and so the thing I hope that you will take away from this tool is that we're going to be talking about something that's a little bit like an economics model.",
            "How many people have taken the concourse?",
            "Thank you, good fraction of people.",
            "So economics you often have these sort of models like supply curves and demand curves and things and these are not meant to be sort of hyper quantitative to tell you exactly how many widgets you will sell tomorrow.",
            "They're meant to give you a sort of stylized intuitive you about what's going on.",
            "So if you raise the price or you lower the price, you have some sense for how that will affect the world, even if you're not quite sure how much and so the things we're going to talk about, I hope will kind of give you that ability.",
            "But for estimating this speed of light.",
            "So, Armada."
        ],
        [
            "Of computation, so the analysis we're going to do is going to be built on a wildly oversimplified picture of how a computer gets its job done.",
            "And So what we're going to do is represent our computer with computation and memory only.",
            "So that's all we have.",
            "We have a computing core here that runs at some speed, some number of flops or operations it can do per second and then we've got some memory living over here off the chip an we've got a pipe connecting them and the only limitations we're going to worry about in the hardware is how fast our computing core can go and how much data we can ship back and forth across this bus per second.",
            "So."
        ],
        [
            "For a current GPU this year I rounded these numbers off a little to make the math easier later.",
            "You know you can get a GPU that will do six teraflops very happily.",
            "In an ideal case, and the memory bandwidth to the local memory on the GPU is something like 300 gigabytes a second.",
            "Even as I say these numbers now, this is really crazy compared to what we had even a few years ago.",
            "But as you get new numbers for this, you'll be able to update your analysis very quickly.",
            "So the key assumption in this model is not just that we have only two constraints that that's the simplifying part.",
            "The key assumption that we're going to make that will make all of our analysis possible is that I can always stream in memory simultaneously with doing computation.",
            "So while the computing core is busy doing things, I'm assuming that data can be going back and forth freely through memory.",
            "And this is actually a somewhat strong assumption.",
            "If you sit down to write a very highly optimized GPU kernel, you will find that you cannot actually pull this off perfectly.",
            "You'll find that you're holding things in cash, and you need to be getting another piece of memory to keep up this transaction so that it will be ready when you need it.",
            "But the cache is full and you're not done with your cash yet.",
            "'cause it's feeding your computation, and so you sort of have to make a trade off.",
            "You have to give up.",
            "On this assumption of being able to stream memory simultaneously with doing computation so so, keep in mind that this assumption is there, but it's actually not too terrible for a lot of the workloads that we see."
        ],
        [
            "So if we actually ran a sequence of operations on this little stylized computer, and then we loaded it up in our profiler and looked at like a timeline view of what was going on, we might see something like this.",
            "So we might start operation number one here, and while operation number one is running, there's a bunch of compute that's just happening on our little computing core.",
            "And then there are a bunch of loads and stores happening, so the operation is asking for things from memory, and it's also.",
            "Going to store the results back to memory.",
            "And the key assumption is that these are completely overlapped and you can see that this is a little nonsensical right?",
            "Because clearly when I finish my compute over here, I have to store my result at the end, but I'm just assuming that that's all totally asynchronous and it can happen anywhere in here.",
            "So this is the simplifying assumption I'm throwing away all the data dependencies that make analysis confusing and just assuming we can do everything in parallel.",
            "And similarly, there might actually be operations where the loading and storing part is the slowest, and so if you ran another one right afterward, you might see that you're actually doing more memory transactions than than computation."
        ],
        [
            "So.",
            "Let's do a quick example here, so let's start using this little computer model to make some so or early assessments of how different operations behave.",
            "So what I want to do is compute a simple matrix vector multiplication.",
            "So I've got this matrix A which I'm storing in single precision.",
            "It's all good.",
            "Deep learning codes do.",
            "And this is an M by N matrix an what I want to do is multiply this by an N dimensional vector.",
            "I'm going to be.",
            "So to figure this out.",
            "We we need to figure out how much it's going to load in store from memory.",
            "So if you look here if I've got 4 bytes for every entry, I have MN entries that I've got a load for a right, just obligated to touch every element of A and then I have N entries in this vector.",
            "So I've gotta load them both groups and I kind of goofed here.",
            "There should be a two in front of this, 'cause I also have to store the output.",
            "And then the up there we go.",
            "So how much do we have to store?",
            "So this is just the loading part.",
            "Here we go.",
            "And so we also have to store 4 bytes times M, which is the dimensionality of the output.",
            "And then the number of floating point operations that are compute core actually has to execute is actually M * 2 N minus one, and for most of this talk I'm going to drop these little minus ones.",
            "There's actually a little missing addition on the end of your summation here and just call this two MN floating point operations."
        ],
        [
            "OK, so.",
            "What can we do with this?",
            "Let's just take a specific instance, right?",
            "Let's not worry about the general case yet.",
            "Let's suppose I tell you that M is 1024 and an is 512, so we kind of have a bit of a sort of tall skinny matrix is about twice as tall as it is wide.",
            "And what we want to know is what's the best possible throughput we could achieve for this on our little hypothetical hardware platform?",
            "So we can work out how much memory traffic we have, so it's just 4 bytes times the number of loads I have to do in the number of bytes that I have to store so that it turns out to be like about 2 megabytes.",
            "So not too much.",
            "And then we can compute the number of floating point operations we've got to do so that's just two MN.",
            "Turns out that's about a million flops.",
            "And then we can ask what's the running time going to be?",
            "According to our very simplified computer model.",
            "So basically it's going to be the maximum right?",
            "'cause I have these two independent pieces of work that's going to be the maximum of how long it takes to load that 2.1 megabytes at 300 gigabytes a second.",
            "And how long it takes to do my million flops at a rate of 6 teraflops.",
            "So if I work those out it turns out it's going to take about .16 microseconds to do the compute part about 7 microseconds to do the loads and stores.",
            "And the important thing to notice here is that this is the controlling factor, right?",
            "The Max doesn't care about what's in here.",
            "And so the throughput of this whole thing, which is about 142 gigaflops.",
            "Is totally totally ignoring how much compute I have available, right?",
            "I can make very substantial changes to what's going on here, and I don't alter the answer at all.",
            "So in some sense, how many flops I did or how fast my my compute unit was is totally irrelevant for determining this number, which in some sense is our speed limit.",
            "The fastest we can possibly go for this problem?"
        ],
        [
            "So.",
            "In order to kind of quantify this quickly, so rather than kind of having to go through all this analysis every time, it turns out that there's a sort of convenient quantity for thinking about these things.",
            "That's called the intensity.",
            "So the way that intensity is defined is just to take the number of operations you need to do in deep learning.",
            "It's usually floating point operations, but it could be integer or whatever.",
            "And divide by the number of bytes that you need to load or store.",
            "So this is the number of operations per byte.",
            "So for the previous scenario, you can take your million operations divided by 2.1 megabytes and you get 1/2 of a flop per byte.",
            "So so this is what we would call a very low intensity problem and low intensity problems will see are always bottlenecked on memory.",
            "So This is why compute doesn't matter in this case because intensity is really low for every bite.",
            "For every flop we want to do, we actually have to load two bites from memory and that takes a long time.",
            "So this concept."
        ],
        [
            "This idea that there's this Max and that some things matter and some things do not, is embodied in a really cool sort of stylized model introduced by by Williams and Co.",
            "Authors in 2009, and this is sort of meant to be a pedagogical model, right?",
            "It's meant to give you some intuition for what's going on that you can use quickly.",
            "So these two constraints from our computing platform are little computer model or really easy to draw.",
            "If the horizontal axis is intensity, which we just defined and the vertical axis is throughput, which is the thing we're trying to optimize.",
            "So.",
            "For instance, the computing limit of six teraflops.",
            "That's just a ceiling, right?",
            "No matter what we do, no matter what operation we ever run.",
            "If you see your code telling you your timing code says you're going faster than six teraflops, you know something is wrong, right?",
            "This is the cosmic speed limit for your hardware and certainly the hardware vendors are not going to under quote this.",
            "So you also have the bandwidth, right?",
            "That's how much data we can ship back and forth to memory, and that constraint is represented by a line here where the slope is the bandwidth, right?",
            "So if you think about the units here, flops per second divided by flops per byte, then we get bytes per second, and so the bandwidth constraint shows up over here.",
            "And so now you can think of this sort of envelope the the upper bound on what you can achieve.",
            "You can never be outside of this little trapezoidal region here."
        ],
        [
            "And so if we'd like to know how much intensity do we need to to be using all of our computing throughput, we can actually calculate the location of this intersection.",
            "The hinge where where the constraint changes, which is where that Max shifts from being Max, is no longer the Max of is no longer equal to the running time of the memory portion, but suddenly becomes equal to the running time of the compute portion.",
            "So the way we can calculate that is just of course.",
            "Take the height of this thing divided by the slope here to get the X value.",
            "So if you do this for our numbers, it turns out that this little hypothetical machine needs an intensity of 20 flops per byte, which is not too bad for deep learning work, but is still much much much bigger than 1/2 of a flop per byte.",
            "So.",
            "This number is a little bit tricky to just read off for a new GPU or a new CPU, but if you do some, maybe some benchmarks, or even just go ahead and do the back of the envelope for your machine.",
            "It's good to just file this number away so that you know what it is for your hardware and you can compare things against it as you go.",
            "So."
        ],
        [
            "So it's easy to see the relationship between compute and memory in this model, and so based on the sort of theoretical numbers for our system, we decided that 20 flops per byte was what we needed to be compute bound.",
            "And the reason this is useful is because anything below 20 flops per byte, like the matrix vector multiplication.",
            "If we carry out this little computation to find the intensity, and we see it's less than 25 per byte, we know that compute is just not the limiting factor.",
            "We could just read it right off.",
            "So let's do another example.",
            "Let's see."
        ],
        [
            "OK, so let's do another example.",
            "So let's suppose that our goal now is to do a matrix matrix multiplication, where a is M * K and B as obviously has to have K rows here and columns and then C is going to be M by N. And what I want to do is I want to take C and I want to add the product a * B to it.",
            "So.",
            "We can go through the same plug and chug again here.",
            "We can take the number of bytes that I have to load and store for this, so MK for this guy KN for this guy and then 2M N 'cause I've got a load C first add to it and then store C. I multiply by 4 bytes, the number of flops that we need to compute.",
            "This is roughly 2 times MKN.",
            "It's really the number of flops for this thing.",
            "The extra terms over here aren't that important.",
            "And so if we compute the intensity for this, we have about 64 flops per byte.",
            "So once you know these two numbers, you can just divide it all out and say, like, alright, this guy should be compute bound.",
            "So unlike matrix vector matrix matrix is in some sense much more efficient because you actually get to use all that floating point throughput that you've got on your GPU.",
            "And the reason that that works is because the structure of this operation has much higher intensity and this is.",
            "In a sense, a fundamental part of how this operation works."
        ],
        [
            "So one thing that I introduced here that I didn't talk too much about before is that I just analyzed kind of two operations together, right?",
            "I took matrix product and I added something to it.",
            "I did two operations at once as part of my analysis and this is a general trick.",
            "You're welcome to aggregate as much stuff into your roofline analysis as you want and just total up all of the floating point operations and total up all of the memory transactions and compute the intensity.",
            "But just remember again that there's an.",
            "In some, there's an assumption baked into this when we do it, what we're assuming when I put when I do, my roofline analysis is that I'm looking at one of these little blocks where the loads and stores in the compute can be overlapped, but once I group two things together like I do a product and I do in addition at the same time."
        ],
        [
            "What I'm assuming is that these two things can be analyzed as one that in effect the memory transactions can both be overlapped, which might not always be possible to do, but in the case of like matrix matrix multiplication, you can certainly do so.",
            "You guys will notice if you use like a Blas library that the gem call is very happy to let you do an extra addition in there, and so you can actually pull this off very efficiently.",
            "So so just don't forget about this assumption again, so you can aggregate as much stuff as you want into your roofline analysis, but it will start to get really hard to guarantee that all of these transactions overlap."
        ],
        [
            "So this roofline model is basically your upper speed limit, right?",
            "So with this analysis, all the things we've been carrying out have been giving us a throughput number, and that throughput number is basically the point along this line.",
            "It's saying this is the upper limit for what you can possibly achieve for this transaction for this operation.",
            "So for something like matrix vector right, it's kind of living down here where it's bottlenecked on memory, and there's just not much we can do to make it go faster.",
            "Without having it be much more intense, whereas something like matrix matrix is living up here and we might be quite happy if it's running at full speed and so one of the values of the roofline model is that it tells you what is C, what is the speed of light, and so if you run your matrix matrix and you find that it's taking up all of your computing time but it's not achieving this limit, then you kind of know where you should be looking for a problem where there's some room to improve, and so as a general sort of philosophy for how to go about choosing.",
            "What's worthwhile to look at?",
            "You can go for the thing that's using up a lot of running time and has some headroom here.",
            "So if your matrix vector.",
            "Operation is already getting the peak bandwidth.",
            "You might as well just give up.",
            "Don't don't bother spending time on it, whereas if matrix matrix is not running fast enough, maybe pushing further down this way, I'll help you a little bit in practice.",
            "But maybe it means you need to call NVIDIA and ask them to fix the blast kernel or something."
        ],
        [
            "So in practice this theoretical limit is hard to reach.",
            "There are lots and lots and lots of things you have to worry about.",
            "So if you're writing your own CUDA kernel, for instance, actually getting the peak is really hard, but you can often do it.",
            "So it's great to be using something like koblas that's already been optimized for you by very sharp people with intimate knowledge of the hardware.",
            "But there are some problems.",
            "For instance, if you take a BLAS library and you start making the matrices too small, they're just different kinds of resources in the hardware that you're not fully deploying, and you'll see that instead of sort of marching perfectly up this line, you have, you know, kind of a little curve 'cause in this region you're sort of trying to optimize for both bandwidth and compute.",
            "And the code just isn't as fast as it could be.",
            "So this is just an approximation still on.",
            "When in doubt, you can often sanity check these boundaries with little micro benchmarks you can just write very tiny kernels to kind of feel out.",
            "What is the best bandwidth and what is the best throughput you can get.",
            "So."
        ],
        [
            "This is an example of where this would have saved me personally.",
            "An incredible amount of time.",
            "A few years ago, while we were working on like this HPC system that we built for deep learning, I spent a long time optimizing a specialized matrix multiplication kernel to handle like locally connected networks place where koblas wasn't running fast enough, and it turned out that there is actually a combination of compiler plus architecture problem on Kepler that made it so that you could never achieve more than 50% of the floating point peak using CUDA code.",
            "And so, knowing this in advance would have saved me a lot of time because I was stuck at getting about half of what I thought I should be getting.",
            "So keep in mind that doing a little microbenchmarking ahead of time to make sure that something reasonable is going on is is a good idea.",
            "So in practice though, this isn't too bad, so this is, uh, from uh, kind of neat paper by ofenbach and some coauthors.",
            "Basically what they did is they took a bunch of Z on CPUs and ran kind of dense linear algebra jobs on them.",
            "So do you like adding a plus Y here?",
            "So this is a BLAS operation to add 2 giant arrays together.",
            "Doing like double precision matrix vector right which we said would be bandwidth bound and then doing a double precision matrix, matrix multiplication and so all of these points here on these little squiggly curves are actual kernels that they ran with little hyperparameters in them to tune their performance, and so you can see that as a function of the intensity that they actually measure from the kernel.",
            "It actually does follow this like nice roofline model.",
            "So this is reasonably accurate if you can compute these numbers.",
            "And so the reason that this kind of squiggles here is because this is the measured intensity.",
            "And if you have something like a cache miss on your X86 CPU, that means you actually have to double load some things.",
            "So the result is that if your kernel is not tuned quite right and you have some more cache misses, a bigger matrix multiply can actually have slightly lower intensity.",
            "So that's why you see this little squiggle.",
            "And just as an aside.",
            "Yeah, go ahead.",
            "This one over here, so this is double precision.",
            "Alpha Times X + y.",
            "This is a Blas operation that just takes two big vectors X&Y.",
            "It multiplies X by a scalar Alpha and then adds them together.",
            "So this if you're building neural Nets model, this is just saying multiply by a constant, like the learning rate and then add my two arrays together.",
            "So we use this all the time and the critical observation here is that your way, way, way down this curve.",
            "This is the consummate memory bandwidth, limited operation.",
            "So.",
            "Just as an aside though.",
            "This model doesn't take into account things like cache misses on its own, but if you happen to have some knowledge about the fact that you're not going to be able to deal with this problem you're dealing with is going to have a lot of cache misses 'cause you're doing random access.",
            "For example, right?",
            "Again, we were talking about attentional models and what makes those sorts of things hard.",
            "And if you don't know where you're going to access in an image, and so you're kind of bouncing around and you wind up double loading some things.",
            "What that is is that you have to add that memory transfer into your roofline analysis annual see your intensity starting to go down.",
            "So if you can actually think through how much extra stuff you're going to load for these sort of more sophisticated models, you can still do this analysis."
        ],
        [
            "OK.",
            "So in summary, what we want to do is find the maximum potential throughput, which is to say the speed of light to know the best performance our code could ever possibly get.",
            "And then we're going to benchmark ourselves against this every time we go to optimize the code, change the model, look at new hardware we're going to, we're going to keep this in mind, and a factor speedup is nice for you, but it's not actionable really for figuring out the next step.",
            "So the thing that I think is a good first pass on figuring out what to do is to look at operational intensity and this roofline model to kind of just sort of quickly spec out.",
            "What are the reasonable things you could do and what kind of performance you might achieve."
        ],
        [
            "Alright, so let's look at some more issues that you run into on a single node."
        ],
        [
            "Mini batch size is a really classic thing to tweak in your machine learning algorithm, right?",
            "So historically, what people would find is that a mini batch size of 1 always leads to the fastest convergence that if you have some big optimization problem and I want to minimize the number of iterations that it takes to get to the solution, setting your mini batch size to one is the best bet and so.",
            "This, however, on current hardware does not imply that it will be the fastest experiment and the reason is because of these systems issues.",
            "So, So what size should we actually try to use?",
            "Well, let's just take one of our examples."
        ],
        [
            "Let's suppose I have a deep neural network and each of my layers is N by N, right?",
            "I have N input neurons and an output neurons, and I've got a mini batch size of M. Well, my number of operations is 2 N squared M. That's easy to just pick out.",
            "Here's the number of memory transactions that I've got to do when I do a matrix matrix multiply of this size.",
            "And so for a range of mini batch sizes for a range of choices of M and some various choices of NI can show you how the how the intensity changes.",
            "This is not the throughput.",
            "This is the intensity how the intensity changes with different choices and you can see that if we're going to use our little 20 flops per byte benchmark that all of these guys kind of crossover somewhere around, like maybe 50 to 50 to 60.",
            "So what this says is that if you're using a mini batch size down here, you are your bandwidth limited.",
            "You are on the left side of the roofline model, and you're not using all of your resources.",
            "But if you're up here, you're now compute limited and you can't possibly process mini batches any faster.",
            "You're not going to get any more work out of your system by making the mini batch bigger.",
            "And so the."
        ],
        [
            "Fact is that if you're below, let's say 64 about the crossing point, then using a bigger mini batch is almost like a free lunch because you get double the mini batch size.",
            "Let's say if you go from 32 to 64, but the running time does not double, you're sort of getting some operations for free.",
            "And beyond 64.",
            "The free lunch goes away and if you double M from 64 to 128, you'll just.",
            "You're just going to double the running time of your operations.",
            "And so once you factor this in to the convergence improvements, what you usually see is a curve like this, where for a tiny mini batch size our experiment time is really high because it doesn't take us very many iterations, but every iteration is just really slow in terms of total throughput.",
            "And as we increase the mini batch size our convergence gets convergence improves, but each iteration takes a little longer, but not so long that we don't see.",
            "An improvement in the experiment time and then once we cross this sort of critical point where every mini batch isn't really buying us anything in terms of optimization, but it's just taking longer to compute.",
            "We'll just see the running time start going back up.",
            "So if you're trying to guess what you should set your mini batch 211 reasonable piece of advice is just to raise it to the point where you're now compute bound 'cause in some sense you've eaten up the free lunch and gotten as much convergence.",
            "Improvement out of stochastic gradient as you can get and beyond this year, you're going to hit diminishing returns."
        ],
        [
            "So, and this is kind of a general thought that you'll often hear is talking about, like oh, we want to compute limited.",
            "We want to compute limited and the reason for that is that if you aren't compute limited, there's probably a free lunch somewhere to be had.",
            "It means there are resources on the hardware that are waiting to do more operations for you and make things run faster, but for some reason or another, because of how your job is set up, you're not actually utilizing them, and so if you see that you're not compute limited, you could try using a bigger model.",
            "Bigger mini batch to try to get some gains from that."
        ],
        [
            "OK, so your model is supposed to be compute limited, but you're not achieving the throughput you expect.",
            "There's a bunch of things you could try to do to optimize this.",
            "I'm actually going to jump over a couple of these slides.",
            "But just want to point out that if you're trying to optimize this thing for improved memory, bandwidth is not going to help."
        ],
        [
            "So there are a bunch of things to try for people who are interested in digging into CUDA kernels and things like that.",
            "If you want to take a foray into to creating your own systems libraries for deep learning, you can come back to this slide or start digging through optimization manuals.",
            "But the high level picture of I want you to take away from this is that whenever you see something with low intensity, whatever it is, matrix, vector or or.",
            "Adding two arrays together, there's some bucket of things to try to improve memory bandwidth.",
            "And you should ignore everything else in the universe as far as things to try.",
            "And for high intensity workloads, there's another bucket of things to try that is usually different and you should ignore all the memory bandwidth stuff."
        ],
        [
            "One final note, again on code complexity in the speed of light.",
            "The more tricks you bring to bear to make your code faster, the more complicated it gets, and the harder it will be to get even closer to the speed of light.",
            "And So what people often do to mitigate this is they'll write separate pieces of code for all of the different parameters that they want.",
            "So if you're going to do a matrix matrix multiply, you'll pick lots of different shapes and sizes of matrices, and you're right.",
            "A kernel that works really well down here.",
            "Another kernel that works kind of up in this middle region and another kernel that works in this region where we're very compute limited.",
            "So if you're digging through sort of code online like you go through CUDA convet or something for example, you'll see gigantic dispatch codes that are figuring out exactly what kind of convolution are you asking for and bouncing you to a kernel that's good for that."
        ],
        [
            "Alright.",
            "So let's start talking about multi node which is actually kind of the main event when it comes to deep learning.",
            "So as I said, a lot of libraries are available now to do deep learning operations which is really fantastic."
        ],
        [
            "So to go even faster, what we want is to use many CPUs or use many GPU's in many machines at once, and the unfortunate state of things is that there are relatively fewer tools and libraries to help you with this, and so in a sense you're going to be on your own much more often, and figuring out the performance issues in these systems, and sadly.",
            "Even for very good systems, it's not that easy to automate, so this has been a long long standing problem in supercomputing.",
            "How do you make like a big physical simulation?",
            "Run fast on a supercomputer and it's just hard to do, and a lot of the tools we've been talking about are useful there too.",
            "So what we're going to actually do is sort of come up with a trick to just reuse some of our analysis tools from before to guide how we make decisions about parallelizing things."
        ],
        [
            "So to sort of step back for a second, I want to pose the higher level question or just what can we hope to achieve by having more computing power?",
            "Before we've actually been talking about a fixed budget, right?",
            "It's like we bought our GPU.",
            "It's there, and we're doing all of our analysis about what to change in our problem to work better on this GPU.",
            "Now we're with a different situation where we have the option if we'd like to use this other nice GPU that's hanging out over here, willing to do work for us.",
            "And the ideal case that we always have in mind is that if we start with."
        ],
        [
            "A job that runs nicely on a single node.",
            "What we'd really love to do is achieve higher throughput for this same job by just breaking it onto these two nodes.",
            "And so if we were really lucky if we did a really great job, then maybe we could run 2X faster.",
            "Ideally we would get 2X the throughput for this system in aggregate.",
            "So this this designer item is called strong scaling.",
            "So when I say I want to run the same job twice as fast, we say that that is trying to scale strongly.",
            "And so you'll hear this term quite a bit."
        ],
        [
            "The alternative, though, which wasn't really useful before, is that we could parallelize and make the workload larger at the same time.",
            "So what if instead of just trying to take my existing DNN and split it in half, I actually just go for a bigger one?",
            "Or I use a bigger mini batch size for example, and then I parallelize it.",
            "This is called weak scaling, and when to do this is maybe not clear.",
            "If you're not sure how big of a model you want or how big of a mini batch size you want.",
            "But the good news is that weak scaling is pretty easy, all things considered.",
            "Because if I just keep making my job bigger and bigger, virtually any implementation you can is going to scale weekly.",
            "You can do really horrible things on supercomputers and have really terribly written code, but if you make your problem large enough and you do your little analysis, you'll just find out that your matrix multiply has so much arithmetic in it that it dwarfs everything else you ever want to do.",
            "And so the good news is that if you want to scale weekly, and you've got good single node code, you can do it.",
            "Yeah, yeah, sure.",
            "Sure.",
            "So it depends on the structure of your neural net, of course.",
            "Let's take those sort of a really lame neural net, right like a one layer neural net that is just totally dominated by a giant matrix matrix multiplication zero matrix matrix is like the sort of perfect weak scaling problem because the number of flops is MNK.",
            "Right?",
            "It's kind of like a cubic term, and the number of memory operations I've got to do is like M N + K N right?",
            "It's all quadratic, and so if I just make those numbers big enough in some axis, right?",
            "If I just keep scaling them all up, eventually the memory portion of that is tiny, and so for the purposes of getting the best throughput right, keeping your GPU's busy?",
            "This will work right because the number of flops per byte is just going up and up and up and up and up.",
            "I think the reservation your voicing is maybe two fold, which is real, which is that maybe you just don't want a neural network.",
            "That is, you know enormous, right?",
            "If M, it has to be like 50,000.",
            "Right?",
            "Yes.",
            "Yes, and So what?",
            "You're sort of getting out there is that.",
            "To scale weekly, I usually mean to sort of push all the parameters to their limits so that you are compute bound and what you're saying is that I might not actually want to make it double right.",
            "I could run out of memory that we kind of assume we've got a lot of memory at this point, and so on.",
            "So yeah, so there is some variation there.",
            "But the good news is that at least in practice, you can if you can use a bigger mini batch.",
            "Let's say that you've been memory constrained.",
            "Or compute constrained with like a mini batch of 64, right?",
            "And now I have double the compute.",
            "So on one node it didn't make sense to double the mini batch size, right?",
            "I was already compute bound if I now have another node and I think that doubling the mini batch will actually help me in terms of convergence rate.",
            "That's a freebie.",
            "So in some sense I now go to 128.",
            "I have twice as much work twice as many nodes.",
            "There is some communication, but for something like a common it, it doesn't really matter, so I'll run faster, hopefully and I and so this is different than the strong.",
            "Excuse me, this is different from the strong scaling case where I actually want to cut my mini batch in half as I go to two nodes.",
            "Did that sort of make sense.",
            "OK, alright, we catch you afterward.",
            "Alright, so."
        ],
        [
            "So so an example of this from some of our own work at Stanford awhile back is that small networks don't get that much faster if you have a lot more GPU's.",
            "So here's an experiment we ran with like a big neural net that's gotten.",
            "Yeah, 185 million parameters.",
            "That would be like the size of a decent comp net, and this is actually model parallel.",
            "This is a neural Nets that's been broken up over many many GPU's, and if you were spending about maybe .75 seconds per iteration before.",
            "Early on you're getting really great scaling.",
            "You're going faster, right?",
            "You're almost scaling strongly as you go to more nodes, but eventually as you chop up this piece of work, you get sort of less and less and less improvement, and so weak scaling corresponds to the option if you will.",
            "To say maybe I just want to have like an 11 billion parameter network.",
            "I'm going to take this job and I'm going to play it by a factor of 100.",
            "To make a vastly, vastly larger network, an wow look at that.",
            "It takes, you know, not not much more time than this little one did on on one GPU, and so there's of course an open question of is this a useful network for something like supervised learning?",
            "Clearly it isn't.",
            "But the point being is that as you add GPU's, you very often have this ability to just go to a bigger, bigger model.",
            "If you would like.",
            "And it's much easier to pull that off than to make this curve stay flat.",
            "That's the high level thing you should take away."
        ],
        [
            "OK, so if you can use a bigger model as I said.",
            "Why not do it?",
            "If you think having a doubled.",
            "If you think doubling the mini batch size would make you converge faster than this is a good candidate to just scale up an all things equal.",
            "You should try to do this first.",
            "So for the rest of the time I'm going to just say in practice we often don't want to do this.",
            "Sometimes we don't want a bigger network because we don't have enough data.",
            "Maybe we've already hit diminishing returns where doubling the mini batch size just isn't going to help us converge any faster.",
            "And of course the real thing that we want about strong scaling is that it squishes our cycle time down.",
            "It helps us take the same model that we're really interested in learning about and get through experiments more quickly.",
            "So.",
            "What makes strong scaling so difficult?",
            "There's actually a bunch of factors so."
        ],
        [
            "Understand this, we need to go back and we need to analyze the performance of our multi node system.",
            "So first, what we're going to do is try to partition the work and just start by assuming that our network has infinite bandwidth.",
            "So we're just going to totally take away the communications problem we were talking about, and assume that all we want to do is break the work into two separate nodes and then see how it goes, and if they ever need to communicate.",
            "I'm just going to assume that's infinitely fast."
        ],
        [
            "So as an example which is apropos to what we were just talking about, is that it's really common practice to take a training job and partition it onto multiple nodes by splitting our mini batch in half.",
            "So let's say that X is a bunch of examples, right each column of X is like one training vector and our mini batch size is N. And now here is our little neural network weight matrix.",
            "I want to multiply these together, and that's going to give me a matrix of activations.",
            "Then with data parallels on what we're doing is breaking this mini batch and putting the first half on node 1, three in the second half on node two, or mini batch size on each node.",
            "Just got cut in half.",
            "And now when we multiply this by W, we're going to end up with some activations that live on node number one, and we're going to do some activations that live on node number 2.",
            "OK."
        ],
        [
            "So there is of course the wrinkle that we have to keep W synchronized right?",
            "So every time we run an update and we change W, the two nodes are going to talk, but we're going to assume that that's sort of infinitely fast for now.",
            "So the question to ask is what happens to the workload on node number one?",
            "So we can work this out right?",
            "We just go back to our plain old roofline analysis.",
            "We know there's two MKN flops to do this multiply.",
            "Here's the number of memory transactions.",
            "Here's the intensity right flops divided by memory transactions, and so this is what it was before.",
            "If you were to just do this whole thing on one node after we partition it, things change a little bit, right?",
            "The number of loads and stores I've gotta do for these two matrices is half as much.",
            "And then the number of flops that I've got to do is also half as much.",
            "So if I figure out what the intensity is, what you'll notice is that there an extra term of MK popped in there, so.",
            "In terms of intensity, it's clear that my denominator went up my overall intensity on node one just went down by by this partitioning scheme, and so this is part of what makes strong scaling hard.",
            "It's not just the communication, right communication will hurt you, but even if your network is infinitely fast, you still have the problem that when you try to cut things up, you're making your existing node that you worked so hard to optimize, even less efficient, and so you give up stuff.",
            "On the individual nodes.",
            "And why?"
        ],
        [
            "This winds up hurting you.",
            "Is sort of going to depend on the workload, and so again we have our little roofline analysis here for this operation.",
            "That node number one has to do.",
            "We could have a couple of situations.",
            "So let's say that MKN is really big.",
            "We got a giant giant matrix that we're working on, then we're way over here, right?",
            "We have lots and lots of arithmetic, operational intensity, and so when I break the job up, the intensity is going to go down, but it's OK, node one is still going as fast as it could possibly go in terms of throughput, so so we're not going to worry about it.",
            "The bad case is when we're here, where it's sort of medium sized, where we're sort of hanging out in this ambiguous zone where we're just barely using all of the resources on node one, because now if we cut the work in half, we're going to start wandering down, and so the key thing to realize is that the very thing we thought was going to save us, which is to divvy up the work, and so node one now has less to do is also making node one slower.",
            "Ah.",
            "Yes, so if you had an infinitely fast network, though, you wouldn't need async STD, right, and so this analysis still applies in that case.",
            "So no matter what you do, if you had that real infinitely fast network, you still have to deal with this, and this is.",
            "This is why strong scaling is very hard, so for weak scaling you can get away with it 'cause you just leave your mini batch alone and now async STD could help you well if you have a very fast network is not a problem, but.",
            "The point being is that this is a factor no matter how fast your network is, so you can't forget this.",
            "OK, yeah.",
            "Yes.",
            "Right, so in here everybody has a copy of the parameters and they just exchanged them because we said the network was infinitely fast.",
            "We're just not worrying about that part right now, and so the point is, even when you just totally ignore the network problem, you still have a hard time strong scaling.",
            "Exactly.",
            "Yep.",
            "Yeah, exactly.",
            "And so the question of whether you can scale strongly whether you have any hope at all.",
            "The first thing you should check is which of these two regimes you're in.",
            "If you're hanging out over here, right?",
            "If you compute the intensity and you're right next to this threshold, don't bother like even an infinitely fast network cannot help you.",
            "You are going to lose a lot when you try to go to multiple nodes.",
            "On the other hand, if you're over here, well, at least the first Test has been passed and you have have a chance.",
            "So what I'm going to assume now is that we're somewhere over in this regime, right?",
            "If you're kind of in the middle, you might still want to try to scale up.",
            "But but if we're over here, then it's no problem.",
            "So I'm going to assume that we usually run at the full throughput."
        ],
        [
            "Alright, so this is important even with infinite memory bandwidth we might not be able to stay efficient, and So what we're going to do next though is, say, assuming that the local throughput is nice that we didn't lose anything.",
            "This way we want to analyze the communication and let's figure out how much is that hurting us."
        ],
        [
            "So to do that, there are a lot of ways you can try to get around communications problems.",
            "So you mentioned async STD.",
            "If you're talking about multiple GPU's or multiple cores on one node, there frameworks like Hog Wild to sort of try to deal with synchronization problems and speed things up.",
            "But we're just going to try to get a first cut and how much the network bandwidth problem hurts you.",
            "So what we're going to do is use this same sort of roofline model, but we're going to make the distinction now between local and remote memory, and so before what I had was node one talking to its own memory, and for the purposes of figuring out how efficient node number one is, that's the thing I want to deal with.",
            "But now when I talk about a distributed job where let's say I have to copy W from my neighbor right to keep us In Sync, I."
        ],
        [
            "I want to talk about node one is being like its own little self contained computing core that has a certain throughput that I figured out before and then when it talks to memory to get something done.",
            "I'm not talking about local memory anymore, I'm talking about memory transfers over the network to node #2's memory.",
            "So it's the same hypothetical model where we're assuming that the only constraints are how fast node number one goes and how much you can move across the network."
        ],
        [
            "So back to our trusty roofline model, we can actually analyze the performance the same way, but the slope of this line is now 6 gigabytes a second instead of 300, so that's a little bit of a comedown from what we're used to, and this line is still the same height, right?",
            "We assume no number one is really flying and doing 6 teraflops the 2nd.",
            "So compared to the old view, this is an incredibly skewed system, right?",
            "Lots of compute, but very, very little bandwidth.",
            "And so if we go through and compute what is the location of this?",
            "It's now about 1000 flops per byte.",
            "And so you should walk away from this problem, realizing that if your work everything you do on node number one doesn't do at least 1000 flops for every bite you send over the network, you are in trouble.",
            "You are going to end up down here where you can't, you just can't keep up.",
            "The network is not going to be able to feed you with say, parameter updates or whatever.",
            "Quickly enough for you to keep all the processors busy and you're going to end up with Dead Air.",
            "So.",
            "Basically we need much higher intensity.",
            "An keep in mind that to do this thousand flops, let's say I took a really tiny job with only 1000 flops in it.",
            "That would be too small because depending on what I'm doing, my processor probably can't do 1000 flops at 6 teraflops right?",
            "So I need to have enough work that I really do get this number so, so don't forget that assumption.",
            "OK, so getting to the again the question of what do we do about communication?",
            "Why does it matter or how?",
            "How do we understand that?",
            "Let's take an example where we want to do gradient updates.",
            "In this case where we've got these parameters shared over the two nodes.",
            "So what we want to do is look at node one's world what it sees when it it is a part of this operation and we want to do something that's kind of like a little gradient update.",
            "So D is a matrix that has these deltas from backdrop, right?",
            "And then X is the input to the layer, and when I multiply these two together, it's kind of like an outer product.",
            "I multiply it by the step length for SGD and then I added onto W. So this is like a very simple gradient update that I want to do.",
            "With the understanding that once I do it, I need to somehow orchestrate a transfer of W over to node 2, right?",
            "So that's part of the contract of doing synchronous or asynchronous SGD?",
            "I've got to get this sent.",
            "So.",
            "If we're going to do synchronous, then this is a real problem because I have to wait for it before I can start something else, right?",
            "A little bit about that assumption in just a second, but if you're doing async SGD, as Yoshua pointed out, you could just launch this and then then pretend that it's gone."
        ],
        [
            "So node one has to perform the computation on its local chunk, which is to say it needs to do this matrix times this matrix and then it's got to make this addition and it has to send the update to node number 2."
        ],
        [
            "So the remote memory or the number of flops that we have to do is easy to compute.",
            "MNK for this multiply and then two MK2 add up these.",
            "Sorry to get this from over the addition here.",
            "And then remote memory access, which is the amount of stuff I need to touch on node #2 in this whole operation right?",
            "Sending this data across or Alternatively receiving an update from #2 is 4 MK.",
            "It's the size of this thing and so the overall intensity of this workload, which is the number of flops our machine is is willing to carry out as part of this algorithm for every bite of network traffic.",
            "Is this one?",
            "So basically an over 4 so the mini batch size divided by 4."
        ],
        [
            "If we go back to our roofline model and ask how big does N have to be in order for us to be compute limited, we'll find out that we need end to be something like 4000, so we need a really big mini batch in order to make it so that when I send this.",
            "So that when I send my updates to the other side, or Alternatively I'm receiving updates from the other side, that communication doesn't take so long that I'm just sitting there waiting and so to yoshua's point, this is important if you're actually going to wait, so if you're waiting for this communication to be done to make your update, then obviously you want to make sure that there's enough work for you to be doing that.",
            "You're busy the whole time while that transaction is outstanding, and so that's what this number says."
        ],
        [
            "So.",
            "So getting back to just the question of like async SGD, this is.",
            "This is really the important piece.",
            "The key assumption in here was that we could always stream memory simultaneously with computation, right?",
            "And what I did in this previous thing was I introduced Independency where I can't even do these in parallel right?",
            "Because I've got to wait for this to finish before I can send my update off.",
            "Man, if I were doing a loop here right, I'd have to wait for this guy before I could before I could update W. So this is a problem and there are a few different ways to deal with one that I didn't write down here is to do async SGD where I just keep looping on this no matter what, and I do my memory transactions in parallel.",
            "So essentially what that does is it restores this assumption it gets you back to the nice roofline model case where I can just keep sending things.",
            "Alternatively, I could try to overlap my send with other operations, so usually I have many different layers and so if I'm computing many layers at once, I can be quietly sending the updates for each one while I'm busy and you can do another roofline analysis to determine if that's enough.",
            "And finally you can if you want, and many supercomputers do this is you can actually stream W in the middle of your update, so while your matrix multiply kernel is filling in the results of W. You can be sitting there watching catching the blocks of W as their finished and shipping them off to the other side.",
            "So the reason that I bring this up is that async STD and hog wild and a lot of these are one way of getting around this latency.",
            "But if you want to really scale strongly and not introduce like these confounding factors of convergence rate and so on right, I want the same job to run in lockstep faster.",
            "There are lots of other tricks available to you.",
            "You actually can pull a lot of this off if you're willing to sort of take to heart the roofline model and also try to keep the assumptions true that make that analysis workout.",
            "So if I can actually stream W while it's being computed, and Ann is big enough, I can actually pull this off.",
            "I can actually hide all of those transfers, and I don't have to wait at all.",
            "It just looks like synchronous SGD running at full speed, so this is a more systems oriented way.",
            "Where is async STD.",
            "Hog wild.",
            "A lot of these other algorithms is more like an algorithm IK way of trying to escape the same problems.",
            "Who?",
            "So yes, they are not friendly to use.",
            "Maybe a couple of good examples are things like MKL will happily work on say multiple CPU sockets, which kind of has similar questions of how do I access memory that belongs to other sockets?",
            "There's actually a bottleneck issue there.",
            "Koblas XD will run matrix multiplications, but I believe I believe that it tries to run them out of main memory.",
            "So it's almost like having a network bottleneck through your PCI Express bus and so to figure out if Kublai's XD is worthwhile, you can do an analysis like this and then thinking about real distributed machines.",
            "There are things like Blason, scalapack and you can actually find them doing these types of analysis in the code will look at your matrix multiply.",
            "Do a little analysis of how long it's going to take with different sorts of transfer schemes, and then pick the order that's going to give them the highest intensity.",
            "OK, so anyway the to put this in a box.",
            "Don't forget this overlap assumption right?",
            "Fight it with all you can fight it with asynchronous things if you need to, but in fact if you have a high performance computing cluster there are lots of options available to you to try to optimize your code to make this true and make this roofline analysis workout."
        ],
        [
            "OK.",
            "So the last few minutes we're going to try to put this all together.",
            "So we've seen how partitioning affects our ability, our ability to scale right if we cut up our workload, it hurts our local performance, but we can make an assessment about whether that's important or not.",
            "The changes in the distribution might also introduce a network bandwidth limit that we have to fight against.",
            "And we can use this little roofline model model, which is later a back of the envelope calculation to get a sense for how these issues affect us.",
            "Doesn't make any sense at all, for instance, to be trying to scale to multiple nodes, or should I just call it quits and use my one GPU?"
        ],
        [
            "So here's a sort of suggested design process.",
            "The first is scale up weekly if you if you can.",
            "It's much, much easier than trying to do all these streaming tricks and so on.",
            "So if a double mini batch size would be good for you, go for it.",
            "So once you're, you've decided that you want to scale strongly, you've reached the biggest problem you reasonably want to solve, But you want to go faster then start out by picking a partition of all the work and all of the data across your nodes.",
            "Just pick a candidate that you think seems reasonable.",
            "Estimate the local node Max throughput, which you could either do by a benchmark if you've got one, or you can use the roofline model to help you, and this is like your first Test.",
            "If you're not happy with the output of this, do not proceed.",
            "Don't bother trying to strong scale.",
            "If it turns out that each node is going to have two little work and it's not going to give you an improvement.",
            "And similarly, if you use the local throughput.",
            "You can put this now into a another roofline model.",
            "That sort of represents your cluster, and you can ask what is the overall Max throughput of the work on each of these nodes.",
            "So if you're happy with this, then great you should go back to doing deep learning.",
            "So if you're happy with how you partitioned it and you've got the communication setup awesome, you can get back to real work.",
            "If not, then I'll list a couple of things you can try on the next slide, or just pick a new partitioning because I used some kind of contrived parameters in here so that the partitioning was easy to work with and was illustrative, but you can find cases where one partitioning of the work is really awful.",
            "Where you have really low intensity and your job just won't parallelize no matter what you do.",
            "On the other hand, there are cases where you pick a partitioning in a clever way.",
            "Your intensity is very high and you can go very, very fast, and so you can just use this back of the envelope calculation to quickly iterate through a bunch of ideas and see which ones look good."
        ],
        [
            "So as far as just ways to make this faster, just like on the single node, go hunt for all those operations that are taking up all the time.",
            "That's a given hunt for the partitioning strategy that gives you a very high speed of light for which the speed limit is really high, or the intensity is very high, and that's a good candidate for something to try.",
            "And then look for opportunities to increase the amount of communication and compute overlap.",
            "So look for places where you can overlap work you're doing locali and sending data at the same time or receiving data at the same time, because that makes the assumptions true for our analysis and really engages all the hardware.",
            "If you want to try to use asynchronous optimizers, things like that you can.",
            "And then when in doubt, you can a judiciously apply the metal solution.",
            "And this is where I think I wind up doing this most often, which is thinking about if I want to buy more GPU's or whether we want to look at a faster network, because if we're compute limited then more GPU's might actually help.",
            "I'm going to have more compute floating around, I can hack up my job into separate pieces and more GPU's might go faster, might not, but it, but at least it's a plausible option.",
            "On the other hand, if our roofline analysis says we're bandwidth limited, if no matter how fast our GPU is, we're just sitting there waiting for stuff to show up.",
            "Then, instead of looking at more GPU's, we might want to look at a faster network to sort of innocence, bring those machines closer to us.",
            "And even if you've already got InfiniBand, there other Network Solutions that can run faster."
        ],
        [
            "So, so in conclusion."
        ],
        [
            "The key idea is here.",
            "We're just sort of measure against the speed of light.",
            "Try to figure out what's the fastest your system could ever run, and then strive to get as close to that as you can.",
            "We talk about this really simple performance modeling approach to sort of intuitively understand the tradeoffs, right?",
            "It doesn't account for things like the convergence rate of SGD for different mini batch sizes, or the.",
            "Or how say something like Hog Wild will affect your convergence for different scales, but it will give you a sense for the system side of it and then the algorithm excite is up to you guys as machine learning researchers.",
            "And then the challenging part of multi node training is usually figuring out this partitioning and communication 'cause some partitions are really good and some partitions are really bad.",
            "Very experienced people will have an intuition for which one to try, but you can use this back of the envelope calculation to sort of get you started."
        ],
        [
            "So I just want to say thank you for your guys time also to Greg, Dima, San Bryan, Catanzaro at Baidu who have sort of brought me over to thinking this way a little bit as opposed to just kind of intuiting all the things that are going on.",
            "So they're very useful references.",
            "And if you want to know more about roofline there are a couple here and this is a HPC paper we had a couple a little while ago for that plot so thank you guys."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very hard to figure out what was going to be useful for the crowd here.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is systems and parallel the parallelization for deep learning.",
                    "label": 0
                },
                {
                    "sent": "Basically talking about how do you make your your deep learning experiments run faster than the reason that I think this is a challenging topic.",
                    "label": 0
                },
                {
                    "sent": "That's kind of difficult to get our arms around.",
                    "label": 0
                },
                {
                    "sent": "Is that AI and deep learning of course depends very heavily on good systems knowledge, right?",
                    "label": 1
                },
                {
                    "sent": "We need our experiments to run fast and deep learning especially really likes really likes these big dense operations that run great on hardware.",
                    "label": 0
                },
                {
                    "sent": "But to actually build a system, if you want to build a bleeding edge, highly scalable system, or maybe you have a job just running on your own GPU at home that you wish would run in less than a month.",
                    "label": 0
                },
                {
                    "sent": "There are all these tools out there to solve the problem, and so if you're a systems expert and you've been studying systems your whole life, you know all of the tools in the store you've been working with them constantly.",
                    "label": 0
                },
                {
                    "sent": "You know how they what.",
                    "label": 0
                },
                {
                    "sent": "Therefore, when to use them, how they can break.",
                    "label": 0
                },
                {
                    "sent": "When they're not appropriate, and even if you're not really sure exactly what the right tool is, you at least know which part of the story you should go looking in.",
                    "label": 0
                },
                {
                    "sent": "Whereas me when I end up at like Home Depot, I spend most of the time just kind of like wandering around, 'cause I'm not really sure.",
                    "label": 0
                },
                {
                    "sent": "And so when you start as a sort of a machine learning researcher or deep learning researcher, and you want to figure out how do you make your your systems faster, I think there's this sort of same sensation that it's just not clear where to look because it's so complex.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What I wanted to do instead is.",
                    "label": 1
                },
                {
                    "sent": "Is think about what are sort of the simple tools that can stand in for this level of experience 'cause most of the experts on this topic that I talked to I think they have this sense of where to look not so much from from training per say from having read a whole bunch of textbooks that say do this.",
                    "label": 0
                },
                {
                    "sent": "Do this do this, but because they've solved lots and lots and lots of systems problems, they've seen all kinds of failures before.",
                    "label": 0
                },
                {
                    "sent": "And so the best way to actually learn this stuff and really get it down path is to see a huge number of examples overtime to wind up building lots and lots of deep learning systems.",
                    "label": 1
                },
                {
                    "sent": "HPC Systems, cloud systems and just kind of knowing building up that Sixth sense for what is fast and what is slow.",
                    "label": 0
                },
                {
                    "sent": "But since we don't have time for that.",
                    "label": 0
                },
                {
                    "sent": "Instead, going to sort of narrow the scope of this.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do instead is talk about some simple tools.",
                    "label": 0
                },
                {
                    "sent": "What I want to try to get at are some conceptual tools that you can use to understand or strategize about how to make your deep learning systems run faster or be more scalable so you can think of this is sort of starting out with your measuring stick for getting some information and some tools for planning and deciding what you're going to do next as you try to make your deep learning system faster.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I guess break this into a few pieces and actually this this should be pretty short and I haven't taught this specific material before, so if you have questions along the way about the systems aspect or how to how these things relate, definitely just kind of keep the questions coming.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk for a minute about sort of the basic motivations, like why do we care about any of this?",
                    "label": 0
                },
                {
                    "sent": "So just in case you needed a reason to make your jobs run faster.",
                    "label": 0
                },
                {
                    "sent": "And then I'll also talk a bit about how do we talk about performance on a single node.",
                    "label": 1
                },
                {
                    "sent": "So if you just have one GPU in your workstation, or heaven forbid you guys are trying to develop deep learning software on your laptops.",
                    "label": 0
                },
                {
                    "sent": "So you have some idea for what are the bottlenecks and what are the performance tradeoffs you have to make when you're just talking about a single node.",
                    "label": 0
                },
                {
                    "sent": "And then we want to talk about.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Multiple nodes where things kind of get a little bit more interesting, but in some ways simpler because you have a lot more control over network traffic and then the sort of things happening on your machine.",
                    "label": 0
                },
                {
                    "sent": "Whereas when it comes to a piece of hardware that's very complex and has a lot of stuff going on underneath it can be very opaque when you're trying to make things run faster.",
                    "label": 0
                },
                {
                    "sent": "So the first reason actually, maybe even the most important reason to try to make your jobs run faster in deep learning, is the fact that our workflow is researchers is very much a kind of guided exploration that we don't really know, like.",
                    "label": 0
                },
                {
                    "sent": "What is the best model for image NET?",
                    "label": 0
                },
                {
                    "sent": "We don't really know what is the best model for speech recognition, and So what we do is we start out with something that kind of feels right.",
                    "label": 0
                },
                {
                    "sent": "That has the right structure in the right parts, and then we launch an experiment.",
                    "label": 0
                },
                {
                    "sent": "And then we wait and we hang out.",
                    "label": 0
                },
                {
                    "sent": "We read papers.",
                    "label": 0
                },
                {
                    "sent": "We maybe try coding up some other experiments in parallel, but mostly we're just hanging out waiting for this to finish to see which things worked in which things failed, and my underfitting, like my baby, my model is too small and my overfitting 'cause my model is now too big.",
                    "label": 0
                },
                {
                    "sent": "did I hit some kind of nasty optimization problem that I now want to go back and visualize?",
                    "label": 0
                },
                {
                    "sent": "We're sort of sitting around waiting for all of this feedback in order to decide what to do next.",
                    "label": 0
                },
                {
                    "sent": "And so if we could just get that end to end experiment time down, that would be really great, because in our experience, one of the things that.",
                    "label": 0
                },
                {
                    "sent": "Really, Gates research progress that determines how quickly we can move toward our goal is the speed with which we can get around this cycle from the from the moment we have some idea for what we're going to do to the time we can get it into some code and then get results to sort of jog the next idea an the sort of sad reality is that this little innocent PowerPoint arrow here is something like weeks or months, so you quickly jump here 'cause you coded it up in Pilar and and then you're stuck for about a month.",
                    "label": 1
                },
                {
                    "sent": "And then you get to go back around quickly.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so that's one big reason, but the other one that I think is maybe a more sort of high level or strategic reason to worry about whether your model scale is that scaling is really an argument for deep learning itself.",
                    "label": 0
                },
                {
                    "sent": "One of the reasons I think that deep learning has been so successful is partly because there's been genuine progress on the problems, right?",
                    "label": 0
                },
                {
                    "sent": "We're much better at debugging optimization issues and sort of dealing with all the little exceptional cases of deep learning now than we used to be, we've discovered.",
                    "label": 0
                },
                {
                    "sent": "Like going around this cycle many times we've discovered a lot of different models that in fact are very good for things like computer vision and speech and language.",
                    "label": 0
                },
                {
                    "sent": "So we've made a lot of good discoveries.",
                    "label": 0
                },
                {
                    "sent": "But I think the reason a lot of the results today are so eye popping is actually because of scalability.",
                    "label": 0
                },
                {
                    "sent": "The fact that with previous algorithms like if you did a K nearest neighbor or support vector machine, or maybe you did boosting with trees or something like that.",
                    "label": 0
                },
                {
                    "sent": "If I gave you a lot more data and a lot more computing power, you could make a bigger model, but at some point your machine learning algorithm would just start to see diminishing returns.",
                    "label": 0
                },
                {
                    "sent": "You'll get more accurate for a little while, but then it would slow down.",
                    "label": 0
                },
                {
                    "sent": "And So what seems to be different about deep learning is that we've found ourselves on this different trajectory that for a very long time in the past I mean the 80s nineties and so on.",
                    "label": 0
                },
                {
                    "sent": "Deep learning was down here, we just didn't have enough data.",
                    "label": 1
                },
                {
                    "sent": "We didn't have enough computing power to make progress and build state of the art systems.",
                    "label": 0
                },
                {
                    "sent": "And so all these sort of hand engineered things or these more traditional machine learning systems were always better and you just couldn't beat them no matter what you did.",
                    "label": 0
                },
                {
                    "sent": "And so I think what we're seeing in more recent history is that as time and effort have gone along into this problem and we've added more data in computing power, we are slowly working our way up this curve.",
                    "label": 0
                },
                {
                    "sent": "And you're now seeing deep learning kind of overtake all of these other systems.",
                    "label": 0
                },
                {
                    "sent": "And so in the past deep learning maybe hasn't been a great competitor.",
                    "label": 0
                },
                {
                    "sent": "I think right now we're seeing deep learning sort of crossover to be maybe a close second and a lot of applications to suddenly being the top performer in a lot of applications and in the future think we will be up here.",
                    "label": 0
                },
                {
                    "sent": "So the reason that you should worry about making your machine learning job more scalable is that for one, if you put some effort into scalability, you can push your way down this axis and hopefully get even better performance.",
                    "label": 0
                },
                {
                    "sent": "That's one reason.",
                    "label": 0
                },
                {
                    "sent": "But the more sort of future oriented thought is that the models that you choose today and that you're working on.",
                    "label": 0
                },
                {
                    "sent": "If they match the hardware very well and they're very scalable, these are models that are going to get better almost for free as we capture more data and more computing power, and so one of the things that's been nice about deep learning is that a deep, dense Relu network has just been getting better and better and better.",
                    "label": 0
                },
                {
                    "sent": "Even though you haven't had to do a whole lot in terms of fiddling with the structure.",
                    "label": 0
                },
                {
                    "sent": "So the question, though is without actually running that experiment, how do you know which models are going to work?",
                    "label": 0
                },
                {
                    "sent": "How do you have a sense for which models are likely to scale and which ones are not?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In any event, so I think we'd like to try to find models apriori that are likely to benefit from this trend overtime, but it's tough to figure out if you maybe haven't built up this instinct yet.",
                    "label": 0
                },
                {
                    "sent": "For for what is going to be faster, slow?",
                    "label": 0
                },
                {
                    "sent": "Maybe, and so if you're going to build hardware.",
                    "label": 0
                },
                {
                    "sent": "Hopefully you have a very good argument for why the hardware will make things better, and some of the tools we'll talk about it will be helpful for making that decision.",
                    "label": 0
                },
                {
                    "sent": "So maybe the more common case that a lot of the folks in this room will have to deal with is you'll see a new GPU come out and you're trying to decide, like should I upgrade, it has like 20% more memory bandwidth, but it's twice as many teraflops.",
                    "label": 0
                },
                {
                    "sent": "Is that a good deal?",
                    "label": 0
                },
                {
                    "sent": "And if I have to decide, should I buy 2 GPU's which doubles the cost?",
                    "label": 0
                },
                {
                    "sent": "Is that a good idea?",
                    "label": 0
                },
                {
                    "sent": "And so some of the tools will talk about help you make that decision?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are a bunch of different ways to try to make things faster, so certainly we can change the software, right?",
                    "label": 1
                },
                {
                    "sent": "Fifth, running slow, we can dig into our code, go look for bottlenecks and try to make the software faster and to go through this point.",
                    "label": 1
                },
                {
                    "sent": "Certainly we could change the hardware for using CPU's.",
                    "label": 0
                },
                {
                    "sent": "Maybe we should be switching to GPU's.",
                    "label": 0
                },
                {
                    "sent": "I don't think I'm totally agnostic to this, but I hold out the possibility that there are many jobs in their world that will actually run just fine on a CPU, and to know whether you should have a GPU for your particular job.",
                    "label": 0
                },
                {
                    "sent": "Is in some sense a design decision, and then there is the even further question that if a GPU is not enough, even many GPS is not enough.",
                    "label": 0
                },
                {
                    "sent": "Do you want to do something like start looking at in FGA or you want to go even further in terms of capital investment and time and go for something like an ASIC?",
                    "label": 0
                },
                {
                    "sent": "Go actually design A chip to do deep learning.",
                    "label": 0
                },
                {
                    "sent": "So those kinds of decisions are pretty tough and you can't just dive into them unaware.",
                    "label": 0
                },
                {
                    "sent": "And then finally, maybe the one we're most familiar with actually as researchers is that we can change the model in the algorithm, and this is.",
                    "label": 0
                },
                {
                    "sent": "In a way, the hardest solution because it creates this sort of code design problem that on the one hand you're chasing accuracy.",
                    "label": 0
                },
                {
                    "sent": "You're trying to keep your model very accurate and get the best performance while also dealing with this other constraint that you want it to run fast on hardware.",
                    "label": 0
                },
                {
                    "sent": "So there are lots of things that we might like to do, for instance like attentional mechanism.",
                    "label": 0
                },
                {
                    "sent": "We were having a conversation yesterday about attentional mechanisms, where you might really like to have this attention scheme that looks around in a big video.",
                    "label": 1
                },
                {
                    "sent": "But unfortunately, these sorts of things that access data in unusual or unpredictable ways also don't fit well with the hardware, and so trying to make that tradeoff of getting good performance and building an innovative system will also matching up with the hardware becomes very challenging, so some of the tools we're going to talk about actually help a little bit with this, so that while you're busy optimizing for accuracy, you kind of have a sort of soft prior about what to do for the system side.",
                    "label": 0
                },
                {
                    "sent": "So the way that we're going to do this is we're actually going to talk about performance modeling, so.",
                    "label": 0
                },
                {
                    "sent": "In some sense, what I'm going to talk about for the entire remainder of the time here is really easy.",
                    "label": 0
                },
                {
                    "sent": "It's sort of deceptively simple.",
                    "label": 0
                },
                {
                    "sent": "There's going to be a whole bunch of plug and chug later on.",
                    "label": 0
                },
                {
                    "sent": "That's going to feel really pedantic.",
                    "label": 0
                },
                {
                    "sent": "And so when I first saw this was like, OK, that's nice.",
                    "label": 0
                },
                {
                    "sent": "And kind of like sort of ignored it, but the more I started playing with this in some ways.",
                    "label": 0
                },
                {
                    "sent": "Just to put these slides together more, start to realize like this will embody a lot of the intuition in the rules of thumb that very experienced systems designers are using without knowing it.",
                    "label": 0
                },
                {
                    "sent": "They're making a lot of the same judgments.",
                    "label": 0
                },
                {
                    "sent": "That this tool will let you make.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "So we'll work some examples through and so you forgive me.",
                    "label": 0
                },
                {
                    "sent": "A little bit of the plug and chug that we're going to go through.",
                    "label": 0
                },
                {
                    "sent": "But if you get a chance to do this on like a cocktail napkin with your own GPU, for example, and your own models, think you'll find that kind of can bring you some insights about what's happening.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So just as a preliminary, I want to point out that one of the reasons that optimizing software is maybe not the thing we have a lot of experience with is deep learning researchers is because deep learning has become very popular and so all of the things that we like to use for building neural networks have been picked up by NVIDIA and Intel and so on.",
                    "label": 0
                },
                {
                    "sent": "And people are building beautiful libraries for us and doing all of the really nasty low level optimization to make things fast.",
                    "label": 0
                },
                {
                    "sent": "So I won't talk in a lot of depth.",
                    "label": 0
                },
                {
                    "sent": "About all of the sort of nitty gritty hardware aspects.",
                    "label": 0
                },
                {
                    "sent": "If you want to make things fast.",
                    "label": 0
                },
                {
                    "sent": "So instead.",
                    "label": 0
                },
                {
                    "sent": "Just want you to keep in mind the fact that deep learning is this really nice architecture where we sort of have Legos.",
                    "label": 0
                },
                {
                    "sent": "We can put together and a lot of those Legos are really fast, and so we'll try to analyze performance in terms of how fast these different operations run.",
                    "label": 0
                },
                {
                    "sent": "Sort of small collections of these operations are likely to run.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we actually measure performance?",
                    "label": 0
                },
                {
                    "sent": "So it's just sort of a definition here.",
                    "label": 0
                },
                {
                    "sent": "So if I give you a fixed problem size, let's say I want to do a convolution in a convolutional neural network.",
                    "label": 1
                },
                {
                    "sent": "I need to give you a bunch of hyperparameters, right?",
                    "label": 0
                },
                {
                    "sent": "I need to give you the filter size.",
                    "label": 0
                },
                {
                    "sent": "I need to tell you how many input Maps there are.",
                    "label": 0
                },
                {
                    "sent": "I need to tell you how many output Maps you're going to need to compute.",
                    "label": 0
                },
                {
                    "sent": "You need to decide is it double precision or fluent, single precision, or even like 16 bit precision floating point.",
                    "label": 0
                },
                {
                    "sent": "And the question is, is what are we actually trying to optimize when we go to make this system faster?",
                    "label": 0
                },
                {
                    "sent": "So to try to do that, let's assume much of the time that we're going to hold the number of operations fixed.",
                    "label": 0
                },
                {
                    "sent": "If we could do that, we're just talking about a single instance of running convolution, then what we're going to try to maximize is the throughput, the number of operations that we can get done every 2nd, and so not surprisingly, right.",
                    "label": 0
                },
                {
                    "sent": "If the number of operations is held fixed, then making this large means you're making your running time really small.",
                    "label": 0
                },
                {
                    "sent": "So we're going to talk a lot about throughput for the rest of this, and it just means how many operations I'm pushing through per second and no matter what I'm doing, obviously finishing more of my operations is better.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so one caveat with this, though, that I'll warn you about, as I did say.",
                    "label": 0
                },
                {
                    "sent": "This goes against all of the things we normally do right.",
                    "label": 0
                },
                {
                    "sent": "The one thing that we do a lot is fiddle with the model, change the hyperparameters, and so on.",
                    "label": 0
                },
                {
                    "sent": "This is like part of what deep learning research has been doing that this actually made a lot of progress, and so you should keep in mind as you're doing some of these analysis that changes you make to the system that alter the number of operations or that alter things like the convergence time will have an effect.",
                    "label": 0
                },
                {
                    "sent": "And so I'll go into this in a little bit more detail later, as a sort of illustrative example, but just keep in mind that throughput in terms of the number of operations I get done per second doesn't necessarily consider things like convergence rate, so I might have wonderful throughput, But if my algorithm is taking longer to converge now because of whatever I changed, it really doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "So in a sense, throughput is a sort of gameable metric.",
                    "label": 1
                },
                {
                    "sent": "So, so it's something that you can actually see.",
                    "label": 0
                },
                {
                    "sent": "If you want to maximize throughput, the answer is to get an enormous number of GPU's and make your mini batch something like a million.",
                    "label": 0
                },
                {
                    "sent": "And then and then your your throughput will be incredible.",
                    "label": 0
                },
                {
                    "sent": "It will be something you know like 1000 GPU's running at peak and the number of operations you're finishing will be awesome.",
                    "label": 1
                },
                {
                    "sent": "But the wall time for your whole experiment will be the same as if you just had a couple of GPU's, so keep that in mind.",
                    "label": 0
                },
                {
                    "sent": "Don't don't get fooled by by what we're optimizing here.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's start talking about the principles for how we do some of this analysis on one machine or one node.",
                    "label": 0
                },
                {
                    "sent": "So when I talk about a node, actually I'm sort of referring to like one CPU socket or one GPU, one FGA in some sense is like one enclosed unit of computation with its own address space and local memory.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the best question to ask yourself when you start this out and this will motivate why we're we're doing.",
                    "label": 0
                },
                {
                    "sent": "Some analysis here is to keep asking how much is there that could be gained.",
                    "label": 1
                },
                {
                    "sent": "So if it turns out that my system is already very well optimized, which it will be, for instance, if you're using like packages to do, very large matrix multiplies that are already well optimized, then there's some question about whether it's worth it.",
                    "label": 0
                },
                {
                    "sent": "And so one of the important things about doing the back of the envelope analysis.",
                    "label": 0
                },
                {
                    "sent": "I'm going to teach you to do here is that you keep rechecking it as you move along and make improvements to ask yourself is it really worth spending another week reading about optimization techniques.",
                    "label": 0
                },
                {
                    "sent": "When my job only takes 4 days to run now, so there's going to be some question of whether you want to stop optimizing and go back to deep learning or not.",
                    "label": 0
                },
                {
                    "sent": "So to answer this, we're going to talk about ways to sort of assess how much potential gain you can get, and then we'll go back and we say we're going to look for the biggest potential gains and will start optimizing those things until we get tired.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One thing that sort of irks the systems people I know is how we measure how fast our machine learning work is going, because we often go to conferences like Man, I spent like a month learning CUDA and I made this really amazing kernel and it's like 4 times faster than the previous code I was using and this drives all of my systems friends crazy because they say your baseline is not how slow your code runs.",
                    "label": 0
                },
                {
                    "sent": "If you really terrible code to begin with and now your new code is running four times faster, that is just not interesting to a systems researcher 'cause they don't, they don't know how to interpret this.",
                    "label": 0
                },
                {
                    "sent": "There's no way to compare this to all the work that's been done before, so if you ran your code in Theano and someone else ran it in torch, and someone else ran it in cafe, I don't know how to compare you.",
                    "label": 0
                },
                {
                    "sent": "If you just tell me how much faster you're going today.",
                    "label": 0
                },
                {
                    "sent": "So on the one hand, telling people that you got a 10X speedup or a 5X speedup.",
                    "label": 0
                },
                {
                    "sent": "This is awesome for you.",
                    "label": 0
                },
                {
                    "sent": "This is really great right?",
                    "label": 0
                },
                {
                    "sent": "Because it totally changes how you do you work.",
                    "label": 0
                },
                {
                    "sent": "If you can achieve that speed up.",
                    "label": 1
                },
                {
                    "sent": "However, this is also a totally non actionable metric that changes how you do your work today, but it doesn't tell you what to do next, so you don't know.",
                    "label": 0
                },
                {
                    "sent": "Can you get another 10 X?",
                    "label": 0
                },
                {
                    "sent": "So how do you know if there's actually more to be done?",
                    "label": 1
                },
                {
                    "sent": "So instead.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Add the baseline that we want to measure everything against.",
                    "label": 0
                },
                {
                    "sent": "When we're asking what's the next thing to do.",
                    "label": 0
                },
                {
                    "sent": "Should I buy another GPU?",
                    "label": 0
                },
                {
                    "sent": "Should I spend more time optimizing?",
                    "label": 0
                },
                {
                    "sent": "Should I be investigating novel hardware?",
                    "label": 0
                },
                {
                    "sent": "What should I do?",
                    "label": 0
                },
                {
                    "sent": "Our baseline is the fastest sarcodes could ever possibly run, so we sort of call this the speed of light measurement.",
                    "label": 1
                },
                {
                    "sent": "It's the it's the speed limit.",
                    "label": 0
                },
                {
                    "sent": "The cosmic speed limit for your neural network, and the speed limit is going to be set by the hardware platform that you're running on, and so if you knew how to calculate this if you knew what the maximum potential throughput was, then you would know that as you started to get close to that, you should just give up because you're getting close to the speed of light.",
                    "label": 0
                },
                {
                    "sent": "And you're not going to be able to go faster than that.",
                    "label": 0
                },
                {
                    "sent": "And so just is like rules of thumb to get a sense for what people will be happy with if you're running at about half the speed of light, you're doing pretty good like you're doing a good job.",
                    "label": 0
                },
                {
                    "sent": "If you're going to see because there's about a potential 2X speedup left, which means there you could buy another GPU or double your cluster and get that potentially.",
                    "label": 0
                },
                {
                    "sent": "So 2X is not nothing, but it's also not something you should feel bad about, 'cause you're certainly not going to get all the way till 1.0 C. And then if you're getting something like .8 C, right, you're getting 80% of the speed of light throughput right then?",
                    "label": 0
                },
                {
                    "sent": "Then you're doing great.",
                    "label": 0
                },
                {
                    "sent": "'cause that means there's only about 25% speedup left on the table an as in physics.",
                    "label": 0
                },
                {
                    "sent": "Usually it's going to cost you more to go faster if you're already close to the speed of light.",
                    "label": 1
                },
                {
                    "sent": "So if you keep track of this metric for yourself and always benchmark yourself against the fastest you could possibly run, then you'll know how much effort it's going to take you to go even faster.",
                    "label": 0
                },
                {
                    "sent": "And if it's time to start looking at something else.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I want to put this in a box here just to remind you so, so don't worry about how slow your code was before.",
                    "label": 0
                },
                {
                    "sent": "Try to run an analysis to figure out what's the fastest you could run and benchmark your progress against that.",
                    "label": 0
                },
                {
                    "sent": "So our goal here, the tools that we're going to start cranking through is to try to figure out for a single node, like your GPU for example.",
                    "label": 0
                },
                {
                    "sent": "How do you estimate the speed of light?",
                    "label": 1
                },
                {
                    "sent": "How do I know in advance without trying to run tons and tons and tons of benchmarks?",
                    "label": 1
                },
                {
                    "sent": "What is the fastest I could possibly go?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's talk now about performance modeling that will help us answer this question.",
                    "label": 0
                },
                {
                    "sent": "So let's suppose that I give you a fixed amount of computation that you have to perform.",
                    "label": 1
                },
                {
                    "sent": "So I tell you, please do this convolution or this matrix multiply as fast as you can, and I'm going to give you all the hyperparameters so you know exactly how much work you have to accomplish.",
                    "label": 1
                },
                {
                    "sent": "What we want to do is estimate the maximum potential throughput.",
                    "label": 0
                },
                {
                    "sent": "We want to know if I did everything right if my hardware were perfect, what's the fastest I could possibly go?",
                    "label": 0
                },
                {
                    "sent": "And this is incredibly hard to do in general, so if you have an X86 processor there, there's all kinds of incredible stuff going on in there to make your programming faster and more convenient, and so figuring out what it's going to do and when and when you're going to run afoul of it is really, really hard.",
                    "label": 1
                },
                {
                    "sent": "So nevertheless, we're going to use a really simple scheme here.",
                    "label": 0
                },
                {
                    "sent": "That's very quick.",
                    "label": 0
                },
                {
                    "sent": "You can do it on a cocktail napkin an it'll give you intuition, and so the thing I hope that you will take away from this tool is that we're going to be talking about something that's a little bit like an economics model.",
                    "label": 0
                },
                {
                    "sent": "How many people have taken the concourse?",
                    "label": 0
                },
                {
                    "sent": "Thank you, good fraction of people.",
                    "label": 0
                },
                {
                    "sent": "So economics you often have these sort of models like supply curves and demand curves and things and these are not meant to be sort of hyper quantitative to tell you exactly how many widgets you will sell tomorrow.",
                    "label": 0
                },
                {
                    "sent": "They're meant to give you a sort of stylized intuitive you about what's going on.",
                    "label": 0
                },
                {
                    "sent": "So if you raise the price or you lower the price, you have some sense for how that will affect the world, even if you're not quite sure how much and so the things we're going to talk about, I hope will kind of give you that ability.",
                    "label": 0
                },
                {
                    "sent": "But for estimating this speed of light.",
                    "label": 0
                },
                {
                    "sent": "So, Armada.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of computation, so the analysis we're going to do is going to be built on a wildly oversimplified picture of how a computer gets its job done.",
                    "label": 0
                },
                {
                    "sent": "And So what we're going to do is represent our computer with computation and memory only.",
                    "label": 1
                },
                {
                    "sent": "So that's all we have.",
                    "label": 0
                },
                {
                    "sent": "We have a computing core here that runs at some speed, some number of flops or operations it can do per second and then we've got some memory living over here off the chip an we've got a pipe connecting them and the only limitations we're going to worry about in the hardware is how fast our computing core can go and how much data we can ship back and forth across this bus per second.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For a current GPU this year I rounded these numbers off a little to make the math easier later.",
                    "label": 0
                },
                {
                    "sent": "You know you can get a GPU that will do six teraflops very happily.",
                    "label": 0
                },
                {
                    "sent": "In an ideal case, and the memory bandwidth to the local memory on the GPU is something like 300 gigabytes a second.",
                    "label": 0
                },
                {
                    "sent": "Even as I say these numbers now, this is really crazy compared to what we had even a few years ago.",
                    "label": 0
                },
                {
                    "sent": "But as you get new numbers for this, you'll be able to update your analysis very quickly.",
                    "label": 0
                },
                {
                    "sent": "So the key assumption in this model is not just that we have only two constraints that that's the simplifying part.",
                    "label": 0
                },
                {
                    "sent": "The key assumption that we're going to make that will make all of our analysis possible is that I can always stream in memory simultaneously with doing computation.",
                    "label": 1
                },
                {
                    "sent": "So while the computing core is busy doing things, I'm assuming that data can be going back and forth freely through memory.",
                    "label": 0
                },
                {
                    "sent": "And this is actually a somewhat strong assumption.",
                    "label": 0
                },
                {
                    "sent": "If you sit down to write a very highly optimized GPU kernel, you will find that you cannot actually pull this off perfectly.",
                    "label": 0
                },
                {
                    "sent": "You'll find that you're holding things in cash, and you need to be getting another piece of memory to keep up this transaction so that it will be ready when you need it.",
                    "label": 0
                },
                {
                    "sent": "But the cache is full and you're not done with your cash yet.",
                    "label": 0
                },
                {
                    "sent": "'cause it's feeding your computation, and so you sort of have to make a trade off.",
                    "label": 0
                },
                {
                    "sent": "You have to give up.",
                    "label": 0
                },
                {
                    "sent": "On this assumption of being able to stream memory simultaneously with doing computation so so, keep in mind that this assumption is there, but it's actually not too terrible for a lot of the workloads that we see.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we actually ran a sequence of operations on this little stylized computer, and then we loaded it up in our profiler and looked at like a timeline view of what was going on, we might see something like this.",
                    "label": 1
                },
                {
                    "sent": "So we might start operation number one here, and while operation number one is running, there's a bunch of compute that's just happening on our little computing core.",
                    "label": 0
                },
                {
                    "sent": "And then there are a bunch of loads and stores happening, so the operation is asking for things from memory, and it's also.",
                    "label": 0
                },
                {
                    "sent": "Going to store the results back to memory.",
                    "label": 0
                },
                {
                    "sent": "And the key assumption is that these are completely overlapped and you can see that this is a little nonsensical right?",
                    "label": 0
                },
                {
                    "sent": "Because clearly when I finish my compute over here, I have to store my result at the end, but I'm just assuming that that's all totally asynchronous and it can happen anywhere in here.",
                    "label": 0
                },
                {
                    "sent": "So this is the simplifying assumption I'm throwing away all the data dependencies that make analysis confusing and just assuming we can do everything in parallel.",
                    "label": 0
                },
                {
                    "sent": "And similarly, there might actually be operations where the loading and storing part is the slowest, and so if you ran another one right afterward, you might see that you're actually doing more memory transactions than than computation.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's do a quick example here, so let's start using this little computer model to make some so or early assessments of how different operations behave.",
                    "label": 0
                },
                {
                    "sent": "So what I want to do is compute a simple matrix vector multiplication.",
                    "label": 0
                },
                {
                    "sent": "So I've got this matrix A which I'm storing in single precision.",
                    "label": 0
                },
                {
                    "sent": "It's all good.",
                    "label": 0
                },
                {
                    "sent": "Deep learning codes do.",
                    "label": 0
                },
                {
                    "sent": "And this is an M by N matrix an what I want to do is multiply this by an N dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be.",
                    "label": 0
                },
                {
                    "sent": "So to figure this out.",
                    "label": 0
                },
                {
                    "sent": "We we need to figure out how much it's going to load in store from memory.",
                    "label": 1
                },
                {
                    "sent": "So if you look here if I've got 4 bytes for every entry, I have MN entries that I've got a load for a right, just obligated to touch every element of A and then I have N entries in this vector.",
                    "label": 0
                },
                {
                    "sent": "So I've gotta load them both groups and I kind of goofed here.",
                    "label": 0
                },
                {
                    "sent": "There should be a two in front of this, 'cause I also have to store the output.",
                    "label": 0
                },
                {
                    "sent": "And then the up there we go.",
                    "label": 1
                },
                {
                    "sent": "So how much do we have to store?",
                    "label": 0
                },
                {
                    "sent": "So this is just the loading part.",
                    "label": 0
                },
                {
                    "sent": "Here we go.",
                    "label": 0
                },
                {
                    "sent": "And so we also have to store 4 bytes times M, which is the dimensionality of the output.",
                    "label": 0
                },
                {
                    "sent": "And then the number of floating point operations that are compute core actually has to execute is actually M * 2 N minus one, and for most of this talk I'm going to drop these little minus ones.",
                    "label": 0
                },
                {
                    "sent": "There's actually a little missing addition on the end of your summation here and just call this two MN floating point operations.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "What can we do with this?",
                    "label": 0
                },
                {
                    "sent": "Let's just take a specific instance, right?",
                    "label": 0
                },
                {
                    "sent": "Let's not worry about the general case yet.",
                    "label": 0
                },
                {
                    "sent": "Let's suppose I tell you that M is 1024 and an is 512, so we kind of have a bit of a sort of tall skinny matrix is about twice as tall as it is wide.",
                    "label": 0
                },
                {
                    "sent": "And what we want to know is what's the best possible throughput we could achieve for this on our little hypothetical hardware platform?",
                    "label": 1
                },
                {
                    "sent": "So we can work out how much memory traffic we have, so it's just 4 bytes times the number of loads I have to do in the number of bytes that I have to store so that it turns out to be like about 2 megabytes.",
                    "label": 0
                },
                {
                    "sent": "So not too much.",
                    "label": 0
                },
                {
                    "sent": "And then we can compute the number of floating point operations we've got to do so that's just two MN.",
                    "label": 0
                },
                {
                    "sent": "Turns out that's about a million flops.",
                    "label": 0
                },
                {
                    "sent": "And then we can ask what's the running time going to be?",
                    "label": 0
                },
                {
                    "sent": "According to our very simplified computer model.",
                    "label": 0
                },
                {
                    "sent": "So basically it's going to be the maximum right?",
                    "label": 0
                },
                {
                    "sent": "'cause I have these two independent pieces of work that's going to be the maximum of how long it takes to load that 2.1 megabytes at 300 gigabytes a second.",
                    "label": 0
                },
                {
                    "sent": "And how long it takes to do my million flops at a rate of 6 teraflops.",
                    "label": 0
                },
                {
                    "sent": "So if I work those out it turns out it's going to take about .16 microseconds to do the compute part about 7 microseconds to do the loads and stores.",
                    "label": 1
                },
                {
                    "sent": "And the important thing to notice here is that this is the controlling factor, right?",
                    "label": 0
                },
                {
                    "sent": "The Max doesn't care about what's in here.",
                    "label": 0
                },
                {
                    "sent": "And so the throughput of this whole thing, which is about 142 gigaflops.",
                    "label": 0
                },
                {
                    "sent": "Is totally totally ignoring how much compute I have available, right?",
                    "label": 0
                },
                {
                    "sent": "I can make very substantial changes to what's going on here, and I don't alter the answer at all.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, how many flops I did or how fast my my compute unit was is totally irrelevant for determining this number, which in some sense is our speed limit.",
                    "label": 0
                },
                {
                    "sent": "The fastest we can possibly go for this problem?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In order to kind of quantify this quickly, so rather than kind of having to go through all this analysis every time, it turns out that there's a sort of convenient quantity for thinking about these things.",
                    "label": 0
                },
                {
                    "sent": "That's called the intensity.",
                    "label": 0
                },
                {
                    "sent": "So the way that intensity is defined is just to take the number of operations you need to do in deep learning.",
                    "label": 0
                },
                {
                    "sent": "It's usually floating point operations, but it could be integer or whatever.",
                    "label": 0
                },
                {
                    "sent": "And divide by the number of bytes that you need to load or store.",
                    "label": 1
                },
                {
                    "sent": "So this is the number of operations per byte.",
                    "label": 0
                },
                {
                    "sent": "So for the previous scenario, you can take your million operations divided by 2.1 megabytes and you get 1/2 of a flop per byte.",
                    "label": 0
                },
                {
                    "sent": "So so this is what we would call a very low intensity problem and low intensity problems will see are always bottlenecked on memory.",
                    "label": 0
                },
                {
                    "sent": "So This is why compute doesn't matter in this case because intensity is really low for every bite.",
                    "label": 0
                },
                {
                    "sent": "For every flop we want to do, we actually have to load two bites from memory and that takes a long time.",
                    "label": 0
                },
                {
                    "sent": "So this concept.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This idea that there's this Max and that some things matter and some things do not, is embodied in a really cool sort of stylized model introduced by by Williams and Co.",
                    "label": 0
                },
                {
                    "sent": "Authors in 2009, and this is sort of meant to be a pedagogical model, right?",
                    "label": 0
                },
                {
                    "sent": "It's meant to give you some intuition for what's going on that you can use quickly.",
                    "label": 0
                },
                {
                    "sent": "So these two constraints from our computing platform are little computer model or really easy to draw.",
                    "label": 1
                },
                {
                    "sent": "If the horizontal axis is intensity, which we just defined and the vertical axis is throughput, which is the thing we're trying to optimize.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "For instance, the computing limit of six teraflops.",
                    "label": 0
                },
                {
                    "sent": "That's just a ceiling, right?",
                    "label": 0
                },
                {
                    "sent": "No matter what we do, no matter what operation we ever run.",
                    "label": 0
                },
                {
                    "sent": "If you see your code telling you your timing code says you're going faster than six teraflops, you know something is wrong, right?",
                    "label": 0
                },
                {
                    "sent": "This is the cosmic speed limit for your hardware and certainly the hardware vendors are not going to under quote this.",
                    "label": 0
                },
                {
                    "sent": "So you also have the bandwidth, right?",
                    "label": 0
                },
                {
                    "sent": "That's how much data we can ship back and forth to memory, and that constraint is represented by a line here where the slope is the bandwidth, right?",
                    "label": 0
                },
                {
                    "sent": "So if you think about the units here, flops per second divided by flops per byte, then we get bytes per second, and so the bandwidth constraint shows up over here.",
                    "label": 0
                },
                {
                    "sent": "And so now you can think of this sort of envelope the the upper bound on what you can achieve.",
                    "label": 0
                },
                {
                    "sent": "You can never be outside of this little trapezoidal region here.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so if we'd like to know how much intensity do we need to to be using all of our computing throughput, we can actually calculate the location of this intersection.",
                    "label": 0
                },
                {
                    "sent": "The hinge where where the constraint changes, which is where that Max shifts from being Max, is no longer the Max of is no longer equal to the running time of the memory portion, but suddenly becomes equal to the running time of the compute portion.",
                    "label": 0
                },
                {
                    "sent": "So the way we can calculate that is just of course.",
                    "label": 0
                },
                {
                    "sent": "Take the height of this thing divided by the slope here to get the X value.",
                    "label": 0
                },
                {
                    "sent": "So if you do this for our numbers, it turns out that this little hypothetical machine needs an intensity of 20 flops per byte, which is not too bad for deep learning work, but is still much much much bigger than 1/2 of a flop per byte.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This number is a little bit tricky to just read off for a new GPU or a new CPU, but if you do some, maybe some benchmarks, or even just go ahead and do the back of the envelope for your machine.",
                    "label": 0
                },
                {
                    "sent": "It's good to just file this number away so that you know what it is for your hardware and you can compare things against it as you go.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's easy to see the relationship between compute and memory in this model, and so based on the sort of theoretical numbers for our system, we decided that 20 flops per byte was what we needed to be compute bound.",
                    "label": 1
                },
                {
                    "sent": "And the reason this is useful is because anything below 20 flops per byte, like the matrix vector multiplication.",
                    "label": 0
                },
                {
                    "sent": "If we carry out this little computation to find the intensity, and we see it's less than 25 per byte, we know that compute is just not the limiting factor.",
                    "label": 0
                },
                {
                    "sent": "We could just read it right off.",
                    "label": 0
                },
                {
                    "sent": "So let's do another example.",
                    "label": 0
                },
                {
                    "sent": "Let's see.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let's do another example.",
                    "label": 0
                },
                {
                    "sent": "So let's suppose that our goal now is to do a matrix matrix multiplication, where a is M * K and B as obviously has to have K rows here and columns and then C is going to be M by N. And what I want to do is I want to take C and I want to add the product a * B to it.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We can go through the same plug and chug again here.",
                    "label": 0
                },
                {
                    "sent": "We can take the number of bytes that I have to load and store for this, so MK for this guy KN for this guy and then 2M N 'cause I've got a load C first add to it and then store C. I multiply by 4 bytes, the number of flops that we need to compute.",
                    "label": 1
                },
                {
                    "sent": "This is roughly 2 times MKN.",
                    "label": 0
                },
                {
                    "sent": "It's really the number of flops for this thing.",
                    "label": 0
                },
                {
                    "sent": "The extra terms over here aren't that important.",
                    "label": 1
                },
                {
                    "sent": "And so if we compute the intensity for this, we have about 64 flops per byte.",
                    "label": 0
                },
                {
                    "sent": "So once you know these two numbers, you can just divide it all out and say, like, alright, this guy should be compute bound.",
                    "label": 0
                },
                {
                    "sent": "So unlike matrix vector matrix matrix is in some sense much more efficient because you actually get to use all that floating point throughput that you've got on your GPU.",
                    "label": 0
                },
                {
                    "sent": "And the reason that that works is because the structure of this operation has much higher intensity and this is.",
                    "label": 0
                },
                {
                    "sent": "In a sense, a fundamental part of how this operation works.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one thing that I introduced here that I didn't talk too much about before is that I just analyzed kind of two operations together, right?",
                    "label": 0
                },
                {
                    "sent": "I took matrix product and I added something to it.",
                    "label": 0
                },
                {
                    "sent": "I did two operations at once as part of my analysis and this is a general trick.",
                    "label": 0
                },
                {
                    "sent": "You're welcome to aggregate as much stuff into your roofline analysis as you want and just total up all of the floating point operations and total up all of the memory transactions and compute the intensity.",
                    "label": 0
                },
                {
                    "sent": "But just remember again that there's an.",
                    "label": 0
                },
                {
                    "sent": "In some, there's an assumption baked into this when we do it, what we're assuming when I put when I do, my roofline analysis is that I'm looking at one of these little blocks where the loads and stores in the compute can be overlapped, but once I group two things together like I do a product and I do in addition at the same time.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I'm assuming is that these two things can be analyzed as one that in effect the memory transactions can both be overlapped, which might not always be possible to do, but in the case of like matrix matrix multiplication, you can certainly do so.",
                    "label": 0
                },
                {
                    "sent": "You guys will notice if you use like a Blas library that the gem call is very happy to let you do an extra addition in there, and so you can actually pull this off very efficiently.",
                    "label": 0
                },
                {
                    "sent": "So so just don't forget about this assumption again, so you can aggregate as much stuff as you want into your roofline analysis, but it will start to get really hard to guarantee that all of these transactions overlap.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this roofline model is basically your upper speed limit, right?",
                    "label": 1
                },
                {
                    "sent": "So with this analysis, all the things we've been carrying out have been giving us a throughput number, and that throughput number is basically the point along this line.",
                    "label": 1
                },
                {
                    "sent": "It's saying this is the upper limit for what you can possibly achieve for this transaction for this operation.",
                    "label": 0
                },
                {
                    "sent": "So for something like matrix vector right, it's kind of living down here where it's bottlenecked on memory, and there's just not much we can do to make it go faster.",
                    "label": 0
                },
                {
                    "sent": "Without having it be much more intense, whereas something like matrix matrix is living up here and we might be quite happy if it's running at full speed and so one of the values of the roofline model is that it tells you what is C, what is the speed of light, and so if you run your matrix matrix and you find that it's taking up all of your computing time but it's not achieving this limit, then you kind of know where you should be looking for a problem where there's some room to improve, and so as a general sort of philosophy for how to go about choosing.",
                    "label": 0
                },
                {
                    "sent": "What's worthwhile to look at?",
                    "label": 1
                },
                {
                    "sent": "You can go for the thing that's using up a lot of running time and has some headroom here.",
                    "label": 0
                },
                {
                    "sent": "So if your matrix vector.",
                    "label": 0
                },
                {
                    "sent": "Operation is already getting the peak bandwidth.",
                    "label": 0
                },
                {
                    "sent": "You might as well just give up.",
                    "label": 0
                },
                {
                    "sent": "Don't don't bother spending time on it, whereas if matrix matrix is not running fast enough, maybe pushing further down this way, I'll help you a little bit in practice.",
                    "label": 0
                },
                {
                    "sent": "But maybe it means you need to call NVIDIA and ask them to fix the blast kernel or something.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in practice this theoretical limit is hard to reach.",
                    "label": 1
                },
                {
                    "sent": "There are lots and lots and lots of things you have to worry about.",
                    "label": 0
                },
                {
                    "sent": "So if you're writing your own CUDA kernel, for instance, actually getting the peak is really hard, but you can often do it.",
                    "label": 0
                },
                {
                    "sent": "So it's great to be using something like koblas that's already been optimized for you by very sharp people with intimate knowledge of the hardware.",
                    "label": 0
                },
                {
                    "sent": "But there are some problems.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you take a BLAS library and you start making the matrices too small, they're just different kinds of resources in the hardware that you're not fully deploying, and you'll see that instead of sort of marching perfectly up this line, you have, you know, kind of a little curve 'cause in this region you're sort of trying to optimize for both bandwidth and compute.",
                    "label": 0
                },
                {
                    "sent": "And the code just isn't as fast as it could be.",
                    "label": 0
                },
                {
                    "sent": "So this is just an approximation still on.",
                    "label": 0
                },
                {
                    "sent": "When in doubt, you can often sanity check these boundaries with little micro benchmarks you can just write very tiny kernels to kind of feel out.",
                    "label": 0
                },
                {
                    "sent": "What is the best bandwidth and what is the best throughput you can get.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is an example of where this would have saved me personally.",
                    "label": 0
                },
                {
                    "sent": "An incredible amount of time.",
                    "label": 0
                },
                {
                    "sent": "A few years ago, while we were working on like this HPC system that we built for deep learning, I spent a long time optimizing a specialized matrix multiplication kernel to handle like locally connected networks place where koblas wasn't running fast enough, and it turned out that there is actually a combination of compiler plus architecture problem on Kepler that made it so that you could never achieve more than 50% of the floating point peak using CUDA code.",
                    "label": 0
                },
                {
                    "sent": "And so, knowing this in advance would have saved me a lot of time because I was stuck at getting about half of what I thought I should be getting.",
                    "label": 0
                },
                {
                    "sent": "So keep in mind that doing a little microbenchmarking ahead of time to make sure that something reasonable is going on is is a good idea.",
                    "label": 0
                },
                {
                    "sent": "So in practice though, this isn't too bad, so this is, uh, from uh, kind of neat paper by ofenbach and some coauthors.",
                    "label": 0
                },
                {
                    "sent": "Basically what they did is they took a bunch of Z on CPUs and ran kind of dense linear algebra jobs on them.",
                    "label": 0
                },
                {
                    "sent": "So do you like adding a plus Y here?",
                    "label": 0
                },
                {
                    "sent": "So this is a BLAS operation to add 2 giant arrays together.",
                    "label": 0
                },
                {
                    "sent": "Doing like double precision matrix vector right which we said would be bandwidth bound and then doing a double precision matrix, matrix multiplication and so all of these points here on these little squiggly curves are actual kernels that they ran with little hyperparameters in them to tune their performance, and so you can see that as a function of the intensity that they actually measure from the kernel.",
                    "label": 0
                },
                {
                    "sent": "It actually does follow this like nice roofline model.",
                    "label": 0
                },
                {
                    "sent": "So this is reasonably accurate if you can compute these numbers.",
                    "label": 0
                },
                {
                    "sent": "And so the reason that this kind of squiggles here is because this is the measured intensity.",
                    "label": 0
                },
                {
                    "sent": "And if you have something like a cache miss on your X86 CPU, that means you actually have to double load some things.",
                    "label": 0
                },
                {
                    "sent": "So the result is that if your kernel is not tuned quite right and you have some more cache misses, a bigger matrix multiply can actually have slightly lower intensity.",
                    "label": 0
                },
                {
                    "sent": "So that's why you see this little squiggle.",
                    "label": 0
                },
                {
                    "sent": "And just as an aside.",
                    "label": 0
                },
                {
                    "sent": "Yeah, go ahead.",
                    "label": 0
                },
                {
                    "sent": "This one over here, so this is double precision.",
                    "label": 0
                },
                {
                    "sent": "Alpha Times X + y.",
                    "label": 0
                },
                {
                    "sent": "This is a Blas operation that just takes two big vectors X&Y.",
                    "label": 0
                },
                {
                    "sent": "It multiplies X by a scalar Alpha and then adds them together.",
                    "label": 0
                },
                {
                    "sent": "So this if you're building neural Nets model, this is just saying multiply by a constant, like the learning rate and then add my two arrays together.",
                    "label": 0
                },
                {
                    "sent": "So we use this all the time and the critical observation here is that your way, way, way down this curve.",
                    "label": 0
                },
                {
                    "sent": "This is the consummate memory bandwidth, limited operation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Just as an aside though.",
                    "label": 0
                },
                {
                    "sent": "This model doesn't take into account things like cache misses on its own, but if you happen to have some knowledge about the fact that you're not going to be able to deal with this problem you're dealing with is going to have a lot of cache misses 'cause you're doing random access.",
                    "label": 0
                },
                {
                    "sent": "For example, right?",
                    "label": 0
                },
                {
                    "sent": "Again, we were talking about attentional models and what makes those sorts of things hard.",
                    "label": 0
                },
                {
                    "sent": "And if you don't know where you're going to access in an image, and so you're kind of bouncing around and you wind up double loading some things.",
                    "label": 0
                },
                {
                    "sent": "What that is is that you have to add that memory transfer into your roofline analysis annual see your intensity starting to go down.",
                    "label": 0
                },
                {
                    "sent": "So if you can actually think through how much extra stuff you're going to load for these sort of more sophisticated models, you can still do this analysis.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So in summary, what we want to do is find the maximum potential throughput, which is to say the speed of light to know the best performance our code could ever possibly get.",
                    "label": 1
                },
                {
                    "sent": "And then we're going to benchmark ourselves against this every time we go to optimize the code, change the model, look at new hardware we're going to, we're going to keep this in mind, and a factor speedup is nice for you, but it's not actionable really for figuring out the next step.",
                    "label": 1
                },
                {
                    "sent": "So the thing that I think is a good first pass on figuring out what to do is to look at operational intensity and this roofline model to kind of just sort of quickly spec out.",
                    "label": 0
                },
                {
                    "sent": "What are the reasonable things you could do and what kind of performance you might achieve.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so let's look at some more issues that you run into on a single node.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Mini batch size is a really classic thing to tweak in your machine learning algorithm, right?",
                    "label": 0
                },
                {
                    "sent": "So historically, what people would find is that a mini batch size of 1 always leads to the fastest convergence that if you have some big optimization problem and I want to minimize the number of iterations that it takes to get to the solution, setting your mini batch size to one is the best bet and so.",
                    "label": 0
                },
                {
                    "sent": "This, however, on current hardware does not imply that it will be the fastest experiment and the reason is because of these systems issues.",
                    "label": 1
                },
                {
                    "sent": "So, So what size should we actually try to use?",
                    "label": 1
                },
                {
                    "sent": "Well, let's just take one of our examples.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's suppose I have a deep neural network and each of my layers is N by N, right?",
                    "label": 0
                },
                {
                    "sent": "I have N input neurons and an output neurons, and I've got a mini batch size of M. Well, my number of operations is 2 N squared M. That's easy to just pick out.",
                    "label": 0
                },
                {
                    "sent": "Here's the number of memory transactions that I've got to do when I do a matrix matrix multiply of this size.",
                    "label": 0
                },
                {
                    "sent": "And so for a range of mini batch sizes for a range of choices of M and some various choices of NI can show you how the how the intensity changes.",
                    "label": 0
                },
                {
                    "sent": "This is not the throughput.",
                    "label": 0
                },
                {
                    "sent": "This is the intensity how the intensity changes with different choices and you can see that if we're going to use our little 20 flops per byte benchmark that all of these guys kind of crossover somewhere around, like maybe 50 to 50 to 60.",
                    "label": 0
                },
                {
                    "sent": "So what this says is that if you're using a mini batch size down here, you are your bandwidth limited.",
                    "label": 0
                },
                {
                    "sent": "You are on the left side of the roofline model, and you're not using all of your resources.",
                    "label": 0
                },
                {
                    "sent": "But if you're up here, you're now compute limited and you can't possibly process mini batches any faster.",
                    "label": 0
                },
                {
                    "sent": "You're not going to get any more work out of your system by making the mini batch bigger.",
                    "label": 0
                },
                {
                    "sent": "And so the.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fact is that if you're below, let's say 64 about the crossing point, then using a bigger mini batch is almost like a free lunch because you get double the mini batch size.",
                    "label": 0
                },
                {
                    "sent": "Let's say if you go from 32 to 64, but the running time does not double, you're sort of getting some operations for free.",
                    "label": 0
                },
                {
                    "sent": "And beyond 64.",
                    "label": 0
                },
                {
                    "sent": "The free lunch goes away and if you double M from 64 to 128, you'll just.",
                    "label": 0
                },
                {
                    "sent": "You're just going to double the running time of your operations.",
                    "label": 0
                },
                {
                    "sent": "And so once you factor this in to the convergence improvements, what you usually see is a curve like this, where for a tiny mini batch size our experiment time is really high because it doesn't take us very many iterations, but every iteration is just really slow in terms of total throughput.",
                    "label": 0
                },
                {
                    "sent": "And as we increase the mini batch size our convergence gets convergence improves, but each iteration takes a little longer, but not so long that we don't see.",
                    "label": 0
                },
                {
                    "sent": "An improvement in the experiment time and then once we cross this sort of critical point where every mini batch isn't really buying us anything in terms of optimization, but it's just taking longer to compute.",
                    "label": 0
                },
                {
                    "sent": "We'll just see the running time start going back up.",
                    "label": 0
                },
                {
                    "sent": "So if you're trying to guess what you should set your mini batch 211 reasonable piece of advice is just to raise it to the point where you're now compute bound 'cause in some sense you've eaten up the free lunch and gotten as much convergence.",
                    "label": 0
                },
                {
                    "sent": "Improvement out of stochastic gradient as you can get and beyond this year, you're going to hit diminishing returns.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, and this is kind of a general thought that you'll often hear is talking about, like oh, we want to compute limited.",
                    "label": 0
                },
                {
                    "sent": "We want to compute limited and the reason for that is that if you aren't compute limited, there's probably a free lunch somewhere to be had.",
                    "label": 1
                },
                {
                    "sent": "It means there are resources on the hardware that are waiting to do more operations for you and make things run faster, but for some reason or another, because of how your job is set up, you're not actually utilizing them, and so if you see that you're not compute limited, you could try using a bigger model.",
                    "label": 0
                },
                {
                    "sent": "Bigger mini batch to try to get some gains from that.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so your model is supposed to be compute limited, but you're not achieving the throughput you expect.",
                    "label": 1
                },
                {
                    "sent": "There's a bunch of things you could try to do to optimize this.",
                    "label": 0
                },
                {
                    "sent": "I'm actually going to jump over a couple of these slides.",
                    "label": 0
                },
                {
                    "sent": "But just want to point out that if you're trying to optimize this thing for improved memory, bandwidth is not going to help.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there are a bunch of things to try for people who are interested in digging into CUDA kernels and things like that.",
                    "label": 0
                },
                {
                    "sent": "If you want to take a foray into to creating your own systems libraries for deep learning, you can come back to this slide or start digging through optimization manuals.",
                    "label": 0
                },
                {
                    "sent": "But the high level picture of I want you to take away from this is that whenever you see something with low intensity, whatever it is, matrix, vector or or.",
                    "label": 0
                },
                {
                    "sent": "Adding two arrays together, there's some bucket of things to try to improve memory bandwidth.",
                    "label": 1
                },
                {
                    "sent": "And you should ignore everything else in the universe as far as things to try.",
                    "label": 0
                },
                {
                    "sent": "And for high intensity workloads, there's another bucket of things to try that is usually different and you should ignore all the memory bandwidth stuff.",
                    "label": 1
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One final note, again on code complexity in the speed of light.",
                    "label": 1
                },
                {
                    "sent": "The more tricks you bring to bear to make your code faster, the more complicated it gets, and the harder it will be to get even closer to the speed of light.",
                    "label": 0
                },
                {
                    "sent": "And So what people often do to mitigate this is they'll write separate pieces of code for all of the different parameters that they want.",
                    "label": 1
                },
                {
                    "sent": "So if you're going to do a matrix matrix multiply, you'll pick lots of different shapes and sizes of matrices, and you're right.",
                    "label": 0
                },
                {
                    "sent": "A kernel that works really well down here.",
                    "label": 0
                },
                {
                    "sent": "Another kernel that works kind of up in this middle region and another kernel that works in this region where we're very compute limited.",
                    "label": 0
                },
                {
                    "sent": "So if you're digging through sort of code online like you go through CUDA convet or something for example, you'll see gigantic dispatch codes that are figuring out exactly what kind of convolution are you asking for and bouncing you to a kernel that's good for that.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So let's start talking about multi node which is actually kind of the main event when it comes to deep learning.",
                    "label": 0
                },
                {
                    "sent": "So as I said, a lot of libraries are available now to do deep learning operations which is really fantastic.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to go even faster, what we want is to use many CPUs or use many GPU's in many machines at once, and the unfortunate state of things is that there are relatively fewer tools and libraries to help you with this, and so in a sense you're going to be on your own much more often, and figuring out the performance issues in these systems, and sadly.",
                    "label": 1
                },
                {
                    "sent": "Even for very good systems, it's not that easy to automate, so this has been a long long standing problem in supercomputing.",
                    "label": 0
                },
                {
                    "sent": "How do you make like a big physical simulation?",
                    "label": 0
                },
                {
                    "sent": "Run fast on a supercomputer and it's just hard to do, and a lot of the tools we've been talking about are useful there too.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to actually do is sort of come up with a trick to just reuse some of our analysis tools from before to guide how we make decisions about parallelizing things.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to sort of step back for a second, I want to pose the higher level question or just what can we hope to achieve by having more computing power?",
                    "label": 1
                },
                {
                    "sent": "Before we've actually been talking about a fixed budget, right?",
                    "label": 0
                },
                {
                    "sent": "It's like we bought our GPU.",
                    "label": 0
                },
                {
                    "sent": "It's there, and we're doing all of our analysis about what to change in our problem to work better on this GPU.",
                    "label": 0
                },
                {
                    "sent": "Now we're with a different situation where we have the option if we'd like to use this other nice GPU that's hanging out over here, willing to do work for us.",
                    "label": 0
                },
                {
                    "sent": "And the ideal case that we always have in mind is that if we start with.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A job that runs nicely on a single node.",
                    "label": 0
                },
                {
                    "sent": "What we'd really love to do is achieve higher throughput for this same job by just breaking it onto these two nodes.",
                    "label": 1
                },
                {
                    "sent": "And so if we were really lucky if we did a really great job, then maybe we could run 2X faster.",
                    "label": 0
                },
                {
                    "sent": "Ideally we would get 2X the throughput for this system in aggregate.",
                    "label": 1
                },
                {
                    "sent": "So this this designer item is called strong scaling.",
                    "label": 0
                },
                {
                    "sent": "So when I say I want to run the same job twice as fast, we say that that is trying to scale strongly.",
                    "label": 0
                },
                {
                    "sent": "And so you'll hear this term quite a bit.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The alternative, though, which wasn't really useful before, is that we could parallelize and make the workload larger at the same time.",
                    "label": 1
                },
                {
                    "sent": "So what if instead of just trying to take my existing DNN and split it in half, I actually just go for a bigger one?",
                    "label": 0
                },
                {
                    "sent": "Or I use a bigger mini batch size for example, and then I parallelize it.",
                    "label": 1
                },
                {
                    "sent": "This is called weak scaling, and when to do this is maybe not clear.",
                    "label": 0
                },
                {
                    "sent": "If you're not sure how big of a model you want or how big of a mini batch size you want.",
                    "label": 0
                },
                {
                    "sent": "But the good news is that weak scaling is pretty easy, all things considered.",
                    "label": 0
                },
                {
                    "sent": "Because if I just keep making my job bigger and bigger, virtually any implementation you can is going to scale weekly.",
                    "label": 0
                },
                {
                    "sent": "You can do really horrible things on supercomputers and have really terribly written code, but if you make your problem large enough and you do your little analysis, you'll just find out that your matrix multiply has so much arithmetic in it that it dwarfs everything else you ever want to do.",
                    "label": 0
                },
                {
                    "sent": "And so the good news is that if you want to scale weekly, and you've got good single node code, you can do it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, sure.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "So it depends on the structure of your neural net, of course.",
                    "label": 0
                },
                {
                    "sent": "Let's take those sort of a really lame neural net, right like a one layer neural net that is just totally dominated by a giant matrix matrix multiplication zero matrix matrix is like the sort of perfect weak scaling problem because the number of flops is MNK.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "It's kind of like a cubic term, and the number of memory operations I've got to do is like M N + K N right?",
                    "label": 0
                },
                {
                    "sent": "It's all quadratic, and so if I just make those numbers big enough in some axis, right?",
                    "label": 0
                },
                {
                    "sent": "If I just keep scaling them all up, eventually the memory portion of that is tiny, and so for the purposes of getting the best throughput right, keeping your GPU's busy?",
                    "label": 0
                },
                {
                    "sent": "This will work right because the number of flops per byte is just going up and up and up and up and up.",
                    "label": 0
                },
                {
                    "sent": "I think the reservation your voicing is maybe two fold, which is real, which is that maybe you just don't want a neural network.",
                    "label": 0
                },
                {
                    "sent": "That is, you know enormous, right?",
                    "label": 0
                },
                {
                    "sent": "If M, it has to be like 50,000.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, and So what?",
                    "label": 0
                },
                {
                    "sent": "You're sort of getting out there is that.",
                    "label": 0
                },
                {
                    "sent": "To scale weekly, I usually mean to sort of push all the parameters to their limits so that you are compute bound and what you're saying is that I might not actually want to make it double right.",
                    "label": 0
                },
                {
                    "sent": "I could run out of memory that we kind of assume we've got a lot of memory at this point, and so on.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so there is some variation there.",
                    "label": 0
                },
                {
                    "sent": "But the good news is that at least in practice, you can if you can use a bigger mini batch.",
                    "label": 0
                },
                {
                    "sent": "Let's say that you've been memory constrained.",
                    "label": 0
                },
                {
                    "sent": "Or compute constrained with like a mini batch of 64, right?",
                    "label": 0
                },
                {
                    "sent": "And now I have double the compute.",
                    "label": 0
                },
                {
                    "sent": "So on one node it didn't make sense to double the mini batch size, right?",
                    "label": 0
                },
                {
                    "sent": "I was already compute bound if I now have another node and I think that doubling the mini batch will actually help me in terms of convergence rate.",
                    "label": 0
                },
                {
                    "sent": "That's a freebie.",
                    "label": 0
                },
                {
                    "sent": "So in some sense I now go to 128.",
                    "label": 0
                },
                {
                    "sent": "I have twice as much work twice as many nodes.",
                    "label": 0
                },
                {
                    "sent": "There is some communication, but for something like a common it, it doesn't really matter, so I'll run faster, hopefully and I and so this is different than the strong.",
                    "label": 0
                },
                {
                    "sent": "Excuse me, this is different from the strong scaling case where I actually want to cut my mini batch in half as I go to two nodes.",
                    "label": 0
                },
                {
                    "sent": "Did that sort of make sense.",
                    "label": 0
                },
                {
                    "sent": "OK, alright, we catch you afterward.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so an example of this from some of our own work at Stanford awhile back is that small networks don't get that much faster if you have a lot more GPU's.",
                    "label": 0
                },
                {
                    "sent": "So here's an experiment we ran with like a big neural net that's gotten.",
                    "label": 0
                },
                {
                    "sent": "Yeah, 185 million parameters.",
                    "label": 0
                },
                {
                    "sent": "That would be like the size of a decent comp net, and this is actually model parallel.",
                    "label": 0
                },
                {
                    "sent": "This is a neural Nets that's been broken up over many many GPU's, and if you were spending about maybe .75 seconds per iteration before.",
                    "label": 0
                },
                {
                    "sent": "Early on you're getting really great scaling.",
                    "label": 0
                },
                {
                    "sent": "You're going faster, right?",
                    "label": 0
                },
                {
                    "sent": "You're almost scaling strongly as you go to more nodes, but eventually as you chop up this piece of work, you get sort of less and less and less improvement, and so weak scaling corresponds to the option if you will.",
                    "label": 0
                },
                {
                    "sent": "To say maybe I just want to have like an 11 billion parameter network.",
                    "label": 0
                },
                {
                    "sent": "I'm going to take this job and I'm going to play it by a factor of 100.",
                    "label": 0
                },
                {
                    "sent": "To make a vastly, vastly larger network, an wow look at that.",
                    "label": 0
                },
                {
                    "sent": "It takes, you know, not not much more time than this little one did on on one GPU, and so there's of course an open question of is this a useful network for something like supervised learning?",
                    "label": 0
                },
                {
                    "sent": "Clearly it isn't.",
                    "label": 0
                },
                {
                    "sent": "But the point being is that as you add GPU's, you very often have this ability to just go to a bigger, bigger model.",
                    "label": 0
                },
                {
                    "sent": "If you would like.",
                    "label": 0
                },
                {
                    "sent": "And it's much easier to pull that off than to make this curve stay flat.",
                    "label": 0
                },
                {
                    "sent": "That's the high level thing you should take away.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so if you can use a bigger model as I said.",
                    "label": 1
                },
                {
                    "sent": "Why not do it?",
                    "label": 0
                },
                {
                    "sent": "If you think having a doubled.",
                    "label": 1
                },
                {
                    "sent": "If you think doubling the mini batch size would make you converge faster than this is a good candidate to just scale up an all things equal.",
                    "label": 0
                },
                {
                    "sent": "You should try to do this first.",
                    "label": 1
                },
                {
                    "sent": "So for the rest of the time I'm going to just say in practice we often don't want to do this.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we don't want a bigger network because we don't have enough data.",
                    "label": 0
                },
                {
                    "sent": "Maybe we've already hit diminishing returns where doubling the mini batch size just isn't going to help us converge any faster.",
                    "label": 0
                },
                {
                    "sent": "And of course the real thing that we want about strong scaling is that it squishes our cycle time down.",
                    "label": 0
                },
                {
                    "sent": "It helps us take the same model that we're really interested in learning about and get through experiments more quickly.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 1
                },
                {
                    "sent": "What makes strong scaling so difficult?",
                    "label": 0
                },
                {
                    "sent": "There's actually a bunch of factors so.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Understand this, we need to go back and we need to analyze the performance of our multi node system.",
                    "label": 1
                },
                {
                    "sent": "So first, what we're going to do is try to partition the work and just start by assuming that our network has infinite bandwidth.",
                    "label": 1
                },
                {
                    "sent": "So we're just going to totally take away the communications problem we were talking about, and assume that all we want to do is break the work into two separate nodes and then see how it goes, and if they ever need to communicate.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to assume that's infinitely fast.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as an example which is apropos to what we were just talking about, is that it's really common practice to take a training job and partition it onto multiple nodes by splitting our mini batch in half.",
                    "label": 0
                },
                {
                    "sent": "So let's say that X is a bunch of examples, right each column of X is like one training vector and our mini batch size is N. And now here is our little neural network weight matrix.",
                    "label": 0
                },
                {
                    "sent": "I want to multiply these together, and that's going to give me a matrix of activations.",
                    "label": 0
                },
                {
                    "sent": "Then with data parallels on what we're doing is breaking this mini batch and putting the first half on node 1, three in the second half on node two, or mini batch size on each node.",
                    "label": 0
                },
                {
                    "sent": "Just got cut in half.",
                    "label": 0
                },
                {
                    "sent": "And now when we multiply this by W, we're going to end up with some activations that live on node number one, and we're going to do some activations that live on node number 2.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there is of course the wrinkle that we have to keep W synchronized right?",
                    "label": 0
                },
                {
                    "sent": "So every time we run an update and we change W, the two nodes are going to talk, but we're going to assume that that's sort of infinitely fast for now.",
                    "label": 0
                },
                {
                    "sent": "So the question to ask is what happens to the workload on node number one?",
                    "label": 0
                },
                {
                    "sent": "So we can work this out right?",
                    "label": 1
                },
                {
                    "sent": "We just go back to our plain old roofline analysis.",
                    "label": 0
                },
                {
                    "sent": "We know there's two MKN flops to do this multiply.",
                    "label": 0
                },
                {
                    "sent": "Here's the number of memory transactions.",
                    "label": 0
                },
                {
                    "sent": "Here's the intensity right flops divided by memory transactions, and so this is what it was before.",
                    "label": 0
                },
                {
                    "sent": "If you were to just do this whole thing on one node after we partition it, things change a little bit, right?",
                    "label": 0
                },
                {
                    "sent": "The number of loads and stores I've gotta do for these two matrices is half as much.",
                    "label": 0
                },
                {
                    "sent": "And then the number of flops that I've got to do is also half as much.",
                    "label": 1
                },
                {
                    "sent": "So if I figure out what the intensity is, what you'll notice is that there an extra term of MK popped in there, so.",
                    "label": 0
                },
                {
                    "sent": "In terms of intensity, it's clear that my denominator went up my overall intensity on node one just went down by by this partitioning scheme, and so this is part of what makes strong scaling hard.",
                    "label": 0
                },
                {
                    "sent": "It's not just the communication, right communication will hurt you, but even if your network is infinitely fast, you still have the problem that when you try to cut things up, you're making your existing node that you worked so hard to optimize, even less efficient, and so you give up stuff.",
                    "label": 0
                },
                {
                    "sent": "On the individual nodes.",
                    "label": 0
                },
                {
                    "sent": "And why?",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This winds up hurting you.",
                    "label": 1
                },
                {
                    "sent": "Is sort of going to depend on the workload, and so again we have our little roofline analysis here for this operation.",
                    "label": 0
                },
                {
                    "sent": "That node number one has to do.",
                    "label": 0
                },
                {
                    "sent": "We could have a couple of situations.",
                    "label": 0
                },
                {
                    "sent": "So let's say that MKN is really big.",
                    "label": 0
                },
                {
                    "sent": "We got a giant giant matrix that we're working on, then we're way over here, right?",
                    "label": 0
                },
                {
                    "sent": "We have lots and lots of arithmetic, operational intensity, and so when I break the job up, the intensity is going to go down, but it's OK, node one is still going as fast as it could possibly go in terms of throughput, so so we're not going to worry about it.",
                    "label": 0
                },
                {
                    "sent": "The bad case is when we're here, where it's sort of medium sized, where we're sort of hanging out in this ambiguous zone where we're just barely using all of the resources on node one, because now if we cut the work in half, we're going to start wandering down, and so the key thing to realize is that the very thing we thought was going to save us, which is to divvy up the work, and so node one now has less to do is also making node one slower.",
                    "label": 0
                },
                {
                    "sent": "Ah.",
                    "label": 0
                },
                {
                    "sent": "Yes, so if you had an infinitely fast network, though, you wouldn't need async STD, right, and so this analysis still applies in that case.",
                    "label": 0
                },
                {
                    "sent": "So no matter what you do, if you had that real infinitely fast network, you still have to deal with this, and this is.",
                    "label": 0
                },
                {
                    "sent": "This is why strong scaling is very hard, so for weak scaling you can get away with it 'cause you just leave your mini batch alone and now async STD could help you well if you have a very fast network is not a problem, but.",
                    "label": 0
                },
                {
                    "sent": "The point being is that this is a factor no matter how fast your network is, so you can't forget this.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Right, so in here everybody has a copy of the parameters and they just exchanged them because we said the network was infinitely fast.",
                    "label": 0
                },
                {
                    "sent": "We're just not worrying about that part right now, and so the point is, even when you just totally ignore the network problem, you still have a hard time strong scaling.",
                    "label": 0
                },
                {
                    "sent": "Exactly.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Yeah, exactly.",
                    "label": 0
                },
                {
                    "sent": "And so the question of whether you can scale strongly whether you have any hope at all.",
                    "label": 0
                },
                {
                    "sent": "The first thing you should check is which of these two regimes you're in.",
                    "label": 0
                },
                {
                    "sent": "If you're hanging out over here, right?",
                    "label": 0
                },
                {
                    "sent": "If you compute the intensity and you're right next to this threshold, don't bother like even an infinitely fast network cannot help you.",
                    "label": 0
                },
                {
                    "sent": "You are going to lose a lot when you try to go to multiple nodes.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if you're over here, well, at least the first Test has been passed and you have have a chance.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to assume now is that we're somewhere over in this regime, right?",
                    "label": 0
                },
                {
                    "sent": "If you're kind of in the middle, you might still want to try to scale up.",
                    "label": 0
                },
                {
                    "sent": "But but if we're over here, then it's no problem.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to assume that we usually run at the full throughput.",
                    "label": 1
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so this is important even with infinite memory bandwidth we might not be able to stay efficient, and So what we're going to do next though is, say, assuming that the local throughput is nice that we didn't lose anything.",
                    "label": 0
                },
                {
                    "sent": "This way we want to analyze the communication and let's figure out how much is that hurting us.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So to do that, there are a lot of ways you can try to get around communications problems.",
                    "label": 0
                },
                {
                    "sent": "So you mentioned async STD.",
                    "label": 0
                },
                {
                    "sent": "If you're talking about multiple GPU's or multiple cores on one node, there frameworks like Hog Wild to sort of try to deal with synchronization problems and speed things up.",
                    "label": 0
                },
                {
                    "sent": "But we're just going to try to get a first cut and how much the network bandwidth problem hurts you.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do is use this same sort of roofline model, but we're going to make the distinction now between local and remote memory, and so before what I had was node one talking to its own memory, and for the purposes of figuring out how efficient node number one is, that's the thing I want to deal with.",
                    "label": 1
                },
                {
                    "sent": "But now when I talk about a distributed job where let's say I have to copy W from my neighbor right to keep us In Sync, I.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to talk about node one is being like its own little self contained computing core that has a certain throughput that I figured out before and then when it talks to memory to get something done.",
                    "label": 0
                },
                {
                    "sent": "I'm not talking about local memory anymore, I'm talking about memory transfers over the network to node #2's memory.",
                    "label": 0
                },
                {
                    "sent": "So it's the same hypothetical model where we're assuming that the only constraints are how fast node number one goes and how much you can move across the network.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So back to our trusty roofline model, we can actually analyze the performance the same way, but the slope of this line is now 6 gigabytes a second instead of 300, so that's a little bit of a comedown from what we're used to, and this line is still the same height, right?",
                    "label": 0
                },
                {
                    "sent": "We assume no number one is really flying and doing 6 teraflops the 2nd.",
                    "label": 0
                },
                {
                    "sent": "So compared to the old view, this is an incredibly skewed system, right?",
                    "label": 0
                },
                {
                    "sent": "Lots of compute, but very, very little bandwidth.",
                    "label": 0
                },
                {
                    "sent": "And so if we go through and compute what is the location of this?",
                    "label": 0
                },
                {
                    "sent": "It's now about 1000 flops per byte.",
                    "label": 0
                },
                {
                    "sent": "And so you should walk away from this problem, realizing that if your work everything you do on node number one doesn't do at least 1000 flops for every bite you send over the network, you are in trouble.",
                    "label": 0
                },
                {
                    "sent": "You are going to end up down here where you can't, you just can't keep up.",
                    "label": 0
                },
                {
                    "sent": "The network is not going to be able to feed you with say, parameter updates or whatever.",
                    "label": 0
                },
                {
                    "sent": "Quickly enough for you to keep all the processors busy and you're going to end up with Dead Air.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Basically we need much higher intensity.",
                    "label": 1
                },
                {
                    "sent": "An keep in mind that to do this thousand flops, let's say I took a really tiny job with only 1000 flops in it.",
                    "label": 0
                },
                {
                    "sent": "That would be too small because depending on what I'm doing, my processor probably can't do 1000 flops at 6 teraflops right?",
                    "label": 1
                },
                {
                    "sent": "So I need to have enough work that I really do get this number so, so don't forget that assumption.",
                    "label": 0
                },
                {
                    "sent": "OK, so getting to the again the question of what do we do about communication?",
                    "label": 0
                },
                {
                    "sent": "Why does it matter or how?",
                    "label": 1
                },
                {
                    "sent": "How do we understand that?",
                    "label": 0
                },
                {
                    "sent": "Let's take an example where we want to do gradient updates.",
                    "label": 0
                },
                {
                    "sent": "In this case where we've got these parameters shared over the two nodes.",
                    "label": 0
                },
                {
                    "sent": "So what we want to do is look at node one's world what it sees when it it is a part of this operation and we want to do something that's kind of like a little gradient update.",
                    "label": 0
                },
                {
                    "sent": "So D is a matrix that has these deltas from backdrop, right?",
                    "label": 0
                },
                {
                    "sent": "And then X is the input to the layer, and when I multiply these two together, it's kind of like an outer product.",
                    "label": 0
                },
                {
                    "sent": "I multiply it by the step length for SGD and then I added onto W. So this is like a very simple gradient update that I want to do.",
                    "label": 0
                },
                {
                    "sent": "With the understanding that once I do it, I need to somehow orchestrate a transfer of W over to node 2, right?",
                    "label": 0
                },
                {
                    "sent": "So that's part of the contract of doing synchronous or asynchronous SGD?",
                    "label": 0
                },
                {
                    "sent": "I've got to get this sent.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If we're going to do synchronous, then this is a real problem because I have to wait for it before I can start something else, right?",
                    "label": 0
                },
                {
                    "sent": "A little bit about that assumption in just a second, but if you're doing async SGD, as Yoshua pointed out, you could just launch this and then then pretend that it's gone.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So node one has to perform the computation on its local chunk, which is to say it needs to do this matrix times this matrix and then it's got to make this addition and it has to send the update to node number 2.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the remote memory or the number of flops that we have to do is easy to compute.",
                    "label": 0
                },
                {
                    "sent": "MNK for this multiply and then two MK2 add up these.",
                    "label": 0
                },
                {
                    "sent": "Sorry to get this from over the addition here.",
                    "label": 0
                },
                {
                    "sent": "And then remote memory access, which is the amount of stuff I need to touch on node #2 in this whole operation right?",
                    "label": 1
                },
                {
                    "sent": "Sending this data across or Alternatively receiving an update from #2 is 4 MK.",
                    "label": 0
                },
                {
                    "sent": "It's the size of this thing and so the overall intensity of this workload, which is the number of flops our machine is is willing to carry out as part of this algorithm for every bite of network traffic.",
                    "label": 1
                },
                {
                    "sent": "Is this one?",
                    "label": 0
                },
                {
                    "sent": "So basically an over 4 so the mini batch size divided by 4.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If we go back to our roofline model and ask how big does N have to be in order for us to be compute limited, we'll find out that we need end to be something like 4000, so we need a really big mini batch in order to make it so that when I send this.",
                    "label": 1
                },
                {
                    "sent": "So that when I send my updates to the other side, or Alternatively I'm receiving updates from the other side, that communication doesn't take so long that I'm just sitting there waiting and so to yoshua's point, this is important if you're actually going to wait, so if you're waiting for this communication to be done to make your update, then obviously you want to make sure that there's enough work for you to be doing that.",
                    "label": 0
                },
                {
                    "sent": "You're busy the whole time while that transaction is outstanding, and so that's what this number says.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So getting back to just the question of like async SGD, this is.",
                    "label": 0
                },
                {
                    "sent": "This is really the important piece.",
                    "label": 0
                },
                {
                    "sent": "The key assumption in here was that we could always stream memory simultaneously with computation, right?",
                    "label": 1
                },
                {
                    "sent": "And what I did in this previous thing was I introduced Independency where I can't even do these in parallel right?",
                    "label": 0
                },
                {
                    "sent": "Because I've got to wait for this to finish before I can send my update off.",
                    "label": 0
                },
                {
                    "sent": "Man, if I were doing a loop here right, I'd have to wait for this guy before I could before I could update W. So this is a problem and there are a few different ways to deal with one that I didn't write down here is to do async SGD where I just keep looping on this no matter what, and I do my memory transactions in parallel.",
                    "label": 0
                },
                {
                    "sent": "So essentially what that does is it restores this assumption it gets you back to the nice roofline model case where I can just keep sending things.",
                    "label": 0
                },
                {
                    "sent": "Alternatively, I could try to overlap my send with other operations, so usually I have many different layers and so if I'm computing many layers at once, I can be quietly sending the updates for each one while I'm busy and you can do another roofline analysis to determine if that's enough.",
                    "label": 0
                },
                {
                    "sent": "And finally you can if you want, and many supercomputers do this is you can actually stream W in the middle of your update, so while your matrix multiply kernel is filling in the results of W. You can be sitting there watching catching the blocks of W as their finished and shipping them off to the other side.",
                    "label": 0
                },
                {
                    "sent": "So the reason that I bring this up is that async STD and hog wild and a lot of these are one way of getting around this latency.",
                    "label": 0
                },
                {
                    "sent": "But if you want to really scale strongly and not introduce like these confounding factors of convergence rate and so on right, I want the same job to run in lockstep faster.",
                    "label": 0
                },
                {
                    "sent": "There are lots of other tricks available to you.",
                    "label": 0
                },
                {
                    "sent": "You actually can pull a lot of this off if you're willing to sort of take to heart the roofline model and also try to keep the assumptions true that make that analysis workout.",
                    "label": 0
                },
                {
                    "sent": "So if I can actually stream W while it's being computed, and Ann is big enough, I can actually pull this off.",
                    "label": 1
                },
                {
                    "sent": "I can actually hide all of those transfers, and I don't have to wait at all.",
                    "label": 0
                },
                {
                    "sent": "It just looks like synchronous SGD running at full speed, so this is a more systems oriented way.",
                    "label": 0
                },
                {
                    "sent": "Where is async STD.",
                    "label": 0
                },
                {
                    "sent": "Hog wild.",
                    "label": 0
                },
                {
                    "sent": "A lot of these other algorithms is more like an algorithm IK way of trying to escape the same problems.",
                    "label": 0
                },
                {
                    "sent": "Who?",
                    "label": 0
                },
                {
                    "sent": "So yes, they are not friendly to use.",
                    "label": 0
                },
                {
                    "sent": "Maybe a couple of good examples are things like MKL will happily work on say multiple CPU sockets, which kind of has similar questions of how do I access memory that belongs to other sockets?",
                    "label": 0
                },
                {
                    "sent": "There's actually a bottleneck issue there.",
                    "label": 0
                },
                {
                    "sent": "Koblas XD will run matrix multiplications, but I believe I believe that it tries to run them out of main memory.",
                    "label": 0
                },
                {
                    "sent": "So it's almost like having a network bottleneck through your PCI Express bus and so to figure out if Kublai's XD is worthwhile, you can do an analysis like this and then thinking about real distributed machines.",
                    "label": 0
                },
                {
                    "sent": "There are things like Blason, scalapack and you can actually find them doing these types of analysis in the code will look at your matrix multiply.",
                    "label": 0
                },
                {
                    "sent": "Do a little analysis of how long it's going to take with different sorts of transfer schemes, and then pick the order that's going to give them the highest intensity.",
                    "label": 1
                },
                {
                    "sent": "OK, so anyway the to put this in a box.",
                    "label": 0
                },
                {
                    "sent": "Don't forget this overlap assumption right?",
                    "label": 0
                },
                {
                    "sent": "Fight it with all you can fight it with asynchronous things if you need to, but in fact if you have a high performance computing cluster there are lots of options available to you to try to optimize your code to make this true and make this roofline analysis workout.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the last few minutes we're going to try to put this all together.",
                    "label": 0
                },
                {
                    "sent": "So we've seen how partitioning affects our ability, our ability to scale right if we cut up our workload, it hurts our local performance, but we can make an assessment about whether that's important or not.",
                    "label": 1
                },
                {
                    "sent": "The changes in the distribution might also introduce a network bandwidth limit that we have to fight against.",
                    "label": 1
                },
                {
                    "sent": "And we can use this little roofline model model, which is later a back of the envelope calculation to get a sense for how these issues affect us.",
                    "label": 0
                },
                {
                    "sent": "Doesn't make any sense at all, for instance, to be trying to scale to multiple nodes, or should I just call it quits and use my one GPU?",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's a sort of suggested design process.",
                    "label": 0
                },
                {
                    "sent": "The first is scale up weekly if you if you can.",
                    "label": 0
                },
                {
                    "sent": "It's much, much easier than trying to do all these streaming tricks and so on.",
                    "label": 0
                },
                {
                    "sent": "So if a double mini batch size would be good for you, go for it.",
                    "label": 0
                },
                {
                    "sent": "So once you're, you've decided that you want to scale strongly, you've reached the biggest problem you reasonably want to solve, But you want to go faster then start out by picking a partition of all the work and all of the data across your nodes.",
                    "label": 0
                },
                {
                    "sent": "Just pick a candidate that you think seems reasonable.",
                    "label": 0
                },
                {
                    "sent": "Estimate the local node Max throughput, which you could either do by a benchmark if you've got one, or you can use the roofline model to help you, and this is like your first Test.",
                    "label": 0
                },
                {
                    "sent": "If you're not happy with the output of this, do not proceed.",
                    "label": 0
                },
                {
                    "sent": "Don't bother trying to strong scale.",
                    "label": 0
                },
                {
                    "sent": "If it turns out that each node is going to have two little work and it's not going to give you an improvement.",
                    "label": 0
                },
                {
                    "sent": "And similarly, if you use the local throughput.",
                    "label": 0
                },
                {
                    "sent": "You can put this now into a another roofline model.",
                    "label": 0
                },
                {
                    "sent": "That sort of represents your cluster, and you can ask what is the overall Max throughput of the work on each of these nodes.",
                    "label": 1
                },
                {
                    "sent": "So if you're happy with this, then great you should go back to doing deep learning.",
                    "label": 0
                },
                {
                    "sent": "So if you're happy with how you partitioned it and you've got the communication setup awesome, you can get back to real work.",
                    "label": 0
                },
                {
                    "sent": "If not, then I'll list a couple of things you can try on the next slide, or just pick a new partitioning because I used some kind of contrived parameters in here so that the partitioning was easy to work with and was illustrative, but you can find cases where one partitioning of the work is really awful.",
                    "label": 0
                },
                {
                    "sent": "Where you have really low intensity and your job just won't parallelize no matter what you do.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, there are cases where you pick a partitioning in a clever way.",
                    "label": 0
                },
                {
                    "sent": "Your intensity is very high and you can go very, very fast, and so you can just use this back of the envelope calculation to quickly iterate through a bunch of ideas and see which ones look good.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So as far as just ways to make this faster, just like on the single node, go hunt for all those operations that are taking up all the time.",
                    "label": 0
                },
                {
                    "sent": "That's a given hunt for the partitioning strategy that gives you a very high speed of light for which the speed limit is really high, or the intensity is very high, and that's a good candidate for something to try.",
                    "label": 1
                },
                {
                    "sent": "And then look for opportunities to increase the amount of communication and compute overlap.",
                    "label": 0
                },
                {
                    "sent": "So look for places where you can overlap work you're doing locali and sending data at the same time or receiving data at the same time, because that makes the assumptions true for our analysis and really engages all the hardware.",
                    "label": 0
                },
                {
                    "sent": "If you want to try to use asynchronous optimizers, things like that you can.",
                    "label": 0
                },
                {
                    "sent": "And then when in doubt, you can a judiciously apply the metal solution.",
                    "label": 1
                },
                {
                    "sent": "And this is where I think I wind up doing this most often, which is thinking about if I want to buy more GPU's or whether we want to look at a faster network, because if we're compute limited then more GPU's might actually help.",
                    "label": 0
                },
                {
                    "sent": "I'm going to have more compute floating around, I can hack up my job into separate pieces and more GPU's might go faster, might not, but it, but at least it's a plausible option.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if our roofline analysis says we're bandwidth limited, if no matter how fast our GPU is, we're just sitting there waiting for stuff to show up.",
                    "label": 0
                },
                {
                    "sent": "Then, instead of looking at more GPU's, we might want to look at a faster network to sort of innocence, bring those machines closer to us.",
                    "label": 0
                },
                {
                    "sent": "And even if you've already got InfiniBand, there other Network Solutions that can run faster.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, so in conclusion.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The key idea is here.",
                    "label": 0
                },
                {
                    "sent": "We're just sort of measure against the speed of light.",
                    "label": 1
                },
                {
                    "sent": "Try to figure out what's the fastest your system could ever run, and then strive to get as close to that as you can.",
                    "label": 0
                },
                {
                    "sent": "We talk about this really simple performance modeling approach to sort of intuitively understand the tradeoffs, right?",
                    "label": 0
                },
                {
                    "sent": "It doesn't account for things like the convergence rate of SGD for different mini batch sizes, or the.",
                    "label": 0
                },
                {
                    "sent": "Or how say something like Hog Wild will affect your convergence for different scales, but it will give you a sense for the system side of it and then the algorithm excite is up to you guys as machine learning researchers.",
                    "label": 0
                },
                {
                    "sent": "And then the challenging part of multi node training is usually figuring out this partitioning and communication 'cause some partitions are really good and some partitions are really bad.",
                    "label": 0
                },
                {
                    "sent": "Very experienced people will have an intuition for which one to try, but you can use this back of the envelope calculation to sort of get you started.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I just want to say thank you for your guys time also to Greg, Dima, San Bryan, Catanzaro at Baidu who have sort of brought me over to thinking this way a little bit as opposed to just kind of intuiting all the things that are going on.",
                    "label": 0
                },
                {
                    "sent": "So they're very useful references.",
                    "label": 0
                },
                {
                    "sent": "And if you want to know more about roofline there are a couple here and this is a HPC paper we had a couple a little while ago for that plot so thank you guys.",
                    "label": 0
                }
            ]
        }
    }
}