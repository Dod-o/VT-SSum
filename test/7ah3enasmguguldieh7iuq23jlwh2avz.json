{
    "id": "7ah3enasmguguldieh7iuq23jlwh2avz",
    "title": "Unsupervised Learning of Syntactic Structure",
    "info": {
        "author": [
            "Christopher Manning, Computer Science Department, Stanford University"
        ],
        "published": "Oct. 31, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/mlcs07_manning_uls/",
    "segmentation": [
        [
            "OK.",
            "So I mean a lot of my work in recent years is been in natural language processing, but I've been also interested in these linguistic issues about extend probabilistic models to language since there's this.",
            "This is clear fact that has been suppressed for a long time, and a lot of the discussions of formal linguistics, but it was actually expressed very clearly.",
            "It's appears text from 1921 by just begins chapter of his book, where talks about variation with the sentence.",
            "Everyone knows that language is variable, and I think that's true for common person on the street and everyone else, but somehow, but that hasn't been the way in which linguistic theory has been instructed in the following.",
            "80 years, and in particular I'd be interested in probabilistic notions of grandma that there's been a lot of probabilistic talk this morning already, so I won't.",
            "About this for too long, but I mean in the general context of what's happened in linguistics and psycholinguistics they last 20 years.",
            "I think enormous amount of literature has been either doing rules or you're doing your networks and that there hasn't been saved under representation of probabilistic ideas which allow you to have variation over and kind of more complex structures that people are more familiar with linguistic theories."
        ],
        [
            "So linguistic theory is being very.",
            "Around these ideas that you can't learn things in data because the data just isn't sufficient invents was already talking about some examples of that in his talk, whereas I mean I guess once guided, a lot of the work of recent computational linguistics is just how much data there is out there, and particularly for the case of child language learners horrify and came up with an estimate the children by adult board chief minus 20, actually.",
            "That yeah, I'm 200 million words of language input.",
            "You know that's a lot.",
            "That's a lot.",
            "I mean, we do have bigger corpora than that, but that's not even corpus.",
            "And so you know, that's a huge amount of data, and so I wanted my work and being sort of well, how much can you do with that kind of amount of data?"
        ],
        [
            "Trying to build models that look at that empirically.",
            "My cell linguistics is being sort of stuck in this model, which has been dominated by trumpski way even in this most recent writings that a lot of that kind of recent developments of formal linguistics is seen as having their roots in the logical structure of linguistic theory and trust in linguistics.",
            "Last appointed boots and linguistic, but it's important to realize that means the log these ideas were developed in the 1950s and have a singular difference from the 1950s is that in the 1950s.",
            "People knew.",
            "In fact, through the 1970s.",
            "About learning machine learning, looks really only starting in like this, not to be much more science about new machine learning, so it's the best way to sneak kind of actually putting some of these ideas about what kind of things we can learn."
        ],
        [
            "And for Vincent singing before, I guess his prices are good.",
            "Turn off on the sides.",
            "Yeah, so this is famous result by gold.",
            "That service is sort of based off of Trump skin and this is.",
            "So it's normally quoted as that you know you can't learn grammars of things like context free grammars from data alone.",
            "I mean the result is actually stronger than that, and I think actually kind of going with the stronger resolved so that in some sense shows.",
            "Use of this result is actually you can't even learn finite state languages, so regular languages that you can't learn regular languages from day one.",
            "The only thing that you can learn from data alone in the sense as gold formalized is languages that are actually finite, that they produce a finite number of utterances.",
            "And so I mean that kind of stuff.",
            "This suggests that hold G he must have to find something.",
            "Something wrongly, somehow.",
            "Did you ever get such a strong result, which stands and so absolutely opposed what we see in the world around us, the kind of things that people can learn and thought himself to talk.",
            "Spider bit about different ways in which she resolved to be worked around.",
            "So what is the suggest that maybe children do pick up on subtle convert negative evidence?",
            "So there's a thread of work, including by my colleague.",
            "Stanford.",
            "Exactly what happens by the law that although there isn't that much explicit correction, a lot of the time.",
            "But there are lots of ways that kids can pick up.",
            "They could have evidence one of the most obvious ones is whether they understood or not.",
            "That is a form of negative evidence.",
            "Another one is that even if parents don't explicitly correct a lot of times, they rephrase that they'll give answers to refer back to what the kids said and say it in different ways.",
            "You can strip the language class with an egg knowledge that's effective program, but the other one is that gold assumed this.",
            "Conditions.",
            "We have adversarial condition of data presentation that you had to be able to be able to cope with, which doesn't seem like how the world actually is.",
            "So if you change that condition, for example in some kind of probabilistic model."
        ],
        [
            "That was proved very soon thereafter by pointing and sensually.",
            "Show that if you have a prior on the likelihood of your class of grammars that you can actually learn where the little star somewhere there prefers to.",
            "Technically the notion of learnability is being changed is no longer builds notion of learnability as probabilistic approach.",
            "The right grammar, but you can learn not only the regular languages.",
            "Finite state appamada also learn probabilistic context free grammars.",
            "OK, so.",
            "Please give us talking to it stabilizes and thankfully computational learning theory, linguist, guy and he wanted to say don't give me that haunting stuff.",
            "It doesn't impress me and innocence, he's right.",
            "This is kind of a little bit trivial when you come down to it, which is that if the effectively what you have in the morning model is that you kind of have an ordering of all possible grammars for languages by their prior probability.",
            "And as you start to see enough data that you can tell that the predictions of some of those grammars.",
            "Not too far from the data that are actually seeing that you kind of cost them off from the top of the list and your current grammar is the best one still on the list.",
            "You know, that's kind of.",
            "The result and so.",
            "Um?",
            "That's kind of an interesting.",
            "I mean, in particular, you've got a lot of prior knowledge there that ordered that produces your initial ordering results.",
            "So in my own work I guess I computational learning theory is really hard stuff, so I've never really been here getting into doing computational learning theoretical results of how you could learn things, but what I think interested in is, can you actually empirically pursue this question of OK?",
            "Give us some data?",
            "Can we learn linguistic structure from the data?",
            "And so today I'm going to talk about two studies.",
            "So first of all, the work with Dan Klein on free structure learning.",
            "I presume people wanted me to talk about that when I was invited.",
            "Yeah, this is work for a couple of years ago, and although I'm kind of out of order 'cause the previous two talks have already talked about how they can do better than this work.",
            "But I will present this work anyway and I think the way it's kind of nice because it's it's motivated in a kind of a linguistic directions.",
            "I believe the script for a moment.",
            "Paul Smolensky attributes today broomall hard the remarks which I will loosely paraphrased roughly as follows.",
            "Linguistic backwards, I spend all of my time where here how to come up with learning mechanisms that are powerful enough to learn the kinds of things that kids learn.",
            "But numerous.",
            "They spend all their time trying to constrain what their models are, so they couldn't possibly learn things that don't happen and.",
            "I think this work is kind of nice day in the direction of trying to workout.",
            "What are the minimal things that you need to assume to be able to get something that can learn?",
            "You hear what little bits of knowledge of language for universal grammar do you have to put into something that's sufficient to bootstrap learning?",
            "But then I'll talk about some more recent work with technical, which is then learning semantic roles.",
            "Once you have some."
        ],
        [
            "First task.",
            "We want to structure over that which is.",
            "People should be thinking should be possible.",
            "Text that proved to be kind of a hard."
        ],
        [
            "To get results from.",
            "Some of the work on models of trying to do this and what doesn't work and what kind of properties of language.",
            "It seems that you do need models.",
            "One place to start is with models of dependencies of which words on my bridge words.",
            "So here we have.",
            "That's an amended bills at one flying field that is modifying Bill and can be learned.",
            "Those kind of dependencies for data alone.",
            "People have tried to do that using models that kind of stopped a lot of pets and try to link together words where they show, sometimes associations against you.",
            "The model of words.",
            "My pastor.",
            "Play trivia model, which worked out with associations in the question, is that work?",
            "So obviously got resolved but it's dependencies pendency accuracy or by 40% of the time.",
            "So that's sort of just a little.",
            "Very much didn't actually work out of baseline and turned out that work very, very little, because if he doesn't randomly without words and sentence, it works a little bit better than that, so that barely had come up with a learning method that works worse than random.",
            "Interesting to know."
        ],
        [
            "Why did it work with the vendor?",
            "Look at language.",
            "There are all kinds of dependencies in language, alot of which aren't syntactic structures.",
            "So here's the sentence on West narrowly passed image Bill what you're wanting to learn is that passing bills is a good dependency, and you're hoping to learn that because you think the passing bills happens alot.",
            "So you'll pick that up as a dependency.",
            "That is that you don't pick that up.",
            "You pick up more strongly that Congress and bills are associated.",
            "So you make a link between Congress and bills.",
            "On the more extreme example of that is something like this.",
            "Expect Brush backs that know being balls.",
            "Now this is as much a foreign language for me as a lot of the audience here, but apparently these are terms in baseball, so you learn in Association between big balls and brush backs.",
            "Which is much stronger than any other Association that you could possibly get out of the sentence.",
            "So you learn topical associations, and so that's effectively a multiple patent model learns, which doesn't help it to words in text at all.",
            "Thinking on that, I mean, here's the kind of example with two.",
            "Using it right so?",
            "As well as whatever other 'cause it's got for that model, you know there's no reason to connect up this new with your problem.",
            "The other new with your there just perfectly with one according to that model, and so that suggests the kinds of things that we might want to add to the model.",
            "First, something deal with this and then something to deal with that."
        ],
        [
            "So the obvious next example is to say, well, we had a problem with words there, right?",
            "Yeah, Congress and bills and flashbacks of being balls, so maybe we can fix that out by going to parts of speech, 'cause then we won't have these kind of topical Association.",
            "So rather than trying to work over this will learn over part of speech classes."
        ],
        [
            "And doesn't work.",
            "Let's do another early work on that.",
            "So Carolyn Chiniak in 1992 had.",
            "Done models of expired grammar learning and if you have the simplest form expire, Grandma's completely equivalent to a dependency grammar.",
            "So they build such a model and conclude from that EM learning over this kind of grammar doesn't work.",
            "That was one of the motivations for leading people to construct.",
            "Resource is like the pantry may, which was you couldn't do it unsupervised, so they've done a little bit better than random, so that's good.",
            "But they have done very well.",
            "And then you can probably have done.",
            "Very well.",
            "Is yeah, so another simple baseline you can use for dependencies is connecting up, connecting up adjacent words say something to these numbers.",
            "These numbers were actually for undirected dependencies.",
            "Fancies with an error on one end shows the direction of dependencies.",
            "Or you could just look at the links and ignore the errors, and these results were actually for undirected dependencies, because that's what has happened did.",
            "Anyway, so just linking up adjacent words because dependencies are often with adjacent words works better than the."
        ],
        [
            "Results.",
            "OK, but well, these kind of facts or suggest what kind of things you need from model to actually work.",
            "And if you look at the kinds of things that you need for a model to actually work and then you go off and look at some of the supervised puzzles that exists.",
            "In particular, the clearest example is probably something like Collins's parser, which is largely a dependency parser.",
            "What kinds of things that you put in place is probabilistic model well?",
            "The word classes in there.",
            "There are parts of speech to may useful for making the system work, but then there's also the factors that distance the house far away dependencies are and there are factors that count dependencies.",
            "I can skip over that point that you want to get this factor in that you have one subject but not three subjects, and so you can put in both of those factors into your model so you have a model that is over word classes and has a motion of distance.",
            "If you just have distance, it gives you a.",
            "Primitive.",
            "Because you can count whether you're the first thing or not.",
            "The first thing so you can count the ways linguists like the account where you don't.",
            "You have to just do present or not present.",
            "That's already now work so you know I have a model that works.",
            "I'm somewhat above just linking up adjacent."
        ],
        [
            "Leads into how you can do constituency parsing.",
            "So although inside pendency grammar there's a latent notion of constituency that you can take closures of sets of directed dependencies, and that they give you constituents.",
            "Constituency is very late in the representation.",
            "You have to really dig it out, and none of the probabilistic factors directly pay attention.",
            "So it seems like.",
            "But see.",
            "Directly something you want to be capturing to get the boundaries of things, and that will be very useful for language learning as it is very useful for all kinds of parts and made although dependency parsing this become very popular late way, I think to my mind, actually there's a kind of confusion there that people like to people like the output of dependency parsers because they kind of look like semantic.",
            "Static black kind of representations of the the obvious thing you want to use for lots of tasks where you're doing kind of shallow meaning interpretation, but that doesn't actually mean in terms of doing parsing as well as possible.",
            "That dependency very representations give you the best representations to do puzzles.",
            "I think actually Instituions is useful.",
            "Even some of these results for unsupervised learning."
        ],
        [
            "Well, the classic results are doing.",
            "Unsupervised constituency learning will also bad.",
            "He always results in that.",
            "I'm using probabilistic models.",
            "He said she's with Larry and Young and so they said, taken Droopy, CFG mom, the expectation maximization algorithm and the CFG."
        ],
        [
            "And so.",
            "So that then leads into more, while some people done the Citroen C grammar learning, there have been various results in various people Apopka mainstream linguistics, and we can evaluate these kind of results against entry may, but most of the early results are actually."
        ],
        [
            "And in particular, for enough that they fell below very simple baseline.",
            "So if your English is very simple, baseline is to assume uniform by branching structure.",
            "That's not true of all languages.",
            "Some other languages have much more left branching structure, but for English is a good baseline and it gives you about 46% of getting their constituents divide, which is again better than some field work."
        ],
        [
            "So we wanted to try and do better than that.",
            "And here's one story as to how you can come to where this work started from.",
            "Starting in the early 90s and clicking by Nicki and various others that there was this strain of work.",
            "Distributional clustering, which I think was the timer real eye opener.",
            "So many people and showing that you can actually learn stuff very successfully.",
            "I'm just having a lot of corpus text and the idea was that you just took a word and then it's environments which is the word to the left.",
            "And you collected at the bunch of those environments, so you've seen it with president in a whole bunch of places, and then use that as the basis of cross training.",
            "So your clustering words based on it left and right context, and doing that just works really successfully.",
            "You can get out syntactic categories which are actually quite good syntactic categories."
        ],
        [
            "The one way of thinking of what we're doing is the say or can you do the same thing?",
            "This impacts.",
            "What would distributional clustering and syntax will apply?",
            "Well, you have a constituent computer constituent now like fell in September, which has its internal spam and then it has a context.",
            "So here it's context is after payrolls and before the end of the sentence, payrolls fell in those accused of constituent by achieving situation.",
            "And it also has a context.",
            "Actually in September, can we use these kinds of spans and context to learn syntax and or same way that distribution constrained word word classes?",
            "But they can actually work with distributional clustering is sort of the natural way that fits into this literature relay.",
            "But I mean that wasn't actually the starting place to where this idea came from.",
            "The actual starting place from the idea was when the user was in Sydney, I taught intro syntax where we talked, the kind of.",
            "Kind of descriptive, old-fashioned syntax, and at least in the US, doesn't get very much, so I think they can get somewhat more that in the UK, and so you know I taught things like test for constituency and, well, one of the things that I used to say about tests for constituents City is that looking at internal structure of Frasers normally doesn't make it very easy to tell whether things should be grouped together as the same class of constituent or not.",
            "So you have something like English now.",
            "Regular noun phrase followed by a prepositional phrase.",
            "You know the man in the center of the room, or something like that.",
            "How he meant to tell that I and the man in the center of the room look alike?",
            "You can't but a way that you might can learn about constituency and what things should be the same phrase or class is looking at the context that they appear in that if you find things that appear in the same context, that's actually a much more useful notion of finding constituency, and so that was actually LED in the 1st place to this trying to build.",
            "A model that worked with context 'cause the observation was that PCF's only directly work with the spans.",
            "You can tell a little bit of a story of well, this is inside outside algorithm and somewhere the outside comes into the story, but directly the probabilities are only over the spans and the learning procedure makes no direct use and I would argue no effective use whatsoever of the context in which things occur, whereas that seems to be what's where the actual in."
        ],
        [
            "For May shun this useful resides.",
            "OK, well we could try and do distributional clustering of Frasers and try and learn something.",
            "Does that work well?",
            "It sort of seems like there's still 1 missing idea which is distributional classes are easy to find.",
            "All of these are distributional classes, but some of them aren't phrases.",
            "So of the with without men kind of ones that rents also mentioned aren't phrases where some of them like in the end on time for now.",
            "Are phrases.",
            "And this is just a plausibility argument, but I can present it anyway that if you if you take this kind of representation of contexts and you have phrases of different classes and you do a principle components analysis and draw your picture with the first 2 dimensions, and if you have things that are phrases of different classes, they're pretty well separated in the first 2 dimensions, you can see them quite well, whereas if you take.",
            "All substrings of words and represent them by their left and right contexts, and do your PCA and map the first 2 principal components.",
            "You have a complete mess.",
            "You have no good separation of things that are constituents and aren't constituents.",
            "Now that's only plausibility argument 'cause it could be that the third dimension is a really good one.",
            "If you looked at it, and you'll be able to see it all."
        ],
        [
            "There.",
            "But that kind of motivates wanting to put in something else in there, so we want to make use of contexts, but then we want to.",
            "Avoid we want to be able to tell which things are constituents and effectively the idea of how to tell which things are constituents is to say, well, we have to find things that that will tile into a complete powers of the sentence.",
            "OK, and then another of the problems with PCF models is that they have because if they have the lot of hidden structure above them they have all of these bad symmetries which you don't have any evidence for how to break when you're starting to learn.",
            "So we wanted to model that kind of much more directly connected to what you saw on the surface so that we didn't have too much hidden."
        ],
        [
            "Capture that made it difficult to learn and so that led to this constituent context model and so the idea of that is you had constituents which had a context and so you had probabilities over the constituent in the context and you had probabilities for every constituent in the sentence.",
            "So note that the crucial difference between this and the context free grammar is that although you have all the same constituents as a context free grammar, each one is directly rooted in a substring of words that you have no.",
            "Kind of higher level tree structure.",
            "Even though you can reconstruct it afterwards, and so we have all of the constituents.",
            "Then we also have probabilities over all the non constituents.",
            "Our complete probability of a representation is the product of probability of all the constituents bands times the product of the probability of all the non constituent spans.",
            "And then we learn a model of this form.",
            "Using the EM algorithm, so there's no excitingly different.",
            "Learn learning algorithm here is just using the EM algorithm, which I'm sure most of you already know.",
            "For people who don't know, it's kind of you know the standard sledgehammer of probabilistic learning models, where you take the stuff that you don't know and you have guesses at what it is based on what you can see, and then you presume your guesses are right and you count how often different things occur, and then you re estimate your probabilities.",
            "Can you repeat that process over and over again and you after awhile it converges and learn something and you hope that what?"
        ],
        [
            "Flint is something good.",
            "OK yeah, I just threw in this slide during yours talk.",
            "This is one of the little details that normally doesn't appear in the 20 minute version and I'd better rush but but related exactly to something that he was saying.",
            "So you know, for them you have to start somewhere.",
            "You have to have an initialization of your grammar and well, we were wanting to have sort of.",
            "You know, default.",
            "We know nothing's little Yugi's possible.",
            "Initialization of the grammar and well, you could think that the way to do that is to start with all possible trees and put a uniform probability distribution over every possible tree and start with that.",
            "It turns out that actually if you do that for the CCM model, it works a lot less well than the results that we actually present.",
            "So the results that we actually present.",
            "Use a different initialization of the grammar, and that initialization is the split uniform initialization and the grandma and this kind of a little bit gets at the skew trees that you have was talking about while being weaker than specifically biasing things to splits, so I mean.",
            "So if you take a tree uniform initialization, the vast majority of trees, and therefore the bulk of the probability mass, is in the non skewed direction.",
            "So you get fundamentally the wrong prior for how natural languages are.",
            "If you take split uniform.",
            "So what you're saying is I've got the whole span and I'm going to make a first split of the span, and I've got a uniform probability over where to make my first split.",
            "And then you do that recursively for the sub spans.",
            "Just that little change doesn't give you a very profound bias, but it doesn't mean that you're more likely to get skewed starts."
        ],
        [
            "OK, so that gave us some decent results, so right branch just left a lot and that's because this is now moving to the Wall Street Journal corpus that other people have talked about earlier rather than the earlier work was on the latest corpus.",
            "But this model performed nicely above the right branching baseline.",
            "And it produced puzzles that were not too unreasonable.",
            "So here's the screen was a sea of red, and so it's gotten this PP structure.",
            "It's gotten this bigger sea of red NP structure.",
            "For this example.",
            "The main thing that's wrong is it's attached the verb to the subject before it attaches it to the."
        ],
        [
            "Eject.",
            "OK, so at that point we had two models of trying to learn linguistic structure.",
            "The dependency model and the constituent can't."
        ],
        [
            "Ex model and so can you combine them.",
            "Yes you can.",
            "And so there's a particular way they're combined that I won't talk about in detail now, which was a product model.",
            "And so combining the two of them dependencies arrived about 65% of the time and the constituency is right about 88% of the time, which is actually really good in particular.",
            "This was this was a subtle point that also came up in rents.",
            "This talk that the models we're learning a binary branching, whereas the tree bank isn't binary branching.",
            "So we didn't.",
            "We didn't binary branch the tree bank, so there's kind of an upper bound as to how well we can do, because even if we get everything right in some sense we put in extra bracket so the upper bound is lower than 100, and so this is kind of fairly close to how well parser trained on.",
            "Supervised"
        ],
        [
            "That actually performs in this model.",
            "And we've been up to run up on other languages as well, and getting ingeneral fairly reasonable results.",
            "So it is noticeable Chinese hard language to learn that the Chinese results much worse than the results from English or German."
        ],
        [
            "OK.",
            "So."
        ],
        [
            "Maybe I should skip it little this."
        ],
        [
            "Point and say a little about the learning of semantic roles.",
            "OK, so now I'll talk for a bit about more recent bit of work, which is then trying to sort of push a little bit further how far we can do with unsupervised learning so well.",
            "It's one good thing to be able to get out syntactic structure that would be kind of nice to actually get further in the direction of linking from linguistic forms to water.",
            "The meaning of sentence is, and so as well as knowing about something about the syntax.",
            "We'd like to know about an event that he is the Marys, the agent of the opening the door is the thing that was opened and the instrument with the key and the tricky part there is that you get this variant expression so you can have the key open the door or the door opens.",
            "You can have different semantic roles that are being expressed by the same syntactic positions, and so somehow, Despite that we'd like to be able to learn in terms of semantic roles from."
        ],
        [
            "Supervise from text.",
            "And so this is another hidden data problem because it won't be present in the import in particular.",
            "For this work we are going to assume that we have the syntactic structure and actually we're going to assume that we have these surface dependencies.",
            "So effectively we're describing surface positions in terms of these surface dependencies like subject and object, which you can think of as just configurations and trees, and indeed how we assign these names is just using regular expressions that match configurations and trees.",
            "OK, and so this kind of semantic role labeling has been studied a lot in supervised learning in recent years, and the question is can you?"
        ],
        [
            "So do it is unsupervised learning.",
            "But why might you think that you should be able to do this?",
            "Well if he rolls, are nouns that we want to assign something to?",
            "Look at them in terms of grammatical roles.",
            "You get sets of nouns that fill the grammatical roles."
        ],
        [
            "But Alternatively, you could look at them in terms of semantic roles and get sets of nouns that fill the semantic roles, and the hope is that OK.",
            "This is really artificial data I've created, But the hope is that things cluster better in this space here, right that you're having better clustering in terms of semantic roles than you do in terms of the syntactic grammatical relations, so maybe we could use that to learn a model of how to put these things together.",
            "And as a result of that model, we'll learn about possible linkings as to how different semantic roles can be."
        ],
        [
            "Expressed as syntactic roles.",
            "OK, so this is what we have in full that we have observed data where we have had words, parts of speech in grammatical roles, and what we'd like to learn is the semantic roles that correspond to those, and we'll do that by learning this model of linkings between semantic roles and the."
        ],
        [
            "That they occur in.",
            "OK, so this is the model how it's built, so it's a generative probabilistic model.",
            "Again, we start with some verb and the verb is going to generate a linking, so linking is actually a complete specification of how the verbs Corre.",
            "Subcategorized arguments are going to be realized, so give will have the Arg 0, which is loosely the agent.",
            "This is using the kind of numbering rolls that's in Prop Bank.",
            "That's what we use this data as I explain in the moment.",
            "So the agent will become the subject, the.",
            "Theme will become the second object and the recipient will become the first object.",
            "OK, so from the linking so the linking is just a set of linkings from semantic roles to syntactic positions.",
            "Then we have an ordered linking, so the order linking is partly a technicality to put a surface ordering onto the way things occur and is also where adjuncts get introduced so they ordered.",
            "Linking says on the surface.",
            "Going from left to right, the things that appear are going to be the subject Temple.",
            "Add giant, then the second object in the first object.",
            "And then from the ordered linking we have surface expression.",
            "So this is the stuff that we assume is observed.",
            "So we have the first syntactic category subject.",
            "The sorry this partisan observed the role that it expresses and the word that is the head word.",
            "And so you have one of these.",
            "Each of those.",
            "So it's kind of a fairly redundant probabilistic model 'cause this part pulls apart.",
            "What's in there but adds in the."
        ],
        [
            "Deserved headwords.",
            "OK, so there's a huge space of possible linkings where we're linking from semantic roles to syntactic roles, and if we tried to sort of estimate that by some enormous multinomial that might not work very well.",
            "So the strategy we took was to decompose the linking into a series of derivation steps.",
            "So the derivation steps are kind of similar to the kind of violence changing operations learn many linguistic theories, and this gives us some much smaller parameter space with kind of informative price."
        ],
        [
            "Moreover So what we did to make a linking is that you started off with a verb which had an inherent violence pattern.",
            "Where to be honest, what we did was we assume that all verbs were underlyingly transitive 0 to the subject Arg.",
            "One went to the object and everything else was done by valence changing rule and so you could learn for a sequence of violence changing rules.",
            "So you could have.",
            "Some number of things where you added rolls or dropped rolls or rearranged rolls, and then once you run out of things to do, you know opt.",
            "OK, so that the."
        ],
        [
            "This was kind of our linking model that we learn probabilities over.",
            "OK, so at that, so that's kind of going from V2.",
            "L is really kind of this decomposed structured conditional probability distribution.",
            "That is how you get there that's mediated by those linkings.",
            "OK, so given that the stuff we can observe as the stuff that's shaded in Gray and we want to learn the stuff that is an observed, you might think that that's a 2 terrable space to deal with, but actually partly because of the way things are constrained in various ways.",
            "It actually turns out that a small enough space that again we can just learn it using the EM algorithm, that there's a fairly constrained space we can just enumerate over at all and just do exact EM computation.",
            "So that's what we do."
        ],
        [
            "OK, so how we did this was using Penn Treebank data to start off with but we wanted a good number of examples of each verb that we were trying to learn.",
            "So if we didn't have enough examples of a verb.",
            "We would add in extrovert instances from a verb in the 1st place.",
            "We took them from Blip, which was data that speed past with chiniak spas.",
            "Or if we still didn't have enough examples of a verb, we took extra examples which you passed ourselves from giga word.",
            "So we'd end up.",
            "Gosh I forget the exact number I think is of the order of 300 examples of each verb that we're going to learn on.",
            "And there are the usual questions as to how to evaluate this.",
            "If you're just again going to use the supervised gold standards as what you're going to evaluate the arm, the obvious thing to look at.",
            "Is the kind of evaluation that's being done for supervised, supervised, semantic role labeling, and in particular we tested on the prop bank data, where in particular, pop bank divides things into core roles and adjunct roles.",
            "VR games in our model as we built it, we were really learning about the sub categorization of verbs.",
            "There's nothing substantive in there that learns about adjuncts whatsoever.",
            "There's just you can add an adjunct operation in once you go to the order.",
            "Thinkings, so the evaluation we do is on course rolls only.",
            "So course rolls only means that you're not differentiating different subtypes of add junks, not that that's not an interesting problem to do, but identifying adjuncts is clearly kind of a different distributional clustering problem.",
            "You know it's arguable to the extent there's a clean division between arguments and adjuncts, but clearly at any rate the verbs are giving week information about the add junks, and you want to be learning using different methods to cluster add junks.",
            "Rather than using the properties of the verbs violence, which is what we built our model over.",
            "Again, we had a baseline model, so the baseline model he is, the subject is always the Arg zero.",
            "If there's one object or the second object if there are two, it's the Arg one, and if there's two objects, the first one is the arc two and assume everyone else.",
            "Everything else is an adjunct.",
            "This simple little wool is actually again a very high baseline, because it just turns out that you know in English most verbs.",
            "Transitive or most verbs are transitive and this gets them right if they are intransitive, more of them have an agent than a patient.",
            "Just get some right so you get a lot of the stuff right by just doing this."
        ],
        [
            "And nothing else.",
            "And so these are the results as to how far this gets us.",
            "So I mean there are two possible tasks here.",
            "Classification means I'll tell you which of the arguments, and then you just have to label them.",
            "Identification and classification means that you also have to pick out of the sentence which things are going to be the arguments.",
            "So if I just look at the results he so these are sort of F1 and precision and recall again that using my baseline you get about 3/4 of things right?",
            "This is the result of the Tuten over at our semantic role system that I was involved in.",
            "Which is one of the best semantic role systems that gets around 93% of things right?",
            "So unsupervised system is learning a fair bit.",
            "It's learning to do noticeably better than the baseline system, which includes, among other things, that it is learning that fault.",
            "Associations between zero and subject, and our guanzon objects, because that's not actually information that's given to it, but it's still well below the results of supervised systems.",
            "You can sort of see.",
            "This more clearly, perhaps in the classification, that the sort of the amount of the space that is getting right in the."
        ],
        [
            "Oceans.",
            "OK, so these are the kind of things that can learn for its verb lexicons, so this is what it learns for the verb give.",
            "It's learning the most common pattern is the subjects, the Arg zero.",
            "The first object is the Arg to the recipient, and the second object linearly is the Arg one which is the theme.",
            "Then another common linking pattern is subjects at zero.",
            "The object.",
            "The first only.",
            "Object is the Arg one and then you have a two phrase for the R2, so that's nice.",
            "It's learned the ditransitive alternation, and then the third thing it gets here is subject to xargs zero and the only object is Arg one, and that's OK too.",
            "That's the she gave a lot of money, kind of usage of the verb give, so that's pretty nice.",
            "Here's an example with pay, which sort of starts nice, but then goes downhill a bit.",
            "So Bill paid the money.",
            "That's OK, is that subject and object which is the first one, then the second most common one is John paid the money for a track or something that's OK. Jeon paid that happens a fair bit.",
            "That's OK.",
            "This is where things go downhill a little bit.",
            "'cause John paid the money to the store keeper that were actually assigning the same role to PP 2's PP 4.",
            "Which is just wrong so.",
            "And then the last one here is Jaune paid.",
            "John paid Bill $5 so that that one is kind of doing the ditransitive version again, which is actually OK, so a lot of it's OK, but we get some."
        ],
        [
            "It's a bit mixed up.",
            "OK, so so so that at least shows a little bit of push it how we can push things further that we can build a model of verb linking that is again without labeled data for a lot of verbs that can learn linking patterns that look kind of like the linking patterns that linguists would propose for.",
            "It's still a fair bit below supervised models, but it's kind of doing decently above how well you know.",
            "Our quite informed baseline model does.",
            "It's again the case and this is a theme for both these pieces of work.",
            "Then getting things to work well requires carefully designed model structures.",
            "So I mean for both of these pieces of work at the end of the day, we're running the EM algorithm.",
            "There's no fancy stuff in there, but you know the versions of both of them that work are dependent on having model structures, which in both cases weren't the first one we tried that a model structures that actually makes salient the kind of generalizations that you.",
            "Need to be able to learn to learn language so effectively.",
            "What's left of Eugene in our models is somewhere is the combination of choosing the right kind of model structures and the bit of initialization that we give our models so that you know something like having the uniform split rather than the tree uniform initialization.",
            "We haven't actually composed these two models that I've talked about together, and so I don't have results from that, though I guess we all know what happens when you compose two systems, neither of which is perfect and the results become less perfect.",
            "But in principle you can compose these two systems together, and so that's I think at least, pointing nicely in the direction of how you can kind of get from plaintext to actually having at least as kind of a simple semantic representation of events that can be learned from data alone.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I mean a lot of my work in recent years is been in natural language processing, but I've been also interested in these linguistic issues about extend probabilistic models to language since there's this.",
                    "label": 0
                },
                {
                    "sent": "This is clear fact that has been suppressed for a long time, and a lot of the discussions of formal linguistics, but it was actually expressed very clearly.",
                    "label": 0
                },
                {
                    "sent": "It's appears text from 1921 by just begins chapter of his book, where talks about variation with the sentence.",
                    "label": 0
                },
                {
                    "sent": "Everyone knows that language is variable, and I think that's true for common person on the street and everyone else, but somehow, but that hasn't been the way in which linguistic theory has been instructed in the following.",
                    "label": 1
                },
                {
                    "sent": "80 years, and in particular I'd be interested in probabilistic notions of grandma that there's been a lot of probabilistic talk this morning already, so I won't.",
                    "label": 0
                },
                {
                    "sent": "About this for too long, but I mean in the general context of what's happened in linguistics and psycholinguistics they last 20 years.",
                    "label": 0
                },
                {
                    "sent": "I think enormous amount of literature has been either doing rules or you're doing your networks and that there hasn't been saved under representation of probabilistic ideas which allow you to have variation over and kind of more complex structures that people are more familiar with linguistic theories.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So linguistic theory is being very.",
                    "label": 0
                },
                {
                    "sent": "Around these ideas that you can't learn things in data because the data just isn't sufficient invents was already talking about some examples of that in his talk, whereas I mean I guess once guided, a lot of the work of recent computational linguistics is just how much data there is out there, and particularly for the case of child language learners horrify and came up with an estimate the children by adult board chief minus 20, actually.",
                    "label": 0
                },
                {
                    "sent": "That yeah, I'm 200 million words of language input.",
                    "label": 1
                },
                {
                    "sent": "You know that's a lot.",
                    "label": 0
                },
                {
                    "sent": "That's a lot.",
                    "label": 0
                },
                {
                    "sent": "I mean, we do have bigger corpora than that, but that's not even corpus.",
                    "label": 0
                },
                {
                    "sent": "And so you know, that's a huge amount of data, and so I wanted my work and being sort of well, how much can you do with that kind of amount of data?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Trying to build models that look at that empirically.",
                    "label": 0
                },
                {
                    "sent": "My cell linguistics is being sort of stuck in this model, which has been dominated by trumpski way even in this most recent writings that a lot of that kind of recent developments of formal linguistics is seen as having their roots in the logical structure of linguistic theory and trust in linguistics.",
                    "label": 0
                },
                {
                    "sent": "Last appointed boots and linguistic, but it's important to realize that means the log these ideas were developed in the 1950s and have a singular difference from the 1950s is that in the 1950s.",
                    "label": 1
                },
                {
                    "sent": "People knew.",
                    "label": 0
                },
                {
                    "sent": "In fact, through the 1970s.",
                    "label": 0
                },
                {
                    "sent": "About learning machine learning, looks really only starting in like this, not to be much more science about new machine learning, so it's the best way to sneak kind of actually putting some of these ideas about what kind of things we can learn.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And for Vincent singing before, I guess his prices are good.",
                    "label": 0
                },
                {
                    "sent": "Turn off on the sides.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is famous result by gold.",
                    "label": 1
                },
                {
                    "sent": "That service is sort of based off of Trump skin and this is.",
                    "label": 0
                },
                {
                    "sent": "So it's normally quoted as that you know you can't learn grammars of things like context free grammars from data alone.",
                    "label": 0
                },
                {
                    "sent": "I mean the result is actually stronger than that, and I think actually kind of going with the stronger resolved so that in some sense shows.",
                    "label": 0
                },
                {
                    "sent": "Use of this result is actually you can't even learn finite state languages, so regular languages that you can't learn regular languages from day one.",
                    "label": 0
                },
                {
                    "sent": "The only thing that you can learn from data alone in the sense as gold formalized is languages that are actually finite, that they produce a finite number of utterances.",
                    "label": 0
                },
                {
                    "sent": "And so I mean that kind of stuff.",
                    "label": 0
                },
                {
                    "sent": "This suggests that hold G he must have to find something.",
                    "label": 0
                },
                {
                    "sent": "Something wrongly, somehow.",
                    "label": 0
                },
                {
                    "sent": "Did you ever get such a strong result, which stands and so absolutely opposed what we see in the world around us, the kind of things that people can learn and thought himself to talk.",
                    "label": 0
                },
                {
                    "sent": "Spider bit about different ways in which she resolved to be worked around.",
                    "label": 0
                },
                {
                    "sent": "So what is the suggest that maybe children do pick up on subtle convert negative evidence?",
                    "label": 0
                },
                {
                    "sent": "So there's a thread of work, including by my colleague.",
                    "label": 0
                },
                {
                    "sent": "Stanford.",
                    "label": 0
                },
                {
                    "sent": "Exactly what happens by the law that although there isn't that much explicit correction, a lot of the time.",
                    "label": 0
                },
                {
                    "sent": "But there are lots of ways that kids can pick up.",
                    "label": 0
                },
                {
                    "sent": "They could have evidence one of the most obvious ones is whether they understood or not.",
                    "label": 0
                },
                {
                    "sent": "That is a form of negative evidence.",
                    "label": 1
                },
                {
                    "sent": "Another one is that even if parents don't explicitly correct a lot of times, they rephrase that they'll give answers to refer back to what the kids said and say it in different ways.",
                    "label": 0
                },
                {
                    "sent": "You can strip the language class with an egg knowledge that's effective program, but the other one is that gold assumed this.",
                    "label": 1
                },
                {
                    "sent": "Conditions.",
                    "label": 0
                },
                {
                    "sent": "We have adversarial condition of data presentation that you had to be able to be able to cope with, which doesn't seem like how the world actually is.",
                    "label": 0
                },
                {
                    "sent": "So if you change that condition, for example in some kind of probabilistic model.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That was proved very soon thereafter by pointing and sensually.",
                    "label": 0
                },
                {
                    "sent": "Show that if you have a prior on the likelihood of your class of grammars that you can actually learn where the little star somewhere there prefers to.",
                    "label": 0
                },
                {
                    "sent": "Technically the notion of learnability is being changed is no longer builds notion of learnability as probabilistic approach.",
                    "label": 0
                },
                {
                    "sent": "The right grammar, but you can learn not only the regular languages.",
                    "label": 0
                },
                {
                    "sent": "Finite state appamada also learn probabilistic context free grammars.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Please give us talking to it stabilizes and thankfully computational learning theory, linguist, guy and he wanted to say don't give me that haunting stuff.",
                    "label": 0
                },
                {
                    "sent": "It doesn't impress me and innocence, he's right.",
                    "label": 0
                },
                {
                    "sent": "This is kind of a little bit trivial when you come down to it, which is that if the effectively what you have in the morning model is that you kind of have an ordering of all possible grammars for languages by their prior probability.",
                    "label": 0
                },
                {
                    "sent": "And as you start to see enough data that you can tell that the predictions of some of those grammars.",
                    "label": 0
                },
                {
                    "sent": "Not too far from the data that are actually seeing that you kind of cost them off from the top of the list and your current grammar is the best one still on the list.",
                    "label": 0
                },
                {
                    "sent": "You know, that's kind of.",
                    "label": 0
                },
                {
                    "sent": "The result and so.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "That's kind of an interesting.",
                    "label": 0
                },
                {
                    "sent": "I mean, in particular, you've got a lot of prior knowledge there that ordered that produces your initial ordering results.",
                    "label": 0
                },
                {
                    "sent": "So in my own work I guess I computational learning theory is really hard stuff, so I've never really been here getting into doing computational learning theoretical results of how you could learn things, but what I think interested in is, can you actually empirically pursue this question of OK?",
                    "label": 0
                },
                {
                    "sent": "Give us some data?",
                    "label": 0
                },
                {
                    "sent": "Can we learn linguistic structure from the data?",
                    "label": 0
                },
                {
                    "sent": "And so today I'm going to talk about two studies.",
                    "label": 0
                },
                {
                    "sent": "So first of all, the work with Dan Klein on free structure learning.",
                    "label": 0
                },
                {
                    "sent": "I presume people wanted me to talk about that when I was invited.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this is work for a couple of years ago, and although I'm kind of out of order 'cause the previous two talks have already talked about how they can do better than this work.",
                    "label": 0
                },
                {
                    "sent": "But I will present this work anyway and I think the way it's kind of nice because it's it's motivated in a kind of a linguistic directions.",
                    "label": 0
                },
                {
                    "sent": "I believe the script for a moment.",
                    "label": 0
                },
                {
                    "sent": "Paul Smolensky attributes today broomall hard the remarks which I will loosely paraphrased roughly as follows.",
                    "label": 0
                },
                {
                    "sent": "Linguistic backwards, I spend all of my time where here how to come up with learning mechanisms that are powerful enough to learn the kinds of things that kids learn.",
                    "label": 0
                },
                {
                    "sent": "But numerous.",
                    "label": 0
                },
                {
                    "sent": "They spend all their time trying to constrain what their models are, so they couldn't possibly learn things that don't happen and.",
                    "label": 0
                },
                {
                    "sent": "I think this work is kind of nice day in the direction of trying to workout.",
                    "label": 0
                },
                {
                    "sent": "What are the minimal things that you need to assume to be able to get something that can learn?",
                    "label": 0
                },
                {
                    "sent": "You hear what little bits of knowledge of language for universal grammar do you have to put into something that's sufficient to bootstrap learning?",
                    "label": 0
                },
                {
                    "sent": "But then I'll talk about some more recent work with technical, which is then learning semantic roles.",
                    "label": 0
                },
                {
                    "sent": "Once you have some.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First task.",
                    "label": 0
                },
                {
                    "sent": "We want to structure over that which is.",
                    "label": 0
                },
                {
                    "sent": "People should be thinking should be possible.",
                    "label": 1
                },
                {
                    "sent": "Text that proved to be kind of a hard.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To get results from.",
                    "label": 0
                },
                {
                    "sent": "Some of the work on models of trying to do this and what doesn't work and what kind of properties of language.",
                    "label": 0
                },
                {
                    "sent": "It seems that you do need models.",
                    "label": 0
                },
                {
                    "sent": "One place to start is with models of dependencies of which words on my bridge words.",
                    "label": 0
                },
                {
                    "sent": "So here we have.",
                    "label": 0
                },
                {
                    "sent": "That's an amended bills at one flying field that is modifying Bill and can be learned.",
                    "label": 0
                },
                {
                    "sent": "Those kind of dependencies for data alone.",
                    "label": 0
                },
                {
                    "sent": "People have tried to do that using models that kind of stopped a lot of pets and try to link together words where they show, sometimes associations against you.",
                    "label": 0
                },
                {
                    "sent": "The model of words.",
                    "label": 0
                },
                {
                    "sent": "My pastor.",
                    "label": 0
                },
                {
                    "sent": "Play trivia model, which worked out with associations in the question, is that work?",
                    "label": 0
                },
                {
                    "sent": "So obviously got resolved but it's dependencies pendency accuracy or by 40% of the time.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of just a little.",
                    "label": 0
                },
                {
                    "sent": "Very much didn't actually work out of baseline and turned out that work very, very little, because if he doesn't randomly without words and sentence, it works a little bit better than that, so that barely had come up with a learning method that works worse than random.",
                    "label": 0
                },
                {
                    "sent": "Interesting to know.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Why did it work with the vendor?",
                    "label": 0
                },
                {
                    "sent": "Look at language.",
                    "label": 0
                },
                {
                    "sent": "There are all kinds of dependencies in language, alot of which aren't syntactic structures.",
                    "label": 0
                },
                {
                    "sent": "So here's the sentence on West narrowly passed image Bill what you're wanting to learn is that passing bills is a good dependency, and you're hoping to learn that because you think the passing bills happens alot.",
                    "label": 0
                },
                {
                    "sent": "So you'll pick that up as a dependency.",
                    "label": 0
                },
                {
                    "sent": "That is that you don't pick that up.",
                    "label": 0
                },
                {
                    "sent": "You pick up more strongly that Congress and bills are associated.",
                    "label": 0
                },
                {
                    "sent": "So you make a link between Congress and bills.",
                    "label": 0
                },
                {
                    "sent": "On the more extreme example of that is something like this.",
                    "label": 0
                },
                {
                    "sent": "Expect Brush backs that know being balls.",
                    "label": 0
                },
                {
                    "sent": "Now this is as much a foreign language for me as a lot of the audience here, but apparently these are terms in baseball, so you learn in Association between big balls and brush backs.",
                    "label": 0
                },
                {
                    "sent": "Which is much stronger than any other Association that you could possibly get out of the sentence.",
                    "label": 0
                },
                {
                    "sent": "So you learn topical associations, and so that's effectively a multiple patent model learns, which doesn't help it to words in text at all.",
                    "label": 0
                },
                {
                    "sent": "Thinking on that, I mean, here's the kind of example with two.",
                    "label": 0
                },
                {
                    "sent": "Using it right so?",
                    "label": 0
                },
                {
                    "sent": "As well as whatever other 'cause it's got for that model, you know there's no reason to connect up this new with your problem.",
                    "label": 0
                },
                {
                    "sent": "The other new with your there just perfectly with one according to that model, and so that suggests the kinds of things that we might want to add to the model.",
                    "label": 0
                },
                {
                    "sent": "First, something deal with this and then something to deal with that.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the obvious next example is to say, well, we had a problem with words there, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, Congress and bills and flashbacks of being balls, so maybe we can fix that out by going to parts of speech, 'cause then we won't have these kind of topical Association.",
                    "label": 0
                },
                {
                    "sent": "So rather than trying to work over this will learn over part of speech classes.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Let's do another early work on that.",
                    "label": 0
                },
                {
                    "sent": "So Carolyn Chiniak in 1992 had.",
                    "label": 0
                },
                {
                    "sent": "Done models of expired grammar learning and if you have the simplest form expire, Grandma's completely equivalent to a dependency grammar.",
                    "label": 0
                },
                {
                    "sent": "So they build such a model and conclude from that EM learning over this kind of grammar doesn't work.",
                    "label": 1
                },
                {
                    "sent": "That was one of the motivations for leading people to construct.",
                    "label": 0
                },
                {
                    "sent": "Resource is like the pantry may, which was you couldn't do it unsupervised, so they've done a little bit better than random, so that's good.",
                    "label": 0
                },
                {
                    "sent": "But they have done very well.",
                    "label": 0
                },
                {
                    "sent": "And then you can probably have done.",
                    "label": 0
                },
                {
                    "sent": "Very well.",
                    "label": 0
                },
                {
                    "sent": "Is yeah, so another simple baseline you can use for dependencies is connecting up, connecting up adjacent words say something to these numbers.",
                    "label": 0
                },
                {
                    "sent": "These numbers were actually for undirected dependencies.",
                    "label": 0
                },
                {
                    "sent": "Fancies with an error on one end shows the direction of dependencies.",
                    "label": 0
                },
                {
                    "sent": "Or you could just look at the links and ignore the errors, and these results were actually for undirected dependencies, because that's what has happened did.",
                    "label": 1
                },
                {
                    "sent": "Anyway, so just linking up adjacent words because dependencies are often with adjacent words works better than the.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results.",
                    "label": 0
                },
                {
                    "sent": "OK, but well, these kind of facts or suggest what kind of things you need from model to actually work.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the kinds of things that you need for a model to actually work and then you go off and look at some of the supervised puzzles that exists.",
                    "label": 0
                },
                {
                    "sent": "In particular, the clearest example is probably something like Collins's parser, which is largely a dependency parser.",
                    "label": 0
                },
                {
                    "sent": "What kinds of things that you put in place is probabilistic model well?",
                    "label": 0
                },
                {
                    "sent": "The word classes in there.",
                    "label": 0
                },
                {
                    "sent": "There are parts of speech to may useful for making the system work, but then there's also the factors that distance the house far away dependencies are and there are factors that count dependencies.",
                    "label": 0
                },
                {
                    "sent": "I can skip over that point that you want to get this factor in that you have one subject but not three subjects, and so you can put in both of those factors into your model so you have a model that is over word classes and has a motion of distance.",
                    "label": 0
                },
                {
                    "sent": "If you just have distance, it gives you a.",
                    "label": 0
                },
                {
                    "sent": "Primitive.",
                    "label": 0
                },
                {
                    "sent": "Because you can count whether you're the first thing or not.",
                    "label": 0
                },
                {
                    "sent": "The first thing so you can count the ways linguists like the account where you don't.",
                    "label": 0
                },
                {
                    "sent": "You have to just do present or not present.",
                    "label": 0
                },
                {
                    "sent": "That's already now work so you know I have a model that works.",
                    "label": 0
                },
                {
                    "sent": "I'm somewhat above just linking up adjacent.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Leads into how you can do constituency parsing.",
                    "label": 0
                },
                {
                    "sent": "So although inside pendency grammar there's a latent notion of constituency that you can take closures of sets of directed dependencies, and that they give you constituents.",
                    "label": 0
                },
                {
                    "sent": "Constituency is very late in the representation.",
                    "label": 0
                },
                {
                    "sent": "You have to really dig it out, and none of the probabilistic factors directly pay attention.",
                    "label": 0
                },
                {
                    "sent": "So it seems like.",
                    "label": 0
                },
                {
                    "sent": "But see.",
                    "label": 0
                },
                {
                    "sent": "Directly something you want to be capturing to get the boundaries of things, and that will be very useful for language learning as it is very useful for all kinds of parts and made although dependency parsing this become very popular late way, I think to my mind, actually there's a kind of confusion there that people like to people like the output of dependency parsers because they kind of look like semantic.",
                    "label": 0
                },
                {
                    "sent": "Static black kind of representations of the the obvious thing you want to use for lots of tasks where you're doing kind of shallow meaning interpretation, but that doesn't actually mean in terms of doing parsing as well as possible.",
                    "label": 0
                },
                {
                    "sent": "That dependency very representations give you the best representations to do puzzles.",
                    "label": 0
                },
                {
                    "sent": "I think actually Instituions is useful.",
                    "label": 0
                },
                {
                    "sent": "Even some of these results for unsupervised learning.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, the classic results are doing.",
                    "label": 0
                },
                {
                    "sent": "Unsupervised constituency learning will also bad.",
                    "label": 0
                },
                {
                    "sent": "He always results in that.",
                    "label": 0
                },
                {
                    "sent": "I'm using probabilistic models.",
                    "label": 0
                },
                {
                    "sent": "He said she's with Larry and Young and so they said, taken Droopy, CFG mom, the expectation maximization algorithm and the CFG.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "So that then leads into more, while some people done the Citroen C grammar learning, there have been various results in various people Apopka mainstream linguistics, and we can evaluate these kind of results against entry may, but most of the early results are actually.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And in particular, for enough that they fell below very simple baseline.",
                    "label": 0
                },
                {
                    "sent": "So if your English is very simple, baseline is to assume uniform by branching structure.",
                    "label": 1
                },
                {
                    "sent": "That's not true of all languages.",
                    "label": 0
                },
                {
                    "sent": "Some other languages have much more left branching structure, but for English is a good baseline and it gives you about 46% of getting their constituents divide, which is again better than some field work.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we wanted to try and do better than that.",
                    "label": 0
                },
                {
                    "sent": "And here's one story as to how you can come to where this work started from.",
                    "label": 0
                },
                {
                    "sent": "Starting in the early 90s and clicking by Nicki and various others that there was this strain of work.",
                    "label": 0
                },
                {
                    "sent": "Distributional clustering, which I think was the timer real eye opener.",
                    "label": 0
                },
                {
                    "sent": "So many people and showing that you can actually learn stuff very successfully.",
                    "label": 0
                },
                {
                    "sent": "I'm just having a lot of corpus text and the idea was that you just took a word and then it's environments which is the word to the left.",
                    "label": 0
                },
                {
                    "sent": "And you collected at the bunch of those environments, so you've seen it with president in a whole bunch of places, and then use that as the basis of cross training.",
                    "label": 0
                },
                {
                    "sent": "So your clustering words based on it left and right context, and doing that just works really successfully.",
                    "label": 0
                },
                {
                    "sent": "You can get out syntactic categories which are actually quite good syntactic categories.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The one way of thinking of what we're doing is the say or can you do the same thing?",
                    "label": 0
                },
                {
                    "sent": "This impacts.",
                    "label": 0
                },
                {
                    "sent": "What would distributional clustering and syntax will apply?",
                    "label": 0
                },
                {
                    "sent": "Well, you have a constituent computer constituent now like fell in September, which has its internal spam and then it has a context.",
                    "label": 0
                },
                {
                    "sent": "So here it's context is after payrolls and before the end of the sentence, payrolls fell in those accused of constituent by achieving situation.",
                    "label": 0
                },
                {
                    "sent": "And it also has a context.",
                    "label": 0
                },
                {
                    "sent": "Actually in September, can we use these kinds of spans and context to learn syntax and or same way that distribution constrained word word classes?",
                    "label": 1
                },
                {
                    "sent": "But they can actually work with distributional clustering is sort of the natural way that fits into this literature relay.",
                    "label": 0
                },
                {
                    "sent": "But I mean that wasn't actually the starting place to where this idea came from.",
                    "label": 0
                },
                {
                    "sent": "The actual starting place from the idea was when the user was in Sydney, I taught intro syntax where we talked, the kind of.",
                    "label": 0
                },
                {
                    "sent": "Kind of descriptive, old-fashioned syntax, and at least in the US, doesn't get very much, so I think they can get somewhat more that in the UK, and so you know I taught things like test for constituency and, well, one of the things that I used to say about tests for constituents City is that looking at internal structure of Frasers normally doesn't make it very easy to tell whether things should be grouped together as the same class of constituent or not.",
                    "label": 0
                },
                {
                    "sent": "So you have something like English now.",
                    "label": 0
                },
                {
                    "sent": "Regular noun phrase followed by a prepositional phrase.",
                    "label": 0
                },
                {
                    "sent": "You know the man in the center of the room, or something like that.",
                    "label": 0
                },
                {
                    "sent": "How he meant to tell that I and the man in the center of the room look alike?",
                    "label": 0
                },
                {
                    "sent": "You can't but a way that you might can learn about constituency and what things should be the same phrase or class is looking at the context that they appear in that if you find things that appear in the same context, that's actually a much more useful notion of finding constituency, and so that was actually LED in the 1st place to this trying to build.",
                    "label": 0
                },
                {
                    "sent": "A model that worked with context 'cause the observation was that PCF's only directly work with the spans.",
                    "label": 0
                },
                {
                    "sent": "You can tell a little bit of a story of well, this is inside outside algorithm and somewhere the outside comes into the story, but directly the probabilities are only over the spans and the learning procedure makes no direct use and I would argue no effective use whatsoever of the context in which things occur, whereas that seems to be what's where the actual in.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For May shun this useful resides.",
                    "label": 0
                },
                {
                    "sent": "OK, well we could try and do distributional clustering of Frasers and try and learn something.",
                    "label": 0
                },
                {
                    "sent": "Does that work well?",
                    "label": 0
                },
                {
                    "sent": "It sort of seems like there's still 1 missing idea which is distributional classes are easy to find.",
                    "label": 1
                },
                {
                    "sent": "All of these are distributional classes, but some of them aren't phrases.",
                    "label": 0
                },
                {
                    "sent": "So of the with without men kind of ones that rents also mentioned aren't phrases where some of them like in the end on time for now.",
                    "label": 1
                },
                {
                    "sent": "Are phrases.",
                    "label": 0
                },
                {
                    "sent": "And this is just a plausibility argument, but I can present it anyway that if you if you take this kind of representation of contexts and you have phrases of different classes and you do a principle components analysis and draw your picture with the first 2 dimensions, and if you have things that are phrases of different classes, they're pretty well separated in the first 2 dimensions, you can see them quite well, whereas if you take.",
                    "label": 0
                },
                {
                    "sent": "All substrings of words and represent them by their left and right contexts, and do your PCA and map the first 2 principal components.",
                    "label": 0
                },
                {
                    "sent": "You have a complete mess.",
                    "label": 0
                },
                {
                    "sent": "You have no good separation of things that are constituents and aren't constituents.",
                    "label": 0
                },
                {
                    "sent": "Now that's only plausibility argument 'cause it could be that the third dimension is a really good one.",
                    "label": 0
                },
                {
                    "sent": "If you looked at it, and you'll be able to see it all.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There.",
                    "label": 0
                },
                {
                    "sent": "But that kind of motivates wanting to put in something else in there, so we want to make use of contexts, but then we want to.",
                    "label": 0
                },
                {
                    "sent": "Avoid we want to be able to tell which things are constituents and effectively the idea of how to tell which things are constituents is to say, well, we have to find things that that will tile into a complete powers of the sentence.",
                    "label": 0
                },
                {
                    "sent": "OK, and then another of the problems with PCF models is that they have because if they have the lot of hidden structure above them they have all of these bad symmetries which you don't have any evidence for how to break when you're starting to learn.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to model that kind of much more directly connected to what you saw on the surface so that we didn't have too much hidden.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Capture that made it difficult to learn and so that led to this constituent context model and so the idea of that is you had constituents which had a context and so you had probabilities over the constituent in the context and you had probabilities for every constituent in the sentence.",
                    "label": 0
                },
                {
                    "sent": "So note that the crucial difference between this and the context free grammar is that although you have all the same constituents as a context free grammar, each one is directly rooted in a substring of words that you have no.",
                    "label": 0
                },
                {
                    "sent": "Kind of higher level tree structure.",
                    "label": 0
                },
                {
                    "sent": "Even though you can reconstruct it afterwards, and so we have all of the constituents.",
                    "label": 0
                },
                {
                    "sent": "Then we also have probabilities over all the non constituents.",
                    "label": 0
                },
                {
                    "sent": "Our complete probability of a representation is the product of probability of all the constituents bands times the product of the probability of all the non constituent spans.",
                    "label": 0
                },
                {
                    "sent": "And then we learn a model of this form.",
                    "label": 0
                },
                {
                    "sent": "Using the EM algorithm, so there's no excitingly different.",
                    "label": 0
                },
                {
                    "sent": "Learn learning algorithm here is just using the EM algorithm, which I'm sure most of you already know.",
                    "label": 0
                },
                {
                    "sent": "For people who don't know, it's kind of you know the standard sledgehammer of probabilistic learning models, where you take the stuff that you don't know and you have guesses at what it is based on what you can see, and then you presume your guesses are right and you count how often different things occur, and then you re estimate your probabilities.",
                    "label": 0
                },
                {
                    "sent": "Can you repeat that process over and over again and you after awhile it converges and learn something and you hope that what?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Flint is something good.",
                    "label": 0
                },
                {
                    "sent": "OK yeah, I just threw in this slide during yours talk.",
                    "label": 0
                },
                {
                    "sent": "This is one of the little details that normally doesn't appear in the 20 minute version and I'd better rush but but related exactly to something that he was saying.",
                    "label": 0
                },
                {
                    "sent": "So you know, for them you have to start somewhere.",
                    "label": 0
                },
                {
                    "sent": "You have to have an initialization of your grammar and well, we were wanting to have sort of.",
                    "label": 0
                },
                {
                    "sent": "You know, default.",
                    "label": 0
                },
                {
                    "sent": "We know nothing's little Yugi's possible.",
                    "label": 0
                },
                {
                    "sent": "Initialization of the grammar and well, you could think that the way to do that is to start with all possible trees and put a uniform probability distribution over every possible tree and start with that.",
                    "label": 0
                },
                {
                    "sent": "It turns out that actually if you do that for the CCM model, it works a lot less well than the results that we actually present.",
                    "label": 0
                },
                {
                    "sent": "So the results that we actually present.",
                    "label": 0
                },
                {
                    "sent": "Use a different initialization of the grammar, and that initialization is the split uniform initialization and the grandma and this kind of a little bit gets at the skew trees that you have was talking about while being weaker than specifically biasing things to splits, so I mean.",
                    "label": 0
                },
                {
                    "sent": "So if you take a tree uniform initialization, the vast majority of trees, and therefore the bulk of the probability mass, is in the non skewed direction.",
                    "label": 0
                },
                {
                    "sent": "So you get fundamentally the wrong prior for how natural languages are.",
                    "label": 0
                },
                {
                    "sent": "If you take split uniform.",
                    "label": 1
                },
                {
                    "sent": "So what you're saying is I've got the whole span and I'm going to make a first split of the span, and I've got a uniform probability over where to make my first split.",
                    "label": 0
                },
                {
                    "sent": "And then you do that recursively for the sub spans.",
                    "label": 0
                },
                {
                    "sent": "Just that little change doesn't give you a very profound bias, but it doesn't mean that you're more likely to get skewed starts.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that gave us some decent results, so right branch just left a lot and that's because this is now moving to the Wall Street Journal corpus that other people have talked about earlier rather than the earlier work was on the latest corpus.",
                    "label": 0
                },
                {
                    "sent": "But this model performed nicely above the right branching baseline.",
                    "label": 0
                },
                {
                    "sent": "And it produced puzzles that were not too unreasonable.",
                    "label": 0
                },
                {
                    "sent": "So here's the screen was a sea of red, and so it's gotten this PP structure.",
                    "label": 0
                },
                {
                    "sent": "It's gotten this bigger sea of red NP structure.",
                    "label": 0
                },
                {
                    "sent": "For this example.",
                    "label": 0
                },
                {
                    "sent": "The main thing that's wrong is it's attached the verb to the subject before it attaches it to the.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Eject.",
                    "label": 0
                },
                {
                    "sent": "OK, so at that point we had two models of trying to learn linguistic structure.",
                    "label": 1
                },
                {
                    "sent": "The dependency model and the constituent can't.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ex model and so can you combine them.",
                    "label": 0
                },
                {
                    "sent": "Yes you can.",
                    "label": 0
                },
                {
                    "sent": "And so there's a particular way they're combined that I won't talk about in detail now, which was a product model.",
                    "label": 0
                },
                {
                    "sent": "And so combining the two of them dependencies arrived about 65% of the time and the constituency is right about 88% of the time, which is actually really good in particular.",
                    "label": 1
                },
                {
                    "sent": "This was this was a subtle point that also came up in rents.",
                    "label": 0
                },
                {
                    "sent": "This talk that the models we're learning a binary branching, whereas the tree bank isn't binary branching.",
                    "label": 0
                },
                {
                    "sent": "So we didn't.",
                    "label": 0
                },
                {
                    "sent": "We didn't binary branch the tree bank, so there's kind of an upper bound as to how well we can do, because even if we get everything right in some sense we put in extra bracket so the upper bound is lower than 100, and so this is kind of fairly close to how well parser trained on.",
                    "label": 0
                },
                {
                    "sent": "Supervised",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That actually performs in this model.",
                    "label": 0
                },
                {
                    "sent": "And we've been up to run up on other languages as well, and getting ingeneral fairly reasonable results.",
                    "label": 0
                },
                {
                    "sent": "So it is noticeable Chinese hard language to learn that the Chinese results much worse than the results from English or German.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe I should skip it little this.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Point and say a little about the learning of semantic roles.",
                    "label": 1
                },
                {
                    "sent": "OK, so now I'll talk for a bit about more recent bit of work, which is then trying to sort of push a little bit further how far we can do with unsupervised learning so well.",
                    "label": 0
                },
                {
                    "sent": "It's one good thing to be able to get out syntactic structure that would be kind of nice to actually get further in the direction of linking from linguistic forms to water.",
                    "label": 0
                },
                {
                    "sent": "The meaning of sentence is, and so as well as knowing about something about the syntax.",
                    "label": 1
                },
                {
                    "sent": "We'd like to know about an event that he is the Marys, the agent of the opening the door is the thing that was opened and the instrument with the key and the tricky part there is that you get this variant expression so you can have the key open the door or the door opens.",
                    "label": 0
                },
                {
                    "sent": "You can have different semantic roles that are being expressed by the same syntactic positions, and so somehow, Despite that we'd like to be able to learn in terms of semantic roles from.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Supervise from text.",
                    "label": 0
                },
                {
                    "sent": "And so this is another hidden data problem because it won't be present in the import in particular.",
                    "label": 0
                },
                {
                    "sent": "For this work we are going to assume that we have the syntactic structure and actually we're going to assume that we have these surface dependencies.",
                    "label": 0
                },
                {
                    "sent": "So effectively we're describing surface positions in terms of these surface dependencies like subject and object, which you can think of as just configurations and trees, and indeed how we assign these names is just using regular expressions that match configurations and trees.",
                    "label": 0
                },
                {
                    "sent": "OK, and so this kind of semantic role labeling has been studied a lot in supervised learning in recent years, and the question is can you?",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So do it is unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "But why might you think that you should be able to do this?",
                    "label": 0
                },
                {
                    "sent": "Well if he rolls, are nouns that we want to assign something to?",
                    "label": 0
                },
                {
                    "sent": "Look at them in terms of grammatical roles.",
                    "label": 0
                },
                {
                    "sent": "You get sets of nouns that fill the grammatical roles.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But Alternatively, you could look at them in terms of semantic roles and get sets of nouns that fill the semantic roles, and the hope is that OK.",
                    "label": 0
                },
                {
                    "sent": "This is really artificial data I've created, But the hope is that things cluster better in this space here, right that you're having better clustering in terms of semantic roles than you do in terms of the syntactic grammatical relations, so maybe we could use that to learn a model of how to put these things together.",
                    "label": 0
                },
                {
                    "sent": "And as a result of that model, we'll learn about possible linkings as to how different semantic roles can be.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Expressed as syntactic roles.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what we have in full that we have observed data where we have had words, parts of speech in grammatical roles, and what we'd like to learn is the semantic roles that correspond to those, and we'll do that by learning this model of linkings between semantic roles and the.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That they occur in.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the model how it's built, so it's a generative probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "Again, we start with some verb and the verb is going to generate a linking, so linking is actually a complete specification of how the verbs Corre.",
                    "label": 0
                },
                {
                    "sent": "Subcategorized arguments are going to be realized, so give will have the Arg 0, which is loosely the agent.",
                    "label": 0
                },
                {
                    "sent": "This is using the kind of numbering rolls that's in Prop Bank.",
                    "label": 0
                },
                {
                    "sent": "That's what we use this data as I explain in the moment.",
                    "label": 0
                },
                {
                    "sent": "So the agent will become the subject, the.",
                    "label": 0
                },
                {
                    "sent": "Theme will become the second object and the recipient will become the first object.",
                    "label": 0
                },
                {
                    "sent": "OK, so from the linking so the linking is just a set of linkings from semantic roles to syntactic positions.",
                    "label": 0
                },
                {
                    "sent": "Then we have an ordered linking, so the order linking is partly a technicality to put a surface ordering onto the way things occur and is also where adjuncts get introduced so they ordered.",
                    "label": 0
                },
                {
                    "sent": "Linking says on the surface.",
                    "label": 0
                },
                {
                    "sent": "Going from left to right, the things that appear are going to be the subject Temple.",
                    "label": 0
                },
                {
                    "sent": "Add giant, then the second object in the first object.",
                    "label": 0
                },
                {
                    "sent": "And then from the ordered linking we have surface expression.",
                    "label": 0
                },
                {
                    "sent": "So this is the stuff that we assume is observed.",
                    "label": 0
                },
                {
                    "sent": "So we have the first syntactic category subject.",
                    "label": 0
                },
                {
                    "sent": "The sorry this partisan observed the role that it expresses and the word that is the head word.",
                    "label": 0
                },
                {
                    "sent": "And so you have one of these.",
                    "label": 0
                },
                {
                    "sent": "Each of those.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of a fairly redundant probabilistic model 'cause this part pulls apart.",
                    "label": 0
                },
                {
                    "sent": "What's in there but adds in the.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Deserved headwords.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's a huge space of possible linkings where we're linking from semantic roles to syntactic roles, and if we tried to sort of estimate that by some enormous multinomial that might not work very well.",
                    "label": 1
                },
                {
                    "sent": "So the strategy we took was to decompose the linking into a series of derivation steps.",
                    "label": 1
                },
                {
                    "sent": "So the derivation steps are kind of similar to the kind of violence changing operations learn many linguistic theories, and this gives us some much smaller parameter space with kind of informative price.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Moreover So what we did to make a linking is that you started off with a verb which had an inherent violence pattern.",
                    "label": 0
                },
                {
                    "sent": "Where to be honest, what we did was we assume that all verbs were underlyingly transitive 0 to the subject Arg.",
                    "label": 0
                },
                {
                    "sent": "One went to the object and everything else was done by valence changing rule and so you could learn for a sequence of violence changing rules.",
                    "label": 0
                },
                {
                    "sent": "So you could have.",
                    "label": 0
                },
                {
                    "sent": "Some number of things where you added rolls or dropped rolls or rearranged rolls, and then once you run out of things to do, you know opt.",
                    "label": 0
                },
                {
                    "sent": "OK, so that the.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This was kind of our linking model that we learn probabilities over.",
                    "label": 0
                },
                {
                    "sent": "OK, so at that, so that's kind of going from V2.",
                    "label": 0
                },
                {
                    "sent": "L is really kind of this decomposed structured conditional probability distribution.",
                    "label": 0
                },
                {
                    "sent": "That is how you get there that's mediated by those linkings.",
                    "label": 0
                },
                {
                    "sent": "OK, so given that the stuff we can observe as the stuff that's shaded in Gray and we want to learn the stuff that is an observed, you might think that that's a 2 terrable space to deal with, but actually partly because of the way things are constrained in various ways.",
                    "label": 0
                },
                {
                    "sent": "It actually turns out that a small enough space that again we can just learn it using the EM algorithm, that there's a fairly constrained space we can just enumerate over at all and just do exact EM computation.",
                    "label": 0
                },
                {
                    "sent": "So that's what we do.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so how we did this was using Penn Treebank data to start off with but we wanted a good number of examples of each verb that we were trying to learn.",
                    "label": 0
                },
                {
                    "sent": "So if we didn't have enough examples of a verb.",
                    "label": 0
                },
                {
                    "sent": "We would add in extrovert instances from a verb in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "We took them from Blip, which was data that speed past with chiniak spas.",
                    "label": 0
                },
                {
                    "sent": "Or if we still didn't have enough examples of a verb, we took extra examples which you passed ourselves from giga word.",
                    "label": 0
                },
                {
                    "sent": "So we'd end up.",
                    "label": 0
                },
                {
                    "sent": "Gosh I forget the exact number I think is of the order of 300 examples of each verb that we're going to learn on.",
                    "label": 0
                },
                {
                    "sent": "And there are the usual questions as to how to evaluate this.",
                    "label": 0
                },
                {
                    "sent": "If you're just again going to use the supervised gold standards as what you're going to evaluate the arm, the obvious thing to look at.",
                    "label": 0
                },
                {
                    "sent": "Is the kind of evaluation that's being done for supervised, supervised, semantic role labeling, and in particular we tested on the prop bank data, where in particular, pop bank divides things into core roles and adjunct roles.",
                    "label": 0
                },
                {
                    "sent": "VR games in our model as we built it, we were really learning about the sub categorization of verbs.",
                    "label": 0
                },
                {
                    "sent": "There's nothing substantive in there that learns about adjuncts whatsoever.",
                    "label": 0
                },
                {
                    "sent": "There's just you can add an adjunct operation in once you go to the order.",
                    "label": 0
                },
                {
                    "sent": "Thinkings, so the evaluation we do is on course rolls only.",
                    "label": 0
                },
                {
                    "sent": "So course rolls only means that you're not differentiating different subtypes of add junks, not that that's not an interesting problem to do, but identifying adjuncts is clearly kind of a different distributional clustering problem.",
                    "label": 0
                },
                {
                    "sent": "You know it's arguable to the extent there's a clean division between arguments and adjuncts, but clearly at any rate the verbs are giving week information about the add junks, and you want to be learning using different methods to cluster add junks.",
                    "label": 0
                },
                {
                    "sent": "Rather than using the properties of the verbs violence, which is what we built our model over.",
                    "label": 0
                },
                {
                    "sent": "Again, we had a baseline model, so the baseline model he is, the subject is always the Arg zero.",
                    "label": 0
                },
                {
                    "sent": "If there's one object or the second object if there are two, it's the Arg one, and if there's two objects, the first one is the arc two and assume everyone else.",
                    "label": 0
                },
                {
                    "sent": "Everything else is an adjunct.",
                    "label": 0
                },
                {
                    "sent": "This simple little wool is actually again a very high baseline, because it just turns out that you know in English most verbs.",
                    "label": 0
                },
                {
                    "sent": "Transitive or most verbs are transitive and this gets them right if they are intransitive, more of them have an agent than a patient.",
                    "label": 0
                },
                {
                    "sent": "Just get some right so you get a lot of the stuff right by just doing this.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And nothing else.",
                    "label": 0
                },
                {
                    "sent": "And so these are the results as to how far this gets us.",
                    "label": 0
                },
                {
                    "sent": "So I mean there are two possible tasks here.",
                    "label": 0
                },
                {
                    "sent": "Classification means I'll tell you which of the arguments, and then you just have to label them.",
                    "label": 0
                },
                {
                    "sent": "Identification and classification means that you also have to pick out of the sentence which things are going to be the arguments.",
                    "label": 0
                },
                {
                    "sent": "So if I just look at the results he so these are sort of F1 and precision and recall again that using my baseline you get about 3/4 of things right?",
                    "label": 0
                },
                {
                    "sent": "This is the result of the Tuten over at our semantic role system that I was involved in.",
                    "label": 0
                },
                {
                    "sent": "Which is one of the best semantic role systems that gets around 93% of things right?",
                    "label": 0
                },
                {
                    "sent": "So unsupervised system is learning a fair bit.",
                    "label": 0
                },
                {
                    "sent": "It's learning to do noticeably better than the baseline system, which includes, among other things, that it is learning that fault.",
                    "label": 0
                },
                {
                    "sent": "Associations between zero and subject, and our guanzon objects, because that's not actually information that's given to it, but it's still well below the results of supervised systems.",
                    "label": 0
                },
                {
                    "sent": "You can sort of see.",
                    "label": 0
                },
                {
                    "sent": "This more clearly, perhaps in the classification, that the sort of the amount of the space that is getting right in the.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oceans.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are the kind of things that can learn for its verb lexicons, so this is what it learns for the verb give.",
                    "label": 0
                },
                {
                    "sent": "It's learning the most common pattern is the subjects, the Arg zero.",
                    "label": 0
                },
                {
                    "sent": "The first object is the Arg to the recipient, and the second object linearly is the Arg one which is the theme.",
                    "label": 0
                },
                {
                    "sent": "Then another common linking pattern is subjects at zero.",
                    "label": 0
                },
                {
                    "sent": "The object.",
                    "label": 0
                },
                {
                    "sent": "The first only.",
                    "label": 0
                },
                {
                    "sent": "Object is the Arg one and then you have a two phrase for the R2, so that's nice.",
                    "label": 0
                },
                {
                    "sent": "It's learned the ditransitive alternation, and then the third thing it gets here is subject to xargs zero and the only object is Arg one, and that's OK too.",
                    "label": 0
                },
                {
                    "sent": "That's the she gave a lot of money, kind of usage of the verb give, so that's pretty nice.",
                    "label": 0
                },
                {
                    "sent": "Here's an example with pay, which sort of starts nice, but then goes downhill a bit.",
                    "label": 0
                },
                {
                    "sent": "So Bill paid the money.",
                    "label": 0
                },
                {
                    "sent": "That's OK, is that subject and object which is the first one, then the second most common one is John paid the money for a track or something that's OK. Jeon paid that happens a fair bit.",
                    "label": 0
                },
                {
                    "sent": "That's OK.",
                    "label": 0
                },
                {
                    "sent": "This is where things go downhill a little bit.",
                    "label": 0
                },
                {
                    "sent": "'cause John paid the money to the store keeper that were actually assigning the same role to PP 2's PP 4.",
                    "label": 0
                },
                {
                    "sent": "Which is just wrong so.",
                    "label": 0
                },
                {
                    "sent": "And then the last one here is Jaune paid.",
                    "label": 0
                },
                {
                    "sent": "John paid Bill $5 so that that one is kind of doing the ditransitive version again, which is actually OK, so a lot of it's OK, but we get some.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's a bit mixed up.",
                    "label": 0
                },
                {
                    "sent": "OK, so so so that at least shows a little bit of push it how we can push things further that we can build a model of verb linking that is again without labeled data for a lot of verbs that can learn linking patterns that look kind of like the linking patterns that linguists would propose for.",
                    "label": 1
                },
                {
                    "sent": "It's still a fair bit below supervised models, but it's kind of doing decently above how well you know.",
                    "label": 0
                },
                {
                    "sent": "Our quite informed baseline model does.",
                    "label": 0
                },
                {
                    "sent": "It's again the case and this is a theme for both these pieces of work.",
                    "label": 0
                },
                {
                    "sent": "Then getting things to work well requires carefully designed model structures.",
                    "label": 0
                },
                {
                    "sent": "So I mean for both of these pieces of work at the end of the day, we're running the EM algorithm.",
                    "label": 0
                },
                {
                    "sent": "There's no fancy stuff in there, but you know the versions of both of them that work are dependent on having model structures, which in both cases weren't the first one we tried that a model structures that actually makes salient the kind of generalizations that you.",
                    "label": 1
                },
                {
                    "sent": "Need to be able to learn to learn language so effectively.",
                    "label": 0
                },
                {
                    "sent": "What's left of Eugene in our models is somewhere is the combination of choosing the right kind of model structures and the bit of initialization that we give our models so that you know something like having the uniform split rather than the tree uniform initialization.",
                    "label": 0
                },
                {
                    "sent": "We haven't actually composed these two models that I've talked about together, and so I don't have results from that, though I guess we all know what happens when you compose two systems, neither of which is perfect and the results become less perfect.",
                    "label": 0
                },
                {
                    "sent": "But in principle you can compose these two systems together, and so that's I think at least, pointing nicely in the direction of how you can kind of get from plaintext to actually having at least as kind of a simple semantic representation of events that can be learned from data alone.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}