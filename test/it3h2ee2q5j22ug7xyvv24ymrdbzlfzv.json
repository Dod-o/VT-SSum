{
    "id": "it3h2ee2q5j22ug7xyvv24ymrdbzlfzv",
    "title": "On the completeness of coding with image features",
    "info": {
        "author": [
            "Timo Dickscheid, University of Bonn"
        ],
        "published": "Dec. 1, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Computer Vision->Image & Video Retrieval"
        ]
    },
    "url": "http://videolectures.net/bmvc09_dickscheid_ccif/",
    "segmentation": [
        [
            "My name is Tim Medicaid and I'm.",
            "Happy to have the first presentation this morning especially."
        ],
        [
            "After this inspiring keynote that we just heard.",
            "Talk about local image features, or more specifically about the completeness of a particular set of local features with respect to the information that is contained in an image.",
            "So let us.",
            "Quickly get into the topic by having a look at some examples we have here an image of a system from the Celtics database overlaid with three different sets of local image features.",
            "On the left we have the popular lower detector which basically captures stock in bright blobs.",
            "In the middle we have a set of straight line segments.",
            "On the right we have a scale invariant junction detector.",
            "It's the recently proposed stop detector with recently proposed I mean that we will propose it in few weeks on ACV, so if you're interested about that.",
            "Just ask me, but what is important about this slide?",
            "Is that these three features that obviously capture really different parts of the image so they cover different image content?"
        ],
        [
            "And this can also be seen from the next slide.",
            "Here we have another image with less structured content, and we have three sets of different block detectors, and even among the plot detectors we see that the area covered by the images, and thus the information that is preserved by the images, is really different."
        ],
        [
            "So what are we doing if we use local features?",
            "In general we use local features to gain robustness against different problems, and we want to decrease the data volume significantly and what we're doing implicitly is we want to preserve important information that is contained in an image, and the question is, can we measure that?",
            "To our knowledge, the information contained in an image that is preserved by local features has not been addressed yet as a measure.",
            "If we lean back, there is already something that we can imagine the information contained in an image is usually expressed by the number of bits that is needed to code a certain area in the image.",
            "So we basically would expect the feature detector too.",
            "So give us features where we need lots of bits for coding the image and this is what we are."
        ],
        [
            "We propose we want to interpret feature detection and description as a coding scheme very similar to what we do when we use the JPEG compression.",
            "And in order to derive.",
            "A measure from this and an evaluation scheme we need to solve 3 problems.",
            "First of all, we need to representation for what we consider to be relevant image content.",
            "Second, we need a representation that is similar to this one that represents the information that is actually coded by a particular set of local features.",
            "And 3rd we need to business measure between these two representations and we would require this distance measure to be small if the set of feature covers the information mostly complete."
        ],
        [
            "This is what we are.",
            "What we want to do, and let's start with the first problem.",
            "How can we represent the relevant image content?",
            "We propose to just derive an entropy density, denoted pH here from the local image statistics you see are shaded relief plot of this entropy density on the upper right.",
            "And we can see, as expected, that the number of bits needed for coding, and thus the information is high on the border of the object depicted and we have some peaks here where the borders meet and it is rather low on very homogeneous areas, especially on the image border.",
            "That's what we wanted to express."
        ],
        [
            "So let us quickly have a look at how we derive that.",
            "This is only very broad overview.",
            "Now there are much more details in the paper.",
            "What we basically do is at each pixel given a certain Patch size, we just derived the entropy.",
            "That is the expected number of bits that would be needed for coding of dispatch, and we do this for different Patch sizes to take scale into account.",
            "You see that here by the expressed by the South and justice take the sum of all these different Patch sizes at each pixel.",
            "And finally we normalize this distribution over the whole image.",
            "And if we have a little bit closer look.",
            "That's how we compute the entropy.",
            "One might see that this is basically the entropy of a Gaussian variable, so we assume each local image Patch to be the sample of a Gaussian process, and we derive the entropy from the power spectrum P. That is, be cause the pixels in the local Patch have correlations that we do not know, so we have to take that into account and that is actually implicitly solved by computing the entropy in the power spectrum.",
            "What we also see is that we take into account the Sigma Square North, which is the noise variance of the image that's basically needed for fixing the significance level so that we do not not code any bits in homogeneous areas where we have only noise."
        ],
        [
            "Alright, having such an representation, we need something similar for the features.",
            "So given a particular feature set, we need the distribution that we can compare to the entropy density for this that represents this local features and we just proposed to use the other topic Gaussian for each of the features where the shape of the Gaussian is similar to the shape of the local feature.",
            "And then we assume each local feature to require a certain number of bits.",
            "Loading this could be done sophisticated, but we for simplicity we assume that every feature has a constant number of bits, same number of bits, and this number of bits is just spread over the Gaussian area.",
            "And these cautions obtained for each of the features in the set.",
            "Again just added over the whole image at normalized, so we get some distributions here.",
            "2 shaded reliefs again, one for each sequence and one for four scale invariant junction features."
        ],
        [
            "And having such two representations, we needed distance measure between the two, and we propose here to use the heling US metric.",
            "Which can be thought of as the following if we have two densities over the image domain, we can consider them to be unit vectors in the space of all possible densities.",
            "And the Hellinger distance is basically 1/2 of the distance between the piercing points on the unit sphere that these densities give us, and it's interesting the heling US metric is related to the callback lifeblood emergence, which is probably much more popular, but the kullback Leibler divergent is not a metric, so we couldn't use it here having a higher divergent in this case wouldn't mean we have a higher or lower completeness necessarily, so that's why we use the heling US metric."
        ],
        [
            "OK, let's have a look at the evaluation scheme that we propose it's.",
            "Quite intuitive and easy.",
            "Having one image, we detect different sets of features here.",
            "In this case we illustrated two sets of features on top we have a set of junction features and on button we have a much more complex set, including plops, junctions, and each segments.",
            "So intuitively we would say the set of features and the bottom is much more complete for representing the information in the image then the set of features.",
            "Depicted."
        ],
        [
            "Top.",
            "Then we go on by computing this feature coding densities for the two sets of features and computing the entropy density over the whole image domain as a reference."
        ],
        [
            "And we compute the Hellinger distance between of these two feature coding density with respect to the entropy density and what we gain our two distance measures.",
            "And as I said before, in this case we would clearly expect the distance two of the distance of feature set two with respect to the reference to be much smaller than that of feature set 1."
        ],
        [
            "So we tried that out using lots of different detectors and and images.",
            "Basically we used.",
            "Different image categories, most of them taking from the 15th in categories database we took 100 or more images for each of these of these sequences and what we see here, it will be considered more detail.",
            "Soon is basically the Hellinger distance of one single detector with respect to the entropy.",
            "So in this case here we see that."
        ],
        [
            "Detector that would be represented by the blue bar.",
            "Here has a distance of about 0.39 and the detector denoted by this bar here has a clearly a smaller distance, so the detector represented here is more complete on average on the aerial image data set and the little black bars just denote the standard deviations over the images of each category.",
            "So let's have a look at some of the results.",
            "First of all, we observed that the maximally stable extremal regions and the asphalt junctions seem to be most complete on average over these over these datasets.",
            "It's here denoted by red circle."
        ],
        [
            "The second we, it's quite significant that the straightedges obviously have a very bad completeness on the forest and products texture databases, which mainly include natural content.",
            "So it's quite intuitive.",
            "We don't have many straight edges here."
        ],
        [
            "And what we also see is we get very similar results for the Harris affine and the Harris LA Plus detectors which basically differ by the shape of the local feature windows.",
            "So there is a fine detector refines the feature window to an affine ellipse and it's quite interesting that this doesn't seem to improve on the completeness in this case."
        ],
        [
            "So far for the separate detectors, but what we were more interested in is what happens if we combine different sets of local feature detectors.",
            "If you want to use more than one detector to get more out of the image, which detectors are really complementarian, which detectors give us more information when we put them together and I will focus here on the degree to which detectors complement the lower detector only for the reason that most of the people are using the lower detector so it's most intuitive to have a look at that, but we have more results in the paper."
        ],
        [
            "And this sorry.",
            "What we see?"
        ],
        [
            "Is the left bar is the lower detector separately?",
            "It's the same that we saw on the plots before, but then we see a group of pairs of detectors.",
            "So here we have combined sets of two detectors, the lower complemented by Harrison finessing afine by edges by MCR and by the S5 JCT detector and on the right we see some combinations of three or more detectors.",
            "And."
        ],
        [
            "We investigate that over the different categories.",
            "We see that the lower detector is most efficiently complemented by the asphalt junctions and the maximally stable extremal regions on average over this datasets that we investigated.",
            "And what we also see."
        ],
        [
            "Which is very interesting.",
            "Taking three detectors at once, so combining the lower detector.",
            "The maximally stable extremal regions and their subjunction detector, we get a quite significant improvement on the completeness, and especially we get results that are almost as good as taking all the six detectors depicted here at once.",
            "So adding another detector to this set of three detectors doesn't improve the completeness anymore."
        ],
        [
            "And 3rd, we found that the Harris affine detector doesn't compliment the lower detector more than the Hessian affine detector, which was at first surprising for me because I expected to hear Steven detector to be more of a corner detector, so it should complement the props given by lower better than the hashing offended tector, which basically extracts plot similar to the lower detector.",
            "But that's from our evaluation scheme is not the case.",
            "So concluding from this one should also consider the Harris definite XR.",
            "Effectively to be more of a blob detector."
        ],
        [
            "So let us summarize what we saw.",
            "We propose to interpret feature detection and description as image coding, and this enabled us to derive a completeness measure of a set of features with respect to the information that is contained in an image, and we think that this approach is very helpful for distinguishing different detectors for getting an impression what information is actually.",
            "Preserved by detector, and more importantly, it helps us to make a good choice of combinations of detectors.",
            "So if we decide to take more than one detector, this measure helps us a lot to choose a combination that is really effective.",
            "And it also gives rise to the problem of mapping the space of all detectors.",
            "So one could imagine having such a distance measure that we just plot these detectors in the space that arises naturally from these distances and have something like a map of all the detectors we actually currently working on that to get an impression about this space.",
            "Alright.",
            "So I'm at the end."
        ],
        [
            "My talk already.",
            "Thanks for your attention and feel free to ask questions.",
            "Actually, we're actually trying.",
            "To detect these peaks of entropy and compare that to this ones, I haven't the results in here, but one important thing to notice that we must not necessarily derive a feature detector.",
            "That is, it expresses exactly this reference, but the point is that we use this reference for seeing the effects of combining different detectors.",
            "Yes, that depends on the application, and that might that might be true, yes.",
            "So, but still we have.",
            "We have a lot of good evaluations on repeatability and robustness of feature detectors, so that would be the first thing one would use to make a choice.",
            "But assuming that this is done and that I have a set of detectors that are really feasible for my application, then I can use this to check if they are complementary.",
            "And if."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My name is Tim Medicaid and I'm.",
                    "label": 0
                },
                {
                    "sent": "Happy to have the first presentation this morning especially.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "After this inspiring keynote that we just heard.",
                    "label": 0
                },
                {
                    "sent": "Talk about local image features, or more specifically about the completeness of a particular set of local features with respect to the information that is contained in an image.",
                    "label": 1
                },
                {
                    "sent": "So let us.",
                    "label": 0
                },
                {
                    "sent": "Quickly get into the topic by having a look at some examples we have here an image of a system from the Celtics database overlaid with three different sets of local image features.",
                    "label": 0
                },
                {
                    "sent": "On the left we have the popular lower detector which basically captures stock in bright blobs.",
                    "label": 0
                },
                {
                    "sent": "In the middle we have a set of straight line segments.",
                    "label": 0
                },
                {
                    "sent": "On the right we have a scale invariant junction detector.",
                    "label": 0
                },
                {
                    "sent": "It's the recently proposed stop detector with recently proposed I mean that we will propose it in few weeks on ACV, so if you're interested about that.",
                    "label": 0
                },
                {
                    "sent": "Just ask me, but what is important about this slide?",
                    "label": 0
                },
                {
                    "sent": "Is that these three features that obviously capture really different parts of the image so they cover different image content?",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this can also be seen from the next slide.",
                    "label": 0
                },
                {
                    "sent": "Here we have another image with less structured content, and we have three sets of different block detectors, and even among the plot detectors we see that the area covered by the images, and thus the information that is preserved by the images, is really different.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are we doing if we use local features?",
                    "label": 0
                },
                {
                    "sent": "In general we use local features to gain robustness against different problems, and we want to decrease the data volume significantly and what we're doing implicitly is we want to preserve important information that is contained in an image, and the question is, can we measure that?",
                    "label": 1
                },
                {
                    "sent": "To our knowledge, the information contained in an image that is preserved by local features has not been addressed yet as a measure.",
                    "label": 0
                },
                {
                    "sent": "If we lean back, there is already something that we can imagine the information contained in an image is usually expressed by the number of bits that is needed to code a certain area in the image.",
                    "label": 0
                },
                {
                    "sent": "So we basically would expect the feature detector too.",
                    "label": 0
                },
                {
                    "sent": "So give us features where we need lots of bits for coding the image and this is what we are.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We propose we want to interpret feature detection and description as a coding scheme very similar to what we do when we use the JPEG compression.",
                    "label": 1
                },
                {
                    "sent": "And in order to derive.",
                    "label": 0
                },
                {
                    "sent": "A measure from this and an evaluation scheme we need to solve 3 problems.",
                    "label": 1
                },
                {
                    "sent": "First of all, we need to representation for what we consider to be relevant image content.",
                    "label": 1
                },
                {
                    "sent": "Second, we need a representation that is similar to this one that represents the information that is actually coded by a particular set of local features.",
                    "label": 0
                },
                {
                    "sent": "And 3rd we need to business measure between these two representations and we would require this distance measure to be small if the set of feature covers the information mostly complete.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is what we are.",
                    "label": 0
                },
                {
                    "sent": "What we want to do, and let's start with the first problem.",
                    "label": 0
                },
                {
                    "sent": "How can we represent the relevant image content?",
                    "label": 1
                },
                {
                    "sent": "We propose to just derive an entropy density, denoted pH here from the local image statistics you see are shaded relief plot of this entropy density on the upper right.",
                    "label": 1
                },
                {
                    "sent": "And we can see, as expected, that the number of bits needed for coding, and thus the information is high on the border of the object depicted and we have some peaks here where the borders meet and it is rather low on very homogeneous areas, especially on the image border.",
                    "label": 0
                },
                {
                    "sent": "That's what we wanted to express.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let us quickly have a look at how we derive that.",
                    "label": 0
                },
                {
                    "sent": "This is only very broad overview.",
                    "label": 0
                },
                {
                    "sent": "Now there are much more details in the paper.",
                    "label": 0
                },
                {
                    "sent": "What we basically do is at each pixel given a certain Patch size, we just derived the entropy.",
                    "label": 0
                },
                {
                    "sent": "That is the expected number of bits that would be needed for coding of dispatch, and we do this for different Patch sizes to take scale into account.",
                    "label": 0
                },
                {
                    "sent": "You see that here by the expressed by the South and justice take the sum of all these different Patch sizes at each pixel.",
                    "label": 0
                },
                {
                    "sent": "And finally we normalize this distribution over the whole image.",
                    "label": 0
                },
                {
                    "sent": "And if we have a little bit closer look.",
                    "label": 0
                },
                {
                    "sent": "That's how we compute the entropy.",
                    "label": 0
                },
                {
                    "sent": "One might see that this is basically the entropy of a Gaussian variable, so we assume each local image Patch to be the sample of a Gaussian process, and we derive the entropy from the power spectrum P. That is, be cause the pixels in the local Patch have correlations that we do not know, so we have to take that into account and that is actually implicitly solved by computing the entropy in the power spectrum.",
                    "label": 1
                },
                {
                    "sent": "What we also see is that we take into account the Sigma Square North, which is the noise variance of the image that's basically needed for fixing the significance level so that we do not not code any bits in homogeneous areas where we have only noise.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, having such an representation, we need something similar for the features.",
                    "label": 0
                },
                {
                    "sent": "So given a particular feature set, we need the distribution that we can compare to the entropy density for this that represents this local features and we just proposed to use the other topic Gaussian for each of the features where the shape of the Gaussian is similar to the shape of the local feature.",
                    "label": 0
                },
                {
                    "sent": "And then we assume each local feature to require a certain number of bits.",
                    "label": 1
                },
                {
                    "sent": "Loading this could be done sophisticated, but we for simplicity we assume that every feature has a constant number of bits, same number of bits, and this number of bits is just spread over the Gaussian area.",
                    "label": 0
                },
                {
                    "sent": "And these cautions obtained for each of the features in the set.",
                    "label": 0
                },
                {
                    "sent": "Again just added over the whole image at normalized, so we get some distributions here.",
                    "label": 0
                },
                {
                    "sent": "2 shaded reliefs again, one for each sequence and one for four scale invariant junction features.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And having such two representations, we needed distance measure between the two, and we propose here to use the heling US metric.",
                    "label": 0
                },
                {
                    "sent": "Which can be thought of as the following if we have two densities over the image domain, we can consider them to be unit vectors in the space of all possible densities.",
                    "label": 0
                },
                {
                    "sent": "And the Hellinger distance is basically 1/2 of the distance between the piercing points on the unit sphere that these densities give us, and it's interesting the heling US metric is related to the callback lifeblood emergence, which is probably much more popular, but the kullback Leibler divergent is not a metric, so we couldn't use it here having a higher divergent in this case wouldn't mean we have a higher or lower completeness necessarily, so that's why we use the heling US metric.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, let's have a look at the evaluation scheme that we propose it's.",
                    "label": 1
                },
                {
                    "sent": "Quite intuitive and easy.",
                    "label": 0
                },
                {
                    "sent": "Having one image, we detect different sets of features here.",
                    "label": 1
                },
                {
                    "sent": "In this case we illustrated two sets of features on top we have a set of junction features and on button we have a much more complex set, including plops, junctions, and each segments.",
                    "label": 0
                },
                {
                    "sent": "So intuitively we would say the set of features and the bottom is much more complete for representing the information in the image then the set of features.",
                    "label": 0
                },
                {
                    "sent": "Depicted.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Top.",
                    "label": 0
                },
                {
                    "sent": "Then we go on by computing this feature coding densities for the two sets of features and computing the entropy density over the whole image domain as a reference.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we compute the Hellinger distance between of these two feature coding density with respect to the entropy density and what we gain our two distance measures.",
                    "label": 0
                },
                {
                    "sent": "And as I said before, in this case we would clearly expect the distance two of the distance of feature set two with respect to the reference to be much smaller than that of feature set 1.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we tried that out using lots of different detectors and and images.",
                    "label": 0
                },
                {
                    "sent": "Basically we used.",
                    "label": 0
                },
                {
                    "sent": "Different image categories, most of them taking from the 15th in categories database we took 100 or more images for each of these of these sequences and what we see here, it will be considered more detail.",
                    "label": 0
                },
                {
                    "sent": "Soon is basically the Hellinger distance of one single detector with respect to the entropy.",
                    "label": 0
                },
                {
                    "sent": "So in this case here we see that.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Detector that would be represented by the blue bar.",
                    "label": 0
                },
                {
                    "sent": "Here has a distance of about 0.39 and the detector denoted by this bar here has a clearly a smaller distance, so the detector represented here is more complete on average on the aerial image data set and the little black bars just denote the standard deviations over the images of each category.",
                    "label": 0
                },
                {
                    "sent": "So let's have a look at some of the results.",
                    "label": 0
                },
                {
                    "sent": "First of all, we observed that the maximally stable extremal regions and the asphalt junctions seem to be most complete on average over these over these datasets.",
                    "label": 0
                },
                {
                    "sent": "It's here denoted by red circle.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The second we, it's quite significant that the straightedges obviously have a very bad completeness on the forest and products texture databases, which mainly include natural content.",
                    "label": 1
                },
                {
                    "sent": "So it's quite intuitive.",
                    "label": 0
                },
                {
                    "sent": "We don't have many straight edges here.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And what we also see is we get very similar results for the Harris affine and the Harris LA Plus detectors which basically differ by the shape of the local feature windows.",
                    "label": 0
                },
                {
                    "sent": "So there is a fine detector refines the feature window to an affine ellipse and it's quite interesting that this doesn't seem to improve on the completeness in this case.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So far for the separate detectors, but what we were more interested in is what happens if we combine different sets of local feature detectors.",
                    "label": 0
                },
                {
                    "sent": "If you want to use more than one detector to get more out of the image, which detectors are really complementarian, which detectors give us more information when we put them together and I will focus here on the degree to which detectors complement the lower detector only for the reason that most of the people are using the lower detector so it's most intuitive to have a look at that, but we have more results in the paper.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this sorry.",
                    "label": 0
                },
                {
                    "sent": "What we see?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the left bar is the lower detector separately?",
                    "label": 0
                },
                {
                    "sent": "It's the same that we saw on the plots before, but then we see a group of pairs of detectors.",
                    "label": 0
                },
                {
                    "sent": "So here we have combined sets of two detectors, the lower complemented by Harrison finessing afine by edges by MCR and by the S5 JCT detector and on the right we see some combinations of three or more detectors.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We investigate that over the different categories.",
                    "label": 0
                },
                {
                    "sent": "We see that the lower detector is most efficiently complemented by the asphalt junctions and the maximally stable extremal regions on average over this datasets that we investigated.",
                    "label": 1
                },
                {
                    "sent": "And what we also see.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is very interesting.",
                    "label": 0
                },
                {
                    "sent": "Taking three detectors at once, so combining the lower detector.",
                    "label": 0
                },
                {
                    "sent": "The maximally stable extremal regions and their subjunction detector, we get a quite significant improvement on the completeness, and especially we get results that are almost as good as taking all the six detectors depicted here at once.",
                    "label": 1
                },
                {
                    "sent": "So adding another detector to this set of three detectors doesn't improve the completeness anymore.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And 3rd, we found that the Harris affine detector doesn't compliment the lower detector more than the Hessian affine detector, which was at first surprising for me because I expected to hear Steven detector to be more of a corner detector, so it should complement the props given by lower better than the hashing offended tector, which basically extracts plot similar to the lower detector.",
                    "label": 1
                },
                {
                    "sent": "But that's from our evaluation scheme is not the case.",
                    "label": 0
                },
                {
                    "sent": "So concluding from this one should also consider the Harris definite XR.",
                    "label": 0
                },
                {
                    "sent": "Effectively to be more of a blob detector.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let us summarize what we saw.",
                    "label": 0
                },
                {
                    "sent": "We propose to interpret feature detection and description as image coding, and this enabled us to derive a completeness measure of a set of features with respect to the information that is contained in an image, and we think that this approach is very helpful for distinguishing different detectors for getting an impression what information is actually.",
                    "label": 0
                },
                {
                    "sent": "Preserved by detector, and more importantly, it helps us to make a good choice of combinations of detectors.",
                    "label": 0
                },
                {
                    "sent": "So if we decide to take more than one detector, this measure helps us a lot to choose a combination that is really effective.",
                    "label": 0
                },
                {
                    "sent": "And it also gives rise to the problem of mapping the space of all detectors.",
                    "label": 1
                },
                {
                    "sent": "So one could imagine having such a distance measure that we just plot these detectors in the space that arises naturally from these distances and have something like a map of all the detectors we actually currently working on that to get an impression about this space.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "So I'm at the end.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My talk already.",
                    "label": 0
                },
                {
                    "sent": "Thanks for your attention and feel free to ask questions.",
                    "label": 1
                },
                {
                    "sent": "Actually, we're actually trying.",
                    "label": 0
                },
                {
                    "sent": "To detect these peaks of entropy and compare that to this ones, I haven't the results in here, but one important thing to notice that we must not necessarily derive a feature detector.",
                    "label": 0
                },
                {
                    "sent": "That is, it expresses exactly this reference, but the point is that we use this reference for seeing the effects of combining different detectors.",
                    "label": 0
                },
                {
                    "sent": "Yes, that depends on the application, and that might that might be true, yes.",
                    "label": 0
                },
                {
                    "sent": "So, but still we have.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of good evaluations on repeatability and robustness of feature detectors, so that would be the first thing one would use to make a choice.",
                    "label": 0
                },
                {
                    "sent": "But assuming that this is done and that I have a set of detectors that are really feasible for my application, then I can use this to check if they are complementary.",
                    "label": 0
                },
                {
                    "sent": "And if.",
                    "label": 0
                }
            ]
        }
    }
}