{
    "id": "izebydgb7pma7v36zb4uqqcvlv4a76my",
    "title": "Inferring Latent Functions with Gaussian Processes in Differential Equations",
    "info": {
        "author": [
            "Neil D. Lawrence, Department of Computer Science, University of Sheffield"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "December 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Gaussian Processes"
        ]
    },
    "url": "http://videolectures.net/dsb06_lawrence_ilfgp/",
    "segmentation": [
        [
            "Very good talks and apologies for not updating my slides with anything talked about then I should also say I'm not a physicist so you're unlikely to hear Euler or LaGrange Orlando van instead of words in this talk.",
            "And in fact the what I'm talking bout itself isn't really a stochastic differential equation, so then some sense I think it's not often considered stochastic process, but there's a lot of cross relations with this and sort of asstastic work that you've seen today.",
            "So the works joint work with Magnus Rattray, who's at University Manchester.",
            "I'm currently at Sheffield but I will be at Manchester from 1st of January and Greater Sanguineti who's sitting over there?",
            "Who's at the University of Sheffield?",
            "OK."
        ],
        [
            "OK, so.",
            "Just before I go ahead so the work is part of a Puma project for microarray analysis and just a little advert were going to be appointing a new post doc to replace Guido who got a lectureship just in September.",
            "So please, if you're interested, talk to me afterwards if you want to sort of come and work on this and all the source code in the talk is available online, you can go to my web page in that.",
            "So if you want to recreate any of the examples, it's all there.",
            "OK, so we are interested in inference about functions and we're interested in differential equation models of systems which has a functional known, and what we're going to focus on is a protein concentration example.",
            "So the function alone will be the concentration of a given."
        ],
        [
            "Protein.",
            "Obviously, Gaussian processes models are probabilistic models for functions, and therefore we're going to use them, which as they provide a sort of framework for performing inference about functions in the presence of uncertainty, which is exactly the situation."
        ],
        [
            "We have so just a quick overview of the application.",
            "I'll be talking about its transcription factor inference, which is modeling the dynamics of gene transcription.",
            "So we are interested in Furing transferring factor concentration in furing.",
            "The parameters of a transcription model.",
            "I mean the nature of the model doesn't matter too much 'cause it's just a differential equation model, but the application here is actually observing gene expression level.",
            "Knowing what the transcription factor which dictates the expression level of these genes is.",
            "And therefore deriving through a model of how that transcription works, what the transcription factor concentration was.",
            "So we know we infer these using a set of known target genes."
        ],
        [
            "So the way it works is we're going to treat the transmit transcription factor concentration as a latent function in a differential equation model, and then we're going to assume a Gaussian process prior distribution for the latent function.",
            "We then derive a Gaussian process covariance which jointly describes the genes and transcript describes the covariance between the observed gene.",
            "RNA concentrations and the transcription factor concentration, and then we maximize the likelihood of this covariance with respect to parameters, and in this case, most of the parameters are physically meaningful, so things like decay rates and sensitivities.",
            "OK, so the application background is obviously the sort of whole by Infomatics story understanding the pro cellular process, improving through mikres Chrome you no precipitation, etc.",
            "Quantitative description of regular medicine requires transcription factor concentrations.",
            "However, transcription factor concentrations are difficult to measure directly, so we're going to show how it can be inferred."
        ],
        [
            "I'll skip the advantages for GPS and go straight to the model, So what we're assuming is our data consists of T measurements, so a time series of M RNA expression levels for any different genes, and what we're going to relate what we're going to do is relate each gene expression, which I denote by X subscript J to the transcription factor concentration by the following differential equation.",
            "So what you're saying is the rate of production of a given gene is equal to some bass transcription rate plus some sensitivity.",
            "Times the transcription factor concentration minus into K, so this is a very simple model.",
            "We didn't propose this model.",
            "It was proposed by these guys here.",
            "Martina Barranco and his collaborators for modeling in a biological system will show later which is P53, which is a tumor suppressor.",
            "So we were excited that they use this model.",
            "Why were we excited 'cause it's a very simple linear differential equation and we understood that meant we could try doing some Gaussian processes on it.",
            "I'll sort of extend the linear stuff to a bit of nonlinear stuff using map approximations.",
            "Bit like previous talk a little later on.",
            "So the dependence of the M RNA transcription rate on the transcription factor is linear, so it's a nice system to model.",
            "So.",
            "I think of this as a simple differential equation.",
            "My definition of a simple differential equation is one I can solve."
        ],
        [
            "Itself because the last time I work with differential equations, courses and undergraduate, so you can get a closed form solution for this differential equation.",
            "And the interesting thing about it is it OK it starts with the constant term the gene expression is a constant plus the sensitivity times this decay time times this integral.",
            "Now the key point here is that this here is what you might call a linear operator.",
            "So by linear operator, if you're not used to that terminology, which certainly I'm not super used to it.",
            "So the way I look at it is instead of considering F is a continuous function, Annie to the Du as a continuous function.",
            "Just think of you as a discrete index on F&U as a discrete index on some vector E. Now this then just becomes the sum across those discrete indexes.",
            "So that's just an inner product.",
            "Now, if I then told you that this F view was a Gaussian variable, because this is simply an inner product on the Gaussian variable times by another vector.",
            "X is also a Gaussian variable.",
            "Now in the functional equivalent, this is the linear operator on functions.",
            "And this if I declare this to be a Gaussian process, this function here is also a Gaussian process.",
            "Furthermore, if I say if I use the RBF covariance to describe my prior over this function here, it turns out this integrals well, the covariance function for this guy is completely analytic to compute.",
            "The fact that you have many of them is Jay of any.",
            "Yeah, that's thanks.",
            "Manfred, that's good.",
            "I should mention that.",
            "So yeah, it's important.",
            "Manfred just asked is the fact that I've got many J important.",
            "Well, in the example will use later.",
            "We've got five and six J.",
            "That sort of figure now given the length of the time series and the number of parameters that are arising.",
            "So we've got this basil transcription.",
            "Rate the decay and the sensitivity.",
            "So every time we have a gene, these are gene specific.",
            "So every time we introduce in Eugene.",
            "You're introducing 3 new parameters.",
            "You've also got parameters of this covariance here, so there's a certain number of parameters, and you actually the more genes you have, the better determined your inference will become."
        ],
        [
            "OK, so the RBF kernel, I'm just using a little visualization 'cause I'm going to use this visualization again in the next slide.",
            "In terms of the covariance function or kernel, I'll use the words interchangeably.",
            "It obviously has this sort of form Now if we compare across different data points and we do a color map on it, what we see is of course the strong correlations of a point with itself.",
            "But as the distance between two points in time increases, these correlations fallaway going down to this green, which is zero when the points are very distant."
        ],
        [
            "These are samples and the key point of course versus some of the other talks is these are smooth functions there infinitely differentiable.",
            "Unlike the, I can never say alstine irlbeck process where the there it isn't differentiable, so we're in a different situation here, but the sort of modeling setup is very similar.",
            "If we sort of had no you process on this F, then we would be thinking of stochastic differential equations.",
            "So these are sort of samples with varying length scale.",
            "I'm sure a lot of you have seen that sort of thing before.",
            "But as we said, the equation describing the solution is in terms of the linear operator, and if we define this linear operator to take this form.",
            "Then we can rewrite."
        ],
        [
            "It's in this form now.",
            "The theory tells us that the covariance of this these function with this linear operator applied is just the double application of this linear operator on the original kernel.",
            "So in terms of what the integrals we need to do, our more explicitly.",
            "In other words, this is the integral that's required for determining the covariance matrix between two genes, so.",
            "It's a double integral.",
            "Over this product of exponentials and the covariance function.",
            "Now the nice thing is, of course that if we make this covariance function, the squared exponential this these integrals become tractable."
        ],
        [
            "So this is the sort of form.",
            "I mean, this is just to put some maths up because NIPS workshops always have a lot of maths in them.",
            "So it's quite a nasty form.",
            "It's got these sort of this is the correlation between two possibly different genes.",
            "Possibly the same gene.",
            "It doesn't matter, and it's dependent on those sensitivities.",
            "And this function H. Now this function is series of Earths.",
            "Now the earths that looks like you're adding to Earth, but actually most of the time if you look at it, you're subtracting two Earths away from each other and these things look like little bumps.",
            "There's this is not symmetric function itself, but it's the addition of this function applied twice, which is giving the symmetry.",
            "So it's sort of completely non obvious kernel function and took me like 2 pages of integrals to workout, but it's all doable."
        ],
        [
            "But as well as that, we also need cross covariance terms between genes and the governing transcription factor.",
            "So we want to perform inference about what's going on with the governing transcription factor given the jeans.",
            "And to do that as well show the equations later.",
            "We need these other cross covariance terms which are between F and a given gene, so it's just that integral applied once and for the RBF, that's the."
        ],
        [
            "Ola.",
            "OK, so if we've got these things then we can make predictions of the transcription factor concentration given the jeans.",
            "So this is a sort of concatenated a stacked vector of all the gene concentrations, and this is that covariance between the jeans and then here is the cross covariance between the jeans and the function, and this sort of recognizes the standard Gaussian process predictions.",
            "So it's sort of quite straightforward in the linear case, but all that was a lot of math.",
            "So what I'd like to do is just show what that looks like Pictoris."
        ],
        [
            "Really.",
            "So I've just got the solution of the differential equation again and what I've got here is two different genes dependent on a given transcription factor.",
            "So this top block is just the RBF covariance we saw earlier.",
            "Then these other blocks are the cross covariance between X&F and then this block.",
            "Here is the self covariance with ex with itself, and then there's the cross covariance with the X1 and X2.",
            "So the two different genes here have different parameters.",
            "One has a high decay rate and high sensitivity and one has a low decay rate.",
            "Anna low sensitivity.",
            "Now rather fun thing you can do with this is once you've defined this covariance you can say well what the typical samples from this covariance look like.",
            "So if I sample a transcription factor concentration.",
            "I can jointly sample a gene expert 2 gene expressions with that transcription fat."
        ],
        [
            "Concentration, so that's what we see on this slide here.",
            "So on the left you've got joint samples from that covariance matrix.",
            "Have just been looking at.",
            "You've got the blue, which is the driving function.",
            "The transcription factor concentration and the cyan which is X1.",
            "The high decay, high sensitivity gene expression and the red curve, which is X 2X to the loader K low sensitivity.",
            "So I'm getting that probably right."
        ],
        [
            "OK, I'll move to this.",
            "Woman is more obvious.",
            "This is a nice example as well, because in this case you've got blue is definitely the blue is the transcription factor concentration, the cyan is the response of X1.",
            "High decay, high sensitivity, and the red is the low decay low sensitivity.",
            "So why are these sort of models perhaps interesting?",
            "Well, Barranco and his collaborators were using them for ranked target prediction.",
            "You can sort of see here if you had a set of gene expressions that look like this.",
            "High decay, high sensitivity, you could cluster them.",
            "And have an idea about what's going on in the background, but if you got other low decay jeans?",
            "I mean, low sensitivity perhaps isn't such an issue, but a low decay gene will just go up and then stay high.",
            "You're not going to understand from simple clustering methods that it's a target of this transcription factor concentration, but although we didn't follow up that experiment, you can use these methods to do that sort of thing.",
            "So here's another example.",
            "Here's a transcription factor concentration, and here's the high decay, high sensitivity and the loader Caillou sensitivity on the right, or you're seeing is a sanity check, because you can rearrange the differential equation to solve for F, so you're seeing.",
            "The solution for F given by well the true F which is hidden by the other two and the numerical solution for F given by the Scion and red lines.",
            "Just as a sanity check.",
            "So you're sampling at the same time.",
            "The solution to this for F and two genes.",
            "OK, so one thing you'll notice is that."
        ],
        [
            "This slight time lag in when the Scion responded."
        ],
        [
            "You actually see that a little bit.",
            "Actually, it's a little bit hard for you to see.",
            "There's a little bit of a time lag before the high correlation start appearing here, which is why I wanted to show this slide again, so you can sort of see all these nice feature."
        ],
        [
            "In the covariance matrix.",
            "Another feature which is important is in gene expression.",
            "Very very important.",
            "I don't think this would work without this is estimating the underlying noise and this is just a plug for work.",
            "With doing that within the Puma project, which I do think is why this method works without proper estimation of the noise, I don't think this would work, so you then add a diagonal term which is actually not a spherical term 'cause we've got an estimate for each genes noise at each time point, and that's added to a diagonal term on that covariance.",
            "We've sort of seen before."
        ],
        [
            "So here's some results from an artificial data set.",
            "So here I'm inventing a transcription factor concentration.",
            "And what we're showing is the three Gaussian bumps, three Gaussian basis functions sum together.",
            "Then, because these are Gaussians, we can actually numerically compute for some different decays and different sensitivities what the gene expressions will look like.",
            "And then we've added noise to reflect what really goes on in the data.",
            "So we've got these five genes and action.",
            "In fact, the parameters used to generate these are taken from the really in Ferd parameters from the biological experiment later.",
            "So in some sense these are realistic parameters.",
            "So what's quite nice is we can then solve for what we think the generating expression level was an it looks a little bit like this, so it captures this structure here and then it does this sort of classic RBF thing of assuming that there's it has a constant conserve length scale across the whole space, so it's sort of sees wiggles often where there aren't wiggles, but it's basically quite a nice estimate.",
            "No, I learned the hyperparameters for that experiment.",
            "So although I should admit that I initialized them once and learned them and it worked and I said, oh I'll just check another initialization and it didn't work.",
            "So he's look at the noise level.",
            "It's quite high, and some of the structure is lossed in these gene lower jeans here.",
            "So I think this one, the blue one and the scion one dominate in determining the structure.",
            "And so for the first initialization I tried, it worked for the next initialization.",
            "And work and I didn't do sampling to create this experiment.",
            "It's just sort of a couple of initializations."
        ],
        [
            "OK, so the real system is recently published biological data set using a linear response model by Branco at all, and the focus of the study focused on the tumor suppressor protein P. 53.",
            "There were five targets there named here.",
            "Now these guys Barranco it'll use quadratic interpolation to try and estimate an observation of the gradient of the gene expression as well as the gene expression, and then they combine these to try and estimate F. They then used an enormous amount of MCMC sampling.",
            "To get their estimate of F&BS&D 10,000,000."
        ],
        [
            "Patiens so we analyze this data."
        ],
        [
            "Ourselves, and this is the result we got.",
            "So the Red Cross, because Barranco at all.",
            "They've got an implicit smoothness assumption, but it's difficult to make it explicit so effectively their estimates are point based and they only come at the times of the experiments.",
            "So at times 0242468 ten, so there's 123456 data points, 5 jeans here, so it's actually quite poorly determined system.",
            "Nuts, they do a lot there.",
            "Are there additional material?",
            "Is some pages long?",
            "Their mathematical material.",
            "So these are our estimate of the underlying function plus the sort of 95% credibility intervals.",
            "Here are the parameters.",
            "Now this is the one with the most differences.",
            "I'll come to reasons why there are strong differences between the basil transcription rates, so we're estimating quite different basil."
        ],
        [
            "Scription rates we did HMC sampling over the covariance parameters, so our results are in black and you see error bars on those.",
            "Their results are in white.",
            "They did this whole Markov chain Monte Carlo so they had error bars over those as well.",
            "This is the parameters for which we."
        ],
        [
            "Different from them the most the other parameters.",
            "We were quite similar know for P. 21 This is the sensitivity and it was fixed.",
            "That's because the system is quite poorly determined, so if you allow all the sensitivities to move, they sort of all move."
        ],
        [
            "Together and the same occurs with the case.",
            "So there was a fix on the decay, but basically it's an excellent correlation with."
        ],
        [
            "Their results.",
            "Hum.",
            "But on these results there's the oscillatory behavior I talked about, which is pretty ugly.",
            "I mean, Rasmussen and Williams talk about it.",
            "It's kind of a well known feature, but there are there in good accordance with the Barranco results.",
            "Now there are differences and those basil transcription rates.",
            "I told you about it, probably to do with technical details like the different methods used for probe level processing, 'cause they're sensitive to that.",
            "And we actually didn't implement.",
            "We should implement this constraint, and we can implement this constraint.",
            "They constrained the concentration of the transcription factor.",
            "At Times zero to be zero, we didn't do that.",
            "The key point though is our results took 13 minutes to produce an Barranco, and his collaborators took 10,000,000 iterations of Monte Carlo.",
            "So really, with the Gaussian process in place here, it gains an enormous amount 'cause we're not doing any sampling over that were basically integrating that out, and that's why things are so fast that people in the audience should be crying out that this is all wrong, because all of the quantities in that equation were positive, but direct examples from the GP will not be so no one said.",
            "Boy, you've got negative concentrations in your inference, so protein concentrations can't be negative, but we're going negative here, so we're even dropping below with the mean negative, so we're explaining part of the data by basically saying, oh and then the concentration went negative and the sign flipped, and this became a repressor rather than an activator.",
            "So that's not good."
        ],
        [
            "So what do we do?",
            "Well, we can just doing for time, by the way.",
            "Not too well.",
            "OK, so I'll be quite quick with that."
        ],
        [
            "Linear stuff, So what we can do is introduce a nonlinearity around the latent function, so that's what we're doing here.",
            "It's a little bit small.",
            "I apologize, so all we're doing here is putting G around the Gaussian process now.",
            "That means that this is no longer a linear operator on the Gaussian process, which is inside G. If G is nonlinear.",
            "So that means that in effect this guy isn't a Gaussian process either.",
            "But what we do then is therefore we just derive the functional gradient of this.",
            "And learn a map solution for FT Now we can also compute the sort of functional Hessian so we can estimate the marginal likelihood and do error bars and that sort of thing so.",
            "The key equation, I guess is in order to deal with the integral, we make a discrete approximation.",
            "Once you've made that discrete approximation, everything is quite trivial.",
            "There's no oil or LaGrange entering here.",
            "It's well, Magnus was the one who did the derivation, and he initially started out by doing everything Euler, LaGrange.",
            "But basically it comes out the same if you just assume the earth of parameters and don't account for the fact that they're generally a function, so skipping quickly through those, that's just the gradient."
        ],
        [
            "And the hassium so.",
            "Is that that's just more massive.",
            "Thing is, you can't go to nips and not put maths in your workshop talk.",
            "But you wanna go through the line by line.",
            "OK. OK, so but the first thing I'm going to do, or the first thing we did with this model is take G to be linear Now what's the point in that?",
            "'cause I just said nonlinearities.",
            "Well, if these linear, this whole process of map approximation of Hessian is exact apart from the slight approximation in the integral.",
            "So this A provides a sanity check.",
            "Because then we can do it 'cause it's a non stochastic covariance function.",
            "It should be exactly the same as the results we have before.",
            "If we use the RBF.",
            "But"
        ],
        [
            "What we want to do is use a covariance function like this because hopefully this will get rid of that weird oscillatory behavior.",
            "This is a nonstationary covariance function.",
            "Well, remembering going back to the group, this oscillatory behavior is characteristic of the RBF, but what's actually going on in P53 is there's a very high increase, and then there's a drop, and then it's probably flat.",
            "Well, that's what's believed to be going on, but because it needs to do the high increase, it needs a short length scale and then we think we're not sure about this.",
            "We think that this may be hallucination of this oscillatory behavior, so by using a nonstationary covariance function like this one.",
            "But now, like this one, we hope we can get rid of that.",
            "So to show what samples look like, this is an RBF."
        ],
        [
            "Then this is the MLP covariance function, so it sort of has this funky behavior."
        ],
        [
            "Here it's completely non stationary so by by forcing the activity to take place in one point, here we're actually get these points symmetric completely non stationary functions.",
            "How?",
            "Just to so here there's this.",
            "It's like a neural network.",
            "It's an infinite hidden layer, neural network, and the hyperparameters are the variance on the weights and the variance on the biases on the hidden layer.",
            "So here the weights and bias variance is quite high, so you get the basis functions appearing everywhere.",
            "In this example the bias variance is 0, so all the sigmoids are sitting on the zero point and you could sort of learn where that zero point is.",
            "If you want to move your activity so it's a nice way of getting nonstationary behavior.",
            "Out of covariance function.",
            "OK, so these are the sort of recreation of the results using the map approximation, which is an approximation in this case."
        ],
        [
            "With the RBF kernel and then here, this is with the MLP kernel.",
            "Again, we've got the negative 'cause we're not using the nonlinearity yet, so it does appear that there is something going on here.",
            "Still even in the MLP kernel, but in fact if we look at the log likelihood the log likelihood of this model is higher than the log likelihood of this MoD."
        ],
        [
            "Nonlinear responses, I'll just quickly show you some results from some different nonlinear responses.",
            "The first one we tried was just an exponential response model.",
            "Just constrain the protein concentration positive.",
            "So you're taking E to the proteins here.",
            "Effectively model modeling the protein in log space.",
            "We then also tried this guy.",
            "So why is this guy interesting?",
            "'cause he's a positive constraint but he's very linear in the positive half space.",
            "So in the negative input being negative space like this, he's down here.",
            "But then he basically becomes a straight line in the positive half space.",
            "So why is that good?",
            "Well that means that when we're in this half space, our class approximation will be really accurate.",
            "When we're in the negative half space, it won't be very accurate.",
            "But basically on the right hand side it becomes more accurate.",
            "And then this guy here is one of the directions will go in is to actually implement dynamics which saturate.",
            "And this is a sigmoid which saturates.",
            "In general, we haven't done the experiments yet, but we will replace that with Michaelis Menten dynamics, which are commonly used."
        ],
        [
            "So here's the map solutions.",
            "This is for the exponential, so you can see now they're all positive.",
            "This is the RBF kernel.",
            "This is the MLP.",
            "Note this sort of larabars here, 'cause you'll see if we move."
        ],
        [
            "Move to this is the function here.",
            "It's sort of like a log, it's the negative log of a logistic actually, but it's also the integral of logistic.",
            "Then we sort of get error bars which are a little bit tighter, and out of all the approximations we use.",
            "This had the highest marginal likelihood under the approximation to the marginal likelihood, so it beat that which doesn't have the oscillatory behavior whether that's significant or not, I don't know."
        ],
        [
            "His the response which saturates.",
            "So this is like a sigmoid response of F. So basically seeing it's constrained to lie between zero and one.",
            "The factor of three is just taken from the fact that I'm seeing in general the three is how high it goes, so I just we just put the factor of three in front of the sigmoid so that even though this is between zero and one is being multiplied up by three and let's say one nice feature about this nonlinearity is as well.",
            "Is as you move away from the data.",
            "Basically the error bars cover the entire region of where you've seen the data before, which is a characteristic.",
            "You might be quite keen on.",
            "In practice, you would actually this parameter here, which we've set as one you would typically learn this parameter and it has implications for what the chemic."
        ],
        [
            "Kinetics are.",
            "OK, so.",
            "Oh, I went too fast through the Hessian there, so we've described how GPS can be used in modeling the dynamics of a simple regulatory network motif.",
            "Now approach has advantages over standard parametric approaches.",
            "There's no need to restrict the inference to the observed time points.",
            "We get the temporal continuity of the inferred functions comes out naturally, so by that I mean, again, we're always seeing barrancos results mapped on to here, but we get the entire sequence of the protein coming out.",
            "The GPS allow us to handle uncertainty in a natural way.",
            "Now what do I mean by that?",
            "Well, if I went all the way, well, I probably have time to go all the way back to the differential equation.",
            "So going all the way back to the differential equation, I can briefly describe what Barranco was doing and why.",
            "It's perhaps not a good thing its way back.",
            "A little just go since there's a little bit of time and what's really nice about Gaussian process is in this application is that you don't need to do what they had to do.",
            "So here what they did is they move the decay onto the left hand side and the be on the left hand side and then divided by S. So then you have an expression for F in terms of things that you're observing, one of which is the gene expression level and one of which is the gradient of the gene expression level.",
            "But you don't observe directly the gradient.",
            "In expression level, So what they did is they interpolated between the gene expression levels with the polynomial and then estimated the gradient polynomial.",
            "But that's sort of inconsistent because you're using one noise modeling how that gradient is varying according to your noise model is not being properly accounted for.",
            "In effect, the Jeep is doing all that for you.",
            "It's in furing the function and dealing with the fact that the gradients are important as well, but it knows the gradients implicitly, so it deals properly with the uncertainty in those gradients.",
            "Let's just go back.",
            "To the solution.",
            "So that's a really nice feature of using GPS in this.",
            "In the in this application.",
            "So the MCMC parameter estimation in the discretized model can be very computationally expensive.",
            "The parameter estimation in our framework can be achieved naturally by type to maximum likelihood or by using efficient hybrid Monte Carlo.",
            "Here we did hybrid Monte Carlo 'cause I think in the end we were basically estimating something of the order of 20 parameters giving given 30 time point observations, which is obviously not that well determined.",
            "So an all the code for doing this is available online.",
            "If you want to take a look.",
            "Thank you so just future directions.",
            "I forgot I had this slide so the problem here is this is a very simple modeling situation.",
            "In fact we could get away with this because someone else had published quite a simple model.",
            "In practice we're ignoring lots of things, transcriptional delays.",
            "We've got a single transcription factor.",
            "Our ultimate goal would be to describe regulatory pathways with more genes, but we think that all these issues can be generally.",
            "Dealt with in the general framework we describe, which is using Gaussian processes to estimate them, but there's lots of greater computational."
        ],
        [
            "Difficulties just other relevant work that we've done is.",
            "We've looked at genome wide models of these systems, so there's also papers by Sebastian James doing this much simpler models.",
            "But genome wide and Guido talked about one of these models that the Dynamics Workshop yesterday.",
            "There's also better Markov chain Monte Carlo models than the one I described briefly from Barranco, and particularly point work by Simon Rogers and Module Army.",
            "That sort of doing this but without the Gaussian process in there.",
            "And sorry for any references I've missed, 'cause there's actually this is quite an active area of work, so I'm sure I've missed someone.",
            "And.",
            "Do you want to open question now or take questions now?",
            "First questions for them.",
            "OK and that's it.",
            "OK. Home, Eric.",
            "Very impressive, could you also?",
            "Maybe use that model for clustering.",
            "We have several of these and you don't know what they are.",
            "Clustering in the FFT's.",
            "I have several transcription factors and only being driven by home.",
            "The high sensitivity.",
            "Right at least might be able cluster.",
            "That's an interesting idea we haven't thought about that much 'cause what we were thinking, what we wanted to do next is trying to integrate some more transcription factors into the system 'cause we think you can trivially add an additional transcription factor, but I think that's a good idea.",
            "What bar encoded was basically rank targets predicted from that FT from P53 to try and give prediction of targets.",
            "But that's a good idea.",
            "Yeah, think about something like that.",
            "Wow.",
            "Model.",
            "Something different then.",
            "Ofer right, so that's an interesting question.",
            "We haven't actually looked.",
            "You mean using a more sensible form of quadrature?",
            "Something or increasing number of points in proximity of.",
            "I don't think.",
            "I don't think we need to do important sampling 'cause it's a 1D integral, but I think there's one thing we haven't looked at all is OK. Is there a much more sensible method of quadrature which we could get away with fewer points and and that would become important if we look at larger systems?",
            "Because basically you're you become cubic in the number of quadrature points, so that could become important.",
            "And it's definitely something we'll be looking at for that case.",
            "Oh when I listen to this question is that?",
            "In Haggen's original paper about vision, portraiture offers an optimal design.",
            "For that.",
            "An optimal design for.",
            "Alright, yeah, so you you mean what you're suggesting is you could rather funkily do Bayesian quadrature with a Gaussian process to get out the.",
            "Yeah, well, I know that Tony doesn't really think that Bayesian quadrature is the right way for 1D integrals, 'cause it's computationally quite expensive, but that's an interesting point.",
            "I think that.",
            "It's a good.",
            "It's a good thought because I think there's other reasons you might want to start doing that.",
            "Including stochastic if you want to do that, integrals to get over a stochastic covariance function, then whether you can leverage the uncertainty estimate you get from that.",
            "That's something I had not thought about, but I know Tony thinks that quadrature in Bayesian quadrature is probably useful for like 12 to 20 input dimensions.",
            "Not useful for a high dimensions and not useful for a low because, but I think there are other reasons why you might want to use that, that's interesting.",
            "And so should I do my open question.",
            "I just think the speaker."
        ],
        [
            "Century Cos pizza right down and open question at the end of my slowed slide slowed slides.",
            "I don't know if it's a good one because I missed the morning session so I missed the discussion going on there.",
            "But one thing I think that we're really keen for is well.",
            "I don't like the S word.",
            "I think it's like it's like should be a last resort.",
            "I think we end up.",
            "We haven't.",
            "We end up sampling with hybrid Monte Carlo at some point, but we do the integrals first if we can.",
            "I think Manfreds have touched on probably this morning when I wasn't here.",
            "His variational approximations with Cedric and John and we saw work earlier on variational approximations, but I've sort of gotten.",
            "We've done the Laplace approximation, which is, I think it's quite a bad idea in some sense.",
            "But how can you produce better posterior approximations without sampling is, to me, seems like a big open question, but maybe it's not a good one.",
            "I mean, yeah, that's it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very good talks and apologies for not updating my slides with anything talked about then I should also say I'm not a physicist so you're unlikely to hear Euler or LaGrange Orlando van instead of words in this talk.",
                    "label": 0
                },
                {
                    "sent": "And in fact the what I'm talking bout itself isn't really a stochastic differential equation, so then some sense I think it's not often considered stochastic process, but there's a lot of cross relations with this and sort of asstastic work that you've seen today.",
                    "label": 0
                },
                {
                    "sent": "So the works joint work with Magnus Rattray, who's at University Manchester.",
                    "label": 1
                },
                {
                    "sent": "I'm currently at Sheffield but I will be at Manchester from 1st of January and Greater Sanguineti who's sitting over there?",
                    "label": 0
                },
                {
                    "sent": "Who's at the University of Sheffield?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Just before I go ahead so the work is part of a Puma project for microarray analysis and just a little advert were going to be appointing a new post doc to replace Guido who got a lectureship just in September.",
                    "label": 1
                },
                {
                    "sent": "So please, if you're interested, talk to me afterwards if you want to sort of come and work on this and all the source code in the talk is available online, you can go to my web page in that.",
                    "label": 0
                },
                {
                    "sent": "So if you want to recreate any of the examples, it's all there.",
                    "label": 0
                },
                {
                    "sent": "OK, so we are interested in inference about functions and we're interested in differential equation models of systems which has a functional known, and what we're going to focus on is a protein concentration example.",
                    "label": 0
                },
                {
                    "sent": "So the function alone will be the concentration of a given.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Protein.",
                    "label": 0
                },
                {
                    "sent": "Obviously, Gaussian processes models are probabilistic models for functions, and therefore we're going to use them, which as they provide a sort of framework for performing inference about functions in the presence of uncertainty, which is exactly the situation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have so just a quick overview of the application.",
                    "label": 0
                },
                {
                    "sent": "I'll be talking about its transcription factor inference, which is modeling the dynamics of gene transcription.",
                    "label": 1
                },
                {
                    "sent": "So we are interested in Furing transferring factor concentration in furing.",
                    "label": 1
                },
                {
                    "sent": "The parameters of a transcription model.",
                    "label": 0
                },
                {
                    "sent": "I mean the nature of the model doesn't matter too much 'cause it's just a differential equation model, but the application here is actually observing gene expression level.",
                    "label": 1
                },
                {
                    "sent": "Knowing what the transcription factor which dictates the expression level of these genes is.",
                    "label": 0
                },
                {
                    "sent": "And therefore deriving through a model of how that transcription works, what the transcription factor concentration was.",
                    "label": 0
                },
                {
                    "sent": "So we know we infer these using a set of known target genes.",
                    "label": 1
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the way it works is we're going to treat the transmit transcription factor concentration as a latent function in a differential equation model, and then we're going to assume a Gaussian process prior distribution for the latent function.",
                    "label": 1
                },
                {
                    "sent": "We then derive a Gaussian process covariance which jointly describes the genes and transcript describes the covariance between the observed gene.",
                    "label": 0
                },
                {
                    "sent": "RNA concentrations and the transcription factor concentration, and then we maximize the likelihood of this covariance with respect to parameters, and in this case, most of the parameters are physically meaningful, so things like decay rates and sensitivities.",
                    "label": 0
                },
                {
                    "sent": "OK, so the application background is obviously the sort of whole by Infomatics story understanding the pro cellular process, improving through mikres Chrome you no precipitation, etc.",
                    "label": 0
                },
                {
                    "sent": "Quantitative description of regular medicine requires transcription factor concentrations.",
                    "label": 0
                },
                {
                    "sent": "However, transcription factor concentrations are difficult to measure directly, so we're going to show how it can be inferred.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'll skip the advantages for GPS and go straight to the model, So what we're assuming is our data consists of T measurements, so a time series of M RNA expression levels for any different genes, and what we're going to relate what we're going to do is relate each gene expression, which I denote by X subscript J to the transcription factor concentration by the following differential equation.",
                    "label": 1
                },
                {
                    "sent": "So what you're saying is the rate of production of a given gene is equal to some bass transcription rate plus some sensitivity.",
                    "label": 0
                },
                {
                    "sent": "Times the transcription factor concentration minus into K, so this is a very simple model.",
                    "label": 0
                },
                {
                    "sent": "We didn't propose this model.",
                    "label": 0
                },
                {
                    "sent": "It was proposed by these guys here.",
                    "label": 0
                },
                {
                    "sent": "Martina Barranco and his collaborators for modeling in a biological system will show later which is P53, which is a tumor suppressor.",
                    "label": 0
                },
                {
                    "sent": "So we were excited that they use this model.",
                    "label": 0
                },
                {
                    "sent": "Why were we excited 'cause it's a very simple linear differential equation and we understood that meant we could try doing some Gaussian processes on it.",
                    "label": 0
                },
                {
                    "sent": "I'll sort of extend the linear stuff to a bit of nonlinear stuff using map approximations.",
                    "label": 0
                },
                {
                    "sent": "Bit like previous talk a little later on.",
                    "label": 0
                },
                {
                    "sent": "So the dependence of the M RNA transcription rate on the transcription factor is linear, so it's a nice system to model.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I think of this as a simple differential equation.",
                    "label": 0
                },
                {
                    "sent": "My definition of a simple differential equation is one I can solve.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Itself because the last time I work with differential equations, courses and undergraduate, so you can get a closed form solution for this differential equation.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing about it is it OK it starts with the constant term the gene expression is a constant plus the sensitivity times this decay time times this integral.",
                    "label": 0
                },
                {
                    "sent": "Now the key point here is that this here is what you might call a linear operator.",
                    "label": 0
                },
                {
                    "sent": "So by linear operator, if you're not used to that terminology, which certainly I'm not super used to it.",
                    "label": 0
                },
                {
                    "sent": "So the way I look at it is instead of considering F is a continuous function, Annie to the Du as a continuous function.",
                    "label": 0
                },
                {
                    "sent": "Just think of you as a discrete index on F&U as a discrete index on some vector E. Now this then just becomes the sum across those discrete indexes.",
                    "label": 0
                },
                {
                    "sent": "So that's just an inner product.",
                    "label": 0
                },
                {
                    "sent": "Now, if I then told you that this F view was a Gaussian variable, because this is simply an inner product on the Gaussian variable times by another vector.",
                    "label": 0
                },
                {
                    "sent": "X is also a Gaussian variable.",
                    "label": 0
                },
                {
                    "sent": "Now in the functional equivalent, this is the linear operator on functions.",
                    "label": 0
                },
                {
                    "sent": "And this if I declare this to be a Gaussian process, this function here is also a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, if I say if I use the RBF covariance to describe my prior over this function here, it turns out this integrals well, the covariance function for this guy is completely analytic to compute.",
                    "label": 0
                },
                {
                    "sent": "The fact that you have many of them is Jay of any.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's thanks.",
                    "label": 0
                },
                {
                    "sent": "Manfred, that's good.",
                    "label": 0
                },
                {
                    "sent": "I should mention that.",
                    "label": 0
                },
                {
                    "sent": "So yeah, it's important.",
                    "label": 0
                },
                {
                    "sent": "Manfred just asked is the fact that I've got many J important.",
                    "label": 0
                },
                {
                    "sent": "Well, in the example will use later.",
                    "label": 0
                },
                {
                    "sent": "We've got five and six J.",
                    "label": 0
                },
                {
                    "sent": "That sort of figure now given the length of the time series and the number of parameters that are arising.",
                    "label": 0
                },
                {
                    "sent": "So we've got this basil transcription.",
                    "label": 0
                },
                {
                    "sent": "Rate the decay and the sensitivity.",
                    "label": 0
                },
                {
                    "sent": "So every time we have a gene, these are gene specific.",
                    "label": 0
                },
                {
                    "sent": "So every time we introduce in Eugene.",
                    "label": 0
                },
                {
                    "sent": "You're introducing 3 new parameters.",
                    "label": 0
                },
                {
                    "sent": "You've also got parameters of this covariance here, so there's a certain number of parameters, and you actually the more genes you have, the better determined your inference will become.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the RBF kernel, I'm just using a little visualization 'cause I'm going to use this visualization again in the next slide.",
                    "label": 0
                },
                {
                    "sent": "In terms of the covariance function or kernel, I'll use the words interchangeably.",
                    "label": 0
                },
                {
                    "sent": "It obviously has this sort of form Now if we compare across different data points and we do a color map on it, what we see is of course the strong correlations of a point with itself.",
                    "label": 0
                },
                {
                    "sent": "But as the distance between two points in time increases, these correlations fallaway going down to this green, which is zero when the points are very distant.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These are samples and the key point of course versus some of the other talks is these are smooth functions there infinitely differentiable.",
                    "label": 0
                },
                {
                    "sent": "Unlike the, I can never say alstine irlbeck process where the there it isn't differentiable, so we're in a different situation here, but the sort of modeling setup is very similar.",
                    "label": 0
                },
                {
                    "sent": "If we sort of had no you process on this F, then we would be thinking of stochastic differential equations.",
                    "label": 0
                },
                {
                    "sent": "So these are sort of samples with varying length scale.",
                    "label": 0
                },
                {
                    "sent": "I'm sure a lot of you have seen that sort of thing before.",
                    "label": 0
                },
                {
                    "sent": "But as we said, the equation describing the solution is in terms of the linear operator, and if we define this linear operator to take this form.",
                    "label": 0
                },
                {
                    "sent": "Then we can rewrite.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's in this form now.",
                    "label": 0
                },
                {
                    "sent": "The theory tells us that the covariance of this these function with this linear operator applied is just the double application of this linear operator on the original kernel.",
                    "label": 0
                },
                {
                    "sent": "So in terms of what the integrals we need to do, our more explicitly.",
                    "label": 1
                },
                {
                    "sent": "In other words, this is the integral that's required for determining the covariance matrix between two genes, so.",
                    "label": 0
                },
                {
                    "sent": "It's a double integral.",
                    "label": 0
                },
                {
                    "sent": "Over this product of exponentials and the covariance function.",
                    "label": 0
                },
                {
                    "sent": "Now the nice thing is, of course that if we make this covariance function, the squared exponential this these integrals become tractable.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the sort of form.",
                    "label": 0
                },
                {
                    "sent": "I mean, this is just to put some maths up because NIPS workshops always have a lot of maths in them.",
                    "label": 0
                },
                {
                    "sent": "So it's quite a nasty form.",
                    "label": 0
                },
                {
                    "sent": "It's got these sort of this is the correlation between two possibly different genes.",
                    "label": 0
                },
                {
                    "sent": "Possibly the same gene.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter, and it's dependent on those sensitivities.",
                    "label": 0
                },
                {
                    "sent": "And this function H. Now this function is series of Earths.",
                    "label": 0
                },
                {
                    "sent": "Now the earths that looks like you're adding to Earth, but actually most of the time if you look at it, you're subtracting two Earths away from each other and these things look like little bumps.",
                    "label": 0
                },
                {
                    "sent": "There's this is not symmetric function itself, but it's the addition of this function applied twice, which is giving the symmetry.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of completely non obvious kernel function and took me like 2 pages of integrals to workout, but it's all doable.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But as well as that, we also need cross covariance terms between genes and the governing transcription factor.",
                    "label": 0
                },
                {
                    "sent": "So we want to perform inference about what's going on with the governing transcription factor given the jeans.",
                    "label": 0
                },
                {
                    "sent": "And to do that as well show the equations later.",
                    "label": 0
                },
                {
                    "sent": "We need these other cross covariance terms which are between F and a given gene, so it's just that integral applied once and for the RBF, that's the.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ola.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we've got these things then we can make predictions of the transcription factor concentration given the jeans.",
                    "label": 0
                },
                {
                    "sent": "So this is a sort of concatenated a stacked vector of all the gene concentrations, and this is that covariance between the jeans and then here is the cross covariance between the jeans and the function, and this sort of recognizes the standard Gaussian process predictions.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of quite straightforward in the linear case, but all that was a lot of math.",
                    "label": 0
                },
                {
                    "sent": "So what I'd like to do is just show what that looks like Pictoris.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really.",
                    "label": 0
                },
                {
                    "sent": "So I've just got the solution of the differential equation again and what I've got here is two different genes dependent on a given transcription factor.",
                    "label": 0
                },
                {
                    "sent": "So this top block is just the RBF covariance we saw earlier.",
                    "label": 0
                },
                {
                    "sent": "Then these other blocks are the cross covariance between X&F and then this block.",
                    "label": 0
                },
                {
                    "sent": "Here is the self covariance with ex with itself, and then there's the cross covariance with the X1 and X2.",
                    "label": 0
                },
                {
                    "sent": "So the two different genes here have different parameters.",
                    "label": 0
                },
                {
                    "sent": "One has a high decay rate and high sensitivity and one has a low decay rate.",
                    "label": 0
                },
                {
                    "sent": "Anna low sensitivity.",
                    "label": 0
                },
                {
                    "sent": "Now rather fun thing you can do with this is once you've defined this covariance you can say well what the typical samples from this covariance look like.",
                    "label": 0
                },
                {
                    "sent": "So if I sample a transcription factor concentration.",
                    "label": 0
                },
                {
                    "sent": "I can jointly sample a gene expert 2 gene expressions with that transcription fat.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Concentration, so that's what we see on this slide here.",
                    "label": 0
                },
                {
                    "sent": "So on the left you've got joint samples from that covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Have just been looking at.",
                    "label": 0
                },
                {
                    "sent": "You've got the blue, which is the driving function.",
                    "label": 0
                },
                {
                    "sent": "The transcription factor concentration and the cyan which is X1.",
                    "label": 0
                },
                {
                    "sent": "The high decay, high sensitivity gene expression and the red curve, which is X 2X to the loader K low sensitivity.",
                    "label": 0
                },
                {
                    "sent": "So I'm getting that probably right.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I'll move to this.",
                    "label": 0
                },
                {
                    "sent": "Woman is more obvious.",
                    "label": 0
                },
                {
                    "sent": "This is a nice example as well, because in this case you've got blue is definitely the blue is the transcription factor concentration, the cyan is the response of X1.",
                    "label": 0
                },
                {
                    "sent": "High decay, high sensitivity, and the red is the low decay low sensitivity.",
                    "label": 0
                },
                {
                    "sent": "So why are these sort of models perhaps interesting?",
                    "label": 0
                },
                {
                    "sent": "Well, Barranco and his collaborators were using them for ranked target prediction.",
                    "label": 0
                },
                {
                    "sent": "You can sort of see here if you had a set of gene expressions that look like this.",
                    "label": 0
                },
                {
                    "sent": "High decay, high sensitivity, you could cluster them.",
                    "label": 0
                },
                {
                    "sent": "And have an idea about what's going on in the background, but if you got other low decay jeans?",
                    "label": 0
                },
                {
                    "sent": "I mean, low sensitivity perhaps isn't such an issue, but a low decay gene will just go up and then stay high.",
                    "label": 0
                },
                {
                    "sent": "You're not going to understand from simple clustering methods that it's a target of this transcription factor concentration, but although we didn't follow up that experiment, you can use these methods to do that sort of thing.",
                    "label": 0
                },
                {
                    "sent": "So here's another example.",
                    "label": 0
                },
                {
                    "sent": "Here's a transcription factor concentration, and here's the high decay, high sensitivity and the loader Caillou sensitivity on the right, or you're seeing is a sanity check, because you can rearrange the differential equation to solve for F, so you're seeing.",
                    "label": 0
                },
                {
                    "sent": "The solution for F given by well the true F which is hidden by the other two and the numerical solution for F given by the Scion and red lines.",
                    "label": 1
                },
                {
                    "sent": "Just as a sanity check.",
                    "label": 0
                },
                {
                    "sent": "So you're sampling at the same time.",
                    "label": 0
                },
                {
                    "sent": "The solution to this for F and two genes.",
                    "label": 0
                },
                {
                    "sent": "OK, so one thing you'll notice is that.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This slight time lag in when the Scion responded.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You actually see that a little bit.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's a little bit hard for you to see.",
                    "label": 0
                },
                {
                    "sent": "There's a little bit of a time lag before the high correlation start appearing here, which is why I wanted to show this slide again, so you can sort of see all these nice feature.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Another feature which is important is in gene expression.",
                    "label": 0
                },
                {
                    "sent": "Very very important.",
                    "label": 0
                },
                {
                    "sent": "I don't think this would work without this is estimating the underlying noise and this is just a plug for work.",
                    "label": 1
                },
                {
                    "sent": "With doing that within the Puma project, which I do think is why this method works without proper estimation of the noise, I don't think this would work, so you then add a diagonal term which is actually not a spherical term 'cause we've got an estimate for each genes noise at each time point, and that's added to a diagonal term on that covariance.",
                    "label": 1
                },
                {
                    "sent": "We've sort of seen before.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's some results from an artificial data set.",
                    "label": 0
                },
                {
                    "sent": "So here I'm inventing a transcription factor concentration.",
                    "label": 0
                },
                {
                    "sent": "And what we're showing is the three Gaussian bumps, three Gaussian basis functions sum together.",
                    "label": 0
                },
                {
                    "sent": "Then, because these are Gaussians, we can actually numerically compute for some different decays and different sensitivities what the gene expressions will look like.",
                    "label": 0
                },
                {
                    "sent": "And then we've added noise to reflect what really goes on in the data.",
                    "label": 0
                },
                {
                    "sent": "So we've got these five genes and action.",
                    "label": 0
                },
                {
                    "sent": "In fact, the parameters used to generate these are taken from the really in Ferd parameters from the biological experiment later.",
                    "label": 0
                },
                {
                    "sent": "So in some sense these are realistic parameters.",
                    "label": 0
                },
                {
                    "sent": "So what's quite nice is we can then solve for what we think the generating expression level was an it looks a little bit like this, so it captures this structure here and then it does this sort of classic RBF thing of assuming that there's it has a constant conserve length scale across the whole space, so it's sort of sees wiggles often where there aren't wiggles, but it's basically quite a nice estimate.",
                    "label": 0
                },
                {
                    "sent": "No, I learned the hyperparameters for that experiment.",
                    "label": 0
                },
                {
                    "sent": "So although I should admit that I initialized them once and learned them and it worked and I said, oh I'll just check another initialization and it didn't work.",
                    "label": 0
                },
                {
                    "sent": "So he's look at the noise level.",
                    "label": 0
                },
                {
                    "sent": "It's quite high, and some of the structure is lossed in these gene lower jeans here.",
                    "label": 0
                },
                {
                    "sent": "So I think this one, the blue one and the scion one dominate in determining the structure.",
                    "label": 0
                },
                {
                    "sent": "And so for the first initialization I tried, it worked for the next initialization.",
                    "label": 0
                },
                {
                    "sent": "And work and I didn't do sampling to create this experiment.",
                    "label": 0
                },
                {
                    "sent": "It's just sort of a couple of initializations.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the real system is recently published biological data set using a linear response model by Branco at all, and the focus of the study focused on the tumor suppressor protein P. 53.",
                    "label": 1
                },
                {
                    "sent": "There were five targets there named here.",
                    "label": 0
                },
                {
                    "sent": "Now these guys Barranco it'll use quadratic interpolation to try and estimate an observation of the gradient of the gene expression as well as the gene expression, and then they combine these to try and estimate F. They then used an enormous amount of MCMC sampling.",
                    "label": 0
                },
                {
                    "sent": "To get their estimate of F&BS&D 10,000,000.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Patiens so we analyze this data.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ourselves, and this is the result we got.",
                    "label": 0
                },
                {
                    "sent": "So the Red Cross, because Barranco at all.",
                    "label": 0
                },
                {
                    "sent": "They've got an implicit smoothness assumption, but it's difficult to make it explicit so effectively their estimates are point based and they only come at the times of the experiments.",
                    "label": 0
                },
                {
                    "sent": "So at times 0242468 ten, so there's 123456 data points, 5 jeans here, so it's actually quite poorly determined system.",
                    "label": 0
                },
                {
                    "sent": "Nuts, they do a lot there.",
                    "label": 0
                },
                {
                    "sent": "Are there additional material?",
                    "label": 0
                },
                {
                    "sent": "Is some pages long?",
                    "label": 0
                },
                {
                    "sent": "Their mathematical material.",
                    "label": 0
                },
                {
                    "sent": "So these are our estimate of the underlying function plus the sort of 95% credibility intervals.",
                    "label": 0
                },
                {
                    "sent": "Here are the parameters.",
                    "label": 0
                },
                {
                    "sent": "Now this is the one with the most differences.",
                    "label": 0
                },
                {
                    "sent": "I'll come to reasons why there are strong differences between the basil transcription rates, so we're estimating quite different basil.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Scription rates we did HMC sampling over the covariance parameters, so our results are in black and you see error bars on those.",
                    "label": 0
                },
                {
                    "sent": "Their results are in white.",
                    "label": 0
                },
                {
                    "sent": "They did this whole Markov chain Monte Carlo so they had error bars over those as well.",
                    "label": 0
                },
                {
                    "sent": "This is the parameters for which we.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different from them the most the other parameters.",
                    "label": 0
                },
                {
                    "sent": "We were quite similar know for P. 21 This is the sensitivity and it was fixed.",
                    "label": 0
                },
                {
                    "sent": "That's because the system is quite poorly determined, so if you allow all the sensitivities to move, they sort of all move.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Together and the same occurs with the case.",
                    "label": 0
                },
                {
                    "sent": "So there was a fix on the decay, but basically it's an excellent correlation with.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Their results.",
                    "label": 0
                },
                {
                    "sent": "Hum.",
                    "label": 0
                },
                {
                    "sent": "But on these results there's the oscillatory behavior I talked about, which is pretty ugly.",
                    "label": 0
                },
                {
                    "sent": "I mean, Rasmussen and Williams talk about it.",
                    "label": 1
                },
                {
                    "sent": "It's kind of a well known feature, but there are there in good accordance with the Barranco results.",
                    "label": 1
                },
                {
                    "sent": "Now there are differences and those basil transcription rates.",
                    "label": 0
                },
                {
                    "sent": "I told you about it, probably to do with technical details like the different methods used for probe level processing, 'cause they're sensitive to that.",
                    "label": 0
                },
                {
                    "sent": "And we actually didn't implement.",
                    "label": 0
                },
                {
                    "sent": "We should implement this constraint, and we can implement this constraint.",
                    "label": 0
                },
                {
                    "sent": "They constrained the concentration of the transcription factor.",
                    "label": 0
                },
                {
                    "sent": "At Times zero to be zero, we didn't do that.",
                    "label": 0
                },
                {
                    "sent": "The key point though is our results took 13 minutes to produce an Barranco, and his collaborators took 10,000,000 iterations of Monte Carlo.",
                    "label": 1
                },
                {
                    "sent": "So really, with the Gaussian process in place here, it gains an enormous amount 'cause we're not doing any sampling over that were basically integrating that out, and that's why things are so fast that people in the audience should be crying out that this is all wrong, because all of the quantities in that equation were positive, but direct examples from the GP will not be so no one said.",
                    "label": 0
                },
                {
                    "sent": "Boy, you've got negative concentrations in your inference, so protein concentrations can't be negative, but we're going negative here, so we're even dropping below with the mean negative, so we're explaining part of the data by basically saying, oh and then the concentration went negative and the sign flipped, and this became a repressor rather than an activator.",
                    "label": 0
                },
                {
                    "sent": "So that's not good.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what do we do?",
                    "label": 0
                },
                {
                    "sent": "Well, we can just doing for time, by the way.",
                    "label": 0
                },
                {
                    "sent": "Not too well.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll be quite quick with that.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Linear stuff, So what we can do is introduce a nonlinearity around the latent function, so that's what we're doing here.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit small.",
                    "label": 0
                },
                {
                    "sent": "I apologize, so all we're doing here is putting G around the Gaussian process now.",
                    "label": 0
                },
                {
                    "sent": "That means that this is no longer a linear operator on the Gaussian process, which is inside G. If G is nonlinear.",
                    "label": 0
                },
                {
                    "sent": "So that means that in effect this guy isn't a Gaussian process either.",
                    "label": 0
                },
                {
                    "sent": "But what we do then is therefore we just derive the functional gradient of this.",
                    "label": 0
                },
                {
                    "sent": "And learn a map solution for FT Now we can also compute the sort of functional Hessian so we can estimate the marginal likelihood and do error bars and that sort of thing so.",
                    "label": 1
                },
                {
                    "sent": "The key equation, I guess is in order to deal with the integral, we make a discrete approximation.",
                    "label": 0
                },
                {
                    "sent": "Once you've made that discrete approximation, everything is quite trivial.",
                    "label": 0
                },
                {
                    "sent": "There's no oil or LaGrange entering here.",
                    "label": 0
                },
                {
                    "sent": "It's well, Magnus was the one who did the derivation, and he initially started out by doing everything Euler, LaGrange.",
                    "label": 0
                },
                {
                    "sent": "But basically it comes out the same if you just assume the earth of parameters and don't account for the fact that they're generally a function, so skipping quickly through those, that's just the gradient.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the hassium so.",
                    "label": 0
                },
                {
                    "sent": "Is that that's just more massive.",
                    "label": 0
                },
                {
                    "sent": "Thing is, you can't go to nips and not put maths in your workshop talk.",
                    "label": 0
                },
                {
                    "sent": "But you wanna go through the line by line.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, so but the first thing I'm going to do, or the first thing we did with this model is take G to be linear Now what's the point in that?",
                    "label": 1
                },
                {
                    "sent": "'cause I just said nonlinearities.",
                    "label": 0
                },
                {
                    "sent": "Well, if these linear, this whole process of map approximation of Hessian is exact apart from the slight approximation in the integral.",
                    "label": 0
                },
                {
                    "sent": "So this A provides a sanity check.",
                    "label": 0
                },
                {
                    "sent": "Because then we can do it 'cause it's a non stochastic covariance function.",
                    "label": 0
                },
                {
                    "sent": "It should be exactly the same as the results we have before.",
                    "label": 0
                },
                {
                    "sent": "If we use the RBF.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we want to do is use a covariance function like this because hopefully this will get rid of that weird oscillatory behavior.",
                    "label": 0
                },
                {
                    "sent": "This is a nonstationary covariance function.",
                    "label": 0
                },
                {
                    "sent": "Well, remembering going back to the group, this oscillatory behavior is characteristic of the RBF, but what's actually going on in P53 is there's a very high increase, and then there's a drop, and then it's probably flat.",
                    "label": 0
                },
                {
                    "sent": "Well, that's what's believed to be going on, but because it needs to do the high increase, it needs a short length scale and then we think we're not sure about this.",
                    "label": 0
                },
                {
                    "sent": "We think that this may be hallucination of this oscillatory behavior, so by using a nonstationary covariance function like this one.",
                    "label": 0
                },
                {
                    "sent": "But now, like this one, we hope we can get rid of that.",
                    "label": 0
                },
                {
                    "sent": "So to show what samples look like, this is an RBF.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then this is the MLP covariance function, so it sort of has this funky behavior.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here it's completely non stationary so by by forcing the activity to take place in one point, here we're actually get these points symmetric completely non stationary functions.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "Just to so here there's this.",
                    "label": 0
                },
                {
                    "sent": "It's like a neural network.",
                    "label": 0
                },
                {
                    "sent": "It's an infinite hidden layer, neural network, and the hyperparameters are the variance on the weights and the variance on the biases on the hidden layer.",
                    "label": 0
                },
                {
                    "sent": "So here the weights and bias variance is quite high, so you get the basis functions appearing everywhere.",
                    "label": 0
                },
                {
                    "sent": "In this example the bias variance is 0, so all the sigmoids are sitting on the zero point and you could sort of learn where that zero point is.",
                    "label": 0
                },
                {
                    "sent": "If you want to move your activity so it's a nice way of getting nonstationary behavior.",
                    "label": 0
                },
                {
                    "sent": "Out of covariance function.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are the sort of recreation of the results using the map approximation, which is an approximation in this case.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With the RBF kernel and then here, this is with the MLP kernel.",
                    "label": 0
                },
                {
                    "sent": "Again, we've got the negative 'cause we're not using the nonlinearity yet, so it does appear that there is something going on here.",
                    "label": 0
                },
                {
                    "sent": "Still even in the MLP kernel, but in fact if we look at the log likelihood the log likelihood of this model is higher than the log likelihood of this MoD.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nonlinear responses, I'll just quickly show you some results from some different nonlinear responses.",
                    "label": 1
                },
                {
                    "sent": "The first one we tried was just an exponential response model.",
                    "label": 1
                },
                {
                    "sent": "Just constrain the protein concentration positive.",
                    "label": 0
                },
                {
                    "sent": "So you're taking E to the proteins here.",
                    "label": 0
                },
                {
                    "sent": "Effectively model modeling the protein in log space.",
                    "label": 0
                },
                {
                    "sent": "We then also tried this guy.",
                    "label": 0
                },
                {
                    "sent": "So why is this guy interesting?",
                    "label": 0
                },
                {
                    "sent": "'cause he's a positive constraint but he's very linear in the positive half space.",
                    "label": 0
                },
                {
                    "sent": "So in the negative input being negative space like this, he's down here.",
                    "label": 0
                },
                {
                    "sent": "But then he basically becomes a straight line in the positive half space.",
                    "label": 0
                },
                {
                    "sent": "So why is that good?",
                    "label": 0
                },
                {
                    "sent": "Well that means that when we're in this half space, our class approximation will be really accurate.",
                    "label": 0
                },
                {
                    "sent": "When we're in the negative half space, it won't be very accurate.",
                    "label": 0
                },
                {
                    "sent": "But basically on the right hand side it becomes more accurate.",
                    "label": 0
                },
                {
                    "sent": "And then this guy here is one of the directions will go in is to actually implement dynamics which saturate.",
                    "label": 0
                },
                {
                    "sent": "And this is a sigmoid which saturates.",
                    "label": 0
                },
                {
                    "sent": "In general, we haven't done the experiments yet, but we will replace that with Michaelis Menten dynamics, which are commonly used.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the map solutions.",
                    "label": 0
                },
                {
                    "sent": "This is for the exponential, so you can see now they're all positive.",
                    "label": 0
                },
                {
                    "sent": "This is the RBF kernel.",
                    "label": 0
                },
                {
                    "sent": "This is the MLP.",
                    "label": 0
                },
                {
                    "sent": "Note this sort of larabars here, 'cause you'll see if we move.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Move to this is the function here.",
                    "label": 0
                },
                {
                    "sent": "It's sort of like a log, it's the negative log of a logistic actually, but it's also the integral of logistic.",
                    "label": 0
                },
                {
                    "sent": "Then we sort of get error bars which are a little bit tighter, and out of all the approximations we use.",
                    "label": 0
                },
                {
                    "sent": "This had the highest marginal likelihood under the approximation to the marginal likelihood, so it beat that which doesn't have the oscillatory behavior whether that's significant or not, I don't know.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "His the response which saturates.",
                    "label": 0
                },
                {
                    "sent": "So this is like a sigmoid response of F. So basically seeing it's constrained to lie between zero and one.",
                    "label": 0
                },
                {
                    "sent": "The factor of three is just taken from the fact that I'm seeing in general the three is how high it goes, so I just we just put the factor of three in front of the sigmoid so that even though this is between zero and one is being multiplied up by three and let's say one nice feature about this nonlinearity is as well.",
                    "label": 0
                },
                {
                    "sent": "Is as you move away from the data.",
                    "label": 0
                },
                {
                    "sent": "Basically the error bars cover the entire region of where you've seen the data before, which is a characteristic.",
                    "label": 0
                },
                {
                    "sent": "You might be quite keen on.",
                    "label": 0
                },
                {
                    "sent": "In practice, you would actually this parameter here, which we've set as one you would typically learn this parameter and it has implications for what the chemic.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kinetics are.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Oh, I went too fast through the Hessian there, so we've described how GPS can be used in modeling the dynamics of a simple regulatory network motif.",
                    "label": 1
                },
                {
                    "sent": "Now approach has advantages over standard parametric approaches.",
                    "label": 0
                },
                {
                    "sent": "There's no need to restrict the inference to the observed time points.",
                    "label": 1
                },
                {
                    "sent": "We get the temporal continuity of the inferred functions comes out naturally, so by that I mean, again, we're always seeing barrancos results mapped on to here, but we get the entire sequence of the protein coming out.",
                    "label": 1
                },
                {
                    "sent": "The GPS allow us to handle uncertainty in a natural way.",
                    "label": 0
                },
                {
                    "sent": "Now what do I mean by that?",
                    "label": 0
                },
                {
                    "sent": "Well, if I went all the way, well, I probably have time to go all the way back to the differential equation.",
                    "label": 0
                },
                {
                    "sent": "So going all the way back to the differential equation, I can briefly describe what Barranco was doing and why.",
                    "label": 0
                },
                {
                    "sent": "It's perhaps not a good thing its way back.",
                    "label": 0
                },
                {
                    "sent": "A little just go since there's a little bit of time and what's really nice about Gaussian process is in this application is that you don't need to do what they had to do.",
                    "label": 0
                },
                {
                    "sent": "So here what they did is they move the decay onto the left hand side and the be on the left hand side and then divided by S. So then you have an expression for F in terms of things that you're observing, one of which is the gene expression level and one of which is the gradient of the gene expression level.",
                    "label": 0
                },
                {
                    "sent": "But you don't observe directly the gradient.",
                    "label": 0
                },
                {
                    "sent": "In expression level, So what they did is they interpolated between the gene expression levels with the polynomial and then estimated the gradient polynomial.",
                    "label": 0
                },
                {
                    "sent": "But that's sort of inconsistent because you're using one noise modeling how that gradient is varying according to your noise model is not being properly accounted for.",
                    "label": 0
                },
                {
                    "sent": "In effect, the Jeep is doing all that for you.",
                    "label": 0
                },
                {
                    "sent": "It's in furing the function and dealing with the fact that the gradients are important as well, but it knows the gradients implicitly, so it deals properly with the uncertainty in those gradients.",
                    "label": 0
                },
                {
                    "sent": "Let's just go back.",
                    "label": 1
                },
                {
                    "sent": "To the solution.",
                    "label": 0
                },
                {
                    "sent": "So that's a really nice feature of using GPS in this.",
                    "label": 0
                },
                {
                    "sent": "In the in this application.",
                    "label": 0
                },
                {
                    "sent": "So the MCMC parameter estimation in the discretized model can be very computationally expensive.",
                    "label": 0
                },
                {
                    "sent": "The parameter estimation in our framework can be achieved naturally by type to maximum likelihood or by using efficient hybrid Monte Carlo.",
                    "label": 1
                },
                {
                    "sent": "Here we did hybrid Monte Carlo 'cause I think in the end we were basically estimating something of the order of 20 parameters giving given 30 time point observations, which is obviously not that well determined.",
                    "label": 0
                },
                {
                    "sent": "So an all the code for doing this is available online.",
                    "label": 0
                },
                {
                    "sent": "If you want to take a look.",
                    "label": 0
                },
                {
                    "sent": "Thank you so just future directions.",
                    "label": 0
                },
                {
                    "sent": "I forgot I had this slide so the problem here is this is a very simple modeling situation.",
                    "label": 0
                },
                {
                    "sent": "In fact we could get away with this because someone else had published quite a simple model.",
                    "label": 0
                },
                {
                    "sent": "In practice we're ignoring lots of things, transcriptional delays.",
                    "label": 0
                },
                {
                    "sent": "We've got a single transcription factor.",
                    "label": 0
                },
                {
                    "sent": "Our ultimate goal would be to describe regulatory pathways with more genes, but we think that all these issues can be generally.",
                    "label": 0
                },
                {
                    "sent": "Dealt with in the general framework we describe, which is using Gaussian processes to estimate them, but there's lots of greater computational.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Difficulties just other relevant work that we've done is.",
                    "label": 1
                },
                {
                    "sent": "We've looked at genome wide models of these systems, so there's also papers by Sebastian James doing this much simpler models.",
                    "label": 0
                },
                {
                    "sent": "But genome wide and Guido talked about one of these models that the Dynamics Workshop yesterday.",
                    "label": 0
                },
                {
                    "sent": "There's also better Markov chain Monte Carlo models than the one I described briefly from Barranco, and particularly point work by Simon Rogers and Module Army.",
                    "label": 0
                },
                {
                    "sent": "That sort of doing this but without the Gaussian process in there.",
                    "label": 0
                },
                {
                    "sent": "And sorry for any references I've missed, 'cause there's actually this is quite an active area of work, so I'm sure I've missed someone.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Do you want to open question now or take questions now?",
                    "label": 0
                },
                {
                    "sent": "First questions for them.",
                    "label": 0
                },
                {
                    "sent": "OK and that's it.",
                    "label": 0
                },
                {
                    "sent": "OK. Home, Eric.",
                    "label": 0
                },
                {
                    "sent": "Very impressive, could you also?",
                    "label": 0
                },
                {
                    "sent": "Maybe use that model for clustering.",
                    "label": 0
                },
                {
                    "sent": "We have several of these and you don't know what they are.",
                    "label": 0
                },
                {
                    "sent": "Clustering in the FFT's.",
                    "label": 0
                },
                {
                    "sent": "I have several transcription factors and only being driven by home.",
                    "label": 0
                },
                {
                    "sent": "The high sensitivity.",
                    "label": 0
                },
                {
                    "sent": "Right at least might be able cluster.",
                    "label": 0
                },
                {
                    "sent": "That's an interesting idea we haven't thought about that much 'cause what we were thinking, what we wanted to do next is trying to integrate some more transcription factors into the system 'cause we think you can trivially add an additional transcription factor, but I think that's a good idea.",
                    "label": 0
                },
                {
                    "sent": "What bar encoded was basically rank targets predicted from that FT from P53 to try and give prediction of targets.",
                    "label": 0
                },
                {
                    "sent": "But that's a good idea.",
                    "label": 0
                },
                {
                    "sent": "Yeah, think about something like that.",
                    "label": 0
                },
                {
                    "sent": "Wow.",
                    "label": 0
                },
                {
                    "sent": "Model.",
                    "label": 0
                },
                {
                    "sent": "Something different then.",
                    "label": 0
                },
                {
                    "sent": "Ofer right, so that's an interesting question.",
                    "label": 0
                },
                {
                    "sent": "We haven't actually looked.",
                    "label": 0
                },
                {
                    "sent": "You mean using a more sensible form of quadrature?",
                    "label": 0
                },
                {
                    "sent": "Something or increasing number of points in proximity of.",
                    "label": 0
                },
                {
                    "sent": "I don't think.",
                    "label": 0
                },
                {
                    "sent": "I don't think we need to do important sampling 'cause it's a 1D integral, but I think there's one thing we haven't looked at all is OK. Is there a much more sensible method of quadrature which we could get away with fewer points and and that would become important if we look at larger systems?",
                    "label": 0
                },
                {
                    "sent": "Because basically you're you become cubic in the number of quadrature points, so that could become important.",
                    "label": 0
                },
                {
                    "sent": "And it's definitely something we'll be looking at for that case.",
                    "label": 0
                },
                {
                    "sent": "Oh when I listen to this question is that?",
                    "label": 0
                },
                {
                    "sent": "In Haggen's original paper about vision, portraiture offers an optimal design.",
                    "label": 0
                },
                {
                    "sent": "For that.",
                    "label": 0
                },
                {
                    "sent": "An optimal design for.",
                    "label": 0
                },
                {
                    "sent": "Alright, yeah, so you you mean what you're suggesting is you could rather funkily do Bayesian quadrature with a Gaussian process to get out the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well, I know that Tony doesn't really think that Bayesian quadrature is the right way for 1D integrals, 'cause it's computationally quite expensive, but that's an interesting point.",
                    "label": 0
                },
                {
                    "sent": "I think that.",
                    "label": 0
                },
                {
                    "sent": "It's a good.",
                    "label": 0
                },
                {
                    "sent": "It's a good thought because I think there's other reasons you might want to start doing that.",
                    "label": 0
                },
                {
                    "sent": "Including stochastic if you want to do that, integrals to get over a stochastic covariance function, then whether you can leverage the uncertainty estimate you get from that.",
                    "label": 0
                },
                {
                    "sent": "That's something I had not thought about, but I know Tony thinks that quadrature in Bayesian quadrature is probably useful for like 12 to 20 input dimensions.",
                    "label": 0
                },
                {
                    "sent": "Not useful for a high dimensions and not useful for a low because, but I think there are other reasons why you might want to use that, that's interesting.",
                    "label": 0
                },
                {
                    "sent": "And so should I do my open question.",
                    "label": 0
                },
                {
                    "sent": "I just think the speaker.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Century Cos pizza right down and open question at the end of my slowed slide slowed slides.",
                    "label": 0
                },
                {
                    "sent": "I don't know if it's a good one because I missed the morning session so I missed the discussion going on there.",
                    "label": 0
                },
                {
                    "sent": "But one thing I think that we're really keen for is well.",
                    "label": 0
                },
                {
                    "sent": "I don't like the S word.",
                    "label": 0
                },
                {
                    "sent": "I think it's like it's like should be a last resort.",
                    "label": 0
                },
                {
                    "sent": "I think we end up.",
                    "label": 0
                },
                {
                    "sent": "We haven't.",
                    "label": 0
                },
                {
                    "sent": "We end up sampling with hybrid Monte Carlo at some point, but we do the integrals first if we can.",
                    "label": 0
                },
                {
                    "sent": "I think Manfreds have touched on probably this morning when I wasn't here.",
                    "label": 0
                },
                {
                    "sent": "His variational approximations with Cedric and John and we saw work earlier on variational approximations, but I've sort of gotten.",
                    "label": 0
                },
                {
                    "sent": "We've done the Laplace approximation, which is, I think it's quite a bad idea in some sense.",
                    "label": 0
                },
                {
                    "sent": "But how can you produce better posterior approximations without sampling is, to me, seems like a big open question, but maybe it's not a good one.",
                    "label": 1
                },
                {
                    "sent": "I mean, yeah, that's it.",
                    "label": 0
                }
            ]
        }
    }
}