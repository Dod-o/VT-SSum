{
    "id": "45seusamt73ikj6nh4b3frypjctx3hjt",
    "title": "Applications of Learning Theory in Algorithmic Game Theory",
    "info": {
        "author": [
            "Tim Roughgarden, Computer Science Department, Stanford University"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_roughgarden_game_theory/",
    "segmentation": [
        [
            "And thanks for having me my first Colt.",
            "I think it's quite a bit over quite a few years overdue, but happy to be here.",
            "So my goal for today is to illustrate some points of contact between learning theory and algorithms game theory.",
            "So what's algorithm game theory?",
            "So that's a field which is on the boundary of theoretic computer science.",
            "On the one hand, and game theory, economics on the other.",
            "That feels about 15 years old.",
            "It's really exploded during that time.",
            "There's now at least three annual conferences devoted solely to results in algorithm game theory.",
            "And I think one of the reasons it's really taken off like a rocket ship in addition to the relevant applications, is that there's kind of an unreasonable number of connections to other parts of theoretical computer science.",
            "And so naturally, today I'll focus on a couple of."
        ],
        [
            "Connections to this community to learning theory."
        ],
        [
            "This isn't an exhaustive list, but I'm just going to focus on two case studies, so they're very different.",
            "They're completely independent from each other, so in the first half or a little bit more of the talk, I'll discuss so-called price of anarchy analysis.",
            "So here the goal is to prove approximation guarantees for game theoretic equilibrium.",
            "Things like Nash equilibrium.",
            "So the price of anarchy has been part of algorithm game theory since it began about 15 years ago, and for a long time people were content to prove bounds about Nash equilibrium.",
            "But then there is an issue that Nash equilibrium can be elusive.",
            "That can be intractable, and there's other issues about that can be hard for players to actually reach, which motivated the need for more robust bounds bounds that apply beyond Nash equilibrium, and so then there's a question you know if you're analyzing a game and you're not going to Lisa Nash equilibria, what are you going to analyze?",
            "And here online learning sort of comes to the rescue an offers a number of, I think, very attractive alternatives and relaxations of Nash equilibrium analysis, and in this talk I'll focus on guarantees that apply to.",
            "Players that at least guarantee themselves no regret, so don't necessarily converge to a Nash equilibrium, but the very least every player has vanishing regret over repeated play of the game, so that's the overview of the first case study.",
            "Then we'll switch gears.",
            "I'll talk about auction design and the design of revenue maximizing auctions, so here the tradition goes back quite a bit further, so you're not so you won't be surprised to hear that economists in the 20th century so thought about this a lot here.",
            "The issue is the traditional reliance on a common prior.",
            "A distribution that is assumed known over the uncertainty in the situation, like what bidders are willing to pay for an item, and here again pack style.",
            "Statistical learning theory.",
            "Learning theory offers a very attractive framework for replacing this op priore known common prior with a more data driven approach, i.e.",
            "Learning a near optimal auction from samples from bids in previous transactions.",
            "So that's going to be the overview of the talk.",
            "Those two case studies please feel free to interrupt with questions at.",
            "Anytime happen to happen, OK."
        ],
        [
            "Alright, so let's jump into the first case, study about price of anarchy or guarantees for equilibrium.",
            "So I'm not going to assume you ever heard of this before and I want to introduce it by."
        ],
        [
            "Way of an example by now the price of anarchy has been studied in many many different application domains, but I still like to illustrate in an example very near and dear to my heart, namely in row."
        ],
        [
            "In games, so recall that in a game you've got players, players have actions and as a function of the actions that players take, every player gets a payoff.",
            "Or in this case I'm going to call it a cost.",
            "In this case, think of it is like the travel time, so let's keep it simple.",
            "Let's say there's just two players.",
            "Let's say each of the players has three actions corresponding to the three St paths.",
            "So the two 2 hot paths, and then the one zigzag path.",
            "So to finish the game description, I have to tell you the costs, so encode those by annotating each of the edges with a cost function.",
            "Describing the travel time as a function of the number X of players that are using that edge.",
            "So there's only two players, so extra is going to be either one or two for some of the edges.",
            "The travel time is always the same.",
            "It doesn't matter how many players use it.",
            "For other edges, it depends if there's more players than it takes longer to traverse the edge.",
            "So the idea of the price of anarchy is to compare two different things.",
            "On the one hand, the outcome of selfish behavior where everybody does whatever they want.",
            "So in Nash equilibrium, on the other hand, what hypothetical Deity could do with full control over the system?",
            "So let's look at those in turn.",
            "So what would selfish players do?",
            "What you have set up this game so it's really easy to understand, right?",
            "So if you're one of these players, you can say, well, look at."
        ],
        [
            "This part of the network, the biggest X can ever be as two so 2X is always going to be smaller than five for the same reason, 5X is always going to be smaller than 12, so independent of what the other player does.",
            "You want to make use of the two X link and the five X link.",
            "So in other words it's called the dominant strategy for each player to route themselves on the zigzag path.",
            "OK, so this is the unique Nash equilibrium and Nash equilibrium recall is just an outcome where nobody can do better by deviating unilaterally.",
            "So let's evaluate this Nash equilibrium.",
            "So the travel time on this link is 4.",
            "On this link it's 10, so each player experiences a cost of 14.",
            "So the joint costs for the two players would be 28.",
            "Now the other side of the equation is we ask.",
            "OK, suppose players didn't get to do whatever they wanted.",
            "Suppose we could pick routes for them.",
            "Could we do better in this network?",
            "We could if we separate."
        ],
        [
            "Them on their own 2 hot paths.",
            "Well, the green player would still have cost fourteen, 2 + 12, but the red player would now have cost only 5 + 5 or 10K, so the joint costs would have dropped to 24.",
            "So by definition the price of anarchy is simply the ratio of these two quantities.",
            "The Nash equilibrium objective function value over the optimal one in this particular network it would be 7 /, 6 in a different network.",
            "It would presumably be different, so one important point is that you know, while in this network there's a unique Nash equilibrium.",
            "In general there can be many.",
            "An we prefer most conservative worst case bounds, so there's other alternatives, but will only look at bounds that apply to every single equilibrium.",
            "OK, the worst case equilibrium.",
            "So like I said at this point, the price of anarchy has been exhaustively studied.",
            "Tight bounds or known for many many different application domains.",
            "For example, for these kinds of routing networks without find cost functions, the worst case, the highest this can ever be is 2.5 is one is 1 example of what's known, but again, there's dozens and dozens of results of this form."
        ],
        [
            "So it's kind of turned into kind of a cottage industry.",
            "It's no longer uncommon for people to send me sort of new papers.",
            "They're writing with titles like this one preventative healthcare.",
            "So this was, you know, the UK was proposing a decentralization of aspects of their healthcare system.",
            "So some researchers were trying to understand what would be the effect would be the cost or the blow up an objective function value from moving this part of the healthcare system from centralized to decentralized."
        ],
        [
            "Or even this one.",
            "Is from the.",
            "I'm not kidding.",
            "The MIT Sports Conference on MIT Conference on sports analytics.",
            "This acute paper actually so the author posits a metaphor between the routing networks.",
            "I just showed you and the decisions of basketball makes about how to pass and how to shoot.",
            "So the authors, thinking of the players being nodes in a network and then the terminal is like the hoop and then an edge corresponds to a pass between players or shot on the basket.",
            "So the analog of the links getting more costly in a Rd network as more people use it.",
            "That just says like for example, if the same player shoots too much, the effectiveness of that player is going to go down, presumably because it's more heavily guarded by the other team, so you know.",
            "Yeah.",
            "For those of you who know braces Paradox, which is this cute example cute cocktail party example that you can actually remove edges from a network and you can make the equilibrium better.",
            "The author suggests that maybe that's why, for a period of time in the mid 90s, the Knicks actually scored more points when their star center, Patrick Ewing was on the bench when he was actually playing.",
            "So you know it's it's fun paper.",
            "So any questions before I sort of segue into why these price of anarchy bounds about Nash equilibrium really aren't good enough.",
            "Why we needed new notions, which is where learning theory sort of came to the rescue, and these questions in the preamble.",
            "OK."
        ],
        [
            "So why weren't we happy just having dozens of tight bounds in the price of anarchy for a wide range of application domains?",
            "Well, let's sort of drill down and think about what does it mean when you prove that, say, the price of anarchy is at most two, what that literally means is that in your game, if players reach an equal Nash equilibrium, you don't care which Nash equilibrium, but if they reach some Nash equilibrium, then it's a guarantee that the performance of your system is near optimal within a factor two of optimal.",
            "So the concern is the strength of this hypothesis.",
            "If the players reach.",
            "Some Nash equilibrium of the game.",
            "Sure, you know it's a 0 sum game or something simple like that.",
            "Maybe you have reason to believe that players will be good at playing a Nash equilibrium.",
            "But more broadly there are issues actually.",
            "I mean, I've been sort of glossing over what kind of Nash equilibrium I'm talking about.",
            "So pure Nash equilibrium.",
            "That's where nobody randomizes.",
            "Every player just picks one outcome, and nobody can be better off by deviating in those routing games.",
            "You always have pure equilibrium, but in lots of games you don't like matching pennies, say.",
            "So if you talk about equilibrium bound just for pure Nash equilibrium.",
            "That can even be vacuous, so that motivates moving to mixed Nash equilibrium where players can randomize.",
            "These always exist, that's what Nash proved, but some of the greatest hits out of algorithm game theory actually explain why mixed Nash Equilibria are in some sense and intractable solution concept.",
            "So the formal statement is there PPD complete to compute.",
            "That's just sort of the analogue of NP completeness suitable for equilibrium computation, so we don't actually believe and say just even say two player General Bimatrix games.",
            "We don't believe there's a polynomial time algorithm to compute a mixed Nash equilibrium in general.",
            "Let alone that distributed players can actually coordinate and compute one at any reasonable amount of time.",
            "So that's a concern, but we have this rich approximation theory developing about how good Nash equilibrium can be, but at the same time results from complexities saying that it might not be a meaningful equilibrium concept in all situations because of its intractability.",
            "So that motive."
        ],
        [
            "It's striving for price of anarchy bounds that apply beyond Nash equilibrium.",
            "OK, maybe the Nash equilibrium particular, but also to other stuff which is much easier to compute and more plausable predictions of gameplay.",
            "So this important agenda was first articulated by Meraki Invita mid last decade, so initially people adopted some concepts familiar from game theory but not the only one.",
            "I'm going to talk about today and what I still think is probably the most elegant solution was proposed by Auburn Blum who I saw out on the coffee break.",
            "And coauthors Evan Darla get Hadji Guy and Aaron Roth.",
            "And so they proposed.",
            "Instead of thinking about players playing a Nash equilibrium, think about repeated play overtime.",
            "And rather than assuming that players converge to an equilibrium, assume merely that each one is achieved.",
            "No regret.",
            "I'll define that formally later, but it's what most of you would think.",
            "It just says.",
            "There's been repeated play of a game for each player.",
            "Its average payoff is at least as good or close to what would have been the best fixed action in hindsight's.",
            "OK, so that's a much, much weaker.",
            "Notion assumption than assuming that players play in Nash equilibrium.",
            "OK, so this is what I'm going to focus on next.",
            "And it turns out very general bounds.",
            "So the good news is very general.",
            "Positive bounds are possible assuming only no regret learning."
        ],
        [
            "So specifically, here's a here's sort of the main.",
            "The main point of the first half of the talk, and I'll state it informally now and will gradually make this more precise over the subsequent slides.",
            "So we had zillions of price of anarchy bounds across lots of different application domains, and the good news is that most of them not 100%, but you know something like 2/3 or 3/4 of these price of anarchy bounds.",
            "Actually, in many cases, sort of unbeknownst to the original provers of these bounds, apply more broadly to no regret learners.",
            "They do not just apply to Nash equilibria, they actually apply to every possible outcome sequence generated by players achieving no regret.",
            "So that's kind of the high level takeaway.",
            "Now on this slide, let me drill down a little bit what I mean by this.",
            "This is not a formal statement in particular, probably wondering you know what does this mean?",
            "What I mean by most?"
        ],
        [
            "Here's how it's going to work.",
            "Alright, so just as far as the interpretation.",
            "So again, let me emphasize.",
            "A special case of no regret learning is picking a Nash equilibrium and playing it over and over and over again.",
            "OK, so you're only proving a stronger bound, only proving a bound to more stuff by proving bounds and #8 number at learners, the Nash equilibrium.",
            "But as many of you know, many many things that achieve no regret for all players are not Nash equilibrium, so in particular computationally much more tractable to achieve no regret than it is to converge to a Nash equilibrium.",
            "So in that sense, this is a much more robust bound than merely for Nash equilibrium.",
            "So how is this going to go?",
            "So how is how is this sort of meta result going to be proved?",
            "So it's going to be approved by what I call an extension theorem, and you can think of this as sort of a black box lifting of a price of anarchy.",
            "Bound originally imagined merely for Nash equilibrium.",
            "But then the extension theorem will be sort of a compiler, and it compiles this bound just for pure Nash equilibria into a robust bound that holds for all outcomes generated by no regret learners.",
            "So that's the idea of an extension theorem.",
            "Part one you prove about for Nash Equilibria Part 2 you apply the extension theorem and you get a much more general and robust conclusion.",
            "Again, I'll make this precise.",
            "So that's the idea.",
            "Now, there's no hope to have, so then the question is OK. What fraction of the price of anarchy bounds in the literature are?",
            "Actually, for which does this extension theorem apply.",
            "So as well see there's not going to be an extension theorem that applies to every conceivable price of anarchy bound for Nash equilibrium.",
            "It applied to some bounds and not to others.",
            "And the good news is that if you look at the price of anarchy bounds that are known in the literature, again, the overwhelming majority say 2/3 or 3/4 of them are in fact proofs of the sort to which this extension theorem applies.",
            "OK, so this is the high level idea, so I need to define what types of proofs can you apply the extension theorem to you.",
            "I need to explain what the extension theorem States and that's where we're going next."
        ],
        [
            "Here's a pictorial representation of what I just said.",
            "I like to think of these extension theorems is sort of a tool for the lazy analyst and present company, not necessarily excluded.",
            "So you're an analyst.",
            "You want theorem, you want the conclusion of your theorem to be as strong as possible.",
            "And so again, we're thinking about a worst case bound over a bunch of outcomes.",
            "So the strongest statement is to have this worst case bound applied to his wider range of outcomes as possible.",
            "Again, not just a Nash equilibrium, but lots of other stuff also.",
            "So at the end of the day, we want a theorem.",
            "Which says that every say outcome generated by no regret learners is close to optimal.",
            "That's what we want because we want to conclude.",
            "But now if we also have to actually prove the thing."
        ],
        [
            "Be a lot nicer.",
            "It be a lot simpler if we only had to worry about some very simple concepts like pure Nash equilibrium.",
            "OK, I don't want to think about these arbitrary number grid algorithm algorithms.",
            "Who can think about this, whereas you just give me a pure equilibrium like in that routing network.",
            "You know, maybe I can bare hands, start thinking about OK. Is this close to optimal far from optimal or what?",
            "OK, so as far as what to prove this is the much simpler thing to prove bounds about, even though that's what we want the conclusion for.",
            "So what the extension theorem really is?"
        ],
        [
            "Enables you to do as an analyst.",
            "It allows you to just focus all your efforts on the left hand side so you look at your game, you understand what's special about your game like your routing, games or whatever.",
            "You prove abound merely if you're pure equilibrium.",
            "And then this extension theorem is just going to apply as a black box and carry that same approximation bound over to the much more general set of outcomes generated by no regret learners.",
            "OK, so that's the that's the point of the extension theorem, you just glue it together with a pure Nash equilibrium bound and you get the kind of theorem you want.",
            "OK, so I know the last couple slides are probably a little vague.",
            "A little high level so I do want to."
        ],
        [
            "Start filling in the details and I did I did want to at least spend maybe 10 minutes 5 to 10 minutes of the talk actually showing you the actual mathematical definition and actual proof.",
            "I know it's day three, but if you have if you can muster energy to understand some math for 10 minutes, this is those 10 minutes.",
            "If you can't I understand, don't worry, after this will do a total reset and move on to the second case study.",
            "Alright, so.",
            "The goal now is to explain this extension theorem and specifically under what conditions does this extension theorem apply?",
            "OK, So what I'm going to do is I'm going to give you a feel for what it means to prove approximation guarantee for equilibrium because we're going to do that.",
            "OK, what would that prove look like and then impose conditions on what the proof looks like so that you get this guarantee not just for Nash equilibrium.",
            "Much more generally, this actually easiest to explain if we sort of zoom out to a quite abstract level.",
            "Alright, so again think about just an arbitrary game, so we've got players.",
            "They've got strategies as survive.",
            "This is the strategy picked by player I, so that's like a path by player I through the network.",
            "This bold S that's a strategy profile, so one strategy, or one action per player so bold S is like a traffic pattern.",
            "And remember, the data were given is for each possible outcome.",
            "What is the cost that each player incurs in that outcome?",
            "OK, so that's a game, an assumption which is important for this result.",
            "Is that what we ideally would want to accomplish is minimizing the joint cost, the sum of the players costs.",
            "That's exactly what we're doing in that routing game example on slide one."
        ],
        [
            "OK, so imagine you had like a problem.",
            "Set a homework OK and it said prove that the price of anarchy of some game is bounded by two.",
            "Alright, so how might you start that proof?",
            "OK, well remember what it means.",
            "So to prove that the price of anarchy is at most two, you have to say every single equilibrium.",
            "Again, there might be many, but every equilibrium has cost at most twice optimal.",
            "So clearly the relevant characters in our proof are a Nash equilibrium.",
            "Pick an arbitrary one, call it S, and then we need to compare its cost to the optimal solution.",
            "S Star star minimizes the cost.",
            "That's what this hypothetical deity would implement.",
            "Alright, so we're going to start with the equilibrium costs.",
            "And now we basically want to chain of inequality is less than or equal to inequality's, terminating with constant times cost of star, right?",
            "That's what that's going to look like, so without doing any thinking at all without even looking at what our game is, let's think about how far we can get.",
            "So, how far can we get without even thinking about what we're doing?",
            "Well, you know at the moment we've got very little going for us, right?",
            "So we've got all we've got really is that?",
            "We know S is in equilibrium.",
            "That's kind of all we've got on our side.",
            "I guess we also know that the objective function is the sum of the player costs.",
            "So let's start with that cost of the equilibrium expand.",
            "That is the sum over the cost of each player I.",
            "So we got to use the equilibrium hypothesis.",
            "How would we do that?",
            "So what does it say?",
            "It says if any player deviates unilaterally, it only gets worse.",
            "Its cost only goes up OK Alright.",
            "So to apply that condition we need to sort of hallucinate a deviation for this player.",
            "An in some sense, like the only other object on the slide, is this optimal solution at star, and actually that kind of makes sense, but we have this equilibrium S we're trying to prove it's not too expensive, and so we basically want to say we really wish the players were all playing at star.",
            "So let's at least kind of like interrogate player.",
            "I hate player, I why aren't you playing your optimal strategy?",
            "Optimal solution estar?",
            "Why aren't you playing Sri?",
            "The player says 'cause I'd be worse off, right?",
            "This is Nash equilibrium.",
            "Now deviation?",
            "My cost would go up.",
            "So that's all this inequality here is saying.",
            "It says if you think about the I TH term and you think about player, I imagining deviating to what it should be doing in the optimal solution.",
            "Its costs would only go up.",
            "So the notation here is that player I is playing, its strategy and estar, everybody else is playing the same thing.",
            "They're playing in S. That's what that notation means.",
            "So that's after unilateral deviation to Sri starting from bold S. OK, so this is the first inequality, and we got here.",
            "Without knowing anything about our game whatsoever.",
            "OK. And this is all we can get, which makes sense.",
            "The price of energy in some games is going to be high in other games it's going to be low, right?",
            "So at some point in this proof we need to actually understand something about the structure of our game, so that has to happen next.",
            "So we're."
        ],
        [
            "I had to come up to a key definition that of a game being smooth and with the key definition is doing is it's sort of saying well what could the rest of this proof look like OK?",
            "And fundamentally, what is the rest of the proof have to do?",
            "OK, so we got this far and notice the right hand side here.",
            "This quantity we could care less about this number.",
            "This is some like weird entangled version of SNS star that has no meaning to us.",
            "The only thing that means anything to us, given that we're trying to prevent rebound, is the equilibrium cost and the optimal costs.",
            "The costs of SNS star.",
            "Those are the only objects of interest, so the rest of the proof fundamentally is a relationship between this number that we don't care about, and the two numbers that we do care about the cost of S and the cost of star.",
            "So the key definition is out of a game.",
            "Being smooth with parameters Lambda and mu.",
            "So a game is smooth if you can relate that entangled quantity that we don't care about in a linear way to the two quantities we do care about if we can upper bound the cost of that entangled quantity by Lambda times the optimal cost plus mu times the equilibrium cost.",
            "It's also important that you is less than one OK.",
            "So you may or may not be able to establish this inequality for a given game, and for certain parameters of Lambda new.",
            "This will depend on the game.",
            "For nice games, you'll be able to establish this condition with reasonable Lambda mu.",
            "What do I mean?",
            "Maybe like lambdas.",
            "Two and Mu is a half a nice small numbers.",
            "Other games you won't be able to do it.",
            "So what I want to do next is OK if you prove a game is smooth, at the very least you can conclude that the price of anarchy of Nash equilibrium is small Nash equilibrium guaranteed to be near optimal, but we want broader statements and will get them later.",
            "But for starters, I claim this is a sufficient condition.",
            "For a game to have a small price of anarchy, where small depends on Lambda and you.",
            "Why is that true?"
        ],
        [
            "So here's the first inequality.",
            "This is copied from the last slide we got this inequality display applying the Nash equilibrium apophysis because the game is smooth.",
            "I can upper bound this.",
            "I can charge this against linear combination of the optimal an equilibrium costs mu remember is less than one, so I can subtract that last term from both sides and divide by 1 minus mu.",
            "So I get that the cost of this arbitrary equilibrium is at most Lambda over 1 minus mu times that of the optimal solution estar.",
            "So for example, if lambdas two and mu is 1/2, you're going to get a four approximation.",
            "For all Nash equilibrium.",
            "So again, the point of this slide is that, at the very least, a game being smooth is a sufficient condition for having a small price of anarchy.",
            "Actually, it's sort of overkill if you think about it, because if you look at the statement, I'm asserting that no man I'm asserting this condition this inequality for every single pair of outcomes SNS star in the derivation.",
            "Actually, we only care about equilibrium S and optimal solutions S star, so the payoff of this stronger definition will come later.",
            "But I just want to plant that seed that actually this is overkill if all we cared about were Nash equilibrium, because we only need this to hold for equilibrium S, But I'm asserting it for all outcomes S, so it will come up in no regrets sequences.",
            "OK.",
            "So.",
            "That's a definition.",
            "So what?",
            "Why should you care?",
            "OK, so to make you care, I need to give you 2 things I need to give you examples.",
            "As in, you can actually establish this definition and applications that people care about.",
            "Two, I need to give you implications beyond just the pure Nash equilibrium bounds and the implications will be to no regret sequences.",
            "So I'm going to do those two things quickly next.",
            "Any questions before then.",
            "Right, I see so the question right?",
            "So even for no regrets.",
            "OK, good question.",
            "Yeah, there's a couple of questions in here.",
            "So the question is basically about to what extent are these techniques overkill and you can ask different versions of this question depending on what bounds you want.",
            "So one question would be you know so.",
            "So jumping the gun a little bit, we're going to get guarantees for no regret sequences.",
            "So they hear a question was, well, maybe this is overkill for guarantees for no regret sequences.",
            "Actually, here you can prove a theorem that this is an overkill.",
            "So basically I mean I won't have time to talk about this, but and there's some fine print here, but more or less bounds for no regrets sequences.",
            "Here the caveat is it's really for if you think about a generalization of no regrets sequences were on average over players.",
            "There's no regret.",
            "So some players have positive regrets in a negative, but it averages out to no regret.",
            "You still going to get bounds for those sequences, and that's you can actually prove using convex duality.",
            "That is the limit of the of the smoothness condition, so there's actually a dual correspondence between this and mild generalization error sequence is the other question you can ask is like, well, what if I didn't care about, never got sequences, they only care about Nash equilibrium, then surely this is overkill because it's giving me something I don't want.",
            "It's giving me this extra generality I don't want, and presumably getting worse bounds, and here amazingly so sometimes that's true.",
            "So sometimes if you only care about your equilibrium, this is overkill.",
            "But amazingly and again I can't.",
            "I don't have time to talk about this.",
            "I'm happy to talk offline.",
            "You can actually prove, even if you only care about Nash equilibrium.",
            "There are classes of games, including routing, games or even for Nash equilibrium.",
            "This sufficient condition gives you the right answer and you can.",
            "You can prove that generically you can always sort of set Lambda and mu to give you the right answer, even for just Nash equilibrium.",
            "So there are cases where this is a completely tight analysis framework.",
            "OK. Any other questions before I talk about examples of the definition being satisfied and why it's worthwhile to satisfy the definition?",
            "OK, so for example."
        ],
        [
            "I'm really just gonna ask you to trust me, so it's just the fact that a huge chunk of the literature, the way they prove bounds on the price of anarchy are smoothness proofs.",
            "There are many examples known before this definition was formalized without the examples.",
            "I don't see how one would have formalized the definition after the definition.",
            "People have come up with many more examples.",
            "In particular, is really caught fire."
        ],
        [
            "Amongst people who think about auctions over the past three years or so, so there's lots of exam."
        ],
        [
            "It turns out this is really the dominant paradigm of proving bounds on equilibrium."
        ],
        [
            "All right, why should you care?",
            "So here's formally what I mean by the extension theorem.",
            "So suppose you have a game which is Lambda mu smooth.",
            "Consider any no regret sequence.",
            "I'll define that for me in the next slide.",
            "If it's just what no regret learners generate.",
            "Consider any no regret sequence, and in fact the average joint cost across the number get sequence is bounded by Lambda over 1 minus mu times the optimal outcome.",
            "And the point here is that this approximation bound is exactly the same bound we were already getting for pure equilibrium.",
            "So we've lifted the guarantee with no quantitative loss from Nash equilibrium to no regret learners.",
            "So there's also analogs of this extension theorem for many other relaxations of Nash equilibrium as well, but I'm just going to focus on these no regret sequences.",
            "So it's formally what I mean by the extension theorem for smooth games, the land over 1 minus mu lifts or extends from Nash equilibrium to no regret sequences."
        ],
        [
            "So what's another great sequence?",
            "So we're thinking about a fixed game, a fixed set of players, like a fixed routing network.",
            "We're thinking about the same players playing the game day after day after day, say capital T days.",
            "So it's like you have a routing network and you're looking at a sequence of T traffic patterns.",
            "Rush rush, hour traffic every morning.",
            "What does it mean?",
            "That what does it mean for a sequence to be no regret?",
            "Well, so you look back in hindsight and use a phrase in terms of costs.",
            "So for a fixed player, I and affixed deviation Qi, the average cost that the player in curd over the sequence should be no worse than the minimum average cost it could have attained in hindsight.",
            "With this fixed action, if it had just played Qi every single day.",
            "So like in the routing framework, you know, maybe you're exploring different ways to get from home to work over some 100 day.",
            "You should have at least guaranteed yourself average travel time as small as any fixed route that you could have chosen day after, day after day.",
            "Now, of course, when you're talking about no regret, you know.",
            "Usually you have an error term, and I know a lot of people care a lot about the exact form of that error term.",
            "Apologies, but I'm just going to totally suppress this.",
            "We all know it's there, it just passes through the approximation results very effortlessly with no problems.",
            "So I'm just for simplicity going to assume this is zero.",
            "I'm not going to worry about the convergence rate, but the understanding is that T is reasonably big and this is going to zero with T. So that's a no regret sequence, and now the proof of the extension theorem really just writes itself OK, so let me show you that if you have a smooth game and you."
        ],
        [
            "No regret sequence then.",
            "The average cost across the new regret sequences land over 1 minus mu times that of an optimal outcome.",
            "So here's the sequence supposed to satisfy the network condition.",
            "The game is fixed, so there's a fixed optimal solution called as Star.",
            "Assume the game is Lambda mu smooth.",
            "So let's just scale out by T for the moment.",
            "We're going to start the exact same way we started when we were thinking abstractly about what an equilibrium guarantee looks like.",
            "So we look at the joint cost, overall days T, and we expanded over the players.",
            "OK, so some over the days T players I cost incurred by on day T. Now the next step is where it seems like we get in trouble, why?",
            "Previously, this is where we use that we were dealing with a Nash equilibrium.",
            "We said if we walk up the playwright and we force it to switch to Sri, it only gets worse.",
            "That's no longer true.",
            "Remember, we're interested in situations where these are not equilibrium.",
            "OK, that's a very boring kind of no regrets sequence.",
            "We're thinking of that all of these traffic patterns are perhaps very far from equilibrium, and it only satisfies this sort of on average overtime equilibrium condition, as in the no regret guarantee.",
            "OK, so now when we walked up to player I on day T and we force it to switch to this optimal strategy, Sri for all we know it's cost drops.",
            "OK, that can't happen if it was an equilibrium, but with another great sequence on some arbitrary day we forced it to switch for all we know, we just improved it.",
            "Its cost dropped.",
            "So let Delta denote the magnitude of that improvement, OK?",
            "So how much it's cost decreased by forcing player I to switch to estar ion Daty.",
            "Alright, so this is just by by notation that equality holds.",
            "So now we're going to do is we're going to separate these terms.",
            "We're going to use smoothness to control the entangled terms, and no regret to control the Deltas.",
            "Remember I made it.",
            "I made a point to that when I define smoothness, it seemed like overkill.",
            "We only needed this disentanglement to hold for equilibrium S, and yet I asserted it held for every single outcome.",
            "Yes, not just equilibrium.",
            "Well, now that's looking for two.",
            "It is 'cause, again, arbitrary ESAB teas are not equilibrium their arbitrary outcomes, so it's really good that we can disentangle arbitrary outcomes, so that's what's happening here.",
            "Smoothness says we can take this entangled term and upper bounded by the suitable linear combination.",
            "Of the optimal cost and the cost in the sequence on day T, here I've just grouped the Delta terms.",
            "Now what I want you to do for the error terms, I want to sum first over players and then the inner sum over days T. So fixed player I if we fix player I what is the meaning of this inner sum?",
            "The meaning of this inner sum says, suppose you went up to player I and forced it to switch to Sri every single day.",
            "Delta IT says you force I to switch to optimal strategy on day T. This is just that summed over all days T. So the inner sum is how much better the player would get if you forced it to switch to exactly the same action as star I every single day, and we know the answer to that.",
            "The player would not be better off if it switched to this fixed action every single day, so each inner sum the deltas can be positive, but each inner sum is non positive.",
            "OK, so is this something?",
            "So that's what gives us the bounds.",
            "So now again we can just rearrange and divide through and the average cost of the number sequence is at most one over Lambda Lambda over 1 minus mu.",
            "OK."
        ],
        [
            "So switching gears.",
            "So now I want to show application or really sort of conceptual contributions of pack style, statistical learning theory and how that's just very recently last year or two.",
            "I've been starting to shape the work and algorithm theory that's happening on auction design.",
            "So let me give you sort of a 2 slide crash course on auction theory.",
            "The good news is, is the takeaway from these two slides is very minimal.",
            "So what I want you to understand is if you have a common prior.",
            "So if you have a seller who knows the distribution from which buyers willingness to pay is drawn, everything is solved.",
            "We know the optimal thing to do, but the optimal thing to do can be pretty complicated, to the point that if you didn't know this prior, if you didn't have this distributional knowledge, it's not clear that you could quickly learn the right thing to do.",
            "OK, so that's why there's interesting technical questions alright, but let me let me give you a little more detail.",
            "So in this talk I just want to focus on single item auctions.",
            "So literally just think about eBay.",
            "For example, there's one seller they have one thing they want to sell.",
            "The seller has identified some potential buyers and of them.",
            "So the jargon here is that of a bitter valuation that just means the most that the bidder would be willing to pay the valuation visa by.",
            "The valuations are what calls private.",
            "What does that mean?",
            "That just means that the seller doesn't already know what the valuation is, right?",
            "So selling is easy when you know exactly what someone is willing to pay for it.",
            "You just give him a, take it, or leave it offer at that price or slightly below.",
            "So there's no knowledge of the valuations, but the traditional approach in economics is, we assume that the seller is aware of a prior distribution from which valuations are drawn, so you don't know the realizations.",
            "You don't know the vis, but you do know capital F. And then given this knowledge of F. How should you sell this item to maximize your expected revenue, where the expectation is over the randomness in the input over the randomness in the Vi's OK?",
            "So this is the basic optimal auction design problem.",
            "They sell a single item to end bidders with valuations drawn from a known distribution to maximize expected revenue.",
            "So."
        ],
        [
            "This problem was completely solved by an economist Roger Myerson, beautiful paper.",
            "One of the reasons he won the Nobel Prize maybe 5 or 10 years ago.",
            "Let me first describe the solution in a very simple case where the bidders are symmetric, meaning the valuations are drawn IID from a distribution capital left.",
            "Go ahead and think of F is like uniform on a suitable suitable support if you like.",
            "So this amazing result says, despite the fact that the design space here is insanely rich, there's like a million different ways.",
            "Million crazy protocols you could think to use to try to sell one item to one event.",
            "Different bidders, actually, you may as well just slap it up on eBay.",
            "That's optimal over the entire design space.",
            "Just kind of amazing.",
            "What do I mean by slap it up on eBay?",
            "I mean formally, a second price auction with a reserve.",
            "OK, so first of all, what's the second price auction with no reserve?",
            "Also known as the Vickrey auction?",
            "That's just you at you ask for a bid from everybody.",
            "You give the item to the highest bidder and you charge them the second highest price, while the second highest price.",
            "Well.",
            "What happens when you win something on eBay, right?",
            "If you build 100 bucks for like some old iPhone, and you win, you're probably not going to pay $100.",
            "Right, if the second highest bid was only 80, eBay is only going to charge you 80 plus and increment like 81 dollars.",
            "OK.",
            "Similarly, art auctions.",
            "Same thing you wind up paying the second highest price at the end of the day.",
            "So that's how eBay sell stuff.",
            "Second price auction.",
            "What about a reserve?",
            "This is just what's called an opening bid in eBay, and this is just a way for a seller to try and guarantee themselves a certain amount of money conditioned on a sale.",
            "So you can set an opening bid to be like $50.",
            "This means anyone is not willing to pay.",
            "50 is basically just deleted.",
            "And then if anybody wins.",
            "There's certainly going to at least the opening bid of reserve price $50.",
            "They pay the Max of the opening bid, or the second highest, the reserve price, or the second highest bid, whichever is higher.",
            "OK, so formally, Myerson theorem says that over the space of all selling procedures for IID bidders, a second price auction with the correctly set reserve price maximizes the sellers expected revenue.",
            "So this is kind of, you know, really just greatest hit of auction theory.",
            "It just gives you a theoretically justified, practically useful.",
            "Optimal solution to this auction design problem?",
            "OK, but so Myerson."
        ],
        [
            "Did not stop here alright, so more broadly, he thought about the case of heterogeneous bidders.",
            "OK, where bidders are not on ID.",
            "But again, we're thinking here of a seller that knows these distributions, knows that bit arise valuation is drawn from its own idiosyncratic distribution F. Sub I.",
            "For example.",
            "Maybe you've been looking at bidding history from these different bidders, so you have some notion of what they're likely to bid.",
            "Here, theoretically we completely understand what the optimal option is.",
            "It's not so simple.",
            "OK, it fits on this slide, but there's really only one takeaway I want you to have this optimal auction, which is the first step in the first step, is the tricky step.",
            "So what you do in the optimal auction is you again get a bid from every bidder and then for bit awry you transform it to what's called a virtual bid, and there's some funky formula for doing that.",
            "And this is the formula.",
            "Once you accept this step, you just awarded to the highest virtual bidder.",
            "Assuming someone has a positive virtual bid and you charge them the natural analogue of the second highest price, OK, but the only take away I want you to have here is that you are with the ID case.",
            "The only thing we needed to know was the reserve price.",
            "The reserve price was the only thing that varied as we vary the distribution here, even to just compute this function, we need to know the CDF and density of the valuation distribution in detail.",
            "OK, so that's the takeaway here.",
            "Is that the optimal auction is known, but it really abuses the full distributional knowledge OK?",
            "And this option can be weird in other senses.",
            "So for example, sometimes you'll have will very often or not very often, but will sometimes award the item to a bitter other than the highest bidder.",
            "OK, so just 'cause you're the highest bidder doesn't mean you're going to win in this auction, and Moreover, this is unique.",
            "If you really want to squeeze the last few pennies out in your in your expected revenue, you're really forced to use auctions that look like this.",
            "Alright, so that's what I want to know from auction theory.",
            "Theoretically we understand it, but when even in a single item auction, when the bidders don't have the same distribution, the optimal auction has detailed distributional dependence."
        ],
        [
            "And that's a bit of an obstacle when you try to actually apply optimal auction theory.",
            "And of course with the rise of everything from search engines to eBay, there's been a lot of interest and actually trying to make this operational.",
            "The problem is, where does the prior come from?",
            "Especially given that the result of the theory the optimal auction, depends in such a sensitive way about your assumption of the details of this prior.",
            "So this is going to be describing work with Richard Cole, so let me tell you about our motivation and then how we looked to pack style learning theory for inspiration about how to formalize it.",
            "So we wanted to know.",
            "Do optimal auctions really have to be so complicated?",
            "So I told you it's unique, so if you really want 100% of the expected revenue, sure, I guess you need to do this weird auction.",
            "What if you want you know 90% or 95% of the best possible expected revenue?",
            "Does it still have to be really weird, or can Alternatively, can you have a much simpler auction which even theoretically is probably almost as good as the optimal auction?",
            "This was the high level question that we wanted to understand.",
            "Now, conceptually, there's an issue.",
            "How do you set up a mathematical model which is going to give you information about this question?",
            "And of course, the issue is you know what do we mean by you know, requires detailed distributional knowledge.",
            "That suggests we need to somehow parameterize the amount of knowledge that the seller has.",
            "So our proposal is to parameterise seller knowledge through samples.",
            "So we'll still posit that there exists these underlying distributions.",
            "F1 through FN again bitter eyes, willingness to pay is drawn from FC by independently.",
            "But the seller does not know these distributions, except inasmuch as it knows S independent samples for them.",
            "So obviously the bigger S is in some sense the more you know about these distributions.",
            "So that's how we parameterize the knowledge you have about the environment about the distributions.",
            "And now the question is, how much data is necessary and sufficient to do almost as well as if you had full distributional knowledge.",
            "So our benchmark, the Holy Grail is to get expected revenue as large as if someone just literally wrote down F1 through FN for us, and we did the optimal auction.",
            "OK, that's as good as we could get.",
            "How big do we have to take S?",
            "How many samples do we need before we can come up with an auction learning auction which is almost as good as that benchmark?",
            "That's the question.",
            "And one thing we found attractive about this idea, in addition to just, you know.",
            "Pack style learning theory tells us that often you get really nice theory if you sort of set things up this way.",
            "It's also you know when people have struggled to make optimal auction theory operational.",
            "This is sort of what they did, so there's a nice paper by ostroski in shorts.",
            "They were contacted by Yahoo to improve revenue in their keyword search auctions, and this is basically exactly what they did, so they obviously had dreams of bidding data from the bidding data.",
            "They backed out kind of empirical estimates of what the valuation distributions looked like, and then, given those estimates of valuation distributions, they applied.",
            "The optimal auction theory."
        ],
        [
            "So just to make sure the formalism is clear, I'll go through this.",
            "I'll go through this example with a single buyer, so we always just had one seller.",
            "We always only had one item now just temporarily let's imagine there's only one potential buyer.",
            "OK, so now the auction space is no longer rich now the auction space is very simple.",
            "Basically you just give this person to take it or leave it offer at some at some dollar amount.",
            "50 bucks whatever.",
            "OK, so the formalism would now say in a training phase if you like, I give you S samples.",
            "These are now just single numbers from a distribution capital F. Again, you don't know F, you only know V1 through.",
            "VS as a function of the samples, you pick a price.",
            "OK, by whatever means, and then I'm going to evaluate the price by applying it to a fresh sample from the same distribution tomorrow.",
            "OK, what's the best you can hope for?",
            "The best you can hope for is that you computed the so called monopoly price.",
            "OK, so that's just the price that maximizes the selling price times the fraction of sale.",
            "OK, so if you knew the distribution capital F. This is the amount of expected revenue that you would get.",
            "And the goal is to choose a function P so that your expected revenue over both the samples.",
            "So both randomness in both the training set and the test set.",
            "If you like your expected revenue is close to this best possible benchmark."
        ],
        [
            "So on this slide I'm just going to state a couple results for the single buyer case, but these are not the main points of this case study, so this is just to calibrate our expectations of what we might hope to be true for the general multibuy or single item auction case alright?",
            "So first of all, if we assume nothing about capital F, you can't make any statements whatsoever.",
            "OK, 'cause for all we know, capital F is zero almost all the time and there's this tiny probability much smaller than 1 / S that it's huge.",
            "OK, so on pathological distributions like that, you really can't say anything interesting about learning from samples, so you need some kind of condition.",
            "OK, and it turns out kind of all of the various conditions that you might hope we work lead to positive results, and I'm not going to numerate them OK.",
            "But so for example, in auction theory there's a notion of regular distributions.",
            "Saying the tails are no heavier than that of a power law distribution and there already you get positive results.",
            "What do I mean by positive results?",
            "Well, actually, amazingly so.",
            "As a consequence of a famous auction theory result called Beaulah Klemperer theorem.",
            "Even just give you 1 sample.",
            "You can already say something nontrivial for these so called regular or log concave distributions.",
            "OK, so clearly with one sample you have learned very little about Capital F, right?",
            "So most notions of learning a distribution would not be satisfied with one sample, yet you cannot actually already prove some kind of revenue guarantee.",
            "So in fact, if you just had one pass transaction and you use the bid from yesterday as the price for tomorrow, that already gives you a factor 2 approximation of the optimal revenue.",
            "OK, so it's kind of a.",
            "This shows you kind of runs on to something and also suggests that perhaps approximating revenue can be strictly easier than learning in the sentence that we're familiar with, so that's the first thing you get off the ground, even with one sample.",
            "But what if you wanted a much better than 50% approximation?",
            "What do you want?",
            "90% or 99%?",
            "Well, the number of samples you're going to need, of course, will scale with one over epsilon if you're shooting for one minus epsilon approximation, and the good news is, it scales as a polynomial.",
            "OK, so this is joint work.",
            "With this, I'm unsure who I saw here and also jiwang.",
            "So as a function of epsilon, we understand the sample complexity when there's one buyer and it's Poly in one over epsilon, and the stronger the conditions making distributions, the better the sample complexity, but."
        ],
        [
            "Then I state this just is kind of a warm up for the single item case single item auction case where you have multiple buyers.",
            "So the best case scenario with multiple buyers is that our positive results in this special case continue to hold.",
            "Right, so we're kind of rooting for right that the sample complexity remains just too small."
        ],
        [
            "In the multi by our case, so the multibar formalism is what you'd expect.",
            "So now a sample is not just one number but end numbers evaluation per bitter as a function of samples.",
            "You pick single item auction and we evaluate your choice of this function from samples to auctions by the expected revenue of your chosen auction.",
            "Again, the expectation of hers over both the samples which governs what auction is.",
            "Then run and also over the bidders on which that auction is run.",
            "Alright."
        ],
        [
            "So some of the positive results continue to hold.",
            "So if all you want is a very coarse approximation of constant factor approximation, again actually one samples enough.",
            "If you use yesterday's bids as tomorrow's prices, you're within, uh, some not so nice constant factor of an optimal solution.",
            "But again, what if you want a good approximation, 90 percent, 99%?",
            "So the good news is if the bidders were homogeneous, symmetric if they're IID valuations, then actually the sample complexity doesn't scale within at all.",
            "OK remains just Poly in one over epsilon.",
            "No matter how many bidders you have, OK.",
            "In hindsight, actually, that sort of totally makes sense.",
            "'cause what did we learn from Myerson in the ideal case ruling that actually optimal auction is super simple?",
            "It's always a second price auction.",
            "You just gotta set the reserve price correctly.",
            "So you really just need to know this one statistic about the distribution.",
            "It doesn't depend on the number of bidders, so it's what you hope to have that the sample complexity and the ID case remains constant.",
            "So it was less."
        ],
        [
            "Curtis, our priority is that if you relax any or if you relax the assumption, you go to heterogeneous bidders.",
            "Actually, for the first time we get a non trivial lower bound on how many samples are needed to learn a near optimal auction.",
            "OK so unlike the case where you only want to constant approximation and you need 1 sample, unlike the case where it's symmetric and you only need a Poly one over epsilon samples, if their heterogeneous and you really want to get approximation the sample complexity meaning the number of samples S necessary to get a 1 minus epsilon approximation.",
            "Actually, scales with the number of bidders OK, and when I say this just to keep it in mind, a sample already has a numbers.",
            "So I'm really saying you need end samples, each of which is an end vector.",
            "OK, so you need N squared numbers to learn a good approximation of the optimal auction, so this is sort of a formalization of something that you know people who work in pricing new, which is that when your community is heterogeneous, you can really get big wins, often by resorting to more complex pricing structures, and in fact morally what this proof really says.",
            "To answer the original motivating question, we said Dear, do near optimal auctions have to be complex, at least in this setup.",
            "The answer is yes.",
            "So our proof really shows that if your learning algorithm, whatever it is, no matter how smart it is, if it fails to learn these virtual valuation functions, these funky 5 functions at the very top of the distribution, if it fails to have a very accurate model of those functions, it cannot be a 1 minus epsilon approximation for small epsilon.",
            "OK, there's really just nothing I can do.",
            "Without that information about the tips of those virtual valuation distributions and so and that as a consequence gives you the sample complexity 'cause you need lots of samples before you can actually accurately learn these functions at the very top rare event part of those distributions."
        ],
        [
            "OK.",
            "So I rushed through that a little bit.",
            "I'm happy to take questions at the end, but I want to share a time to mention some recent developments and some open questions.",
            "So last couple of slides.",
            "So the formalism I just showed you, I think the inspiration that we took from pack style learning theory was obvious.",
            "I hope it was.",
            "That said, we kind of developed everything from scratch both on the upper bound side and on the lower bound side.",
            "So this is recent work with Jamie Morgenstern.",
            "We just posted this in archive a few weeks ago, so here we're trying to make the connections to statistical learning theory very explicit and really sort of stand on the shoulders of the mature understanding and statistical learning theory to prove new results about auctions.",
            "And so here what we're doing is we're saying think about some set of auctions, auctions, which are, maybe, you know, simple.",
            "The auctions you could imagine actually bringing.",
            "You're bringing to your boss and saying let's try this.",
            "OK, so auctions with a few knobs.",
            "So I want to think of these auctions, this Etsy of auctions is like being a hypothesis class and I want to think of junction is a real valued function.",
            "In what sense when auction takes as input a valuation profile or a bid profile and it makes some amount of money.",
            "OK, on this bid profile so it's a real valued function, so a set of auctions is a set of real valued functions.",
            "Given an unknown distribution on valuation profiles and what people want.",
            "Well, you know, figuring out which of these auctions has the highest expected revenue reduces to having a uniform approximation of the revenue of each one of these auctions with respect to the distribution.",
            "So learning the best auction reduces in the usual sense to uniform estimation of all of these real valued functions.",
            "Therefore, if you can compute standard complexity measures of this space of real valued functions like the pseudo dimension, you can just get for free using standard techniques developed in this Community.",
            "Bounds on the sample complexity and so that's what we did.",
            "OK, So what we did is we showed that we develop some understanding about which classes of auctions have high versus low pseudo dimension and the punch line.",
            "So the good news is that for a very wide range of settings so much broader than just single item auctions, we showed that you can get sort of the best of both worlds.",
            "Well, those complex auctions I told you bout back in Myerson theorem, if we look at all options in the world, the pseudo mention is very pseudodementia is very high, but it's possible to choose a sweet spot set of options C. Which on the one hand is simple enough that you can learn the optimal auction from the class.",
            "Well, OK, so the class has low suitor dimension, yet on the other hand, it's expressive enough that it has low representation error, meaning whatever the unknown distributions F1 through FN might be.",
            "And whatever the weird optimal auction is for F1 through FN, there exists an auction in this Class C that has expected revenue almost as good as a Softail auction 1 minus epsilon times this optimal auction.",
            "OK.",
            "So the punch line then is because there exists these sets.",
            "See that have simultaneously low representation error and low learning error.",
            "It means very broadly from a polynomial number of samples.",
            "This is now polynomial in N also, so it doesn't contradict the lower bound you saw in a polynomial number of samples.",
            "You can learn a near optimal auction.",
            "So that's kind of again just sort of what's going on now in the age."
        ],
        [
            "The community.",
            "So hopefully I've illustrated a couple of they just real gifts that learning theory is already given.",
            "The algorithm game theory, and really, you know it's played a really some of these concepts have played a core role in, you know what's currently hot in a GT, so I hope I gave you some sense for that.",
            "I think it's uncontroversial to predict there will be many more points of contact.",
            "I really hope to see you know papers and cold for years to come in some of these areas and on new points of contact just to leave you with a couple of concrete open directions.",
            "Some of these we already touched on in the questions, so let me just point out the last couple things I was telling you about sample complexity options.",
            "I think there's I mean, it's so nascent, it's not like there's some you know, hard open math problem.",
            "There's just really vast terrains that haven't been explored almost at all, so I think it's really fertile ground to be working in.",
            "So first of all, the results I mentioned in the last slide with Morganstern.",
            "They're only information theoretic.",
            "OK, so our learning algorithms are not efficient, they're just totally generic.",
            "Erm, type learning algorithms, so we're quite pleased with the sample complexity bounds that we had in the settings that we have.",
            "But we really know very little.",
            "About computationally efficient algorithms for learning near optimal auctions.",
            "So in the paper with cold that I mentioned, our algorithm is sufficient for the single item case, but again, that's a pretty limited setting, and so I'd really like to understand how broadly you can have computationally efficient learning algorithms in this space.",
            "One major assumption that we made that was pointed out is we were thinking about full feedback, so we were taking the perspective of eBay or a search engine who really knows what everybody bid on all of these past transactions.",
            "Then it's a good assumption.",
            "You really do have this information, but if you're a third party just sort of observing what's happening on eBay or on a search engine, you probably don't know the bid of every person in the auction.",
            "You know, maybe you know who won.",
            "Maybe you know what the winner bid, or probably you just know what the winner paid, which maybe is going to be something more like the second highest bid.",
            "So you can think both about sort of bandit settings, but there's also these sort of weird hybrid partial information settings which really model the real world problems in a lot of cases, and you know there's been some partial work on those settings, including from this community, but I think it's safe to say there's a lot more to be done and then, as was pointed out by a couple of people like like Sham, I was just assuming that you get these samples for free.",
            "OK, that the bidders in the previous auctions.",
            "Didn't know you were going to use their data set prices in the future and we're reacting accordingly.",
            "This again, there's some partial results, including a paper by Constance talk like this, along with cayenne pepper in the next session.",
            "But again, very unexplored.",
            "Lots more to do, so I'll stop there, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And thanks for having me my first Colt.",
                    "label": 0
                },
                {
                    "sent": "I think it's quite a bit over quite a few years overdue, but happy to be here.",
                    "label": 0
                },
                {
                    "sent": "So my goal for today is to illustrate some points of contact between learning theory and algorithms game theory.",
                    "label": 1
                },
                {
                    "sent": "So what's algorithm game theory?",
                    "label": 0
                },
                {
                    "sent": "So that's a field which is on the boundary of theoretic computer science.",
                    "label": 0
                },
                {
                    "sent": "On the one hand, and game theory, economics on the other.",
                    "label": 0
                },
                {
                    "sent": "That feels about 15 years old.",
                    "label": 0
                },
                {
                    "sent": "It's really exploded during that time.",
                    "label": 0
                },
                {
                    "sent": "There's now at least three annual conferences devoted solely to results in algorithm game theory.",
                    "label": 0
                },
                {
                    "sent": "And I think one of the reasons it's really taken off like a rocket ship in addition to the relevant applications, is that there's kind of an unreasonable number of connections to other parts of theoretical computer science.",
                    "label": 0
                },
                {
                    "sent": "And so naturally, today I'll focus on a couple of.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Connections to this community to learning theory.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This isn't an exhaustive list, but I'm just going to focus on two case studies, so they're very different.",
                    "label": 0
                },
                {
                    "sent": "They're completely independent from each other, so in the first half or a little bit more of the talk, I'll discuss so-called price of anarchy analysis.",
                    "label": 0
                },
                {
                    "sent": "So here the goal is to prove approximation guarantees for game theoretic equilibrium.",
                    "label": 1
                },
                {
                    "sent": "Things like Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "So the price of anarchy has been part of algorithm game theory since it began about 15 years ago, and for a long time people were content to prove bounds about Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "But then there is an issue that Nash equilibrium can be elusive.",
                    "label": 0
                },
                {
                    "sent": "That can be intractable, and there's other issues about that can be hard for players to actually reach, which motivated the need for more robust bounds bounds that apply beyond Nash equilibrium, and so then there's a question you know if you're analyzing a game and you're not going to Lisa Nash equilibria, what are you going to analyze?",
                    "label": 0
                },
                {
                    "sent": "And here online learning sort of comes to the rescue an offers a number of, I think, very attractive alternatives and relaxations of Nash equilibrium analysis, and in this talk I'll focus on guarantees that apply to.",
                    "label": 0
                },
                {
                    "sent": "Players that at least guarantee themselves no regret, so don't necessarily converge to a Nash equilibrium, but the very least every player has vanishing regret over repeated play of the game, so that's the overview of the first case study.",
                    "label": 0
                },
                {
                    "sent": "Then we'll switch gears.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about auction design and the design of revenue maximizing auctions, so here the tradition goes back quite a bit further, so you're not so you won't be surprised to hear that economists in the 20th century so thought about this a lot here.",
                    "label": 0
                },
                {
                    "sent": "The issue is the traditional reliance on a common prior.",
                    "label": 1
                },
                {
                    "sent": "A distribution that is assumed known over the uncertainty in the situation, like what bidders are willing to pay for an item, and here again pack style.",
                    "label": 0
                },
                {
                    "sent": "Statistical learning theory.",
                    "label": 1
                },
                {
                    "sent": "Learning theory offers a very attractive framework for replacing this op priore known common prior with a more data driven approach, i.e.",
                    "label": 1
                },
                {
                    "sent": "Learning a near optimal auction from samples from bids in previous transactions.",
                    "label": 0
                },
                {
                    "sent": "So that's going to be the overview of the talk.",
                    "label": 0
                },
                {
                    "sent": "Those two case studies please feel free to interrupt with questions at.",
                    "label": 0
                },
                {
                    "sent": "Anytime happen to happen, OK.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so let's jump into the first case, study about price of anarchy or guarantees for equilibrium.",
                    "label": 0
                },
                {
                    "sent": "So I'm not going to assume you ever heard of this before and I want to introduce it by.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Way of an example by now the price of anarchy has been studied in many many different application domains, but I still like to illustrate in an example very near and dear to my heart, namely in row.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In games, so recall that in a game you've got players, players have actions and as a function of the actions that players take, every player gets a payoff.",
                    "label": 0
                },
                {
                    "sent": "Or in this case I'm going to call it a cost.",
                    "label": 0
                },
                {
                    "sent": "In this case, think of it is like the travel time, so let's keep it simple.",
                    "label": 0
                },
                {
                    "sent": "Let's say there's just two players.",
                    "label": 0
                },
                {
                    "sent": "Let's say each of the players has three actions corresponding to the three St paths.",
                    "label": 0
                },
                {
                    "sent": "So the two 2 hot paths, and then the one zigzag path.",
                    "label": 0
                },
                {
                    "sent": "So to finish the game description, I have to tell you the costs, so encode those by annotating each of the edges with a cost function.",
                    "label": 0
                },
                {
                    "sent": "Describing the travel time as a function of the number X of players that are using that edge.",
                    "label": 0
                },
                {
                    "sent": "So there's only two players, so extra is going to be either one or two for some of the edges.",
                    "label": 0
                },
                {
                    "sent": "The travel time is always the same.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter how many players use it.",
                    "label": 0
                },
                {
                    "sent": "For other edges, it depends if there's more players than it takes longer to traverse the edge.",
                    "label": 0
                },
                {
                    "sent": "So the idea of the price of anarchy is to compare two different things.",
                    "label": 1
                },
                {
                    "sent": "On the one hand, the outcome of selfish behavior where everybody does whatever they want.",
                    "label": 0
                },
                {
                    "sent": "So in Nash equilibrium, on the other hand, what hypothetical Deity could do with full control over the system?",
                    "label": 0
                },
                {
                    "sent": "So let's look at those in turn.",
                    "label": 0
                },
                {
                    "sent": "So what would selfish players do?",
                    "label": 0
                },
                {
                    "sent": "What you have set up this game so it's really easy to understand, right?",
                    "label": 0
                },
                {
                    "sent": "So if you're one of these players, you can say, well, look at.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This part of the network, the biggest X can ever be as two so 2X is always going to be smaller than five for the same reason, 5X is always going to be smaller than 12, so independent of what the other player does.",
                    "label": 0
                },
                {
                    "sent": "You want to make use of the two X link and the five X link.",
                    "label": 0
                },
                {
                    "sent": "So in other words it's called the dominant strategy for each player to route themselves on the zigzag path.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the unique Nash equilibrium and Nash equilibrium recall is just an outcome where nobody can do better by deviating unilaterally.",
                    "label": 0
                },
                {
                    "sent": "So let's evaluate this Nash equilibrium.",
                    "label": 1
                },
                {
                    "sent": "So the travel time on this link is 4.",
                    "label": 0
                },
                {
                    "sent": "On this link it's 10, so each player experiences a cost of 14.",
                    "label": 0
                },
                {
                    "sent": "So the joint costs for the two players would be 28.",
                    "label": 0
                },
                {
                    "sent": "Now the other side of the equation is we ask.",
                    "label": 0
                },
                {
                    "sent": "OK, suppose players didn't get to do whatever they wanted.",
                    "label": 0
                },
                {
                    "sent": "Suppose we could pick routes for them.",
                    "label": 0
                },
                {
                    "sent": "Could we do better in this network?",
                    "label": 0
                },
                {
                    "sent": "We could if we separate.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Them on their own 2 hot paths.",
                    "label": 0
                },
                {
                    "sent": "Well, the green player would still have cost fourteen, 2 + 12, but the red player would now have cost only 5 + 5 or 10K, so the joint costs would have dropped to 24.",
                    "label": 0
                },
                {
                    "sent": "So by definition the price of anarchy is simply the ratio of these two quantities.",
                    "label": 1
                },
                {
                    "sent": "The Nash equilibrium objective function value over the optimal one in this particular network it would be 7 /, 6 in a different network.",
                    "label": 0
                },
                {
                    "sent": "It would presumably be different, so one important point is that you know, while in this network there's a unique Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "In general there can be many.",
                    "label": 0
                },
                {
                    "sent": "An we prefer most conservative worst case bounds, so there's other alternatives, but will only look at bounds that apply to every single equilibrium.",
                    "label": 1
                },
                {
                    "sent": "OK, the worst case equilibrium.",
                    "label": 1
                },
                {
                    "sent": "So like I said at this point, the price of anarchy has been exhaustively studied.",
                    "label": 0
                },
                {
                    "sent": "Tight bounds or known for many many different application domains.",
                    "label": 0
                },
                {
                    "sent": "For example, for these kinds of routing networks without find cost functions, the worst case, the highest this can ever be is 2.5 is one is 1 example of what's known, but again, there's dozens and dozens of results of this form.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's kind of turned into kind of a cottage industry.",
                    "label": 0
                },
                {
                    "sent": "It's no longer uncommon for people to send me sort of new papers.",
                    "label": 0
                },
                {
                    "sent": "They're writing with titles like this one preventative healthcare.",
                    "label": 0
                },
                {
                    "sent": "So this was, you know, the UK was proposing a decentralization of aspects of their healthcare system.",
                    "label": 0
                },
                {
                    "sent": "So some researchers were trying to understand what would be the effect would be the cost or the blow up an objective function value from moving this part of the healthcare system from centralized to decentralized.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or even this one.",
                    "label": 0
                },
                {
                    "sent": "Is from the.",
                    "label": 0
                },
                {
                    "sent": "I'm not kidding.",
                    "label": 0
                },
                {
                    "sent": "The MIT Sports Conference on MIT Conference on sports analytics.",
                    "label": 0
                },
                {
                    "sent": "This acute paper actually so the author posits a metaphor between the routing networks.",
                    "label": 0
                },
                {
                    "sent": "I just showed you and the decisions of basketball makes about how to pass and how to shoot.",
                    "label": 0
                },
                {
                    "sent": "So the authors, thinking of the players being nodes in a network and then the terminal is like the hoop and then an edge corresponds to a pass between players or shot on the basket.",
                    "label": 0
                },
                {
                    "sent": "So the analog of the links getting more costly in a Rd network as more people use it.",
                    "label": 0
                },
                {
                    "sent": "That just says like for example, if the same player shoots too much, the effectiveness of that player is going to go down, presumably because it's more heavily guarded by the other team, so you know.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "For those of you who know braces Paradox, which is this cute example cute cocktail party example that you can actually remove edges from a network and you can make the equilibrium better.",
                    "label": 0
                },
                {
                    "sent": "The author suggests that maybe that's why, for a period of time in the mid 90s, the Knicks actually scored more points when their star center, Patrick Ewing was on the bench when he was actually playing.",
                    "label": 0
                },
                {
                    "sent": "So you know it's it's fun paper.",
                    "label": 0
                },
                {
                    "sent": "So any questions before I sort of segue into why these price of anarchy bounds about Nash equilibrium really aren't good enough.",
                    "label": 0
                },
                {
                    "sent": "Why we needed new notions, which is where learning theory sort of came to the rescue, and these questions in the preamble.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why weren't we happy just having dozens of tight bounds in the price of anarchy for a wide range of application domains?",
                    "label": 0
                },
                {
                    "sent": "Well, let's sort of drill down and think about what does it mean when you prove that, say, the price of anarchy is at most two, what that literally means is that in your game, if players reach an equal Nash equilibrium, you don't care which Nash equilibrium, but if they reach some Nash equilibrium, then it's a guarantee that the performance of your system is near optimal within a factor two of optimal.",
                    "label": 0
                },
                {
                    "sent": "So the concern is the strength of this hypothesis.",
                    "label": 0
                },
                {
                    "sent": "If the players reach.",
                    "label": 0
                },
                {
                    "sent": "Some Nash equilibrium of the game.",
                    "label": 0
                },
                {
                    "sent": "Sure, you know it's a 0 sum game or something simple like that.",
                    "label": 0
                },
                {
                    "sent": "Maybe you have reason to believe that players will be good at playing a Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "But more broadly there are issues actually.",
                    "label": 0
                },
                {
                    "sent": "I mean, I've been sort of glossing over what kind of Nash equilibrium I'm talking about.",
                    "label": 0
                },
                {
                    "sent": "So pure Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "That's where nobody randomizes.",
                    "label": 0
                },
                {
                    "sent": "Every player just picks one outcome, and nobody can be better off by deviating in those routing games.",
                    "label": 0
                },
                {
                    "sent": "You always have pure equilibrium, but in lots of games you don't like matching pennies, say.",
                    "label": 0
                },
                {
                    "sent": "So if you talk about equilibrium bound just for pure Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "That can even be vacuous, so that motivates moving to mixed Nash equilibrium where players can randomize.",
                    "label": 0
                },
                {
                    "sent": "These always exist, that's what Nash proved, but some of the greatest hits out of algorithm game theory actually explain why mixed Nash Equilibria are in some sense and intractable solution concept.",
                    "label": 1
                },
                {
                    "sent": "So the formal statement is there PPD complete to compute.",
                    "label": 0
                },
                {
                    "sent": "That's just sort of the analogue of NP completeness suitable for equilibrium computation, so we don't actually believe and say just even say two player General Bimatrix games.",
                    "label": 0
                },
                {
                    "sent": "We don't believe there's a polynomial time algorithm to compute a mixed Nash equilibrium in general.",
                    "label": 0
                },
                {
                    "sent": "Let alone that distributed players can actually coordinate and compute one at any reasonable amount of time.",
                    "label": 0
                },
                {
                    "sent": "So that's a concern, but we have this rich approximation theory developing about how good Nash equilibrium can be, but at the same time results from complexities saying that it might not be a meaningful equilibrium concept in all situations because of its intractability.",
                    "label": 0
                },
                {
                    "sent": "So that motive.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's striving for price of anarchy bounds that apply beyond Nash equilibrium.",
                    "label": 1
                },
                {
                    "sent": "OK, maybe the Nash equilibrium particular, but also to other stuff which is much easier to compute and more plausable predictions of gameplay.",
                    "label": 0
                },
                {
                    "sent": "So this important agenda was first articulated by Meraki Invita mid last decade, so initially people adopted some concepts familiar from game theory but not the only one.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about today and what I still think is probably the most elegant solution was proposed by Auburn Blum who I saw out on the coffee break.",
                    "label": 0
                },
                {
                    "sent": "And coauthors Evan Darla get Hadji Guy and Aaron Roth.",
                    "label": 0
                },
                {
                    "sent": "And so they proposed.",
                    "label": 0
                },
                {
                    "sent": "Instead of thinking about players playing a Nash equilibrium, think about repeated play overtime.",
                    "label": 0
                },
                {
                    "sent": "And rather than assuming that players converge to an equilibrium, assume merely that each one is achieved.",
                    "label": 0
                },
                {
                    "sent": "No regret.",
                    "label": 0
                },
                {
                    "sent": "I'll define that formally later, but it's what most of you would think.",
                    "label": 0
                },
                {
                    "sent": "It just says.",
                    "label": 0
                },
                {
                    "sent": "There's been repeated play of a game for each player.",
                    "label": 0
                },
                {
                    "sent": "Its average payoff is at least as good or close to what would have been the best fixed action in hindsight's.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a much, much weaker.",
                    "label": 0
                },
                {
                    "sent": "Notion assumption than assuming that players play in Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what I'm going to focus on next.",
                    "label": 0
                },
                {
                    "sent": "And it turns out very general bounds.",
                    "label": 0
                },
                {
                    "sent": "So the good news is very general.",
                    "label": 0
                },
                {
                    "sent": "Positive bounds are possible assuming only no regret learning.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So specifically, here's a here's sort of the main.",
                    "label": 0
                },
                {
                    "sent": "The main point of the first half of the talk, and I'll state it informally now and will gradually make this more precise over the subsequent slides.",
                    "label": 0
                },
                {
                    "sent": "So we had zillions of price of anarchy bounds across lots of different application domains, and the good news is that most of them not 100%, but you know something like 2/3 or 3/4 of these price of anarchy bounds.",
                    "label": 0
                },
                {
                    "sent": "Actually, in many cases, sort of unbeknownst to the original provers of these bounds, apply more broadly to no regret learners.",
                    "label": 0
                },
                {
                    "sent": "They do not just apply to Nash equilibria, they actually apply to every possible outcome sequence generated by players achieving no regret.",
                    "label": 1
                },
                {
                    "sent": "So that's kind of the high level takeaway.",
                    "label": 0
                },
                {
                    "sent": "Now on this slide, let me drill down a little bit what I mean by this.",
                    "label": 0
                },
                {
                    "sent": "This is not a formal statement in particular, probably wondering you know what does this mean?",
                    "label": 0
                },
                {
                    "sent": "What I mean by most?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's how it's going to work.",
                    "label": 0
                },
                {
                    "sent": "Alright, so just as far as the interpretation.",
                    "label": 0
                },
                {
                    "sent": "So again, let me emphasize.",
                    "label": 0
                },
                {
                    "sent": "A special case of no regret learning is picking a Nash equilibrium and playing it over and over and over again.",
                    "label": 0
                },
                {
                    "sent": "OK, so you're only proving a stronger bound, only proving a bound to more stuff by proving bounds and #8 number at learners, the Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "But as many of you know, many many things that achieve no regret for all players are not Nash equilibrium, so in particular computationally much more tractable to achieve no regret than it is to converge to a Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "So in that sense, this is a much more robust bound than merely for Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "So how is this going to go?",
                    "label": 0
                },
                {
                    "sent": "So how is how is this sort of meta result going to be proved?",
                    "label": 0
                },
                {
                    "sent": "So it's going to be approved by what I call an extension theorem, and you can think of this as sort of a black box lifting of a price of anarchy.",
                    "label": 0
                },
                {
                    "sent": "Bound originally imagined merely for Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "But then the extension theorem will be sort of a compiler, and it compiles this bound just for pure Nash equilibria into a robust bound that holds for all outcomes generated by no regret learners.",
                    "label": 1
                },
                {
                    "sent": "So that's the idea of an extension theorem.",
                    "label": 0
                },
                {
                    "sent": "Part one you prove about for Nash Equilibria Part 2 you apply the extension theorem and you get a much more general and robust conclusion.",
                    "label": 0
                },
                {
                    "sent": "Again, I'll make this precise.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea.",
                    "label": 0
                },
                {
                    "sent": "Now, there's no hope to have, so then the question is OK. What fraction of the price of anarchy bounds in the literature are?",
                    "label": 0
                },
                {
                    "sent": "Actually, for which does this extension theorem apply.",
                    "label": 0
                },
                {
                    "sent": "So as well see there's not going to be an extension theorem that applies to every conceivable price of anarchy bound for Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "It applied to some bounds and not to others.",
                    "label": 0
                },
                {
                    "sent": "And the good news is that if you look at the price of anarchy bounds that are known in the literature, again, the overwhelming majority say 2/3 or 3/4 of them are in fact proofs of the sort to which this extension theorem applies.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the high level idea, so I need to define what types of proofs can you apply the extension theorem to you.",
                    "label": 0
                },
                {
                    "sent": "I need to explain what the extension theorem States and that's where we're going next.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's a pictorial representation of what I just said.",
                    "label": 0
                },
                {
                    "sent": "I like to think of these extension theorems is sort of a tool for the lazy analyst and present company, not necessarily excluded.",
                    "label": 0
                },
                {
                    "sent": "So you're an analyst.",
                    "label": 0
                },
                {
                    "sent": "You want theorem, you want the conclusion of your theorem to be as strong as possible.",
                    "label": 0
                },
                {
                    "sent": "And so again, we're thinking about a worst case bound over a bunch of outcomes.",
                    "label": 0
                },
                {
                    "sent": "So the strongest statement is to have this worst case bound applied to his wider range of outcomes as possible.",
                    "label": 0
                },
                {
                    "sent": "Again, not just a Nash equilibrium, but lots of other stuff also.",
                    "label": 0
                },
                {
                    "sent": "So at the end of the day, we want a theorem.",
                    "label": 0
                },
                {
                    "sent": "Which says that every say outcome generated by no regret learners is close to optimal.",
                    "label": 0
                },
                {
                    "sent": "That's what we want because we want to conclude.",
                    "label": 1
                },
                {
                    "sent": "But now if we also have to actually prove the thing.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Be a lot nicer.",
                    "label": 0
                },
                {
                    "sent": "It be a lot simpler if we only had to worry about some very simple concepts like pure Nash equilibrium.",
                    "label": 1
                },
                {
                    "sent": "OK, I don't want to think about these arbitrary number grid algorithm algorithms.",
                    "label": 0
                },
                {
                    "sent": "Who can think about this, whereas you just give me a pure equilibrium like in that routing network.",
                    "label": 0
                },
                {
                    "sent": "You know, maybe I can bare hands, start thinking about OK. Is this close to optimal far from optimal or what?",
                    "label": 0
                },
                {
                    "sent": "OK, so as far as what to prove this is the much simpler thing to prove bounds about, even though that's what we want the conclusion for.",
                    "label": 0
                },
                {
                    "sent": "So what the extension theorem really is?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Enables you to do as an analyst.",
                    "label": 0
                },
                {
                    "sent": "It allows you to just focus all your efforts on the left hand side so you look at your game, you understand what's special about your game like your routing, games or whatever.",
                    "label": 0
                },
                {
                    "sent": "You prove abound merely if you're pure equilibrium.",
                    "label": 0
                },
                {
                    "sent": "And then this extension theorem is just going to apply as a black box and carry that same approximation bound over to the much more general set of outcomes generated by no regret learners.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the that's the point of the extension theorem, you just glue it together with a pure Nash equilibrium bound and you get the kind of theorem you want.",
                    "label": 1
                },
                {
                    "sent": "OK, so I know the last couple slides are probably a little vague.",
                    "label": 0
                },
                {
                    "sent": "A little high level so I do want to.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Start filling in the details and I did I did want to at least spend maybe 10 minutes 5 to 10 minutes of the talk actually showing you the actual mathematical definition and actual proof.",
                    "label": 0
                },
                {
                    "sent": "I know it's day three, but if you have if you can muster energy to understand some math for 10 minutes, this is those 10 minutes.",
                    "label": 0
                },
                {
                    "sent": "If you can't I understand, don't worry, after this will do a total reset and move on to the second case study.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "The goal now is to explain this extension theorem and specifically under what conditions does this extension theorem apply?",
                    "label": 0
                },
                {
                    "sent": "OK, So what I'm going to do is I'm going to give you a feel for what it means to prove approximation guarantee for equilibrium because we're going to do that.",
                    "label": 0
                },
                {
                    "sent": "OK, what would that prove look like and then impose conditions on what the proof looks like so that you get this guarantee not just for Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "Much more generally, this actually easiest to explain if we sort of zoom out to a quite abstract level.",
                    "label": 0
                },
                {
                    "sent": "Alright, so again think about just an arbitrary game, so we've got players.",
                    "label": 0
                },
                {
                    "sent": "They've got strategies as survive.",
                    "label": 0
                },
                {
                    "sent": "This is the strategy picked by player I, so that's like a path by player I through the network.",
                    "label": 0
                },
                {
                    "sent": "This bold S that's a strategy profile, so one strategy, or one action per player so bold S is like a traffic pattern.",
                    "label": 0
                },
                {
                    "sent": "And remember, the data were given is for each possible outcome.",
                    "label": 0
                },
                {
                    "sent": "What is the cost that each player incurs in that outcome?",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a game, an assumption which is important for this result.",
                    "label": 0
                },
                {
                    "sent": "Is that what we ideally would want to accomplish is minimizing the joint cost, the sum of the players costs.",
                    "label": 0
                },
                {
                    "sent": "That's exactly what we're doing in that routing game example on slide one.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so imagine you had like a problem.",
                    "label": 0
                },
                {
                    "sent": "Set a homework OK and it said prove that the price of anarchy of some game is bounded by two.",
                    "label": 0
                },
                {
                    "sent": "Alright, so how might you start that proof?",
                    "label": 0
                },
                {
                    "sent": "OK, well remember what it means.",
                    "label": 0
                },
                {
                    "sent": "So to prove that the price of anarchy is at most two, you have to say every single equilibrium.",
                    "label": 0
                },
                {
                    "sent": "Again, there might be many, but every equilibrium has cost at most twice optimal.",
                    "label": 0
                },
                {
                    "sent": "So clearly the relevant characters in our proof are a Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "Pick an arbitrary one, call it S, and then we need to compare its cost to the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "S Star star minimizes the cost.",
                    "label": 0
                },
                {
                    "sent": "That's what this hypothetical deity would implement.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we're going to start with the equilibrium costs.",
                    "label": 0
                },
                {
                    "sent": "And now we basically want to chain of inequality is less than or equal to inequality's, terminating with constant times cost of star, right?",
                    "label": 0
                },
                {
                    "sent": "That's what that's going to look like, so without doing any thinking at all without even looking at what our game is, let's think about how far we can get.",
                    "label": 0
                },
                {
                    "sent": "So, how far can we get without even thinking about what we're doing?",
                    "label": 0
                },
                {
                    "sent": "Well, you know at the moment we've got very little going for us, right?",
                    "label": 0
                },
                {
                    "sent": "So we've got all we've got really is that?",
                    "label": 0
                },
                {
                    "sent": "We know S is in equilibrium.",
                    "label": 0
                },
                {
                    "sent": "That's kind of all we've got on our side.",
                    "label": 0
                },
                {
                    "sent": "I guess we also know that the objective function is the sum of the player costs.",
                    "label": 1
                },
                {
                    "sent": "So let's start with that cost of the equilibrium expand.",
                    "label": 1
                },
                {
                    "sent": "That is the sum over the cost of each player I.",
                    "label": 0
                },
                {
                    "sent": "So we got to use the equilibrium hypothesis.",
                    "label": 0
                },
                {
                    "sent": "How would we do that?",
                    "label": 0
                },
                {
                    "sent": "So what does it say?",
                    "label": 0
                },
                {
                    "sent": "It says if any player deviates unilaterally, it only gets worse.",
                    "label": 0
                },
                {
                    "sent": "Its cost only goes up OK Alright.",
                    "label": 0
                },
                {
                    "sent": "So to apply that condition we need to sort of hallucinate a deviation for this player.",
                    "label": 0
                },
                {
                    "sent": "An in some sense, like the only other object on the slide, is this optimal solution at star, and actually that kind of makes sense, but we have this equilibrium S we're trying to prove it's not too expensive, and so we basically want to say we really wish the players were all playing at star.",
                    "label": 0
                },
                {
                    "sent": "So let's at least kind of like interrogate player.",
                    "label": 0
                },
                {
                    "sent": "I hate player, I why aren't you playing your optimal strategy?",
                    "label": 0
                },
                {
                    "sent": "Optimal solution estar?",
                    "label": 0
                },
                {
                    "sent": "Why aren't you playing Sri?",
                    "label": 0
                },
                {
                    "sent": "The player says 'cause I'd be worse off, right?",
                    "label": 0
                },
                {
                    "sent": "This is Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "Now deviation?",
                    "label": 0
                },
                {
                    "sent": "My cost would go up.",
                    "label": 0
                },
                {
                    "sent": "So that's all this inequality here is saying.",
                    "label": 0
                },
                {
                    "sent": "It says if you think about the I TH term and you think about player, I imagining deviating to what it should be doing in the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "Its costs would only go up.",
                    "label": 0
                },
                {
                    "sent": "So the notation here is that player I is playing, its strategy and estar, everybody else is playing the same thing.",
                    "label": 0
                },
                {
                    "sent": "They're playing in S. That's what that notation means.",
                    "label": 0
                },
                {
                    "sent": "So that's after unilateral deviation to Sri starting from bold S. OK, so this is the first inequality, and we got here.",
                    "label": 0
                },
                {
                    "sent": "Without knowing anything about our game whatsoever.",
                    "label": 0
                },
                {
                    "sent": "OK. And this is all we can get, which makes sense.",
                    "label": 0
                },
                {
                    "sent": "The price of energy in some games is going to be high in other games it's going to be low, right?",
                    "label": 0
                },
                {
                    "sent": "So at some point in this proof we need to actually understand something about the structure of our game, so that has to happen next.",
                    "label": 0
                },
                {
                    "sent": "So we're.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I had to come up to a key definition that of a game being smooth and with the key definition is doing is it's sort of saying well what could the rest of this proof look like OK?",
                    "label": 0
                },
                {
                    "sent": "And fundamentally, what is the rest of the proof have to do?",
                    "label": 0
                },
                {
                    "sent": "OK, so we got this far and notice the right hand side here.",
                    "label": 0
                },
                {
                    "sent": "This quantity we could care less about this number.",
                    "label": 0
                },
                {
                    "sent": "This is some like weird entangled version of SNS star that has no meaning to us.",
                    "label": 0
                },
                {
                    "sent": "The only thing that means anything to us, given that we're trying to prevent rebound, is the equilibrium cost and the optimal costs.",
                    "label": 0
                },
                {
                    "sent": "The costs of SNS star.",
                    "label": 0
                },
                {
                    "sent": "Those are the only objects of interest, so the rest of the proof fundamentally is a relationship between this number that we don't care about, and the two numbers that we do care about the cost of S and the cost of star.",
                    "label": 0
                },
                {
                    "sent": "So the key definition is out of a game.",
                    "label": 1
                },
                {
                    "sent": "Being smooth with parameters Lambda and mu.",
                    "label": 0
                },
                {
                    "sent": "So a game is smooth if you can relate that entangled quantity that we don't care about in a linear way to the two quantities we do care about if we can upper bound the cost of that entangled quantity by Lambda times the optimal cost plus mu times the equilibrium cost.",
                    "label": 0
                },
                {
                    "sent": "It's also important that you is less than one OK.",
                    "label": 0
                },
                {
                    "sent": "So you may or may not be able to establish this inequality for a given game, and for certain parameters of Lambda new.",
                    "label": 0
                },
                {
                    "sent": "This will depend on the game.",
                    "label": 0
                },
                {
                    "sent": "For nice games, you'll be able to establish this condition with reasonable Lambda mu.",
                    "label": 0
                },
                {
                    "sent": "What do I mean?",
                    "label": 0
                },
                {
                    "sent": "Maybe like lambdas.",
                    "label": 0
                },
                {
                    "sent": "Two and Mu is a half a nice small numbers.",
                    "label": 0
                },
                {
                    "sent": "Other games you won't be able to do it.",
                    "label": 0
                },
                {
                    "sent": "So what I want to do next is OK if you prove a game is smooth, at the very least you can conclude that the price of anarchy of Nash equilibrium is small Nash equilibrium guaranteed to be near optimal, but we want broader statements and will get them later.",
                    "label": 0
                },
                {
                    "sent": "But for starters, I claim this is a sufficient condition.",
                    "label": 0
                },
                {
                    "sent": "For a game to have a small price of anarchy, where small depends on Lambda and you.",
                    "label": 0
                },
                {
                    "sent": "Why is that true?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's the first inequality.",
                    "label": 0
                },
                {
                    "sent": "This is copied from the last slide we got this inequality display applying the Nash equilibrium apophysis because the game is smooth.",
                    "label": 1
                },
                {
                    "sent": "I can upper bound this.",
                    "label": 0
                },
                {
                    "sent": "I can charge this against linear combination of the optimal an equilibrium costs mu remember is less than one, so I can subtract that last term from both sides and divide by 1 minus mu.",
                    "label": 0
                },
                {
                    "sent": "So I get that the cost of this arbitrary equilibrium is at most Lambda over 1 minus mu times that of the optimal solution estar.",
                    "label": 0
                },
                {
                    "sent": "So for example, if lambdas two and mu is 1/2, you're going to get a four approximation.",
                    "label": 0
                },
                {
                    "sent": "For all Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "So again, the point of this slide is that, at the very least, a game being smooth is a sufficient condition for having a small price of anarchy.",
                    "label": 0
                },
                {
                    "sent": "Actually, it's sort of overkill if you think about it, because if you look at the statement, I'm asserting that no man I'm asserting this condition this inequality for every single pair of outcomes SNS star in the derivation.",
                    "label": 0
                },
                {
                    "sent": "Actually, we only care about equilibrium S and optimal solutions S star, so the payoff of this stronger definition will come later.",
                    "label": 0
                },
                {
                    "sent": "But I just want to plant that seed that actually this is overkill if all we cared about were Nash equilibrium, because we only need this to hold for equilibrium S, But I'm asserting it for all outcomes S, so it will come up in no regrets sequences.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That's a definition.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "Why should you care?",
                    "label": 0
                },
                {
                    "sent": "OK, so to make you care, I need to give you 2 things I need to give you examples.",
                    "label": 0
                },
                {
                    "sent": "As in, you can actually establish this definition and applications that people care about.",
                    "label": 1
                },
                {
                    "sent": "Two, I need to give you implications beyond just the pure Nash equilibrium bounds and the implications will be to no regret sequences.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to do those two things quickly next.",
                    "label": 0
                },
                {
                    "sent": "Any questions before then.",
                    "label": 0
                },
                {
                    "sent": "Right, I see so the question right?",
                    "label": 0
                },
                {
                    "sent": "So even for no regrets.",
                    "label": 0
                },
                {
                    "sent": "OK, good question.",
                    "label": 0
                },
                {
                    "sent": "Yeah, there's a couple of questions in here.",
                    "label": 0
                },
                {
                    "sent": "So the question is basically about to what extent are these techniques overkill and you can ask different versions of this question depending on what bounds you want.",
                    "label": 0
                },
                {
                    "sent": "So one question would be you know so.",
                    "label": 0
                },
                {
                    "sent": "So jumping the gun a little bit, we're going to get guarantees for no regret sequences.",
                    "label": 0
                },
                {
                    "sent": "So they hear a question was, well, maybe this is overkill for guarantees for no regret sequences.",
                    "label": 0
                },
                {
                    "sent": "Actually, here you can prove a theorem that this is an overkill.",
                    "label": 0
                },
                {
                    "sent": "So basically I mean I won't have time to talk about this, but and there's some fine print here, but more or less bounds for no regrets sequences.",
                    "label": 0
                },
                {
                    "sent": "Here the caveat is it's really for if you think about a generalization of no regrets sequences were on average over players.",
                    "label": 0
                },
                {
                    "sent": "There's no regret.",
                    "label": 0
                },
                {
                    "sent": "So some players have positive regrets in a negative, but it averages out to no regret.",
                    "label": 0
                },
                {
                    "sent": "You still going to get bounds for those sequences, and that's you can actually prove using convex duality.",
                    "label": 0
                },
                {
                    "sent": "That is the limit of the of the smoothness condition, so there's actually a dual correspondence between this and mild generalization error sequence is the other question you can ask is like, well, what if I didn't care about, never got sequences, they only care about Nash equilibrium, then surely this is overkill because it's giving me something I don't want.",
                    "label": 0
                },
                {
                    "sent": "It's giving me this extra generality I don't want, and presumably getting worse bounds, and here amazingly so sometimes that's true.",
                    "label": 0
                },
                {
                    "sent": "So sometimes if you only care about your equilibrium, this is overkill.",
                    "label": 0
                },
                {
                    "sent": "But amazingly and again I can't.",
                    "label": 0
                },
                {
                    "sent": "I don't have time to talk about this.",
                    "label": 0
                },
                {
                    "sent": "I'm happy to talk offline.",
                    "label": 0
                },
                {
                    "sent": "You can actually prove, even if you only care about Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "There are classes of games, including routing, games or even for Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "This sufficient condition gives you the right answer and you can.",
                    "label": 0
                },
                {
                    "sent": "You can prove that generically you can always sort of set Lambda and mu to give you the right answer, even for just Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "So there are cases where this is a completely tight analysis framework.",
                    "label": 0
                },
                {
                    "sent": "OK. Any other questions before I talk about examples of the definition being satisfied and why it's worthwhile to satisfy the definition?",
                    "label": 0
                },
                {
                    "sent": "OK, so for example.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm really just gonna ask you to trust me, so it's just the fact that a huge chunk of the literature, the way they prove bounds on the price of anarchy are smoothness proofs.",
                    "label": 0
                },
                {
                    "sent": "There are many examples known before this definition was formalized without the examples.",
                    "label": 0
                },
                {
                    "sent": "I don't see how one would have formalized the definition after the definition.",
                    "label": 0
                },
                {
                    "sent": "People have come up with many more examples.",
                    "label": 0
                },
                {
                    "sent": "In particular, is really caught fire.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Amongst people who think about auctions over the past three years or so, so there's lots of exam.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It turns out this is really the dominant paradigm of proving bounds on equilibrium.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All right, why should you care?",
                    "label": 0
                },
                {
                    "sent": "So here's formally what I mean by the extension theorem.",
                    "label": 0
                },
                {
                    "sent": "So suppose you have a game which is Lambda mu smooth.",
                    "label": 0
                },
                {
                    "sent": "Consider any no regret sequence.",
                    "label": 0
                },
                {
                    "sent": "I'll define that for me in the next slide.",
                    "label": 0
                },
                {
                    "sent": "If it's just what no regret learners generate.",
                    "label": 0
                },
                {
                    "sent": "Consider any no regret sequence, and in fact the average joint cost across the number get sequence is bounded by Lambda over 1 minus mu times the optimal outcome.",
                    "label": 0
                },
                {
                    "sent": "And the point here is that this approximation bound is exactly the same bound we were already getting for pure equilibrium.",
                    "label": 0
                },
                {
                    "sent": "So we've lifted the guarantee with no quantitative loss from Nash equilibrium to no regret learners.",
                    "label": 0
                },
                {
                    "sent": "So there's also analogs of this extension theorem for many other relaxations of Nash equilibrium as well, but I'm just going to focus on these no regret sequences.",
                    "label": 0
                },
                {
                    "sent": "So it's formally what I mean by the extension theorem for smooth games, the land over 1 minus mu lifts or extends from Nash equilibrium to no regret sequences.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's another great sequence?",
                    "label": 0
                },
                {
                    "sent": "So we're thinking about a fixed game, a fixed set of players, like a fixed routing network.",
                    "label": 0
                },
                {
                    "sent": "We're thinking about the same players playing the game day after day after day, say capital T days.",
                    "label": 1
                },
                {
                    "sent": "So it's like you have a routing network and you're looking at a sequence of T traffic patterns.",
                    "label": 0
                },
                {
                    "sent": "Rush rush, hour traffic every morning.",
                    "label": 0
                },
                {
                    "sent": "What does it mean?",
                    "label": 0
                },
                {
                    "sent": "That what does it mean for a sequence to be no regret?",
                    "label": 0
                },
                {
                    "sent": "Well, so you look back in hindsight and use a phrase in terms of costs.",
                    "label": 0
                },
                {
                    "sent": "So for a fixed player, I and affixed deviation Qi, the average cost that the player in curd over the sequence should be no worse than the minimum average cost it could have attained in hindsight.",
                    "label": 1
                },
                {
                    "sent": "With this fixed action, if it had just played Qi every single day.",
                    "label": 0
                },
                {
                    "sent": "So like in the routing framework, you know, maybe you're exploring different ways to get from home to work over some 100 day.",
                    "label": 0
                },
                {
                    "sent": "You should have at least guaranteed yourself average travel time as small as any fixed route that you could have chosen day after, day after day.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, when you're talking about no regret, you know.",
                    "label": 0
                },
                {
                    "sent": "Usually you have an error term, and I know a lot of people care a lot about the exact form of that error term.",
                    "label": 0
                },
                {
                    "sent": "Apologies, but I'm just going to totally suppress this.",
                    "label": 0
                },
                {
                    "sent": "We all know it's there, it just passes through the approximation results very effortlessly with no problems.",
                    "label": 0
                },
                {
                    "sent": "So I'm just for simplicity going to assume this is zero.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to worry about the convergence rate, but the understanding is that T is reasonably big and this is going to zero with T. So that's a no regret sequence, and now the proof of the extension theorem really just writes itself OK, so let me show you that if you have a smooth game and you.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No regret sequence then.",
                    "label": 0
                },
                {
                    "sent": "The average cost across the new regret sequences land over 1 minus mu times that of an optimal outcome.",
                    "label": 0
                },
                {
                    "sent": "So here's the sequence supposed to satisfy the network condition.",
                    "label": 0
                },
                {
                    "sent": "The game is fixed, so there's a fixed optimal solution called as Star.",
                    "label": 0
                },
                {
                    "sent": "Assume the game is Lambda mu smooth.",
                    "label": 0
                },
                {
                    "sent": "So let's just scale out by T for the moment.",
                    "label": 0
                },
                {
                    "sent": "We're going to start the exact same way we started when we were thinking abstractly about what an equilibrium guarantee looks like.",
                    "label": 0
                },
                {
                    "sent": "So we look at the joint cost, overall days T, and we expanded over the players.",
                    "label": 0
                },
                {
                    "sent": "OK, so some over the days T players I cost incurred by on day T. Now the next step is where it seems like we get in trouble, why?",
                    "label": 0
                },
                {
                    "sent": "Previously, this is where we use that we were dealing with a Nash equilibrium.",
                    "label": 0
                },
                {
                    "sent": "We said if we walk up the playwright and we force it to switch to Sri, it only gets worse.",
                    "label": 0
                },
                {
                    "sent": "That's no longer true.",
                    "label": 0
                },
                {
                    "sent": "Remember, we're interested in situations where these are not equilibrium.",
                    "label": 0
                },
                {
                    "sent": "OK, that's a very boring kind of no regrets sequence.",
                    "label": 0
                },
                {
                    "sent": "We're thinking of that all of these traffic patterns are perhaps very far from equilibrium, and it only satisfies this sort of on average overtime equilibrium condition, as in the no regret guarantee.",
                    "label": 0
                },
                {
                    "sent": "OK, so now when we walked up to player I on day T and we force it to switch to this optimal strategy, Sri for all we know it's cost drops.",
                    "label": 0
                },
                {
                    "sent": "OK, that can't happen if it was an equilibrium, but with another great sequence on some arbitrary day we forced it to switch for all we know, we just improved it.",
                    "label": 0
                },
                {
                    "sent": "Its cost dropped.",
                    "label": 0
                },
                {
                    "sent": "So let Delta denote the magnitude of that improvement, OK?",
                    "label": 0
                },
                {
                    "sent": "So how much it's cost decreased by forcing player I to switch to estar ion Daty.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is just by by notation that equality holds.",
                    "label": 0
                },
                {
                    "sent": "So now we're going to do is we're going to separate these terms.",
                    "label": 0
                },
                {
                    "sent": "We're going to use smoothness to control the entangled terms, and no regret to control the Deltas.",
                    "label": 0
                },
                {
                    "sent": "Remember I made it.",
                    "label": 0
                },
                {
                    "sent": "I made a point to that when I define smoothness, it seemed like overkill.",
                    "label": 0
                },
                {
                    "sent": "We only needed this disentanglement to hold for equilibrium S, and yet I asserted it held for every single outcome.",
                    "label": 0
                },
                {
                    "sent": "Yes, not just equilibrium.",
                    "label": 0
                },
                {
                    "sent": "Well, now that's looking for two.",
                    "label": 0
                },
                {
                    "sent": "It is 'cause, again, arbitrary ESAB teas are not equilibrium their arbitrary outcomes, so it's really good that we can disentangle arbitrary outcomes, so that's what's happening here.",
                    "label": 0
                },
                {
                    "sent": "Smoothness says we can take this entangled term and upper bounded by the suitable linear combination.",
                    "label": 0
                },
                {
                    "sent": "Of the optimal cost and the cost in the sequence on day T, here I've just grouped the Delta terms.",
                    "label": 0
                },
                {
                    "sent": "Now what I want you to do for the error terms, I want to sum first over players and then the inner sum over days T. So fixed player I if we fix player I what is the meaning of this inner sum?",
                    "label": 0
                },
                {
                    "sent": "The meaning of this inner sum says, suppose you went up to player I and forced it to switch to Sri every single day.",
                    "label": 0
                },
                {
                    "sent": "Delta IT says you force I to switch to optimal strategy on day T. This is just that summed over all days T. So the inner sum is how much better the player would get if you forced it to switch to exactly the same action as star I every single day, and we know the answer to that.",
                    "label": 0
                },
                {
                    "sent": "The player would not be better off if it switched to this fixed action every single day, so each inner sum the deltas can be positive, but each inner sum is non positive.",
                    "label": 0
                },
                {
                    "sent": "OK, so is this something?",
                    "label": 0
                },
                {
                    "sent": "So that's what gives us the bounds.",
                    "label": 0
                },
                {
                    "sent": "So now again we can just rearrange and divide through and the average cost of the number sequence is at most one over Lambda Lambda over 1 minus mu.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So switching gears.",
                    "label": 0
                },
                {
                    "sent": "So now I want to show application or really sort of conceptual contributions of pack style, statistical learning theory and how that's just very recently last year or two.",
                    "label": 0
                },
                {
                    "sent": "I've been starting to shape the work and algorithm theory that's happening on auction design.",
                    "label": 0
                },
                {
                    "sent": "So let me give you sort of a 2 slide crash course on auction theory.",
                    "label": 0
                },
                {
                    "sent": "The good news is, is the takeaway from these two slides is very minimal.",
                    "label": 0
                },
                {
                    "sent": "So what I want you to understand is if you have a common prior.",
                    "label": 0
                },
                {
                    "sent": "So if you have a seller who knows the distribution from which buyers willingness to pay is drawn, everything is solved.",
                    "label": 0
                },
                {
                    "sent": "We know the optimal thing to do, but the optimal thing to do can be pretty complicated, to the point that if you didn't know this prior, if you didn't have this distributional knowledge, it's not clear that you could quickly learn the right thing to do.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's why there's interesting technical questions alright, but let me let me give you a little more detail.",
                    "label": 0
                },
                {
                    "sent": "So in this talk I just want to focus on single item auctions.",
                    "label": 0
                },
                {
                    "sent": "So literally just think about eBay.",
                    "label": 0
                },
                {
                    "sent": "For example, there's one seller they have one thing they want to sell.",
                    "label": 0
                },
                {
                    "sent": "The seller has identified some potential buyers and of them.",
                    "label": 0
                },
                {
                    "sent": "So the jargon here is that of a bitter valuation that just means the most that the bidder would be willing to pay the valuation visa by.",
                    "label": 0
                },
                {
                    "sent": "The valuations are what calls private.",
                    "label": 0
                },
                {
                    "sent": "What does that mean?",
                    "label": 0
                },
                {
                    "sent": "That just means that the seller doesn't already know what the valuation is, right?",
                    "label": 0
                },
                {
                    "sent": "So selling is easy when you know exactly what someone is willing to pay for it.",
                    "label": 0
                },
                {
                    "sent": "You just give him a, take it, or leave it offer at that price or slightly below.",
                    "label": 0
                },
                {
                    "sent": "So there's no knowledge of the valuations, but the traditional approach in economics is, we assume that the seller is aware of a prior distribution from which valuations are drawn, so you don't know the realizations.",
                    "label": 0
                },
                {
                    "sent": "You don't know the vis, but you do know capital F. And then given this knowledge of F. How should you sell this item to maximize your expected revenue, where the expectation is over the randomness in the input over the randomness in the Vi's OK?",
                    "label": 0
                },
                {
                    "sent": "So this is the basic optimal auction design problem.",
                    "label": 0
                },
                {
                    "sent": "They sell a single item to end bidders with valuations drawn from a known distribution to maximize expected revenue.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This problem was completely solved by an economist Roger Myerson, beautiful paper.",
                    "label": 0
                },
                {
                    "sent": "One of the reasons he won the Nobel Prize maybe 5 or 10 years ago.",
                    "label": 0
                },
                {
                    "sent": "Let me first describe the solution in a very simple case where the bidders are symmetric, meaning the valuations are drawn IID from a distribution capital left.",
                    "label": 0
                },
                {
                    "sent": "Go ahead and think of F is like uniform on a suitable suitable support if you like.",
                    "label": 1
                },
                {
                    "sent": "So this amazing result says, despite the fact that the design space here is insanely rich, there's like a million different ways.",
                    "label": 0
                },
                {
                    "sent": "Million crazy protocols you could think to use to try to sell one item to one event.",
                    "label": 0
                },
                {
                    "sent": "Different bidders, actually, you may as well just slap it up on eBay.",
                    "label": 0
                },
                {
                    "sent": "That's optimal over the entire design space.",
                    "label": 0
                },
                {
                    "sent": "Just kind of amazing.",
                    "label": 0
                },
                {
                    "sent": "What do I mean by slap it up on eBay?",
                    "label": 0
                },
                {
                    "sent": "I mean formally, a second price auction with a reserve.",
                    "label": 0
                },
                {
                    "sent": "OK, so first of all, what's the second price auction with no reserve?",
                    "label": 0
                },
                {
                    "sent": "Also known as the Vickrey auction?",
                    "label": 0
                },
                {
                    "sent": "That's just you at you ask for a bid from everybody.",
                    "label": 0
                },
                {
                    "sent": "You give the item to the highest bidder and you charge them the second highest price, while the second highest price.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "What happens when you win something on eBay, right?",
                    "label": 0
                },
                {
                    "sent": "If you build 100 bucks for like some old iPhone, and you win, you're probably not going to pay $100.",
                    "label": 0
                },
                {
                    "sent": "Right, if the second highest bid was only 80, eBay is only going to charge you 80 plus and increment like 81 dollars.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Similarly, art auctions.",
                    "label": 0
                },
                {
                    "sent": "Same thing you wind up paying the second highest price at the end of the day.",
                    "label": 0
                },
                {
                    "sent": "So that's how eBay sell stuff.",
                    "label": 0
                },
                {
                    "sent": "Second price auction.",
                    "label": 0
                },
                {
                    "sent": "What about a reserve?",
                    "label": 0
                },
                {
                    "sent": "This is just what's called an opening bid in eBay, and this is just a way for a seller to try and guarantee themselves a certain amount of money conditioned on a sale.",
                    "label": 0
                },
                {
                    "sent": "So you can set an opening bid to be like $50.",
                    "label": 0
                },
                {
                    "sent": "This means anyone is not willing to pay.",
                    "label": 0
                },
                {
                    "sent": "50 is basically just deleted.",
                    "label": 0
                },
                {
                    "sent": "And then if anybody wins.",
                    "label": 0
                },
                {
                    "sent": "There's certainly going to at least the opening bid of reserve price $50.",
                    "label": 0
                },
                {
                    "sent": "They pay the Max of the opening bid, or the second highest, the reserve price, or the second highest bid, whichever is higher.",
                    "label": 0
                },
                {
                    "sent": "OK, so formally, Myerson theorem says that over the space of all selling procedures for IID bidders, a second price auction with the correctly set reserve price maximizes the sellers expected revenue.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of, you know, really just greatest hit of auction theory.",
                    "label": 0
                },
                {
                    "sent": "It just gives you a theoretically justified, practically useful.",
                    "label": 0
                },
                {
                    "sent": "Optimal solution to this auction design problem?",
                    "label": 1
                },
                {
                    "sent": "OK, but so Myerson.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Did not stop here alright, so more broadly, he thought about the case of heterogeneous bidders.",
                    "label": 0
                },
                {
                    "sent": "OK, where bidders are not on ID.",
                    "label": 0
                },
                {
                    "sent": "But again, we're thinking here of a seller that knows these distributions, knows that bit arise valuation is drawn from its own idiosyncratic distribution F. Sub I.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "Maybe you've been looking at bidding history from these different bidders, so you have some notion of what they're likely to bid.",
                    "label": 0
                },
                {
                    "sent": "Here, theoretically we completely understand what the optimal option is.",
                    "label": 0
                },
                {
                    "sent": "It's not so simple.",
                    "label": 0
                },
                {
                    "sent": "OK, it fits on this slide, but there's really only one takeaway I want you to have this optimal auction, which is the first step in the first step, is the tricky step.",
                    "label": 0
                },
                {
                    "sent": "So what you do in the optimal auction is you again get a bid from every bidder and then for bit awry you transform it to what's called a virtual bid, and there's some funky formula for doing that.",
                    "label": 0
                },
                {
                    "sent": "And this is the formula.",
                    "label": 0
                },
                {
                    "sent": "Once you accept this step, you just awarded to the highest virtual bidder.",
                    "label": 0
                },
                {
                    "sent": "Assuming someone has a positive virtual bid and you charge them the natural analogue of the second highest price, OK, but the only take away I want you to have here is that you are with the ID case.",
                    "label": 0
                },
                {
                    "sent": "The only thing we needed to know was the reserve price.",
                    "label": 1
                },
                {
                    "sent": "The reserve price was the only thing that varied as we vary the distribution here, even to just compute this function, we need to know the CDF and density of the valuation distribution in detail.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the takeaway here.",
                    "label": 0
                },
                {
                    "sent": "Is that the optimal auction is known, but it really abuses the full distributional knowledge OK?",
                    "label": 0
                },
                {
                    "sent": "And this option can be weird in other senses.",
                    "label": 0
                },
                {
                    "sent": "So for example, sometimes you'll have will very often or not very often, but will sometimes award the item to a bitter other than the highest bidder.",
                    "label": 1
                },
                {
                    "sent": "OK, so just 'cause you're the highest bidder doesn't mean you're going to win in this auction, and Moreover, this is unique.",
                    "label": 0
                },
                {
                    "sent": "If you really want to squeeze the last few pennies out in your in your expected revenue, you're really forced to use auctions that look like this.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's what I want to know from auction theory.",
                    "label": 0
                },
                {
                    "sent": "Theoretically we understand it, but when even in a single item auction, when the bidders don't have the same distribution, the optimal auction has detailed distributional dependence.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's a bit of an obstacle when you try to actually apply optimal auction theory.",
                    "label": 0
                },
                {
                    "sent": "And of course with the rise of everything from search engines to eBay, there's been a lot of interest and actually trying to make this operational.",
                    "label": 0
                },
                {
                    "sent": "The problem is, where does the prior come from?",
                    "label": 1
                },
                {
                    "sent": "Especially given that the result of the theory the optimal auction, depends in such a sensitive way about your assumption of the details of this prior.",
                    "label": 1
                },
                {
                    "sent": "So this is going to be describing work with Richard Cole, so let me tell you about our motivation and then how we looked to pack style learning theory for inspiration about how to formalize it.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to know.",
                    "label": 0
                },
                {
                    "sent": "Do optimal auctions really have to be so complicated?",
                    "label": 0
                },
                {
                    "sent": "So I told you it's unique, so if you really want 100% of the expected revenue, sure, I guess you need to do this weird auction.",
                    "label": 0
                },
                {
                    "sent": "What if you want you know 90% or 95% of the best possible expected revenue?",
                    "label": 0
                },
                {
                    "sent": "Does it still have to be really weird, or can Alternatively, can you have a much simpler auction which even theoretically is probably almost as good as the optimal auction?",
                    "label": 0
                },
                {
                    "sent": "This was the high level question that we wanted to understand.",
                    "label": 0
                },
                {
                    "sent": "Now, conceptually, there's an issue.",
                    "label": 0
                },
                {
                    "sent": "How do you set up a mathematical model which is going to give you information about this question?",
                    "label": 0
                },
                {
                    "sent": "And of course, the issue is you know what do we mean by you know, requires detailed distributional knowledge.",
                    "label": 0
                },
                {
                    "sent": "That suggests we need to somehow parameterize the amount of knowledge that the seller has.",
                    "label": 0
                },
                {
                    "sent": "So our proposal is to parameterise seller knowledge through samples.",
                    "label": 0
                },
                {
                    "sent": "So we'll still posit that there exists these underlying distributions.",
                    "label": 0
                },
                {
                    "sent": "F1 through FN again bitter eyes, willingness to pay is drawn from FC by independently.",
                    "label": 0
                },
                {
                    "sent": "But the seller does not know these distributions, except inasmuch as it knows S independent samples for them.",
                    "label": 0
                },
                {
                    "sent": "So obviously the bigger S is in some sense the more you know about these distributions.",
                    "label": 0
                },
                {
                    "sent": "So that's how we parameterize the knowledge you have about the environment about the distributions.",
                    "label": 0
                },
                {
                    "sent": "And now the question is, how much data is necessary and sufficient to do almost as well as if you had full distributional knowledge.",
                    "label": 0
                },
                {
                    "sent": "So our benchmark, the Holy Grail is to get expected revenue as large as if someone just literally wrote down F1 through FN for us, and we did the optimal auction.",
                    "label": 0
                },
                {
                    "sent": "OK, that's as good as we could get.",
                    "label": 0
                },
                {
                    "sent": "How big do we have to take S?",
                    "label": 0
                },
                {
                    "sent": "How many samples do we need before we can come up with an auction learning auction which is almost as good as that benchmark?",
                    "label": 0
                },
                {
                    "sent": "That's the question.",
                    "label": 0
                },
                {
                    "sent": "And one thing we found attractive about this idea, in addition to just, you know.",
                    "label": 0
                },
                {
                    "sent": "Pack style learning theory tells us that often you get really nice theory if you sort of set things up this way.",
                    "label": 0
                },
                {
                    "sent": "It's also you know when people have struggled to make optimal auction theory operational.",
                    "label": 0
                },
                {
                    "sent": "This is sort of what they did, so there's a nice paper by ostroski in shorts.",
                    "label": 0
                },
                {
                    "sent": "They were contacted by Yahoo to improve revenue in their keyword search auctions, and this is basically exactly what they did, so they obviously had dreams of bidding data from the bidding data.",
                    "label": 0
                },
                {
                    "sent": "They backed out kind of empirical estimates of what the valuation distributions looked like, and then, given those estimates of valuation distributions, they applied.",
                    "label": 0
                },
                {
                    "sent": "The optimal auction theory.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to make sure the formalism is clear, I'll go through this.",
                    "label": 0
                },
                {
                    "sent": "I'll go through this example with a single buyer, so we always just had one seller.",
                    "label": 0
                },
                {
                    "sent": "We always only had one item now just temporarily let's imagine there's only one potential buyer.",
                    "label": 0
                },
                {
                    "sent": "OK, so now the auction space is no longer rich now the auction space is very simple.",
                    "label": 0
                },
                {
                    "sent": "Basically you just give this person to take it or leave it offer at some at some dollar amount.",
                    "label": 0
                },
                {
                    "sent": "50 bucks whatever.",
                    "label": 0
                },
                {
                    "sent": "OK, so the formalism would now say in a training phase if you like, I give you S samples.",
                    "label": 0
                },
                {
                    "sent": "These are now just single numbers from a distribution capital F. Again, you don't know F, you only know V1 through.",
                    "label": 0
                },
                {
                    "sent": "VS as a function of the samples, you pick a price.",
                    "label": 0
                },
                {
                    "sent": "OK, by whatever means, and then I'm going to evaluate the price by applying it to a fresh sample from the same distribution tomorrow.",
                    "label": 0
                },
                {
                    "sent": "OK, what's the best you can hope for?",
                    "label": 0
                },
                {
                    "sent": "The best you can hope for is that you computed the so called monopoly price.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's just the price that maximizes the selling price times the fraction of sale.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you knew the distribution capital F. This is the amount of expected revenue that you would get.",
                    "label": 0
                },
                {
                    "sent": "And the goal is to choose a function P so that your expected revenue over both the samples.",
                    "label": 0
                },
                {
                    "sent": "So both randomness in both the training set and the test set.",
                    "label": 0
                },
                {
                    "sent": "If you like your expected revenue is close to this best possible benchmark.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So on this slide I'm just going to state a couple results for the single buyer case, but these are not the main points of this case study, so this is just to calibrate our expectations of what we might hope to be true for the general multibuy or single item auction case alright?",
                    "label": 0
                },
                {
                    "sent": "So first of all, if we assume nothing about capital F, you can't make any statements whatsoever.",
                    "label": 0
                },
                {
                    "sent": "OK, 'cause for all we know, capital F is zero almost all the time and there's this tiny probability much smaller than 1 / S that it's huge.",
                    "label": 0
                },
                {
                    "sent": "OK, so on pathological distributions like that, you really can't say anything interesting about learning from samples, so you need some kind of condition.",
                    "label": 0
                },
                {
                    "sent": "OK, and it turns out kind of all of the various conditions that you might hope we work lead to positive results, and I'm not going to numerate them OK.",
                    "label": 0
                },
                {
                    "sent": "But so for example, in auction theory there's a notion of regular distributions.",
                    "label": 0
                },
                {
                    "sent": "Saying the tails are no heavier than that of a power law distribution and there already you get positive results.",
                    "label": 0
                },
                {
                    "sent": "What do I mean by positive results?",
                    "label": 0
                },
                {
                    "sent": "Well, actually, amazingly so.",
                    "label": 0
                },
                {
                    "sent": "As a consequence of a famous auction theory result called Beaulah Klemperer theorem.",
                    "label": 0
                },
                {
                    "sent": "Even just give you 1 sample.",
                    "label": 0
                },
                {
                    "sent": "You can already say something nontrivial for these so called regular or log concave distributions.",
                    "label": 0
                },
                {
                    "sent": "OK, so clearly with one sample you have learned very little about Capital F, right?",
                    "label": 0
                },
                {
                    "sent": "So most notions of learning a distribution would not be satisfied with one sample, yet you cannot actually already prove some kind of revenue guarantee.",
                    "label": 0
                },
                {
                    "sent": "So in fact, if you just had one pass transaction and you use the bid from yesterday as the price for tomorrow, that already gives you a factor 2 approximation of the optimal revenue.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's kind of a.",
                    "label": 0
                },
                {
                    "sent": "This shows you kind of runs on to something and also suggests that perhaps approximating revenue can be strictly easier than learning in the sentence that we're familiar with, so that's the first thing you get off the ground, even with one sample.",
                    "label": 0
                },
                {
                    "sent": "But what if you wanted a much better than 50% approximation?",
                    "label": 0
                },
                {
                    "sent": "What do you want?",
                    "label": 0
                },
                {
                    "sent": "90% or 99%?",
                    "label": 0
                },
                {
                    "sent": "Well, the number of samples you're going to need, of course, will scale with one over epsilon if you're shooting for one minus epsilon approximation, and the good news is, it scales as a polynomial.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is joint work.",
                    "label": 0
                },
                {
                    "sent": "With this, I'm unsure who I saw here and also jiwang.",
                    "label": 0
                },
                {
                    "sent": "So as a function of epsilon, we understand the sample complexity when there's one buyer and it's Poly in one over epsilon, and the stronger the conditions making distributions, the better the sample complexity, but.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then I state this just is kind of a warm up for the single item case single item auction case where you have multiple buyers.",
                    "label": 0
                },
                {
                    "sent": "So the best case scenario with multiple buyers is that our positive results in this special case continue to hold.",
                    "label": 0
                },
                {
                    "sent": "Right, so we're kind of rooting for right that the sample complexity remains just too small.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the multi by our case, so the multibar formalism is what you'd expect.",
                    "label": 0
                },
                {
                    "sent": "So now a sample is not just one number but end numbers evaluation per bitter as a function of samples.",
                    "label": 0
                },
                {
                    "sent": "You pick single item auction and we evaluate your choice of this function from samples to auctions by the expected revenue of your chosen auction.",
                    "label": 0
                },
                {
                    "sent": "Again, the expectation of hers over both the samples which governs what auction is.",
                    "label": 0
                },
                {
                    "sent": "Then run and also over the bidders on which that auction is run.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So some of the positive results continue to hold.",
                    "label": 0
                },
                {
                    "sent": "So if all you want is a very coarse approximation of constant factor approximation, again actually one samples enough.",
                    "label": 0
                },
                {
                    "sent": "If you use yesterday's bids as tomorrow's prices, you're within, uh, some not so nice constant factor of an optimal solution.",
                    "label": 0
                },
                {
                    "sent": "But again, what if you want a good approximation, 90 percent, 99%?",
                    "label": 0
                },
                {
                    "sent": "So the good news is if the bidders were homogeneous, symmetric if they're IID valuations, then actually the sample complexity doesn't scale within at all.",
                    "label": 0
                },
                {
                    "sent": "OK remains just Poly in one over epsilon.",
                    "label": 0
                },
                {
                    "sent": "No matter how many bidders you have, OK.",
                    "label": 0
                },
                {
                    "sent": "In hindsight, actually, that sort of totally makes sense.",
                    "label": 0
                },
                {
                    "sent": "'cause what did we learn from Myerson in the ideal case ruling that actually optimal auction is super simple?",
                    "label": 0
                },
                {
                    "sent": "It's always a second price auction.",
                    "label": 0
                },
                {
                    "sent": "You just gotta set the reserve price correctly.",
                    "label": 0
                },
                {
                    "sent": "So you really just need to know this one statistic about the distribution.",
                    "label": 0
                },
                {
                    "sent": "It doesn't depend on the number of bidders, so it's what you hope to have that the sample complexity and the ID case remains constant.",
                    "label": 0
                },
                {
                    "sent": "So it was less.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Curtis, our priority is that if you relax any or if you relax the assumption, you go to heterogeneous bidders.",
                    "label": 0
                },
                {
                    "sent": "Actually, for the first time we get a non trivial lower bound on how many samples are needed to learn a near optimal auction.",
                    "label": 0
                },
                {
                    "sent": "OK so unlike the case where you only want to constant approximation and you need 1 sample, unlike the case where it's symmetric and you only need a Poly one over epsilon samples, if their heterogeneous and you really want to get approximation the sample complexity meaning the number of samples S necessary to get a 1 minus epsilon approximation.",
                    "label": 0
                },
                {
                    "sent": "Actually, scales with the number of bidders OK, and when I say this just to keep it in mind, a sample already has a numbers.",
                    "label": 0
                },
                {
                    "sent": "So I'm really saying you need end samples, each of which is an end vector.",
                    "label": 0
                },
                {
                    "sent": "OK, so you need N squared numbers to learn a good approximation of the optimal auction, so this is sort of a formalization of something that you know people who work in pricing new, which is that when your community is heterogeneous, you can really get big wins, often by resorting to more complex pricing structures, and in fact morally what this proof really says.",
                    "label": 0
                },
                {
                    "sent": "To answer the original motivating question, we said Dear, do near optimal auctions have to be complex, at least in this setup.",
                    "label": 0
                },
                {
                    "sent": "The answer is yes.",
                    "label": 0
                },
                {
                    "sent": "So our proof really shows that if your learning algorithm, whatever it is, no matter how smart it is, if it fails to learn these virtual valuation functions, these funky 5 functions at the very top of the distribution, if it fails to have a very accurate model of those functions, it cannot be a 1 minus epsilon approximation for small epsilon.",
                    "label": 0
                },
                {
                    "sent": "OK, there's really just nothing I can do.",
                    "label": 0
                },
                {
                    "sent": "Without that information about the tips of those virtual valuation distributions and so and that as a consequence gives you the sample complexity 'cause you need lots of samples before you can actually accurately learn these functions at the very top rare event part of those distributions.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So I rushed through that a little bit.",
                    "label": 0
                },
                {
                    "sent": "I'm happy to take questions at the end, but I want to share a time to mention some recent developments and some open questions.",
                    "label": 0
                },
                {
                    "sent": "So last couple of slides.",
                    "label": 0
                },
                {
                    "sent": "So the formalism I just showed you, I think the inspiration that we took from pack style learning theory was obvious.",
                    "label": 0
                },
                {
                    "sent": "I hope it was.",
                    "label": 0
                },
                {
                    "sent": "That said, we kind of developed everything from scratch both on the upper bound side and on the lower bound side.",
                    "label": 0
                },
                {
                    "sent": "So this is recent work with Jamie Morgenstern.",
                    "label": 0
                },
                {
                    "sent": "We just posted this in archive a few weeks ago, so here we're trying to make the connections to statistical learning theory very explicit and really sort of stand on the shoulders of the mature understanding and statistical learning theory to prove new results about auctions.",
                    "label": 0
                },
                {
                    "sent": "And so here what we're doing is we're saying think about some set of auctions, auctions, which are, maybe, you know, simple.",
                    "label": 0
                },
                {
                    "sent": "The auctions you could imagine actually bringing.",
                    "label": 0
                },
                {
                    "sent": "You're bringing to your boss and saying let's try this.",
                    "label": 0
                },
                {
                    "sent": "OK, so auctions with a few knobs.",
                    "label": 0
                },
                {
                    "sent": "So I want to think of these auctions, this Etsy of auctions is like being a hypothesis class and I want to think of junction is a real valued function.",
                    "label": 0
                },
                {
                    "sent": "In what sense when auction takes as input a valuation profile or a bid profile and it makes some amount of money.",
                    "label": 0
                },
                {
                    "sent": "OK, on this bid profile so it's a real valued function, so a set of auctions is a set of real valued functions.",
                    "label": 0
                },
                {
                    "sent": "Given an unknown distribution on valuation profiles and what people want.",
                    "label": 0
                },
                {
                    "sent": "Well, you know, figuring out which of these auctions has the highest expected revenue reduces to having a uniform approximation of the revenue of each one of these auctions with respect to the distribution.",
                    "label": 0
                },
                {
                    "sent": "So learning the best auction reduces in the usual sense to uniform estimation of all of these real valued functions.",
                    "label": 0
                },
                {
                    "sent": "Therefore, if you can compute standard complexity measures of this space of real valued functions like the pseudo dimension, you can just get for free using standard techniques developed in this Community.",
                    "label": 0
                },
                {
                    "sent": "Bounds on the sample complexity and so that's what we did.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we did is we showed that we develop some understanding about which classes of auctions have high versus low pseudo dimension and the punch line.",
                    "label": 0
                },
                {
                    "sent": "So the good news is that for a very wide range of settings so much broader than just single item auctions, we showed that you can get sort of the best of both worlds.",
                    "label": 0
                },
                {
                    "sent": "Well, those complex auctions I told you bout back in Myerson theorem, if we look at all options in the world, the pseudo mention is very pseudodementia is very high, but it's possible to choose a sweet spot set of options C. Which on the one hand is simple enough that you can learn the optimal auction from the class.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, so the class has low suitor dimension, yet on the other hand, it's expressive enough that it has low representation error, meaning whatever the unknown distributions F1 through FN might be.",
                    "label": 0
                },
                {
                    "sent": "And whatever the weird optimal auction is for F1 through FN, there exists an auction in this Class C that has expected revenue almost as good as a Softail auction 1 minus epsilon times this optimal auction.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the punch line then is because there exists these sets.",
                    "label": 0
                },
                {
                    "sent": "See that have simultaneously low representation error and low learning error.",
                    "label": 0
                },
                {
                    "sent": "It means very broadly from a polynomial number of samples.",
                    "label": 0
                },
                {
                    "sent": "This is now polynomial in N also, so it doesn't contradict the lower bound you saw in a polynomial number of samples.",
                    "label": 0
                },
                {
                    "sent": "You can learn a near optimal auction.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of again just sort of what's going on now in the age.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The community.",
                    "label": 0
                },
                {
                    "sent": "So hopefully I've illustrated a couple of they just real gifts that learning theory is already given.",
                    "label": 0
                },
                {
                    "sent": "The algorithm game theory, and really, you know it's played a really some of these concepts have played a core role in, you know what's currently hot in a GT, so I hope I gave you some sense for that.",
                    "label": 0
                },
                {
                    "sent": "I think it's uncontroversial to predict there will be many more points of contact.",
                    "label": 0
                },
                {
                    "sent": "I really hope to see you know papers and cold for years to come in some of these areas and on new points of contact just to leave you with a couple of concrete open directions.",
                    "label": 0
                },
                {
                    "sent": "Some of these we already touched on in the questions, so let me just point out the last couple things I was telling you about sample complexity options.",
                    "label": 0
                },
                {
                    "sent": "I think there's I mean, it's so nascent, it's not like there's some you know, hard open math problem.",
                    "label": 0
                },
                {
                    "sent": "There's just really vast terrains that haven't been explored almost at all, so I think it's really fertile ground to be working in.",
                    "label": 0
                },
                {
                    "sent": "So first of all, the results I mentioned in the last slide with Morganstern.",
                    "label": 0
                },
                {
                    "sent": "They're only information theoretic.",
                    "label": 0
                },
                {
                    "sent": "OK, so our learning algorithms are not efficient, they're just totally generic.",
                    "label": 0
                },
                {
                    "sent": "Erm, type learning algorithms, so we're quite pleased with the sample complexity bounds that we had in the settings that we have.",
                    "label": 1
                },
                {
                    "sent": "But we really know very little.",
                    "label": 0
                },
                {
                    "sent": "About computationally efficient algorithms for learning near optimal auctions.",
                    "label": 0
                },
                {
                    "sent": "So in the paper with cold that I mentioned, our algorithm is sufficient for the single item case, but again, that's a pretty limited setting, and so I'd really like to understand how broadly you can have computationally efficient learning algorithms in this space.",
                    "label": 0
                },
                {
                    "sent": "One major assumption that we made that was pointed out is we were thinking about full feedback, so we were taking the perspective of eBay or a search engine who really knows what everybody bid on all of these past transactions.",
                    "label": 0
                },
                {
                    "sent": "Then it's a good assumption.",
                    "label": 0
                },
                {
                    "sent": "You really do have this information, but if you're a third party just sort of observing what's happening on eBay or on a search engine, you probably don't know the bid of every person in the auction.",
                    "label": 0
                },
                {
                    "sent": "You know, maybe you know who won.",
                    "label": 0
                },
                {
                    "sent": "Maybe you know what the winner bid, or probably you just know what the winner paid, which maybe is going to be something more like the second highest bid.",
                    "label": 0
                },
                {
                    "sent": "So you can think both about sort of bandit settings, but there's also these sort of weird hybrid partial information settings which really model the real world problems in a lot of cases, and you know there's been some partial work on those settings, including from this community, but I think it's safe to say there's a lot more to be done and then, as was pointed out by a couple of people like like Sham, I was just assuming that you get these samples for free.",
                    "label": 0
                },
                {
                    "sent": "OK, that the bidders in the previous auctions.",
                    "label": 0
                },
                {
                    "sent": "Didn't know you were going to use their data set prices in the future and we're reacting accordingly.",
                    "label": 0
                },
                {
                    "sent": "This again, there's some partial results, including a paper by Constance talk like this, along with cayenne pepper in the next session.",
                    "label": 0
                },
                {
                    "sent": "But again, very unexplored.",
                    "label": 0
                },
                {
                    "sent": "Lots more to do, so I'll stop there, thanks.",
                    "label": 0
                }
            ]
        }
    }
}