{
    "id": "6mxzaef5nv3eydiizzhg4rfbf2vturws",
    "title": "Boilerplate Detection Using Shallow Text Features",
    "info": {
        "author": [
            "Christian Kohlsch\u00fctter, L3S Research Center, Leibniz University of Hannover"
        ],
        "published": "Oct. 7, 2010",
        "recorded": "February 2010",
        "category": [
            "Top->Computer Science->Web Mining"
        ]
    },
    "url": "http://videolectures.net/wsdm2010_kohlschutter_bdu/",
    "segmentation": [
        [
            "Boiler plate."
        ],
        [
            "Section what is boilerplate?"
        ],
        [
            "Actually, you know it when you see it.",
            "So boilerplate is the mostly text on a web page that is not the actual main content, but the surrounding information.",
            "Kind of clutter navigation elements, related things, but it's not the main content.",
            "And usually if people go to the website, they go to the website because of the main content and not because of the boiler plate.",
            "So it's a really important task to first of all detect the boilerplate and eventually remove it."
        ],
        [
            "So once we have removed it, we have."
        ],
        [
            "First one to also only get a text.",
            "The main content basically, and this is what we want to have.",
            "But but the computer algorithms."
        ],
        [
            "BC initially is HTML code, so the question is called.",
            "How can we from this representation very efficiently to the representation of the ASCII full text?"
        ],
        [
            "And there are several existing approaches, of course.",
            "Generally you have to differentiate between machine learning and heuristically approaches their site specific solutions.",
            "You can train as long as you have sufficient data from one side.",
            "You can always train it according to that site, but of course it's not scalable to web scale.",
            "They're wishing based methods.",
            "Basically you render the page and then you do some analysis over there, but it's of course really costly because rendering is cannot be done in milliseconds.",
            "Let's say also you're not in bed scale if you're running a crawl over.",
            "Indexing system.",
            "Um, then you couldn't have approaches that are based on token level or engrams, and they are shallow text features and contextual features."
        ],
        [
            "So what are shallow text features?",
            "Shadow text features?",
            "Something that is not really dependent on a particular side, not even on a particular language.",
            "What you can do is just count.",
            "Right, so we examine the document on a very shallow level.",
            "That's why, and we have, for instance numbers like words and tokens that in a particular block lock can be defined as any.",
            "Well, text that is separated by HTML block level elements, so they're extremely block level elements in the inland elements in the block level, elements are creating new blocks.",
            "As an example example above, the first block would be Hello World and the second block would be.",
            "This is a test.",
            "Why don't we just count the number of words that are contained in such a block or the tokens?",
            "You can take averages like the average length of such tokens, or the sentences containing such a block.",
            "You can take ratios like the number uppercase words versus all the words number, fullstops and so on.",
            "You can actually look at the classes of HTML tags like so if there's a. Paragraph headlines is whatever.",
            "And then you can look at densities.",
            "Maybe you know one density already gets.",
            "In other words, it's called Anchor text percentage.",
            "That's the number of words or tokens that are contained in the Lincoln.",
            "This example test versus the number of tokens overall with block.",
            "So it's in this case it's one out of four words.",
            "It's linked to the Anchor text, percentage is 25%.",
            "And another density we've introduced in."
        ],
        [
            "Previous work at your Cam and update up.",
            "That's text density.",
            "Basically the idea comes from the area.",
            "Computer vision where we say that some areas on the actually rendered page appear more dense than other areas.",
            "So if you take a full text apparently looks more black than areas that are only consisting of a few words like a navigation.",
            "So how can we from this vendor representation, so a similar representation if we only have a string?",
            "Well, what we could do is we take the string and wrap it at a certain fixed line with for instance 80 characters that you know turn out to be a good limit.",
            "And well, you Rapidan then if the text doesn't fit in one line, you rap it to the next line, and so on.",
            "And if there's nothing to wrap well, you take just the next block and then you compute the density of such a block.",
            "Essentially by counting the number of tokens in the block divided by the number left lines.",
            "So it's a really easy measure, and Interestingly, it doesn't depend on the length of the whole block, because the average OK."
        ],
        [
            "And there are contextual features.",
            "Context can be seen as an intra document context, so you if you have a block need then you can look at the position of the block.",
            "You can also look at the overall features of the preceding and following blog.",
            "But you can also look at context Inter document level.",
            "So if you have information about.",
            "Other documents from from the same host of well from the from the same language corpus.",
            "Maybe then it would make sense to count frequent blocks or account locks and see how frequent they are.",
            "So for instance, on our website it also.",
            "Yes we have always this boiler plate at the bottom.",
            "That's a Copyright line with our address, and even though it appears to be dense regarding to uh, according to our text density measure, it's really frequent, so probably it doesn't really contribute contribute to the main content.",
            "Right, so this into document level, but actually the question is can we avoid collecting this information?",
            "So the approach I'm presenting here is actually local approach.",
            "So given a website, give me full fix, that's it.",
            "So it's possibly really fast because we and also memory efficient because we don't have this sort of information in advance."
        ],
        [
            "We're doing we're three experiments.",
            "First of all, classification accuracy.",
            "So, given a block, can you classify it as boilerplate and content?",
            "And maybe some other classes like headline, supplemental image captions, whatever.",
            "Of course we can apply machine learning techniques with a simple.",
            "What we're doing is decision trees and support vector machines tenfold proselytization, and we have relating with several measures, are represented here.",
            "The F measure and Roc area under court results.",
            "Once we have this, we can say OK even I define what the main content of page is, how difficult it is to extract this.",
            "And here we are comparing the different state of the art approaches like.",
            "Old and established one.",
            "It's BTE body text extraction, which essentially takes the very largest block that has the least amount of tags versus the maximum amount of words that it could find.",
            "We also looking at engram approaches.",
            "Here we show the approach from pasternack from last year.",
            "And in the paper there are other approaches shown as well.",
            "Um and then third, if we have removed the main content, it might be likely that we get a higher ranking right?",
            "So if we remove all this stuff and we're looking at our backs of words level at the document, how much can we improve over the baseline and how much can improve over existing algorithms and measuring this by precision at 10 or the increase on precision intent?",
            "And on any such attempt?",
            "And for this we actually employing standard datasets with accessories and track data collection.",
            "Using is a blocker, 6 corpus with three numbers."
        ],
        [
            "For the classification.",
            "We had to find actually data set that is well annotated enough to the dollar to suit our needs and what we did is actually we were sampling Google News for a certain amount of time, like several months and at the end we had more than 250,000 pages from English Google websites.",
            "And.",
            "These are actually quite representative, I think because so we have many different sites in this corpus, not just a few sites, but really many and from this big corpus we randomly sample more than 600 newsarticles and this we annotated manually in the browser.",
            "So we set up something that users could really easy annotated with my colleagues and me and what we did is we selected particular areas of the text as headline full text.",
            "Image captions or supplemental.",
            "Also, user comments.",
            "And even related information and what we've not selected, we regarded boilerplate.",
            "So what I'm showing here two different results.",
            "First of all, on the bottom you see the classification results for headline full take supplemental user comments and related content separately.",
            "In my presentation, I will be focusing on the difference only between boilerplate and any content below.",
            "For this classification problem.",
            "The paper for all the other other classes, but what you see here is actually that depending on how you look at the data, just the number of blocks, the number of words, or any tokens in the blocks that actually on the block level, the boilerplate is dominating.",
            "So there are lots of blocks that actually considered non relevant for the main content and even on words level, it's still a third of all the tokens.",
            "And talking level even more.",
            "And all the other classes can see are quite minor if you look at article full text and that's quite a large amount of the data and headline supplemented user commented content actually don't contribute so much for this information.",
            "So that's why we actually this presentation.",
            "Concentrate on the two class problem."
        ],
        [
            "Now what we can do is now with all the features that I presented earlier, we put this into a machine learning classifier or set up different classifiers with different features.",
            "Well, you find information gain see which features would perform best and well, just see how far we get this slide shows you several results, so they're more in the paper.",
            "And this shows you actually if one method as well as Roc area under course.",
            "Oh well, it's on the left.",
            "And it also shows you where the decision trees, how many leaves.",
            "These decision trees have and how many features we actually using to perform this classification.",
            "I'm the first role.",
            "Did you see is actually the baseline, so it always predicts content and it's, well, it's around 50% of resources weighted according to the presence of the classes in the corpus, less than 50%.",
            "And what you can actually see is that most of the features already perform quite well.",
            "For instance, what you what you can see here is that if you only take the number of words.",
            "Like anything more than 15 words already has an accuracy of 86%.",
            "And then you will add more features to it.",
            "More local context to the page and so on.",
            "And you're getting better.",
            "But at the end, if you take all the features plus all the local features, plastic, global frequency, you kind of really improve over there.",
            "And I wanted."
        ],
        [
            "Attention to three of these lines.",
            "Actually, if we just take the number of words.",
            "And the link density.",
            "So there is the well I'm out of words that are LinkedIn A blog, but all the numbers we already within F measure at 92% and Roc area under curve 95% quite close to the optimal solution and the cool thing here is that we actually don't have much features, so we actually only having two types of features and then we looked at the current block, the previous and the next block and that's it.",
            "So it's already really good here."
        ],
        [
            "And text entities almost the same.",
            "It's a little bit better for both values, rock area and the little bit even more, but actually it doesn't really matter.",
            "So counting words that makes sense that he seems to be."
        ],
        [
            "Equally good.",
            "And if you're taking all the features that all the local features together, we can even improve rock up to 98.1%.",
            "Of course, then you need more features, and it's questionable.",
            "What features really contributed the most to the specification?",
            "But these numbers are really impressive, I think already."
        ],
        [
            "So."
        ],
        [
            "Main content extraction problem.",
            "Um again.",
            "Humane contravenes headline and full text and image captions will be ignored.",
            "Here was the comments.",
            "So in the news domain you have comments regarding an article and actually we were not regarding.",
            "This is the main content so these results slightly different from the results before and what we did.",
            "Here was the average and medium token level F measure comparison of all the documents that we have in our corpus and we again start with the baseline.",
            "That means keeping everything seems to be good already.",
            "Like average 68% in the median 70%."
        ],
        [
            "If you take, for instance, the approach from pasternack from that of last year, you can improve of course, that 78% versus median 87%."
        ],
        [
            "But surprisingly, if you just eat everything with at least 10 words to block, you already better than any ground flamed approach.",
            "Let's actually surprise as well."
        ],
        [
            "On the well, if you take BTEC or the body text extraction approach from film from 2001.",
            "Well already.",
            "Quite better, that's 89% versus 96."
        ],
        [
            "Median and still if you take the densitometric classifiers, take textonly density, you can further improve.",
            "Especially at the details of the most of the the documents are classified in the same, but later on you can improve."
        ],
        [
            "The number of words classifier slightly improves over this, but it actually has the same features.",
            "And then of course, if you would take the concept from ET to our classifier saying that it is probably one block that is important.",
            "So we actually with this specified we're classifying any block that satisfies our criteria.",
            "But if we only take the largest block."
        ],
        [
            "Then we can.",
            "Then we can actually improve even further.",
            "Show me now.",
            "Yes, so then we actually had 97% of median.",
            "And in and use American.",
            "Also do is, we can say, well, the main content starts with the headline.",
            "So if we know that the headline is somewhere, it's the text of the title tag.",
            "And if you find some common things, so it's really now heuristic that we're adding.",
            "If there are comments below, we are."
        ],
        [
            "America strip them.",
            "You can even further improve it so you risztics can actually improve.",
            "But the simple classifier presented presented you few minutes ago already outperforms the existing algorithms."
        ],
        [
            "Now, having a look at the classifiers that we were using, both the numbers and link density classifier.",
            "What we can see is actually they're quite similar, simple and similar.",
            "If you look at the distributions, the number of words and the classifier over on the left and on the right you see a difference and the difference is the articles of course have a different number of words on average, right?",
            "So the number of words is not really limited.",
            "So actually the classifier can detect this, but you can't see this here, it's just a skewed distribution between.",
            "Non content words and the content words.",
            "So that's why the deposition so well.",
            "So so so empty that's able to see there's a really broad distribution for the for the content text over words with at least let's say 10 words and more and more than 50 words.",
            "If you look at the text density as I said, this is a bounded measure.",
            "You have a much nicer histogram, and if you look at the classification, we sort that we did before.",
            "So we did this menu assessment.",
            "You actually see that the content actually is most of the time in blocks with the text density.",
            "A larger equals than 10 where is the non content is on the left hand side and I also showed you the distribution of links.",
            "Also link text actually seems to have usually a smaller text density and also smaller number of words.",
            "But overall actually these distributions are exposing the same features that we have in the text.",
            "It's just visualized differently."
        ],
        [
            "However, this presentation we can utilize or looking at corpora on a very high level so that you can use corpus on the left and then some other corpora next to it.",
            "For instance, the website UK graph exposes, while the very same properties on text density level.",
            "So you can basically even without a menu annotation already see that with high probability left mode of this distribution is probably the boilerplate level.",
            "Yes, the boilerplate text.",
            "On the right hand side with higher tax densities.",
            "Expect to have the full content.",
            "Same applies for the blocks or six collection on the inside, and it even applies on single document level.",
            "So I just took a random page from the web about com York City travel and expose this distribution.",
            "Interesting Lee, if you take let's say the HTML version of my.",
            "Paper here so I can go to the state Gmail, but it's no boilerplate well.",
            "So we actually can can use this for visualizing what has already been there on the level of number of words.",
            "Now what does that mean?",
            "We have found it classified performs quite well, but is still kind of a model behind it and we can can further utilize to undo."
        ],
        [
            "And what's going on at the site?",
            "And actually, I think we can, so let's start with a really simple approach how we think people could write pages or generative model for the creation of process of text on web pages.",
            "If you start with a really simple Shannon writer, so monkey typing text in and at some point pressing return to create a new block, this is what we would get.",
            "So with some probability the market starts typing.",
            "In this case it all he always starts typing and it with some probability keeps typing or he goes to the next lock and writes in his blog, right?",
            "And the state continues to go like this.",
            "Essentially this is super new trial, so the transition to the next block is success P and emission of another word is failure 1 -- P. Since we already if we go from one state to another, we're meeting is a word or\nCharacter.",
            "We are not using.",
            "This is actually then simple geometric distribution, but the geometric distribution that is displaced by one and you can see, here's a really simple distribution, of course is not really modeling the natural language.",
            "Let's say I give what's a good starting point, especially when you look at corpus level, right?",
            "Because the modifier sticks, you have the more unlikely that you get a really sharp, complex distribution."
        ],
        [
            "Now let's extend this model.",
            "Now, do you say OK?",
            "We have full text and we have.",
            "Boilerplate text that we already know that the more words right and more likely to sell full text or main content, and the less words you have, the more likely it is video boilerplate.",
            "We can model this as a stratified Shannon.",
            "Random wider, saying that at some point you decide whether you want to go and write long text, and then you continue writing long text and go back to write a new blog, or you decide to write short text text with fewer words, and then you continue providing this in the back, and so on and so forth.",
            "And then we of course expected probability of writing.",
            "Uh, convince long texts of continuing to write long text must be higher than to write short text.",
            "And if you actually look at this model."
        ],
        [
            "Um the.",
            "Goodness of the fit is increasing, of course, and the root mean square error is actually half, so this is a much better model to what's going on."
        ],
        [
            "And if you look at the values that we could actually get, it's also explaining the situation quite well.",
            "So on average, what you can see is that the long text is actually let's say, 23.8 words and the short text is 3.5 words, so that we can get from this information.",
            "I thought we can all."
        ],
        [
            "To get from the probability of going from a new block to decide that you want to right now short text is 76%.",
            "It is quite well matches what we have from our Google News assessment at 979% of the blocks of boilerplate."
        ],
        [
            "Finally, well, saying that writing long text is good, but we can see if we just remove the text.",
            "The blocks with.",
            "Less than keywords.",
            "Let's see how far we get and how we can can increase the retrieval performance.",
            "And well, actually we did this on this last six days."
        ],
        [
            "Then we found out it's actually that if you remove all the blocks with less or equal in 10 words, you can essentially."
        ],
        [
            "Yeah.",
            "Improve over the baseline by around 150%.",
            "How much account and over the state of the art years before not.",
            "You can even improve 1/3 of 50."
        ],
        [
            "1%.",
            "Slightly confused."
        ],
        [
            "The next creation can be modeled as a stratified stochastic process."
        ],
        [
            "So you get a very high classification expression extraction accuracy at almost no."
        ],
        [
            "As well as an increase of the prior position as it comes down to trust."
        ],
        [
            "Next steps would be to enforce look this look at this data from multilingual perspective.",
            "Use different domain corpora and see how we can improve the model and for you the next step would be to try this code out the program as well as the datasets are available freely online.",
            "The link is."
        ],
        [
            "Paper and if you have any questions now, it's the very best.",
            "Thank you.",
            "Thank you very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Boiler plate.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Section what is boilerplate?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, you know it when you see it.",
                    "label": 0
                },
                {
                    "sent": "So boilerplate is the mostly text on a web page that is not the actual main content, but the surrounding information.",
                    "label": 0
                },
                {
                    "sent": "Kind of clutter navigation elements, related things, but it's not the main content.",
                    "label": 0
                },
                {
                    "sent": "And usually if people go to the website, they go to the website because of the main content and not because of the boiler plate.",
                    "label": 0
                },
                {
                    "sent": "So it's a really important task to first of all detect the boilerplate and eventually remove it.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So once we have removed it, we have.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First one to also only get a text.",
                    "label": 0
                },
                {
                    "sent": "The main content basically, and this is what we want to have.",
                    "label": 0
                },
                {
                    "sent": "But but the computer algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "BC initially is HTML code, so the question is called.",
                    "label": 0
                },
                {
                    "sent": "How can we from this representation very efficiently to the representation of the ASCII full text?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there are several existing approaches, of course.",
                    "label": 1
                },
                {
                    "sent": "Generally you have to differentiate between machine learning and heuristically approaches their site specific solutions.",
                    "label": 0
                },
                {
                    "sent": "You can train as long as you have sufficient data from one side.",
                    "label": 0
                },
                {
                    "sent": "You can always train it according to that site, but of course it's not scalable to web scale.",
                    "label": 0
                },
                {
                    "sent": "They're wishing based methods.",
                    "label": 0
                },
                {
                    "sent": "Basically you render the page and then you do some analysis over there, but it's of course really costly because rendering is cannot be done in milliseconds.",
                    "label": 0
                },
                {
                    "sent": "Let's say also you're not in bed scale if you're running a crawl over.",
                    "label": 0
                },
                {
                    "sent": "Indexing system.",
                    "label": 0
                },
                {
                    "sent": "Um, then you couldn't have approaches that are based on token level or engrams, and they are shallow text features and contextual features.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what are shallow text features?",
                    "label": 1
                },
                {
                    "sent": "Shadow text features?",
                    "label": 0
                },
                {
                    "sent": "Something that is not really dependent on a particular side, not even on a particular language.",
                    "label": 0
                },
                {
                    "sent": "What you can do is just count.",
                    "label": 0
                },
                {
                    "sent": "Right, so we examine the document on a very shallow level.",
                    "label": 0
                },
                {
                    "sent": "That's why, and we have, for instance numbers like words and tokens that in a particular block lock can be defined as any.",
                    "label": 0
                },
                {
                    "sent": "Well, text that is separated by HTML block level elements, so they're extremely block level elements in the inland elements in the block level, elements are creating new blocks.",
                    "label": 0
                },
                {
                    "sent": "As an example example above, the first block would be Hello World and the second block would be.",
                    "label": 0
                },
                {
                    "sent": "This is a test.",
                    "label": 0
                },
                {
                    "sent": "Why don't we just count the number of words that are contained in such a block or the tokens?",
                    "label": 0
                },
                {
                    "sent": "You can take averages like the average length of such tokens, or the sentences containing such a block.",
                    "label": 0
                },
                {
                    "sent": "You can take ratios like the number uppercase words versus all the words number, fullstops and so on.",
                    "label": 0
                },
                {
                    "sent": "You can actually look at the classes of HTML tags like so if there's a. Paragraph headlines is whatever.",
                    "label": 0
                },
                {
                    "sent": "And then you can look at densities.",
                    "label": 0
                },
                {
                    "sent": "Maybe you know one density already gets.",
                    "label": 1
                },
                {
                    "sent": "In other words, it's called Anchor text percentage.",
                    "label": 0
                },
                {
                    "sent": "That's the number of words or tokens that are contained in the Lincoln.",
                    "label": 0
                },
                {
                    "sent": "This example test versus the number of tokens overall with block.",
                    "label": 0
                },
                {
                    "sent": "So it's in this case it's one out of four words.",
                    "label": 0
                },
                {
                    "sent": "It's linked to the Anchor text, percentage is 25%.",
                    "label": 0
                },
                {
                    "sent": "And another density we've introduced in.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Previous work at your Cam and update up.",
                    "label": 0
                },
                {
                    "sent": "That's text density.",
                    "label": 0
                },
                {
                    "sent": "Basically the idea comes from the area.",
                    "label": 0
                },
                {
                    "sent": "Computer vision where we say that some areas on the actually rendered page appear more dense than other areas.",
                    "label": 0
                },
                {
                    "sent": "So if you take a full text apparently looks more black than areas that are only consisting of a few words like a navigation.",
                    "label": 0
                },
                {
                    "sent": "So how can we from this vendor representation, so a similar representation if we only have a string?",
                    "label": 0
                },
                {
                    "sent": "Well, what we could do is we take the string and wrap it at a certain fixed line with for instance 80 characters that you know turn out to be a good limit.",
                    "label": 0
                },
                {
                    "sent": "And well, you Rapidan then if the text doesn't fit in one line, you rap it to the next line, and so on.",
                    "label": 0
                },
                {
                    "sent": "And if there's nothing to wrap well, you take just the next block and then you compute the density of such a block.",
                    "label": 0
                },
                {
                    "sent": "Essentially by counting the number of tokens in the block divided by the number left lines.",
                    "label": 0
                },
                {
                    "sent": "So it's a really easy measure, and Interestingly, it doesn't depend on the length of the whole block, because the average OK.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And there are contextual features.",
                    "label": 1
                },
                {
                    "sent": "Context can be seen as an intra document context, so you if you have a block need then you can look at the position of the block.",
                    "label": 0
                },
                {
                    "sent": "You can also look at the overall features of the preceding and following blog.",
                    "label": 1
                },
                {
                    "sent": "But you can also look at context Inter document level.",
                    "label": 0
                },
                {
                    "sent": "So if you have information about.",
                    "label": 0
                },
                {
                    "sent": "Other documents from from the same host of well from the from the same language corpus.",
                    "label": 0
                },
                {
                    "sent": "Maybe then it would make sense to count frequent blocks or account locks and see how frequent they are.",
                    "label": 0
                },
                {
                    "sent": "So for instance, on our website it also.",
                    "label": 0
                },
                {
                    "sent": "Yes we have always this boiler plate at the bottom.",
                    "label": 0
                },
                {
                    "sent": "That's a Copyright line with our address, and even though it appears to be dense regarding to uh, according to our text density measure, it's really frequent, so probably it doesn't really contribute contribute to the main content.",
                    "label": 0
                },
                {
                    "sent": "Right, so this into document level, but actually the question is can we avoid collecting this information?",
                    "label": 0
                },
                {
                    "sent": "So the approach I'm presenting here is actually local approach.",
                    "label": 0
                },
                {
                    "sent": "So given a website, give me full fix, that's it.",
                    "label": 0
                },
                {
                    "sent": "So it's possibly really fast because we and also memory efficient because we don't have this sort of information in advance.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're doing we're three experiments.",
                    "label": 0
                },
                {
                    "sent": "First of all, classification accuracy.",
                    "label": 1
                },
                {
                    "sent": "So, given a block, can you classify it as boilerplate and content?",
                    "label": 0
                },
                {
                    "sent": "And maybe some other classes like headline, supplemental image captions, whatever.",
                    "label": 0
                },
                {
                    "sent": "Of course we can apply machine learning techniques with a simple.",
                    "label": 1
                },
                {
                    "sent": "What we're doing is decision trees and support vector machines tenfold proselytization, and we have relating with several measures, are represented here.",
                    "label": 0
                },
                {
                    "sent": "The F measure and Roc area under court results.",
                    "label": 0
                },
                {
                    "sent": "Once we have this, we can say OK even I define what the main content of page is, how difficult it is to extract this.",
                    "label": 0
                },
                {
                    "sent": "And here we are comparing the different state of the art approaches like.",
                    "label": 0
                },
                {
                    "sent": "Old and established one.",
                    "label": 0
                },
                {
                    "sent": "It's BTE body text extraction, which essentially takes the very largest block that has the least amount of tags versus the maximum amount of words that it could find.",
                    "label": 0
                },
                {
                    "sent": "We also looking at engram approaches.",
                    "label": 0
                },
                {
                    "sent": "Here we show the approach from pasternack from last year.",
                    "label": 1
                },
                {
                    "sent": "And in the paper there are other approaches shown as well.",
                    "label": 0
                },
                {
                    "sent": "Um and then third, if we have removed the main content, it might be likely that we get a higher ranking right?",
                    "label": 0
                },
                {
                    "sent": "So if we remove all this stuff and we're looking at our backs of words level at the document, how much can we improve over the baseline and how much can improve over existing algorithms and measuring this by precision at 10 or the increase on precision intent?",
                    "label": 0
                },
                {
                    "sent": "And on any such attempt?",
                    "label": 0
                },
                {
                    "sent": "And for this we actually employing standard datasets with accessories and track data collection.",
                    "label": 0
                },
                {
                    "sent": "Using is a blocker, 6 corpus with three numbers.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the classification.",
                    "label": 0
                },
                {
                    "sent": "We had to find actually data set that is well annotated enough to the dollar to suit our needs and what we did is actually we were sampling Google News for a certain amount of time, like several months and at the end we had more than 250,000 pages from English Google websites.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "These are actually quite representative, I think because so we have many different sites in this corpus, not just a few sites, but really many and from this big corpus we randomly sample more than 600 newsarticles and this we annotated manually in the browser.",
                    "label": 0
                },
                {
                    "sent": "So we set up something that users could really easy annotated with my colleagues and me and what we did is we selected particular areas of the text as headline full text.",
                    "label": 0
                },
                {
                    "sent": "Image captions or supplemental.",
                    "label": 0
                },
                {
                    "sent": "Also, user comments.",
                    "label": 0
                },
                {
                    "sent": "And even related information and what we've not selected, we regarded boilerplate.",
                    "label": 0
                },
                {
                    "sent": "So what I'm showing here two different results.",
                    "label": 0
                },
                {
                    "sent": "First of all, on the bottom you see the classification results for headline full take supplemental user comments and related content separately.",
                    "label": 1
                },
                {
                    "sent": "In my presentation, I will be focusing on the difference only between boilerplate and any content below.",
                    "label": 0
                },
                {
                    "sent": "For this classification problem.",
                    "label": 0
                },
                {
                    "sent": "The paper for all the other other classes, but what you see here is actually that depending on how you look at the data, just the number of blocks, the number of words, or any tokens in the blocks that actually on the block level, the boilerplate is dominating.",
                    "label": 0
                },
                {
                    "sent": "So there are lots of blocks that actually considered non relevant for the main content and even on words level, it's still a third of all the tokens.",
                    "label": 0
                },
                {
                    "sent": "And talking level even more.",
                    "label": 0
                },
                {
                    "sent": "And all the other classes can see are quite minor if you look at article full text and that's quite a large amount of the data and headline supplemented user commented content actually don't contribute so much for this information.",
                    "label": 0
                },
                {
                    "sent": "So that's why we actually this presentation.",
                    "label": 0
                },
                {
                    "sent": "Concentrate on the two class problem.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now what we can do is now with all the features that I presented earlier, we put this into a machine learning classifier or set up different classifiers with different features.",
                    "label": 0
                },
                {
                    "sent": "Well, you find information gain see which features would perform best and well, just see how far we get this slide shows you several results, so they're more in the paper.",
                    "label": 0
                },
                {
                    "sent": "And this shows you actually if one method as well as Roc area under course.",
                    "label": 0
                },
                {
                    "sent": "Oh well, it's on the left.",
                    "label": 0
                },
                {
                    "sent": "And it also shows you where the decision trees, how many leaves.",
                    "label": 0
                },
                {
                    "sent": "These decision trees have and how many features we actually using to perform this classification.",
                    "label": 0
                },
                {
                    "sent": "I'm the first role.",
                    "label": 0
                },
                {
                    "sent": "Did you see is actually the baseline, so it always predicts content and it's, well, it's around 50% of resources weighted according to the presence of the classes in the corpus, less than 50%.",
                    "label": 0
                },
                {
                    "sent": "And what you can actually see is that most of the features already perform quite well.",
                    "label": 0
                },
                {
                    "sent": "For instance, what you what you can see here is that if you only take the number of words.",
                    "label": 0
                },
                {
                    "sent": "Like anything more than 15 words already has an accuracy of 86%.",
                    "label": 0
                },
                {
                    "sent": "And then you will add more features to it.",
                    "label": 0
                },
                {
                    "sent": "More local context to the page and so on.",
                    "label": 0
                },
                {
                    "sent": "And you're getting better.",
                    "label": 0
                },
                {
                    "sent": "But at the end, if you take all the features plus all the local features, plastic, global frequency, you kind of really improve over there.",
                    "label": 0
                },
                {
                    "sent": "And I wanted.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Attention to three of these lines.",
                    "label": 0
                },
                {
                    "sent": "Actually, if we just take the number of words.",
                    "label": 0
                },
                {
                    "sent": "And the link density.",
                    "label": 0
                },
                {
                    "sent": "So there is the well I'm out of words that are LinkedIn A blog, but all the numbers we already within F measure at 92% and Roc area under curve 95% quite close to the optimal solution and the cool thing here is that we actually don't have much features, so we actually only having two types of features and then we looked at the current block, the previous and the next block and that's it.",
                    "label": 0
                },
                {
                    "sent": "So it's already really good here.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And text entities almost the same.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit better for both values, rock area and the little bit even more, but actually it doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "So counting words that makes sense that he seems to be.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Equally good.",
                    "label": 0
                },
                {
                    "sent": "And if you're taking all the features that all the local features together, we can even improve rock up to 98.1%.",
                    "label": 0
                },
                {
                    "sent": "Of course, then you need more features, and it's questionable.",
                    "label": 0
                },
                {
                    "sent": "What features really contributed the most to the specification?",
                    "label": 0
                },
                {
                    "sent": "But these numbers are really impressive, I think already.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Main content extraction problem.",
                    "label": 0
                },
                {
                    "sent": "Um again.",
                    "label": 0
                },
                {
                    "sent": "Humane contravenes headline and full text and image captions will be ignored.",
                    "label": 0
                },
                {
                    "sent": "Here was the comments.",
                    "label": 0
                },
                {
                    "sent": "So in the news domain you have comments regarding an article and actually we were not regarding.",
                    "label": 0
                },
                {
                    "sent": "This is the main content so these results slightly different from the results before and what we did.",
                    "label": 0
                },
                {
                    "sent": "Here was the average and medium token level F measure comparison of all the documents that we have in our corpus and we again start with the baseline.",
                    "label": 0
                },
                {
                    "sent": "That means keeping everything seems to be good already.",
                    "label": 0
                },
                {
                    "sent": "Like average 68% in the median 70%.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you take, for instance, the approach from pasternack from that of last year, you can improve of course, that 78% versus median 87%.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But surprisingly, if you just eat everything with at least 10 words to block, you already better than any ground flamed approach.",
                    "label": 0
                },
                {
                    "sent": "Let's actually surprise as well.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the well, if you take BTEC or the body text extraction approach from film from 2001.",
                    "label": 0
                },
                {
                    "sent": "Well already.",
                    "label": 0
                },
                {
                    "sent": "Quite better, that's 89% versus 96.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Median and still if you take the densitometric classifiers, take textonly density, you can further improve.",
                    "label": 0
                },
                {
                    "sent": "Especially at the details of the most of the the documents are classified in the same, but later on you can improve.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The number of words classifier slightly improves over this, but it actually has the same features.",
                    "label": 0
                },
                {
                    "sent": "And then of course, if you would take the concept from ET to our classifier saying that it is probably one block that is important.",
                    "label": 0
                },
                {
                    "sent": "So we actually with this specified we're classifying any block that satisfies our criteria.",
                    "label": 0
                },
                {
                    "sent": "But if we only take the largest block.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we can.",
                    "label": 0
                },
                {
                    "sent": "Then we can actually improve even further.",
                    "label": 0
                },
                {
                    "sent": "Show me now.",
                    "label": 0
                },
                {
                    "sent": "Yes, so then we actually had 97% of median.",
                    "label": 0
                },
                {
                    "sent": "And in and use American.",
                    "label": 0
                },
                {
                    "sent": "Also do is, we can say, well, the main content starts with the headline.",
                    "label": 0
                },
                {
                    "sent": "So if we know that the headline is somewhere, it's the text of the title tag.",
                    "label": 0
                },
                {
                    "sent": "And if you find some common things, so it's really now heuristic that we're adding.",
                    "label": 0
                },
                {
                    "sent": "If there are comments below, we are.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "America strip them.",
                    "label": 0
                },
                {
                    "sent": "You can even further improve it so you risztics can actually improve.",
                    "label": 0
                },
                {
                    "sent": "But the simple classifier presented presented you few minutes ago already outperforms the existing algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, having a look at the classifiers that we were using, both the numbers and link density classifier.",
                    "label": 1
                },
                {
                    "sent": "What we can see is actually they're quite similar, simple and similar.",
                    "label": 0
                },
                {
                    "sent": "If you look at the distributions, the number of words and the classifier over on the left and on the right you see a difference and the difference is the articles of course have a different number of words on average, right?",
                    "label": 1
                },
                {
                    "sent": "So the number of words is not really limited.",
                    "label": 0
                },
                {
                    "sent": "So actually the classifier can detect this, but you can't see this here, it's just a skewed distribution between.",
                    "label": 0
                },
                {
                    "sent": "Non content words and the content words.",
                    "label": 0
                },
                {
                    "sent": "So that's why the deposition so well.",
                    "label": 1
                },
                {
                    "sent": "So so so empty that's able to see there's a really broad distribution for the for the content text over words with at least let's say 10 words and more and more than 50 words.",
                    "label": 1
                },
                {
                    "sent": "If you look at the text density as I said, this is a bounded measure.",
                    "label": 0
                },
                {
                    "sent": "You have a much nicer histogram, and if you look at the classification, we sort that we did before.",
                    "label": 0
                },
                {
                    "sent": "So we did this menu assessment.",
                    "label": 0
                },
                {
                    "sent": "You actually see that the content actually is most of the time in blocks with the text density.",
                    "label": 0
                },
                {
                    "sent": "A larger equals than 10 where is the non content is on the left hand side and I also showed you the distribution of links.",
                    "label": 1
                },
                {
                    "sent": "Also link text actually seems to have usually a smaller text density and also smaller number of words.",
                    "label": 0
                },
                {
                    "sent": "But overall actually these distributions are exposing the same features that we have in the text.",
                    "label": 0
                },
                {
                    "sent": "It's just visualized differently.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, this presentation we can utilize or looking at corpora on a very high level so that you can use corpus on the left and then some other corpora next to it.",
                    "label": 0
                },
                {
                    "sent": "For instance, the website UK graph exposes, while the very same properties on text density level.",
                    "label": 0
                },
                {
                    "sent": "So you can basically even without a menu annotation already see that with high probability left mode of this distribution is probably the boilerplate level.",
                    "label": 0
                },
                {
                    "sent": "Yes, the boilerplate text.",
                    "label": 0
                },
                {
                    "sent": "On the right hand side with higher tax densities.",
                    "label": 0
                },
                {
                    "sent": "Expect to have the full content.",
                    "label": 0
                },
                {
                    "sent": "Same applies for the blocks or six collection on the inside, and it even applies on single document level.",
                    "label": 0
                },
                {
                    "sent": "So I just took a random page from the web about com York City travel and expose this distribution.",
                    "label": 0
                },
                {
                    "sent": "Interesting Lee, if you take let's say the HTML version of my.",
                    "label": 0
                },
                {
                    "sent": "Paper here so I can go to the state Gmail, but it's no boilerplate well.",
                    "label": 0
                },
                {
                    "sent": "So we actually can can use this for visualizing what has already been there on the level of number of words.",
                    "label": 0
                },
                {
                    "sent": "Now what does that mean?",
                    "label": 0
                },
                {
                    "sent": "We have found it classified performs quite well, but is still kind of a model behind it and we can can further utilize to undo.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what's going on at the site?",
                    "label": 0
                },
                {
                    "sent": "And actually, I think we can, so let's start with a really simple approach how we think people could write pages or generative model for the creation of process of text on web pages.",
                    "label": 0
                },
                {
                    "sent": "If you start with a really simple Shannon writer, so monkey typing text in and at some point pressing return to create a new block, this is what we would get.",
                    "label": 0
                },
                {
                    "sent": "So with some probability the market starts typing.",
                    "label": 0
                },
                {
                    "sent": "In this case it all he always starts typing and it with some probability keeps typing or he goes to the next lock and writes in his blog, right?",
                    "label": 0
                },
                {
                    "sent": "And the state continues to go like this.",
                    "label": 0
                },
                {
                    "sent": "Essentially this is super new trial, so the transition to the next block is success P and emission of another word is failure 1 -- P. Since we already if we go from one state to another, we're meeting is a word or\nCharacter.",
                    "label": 1
                },
                {
                    "sent": "We are not using.",
                    "label": 0
                },
                {
                    "sent": "This is actually then simple geometric distribution, but the geometric distribution that is displaced by one and you can see, here's a really simple distribution, of course is not really modeling the natural language.",
                    "label": 0
                },
                {
                    "sent": "Let's say I give what's a good starting point, especially when you look at corpus level, right?",
                    "label": 0
                },
                {
                    "sent": "Because the modifier sticks, you have the more unlikely that you get a really sharp, complex distribution.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's extend this model.",
                    "label": 0
                },
                {
                    "sent": "Now, do you say OK?",
                    "label": 0
                },
                {
                    "sent": "We have full text and we have.",
                    "label": 0
                },
                {
                    "sent": "Boilerplate text that we already know that the more words right and more likely to sell full text or main content, and the less words you have, the more likely it is video boilerplate.",
                    "label": 0
                },
                {
                    "sent": "We can model this as a stratified Shannon.",
                    "label": 0
                },
                {
                    "sent": "Random wider, saying that at some point you decide whether you want to go and write long text, and then you continue writing long text and go back to write a new blog, or you decide to write short text text with fewer words, and then you continue providing this in the back, and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "And then we of course expected probability of writing.",
                    "label": 0
                },
                {
                    "sent": "Uh, convince long texts of continuing to write long text must be higher than to write short text.",
                    "label": 1
                },
                {
                    "sent": "And if you actually look at this model.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um the.",
                    "label": 0
                },
                {
                    "sent": "Goodness of the fit is increasing, of course, and the root mean square error is actually half, so this is a much better model to what's going on.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you look at the values that we could actually get, it's also explaining the situation quite well.",
                    "label": 0
                },
                {
                    "sent": "So on average, what you can see is that the long text is actually let's say, 23.8 words and the short text is 3.5 words, so that we can get from this information.",
                    "label": 0
                },
                {
                    "sent": "I thought we can all.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To get from the probability of going from a new block to decide that you want to right now short text is 76%.",
                    "label": 0
                },
                {
                    "sent": "It is quite well matches what we have from our Google News assessment at 979% of the blocks of boilerplate.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, well, saying that writing long text is good, but we can see if we just remove the text.",
                    "label": 0
                },
                {
                    "sent": "The blocks with.",
                    "label": 0
                },
                {
                    "sent": "Less than keywords.",
                    "label": 0
                },
                {
                    "sent": "Let's see how far we get and how we can can increase the retrieval performance.",
                    "label": 0
                },
                {
                    "sent": "And well, actually we did this on this last six days.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we found out it's actually that if you remove all the blocks with less or equal in 10 words, you can essentially.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Improve over the baseline by around 150%.",
                    "label": 0
                },
                {
                    "sent": "How much account and over the state of the art years before not.",
                    "label": 0
                },
                {
                    "sent": "You can even improve 1/3 of 50.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1%.",
                    "label": 0
                },
                {
                    "sent": "Slightly confused.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The next creation can be modeled as a stratified stochastic process.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you get a very high classification expression extraction accuracy at almost no.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As well as an increase of the prior position as it comes down to trust.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next steps would be to enforce look this look at this data from multilingual perspective.",
                    "label": 1
                },
                {
                    "sent": "Use different domain corpora and see how we can improve the model and for you the next step would be to try this code out the program as well as the datasets are available freely online.",
                    "label": 0
                },
                {
                    "sent": "The link is.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Paper and if you have any questions now, it's the very best.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        }
    }
}