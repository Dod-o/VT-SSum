{
    "id": "3tzvsij6ipyqlqmh33v5xlazupupbl5t",
    "title": "Neural Networks",
    "info": {
        "author": [
            "Hugo Larochelle, Twitter, Inc."
        ],
        "published": "Aug. 23, 2016",
        "recorded": "August 2016",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Unsupervised Learning"
        ]
    },
    "url": "http://videolectures.net/deeplearning2016_larochelle_neural_networks/",
    "segmentation": [
        [
            "Alright thanks Joshua so everyone.",
            "So yeah, I've been tasked to essentially give an introduction on neural Nets, so a lot of what I'm going to talk about is fairly basic concepts, But those are concepts that you'll be building on for the rest of this week.",
            "I mentioned this just so you're aware of the amount of pressure Aaron this put on me from this talk."
        ],
        [
            "So well, especially cover is basic.",
            "Feedforward neural net multilayer perceptrons is another way people describe them.",
            "What I'll go through our first talk about essentially, given some input X, how we make predictions using a neural net where I'll refer to F of X as the output.",
            "That is, if you're already somewhat familiar with neural Nets, we take input vector X and people are nuts.",
            "Will compute a series of layers of features which are hidden layers, and then eventually, ultimately computer.",
            "Output layer that serves to make a prediction about the label for some input X, and I'll focus mostly on classification during my talk.",
            "And so I'll talk about first how you compute all of these units in the neural net.",
            "So do forward propagation talk about the different types of units, activation functions that people tend to use have a little bit of discussion as to what is the capacity of neural Nets and why they're interesting in terms of their ability to model various complicated functions.",
            "And then I'll move on to actually talk about.",
            "Given that we've accepted that this is the type of model multilayer neural net that we want to fit to our data.",
            "How do we actually fit it on data?",
            "How we train neural networks?",
            "So I'll discuss the type of loss function we use for classification talk about the backpropagation algorithm, which will allow us to get gradients to perform some gradient based learning and describe some of these gradient descent algorithms are a few other tricks of the trades kind of information that you'll need for applying neural Nets in practice, and ultimately I'll talk a little bit more about motivating the case of deep neural network and talk about some of the more recent.",
            "Advances that have surfaced in the literature that were sort of inspired by this ambition of trying to learn deeper neural networks.",
            "So things like dropout, batch normalization and unsupervised pretraining.",
            "This is for the two lectures, yes, so I am not crazy so.",
            "Thanks for the sanity check."
        ],
        [
            "So in the first part what I'll do is I'll cover forward propagation and backpropagation, mainly only computing the gradients.",
            "Then we'll talk about that.",
            "We have lunch and then after that we talk about the complete learning algorithm for fitting a neural net on data and then talk about the deep learning part.",
            "Out of curiosity, who has seen you know, forward propagation and back propagation before you raise your hand.",
            "OK, it's going to be super useful.",
            "So what did you ask me to present this error so hopefully it'll be like watching a movie you really like for the second time or.",
            "And hopefully it will not be like when you watch a movie you liked and you realize all the special effects weren't that great.",
            "So alright, let's get going."
        ],
        [
            "A non forward propagation and I'll just start simple.",
            "Let's just start with describing what do we mean by an artificial neuron?"
        ],
        [
            "So.",
            "An artificial neuron will be essentially characterized by a few things.",
            "It will take some input X and it will have a set of weights which are connections between the inputs and the activation.",
            "Their neuron itself will have a bias and will have an activation function and the way we will compute the activation of a neuron is that we will first take the bias.",
            "So what I'm going to 1st compute what I call the pre activation function.",
            "I'm noting a of X.",
            "It's going to be a bias plus a linear combination of the inputs.",
            "Where each input dimension in my vector X is weighted by some way WI the weights W eyes are just the elements.",
            "The dimensions in my weight vector W. So this some I can represent it as vector operations by SB plus the dot product between my weight vector W an my input X.",
            "That's how I get my pre activation and we actually get the activation and sometimes will refer as the output of a neuron will just be some nonlinear function G which is our activation function applied on the pre activation.",
            "So if I unroll everything it's that equation here an so this is kind of a visual illustration of what are neuron is and often the bias will just represent it as actually a weight of a unit that is constant and equal to 1.",
            "So that's another way of representing that full sum as just multiplying an extended vector with W&B, added appended to that vector."
        ],
        [
            "So OK, what kind of functions can we represent with an artificial neuron?",
            "So we can just plug the output of some neuron if I want to do that in two dimensions?",
            "'cause that's easier to visualize.",
            "So here we have one axis, where have the first dimension of some input, and here second axis of 2nd second access for the second dimension, my input.",
            "And here this is just the output or the activation of my neuron.",
            "And so typically what we have is that in some region of the input space will have is essentially a flat and small value, and then as I move along in the direction of the vector W, eventually the activation starts increasing.",
            "That's what we typically get, and often we can even get a kind of Ridge that separates do two regions.",
            "So this is the kind of function you can represent and the bias B will determine where along vector W that Ridge will be.",
            "And the range of the values taken by the activation of a neuron is going to be determined by the choice of my activation function.",
            "That's a single neuron."
        ],
        [
            "Or at least do interrupt me at any point during the presentation.",
            "If you have questions, I'll be happy to make this more interactive and discuss some of the details so.",
            "I talked about."
        ],
        [
            "Bashan functions.",
            "What are the choices one would be to essentially have no activation function, so I have a linear neuron.",
            "This just means that the activation function on my pro activation is just the pre activation itself, but that's not very interesting."
        ],
        [
            "We often have sigmoid units.",
            "You have you seen that in doing a stock order this morning so the sigmoid function has the property that essentially squashes the pre activation between zero and one.",
            "As you can see here.",
            "So this is this axis, the pre activation and that's the activation of the neuron.",
            "You can see this squash between zero and one, it's always positive it is bounded and is strictly increasing.",
            "So the higher the activation the higher the activation and the actual mathematical formula for the sigmoid.",
            "Is this year it's 1 / 1 plus the exponential of the negative preactivation."
        ],
        [
            "Another popular choice is the 10 shore hyperbolic tangent function in this case.",
            "Mainly what changes its.",
            "It's quite similar to a sigmoid function, but instead its activation ranges between minus one and one, and so you have this somewhat more complicated formula, which is the exponential of the reactivation minus the exponential of minus the preactivation divided by the exponential deactivation plus the exponential of the negative pre activation.",
            "This is one way of computing it, you could.",
            "Multiply the numerator and the denominator by the exponential of the activation exponential of A and then you would get this formula which is slightly more interesting.",
            "Just because here we only have to compute one exponential, the exponential twice the pre activation and the exponential is more computationally expensive to compute.",
            "So that can be a more convenient form."
        ],
        [
            "Passionately.",
            "And finally, probably right now the most popular activation function is the rectified linear activation function.",
            "We often refer to those as the rare or.",
            "This is painful Relu OK for rectified linear unit.",
            "So if you see this in papers, this is referring to the usage of this activation function.",
            "The rectified linear activation function.",
            "It is bounded below by zero, but it is not upper bounded.",
            "It's also increasing, not strictly though, 'cause here it's not.",
            "It's actually flat.",
            "That's a mistake and then practice.",
            "It will tend to give when you train a neural net with Relu units.",
            "It will tend to give you sparse activities, which is property we observe empirically, and the function itself is simply the maximum between zero and the pre activation.",
            "So if the pre activation is larger than zero you get a linear function, but if it's below 0 you get a flat 0 output.",
            "0 activation alright."
        ],
        [
            "Any questions on that?",
            "OK, So what is the capital capacity of a single neuron?",
            "What kind of things can we achieve an represent using?"
        ],
        [
            "Single neuron, so we've seen this picture already and you can just looking at this already.",
            "See that it seems we are able to represent linear decision boundaries.",
            "So perform linear classification."
        ],
        [
            "So indeed, if we think of a single neuron and we use the sigmoid activation function, one thing we could do if we wanted to take the probabilistic approach of classifiers that do not describe this, to think of the activation of that neuron as representing what is the probability that for some given input X, the label associated with my input is 1.",
            "So think of in my notation, I'm going to assume sometimes a binary classification problem, which case the positive class is 1.",
            "And the negative class going to be 0, so this actually corresponds to the model behind logistic regression that donor presented.",
            "And so if we wanted to make a prediction that is assigned a label to some given input, we would just compute our activation of the neuron linear transformation followed by sigmoid and we would threshold at 0.5.",
            "Why will if that represents the models predictive distribution over the label Y?",
            "If it's it's values, how to?",
            "Five it means it thinks it's more likely that it belongs to the class one in the Class 0, so that's how you would get a classifier performing classification using a single neuron, as one can easily see that you would essentially be drawing a line in your input space between the positives and the negatives."
        ],
        [
            "So what kind of functions could be represents?",
            "I'm going to use a very simple example of Boolean functions, which is a classic example for explaining why we want more than just a single artificial neuron, so it's also even though it's simple, it's kind of interesting in the sense that you know if we would like to use neural Nets to perform some kind of logical reasoning.",
            "We would probably want such functions neural Nets to be able to represent certain simple at least simple Boolean operations.",
            "Some representing three different Boolean operations.",
            "So here true is represented by one and falls by zero.",
            "So if you have the order function, the OR function with output true or one if any of its input X1 and X2 is equal to 1.",
            "So in this case, so the triangles would be the positive class, so we can see if either this dimension on that dimension is equal to 1.",
            "We are in the positive class and output of one, and otherwise if both are zero we get a 0.",
            "So we can see that in this case.",
            "It is easy to draw a line that separates all the positives from the negatives, so we can represent that with a single neuron.",
            "Another function is the end function.",
            "Here I'm showing the end function on the negative of the first argument and the second argument.",
            "So in this case and will be equal to 1 or true only if both arguments are one.",
            "So it means that X one, since I'm taking it's negative needs to be 0 and then X2 needs to be one.",
            "So that's why I have a positive example here.",
            "Now the others, either one is going to be 0, so I get the negative class and again in this situation I can easily chop the space using a line into the positives and the negatives and same thing for North where I would apply the negative on another argument.",
            "So all of these functions I can represent with a single neuron."
        ],
        [
            "However, I take a slightly more complicated, but really not that complicated function.",
            "The X or function which is equal to 1 outputs true if either one is equal to true and the other files or vice versa.",
            "So this case in this case this would be a positive example 01 and 1 zero with the other with an output of one and any other combinations to ones or two zeros would be correspond to an output of false or 0.",
            "So we can see it in this situation we cannot draw a line in separate perfectly.",
            "All the positives on one side and all the negatives on the others.",
            "If I was to draw decision boundary that's linear.",
            "This is actually kind of a famous example.",
            "It was shown it was using historically an perceptrons book to show that a single neuron has some limitations.",
            "However, much like Donut describe, maybe I can transform my input space into another representation and then get a problem that's simpler.",
            "And in this case can be separated by a single neuron by.",
            "Their decision boundary.",
            "So what I'm showing here on this side is what if I take my input space an instead?",
            "Now the two dimensions with which I'm going to draw my inputs.",
            "My examples, the first one is going to be the end over the negative of the first dimension, an X2 and the second dimension is going to be the end of the first mention, and the negative of X2.",
            "So in this case this example here 00 would have an end for both.",
            "Both this and then this NB0, so it would actually map to this point here.",
            "And actually this point also if we run through the calculation of both ends here will also wrap into the same point.",
            "And we can go through the derivations and find out that for both these points they also lie essentially at the same place.",
            "So in this case I can actually draw Now a line between the positives and negatives and the thing to notice here is that the function that I've used to now draw my data differently are these two functions that I shown before that I could actually represent with a single neuron.",
            "So what this example suggests is that if I first compute a few set of neurons.",
            "And then I have some output neuron that is connected to these intermediate neurons and now I can start modeling nonlinear decision boundaries and in this case I can actually perform model this this function here the X or function and so that is at the core like why we are interested in neural Nets.",
            "While neural Nets interesting is that they allow us to go beyond linear classifiers, an actually model nonlinear decision boundaries."
        ],
        [
            "Any questions so far?",
            "Alright, so that brings us to multi nerer neural net."
        ],
        [
            "I'm going to start simply with a single hidden layer neural network, so in this case what this means is that between the input and the output I'm going to have a layer of what I call hidden units.",
            "Each of these units are like artificial neurons, that is, they are connected with the previous layer, the input, and they perform a linear transformation of the input and then apply some activation function.",
            "OK, so you're seeing here essentially a.",
            "Slightly different notation in the context of 1 hidden layer neural net, but we see the same form for the preactivation.",
            "It's a bias.",
            "So in this notation I'm using the exponent to index the layer.",
            "So this is B1 because there's a bias for the units at the first hidden layer.",
            "And then this index I is to indicate that this is for.",
            "This is the bias for the IF layer.",
            "Sorry IF unit in the first hidden layer.",
            "And so I'm going to take that bias to get my pre activation plus summing over the contribution of all the input units.",
            "So all the units in the previous layer, weighted by the corresponding weight of the connection between the JF input and the IF hidden unit, the one for which I'm showing the computation of the pre activation.",
            "And again here this matrix W which contains all the weights between all pairs of hidden unit and input unit.",
            "I'm storing them into this matrix W, one that I'm.",
            "Using Exponent two index to refer again to the first hidden layer.",
            "And I can actually write the computation of all pre activations of all units in the first hidden layer in vector form.",
            "So I can just take the bias vector B1.",
            "And then I, as you've seen here, what this operation corresponds to effectively is to take the vector X and multiply it by the row the I throw of my matrix W. So if I just take the product of X, multiplying on the right with my matrix W, what I'll end up doing is for each row I'm going to dot product between X and the corresponding row of weights, which means that I get all the contributions to the pre activation for all hidden units.",
            "So to sum up, my preactivation is just this vector valued linear transformation on my previous layer, the input layer X.",
            "And then as per usual.",
            "For regular units I get my hidden layer activation by applying my choice of activation function G over my vector, applying it elementwise.",
            "And then finally to get my output, imagine performing binary classification.",
            "So at the output I would like a neuron between zero and one that gives me the probability of belonging to the positive class.",
            "Well, again, going to take a bias so B2 Now because I'm at the second layers, which is the output layer and I'm going to multiply by some weight vector connecting my output unit with all the hidden units that I'm noting W 2 multiplied by the activation of the first hidden layer which is H1.",
            "So here I should probably have a.",
            "One here.",
            "So that's for a single layer neural network.",
            "Any questions on the notation?",
            "Or anything related to this slide.",
            "OK."
        ],
        [
            "So I describe the case of binary classification more generally.",
            "More commonly we would be interested in classifying into multiple class.",
            "So performing multiclass classification.",
            "So in this case what it means is that an input might belong to one out of C, capital C, different classes, and so if I wanted to do this, I would like to represent with my neural Nets some probability distribution over all the classes.",
            "So unconditional distribution of my input.",
            "X belonging to one of the capital C different classes, so I'm noting that mathematically as the probability of Y, my label YB equal to some small C which is a choice between 0 and C -- 1.",
            "Or actually I'm indexing from one to capital C here.",
            "So one way to get these probabilities is essentially have an output layer that is always positive and whose activations sum to one and there's a very simple way of achieving this with the what is known as the softmax activation function.",
            "All that it does is that it takes the pre activation vector at my output layer.",
            "These are the days at the output layer and then I'm going to take the exponential on each dimension.",
            "And finally I'm going to divide each dimension by the sum of the exponentiated.",
            "Reactivations so you can see that writing it this way all my dimensions is going to be normalized.",
            "That is, if I was to sum over all the values in that vector because I've explicitly divided each dimension by that sum, I'm going to get a sum that is equal to 1.",
            "So because of this I can interpret the output as a multinomial distribution over my input X belonging to one out of the C different classes.",
            "And if I wanted to actually perform classification with a model that had a softmax output, I would just look at which unit has the highest activation.",
            "That is, according to my neural net.",
            "What is the class that has the highest probability given my input X?",
            "And that's how it would perform."
        ],
        [
            "So we can go from 1 hidden layer to multiple hidden layers, so I'm just extending my notation here where I'm now I'm using K as some arbitrary hidden layer within my neural net.",
            "Going to use capital L as the number of hidden layers.",
            "So in this case pre activation and some layer K is just going to be a linear transformation of my previous layer.",
            "So in my notation, to make this equation work, I'm going to assume that H zero of X is actually the input.",
            "So the layer below the first hidden layer is the input.",
            "And so now the pre activation is just this linear transformation taking my weight matrix, multiplying it by the activation of the layer below with some bias.",
            "And finally, I'm going to get the hidden layer activation by applying my activation function an.",
            "I do the same thing for the output, but typically for the output layer I'm going to use a different activation function and as I've described if I'm performing multi class classification, I'm going to use the softmax for the activation function at the output and for their hidden layers.",
            "I might use the sigmoid.",
            "I might use a 10 H quite frequently.",
            "Will actually use the relative the rectified linear activation function."
        ],
        [
            "OK, any?",
            "Questions so far with any of this, probably yes.",
            "Yes.",
            "Yeah."
        ],
        [
            "There were describing it as a row vector is the transpose just to get into the vector?",
            "Yeah, excellent point.",
            "So my assumption when I'm writing vectors is that they are column vectors and so because if I write it this way, it suggests that it's a row vector.",
            "The transpose here is to emphasize I'm taking that role vector.",
            "I'm transposing it, so by default if you see a vector notation then I that's a good point.",
            "You mentioning this I eventually emphasized that.",
            "So whenever I use a bold type, it's to refer an the lower case.",
            "It's usually to refer to a vector.",
            "If I'm using bold, but for a capital letter is usually going to be a matrix, and if it's italic instead of like this one here instead of bold, that's going to be a scalar.",
            "OK, so I'm going to try to hopefully help out understand whether we are referring to scalar of vector.",
            "I'm going to try to stay true to that convention.",
            "Great point, yes.",
            "So the rule of thumb is used relative.",
            "That tends to work really well, but I've seen examples where it turns out that the sigmoid for some reason is better, or the 10 H. Take that, I don't know if yes or N as like their own rule of thumb, but typically I start with the relevant and maybe I will try the Sigma 10H if for some reason I'm not satisfied with the results, that would be my rule of thumb, but kind of yeah.",
            "Yes.",
            "Or I mean, I can't say that I've seen a lot of Oh yes, thank you.",
            "So you will have to tell me that often.",
            "So the question is, should we use different activations at different layers at different positions in the different hidden layers of a network?",
            "I haven't seen a whole lot of success in actually kind of fine tuning the type of activation you would use at different places in a neural net, so I would say that usually that's not the biggest bang for the Buck in terms of performing more experiments.",
            "There is some research on trying to make the activation function learnable in some way, and that research is interesting, but I can't say I've seen like tremendous success in terms of like improving performance in a way that's particularly meaningful.",
            "Some special structures like game?",
            "Yeah yeah.",
            "So except for the yeah, that's right.",
            "So except if you want to put structure in terms, for instance, there's forms of attentional mechanisms which correspond to.",
            "Using different types of connections between units and different forms of activations that are inspired by trying to emulate some activations or some form of processing, and in this case I think it makes sense to reason in that space, but the individual unit and the regular sort of power, like layers that are stacked.",
            "I've been seeing a whole lot of gains and trying to really tune the individual choices of units across layers.",
            "Yes.",
            "Sorry.",
            "Oh yes, so yes.",
            "So we will need so it's a yes is the is it important for the activation to be differentiable?",
            "It will be if we want to perform gradient descent because then we will require gradients and so we go through that in a bit.",
            "But if the activation function wasn't differentiable then you need to leverage sort of more advanced principles for training forms of reinforcement learning, for instance.",
            "So if you have stochastic units, for instance for which you cannot.",
            "Back propagate so get a gradient through the sampling.",
            "That's an example where you use an algorithm like reinforce so, but typically like it's a pretty good thing to have a activation function for which you can compute.",
            "Gradients could.",
            "Then you can do end to end learning with gradient descent.",
            "So it is differentiable at every point except the inflection point at which we can get a subgradient.",
            "And there's some theory that suggests that doing subgradient descent, it's still fine.",
            "So in this case that's why it's not the problem.",
            "The fact that there's no, you don't have an exact durative at the inflection point, but that's a good point.",
            "Yes, that's the reason.",
            "Alright, so just to make the division calculation easier or yeah, so the question is why use the exponential here?",
            "I think yeah, that is 1 good motivation.",
            "The fact that when you start computing gradients with respect to the activation, when you use the exponential and we use the loss, which is the log likelihood as we'll see in a few, you get fairly simple forms of gradients.",
            "So I think that is 1 motivation.",
            "Yes.",
            "Activation is one thing, but someone asked me once and I couldn't think.",
            "I like Peach mix with inner layer.",
            "Different activation functions.",
            "Is there any research that you've ever seen that actually shut up?",
            "It's useful, or there's a particular reason like mix really sigmoids and Tange?",
            "I mean 'cause it seems like that could be cocktail or in any particular benefit in any research mixing it with.",
            "Yeah, so the question is whether there's a benefit to maybe within the layer.",
            "Tried to vary the types of activation functions I. I don't feel so rare and have any examples, but I can't think of any that's like really compelling.",
            "Max out which you can think of is a bit of a generalization from Bellevue, where we're basically putting together a bunch of piecewise linear units, right?",
            "So this is something we've done.",
            "We did two years ago.",
            "Ian Goodfellow was First off on that, so that's a case where they are free to learn.",
            "Like you mention that there are a few examples of this and you could imagine that it would learn different functions within a week.",
            "So on average it tends to work about the same as value, sometimes a little better, sometimes little worse.",
            "Somebody's in for speech recognition.",
            "It seems to work very well, but observation?",
            "Throwing in like not entirely swipe the standard mixer functions, the second being more than engineering headed stuff.",
            "There's no necessary.",
            "Direct.",
            "So for the for the for the video, oreg Aaron gave an answer.",
            "It was like, yeah.",
            "He's actually said that examples like the Max out is a form of learning the activation function.",
            "To expand on this, actually you can, because when you get the activation, you will ultimately multiply it to get the pre activation at the next layer.",
            "Going to perform a linear operation.",
            "It could be that actually linear combinations of certain units within the layer is by itself a more sophisticated activation function.",
            "Actually this kind of builds into."
        ],
        [
            "I was going to talk next, which is the capacity of neural Nets.",
            "So."
        ],
        [
            "Move through that.",
            "So just to give you an intuition as to why you can, you can get nonlinearities and what forms of nonlinearities you can potentially model with a neural net.",
            "And if you view a few examples, those are taken from Pascal Vanzile slides so.",
            "For instance, you could imagine, as we've seen before, that I could have one single neuron that represents dysfunction.",
            "Here some sort of Ridge in some direction, and then I could have the same Ridge but shifted by changing the bias with another unit, and then you can imagine that if I take essentially this unit and subtract that unit, I will get as a result a sort of rich.",
            "That's like a bump in this direction going up, and then going down.",
            "So this is what I was referring to, where you can take combinations within layer, just linear combinations and effectively.",
            "It's as if you were getting.",
            "A more sophisticated activation function to some extent."
        ],
        [
            "Then you can start imagining doing this, but in multiple directions.",
            "So here you have something slightly different, which is you have a neuron that increases in One Direction and then the other neuron that starts high and then decreases.",
            "So this would just be you would assume it would take the same weight vector, but the negative of the weight vector and you could do that in two perpendicular directions and then linearly combining all of these in the appropriate way you would obtain in the original input space.",
            "A bump at some place.",
            "So now you can see that, at least in 2D, with four units, you can get a bump positioning above that gives you a high value at some very restricted area in the input space and low everywhere else.",
            "So then."
        ],
        [
            "It's not a big stretch it imagining that if I combine such groups of four units in 2D, I can now easily get fairly complicated decision boundaries where maybe I have a bump right here and then I get a bunch of little bumps here to get.",
            "Also, the positive class in this area and everywhere else there's no bump, so you get like negative in R2 and then positive value over the threshold for classifying in the positive class in the area are one.",
            "And so."
        ],
        [
            "You can actually formalize and use that intuition to actually show, and this was shown by Hornick in 1991 that a single hidden layer neural net with linear output unit can approximate any continuous function arbitrarily well as long as you give it enough hidden units.",
            "As long as the hidden layer is big enough, it is possible to buy and construct well, it would be possible theoretically, construct by and a function that would approximate approximate arbitrarily well some arbitrary continuous function.",
            "This was all actually applies for Simone and tension.",
            "Many other hidden layer activation functions.",
            "And so that's that's a really important result.",
            "It tells us that technically, if we make the hidden layer large enough, we can pretty much model any function we want.",
            "So no matter what is the function that generated my data, I have the flexibility within the family of neural Nets to potentially represent that function to learn that function that is responsible for the data I'm seeing in my training set.",
            "Now.",
            "This result, however, doesn't tell us anything about how do we actually learn the neural net to represent that function, and so this is where we get to the learning algorithm, which I'll.",
            "Talk about next."
        ],
        [
            "Any question at this point on this first section.",
            "Alright, so let's talk about backdrop."
        ],
        [
            "So I'm just going to start by formulating the learning problem.",
            "The problem conceptually of training a neural net on some data."
        ],
        [
            "So what we'll do is use the paradigm of empirical risk minimization with some regularization, so empirical risk minimization.",
            "What do we do?",
            "We frame the problem as follows, so I'm going to use X with the exponentiated index T to refer to the TF input in my data set Y, potentiated T is DTF label.",
            "In my data set, so F of X, as I've mentioned before, that's going to be the output of my neural net.",
            "And here I've just added a little bit of notation that makes it explicit that.",
            "My neural net has parameters and those are represented by Theta, so Theta would in the context of a neural net contain all the weights in the biases in my neural net an I'm going to define a loss function that compares the output on my PF input with the actual true label.",
            "So it's a sign of loss if my prediction isn't correct.",
            "For instance, I'm assigning to higher probability on the wrong class an.",
            "So in empirical risk minimization, what we try to do?",
            "Is minimize the average loss on my training set the average of all the losses when I'm comparing the output on the training set of my neural net with the actual true label?",
            "And also usually we add some regularization that do not mention and waited by some hyperparameters, some factor Lambda that will control the tradeoff between a lot of regularization and fitting my data.",
            "Well, I should say that technically speaking, when we have regularization, we don't talk about empirical risk minimization but structural risk minimization.",
            "If you want to be precise about things.",
            "And so, in this context, learning is cast as an optimization problem.",
            "Learning will correspond to change taking my training set and finding this data.",
            "This data here, which minimizes this expression.",
            "So."
        ],
        [
            "How do we do this?",
            "There are multiple different optimization methods and one that's very simple and works really well in practice and I'll expand on slightly more sophisticated versions of this one that works really, really well is the stochastic gradient descent procedure.",
            "So in stochastic gradient descent, I first start by initializing my parameters in some way and we'll discuss probably after lunch about how to do that properly, and there are certain badly constructed cases you don't want to use, and then for some number of iterations I'm going to take out of my training set a example.",
            "So a pair of input XT and label YT.",
            "And then I'm going to look at what is the direction in which my loss would for that example and for my regularizer would decrease the most.",
            "In other words, I'm going to look at what is the gradient of the loss with respect to my parameter, so nebulize the gradient on this loss, and here as an index to nabla, that is not clear and there is an index to noblem using the parameter with respect to which I'm taking the derivatives and also because I have the sum of the loss.",
            "And the way to regularizer the gradient on all of that distance to some of the gradients.",
            "So I also get Lambda times my gradient of my regularizer so the gradient tells me in which direction the last N regularization would increase.",
            "The most I'm going to go in the opposite direction to take a step in the direction in which would decrease the most.",
            "And I'm going to take a step in that direction, so that direction I'm calling it Delta.",
            "Here I'm going to multiply by step size.",
            "Alpha will usually refer to the Alpha as the learning rate, and I'm going to move in that direction.",
            "That is, I take my current value of my parameters and I just added Alpha times my direction.",
            "So that gives you a new parameter.",
            "I'm going to repeat this several times.",
            "OK, so that's effectively stochastic gradient descent.",
            "It is great in the sense because as you can see I'm computing gradient and you're going in the descent direction for that gradient, and it is to castec because unlike what don't have shown where you would take the whole objective, the whole average of losses in your training set and take the gradient with respect to the whole average.",
            "Here I'm actually taking the gradient for a single example that is stochastically drawn from the training set, and it actually in practice often we linearly iterate over the examples.",
            "And we usually probably inaccurately usually refer to that as stochastic gradient descent.",
            "So if I want to apply this general procedure here, there's nothing that you know particularly refers to a neural net beyond the fact that I've said that it would be the weights in the biases, but you know, if you have some function F. This is essentially really just an optimization procedure.",
            "So what I need to do here in the context of my neural net is to specify what is the loss function pacifi a procedure that is as efficient as possible for computing the gradient of the loss with respect to the parameters, and I need to specify a regularizer and.",
            "And from the regularizer get a gradient on that.",
            "And also I will need to specify how to initialize my parameters.",
            "So before the lunch break I'll actually essentially focus on what is the last function and how to get the gradients of the loss with respect to my parameters, which is essentially the backpropagation algorithm."
        ],
        [
            "So there are no questions.",
            "Let's move to the last function."
        ],
        [
            "So as I hinted, we might actually want to."
        ],
        [
            "Yes.",
            "Oh yeah, sorry.",
            "Yep.",
            "Structural regularizer any of these other alternative forms like dropout or structural priors or any which we call regularization of some point.",
            "We're always doing in structural.",
            "I don't know.",
            "Yes repeating the question this is going to be painful so you shouldn't say that ask questions.",
            "Don't worry about me.",
            "So the question is whether if we use other forms of regularization like dropout or unsupervised pre training are we then?",
            "In the family of would we refer to that as structural risk minimization as opposed to empirical risk minimization?",
            "I believe so actually not sure what is the official.",
            "You know if you actually have to have this explicit regularization term in what you are optimizing, believe that might be true, but an for drop out, it's not necessarily for deep neural Nets explicit what the regularization term is, so I'm actually not sure that's a good question.",
            "OK, so.",
            "So the last function.",
            "So as I said, we're going to use the values of the units at the output layer as a predictive distribution over what is the Class C or I guess Y.",
            "So whether the CF unit and my output layer is going to be interpreted as the probability that my input X belongs to Class C, that is, the model probability is predicted by my model.",
            "So what we could do one form of learning would be to do maximum likelihood, so that is trying to maximize the probabilities.",
            "Of the assigned label.",
            "Then my training set Y for each of my example input examples X because all the math is nicer numerically, it's nicer instead of maximizing the probability is going to maximize the log probabilities, and because I just framed everything into a minimization problem, we're going to minimize the negative of the log probabilities, so my loss function is just going to be minus the log of the value of the YF.",
            "Input azarius unit in my output layer F of X.",
            "You can also write this.",
            "As summing over all possible classes C and I check whether my iterate iterate are C is equal to the true label Y, and if so, this is equal to 1.",
            "This is just the indicator function, otherwise it's at zero and so when is equal to 1, I'm effectively going to have this term be present in the sum, which is just the log.",
            "The probability of that Class C, and the reason I write it in this way we have a minus here is that sometimes you will see people reffering as the loss used for training.",
            "As a cross entropy and in this form it's more explicit that you have a cross entropy.",
            "It's the cross entropy between this distribution.",
            "Here that's the output layer of my network and a multi normal distribution that puts probability one on the correct label and zero everywhere else.",
            "So that's why sometimes also refer to this as the cross entropy."
        ],
        [
            "So.",
            "Let's start computing gradients, so we're going to look at what is the gradient of the loss with respect to computations at the output layer."
        ],
        [
            "And we'll slowly make our way towards the parameters."
        ],
        [
            "So.",
            "If I wanted so the greatness is just a vector of partial derivatives and we just I find it more convenient to start by computing the derivatives and then show that from the expression the generic expression for the derivatives there, Rivett if's will get an expression for the vectorial expression for the full gradient.",
            "So if I wanted to derivative of minus the log of the probability of the correct class with respect to my output for the CF class, well?",
            "The derivative of minus the log that's going to be minus 1 divided by the argument of the log.",
            "That's the probability of the correct class under my model and the derivative of F of XY with respect to F of XC.",
            "But it's going to be 0 if CNY are different.",
            "So if C is a different class in the correct class, why this is going to be 0?",
            "And otherwise that's going to be one, so that's why this one here is actually now an indicator function of whether Y is equal to see.",
            "Now if I want to just take that expression and construct a vector corresponding to the gradient of the loss with respect to all of my outputs, what I'm going to get is, well, all of the terms for all values of C is.",
            "I'm always have this minus 1 / F of XY, and then the only thing that changes with C is this term here, right?",
            "And so I have this vector of whether Y is equal to the first time.",
            "So here for some reason I switched from classes 12C20 to see minus one.",
            "Just make sure everyone is following so keep you on your feet.",
            "So if Y is equal to 0, Y is equal to 1 and so on up to see minus one.",
            "And actually this vector, I'm going to note it down as East of Y or I'm going to refer to it as the one hot vector for label Y.",
            "So it's just a vector that has a bunch of zeros.",
            "Except for one one which is at the dimension corresponding to.",
            "In this case the index Y. OK, so that's a notation I'm going to use throughout.",
            "So the gradient with respect to the output is just the minus the one hot vector divided by, so scaled by one over the probability of the correct class under my model."
        ],
        [
            "We can also go a bit deeper in my network and ask what is the gradient with respect to the pre activation at the output layer.",
            "Now.",
            "So as we will show in a few, it actually has a very simple expression.",
            "You actually seen this expression and during this talk it's going to be simply minus.",
            "Weather my output unit C matches the correct label.",
            "Minus what is the current output?",
            "So after the activation function for the unit C. OK, so if we put all of this together to get a vector, all we get is minus the one hot vector minus the output.",
            "So you can see that this gradient essentially comparing what is the ideal output which would be a probability of one for only the correct class and everything zero, and it's comparing that with the current output of the model.",
            "OK.",
            "So this I haven't shown the duration of this, I will."
        ],
        [
            "Do it with you because I find it instructive and because it used to be that we did that in our neural Nets class.",
            "So you're going to suffer through this if you haven't yet.",
            "So.",
            "I am computing the derivative of my loss minus the log of the property is correct class with respect to the pre activation at my output layer.",
            "So This is why I have a of L + 1 L plus one is the index of the output layer with respect to some arbitrary output C. So I've already shown that the derivative of minus the log of F of XY that's going to be minus 1 / F of XY because of their ative of the log and then times using the chain rule derivative of F of XY with respect to their pre activation at the output neurons C. Next I'm going to write what exactly is F of XY?",
            "Well, that's actually the wife component.",
            "Once I've I've applied the softmax activation function over my pre activation vector at the output layer.",
            "OK so this allows me to expose now the dependency of five XY with respect to the pre activations at my output layer.",
            "Next I'm going to write with the softmax is here.",
            "Notice that I've the softmax over vector is a vector.",
            "So yeah, I'm indexing the wife component.",
            "So what I'm effectively going to get is the exponential of the wife preactivation at my output.",
            "Normalize so divided by the sum of the exponentiated pre activations across the layer.",
            "So all I've done here is just take that expression and replace it with that.",
            "Impression and replace it with this.",
            "Now I'm going to use what is the formula where we're taking a derivation so our gradient with respect to a ratio of two functions of some X that we're doing the duration with respect to.",
            "So in this case.",
            "My function G is the numerator.",
            "Here my function, my function H in that identity here is this sum over all classes of the exponential and exponentiated reactivations.",
            "So what is the expression I'm going to take the Gray, the derivative of G of X with respect to X * 1 over my denominator, so one over my Dominator is just this part here and here.",
            "I'm multiplying by the narrative with respect of G of XG of X.",
            "Is this with respect to my pre activation?",
            "And the same thing here, so G of X is right here.",
            "The square of H of X.",
            "That's the square of my normalization constant and then multiply by the derivative of the denominator H of X, which is the sum of exponentiated terms.",
            "So I take the directive of this expression with respect to my pre activation at node C. Alright, so if I take that.",
            "Narrative here, which respected the exponential of activation at the label Y.",
            "Well if C and why are different?",
            "That's going to be 0 because the those are two different numbers to different pre activations.",
            "So This is why I have the indicator function checking whether Y is equal to 1.",
            "If it's not, it's going to be 0 times something.",
            "So the expression is going to be 0 and then if I take an.",
            "Otherwise if Y is equal to see the derivative of the exponential is the exponential.",
            "So This is why I recover the exponential of the pre activation at the correct label Y and I'm going to compute this durative here, so I'm taking a derivative of a sum, so that's the sum of the derivatives, and since all of the terms here there's only one, so this notice that this is a sum over all possible classes, which I'm noting as C Prime C prime to make it different from C, which is the unit with respect to which I'm taking the derivative.",
            "So in this sum, all the elements are not going to be.",
            "Dependent on the CF unit, except for when C prime is equal to C and then when it is the derivative of the exponentiated of the exponential is the exponential.",
            "So ultimately this whole derivative here simplifies to just the exponential of the pre activation for node C. Next, going to notice that this divided by this normalization constant is just the wife element of the softmax of my pre activations.",
            "That's just the definition of the softmax.",
            "Here for dimension Y.",
            "And similarly this term here in this term here so you can see I've taken the square here and I separated it into two terms.",
            "So I have one term which is the numerator where I get the exponential for the correct label and here is the exponential for the label for the node C, which is the note for which I'm computing the derivative.",
            "So both of these are softmax is but one is for the wife output and the other one is for the CF output.",
            "And then finally, well, to simplify further, the softmax of Dupree activation that's F of X, so I'm replacing all the softmax is here by F of X and this allows me to cancel this with these terms here.",
            "It's ultimately what do I get minus my identity function minus the function.",
            "The output of our model.",
            "This might be a good time to say my slides on the website, so if you actually want to go through this after or as we're going through and going back, the slides are in the speaker section, yes.",
            "So so yeah, so everything simplifies.",
            "It might have seemed surprising just to sum up that derivative of the YF output now depends on some other pre activations for some other unit.",
            "And this is only because of the activation of the summer normalization constant.",
            "This is only because in the softmax I have this term here.",
            "This is why before I before when I was looking at the gradient with respect to the output, I had something very simple to derive.",
            "Here is a bit more complicated because of this normalization constant."
        ],
        [
            "OK so so far I went from the loss and I obtained the gradient with respect to the preactivation."
        ],
        [
            "Now again, I'm going to try to go a bit deeper now.",
            "You might have noticed that this is getting a little bit complicated, so if I was to write the whole expression of the loss with respect to each of my parameters and went through all the derivation, there would be a lot of work.",
            "So what it will?"
        ],
        [
            "Do instead is exploit the chain rule so the chain will allow us to use previous company calculations.",
            "Previously derived expressions to get an expression for gradient of things deeper in the network.",
            "So with the chain Rule says is that if I have some function P that depends on some argument A and I want to get the derivative of that function P with respect to my argument A and imagine also that my function P is actually completely computable.",
            "From a set of intermediate results that each depend on a in these intermediate results, I'm going to call them Qi.",
            "Well, in this case, the narrative of P of a respect array is the sum over all intermediate computations of the multiplication between what is the derivative of a with respect to my IIF intermediate result times.",
            "What is the derivative of Qi if intermediate result with respect to a so you can see that this result is applicable in the context of a neural net.",
            "If I want the gradient on the activation at this unit here.",
            "Well, we have computed the gradient with respect to the activations.",
            "Here we can see that the loss directly only depends on all the pre activations computed here.",
            "So then the only bit that means that is missing is what is then the gradient or deliberative of these intermediate results with respect to my quantity which in our next stage is going to be the activation of the layer below."
        ],
        [
            "So that is what we'll be invoking here for performing our derivations, so if."
        ],
        [
            "I want the narrative of my loss function now with respect to the JF unit at some hidden layer cake.",
            "Any hidden layer cake.",
            "Well, I'll invoke the chain rule and by writing it as it's just going to be the sum over the derivative of my loss function here.",
            "With respect to all pre activations I for all the units I add the layer above.",
            "So we respected the pre activation, add the layer above, that's why we have a K plus one here and I'm going to play this by what is the narrative of the pre activation of the layer above K plus one for the IF unit with respect to the activation at the layer K for the JF unit.",
            "OK so just applying the chain rule here and so I'm going to assume that I've computed this.",
            "Already and we showed before, we already did compute the gradient of the pre activation at the output layer and now the only bit that is missing is this part here.",
            "What is the derivative of my pre activation at the layer above with respect to some activation of some unit in the layer below?",
            "Well pre activation is quite simple, it's just a linear transformation so I'm just taking a bias plus the sum of all the weighted contributions of all units in the layer below.",
            "So if I'm taking the durative which inspected the JF unit so The Dirty with respect to the pre activation well with respect to the bias, that zero is the biases of different.",
            "It doesn't depend on the activation of the layer below, and then I have a some overall units J of the wait times the activation and so all of these terms.",
            "Here the directive is respect to specific unit is going to be 0.",
            "If it's not that unit.",
            "So if this J here is not the same JS here.",
            "An when it is, well, you get you have a scalar times my variable with respect to which I'm taking the dirt.",
            "If so, that's just going to be the scalar, so ultimately this will simplify where this sum goes away, and then I remove this so I get the scalar in front of my variable with respect to which I'm taking the derivative.",
            "So This is why this whole term here becomes just the weight between the IF unit at the layer above and the JF unit at the layer cake.",
            "And.",
            "One other way of writing this I have a sum over all units I in my layer K of the weight the weighted.",
            "Partial derivative with respect to the pre activation at the layer K plus one.",
            "So one thing I could do is actually consider the whole gradient, so the whole gradient of with respect to the pre activations at layer K and multiply that C I'm indexing with respect to.",
            "I, so I'm indexing over Rose, so multiplying this by the JF column of my matrix W If I want to write instead of being explicit, some actually write this in linear algebra, so I can just write this instead as.",
            "The gradient of my loss with respect to my pre activations at layer K + 1 multiplied by the JF column of my matrix WK plus one.",
            "So here the notation of using is.",
            "I'm using this dot to refer to all rows.",
            "At the column J and so because of this you can think of it as slicing a matrix.",
            "So I'm taking all rows at the specific column, so then this would be a column vector and so to have it become a row vector that I can multiply with this column vector.",
            "This is why I'm taking the transpose right here.",
            "OK.",
            "So that gives me the derivative with respect to some activation of some unit, that layer K and now."
        ],
        [
            "If I wanted the full gradient so the vector of all derivatives which effects all activations at layer K. For my loss.",
            "Well, I need to do is essentially perform this operation here, but for all columns J.",
            "Well, I get that simply by taking my gradient of the pre activations at the layer above and multiplying that by the full matrix transpose.",
            "Then I'm going to be multiplying by each of the columns and get the vector of all the narrative's.",
            "So ultimately, after doing all these up, these derivations going from preactivation gradient to activation gradient, the layer below is a very simple operation.",
            "Just take the above pre activation gradients and multiply by the transpose of the weight matrix."
        ],
        [
            "And now if I wanted to get at the second stage again, my gradients with respect to the preactivation at this same layer K. So I will again evoke the chain rule.",
            "So in this case the pre activation and deactivation in one layer are, so the activation of 1 unit only depends on the activation of that same unit, unlike at the output the output it's not the case or dependencies because of the normalization constant.",
            "But here I'm going to assume that I'm using an activation function like the sigmoid or the 10 H or the Relu, in which case you just have an element wise computation, so there are no normalization within the layer.",
            "So in this case.",
            "The derivative of my loss with respect to my pre activation for the JF unit at layer K?",
            "Well, that's going to be the narrative of my loss with respect to the activation of that unit HK.",
            "Times what is the derivative of the activation of that unit with respect to its pre activation and what is this term here?",
            "Well that's just the derivative of my choice of activation function G prime.",
            "OK."
        ],
        [
            "And now if I want to do this so this is again the partial derivative for a single unit J.",
            "If I want to get the gradient so the vector of all partial derivatives, all I need to do is take each individual.",
            "Each individual.",
            "I just need to take my gradient my full gradient so the vector of all partial derivatives with respect the activation of all units.",
            "And I just do an elementwise multiplication with its corresponding derivative of the activation function.",
            "So this vector here.",
            "That's just going to be the vector where the first dimension that's the derivative of the activation function for the first unit, and then the second unit and the third unit, and so on, and just pack this into a single vector and this symbol.",
            "Here I used to represent the elementwise multiplication.",
            "So again, computationally it's not a complicated operations to do you take your vector, you construct this other vector of the derivative of the activation function, and you do an elementwise multiplication, and that's it.",
            "You now went from activation gradients to preactivation gradient."
        ],
        [
            "So I mentioned the I should know if there are questions at this point.",
            "Or any other notation or.",
            "And if you will just wrap everything into a single algorithm."
        ],
        [
            "So I mentioned using this durative here."
        ],
        [
            "I'm just show very quickly what is the narrative of the different choices of activation functions.",
            "I mentioned the use of possibly the linear activation function, so essentially no non linearity.",
            "So essentially the output of that activation is just its input.",
            "It's pre activation.",
            "Well you can see that the slope of that activation function.",
            "That's just one.",
            "So it's a very simple decorative."
        ],
        [
            "For the sigmoid function, I won't see I won't show the derivation here, but you can show that it is pretty simple to compute.",
            "Also, it will just be the output of the sigmoid multiplied by 1 minus the output of the sigmoid OK, and so we can see here that what this implies is that the slope from this expression is going to be 0 if either Jie is close to 1, because in this case I'm going to have 1 -- 1, so that's going to be 0.",
            "Or if Jay is close to 0 in this case, I'm going to be multiplying right here by zero, and indeed you see that the slope towards where the activation is equal to 0 when the reactivations very negative and towards the end where the activation is close to one.",
            "So the activation is very positive.",
            "We can see that the slope is indeed converging to be flat, to have a derivative of zero.",
            "And one thing that this implies also is that remember that when I'm performing backpropagation.",
            "Going to do this elementwise multiplication with the gradient that I'm getting from above.",
            "So if I do this, it means that a lot of these gradients I, if I'm at a saturation point, I'm going to multiply by zero or something close to 0, and that's a known issue in neural Nets, which you'll see discuss.",
            "I'll mention so few words about that later on this afternoon, and probably I suspect you will talk about that in the context of stems and R and ends in general, but that means that whenever you add a saturation point, there's no gradient passing through that unit effectively.",
            "And that's a form of issue you can encounter when you are even in practice training neural Nets.",
            "I'll say a few more words about that and trying to avoid these situations this afternoon."
        ],
        [
            "If I look at what is the partial derivative of the tangent function is.",
            "Again we can show that this is this very simple expression, 1 minus the value of the tension squared.",
            "So again, you can see that if you're close to one or one saturation point, you're going to get 1 -- 1 ^2.",
            "So essentially one.",
            "So you're going to get a derivative of 0.",
            "So again, this formula you can see intuitively fits with this function here."
        ],
        [
            "And finally for the rectified linear activation function, super simple gradient.",
            "It is simply one if the pre activation is positive because it's a linear function and it's otherwise zero if the pre activation is negative.",
            "So this is represented by this year.",
            "The identity function of whether the pre activation is larger than 0.",
            "Again, another very simple thing to compute, and then you can use that to back propagate through the activation function."
        ],
        [
            "Any questions on this?"
        ],
        [
            "OK.",
            "So.",
            "All we've done so far is get the gradients with respect to activations and pre activations.",
            "Now we finally need our greatness with respect to our parameters, 'cause those are the parameters that we will be updating with the gradient step.",
            "So."
        ],
        [
            "Let's do that now.",
            "Say I want to get the gradient or the start with the derivative the partial derivative of my loss with respect to the connection between the unit and layer K and the JS unit in the layer below.",
            "Then use the chain rule so I can write it as the derivative with respect to my loss of my last.",
            "Sorry with respect to the ith preactivation at my layer K. Times what is the narrative of this pre activation at layer K for the RF unit with respect to my weight?",
            "The weight between unit I at layer K and unit J at layer below layer K -- 1.",
            "And again because this pre activation function is just a linear transformation.",
            "Take computing that durative very simple so the bias does not depend on the weight, so this goes away and then this whole sum here there's only the term involving.",
            "The unit J with respect to which I'm computing the derivative that matters, and in this case all you will get is so the narrative of the weight multiplied by the activation.",
            "But that's just going to be the activation itself, and so this term here is just replaced by what is the GF activation in my hidden layer K -- 1?",
            "And so now if I wanted to take all these partial derivatives and put them into a gradient format, so get the full matrix of all the partial derivatives, I can see that the element at row I and column J is essentially just, well, take the partial derivative for the KF preactivation.",
            "Sorry, the protective layer K respected that IF unit times the activation of the JF unit at layer K -- 1."
        ],
        [
            "So one thing to obtain this is to just take the gradient vector of the activation of layer K. That's a column vector, and then do an outer product or essentially take the product with the row vector of the activations at layer.",
            "K -- 1 so that means that the entry at the Ro I and the column J.",
            "It's just going to be the IF element of this vector times the JF element of that vector and that is."
        ],
        [
            "Exactly the expression that I have here the IF of element of that vector and a JF element of the activation that layer K -- 1."
        ],
        [
            "So really, getting the gradient of my full weight matrix for the Earth layer is just take my pre activation gradient and do the outer product with the activation gradient.",
            "Sorry actually the activation of the gradient, the activation at the layer below.",
            "And next we can do the same thing with the bias as we've seen, we can interpret the bias as just a weight vector on the constant unit equals to one.",
            "So, not surprisingly, we essentially get the same formula As for the weight, but instead of H, here we have one.",
            "So we just have effectively the derivative with respect to the pre activation.",
            "So I wanted to construct the full gradient for my fold bias vector.",
            "It is essentially just the preactivation gradient with respect to my full loss.",
            "OK, just a grain of the pre activation."
        ],
        [
            "OK, so let's put this all to."
        ],
        [
            "Heather, and to get finally our algorithm, the algorithm that will give us for some given example, XY, what is the gradient of the loss function with respect to all my parameters?"
        ],
        [
            "Anne, what they will essentially do is go through all these different gradients that we've seen in order such that I can always be used computations I've done before to get my computations that I need next.",
            "And the way we do this is we first compute what is the gradient of my loss with respect to the.",
            "We could do.",
            "The activation will do the pre activation since its formula is simpler.",
            "Add the output layer so AL plus one that's the pre activation the output layer and we've seen that the gradient of the loss for this pre activation is just minus the one hot vector minus the output.",
            "So after the softmax.",
            "So I compute this and I'm going to go from the output layer towards the first hidden layer.",
            "So 4K from L + 1 down to one.",
            "And now.",
            "Once I'm at the given layer K, well, I've already, so when I'm starting this iteration, I'm assuming that I've already computed the gradient of the pre activation for that layer.",
            "So in other words I have what is the gradient of my loss with respect to my pre activation?",
            "So the first thing I'm going to do in my iteration is get my gradients with respect to my parameters for that layer.",
            "For layer came well, what is that?",
            "That's just I take my pre activation gradient, I do the outer product with the activation of the layer below.",
            "So here in this algorithm, I assume that I've already performed the forward propagations already went from X up to the output, so I've computed all the activations, all layer.",
            "I've computed my output, so this simple operation now gives me the gradient for the weights, and now if I want the gradient for the bias, that's just haven't shown the gradient of the loss with respect to that pre activation at the layer cake.",
            "And the next I'm going to take the gradient on the activations and push it down to the next layer K -- 1 so that the next iteration I can perform again the same computations to get the gradients for the layers below.",
            "So I can go from the gradients of the loss with respect to my pre activation to get the gradient on the loss with respect to the activation at the layer below.",
            "As I said by just just taking the gradient of the pre activations and multiplying that by the transpose of the weight matrix.",
            "Simple operation and then next I take what I just computed the gradient on the activation, which is right here.",
            "I do an elementwise multiplication with the narrative of the activation function.",
            "Again, other simple computation, and that gives me now the gradient of the loss function with respect to the activation of the layer below, and then I continue my iteration.",
            "So now K becomes K -- 1 and continue like this until I reached the first hidden layer and at the first sentence layer.",
            "You don't compute this line because this is the layer below and there's no preactivation at the input layer.",
            "OK.",
            "So that is the backpropagation algorithm.",
            "As you can see, it's complexity is essentially the same as doing a forward propagation for propagation.",
            "You do computations for each layer, one after another.",
            "Backpropagation you do the similar type of computation, but you go from the top layer and you go down."
        ],
        [
            "One way we can implement backdrop is to consider this perspective on the forward propagation and backpropagation neural Nets.",
            "So what we can do is just represent all of these computations in the forward prop pass as computation graph or flow graph.",
            "So when I'm performing forward propagation, so when I'm trying to compute what is my output of my layer and then ultimately what is the loss for some given input X and some target Y?",
            "Well, I need to take my input X and then I need to combine it with my parameter W and my bias before the 1st hidden layer and compute my pre activation.",
            "So it's just a linear transformation taking X, multiplying it by W plus the bias speed.",
            "Then I'm going to apply under pre activation and activation function that gives me the activation H and then I'm going to get the activation at the next layer which requires me to depend on what is the weight at that layer.",
            "What are the biases at that layer?",
            "And then in this case I'll just have one single single hidden layers.",
            "I already had the output going to compute the output I for classification.",
            "I applied the softmax and once I've done the softmax again, finally compute my loss, which is the negative log of the value of the unit for the correct class.",
            "So a nice way to implement the forward propagation.",
            "This is just have you know, essentially, objects that compute that correspond to each of these boxes, and that makes the particular computation associated with these boxes.",
            "So this here would just be an object that does a linear transformation given some input X and weight, and bias that characterizes this linear transformation.",
            "This box here would be whatever activation function I've used.",
            "Maybe I have an object that's for the Sigma and we have an object for the 10 H. Maybe an object for the Relu.",
            "And here I can reuse that same object that did the linear transformation, but here instead it's going to be instantiated to use the weight matrix W and the bias B and so on."
        ],
        [
            "So that would be one way of sort of structuring your code when implementing the forward path.",
            "And what's great is that if you represent everything in this way, you can actually, fairly simply, at least in terms of organization of your code, get a computation of your gradients.",
            "What you can do is then also take your objects that do these different operations an have them, not just have like, say, a function that does the forward propagation where the forward propagation takes essentially the children of these elements and computes the activation.",
            "But the backdrop also, which would just take what is the gradient.",
            "Which of the loss with respect to myself, assuming that I'm obtaining this by?",
            "Note that is following in the following graph and the flow graph.",
            "Sorry, and then take that gradient and then push.",
            "What is the gradient then for all of the arguments of the forward prop function.",
            "So specifically when you do the forward path, you would just follow these arrows here and computing the loss up to here and then when you performing backpropagation you would just call back, probably in the opposite order, say taking OK.",
            "So initially you would compute what is the gradient of the loss with respect to the loss itself?",
            "Well that's just one.",
            "Now you would callback prop on this box here, saying that, well, the loss which is better.",
            "This is just one and then ask it.",
            "Ask this box to pass in here.",
            "What is the gradient of this loss with respect to the output F of X?",
            "And then you could call back Prop on this guy here, which would then pass.",
            "What is the gradient of the loss rich respect to the arguments of F of X, which is a two here the pre activation and then continue calling backdrop in this opposite order here.",
            "So what is nice also about structuring your code like this is that you can.",
            "Essentially it is very modular.",
            "You can construct arbitrary types of architectures with all of these components fitting into one another and get for free the backpropagation algorithm from it.",
            "And there's a lot of different libraries that effectively do this type of which is called what is known as automatic differentiation.",
            "What I've essentially described having objects that have a forward pass in the backdrop pass function is essentially hard, torch works.",
            "Then there are slightly more sophisticated approaches and libraries that instead of essentially have these objects in this way, will actually give you a functional form of performing all of these operations.",
            "Theano works like this.",
            "There's torch autograph that works like this tensor flow works like this, and thankfully this week you'll get to see how they each different all the implement these notions of automatic differentiation.",
            "So this means to say that all these derivations that I've done in practice, technically, you never have to do all of them.",
            "You do need are ever to do it if you need to.",
            "If you want to implement a new module 4, which for some reason your library doesn't support and in this case you need to be familiar with how to actually compute transformation.",
            "Also be able to derive and implement how to get the gradient of the arguments of that transformation.",
            "A story of the story of the output of that transformation with respect to its argument.",
            "That is common skill that you have to develop if you want to apply and perform research in neural network.",
            "Any questions on this part, yes.",
            "Right, so how do people in the loops that was the question?",
            "I almost got the warning so you.",
            "More about this when you see the tutorial on Torch and Theano and Tensorflow for Theano, it is handled with the scan Operation Torch autograph as a different way of doing it, where you can actually just write your for loop and it will essentially construct a tape that corresponds essentially to the these operations and then go towards the tape through the tape backwards.",
            "Alex Wesco will describe that a bit more in the torch tutorial, and I think Tensorflow as.",
            "Or will have a version of scan at some point that I'm less familiar with, but this will be, I think, covered in the practical tutorials.",
            "And you can explicitly.",
            "That's a good point.",
            "So you also mentioned you can also just explicitly unfold in time the four loop.",
            "Which is another way of doing it.",
            "Other questions.",
            "Yes.",
            "Yes.",
            "Yeah, so if.",
            "Yeah, so if there usually well one approach is to have essentially these components that correspond to computing the forward pass for the Relu, and then within that component you also define how you compute gradients and it's within that component that you will make the assumption.",
            "For instance here what I've sort of men barely mentioned is that for the point at zero I was assuming I'll use the gradient or the narrative to be set to 0 and that you would implement that within that component for the.",
            "Drop method or the backdrop pass.",
            "Yes.",
            "Yeah.",
            "Often times.",
            "It's just much faster to do it from like.",
            "Objects.",
            "Just like everyone compares all the time benchmarks.",
            "The fundamental idea, which is faster in practice?",
            "Yeah.",
            "So I guess the question is amongst all these different packages, Theano to Grafton.",
            "So which one is faster and why?",
            "I guess or in which cases I mean?",
            "I so just maybe do not take too much time.",
            "I would prefer you know, pushing that question when you see the practicals because I think this is a fairly sophisticated like it requires sophisticated answer and understanding the specifics of each case.",
            "I can say from my limited knowledge, which is mostly focused around torch and torch.",
            "Undergrad is that.",
            "Ticular will be pretty convenient when you have a computation graph that tends to vary a lot for a given input, whereas I think the annual intensive deals with that a bit less well.",
            "Torch autograph will have a certain overhead in certain situations, over that you know will not have with tensor flow will not have.",
            "This will be covered later this week, but those are.",
            "Thanks to this is, I think, partly why you have all of these different presentations this week is that there's no one single package for every situation, at least as we speak right now.",
            "Would argue, and it's good to know all these different approaches because behind the scene there are sometimes substantially different, and there's a whole literature on automatic differentiation that actually goes much beyond what is currently implemented in some of these packages, and I think I suspect in.",
            "Future years of research and developments in deep learning will see us taking more and more advantage of some of these tricks.",
            "Is that if you do similar differentiation automatic differentiation, then in principle you can have it is done in Theano.",
            "Once you've done the differentiation, what you have is a symbolic graph that describes the computation.",
            "So now you can apply compiler tricks to make that the computation where efficient are more likely more stable.",
            "Whereas if you have something that the standard torch way where you know you just go and do those.",
            "Calculate the backdrops of each module.",
            "You can't do that.",
            "Also, there is some computations, derivative computations that you will be able to do with things like you know, for example, imagine second derivatives you apply twice, the first ever.",
            "You can't do that with the porch approach, but of course this these are a bit exotic in practice in terms of speed, it's more less the same thing.",
            "Yeah.",
            "Other questions.",
            "What's OK, I worked hard this animation animation.",
            "So let's look at it again, OK?"
        ],
        [
            "Alright, and that's it.",
            "So yeah, so later on finish the training algorithm and talk about deep learning after lunch."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright thanks Joshua so everyone.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I've been tasked to essentially give an introduction on neural Nets, so a lot of what I'm going to talk about is fairly basic concepts, But those are concepts that you'll be building on for the rest of this week.",
                    "label": 0
                },
                {
                    "sent": "I mentioned this just so you're aware of the amount of pressure Aaron this put on me from this talk.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So well, especially cover is basic.",
                    "label": 0
                },
                {
                    "sent": "Feedforward neural net multilayer perceptrons is another way people describe them.",
                    "label": 0
                },
                {
                    "sent": "What I'll go through our first talk about essentially, given some input X, how we make predictions using a neural net where I'll refer to F of X as the output.",
                    "label": 0
                },
                {
                    "sent": "That is, if you're already somewhat familiar with neural Nets, we take input vector X and people are nuts.",
                    "label": 1
                },
                {
                    "sent": "Will compute a series of layers of features which are hidden layers, and then eventually, ultimately computer.",
                    "label": 0
                },
                {
                    "sent": "Output layer that serves to make a prediction about the label for some input X, and I'll focus mostly on classification during my talk.",
                    "label": 0
                },
                {
                    "sent": "And so I'll talk about first how you compute all of these units in the neural net.",
                    "label": 0
                },
                {
                    "sent": "So do forward propagation talk about the different types of units, activation functions that people tend to use have a little bit of discussion as to what is the capacity of neural Nets and why they're interesting in terms of their ability to model various complicated functions.",
                    "label": 1
                },
                {
                    "sent": "And then I'll move on to actually talk about.",
                    "label": 0
                },
                {
                    "sent": "Given that we've accepted that this is the type of model multilayer neural net that we want to fit to our data.",
                    "label": 0
                },
                {
                    "sent": "How do we actually fit it on data?",
                    "label": 0
                },
                {
                    "sent": "How we train neural networks?",
                    "label": 0
                },
                {
                    "sent": "So I'll discuss the type of loss function we use for classification talk about the backpropagation algorithm, which will allow us to get gradients to perform some gradient based learning and describe some of these gradient descent algorithms are a few other tricks of the trades kind of information that you'll need for applying neural Nets in practice, and ultimately I'll talk a little bit more about motivating the case of deep neural network and talk about some of the more recent.",
                    "label": 0
                },
                {
                    "sent": "Advances that have surfaced in the literature that were sort of inspired by this ambition of trying to learn deeper neural networks.",
                    "label": 1
                },
                {
                    "sent": "So things like dropout, batch normalization and unsupervised pretraining.",
                    "label": 0
                },
                {
                    "sent": "This is for the two lectures, yes, so I am not crazy so.",
                    "label": 0
                },
                {
                    "sent": "Thanks for the sanity check.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the first part what I'll do is I'll cover forward propagation and backpropagation, mainly only computing the gradients.",
                    "label": 0
                },
                {
                    "sent": "Then we'll talk about that.",
                    "label": 0
                },
                {
                    "sent": "We have lunch and then after that we talk about the complete learning algorithm for fitting a neural net on data and then talk about the deep learning part.",
                    "label": 1
                },
                {
                    "sent": "Out of curiosity, who has seen you know, forward propagation and back propagation before you raise your hand.",
                    "label": 1
                },
                {
                    "sent": "OK, it's going to be super useful.",
                    "label": 0
                },
                {
                    "sent": "So what did you ask me to present this error so hopefully it'll be like watching a movie you really like for the second time or.",
                    "label": 0
                },
                {
                    "sent": "And hopefully it will not be like when you watch a movie you liked and you realize all the special effects weren't that great.",
                    "label": 0
                },
                {
                    "sent": "So alright, let's get going.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A non forward propagation and I'll just start simple.",
                    "label": 0
                },
                {
                    "sent": "Let's just start with describing what do we mean by an artificial neuron?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "An artificial neuron will be essentially characterized by a few things.",
                    "label": 0
                },
                {
                    "sent": "It will take some input X and it will have a set of weights which are connections between the inputs and the activation.",
                    "label": 0
                },
                {
                    "sent": "Their neuron itself will have a bias and will have an activation function and the way we will compute the activation of a neuron is that we will first take the bias.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to 1st compute what I call the pre activation function.",
                    "label": 0
                },
                {
                    "sent": "I'm noting a of X.",
                    "label": 0
                },
                {
                    "sent": "It's going to be a bias plus a linear combination of the inputs.",
                    "label": 0
                },
                {
                    "sent": "Where each input dimension in my vector X is weighted by some way WI the weights W eyes are just the elements.",
                    "label": 0
                },
                {
                    "sent": "The dimensions in my weight vector W. So this some I can represent it as vector operations by SB plus the dot product between my weight vector W an my input X.",
                    "label": 0
                },
                {
                    "sent": "That's how I get my pre activation and we actually get the activation and sometimes will refer as the output of a neuron will just be some nonlinear function G which is our activation function applied on the pre activation.",
                    "label": 0
                },
                {
                    "sent": "So if I unroll everything it's that equation here an so this is kind of a visual illustration of what are neuron is and often the bias will just represent it as actually a weight of a unit that is constant and equal to 1.",
                    "label": 0
                },
                {
                    "sent": "So that's another way of representing that full sum as just multiplying an extended vector with W&B, added appended to that vector.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So OK, what kind of functions can we represent with an artificial neuron?",
                    "label": 0
                },
                {
                    "sent": "So we can just plug the output of some neuron if I want to do that in two dimensions?",
                    "label": 0
                },
                {
                    "sent": "'cause that's easier to visualize.",
                    "label": 0
                },
                {
                    "sent": "So here we have one axis, where have the first dimension of some input, and here second axis of 2nd second access for the second dimension, my input.",
                    "label": 0
                },
                {
                    "sent": "And here this is just the output or the activation of my neuron.",
                    "label": 0
                },
                {
                    "sent": "And so typically what we have is that in some region of the input space will have is essentially a flat and small value, and then as I move along in the direction of the vector W, eventually the activation starts increasing.",
                    "label": 0
                },
                {
                    "sent": "That's what we typically get, and often we can even get a kind of Ridge that separates do two regions.",
                    "label": 0
                },
                {
                    "sent": "So this is the kind of function you can represent and the bias B will determine where along vector W that Ridge will be.",
                    "label": 0
                },
                {
                    "sent": "And the range of the values taken by the activation of a neuron is going to be determined by the choice of my activation function.",
                    "label": 0
                },
                {
                    "sent": "That's a single neuron.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or at least do interrupt me at any point during the presentation.",
                    "label": 0
                },
                {
                    "sent": "If you have questions, I'll be happy to make this more interactive and discuss some of the details so.",
                    "label": 0
                },
                {
                    "sent": "I talked about.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bashan functions.",
                    "label": 0
                },
                {
                    "sent": "What are the choices one would be to essentially have no activation function, so I have a linear neuron.",
                    "label": 1
                },
                {
                    "sent": "This just means that the activation function on my pro activation is just the pre activation itself, but that's not very interesting.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We often have sigmoid units.",
                    "label": 0
                },
                {
                    "sent": "You have you seen that in doing a stock order this morning so the sigmoid function has the property that essentially squashes the pre activation between zero and one.",
                    "label": 1
                },
                {
                    "sent": "As you can see here.",
                    "label": 0
                },
                {
                    "sent": "So this is this axis, the pre activation and that's the activation of the neuron.",
                    "label": 0
                },
                {
                    "sent": "You can see this squash between zero and one, it's always positive it is bounded and is strictly increasing.",
                    "label": 1
                },
                {
                    "sent": "So the higher the activation the higher the activation and the actual mathematical formula for the sigmoid.",
                    "label": 0
                },
                {
                    "sent": "Is this year it's 1 / 1 plus the exponential of the negative preactivation.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another popular choice is the 10 shore hyperbolic tangent function in this case.",
                    "label": 1
                },
                {
                    "sent": "Mainly what changes its.",
                    "label": 0
                },
                {
                    "sent": "It's quite similar to a sigmoid function, but instead its activation ranges between minus one and one, and so you have this somewhat more complicated formula, which is the exponential of the reactivation minus the exponential of minus the preactivation divided by the exponential deactivation plus the exponential of the negative pre activation.",
                    "label": 0
                },
                {
                    "sent": "This is one way of computing it, you could.",
                    "label": 0
                },
                {
                    "sent": "Multiply the numerator and the denominator by the exponential of the activation exponential of A and then you would get this formula which is slightly more interesting.",
                    "label": 0
                },
                {
                    "sent": "Just because here we only have to compute one exponential, the exponential twice the pre activation and the exponential is more computationally expensive to compute.",
                    "label": 0
                },
                {
                    "sent": "So that can be a more convenient form.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Passionately.",
                    "label": 0
                },
                {
                    "sent": "And finally, probably right now the most popular activation function is the rectified linear activation function.",
                    "label": 0
                },
                {
                    "sent": "We often refer to those as the rare or.",
                    "label": 0
                },
                {
                    "sent": "This is painful Relu OK for rectified linear unit.",
                    "label": 0
                },
                {
                    "sent": "So if you see this in papers, this is referring to the usage of this activation function.",
                    "label": 0
                },
                {
                    "sent": "The rectified linear activation function.",
                    "label": 1
                },
                {
                    "sent": "It is bounded below by zero, but it is not upper bounded.",
                    "label": 1
                },
                {
                    "sent": "It's also increasing, not strictly though, 'cause here it's not.",
                    "label": 0
                },
                {
                    "sent": "It's actually flat.",
                    "label": 0
                },
                {
                    "sent": "That's a mistake and then practice.",
                    "label": 0
                },
                {
                    "sent": "It will tend to give when you train a neural net with Relu units.",
                    "label": 0
                },
                {
                    "sent": "It will tend to give you sparse activities, which is property we observe empirically, and the function itself is simply the maximum between zero and the pre activation.",
                    "label": 0
                },
                {
                    "sent": "So if the pre activation is larger than zero you get a linear function, but if it's below 0 you get a flat 0 output.",
                    "label": 0
                },
                {
                    "sent": "0 activation alright.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Any questions on that?",
                    "label": 0
                },
                {
                    "sent": "OK, So what is the capital capacity of a single neuron?",
                    "label": 1
                },
                {
                    "sent": "What kind of things can we achieve an represent using?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Single neuron, so we've seen this picture already and you can just looking at this already.",
                    "label": 0
                },
                {
                    "sent": "See that it seems we are able to represent linear decision boundaries.",
                    "label": 0
                },
                {
                    "sent": "So perform linear classification.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So indeed, if we think of a single neuron and we use the sigmoid activation function, one thing we could do if we wanted to take the probabilistic approach of classifiers that do not describe this, to think of the activation of that neuron as representing what is the probability that for some given input X, the label associated with my input is 1.",
                    "label": 0
                },
                {
                    "sent": "So think of in my notation, I'm going to assume sometimes a binary classification problem, which case the positive class is 1.",
                    "label": 0
                },
                {
                    "sent": "And the negative class going to be 0, so this actually corresponds to the model behind logistic regression that donor presented.",
                    "label": 0
                },
                {
                    "sent": "And so if we wanted to make a prediction that is assigned a label to some given input, we would just compute our activation of the neuron linear transformation followed by sigmoid and we would threshold at 0.5.",
                    "label": 0
                },
                {
                    "sent": "Why will if that represents the models predictive distribution over the label Y?",
                    "label": 0
                },
                {
                    "sent": "If it's it's values, how to?",
                    "label": 0
                },
                {
                    "sent": "Five it means it thinks it's more likely that it belongs to the class one in the Class 0, so that's how you would get a classifier performing classification using a single neuron, as one can easily see that you would essentially be drawing a line in your input space between the positives and the negatives.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what kind of functions could be represents?",
                    "label": 0
                },
                {
                    "sent": "I'm going to use a very simple example of Boolean functions, which is a classic example for explaining why we want more than just a single artificial neuron, so it's also even though it's simple, it's kind of interesting in the sense that you know if we would like to use neural Nets to perform some kind of logical reasoning.",
                    "label": 0
                },
                {
                    "sent": "We would probably want such functions neural Nets to be able to represent certain simple at least simple Boolean operations.",
                    "label": 0
                },
                {
                    "sent": "Some representing three different Boolean operations.",
                    "label": 0
                },
                {
                    "sent": "So here true is represented by one and falls by zero.",
                    "label": 0
                },
                {
                    "sent": "So if you have the order function, the OR function with output true or one if any of its input X1 and X2 is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "So in this case, so the triangles would be the positive class, so we can see if either this dimension on that dimension is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "We are in the positive class and output of one, and otherwise if both are zero we get a 0.",
                    "label": 0
                },
                {
                    "sent": "So we can see that in this case.",
                    "label": 0
                },
                {
                    "sent": "It is easy to draw a line that separates all the positives from the negatives, so we can represent that with a single neuron.",
                    "label": 0
                },
                {
                    "sent": "Another function is the end function.",
                    "label": 0
                },
                {
                    "sent": "Here I'm showing the end function on the negative of the first argument and the second argument.",
                    "label": 0
                },
                {
                    "sent": "So in this case and will be equal to 1 or true only if both arguments are one.",
                    "label": 0
                },
                {
                    "sent": "So it means that X one, since I'm taking it's negative needs to be 0 and then X2 needs to be one.",
                    "label": 0
                },
                {
                    "sent": "So that's why I have a positive example here.",
                    "label": 0
                },
                {
                    "sent": "Now the others, either one is going to be 0, so I get the negative class and again in this situation I can easily chop the space using a line into the positives and the negatives and same thing for North where I would apply the negative on another argument.",
                    "label": 0
                },
                {
                    "sent": "So all of these functions I can represent with a single neuron.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, I take a slightly more complicated, but really not that complicated function.",
                    "label": 0
                },
                {
                    "sent": "The X or function which is equal to 1 outputs true if either one is equal to true and the other files or vice versa.",
                    "label": 0
                },
                {
                    "sent": "So this case in this case this would be a positive example 01 and 1 zero with the other with an output of one and any other combinations to ones or two zeros would be correspond to an output of false or 0.",
                    "label": 0
                },
                {
                    "sent": "So we can see it in this situation we cannot draw a line in separate perfectly.",
                    "label": 0
                },
                {
                    "sent": "All the positives on one side and all the negatives on the others.",
                    "label": 0
                },
                {
                    "sent": "If I was to draw decision boundary that's linear.",
                    "label": 0
                },
                {
                    "sent": "This is actually kind of a famous example.",
                    "label": 0
                },
                {
                    "sent": "It was shown it was using historically an perceptrons book to show that a single neuron has some limitations.",
                    "label": 0
                },
                {
                    "sent": "However, much like Donut describe, maybe I can transform my input space into another representation and then get a problem that's simpler.",
                    "label": 0
                },
                {
                    "sent": "And in this case can be separated by a single neuron by.",
                    "label": 0
                },
                {
                    "sent": "Their decision boundary.",
                    "label": 0
                },
                {
                    "sent": "So what I'm showing here on this side is what if I take my input space an instead?",
                    "label": 0
                },
                {
                    "sent": "Now the two dimensions with which I'm going to draw my inputs.",
                    "label": 0
                },
                {
                    "sent": "My examples, the first one is going to be the end over the negative of the first dimension, an X2 and the second dimension is going to be the end of the first mention, and the negative of X2.",
                    "label": 0
                },
                {
                    "sent": "So in this case this example here 00 would have an end for both.",
                    "label": 0
                },
                {
                    "sent": "Both this and then this NB0, so it would actually map to this point here.",
                    "label": 0
                },
                {
                    "sent": "And actually this point also if we run through the calculation of both ends here will also wrap into the same point.",
                    "label": 0
                },
                {
                    "sent": "And we can go through the derivations and find out that for both these points they also lie essentially at the same place.",
                    "label": 0
                },
                {
                    "sent": "So in this case I can actually draw Now a line between the positives and negatives and the thing to notice here is that the function that I've used to now draw my data differently are these two functions that I shown before that I could actually represent with a single neuron.",
                    "label": 0
                },
                {
                    "sent": "So what this example suggests is that if I first compute a few set of neurons.",
                    "label": 0
                },
                {
                    "sent": "And then I have some output neuron that is connected to these intermediate neurons and now I can start modeling nonlinear decision boundaries and in this case I can actually perform model this this function here the X or function and so that is at the core like why we are interested in neural Nets.",
                    "label": 0
                },
                {
                    "sent": "While neural Nets interesting is that they allow us to go beyond linear classifiers, an actually model nonlinear decision boundaries.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any questions so far?",
                    "label": 0
                },
                {
                    "sent": "Alright, so that brings us to multi nerer neural net.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to start simply with a single hidden layer neural network, so in this case what this means is that between the input and the output I'm going to have a layer of what I call hidden units.",
                    "label": 0
                },
                {
                    "sent": "Each of these units are like artificial neurons, that is, they are connected with the previous layer, the input, and they perform a linear transformation of the input and then apply some activation function.",
                    "label": 0
                },
                {
                    "sent": "OK, so you're seeing here essentially a.",
                    "label": 0
                },
                {
                    "sent": "Slightly different notation in the context of 1 hidden layer neural net, but we see the same form for the preactivation.",
                    "label": 0
                },
                {
                    "sent": "It's a bias.",
                    "label": 0
                },
                {
                    "sent": "So in this notation I'm using the exponent to index the layer.",
                    "label": 0
                },
                {
                    "sent": "So this is B1 because there's a bias for the units at the first hidden layer.",
                    "label": 0
                },
                {
                    "sent": "And then this index I is to indicate that this is for.",
                    "label": 0
                },
                {
                    "sent": "This is the bias for the IF layer.",
                    "label": 0
                },
                {
                    "sent": "Sorry IF unit in the first hidden layer.",
                    "label": 0
                },
                {
                    "sent": "And so I'm going to take that bias to get my pre activation plus summing over the contribution of all the input units.",
                    "label": 0
                },
                {
                    "sent": "So all the units in the previous layer, weighted by the corresponding weight of the connection between the JF input and the IF hidden unit, the one for which I'm showing the computation of the pre activation.",
                    "label": 0
                },
                {
                    "sent": "And again here this matrix W which contains all the weights between all pairs of hidden unit and input unit.",
                    "label": 0
                },
                {
                    "sent": "I'm storing them into this matrix W, one that I'm.",
                    "label": 0
                },
                {
                    "sent": "Using Exponent two index to refer again to the first hidden layer.",
                    "label": 0
                },
                {
                    "sent": "And I can actually write the computation of all pre activations of all units in the first hidden layer in vector form.",
                    "label": 0
                },
                {
                    "sent": "So I can just take the bias vector B1.",
                    "label": 0
                },
                {
                    "sent": "And then I, as you've seen here, what this operation corresponds to effectively is to take the vector X and multiply it by the row the I throw of my matrix W. So if I just take the product of X, multiplying on the right with my matrix W, what I'll end up doing is for each row I'm going to dot product between X and the corresponding row of weights, which means that I get all the contributions to the pre activation for all hidden units.",
                    "label": 0
                },
                {
                    "sent": "So to sum up, my preactivation is just this vector valued linear transformation on my previous layer, the input layer X.",
                    "label": 0
                },
                {
                    "sent": "And then as per usual.",
                    "label": 0
                },
                {
                    "sent": "For regular units I get my hidden layer activation by applying my choice of activation function G over my vector, applying it elementwise.",
                    "label": 0
                },
                {
                    "sent": "And then finally to get my output, imagine performing binary classification.",
                    "label": 0
                },
                {
                    "sent": "So at the output I would like a neuron between zero and one that gives me the probability of belonging to the positive class.",
                    "label": 0
                },
                {
                    "sent": "Well, again, going to take a bias so B2 Now because I'm at the second layers, which is the output layer and I'm going to multiply by some weight vector connecting my output unit with all the hidden units that I'm noting W 2 multiplied by the activation of the first hidden layer which is H1.",
                    "label": 0
                },
                {
                    "sent": "So here I should probably have a.",
                    "label": 0
                },
                {
                    "sent": "One here.",
                    "label": 0
                },
                {
                    "sent": "So that's for a single layer neural network.",
                    "label": 0
                },
                {
                    "sent": "Any questions on the notation?",
                    "label": 0
                },
                {
                    "sent": "Or anything related to this slide.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I describe the case of binary classification more generally.",
                    "label": 0
                },
                {
                    "sent": "More commonly we would be interested in classifying into multiple class.",
                    "label": 1
                },
                {
                    "sent": "So performing multiclass classification.",
                    "label": 1
                },
                {
                    "sent": "So in this case what it means is that an input might belong to one out of C, capital C, different classes, and so if I wanted to do this, I would like to represent with my neural Nets some probability distribution over all the classes.",
                    "label": 0
                },
                {
                    "sent": "So unconditional distribution of my input.",
                    "label": 0
                },
                {
                    "sent": "X belonging to one of the capital C different classes, so I'm noting that mathematically as the probability of Y, my label YB equal to some small C which is a choice between 0 and C -- 1.",
                    "label": 0
                },
                {
                    "sent": "Or actually I'm indexing from one to capital C here.",
                    "label": 0
                },
                {
                    "sent": "So one way to get these probabilities is essentially have an output layer that is always positive and whose activations sum to one and there's a very simple way of achieving this with the what is known as the softmax activation function.",
                    "label": 1
                },
                {
                    "sent": "All that it does is that it takes the pre activation vector at my output layer.",
                    "label": 1
                },
                {
                    "sent": "These are the days at the output layer and then I'm going to take the exponential on each dimension.",
                    "label": 0
                },
                {
                    "sent": "And finally I'm going to divide each dimension by the sum of the exponentiated.",
                    "label": 0
                },
                {
                    "sent": "Reactivations so you can see that writing it this way all my dimensions is going to be normalized.",
                    "label": 0
                },
                {
                    "sent": "That is, if I was to sum over all the values in that vector because I've explicitly divided each dimension by that sum, I'm going to get a sum that is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "So because of this I can interpret the output as a multinomial distribution over my input X belonging to one out of the C different classes.",
                    "label": 1
                },
                {
                    "sent": "And if I wanted to actually perform classification with a model that had a softmax output, I would just look at which unit has the highest activation.",
                    "label": 0
                },
                {
                    "sent": "That is, according to my neural net.",
                    "label": 0
                },
                {
                    "sent": "What is the class that has the highest probability given my input X?",
                    "label": 0
                },
                {
                    "sent": "And that's how it would perform.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can go from 1 hidden layer to multiple hidden layers, so I'm just extending my notation here where I'm now I'm using K as some arbitrary hidden layer within my neural net.",
                    "label": 0
                },
                {
                    "sent": "Going to use capital L as the number of hidden layers.",
                    "label": 0
                },
                {
                    "sent": "So in this case pre activation and some layer K is just going to be a linear transformation of my previous layer.",
                    "label": 0
                },
                {
                    "sent": "So in my notation, to make this equation work, I'm going to assume that H zero of X is actually the input.",
                    "label": 0
                },
                {
                    "sent": "So the layer below the first hidden layer is the input.",
                    "label": 0
                },
                {
                    "sent": "And so now the pre activation is just this linear transformation taking my weight matrix, multiplying it by the activation of the layer below with some bias.",
                    "label": 0
                },
                {
                    "sent": "And finally, I'm going to get the hidden layer activation by applying my activation function an.",
                    "label": 0
                },
                {
                    "sent": "I do the same thing for the output, but typically for the output layer I'm going to use a different activation function and as I've described if I'm performing multi class classification, I'm going to use the softmax for the activation function at the output and for their hidden layers.",
                    "label": 0
                },
                {
                    "sent": "I might use the sigmoid.",
                    "label": 0
                },
                {
                    "sent": "I might use a 10 H quite frequently.",
                    "label": 0
                },
                {
                    "sent": "Will actually use the relative the rectified linear activation function.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, any?",
                    "label": 0
                },
                {
                    "sent": "Questions so far with any of this, probably yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There were describing it as a row vector is the transpose just to get into the vector?",
                    "label": 0
                },
                {
                    "sent": "Yeah, excellent point.",
                    "label": 0
                },
                {
                    "sent": "So my assumption when I'm writing vectors is that they are column vectors and so because if I write it this way, it suggests that it's a row vector.",
                    "label": 0
                },
                {
                    "sent": "The transpose here is to emphasize I'm taking that role vector.",
                    "label": 0
                },
                {
                    "sent": "I'm transposing it, so by default if you see a vector notation then I that's a good point.",
                    "label": 0
                },
                {
                    "sent": "You mentioning this I eventually emphasized that.",
                    "label": 0
                },
                {
                    "sent": "So whenever I use a bold type, it's to refer an the lower case.",
                    "label": 0
                },
                {
                    "sent": "It's usually to refer to a vector.",
                    "label": 0
                },
                {
                    "sent": "If I'm using bold, but for a capital letter is usually going to be a matrix, and if it's italic instead of like this one here instead of bold, that's going to be a scalar.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to try to hopefully help out understand whether we are referring to scalar of vector.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to stay true to that convention.",
                    "label": 0
                },
                {
                    "sent": "Great point, yes.",
                    "label": 0
                },
                {
                    "sent": "So the rule of thumb is used relative.",
                    "label": 0
                },
                {
                    "sent": "That tends to work really well, but I've seen examples where it turns out that the sigmoid for some reason is better, or the 10 H. Take that, I don't know if yes or N as like their own rule of thumb, but typically I start with the relevant and maybe I will try the Sigma 10H if for some reason I'm not satisfied with the results, that would be my rule of thumb, but kind of yeah.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Or I mean, I can't say that I've seen a lot of Oh yes, thank you.",
                    "label": 0
                },
                {
                    "sent": "So you will have to tell me that often.",
                    "label": 0
                },
                {
                    "sent": "So the question is, should we use different activations at different layers at different positions in the different hidden layers of a network?",
                    "label": 0
                },
                {
                    "sent": "I haven't seen a whole lot of success in actually kind of fine tuning the type of activation you would use at different places in a neural net, so I would say that usually that's not the biggest bang for the Buck in terms of performing more experiments.",
                    "label": 0
                },
                {
                    "sent": "There is some research on trying to make the activation function learnable in some way, and that research is interesting, but I can't say I've seen like tremendous success in terms of like improving performance in a way that's particularly meaningful.",
                    "label": 0
                },
                {
                    "sent": "Some special structures like game?",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "So except for the yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "So except if you want to put structure in terms, for instance, there's forms of attentional mechanisms which correspond to.",
                    "label": 0
                },
                {
                    "sent": "Using different types of connections between units and different forms of activations that are inspired by trying to emulate some activations or some form of processing, and in this case I think it makes sense to reason in that space, but the individual unit and the regular sort of power, like layers that are stacked.",
                    "label": 0
                },
                {
                    "sent": "I've been seeing a whole lot of gains and trying to really tune the individual choices of units across layers.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Oh yes, so yes.",
                    "label": 0
                },
                {
                    "sent": "So we will need so it's a yes is the is it important for the activation to be differentiable?",
                    "label": 0
                },
                {
                    "sent": "It will be if we want to perform gradient descent because then we will require gradients and so we go through that in a bit.",
                    "label": 0
                },
                {
                    "sent": "But if the activation function wasn't differentiable then you need to leverage sort of more advanced principles for training forms of reinforcement learning, for instance.",
                    "label": 0
                },
                {
                    "sent": "So if you have stochastic units, for instance for which you cannot.",
                    "label": 0
                },
                {
                    "sent": "Back propagate so get a gradient through the sampling.",
                    "label": 0
                },
                {
                    "sent": "That's an example where you use an algorithm like reinforce so, but typically like it's a pretty good thing to have a activation function for which you can compute.",
                    "label": 1
                },
                {
                    "sent": "Gradients could.",
                    "label": 0
                },
                {
                    "sent": "Then you can do end to end learning with gradient descent.",
                    "label": 0
                },
                {
                    "sent": "So it is differentiable at every point except the inflection point at which we can get a subgradient.",
                    "label": 0
                },
                {
                    "sent": "And there's some theory that suggests that doing subgradient descent, it's still fine.",
                    "label": 0
                },
                {
                    "sent": "So in this case that's why it's not the problem.",
                    "label": 1
                },
                {
                    "sent": "The fact that there's no, you don't have an exact durative at the inflection point, but that's a good point.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's the reason.",
                    "label": 0
                },
                {
                    "sent": "Alright, so just to make the division calculation easier or yeah, so the question is why use the exponential here?",
                    "label": 0
                },
                {
                    "sent": "I think yeah, that is 1 good motivation.",
                    "label": 0
                },
                {
                    "sent": "The fact that when you start computing gradients with respect to the activation, when you use the exponential and we use the loss, which is the log likelihood as we'll see in a few, you get fairly simple forms of gradients.",
                    "label": 1
                },
                {
                    "sent": "So I think that is 1 motivation.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Activation is one thing, but someone asked me once and I couldn't think.",
                    "label": 0
                },
                {
                    "sent": "I like Peach mix with inner layer.",
                    "label": 0
                },
                {
                    "sent": "Different activation functions.",
                    "label": 0
                },
                {
                    "sent": "Is there any research that you've ever seen that actually shut up?",
                    "label": 0
                },
                {
                    "sent": "It's useful, or there's a particular reason like mix really sigmoids and Tange?",
                    "label": 1
                },
                {
                    "sent": "I mean 'cause it seems like that could be cocktail or in any particular benefit in any research mixing it with.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so the question is whether there's a benefit to maybe within the layer.",
                    "label": 0
                },
                {
                    "sent": "Tried to vary the types of activation functions I. I don't feel so rare and have any examples, but I can't think of any that's like really compelling.",
                    "label": 0
                },
                {
                    "sent": "Max out which you can think of is a bit of a generalization from Bellevue, where we're basically putting together a bunch of piecewise linear units, right?",
                    "label": 0
                },
                {
                    "sent": "So this is something we've done.",
                    "label": 0
                },
                {
                    "sent": "We did two years ago.",
                    "label": 0
                },
                {
                    "sent": "Ian Goodfellow was First off on that, so that's a case where they are free to learn.",
                    "label": 0
                },
                {
                    "sent": "Like you mention that there are a few examples of this and you could imagine that it would learn different functions within a week.",
                    "label": 0
                },
                {
                    "sent": "So on average it tends to work about the same as value, sometimes a little better, sometimes little worse.",
                    "label": 0
                },
                {
                    "sent": "Somebody's in for speech recognition.",
                    "label": 0
                },
                {
                    "sent": "It seems to work very well, but observation?",
                    "label": 0
                },
                {
                    "sent": "Throwing in like not entirely swipe the standard mixer functions, the second being more than engineering headed stuff.",
                    "label": 0
                },
                {
                    "sent": "There's no necessary.",
                    "label": 0
                },
                {
                    "sent": "Direct.",
                    "label": 0
                },
                {
                    "sent": "So for the for the for the video, oreg Aaron gave an answer.",
                    "label": 1
                },
                {
                    "sent": "It was like, yeah.",
                    "label": 0
                },
                {
                    "sent": "He's actually said that examples like the Max out is a form of learning the activation function.",
                    "label": 0
                },
                {
                    "sent": "To expand on this, actually you can, because when you get the activation, you will ultimately multiply it to get the pre activation at the next layer.",
                    "label": 0
                },
                {
                    "sent": "Going to perform a linear operation.",
                    "label": 0
                },
                {
                    "sent": "It could be that actually linear combinations of certain units within the layer is by itself a more sophisticated activation function.",
                    "label": 0
                },
                {
                    "sent": "Actually this kind of builds into.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I was going to talk next, which is the capacity of neural Nets.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Move through that.",
                    "label": 0
                },
                {
                    "sent": "So just to give you an intuition as to why you can, you can get nonlinearities and what forms of nonlinearities you can potentially model with a neural net.",
                    "label": 0
                },
                {
                    "sent": "And if you view a few examples, those are taken from Pascal Vanzile slides so.",
                    "label": 0
                },
                {
                    "sent": "For instance, you could imagine, as we've seen before, that I could have one single neuron that represents dysfunction.",
                    "label": 0
                },
                {
                    "sent": "Here some sort of Ridge in some direction, and then I could have the same Ridge but shifted by changing the bias with another unit, and then you can imagine that if I take essentially this unit and subtract that unit, I will get as a result a sort of rich.",
                    "label": 0
                },
                {
                    "sent": "That's like a bump in this direction going up, and then going down.",
                    "label": 0
                },
                {
                    "sent": "So this is what I was referring to, where you can take combinations within layer, just linear combinations and effectively.",
                    "label": 0
                },
                {
                    "sent": "It's as if you were getting.",
                    "label": 0
                },
                {
                    "sent": "A more sophisticated activation function to some extent.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you can start imagining doing this, but in multiple directions.",
                    "label": 0
                },
                {
                    "sent": "So here you have something slightly different, which is you have a neuron that increases in One Direction and then the other neuron that starts high and then decreases.",
                    "label": 0
                },
                {
                    "sent": "So this would just be you would assume it would take the same weight vector, but the negative of the weight vector and you could do that in two perpendicular directions and then linearly combining all of these in the appropriate way you would obtain in the original input space.",
                    "label": 0
                },
                {
                    "sent": "A bump at some place.",
                    "label": 0
                },
                {
                    "sent": "So now you can see that, at least in 2D, with four units, you can get a bump positioning above that gives you a high value at some very restricted area in the input space and low everywhere else.",
                    "label": 0
                },
                {
                    "sent": "So then.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's not a big stretch it imagining that if I combine such groups of four units in 2D, I can now easily get fairly complicated decision boundaries where maybe I have a bump right here and then I get a bunch of little bumps here to get.",
                    "label": 0
                },
                {
                    "sent": "Also, the positive class in this area and everywhere else there's no bump, so you get like negative in R2 and then positive value over the threshold for classifying in the positive class in the area are one.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can actually formalize and use that intuition to actually show, and this was shown by Hornick in 1991 that a single hidden layer neural net with linear output unit can approximate any continuous function arbitrarily well as long as you give it enough hidden units.",
                    "label": 1
                },
                {
                    "sent": "As long as the hidden layer is big enough, it is possible to buy and construct well, it would be possible theoretically, construct by and a function that would approximate approximate arbitrarily well some arbitrary continuous function.",
                    "label": 0
                },
                {
                    "sent": "This was all actually applies for Simone and tension.",
                    "label": 1
                },
                {
                    "sent": "Many other hidden layer activation functions.",
                    "label": 0
                },
                {
                    "sent": "And so that's that's a really important result.",
                    "label": 0
                },
                {
                    "sent": "It tells us that technically, if we make the hidden layer large enough, we can pretty much model any function we want.",
                    "label": 0
                },
                {
                    "sent": "So no matter what is the function that generated my data, I have the flexibility within the family of neural Nets to potentially represent that function to learn that function that is responsible for the data I'm seeing in my training set.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This result, however, doesn't tell us anything about how do we actually learn the neural net to represent that function, and so this is where we get to the learning algorithm, which I'll.",
                    "label": 0
                },
                {
                    "sent": "Talk about next.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any question at this point on this first section.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let's talk about backdrop.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm just going to start by formulating the learning problem.",
                    "label": 0
                },
                {
                    "sent": "The problem conceptually of training a neural net on some data.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we'll do is use the paradigm of empirical risk minimization with some regularization, so empirical risk minimization.",
                    "label": 1
                },
                {
                    "sent": "What do we do?",
                    "label": 0
                },
                {
                    "sent": "We frame the problem as follows, so I'm going to use X with the exponentiated index T to refer to the TF input in my data set Y, potentiated T is DTF label.",
                    "label": 0
                },
                {
                    "sent": "In my data set, so F of X, as I've mentioned before, that's going to be the output of my neural net.",
                    "label": 0
                },
                {
                    "sent": "And here I've just added a little bit of notation that makes it explicit that.",
                    "label": 0
                },
                {
                    "sent": "My neural net has parameters and those are represented by Theta, so Theta would in the context of a neural net contain all the weights in the biases in my neural net an I'm going to define a loss function that compares the output on my PF input with the actual true label.",
                    "label": 0
                },
                {
                    "sent": "So it's a sign of loss if my prediction isn't correct.",
                    "label": 0
                },
                {
                    "sent": "For instance, I'm assigning to higher probability on the wrong class an.",
                    "label": 1
                },
                {
                    "sent": "So in empirical risk minimization, what we try to do?",
                    "label": 0
                },
                {
                    "sent": "Is minimize the average loss on my training set the average of all the losses when I'm comparing the output on the training set of my neural net with the actual true label?",
                    "label": 0
                },
                {
                    "sent": "And also usually we add some regularization that do not mention and waited by some hyperparameters, some factor Lambda that will control the tradeoff between a lot of regularization and fitting my data.",
                    "label": 0
                },
                {
                    "sent": "Well, I should say that technically speaking, when we have regularization, we don't talk about empirical risk minimization but structural risk minimization.",
                    "label": 0
                },
                {
                    "sent": "If you want to be precise about things.",
                    "label": 1
                },
                {
                    "sent": "And so, in this context, learning is cast as an optimization problem.",
                    "label": 0
                },
                {
                    "sent": "Learning will correspond to change taking my training set and finding this data.",
                    "label": 0
                },
                {
                    "sent": "This data here, which minimizes this expression.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How do we do this?",
                    "label": 0
                },
                {
                    "sent": "There are multiple different optimization methods and one that's very simple and works really well in practice and I'll expand on slightly more sophisticated versions of this one that works really, really well is the stochastic gradient descent procedure.",
                    "label": 0
                },
                {
                    "sent": "So in stochastic gradient descent, I first start by initializing my parameters in some way and we'll discuss probably after lunch about how to do that properly, and there are certain badly constructed cases you don't want to use, and then for some number of iterations I'm going to take out of my training set a example.",
                    "label": 0
                },
                {
                    "sent": "So a pair of input XT and label YT.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to look at what is the direction in which my loss would for that example and for my regularizer would decrease the most.",
                    "label": 0
                },
                {
                    "sent": "In other words, I'm going to look at what is the gradient of the loss with respect to my parameter, so nebulize the gradient on this loss, and here as an index to nabla, that is not clear and there is an index to noblem using the parameter with respect to which I'm taking the derivatives and also because I have the sum of the loss.",
                    "label": 0
                },
                {
                    "sent": "And the way to regularizer the gradient on all of that distance to some of the gradients.",
                    "label": 1
                },
                {
                    "sent": "So I also get Lambda times my gradient of my regularizer so the gradient tells me in which direction the last N regularization would increase.",
                    "label": 0
                },
                {
                    "sent": "The most I'm going to go in the opposite direction to take a step in the direction in which would decrease the most.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to take a step in that direction, so that direction I'm calling it Delta.",
                    "label": 0
                },
                {
                    "sent": "Here I'm going to multiply by step size.",
                    "label": 0
                },
                {
                    "sent": "Alpha will usually refer to the Alpha as the learning rate, and I'm going to move in that direction.",
                    "label": 0
                },
                {
                    "sent": "That is, I take my current value of my parameters and I just added Alpha times my direction.",
                    "label": 0
                },
                {
                    "sent": "So that gives you a new parameter.",
                    "label": 0
                },
                {
                    "sent": "I'm going to repeat this several times.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's effectively stochastic gradient descent.",
                    "label": 1
                },
                {
                    "sent": "It is great in the sense because as you can see I'm computing gradient and you're going in the descent direction for that gradient, and it is to castec because unlike what don't have shown where you would take the whole objective, the whole average of losses in your training set and take the gradient with respect to the whole average.",
                    "label": 0
                },
                {
                    "sent": "Here I'm actually taking the gradient for a single example that is stochastically drawn from the training set, and it actually in practice often we linearly iterate over the examples.",
                    "label": 0
                },
                {
                    "sent": "And we usually probably inaccurately usually refer to that as stochastic gradient descent.",
                    "label": 1
                },
                {
                    "sent": "So if I want to apply this general procedure here, there's nothing that you know particularly refers to a neural net beyond the fact that I've said that it would be the weights in the biases, but you know, if you have some function F. This is essentially really just an optimization procedure.",
                    "label": 0
                },
                {
                    "sent": "So what I need to do here in the context of my neural net is to specify what is the loss function pacifi a procedure that is as efficient as possible for computing the gradient of the loss with respect to the parameters, and I need to specify a regularizer and.",
                    "label": 1
                },
                {
                    "sent": "And from the regularizer get a gradient on that.",
                    "label": 0
                },
                {
                    "sent": "And also I will need to specify how to initialize my parameters.",
                    "label": 0
                },
                {
                    "sent": "So before the lunch break I'll actually essentially focus on what is the last function and how to get the gradients of the loss with respect to my parameters, which is essentially the backpropagation algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are no questions.",
                    "label": 0
                },
                {
                    "sent": "Let's move to the last function.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as I hinted, we might actually want to.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, sorry.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Structural regularizer any of these other alternative forms like dropout or structural priors or any which we call regularization of some point.",
                    "label": 0
                },
                {
                    "sent": "We're always doing in structural.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Yes repeating the question this is going to be painful so you shouldn't say that ask questions.",
                    "label": 0
                },
                {
                    "sent": "Don't worry about me.",
                    "label": 0
                },
                {
                    "sent": "So the question is whether if we use other forms of regularization like dropout or unsupervised pre training are we then?",
                    "label": 0
                },
                {
                    "sent": "In the family of would we refer to that as structural risk minimization as opposed to empirical risk minimization?",
                    "label": 0
                },
                {
                    "sent": "I believe so actually not sure what is the official.",
                    "label": 0
                },
                {
                    "sent": "You know if you actually have to have this explicit regularization term in what you are optimizing, believe that might be true, but an for drop out, it's not necessarily for deep neural Nets explicit what the regularization term is, so I'm actually not sure that's a good question.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So the last function.",
                    "label": 0
                },
                {
                    "sent": "So as I said, we're going to use the values of the units at the output layer as a predictive distribution over what is the Class C or I guess Y.",
                    "label": 0
                },
                {
                    "sent": "So whether the CF unit and my output layer is going to be interpreted as the probability that my input X belongs to Class C, that is, the model probability is predicted by my model.",
                    "label": 0
                },
                {
                    "sent": "So what we could do one form of learning would be to do maximum likelihood, so that is trying to maximize the probabilities.",
                    "label": 0
                },
                {
                    "sent": "Of the assigned label.",
                    "label": 0
                },
                {
                    "sent": "Then my training set Y for each of my example input examples X because all the math is nicer numerically, it's nicer instead of maximizing the probability is going to maximize the log probabilities, and because I just framed everything into a minimization problem, we're going to minimize the negative of the log probabilities, so my loss function is just going to be minus the log of the value of the YF.",
                    "label": 0
                },
                {
                    "sent": "Input azarius unit in my output layer F of X.",
                    "label": 1
                },
                {
                    "sent": "You can also write this.",
                    "label": 0
                },
                {
                    "sent": "As summing over all possible classes C and I check whether my iterate iterate are C is equal to the true label Y, and if so, this is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "This is just the indicator function, otherwise it's at zero and so when is equal to 1, I'm effectively going to have this term be present in the sum, which is just the log.",
                    "label": 0
                },
                {
                    "sent": "The probability of that Class C, and the reason I write it in this way we have a minus here is that sometimes you will see people reffering as the loss used for training.",
                    "label": 0
                },
                {
                    "sent": "As a cross entropy and in this form it's more explicit that you have a cross entropy.",
                    "label": 0
                },
                {
                    "sent": "It's the cross entropy between this distribution.",
                    "label": 0
                },
                {
                    "sent": "Here that's the output layer of my network and a multi normal distribution that puts probability one on the correct label and zero everywhere else.",
                    "label": 0
                },
                {
                    "sent": "So that's why sometimes also refer to this as the cross entropy.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Let's start computing gradients, so we're going to look at what is the gradient of the loss with respect to computations at the output layer.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we'll slowly make our way towards the parameters.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If I wanted so the greatness is just a vector of partial derivatives and we just I find it more convenient to start by computing the derivatives and then show that from the expression the generic expression for the derivatives there, Rivett if's will get an expression for the vectorial expression for the full gradient.",
                    "label": 0
                },
                {
                    "sent": "So if I wanted to derivative of minus the log of the probability of the correct class with respect to my output for the CF class, well?",
                    "label": 0
                },
                {
                    "sent": "The derivative of minus the log that's going to be minus 1 divided by the argument of the log.",
                    "label": 0
                },
                {
                    "sent": "That's the probability of the correct class under my model and the derivative of F of XY with respect to F of XC.",
                    "label": 0
                },
                {
                    "sent": "But it's going to be 0 if CNY are different.",
                    "label": 0
                },
                {
                    "sent": "So if C is a different class in the correct class, why this is going to be 0?",
                    "label": 0
                },
                {
                    "sent": "And otherwise that's going to be one, so that's why this one here is actually now an indicator function of whether Y is equal to see.",
                    "label": 0
                },
                {
                    "sent": "Now if I want to just take that expression and construct a vector corresponding to the gradient of the loss with respect to all of my outputs, what I'm going to get is, well, all of the terms for all values of C is.",
                    "label": 0
                },
                {
                    "sent": "I'm always have this minus 1 / F of XY, and then the only thing that changes with C is this term here, right?",
                    "label": 0
                },
                {
                    "sent": "And so I have this vector of whether Y is equal to the first time.",
                    "label": 0
                },
                {
                    "sent": "So here for some reason I switched from classes 12C20 to see minus one.",
                    "label": 0
                },
                {
                    "sent": "Just make sure everyone is following so keep you on your feet.",
                    "label": 0
                },
                {
                    "sent": "So if Y is equal to 0, Y is equal to 1 and so on up to see minus one.",
                    "label": 0
                },
                {
                    "sent": "And actually this vector, I'm going to note it down as East of Y or I'm going to refer to it as the one hot vector for label Y.",
                    "label": 0
                },
                {
                    "sent": "So it's just a vector that has a bunch of zeros.",
                    "label": 0
                },
                {
                    "sent": "Except for one one which is at the dimension corresponding to.",
                    "label": 0
                },
                {
                    "sent": "In this case the index Y. OK, so that's a notation I'm going to use throughout.",
                    "label": 0
                },
                {
                    "sent": "So the gradient with respect to the output is just the minus the one hot vector divided by, so scaled by one over the probability of the correct class under my model.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can also go a bit deeper in my network and ask what is the gradient with respect to the pre activation at the output layer.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "So as we will show in a few, it actually has a very simple expression.",
                    "label": 0
                },
                {
                    "sent": "You actually seen this expression and during this talk it's going to be simply minus.",
                    "label": 0
                },
                {
                    "sent": "Weather my output unit C matches the correct label.",
                    "label": 0
                },
                {
                    "sent": "Minus what is the current output?",
                    "label": 0
                },
                {
                    "sent": "So after the activation function for the unit C. OK, so if we put all of this together to get a vector, all we get is minus the one hot vector minus the output.",
                    "label": 0
                },
                {
                    "sent": "So you can see that this gradient essentially comparing what is the ideal output which would be a probability of one for only the correct class and everything zero, and it's comparing that with the current output of the model.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this I haven't shown the duration of this, I will.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do it with you because I find it instructive and because it used to be that we did that in our neural Nets class.",
                    "label": 0
                },
                {
                    "sent": "So you're going to suffer through this if you haven't yet.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I am computing the derivative of my loss minus the log of the property is correct class with respect to the pre activation at my output layer.",
                    "label": 0
                },
                {
                    "sent": "So This is why I have a of L + 1 L plus one is the index of the output layer with respect to some arbitrary output C. So I've already shown that the derivative of minus the log of F of XY that's going to be minus 1 / F of XY because of their ative of the log and then times using the chain rule derivative of F of XY with respect to their pre activation at the output neurons C. Next I'm going to write what exactly is F of XY?",
                    "label": 0
                },
                {
                    "sent": "Well, that's actually the wife component.",
                    "label": 0
                },
                {
                    "sent": "Once I've I've applied the softmax activation function over my pre activation vector at the output layer.",
                    "label": 0
                },
                {
                    "sent": "OK so this allows me to expose now the dependency of five XY with respect to the pre activations at my output layer.",
                    "label": 0
                },
                {
                    "sent": "Next I'm going to write with the softmax is here.",
                    "label": 0
                },
                {
                    "sent": "Notice that I've the softmax over vector is a vector.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I'm indexing the wife component.",
                    "label": 0
                },
                {
                    "sent": "So what I'm effectively going to get is the exponential of the wife preactivation at my output.",
                    "label": 0
                },
                {
                    "sent": "Normalize so divided by the sum of the exponentiated pre activations across the layer.",
                    "label": 0
                },
                {
                    "sent": "So all I've done here is just take that expression and replace it with that.",
                    "label": 0
                },
                {
                    "sent": "Impression and replace it with this.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to use what is the formula where we're taking a derivation so our gradient with respect to a ratio of two functions of some X that we're doing the duration with respect to.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "My function G is the numerator.",
                    "label": 0
                },
                {
                    "sent": "Here my function, my function H in that identity here is this sum over all classes of the exponential and exponentiated reactivations.",
                    "label": 0
                },
                {
                    "sent": "So what is the expression I'm going to take the Gray, the derivative of G of X with respect to X * 1 over my denominator, so one over my Dominator is just this part here and here.",
                    "label": 0
                },
                {
                    "sent": "I'm multiplying by the narrative with respect of G of XG of X.",
                    "label": 0
                },
                {
                    "sent": "Is this with respect to my pre activation?",
                    "label": 0
                },
                {
                    "sent": "And the same thing here, so G of X is right here.",
                    "label": 0
                },
                {
                    "sent": "The square of H of X.",
                    "label": 0
                },
                {
                    "sent": "That's the square of my normalization constant and then multiply by the derivative of the denominator H of X, which is the sum of exponentiated terms.",
                    "label": 0
                },
                {
                    "sent": "So I take the directive of this expression with respect to my pre activation at node C. Alright, so if I take that.",
                    "label": 0
                },
                {
                    "sent": "Narrative here, which respected the exponential of activation at the label Y.",
                    "label": 0
                },
                {
                    "sent": "Well if C and why are different?",
                    "label": 0
                },
                {
                    "sent": "That's going to be 0 because the those are two different numbers to different pre activations.",
                    "label": 0
                },
                {
                    "sent": "So This is why I have the indicator function checking whether Y is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "If it's not, it's going to be 0 times something.",
                    "label": 0
                },
                {
                    "sent": "So the expression is going to be 0 and then if I take an.",
                    "label": 0
                },
                {
                    "sent": "Otherwise if Y is equal to see the derivative of the exponential is the exponential.",
                    "label": 0
                },
                {
                    "sent": "So This is why I recover the exponential of the pre activation at the correct label Y and I'm going to compute this durative here, so I'm taking a derivative of a sum, so that's the sum of the derivatives, and since all of the terms here there's only one, so this notice that this is a sum over all possible classes, which I'm noting as C Prime C prime to make it different from C, which is the unit with respect to which I'm taking the derivative.",
                    "label": 0
                },
                {
                    "sent": "So in this sum, all the elements are not going to be.",
                    "label": 0
                },
                {
                    "sent": "Dependent on the CF unit, except for when C prime is equal to C and then when it is the derivative of the exponentiated of the exponential is the exponential.",
                    "label": 0
                },
                {
                    "sent": "So ultimately this whole derivative here simplifies to just the exponential of the pre activation for node C. Next, going to notice that this divided by this normalization constant is just the wife element of the softmax of my pre activations.",
                    "label": 0
                },
                {
                    "sent": "That's just the definition of the softmax.",
                    "label": 0
                },
                {
                    "sent": "Here for dimension Y.",
                    "label": 0
                },
                {
                    "sent": "And similarly this term here in this term here so you can see I've taken the square here and I separated it into two terms.",
                    "label": 0
                },
                {
                    "sent": "So I have one term which is the numerator where I get the exponential for the correct label and here is the exponential for the label for the node C, which is the note for which I'm computing the derivative.",
                    "label": 0
                },
                {
                    "sent": "So both of these are softmax is but one is for the wife output and the other one is for the CF output.",
                    "label": 0
                },
                {
                    "sent": "And then finally, well, to simplify further, the softmax of Dupree activation that's F of X, so I'm replacing all the softmax is here by F of X and this allows me to cancel this with these terms here.",
                    "label": 0
                },
                {
                    "sent": "It's ultimately what do I get minus my identity function minus the function.",
                    "label": 0
                },
                {
                    "sent": "The output of our model.",
                    "label": 0
                },
                {
                    "sent": "This might be a good time to say my slides on the website, so if you actually want to go through this after or as we're going through and going back, the slides are in the speaker section, yes.",
                    "label": 0
                },
                {
                    "sent": "So so yeah, so everything simplifies.",
                    "label": 0
                },
                {
                    "sent": "It might have seemed surprising just to sum up that derivative of the YF output now depends on some other pre activations for some other unit.",
                    "label": 0
                },
                {
                    "sent": "And this is only because of the activation of the summer normalization constant.",
                    "label": 0
                },
                {
                    "sent": "This is only because in the softmax I have this term here.",
                    "label": 0
                },
                {
                    "sent": "This is why before I before when I was looking at the gradient with respect to the output, I had something very simple to derive.",
                    "label": 0
                },
                {
                    "sent": "Here is a bit more complicated because of this normalization constant.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so so far I went from the loss and I obtained the gradient with respect to the preactivation.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now again, I'm going to try to go a bit deeper now.",
                    "label": 0
                },
                {
                    "sent": "You might have noticed that this is getting a little bit complicated, so if I was to write the whole expression of the loss with respect to each of my parameters and went through all the derivation, there would be a lot of work.",
                    "label": 0
                },
                {
                    "sent": "So what it will?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do instead is exploit the chain rule so the chain will allow us to use previous company calculations.",
                    "label": 0
                },
                {
                    "sent": "Previously derived expressions to get an expression for gradient of things deeper in the network.",
                    "label": 0
                },
                {
                    "sent": "So with the chain Rule says is that if I have some function P that depends on some argument A and I want to get the derivative of that function P with respect to my argument A and imagine also that my function P is actually completely computable.",
                    "label": 0
                },
                {
                    "sent": "From a set of intermediate results that each depend on a in these intermediate results, I'm going to call them Qi.",
                    "label": 0
                },
                {
                    "sent": "Well, in this case, the narrative of P of a respect array is the sum over all intermediate computations of the multiplication between what is the derivative of a with respect to my IIF intermediate result times.",
                    "label": 0
                },
                {
                    "sent": "What is the derivative of Qi if intermediate result with respect to a so you can see that this result is applicable in the context of a neural net.",
                    "label": 0
                },
                {
                    "sent": "If I want the gradient on the activation at this unit here.",
                    "label": 0
                },
                {
                    "sent": "Well, we have computed the gradient with respect to the activations.",
                    "label": 0
                },
                {
                    "sent": "Here we can see that the loss directly only depends on all the pre activations computed here.",
                    "label": 0
                },
                {
                    "sent": "So then the only bit that means that is missing is what is then the gradient or deliberative of these intermediate results with respect to my quantity which in our next stage is going to be the activation of the layer below.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that is what we'll be invoking here for performing our derivations, so if.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want the narrative of my loss function now with respect to the JF unit at some hidden layer cake.",
                    "label": 0
                },
                {
                    "sent": "Any hidden layer cake.",
                    "label": 0
                },
                {
                    "sent": "Well, I'll invoke the chain rule and by writing it as it's just going to be the sum over the derivative of my loss function here.",
                    "label": 0
                },
                {
                    "sent": "With respect to all pre activations I for all the units I add the layer above.",
                    "label": 0
                },
                {
                    "sent": "So we respected the pre activation, add the layer above, that's why we have a K plus one here and I'm going to play this by what is the narrative of the pre activation of the layer above K plus one for the IF unit with respect to the activation at the layer K for the JF unit.",
                    "label": 0
                },
                {
                    "sent": "OK so just applying the chain rule here and so I'm going to assume that I've computed this.",
                    "label": 0
                },
                {
                    "sent": "Already and we showed before, we already did compute the gradient of the pre activation at the output layer and now the only bit that is missing is this part here.",
                    "label": 0
                },
                {
                    "sent": "What is the derivative of my pre activation at the layer above with respect to some activation of some unit in the layer below?",
                    "label": 0
                },
                {
                    "sent": "Well pre activation is quite simple, it's just a linear transformation so I'm just taking a bias plus the sum of all the weighted contributions of all units in the layer below.",
                    "label": 0
                },
                {
                    "sent": "So if I'm taking the durative which inspected the JF unit so The Dirty with respect to the pre activation well with respect to the bias, that zero is the biases of different.",
                    "label": 0
                },
                {
                    "sent": "It doesn't depend on the activation of the layer below, and then I have a some overall units J of the wait times the activation and so all of these terms.",
                    "label": 0
                },
                {
                    "sent": "Here the directive is respect to specific unit is going to be 0.",
                    "label": 0
                },
                {
                    "sent": "If it's not that unit.",
                    "label": 0
                },
                {
                    "sent": "So if this J here is not the same JS here.",
                    "label": 0
                },
                {
                    "sent": "An when it is, well, you get you have a scalar times my variable with respect to which I'm taking the dirt.",
                    "label": 0
                },
                {
                    "sent": "If so, that's just going to be the scalar, so ultimately this will simplify where this sum goes away, and then I remove this so I get the scalar in front of my variable with respect to which I'm taking the derivative.",
                    "label": 0
                },
                {
                    "sent": "So This is why this whole term here becomes just the weight between the IF unit at the layer above and the JF unit at the layer cake.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "One other way of writing this I have a sum over all units I in my layer K of the weight the weighted.",
                    "label": 0
                },
                {
                    "sent": "Partial derivative with respect to the pre activation at the layer K plus one.",
                    "label": 0
                },
                {
                    "sent": "So one thing I could do is actually consider the whole gradient, so the whole gradient of with respect to the pre activations at layer K and multiply that C I'm indexing with respect to.",
                    "label": 0
                },
                {
                    "sent": "I, so I'm indexing over Rose, so multiplying this by the JF column of my matrix W If I want to write instead of being explicit, some actually write this in linear algebra, so I can just write this instead as.",
                    "label": 0
                },
                {
                    "sent": "The gradient of my loss with respect to my pre activations at layer K + 1 multiplied by the JF column of my matrix WK plus one.",
                    "label": 0
                },
                {
                    "sent": "So here the notation of using is.",
                    "label": 0
                },
                {
                    "sent": "I'm using this dot to refer to all rows.",
                    "label": 0
                },
                {
                    "sent": "At the column J and so because of this you can think of it as slicing a matrix.",
                    "label": 0
                },
                {
                    "sent": "So I'm taking all rows at the specific column, so then this would be a column vector and so to have it become a row vector that I can multiply with this column vector.",
                    "label": 0
                },
                {
                    "sent": "This is why I'm taking the transpose right here.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that gives me the derivative with respect to some activation of some unit, that layer K and now.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I wanted the full gradient so the vector of all derivatives which effects all activations at layer K. For my loss.",
                    "label": 0
                },
                {
                    "sent": "Well, I need to do is essentially perform this operation here, but for all columns J.",
                    "label": 0
                },
                {
                    "sent": "Well, I get that simply by taking my gradient of the pre activations at the layer above and multiplying that by the full matrix transpose.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to be multiplying by each of the columns and get the vector of all the narrative's.",
                    "label": 0
                },
                {
                    "sent": "So ultimately, after doing all these up, these derivations going from preactivation gradient to activation gradient, the layer below is a very simple operation.",
                    "label": 0
                },
                {
                    "sent": "Just take the above pre activation gradients and multiply by the transpose of the weight matrix.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now if I wanted to get at the second stage again, my gradients with respect to the preactivation at this same layer K. So I will again evoke the chain rule.",
                    "label": 0
                },
                {
                    "sent": "So in this case the pre activation and deactivation in one layer are, so the activation of 1 unit only depends on the activation of that same unit, unlike at the output the output it's not the case or dependencies because of the normalization constant.",
                    "label": 0
                },
                {
                    "sent": "But here I'm going to assume that I'm using an activation function like the sigmoid or the 10 H or the Relu, in which case you just have an element wise computation, so there are no normalization within the layer.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "The derivative of my loss with respect to my pre activation for the JF unit at layer K?",
                    "label": 0
                },
                {
                    "sent": "Well, that's going to be the narrative of my loss with respect to the activation of that unit HK.",
                    "label": 0
                },
                {
                    "sent": "Times what is the derivative of the activation of that unit with respect to its pre activation and what is this term here?",
                    "label": 0
                },
                {
                    "sent": "Well that's just the derivative of my choice of activation function G prime.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now if I want to do this so this is again the partial derivative for a single unit J.",
                    "label": 0
                },
                {
                    "sent": "If I want to get the gradient so the vector of all partial derivatives, all I need to do is take each individual.",
                    "label": 0
                },
                {
                    "sent": "Each individual.",
                    "label": 0
                },
                {
                    "sent": "I just need to take my gradient my full gradient so the vector of all partial derivatives with respect the activation of all units.",
                    "label": 0
                },
                {
                    "sent": "And I just do an elementwise multiplication with its corresponding derivative of the activation function.",
                    "label": 0
                },
                {
                    "sent": "So this vector here.",
                    "label": 0
                },
                {
                    "sent": "That's just going to be the vector where the first dimension that's the derivative of the activation function for the first unit, and then the second unit and the third unit, and so on, and just pack this into a single vector and this symbol.",
                    "label": 0
                },
                {
                    "sent": "Here I used to represent the elementwise multiplication.",
                    "label": 0
                },
                {
                    "sent": "So again, computationally it's not a complicated operations to do you take your vector, you construct this other vector of the derivative of the activation function, and you do an elementwise multiplication, and that's it.",
                    "label": 0
                },
                {
                    "sent": "You now went from activation gradients to preactivation gradient.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I mentioned the I should know if there are questions at this point.",
                    "label": 0
                },
                {
                    "sent": "Or any other notation or.",
                    "label": 0
                },
                {
                    "sent": "And if you will just wrap everything into a single algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I mentioned using this durative here.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm just show very quickly what is the narrative of the different choices of activation functions.",
                    "label": 0
                },
                {
                    "sent": "I mentioned the use of possibly the linear activation function, so essentially no non linearity.",
                    "label": 1
                },
                {
                    "sent": "So essentially the output of that activation is just its input.",
                    "label": 0
                },
                {
                    "sent": "It's pre activation.",
                    "label": 1
                },
                {
                    "sent": "Well you can see that the slope of that activation function.",
                    "label": 0
                },
                {
                    "sent": "That's just one.",
                    "label": 0
                },
                {
                    "sent": "So it's a very simple decorative.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the sigmoid function, I won't see I won't show the derivation here, but you can show that it is pretty simple to compute.",
                    "label": 0
                },
                {
                    "sent": "Also, it will just be the output of the sigmoid multiplied by 1 minus the output of the sigmoid OK, and so we can see here that what this implies is that the slope from this expression is going to be 0 if either Jie is close to 1, because in this case I'm going to have 1 -- 1, so that's going to be 0.",
                    "label": 0
                },
                {
                    "sent": "Or if Jay is close to 0 in this case, I'm going to be multiplying right here by zero, and indeed you see that the slope towards where the activation is equal to 0 when the reactivations very negative and towards the end where the activation is close to one.",
                    "label": 0
                },
                {
                    "sent": "So the activation is very positive.",
                    "label": 0
                },
                {
                    "sent": "We can see that the slope is indeed converging to be flat, to have a derivative of zero.",
                    "label": 0
                },
                {
                    "sent": "And one thing that this implies also is that remember that when I'm performing backpropagation.",
                    "label": 0
                },
                {
                    "sent": "Going to do this elementwise multiplication with the gradient that I'm getting from above.",
                    "label": 0
                },
                {
                    "sent": "So if I do this, it means that a lot of these gradients I, if I'm at a saturation point, I'm going to multiply by zero or something close to 0, and that's a known issue in neural Nets, which you'll see discuss.",
                    "label": 0
                },
                {
                    "sent": "I'll mention so few words about that later on this afternoon, and probably I suspect you will talk about that in the context of stems and R and ends in general, but that means that whenever you add a saturation point, there's no gradient passing through that unit effectively.",
                    "label": 0
                },
                {
                    "sent": "And that's a form of issue you can encounter when you are even in practice training neural Nets.",
                    "label": 0
                },
                {
                    "sent": "I'll say a few more words about that and trying to avoid these situations this afternoon.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If I look at what is the partial derivative of the tangent function is.",
                    "label": 0
                },
                {
                    "sent": "Again we can show that this is this very simple expression, 1 minus the value of the tension squared.",
                    "label": 0
                },
                {
                    "sent": "So again, you can see that if you're close to one or one saturation point, you're going to get 1 -- 1 ^2.",
                    "label": 0
                },
                {
                    "sent": "So essentially one.",
                    "label": 0
                },
                {
                    "sent": "So you're going to get a derivative of 0.",
                    "label": 0
                },
                {
                    "sent": "So again, this formula you can see intuitively fits with this function here.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally for the rectified linear activation function, super simple gradient.",
                    "label": 1
                },
                {
                    "sent": "It is simply one if the pre activation is positive because it's a linear function and it's otherwise zero if the pre activation is negative.",
                    "label": 0
                },
                {
                    "sent": "So this is represented by this year.",
                    "label": 0
                },
                {
                    "sent": "The identity function of whether the pre activation is larger than 0.",
                    "label": 0
                },
                {
                    "sent": "Again, another very simple thing to compute, and then you can use that to back propagate through the activation function.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Any questions on this?",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "All we've done so far is get the gradients with respect to activations and pre activations.",
                    "label": 0
                },
                {
                    "sent": "Now we finally need our greatness with respect to our parameters, 'cause those are the parameters that we will be updating with the gradient step.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's do that now.",
                    "label": 0
                },
                {
                    "sent": "Say I want to get the gradient or the start with the derivative the partial derivative of my loss with respect to the connection between the unit and layer K and the JS unit in the layer below.",
                    "label": 0
                },
                {
                    "sent": "Then use the chain rule so I can write it as the derivative with respect to my loss of my last.",
                    "label": 0
                },
                {
                    "sent": "Sorry with respect to the ith preactivation at my layer K. Times what is the narrative of this pre activation at layer K for the RF unit with respect to my weight?",
                    "label": 0
                },
                {
                    "sent": "The weight between unit I at layer K and unit J at layer below layer K -- 1.",
                    "label": 0
                },
                {
                    "sent": "And again because this pre activation function is just a linear transformation.",
                    "label": 0
                },
                {
                    "sent": "Take computing that durative very simple so the bias does not depend on the weight, so this goes away and then this whole sum here there's only the term involving.",
                    "label": 0
                },
                {
                    "sent": "The unit J with respect to which I'm computing the derivative that matters, and in this case all you will get is so the narrative of the weight multiplied by the activation.",
                    "label": 0
                },
                {
                    "sent": "But that's just going to be the activation itself, and so this term here is just replaced by what is the GF activation in my hidden layer K -- 1?",
                    "label": 0
                },
                {
                    "sent": "And so now if I wanted to take all these partial derivatives and put them into a gradient format, so get the full matrix of all the partial derivatives, I can see that the element at row I and column J is essentially just, well, take the partial derivative for the KF preactivation.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the protective layer K respected that IF unit times the activation of the JF unit at layer K -- 1.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one thing to obtain this is to just take the gradient vector of the activation of layer K. That's a column vector, and then do an outer product or essentially take the product with the row vector of the activations at layer.",
                    "label": 0
                },
                {
                    "sent": "K -- 1 so that means that the entry at the Ro I and the column J.",
                    "label": 0
                },
                {
                    "sent": "It's just going to be the IF element of this vector times the JF element of that vector and that is.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Exactly the expression that I have here the IF of element of that vector and a JF element of the activation that layer K -- 1.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So really, getting the gradient of my full weight matrix for the Earth layer is just take my pre activation gradient and do the outer product with the activation gradient.",
                    "label": 0
                },
                {
                    "sent": "Sorry actually the activation of the gradient, the activation at the layer below.",
                    "label": 0
                },
                {
                    "sent": "And next we can do the same thing with the bias as we've seen, we can interpret the bias as just a weight vector on the constant unit equals to one.",
                    "label": 0
                },
                {
                    "sent": "So, not surprisingly, we essentially get the same formula As for the weight, but instead of H, here we have one.",
                    "label": 0
                },
                {
                    "sent": "So we just have effectively the derivative with respect to the pre activation.",
                    "label": 0
                },
                {
                    "sent": "So I wanted to construct the full gradient for my fold bias vector.",
                    "label": 0
                },
                {
                    "sent": "It is essentially just the preactivation gradient with respect to my full loss.",
                    "label": 0
                },
                {
                    "sent": "OK, just a grain of the pre activation.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let's put this all to.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Heather, and to get finally our algorithm, the algorithm that will give us for some given example, XY, what is the gradient of the loss function with respect to all my parameters?",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne, what they will essentially do is go through all these different gradients that we've seen in order such that I can always be used computations I've done before to get my computations that I need next.",
                    "label": 0
                },
                {
                    "sent": "And the way we do this is we first compute what is the gradient of my loss with respect to the.",
                    "label": 0
                },
                {
                    "sent": "We could do.",
                    "label": 0
                },
                {
                    "sent": "The activation will do the pre activation since its formula is simpler.",
                    "label": 0
                },
                {
                    "sent": "Add the output layer so AL plus one that's the pre activation the output layer and we've seen that the gradient of the loss for this pre activation is just minus the one hot vector minus the output.",
                    "label": 0
                },
                {
                    "sent": "So after the softmax.",
                    "label": 0
                },
                {
                    "sent": "So I compute this and I'm going to go from the output layer towards the first hidden layer.",
                    "label": 0
                },
                {
                    "sent": "So 4K from L + 1 down to one.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                },
                {
                    "sent": "Once I'm at the given layer K, well, I've already, so when I'm starting this iteration, I'm assuming that I've already computed the gradient of the pre activation for that layer.",
                    "label": 0
                },
                {
                    "sent": "So in other words I have what is the gradient of my loss with respect to my pre activation?",
                    "label": 0
                },
                {
                    "sent": "So the first thing I'm going to do in my iteration is get my gradients with respect to my parameters for that layer.",
                    "label": 0
                },
                {
                    "sent": "For layer came well, what is that?",
                    "label": 0
                },
                {
                    "sent": "That's just I take my pre activation gradient, I do the outer product with the activation of the layer below.",
                    "label": 0
                },
                {
                    "sent": "So here in this algorithm, I assume that I've already performed the forward propagations already went from X up to the output, so I've computed all the activations, all layer.",
                    "label": 0
                },
                {
                    "sent": "I've computed my output, so this simple operation now gives me the gradient for the weights, and now if I want the gradient for the bias, that's just haven't shown the gradient of the loss with respect to that pre activation at the layer cake.",
                    "label": 0
                },
                {
                    "sent": "And the next I'm going to take the gradient on the activations and push it down to the next layer K -- 1 so that the next iteration I can perform again the same computations to get the gradients for the layers below.",
                    "label": 0
                },
                {
                    "sent": "So I can go from the gradients of the loss with respect to my pre activation to get the gradient on the loss with respect to the activation at the layer below.",
                    "label": 0
                },
                {
                    "sent": "As I said by just just taking the gradient of the pre activations and multiplying that by the transpose of the weight matrix.",
                    "label": 0
                },
                {
                    "sent": "Simple operation and then next I take what I just computed the gradient on the activation, which is right here.",
                    "label": 0
                },
                {
                    "sent": "I do an elementwise multiplication with the narrative of the activation function.",
                    "label": 0
                },
                {
                    "sent": "Again, other simple computation, and that gives me now the gradient of the loss function with respect to the activation of the layer below, and then I continue my iteration.",
                    "label": 0
                },
                {
                    "sent": "So now K becomes K -- 1 and continue like this until I reached the first hidden layer and at the first sentence layer.",
                    "label": 0
                },
                {
                    "sent": "You don't compute this line because this is the layer below and there's no preactivation at the input layer.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that is the backpropagation algorithm.",
                    "label": 0
                },
                {
                    "sent": "As you can see, it's complexity is essentially the same as doing a forward propagation for propagation.",
                    "label": 0
                },
                {
                    "sent": "You do computations for each layer, one after another.",
                    "label": 0
                },
                {
                    "sent": "Backpropagation you do the similar type of computation, but you go from the top layer and you go down.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One way we can implement backdrop is to consider this perspective on the forward propagation and backpropagation neural Nets.",
                    "label": 0
                },
                {
                    "sent": "So what we can do is just represent all of these computations in the forward prop pass as computation graph or flow graph.",
                    "label": 0
                },
                {
                    "sent": "So when I'm performing forward propagation, so when I'm trying to compute what is my output of my layer and then ultimately what is the loss for some given input X and some target Y?",
                    "label": 0
                },
                {
                    "sent": "Well, I need to take my input X and then I need to combine it with my parameter W and my bias before the 1st hidden layer and compute my pre activation.",
                    "label": 0
                },
                {
                    "sent": "So it's just a linear transformation taking X, multiplying it by W plus the bias speed.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to apply under pre activation and activation function that gives me the activation H and then I'm going to get the activation at the next layer which requires me to depend on what is the weight at that layer.",
                    "label": 0
                },
                {
                    "sent": "What are the biases at that layer?",
                    "label": 0
                },
                {
                    "sent": "And then in this case I'll just have one single single hidden layers.",
                    "label": 0
                },
                {
                    "sent": "I already had the output going to compute the output I for classification.",
                    "label": 0
                },
                {
                    "sent": "I applied the softmax and once I've done the softmax again, finally compute my loss, which is the negative log of the value of the unit for the correct class.",
                    "label": 0
                },
                {
                    "sent": "So a nice way to implement the forward propagation.",
                    "label": 0
                },
                {
                    "sent": "This is just have you know, essentially, objects that compute that correspond to each of these boxes, and that makes the particular computation associated with these boxes.",
                    "label": 0
                },
                {
                    "sent": "So this here would just be an object that does a linear transformation given some input X and weight, and bias that characterizes this linear transformation.",
                    "label": 0
                },
                {
                    "sent": "This box here would be whatever activation function I've used.",
                    "label": 0
                },
                {
                    "sent": "Maybe I have an object that's for the Sigma and we have an object for the 10 H. Maybe an object for the Relu.",
                    "label": 0
                },
                {
                    "sent": "And here I can reuse that same object that did the linear transformation, but here instead it's going to be instantiated to use the weight matrix W and the bias B and so on.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that would be one way of sort of structuring your code when implementing the forward path.",
                    "label": 0
                },
                {
                    "sent": "And what's great is that if you represent everything in this way, you can actually, fairly simply, at least in terms of organization of your code, get a computation of your gradients.",
                    "label": 0
                },
                {
                    "sent": "What you can do is then also take your objects that do these different operations an have them, not just have like, say, a function that does the forward propagation where the forward propagation takes essentially the children of these elements and computes the activation.",
                    "label": 0
                },
                {
                    "sent": "But the backdrop also, which would just take what is the gradient.",
                    "label": 0
                },
                {
                    "sent": "Which of the loss with respect to myself, assuming that I'm obtaining this by?",
                    "label": 0
                },
                {
                    "sent": "Note that is following in the following graph and the flow graph.",
                    "label": 1
                },
                {
                    "sent": "Sorry, and then take that gradient and then push.",
                    "label": 0
                },
                {
                    "sent": "What is the gradient then for all of the arguments of the forward prop function.",
                    "label": 0
                },
                {
                    "sent": "So specifically when you do the forward path, you would just follow these arrows here and computing the loss up to here and then when you performing backpropagation you would just call back, probably in the opposite order, say taking OK.",
                    "label": 0
                },
                {
                    "sent": "So initially you would compute what is the gradient of the loss with respect to the loss itself?",
                    "label": 1
                },
                {
                    "sent": "Well that's just one.",
                    "label": 0
                },
                {
                    "sent": "Now you would callback prop on this box here, saying that, well, the loss which is better.",
                    "label": 0
                },
                {
                    "sent": "This is just one and then ask it.",
                    "label": 0
                },
                {
                    "sent": "Ask this box to pass in here.",
                    "label": 0
                },
                {
                    "sent": "What is the gradient of this loss with respect to the output F of X?",
                    "label": 0
                },
                {
                    "sent": "And then you could call back Prop on this guy here, which would then pass.",
                    "label": 0
                },
                {
                    "sent": "What is the gradient of the loss rich respect to the arguments of F of X, which is a two here the pre activation and then continue calling backdrop in this opposite order here.",
                    "label": 0
                },
                {
                    "sent": "So what is nice also about structuring your code like this is that you can.",
                    "label": 0
                },
                {
                    "sent": "Essentially it is very modular.",
                    "label": 0
                },
                {
                    "sent": "You can construct arbitrary types of architectures with all of these components fitting into one another and get for free the backpropagation algorithm from it.",
                    "label": 0
                },
                {
                    "sent": "And there's a lot of different libraries that effectively do this type of which is called what is known as automatic differentiation.",
                    "label": 0
                },
                {
                    "sent": "What I've essentially described having objects that have a forward pass in the backdrop pass function is essentially hard, torch works.",
                    "label": 0
                },
                {
                    "sent": "Then there are slightly more sophisticated approaches and libraries that instead of essentially have these objects in this way, will actually give you a functional form of performing all of these operations.",
                    "label": 0
                },
                {
                    "sent": "Theano works like this.",
                    "label": 0
                },
                {
                    "sent": "There's torch autograph that works like this tensor flow works like this, and thankfully this week you'll get to see how they each different all the implement these notions of automatic differentiation.",
                    "label": 1
                },
                {
                    "sent": "So this means to say that all these derivations that I've done in practice, technically, you never have to do all of them.",
                    "label": 0
                },
                {
                    "sent": "You do need are ever to do it if you need to.",
                    "label": 0
                },
                {
                    "sent": "If you want to implement a new module 4, which for some reason your library doesn't support and in this case you need to be familiar with how to actually compute transformation.",
                    "label": 0
                },
                {
                    "sent": "Also be able to derive and implement how to get the gradient of the arguments of that transformation.",
                    "label": 0
                },
                {
                    "sent": "A story of the story of the output of that transformation with respect to its argument.",
                    "label": 0
                },
                {
                    "sent": "That is common skill that you have to develop if you want to apply and perform research in neural network.",
                    "label": 0
                },
                {
                    "sent": "Any questions on this part, yes.",
                    "label": 0
                },
                {
                    "sent": "Right, so how do people in the loops that was the question?",
                    "label": 0
                },
                {
                    "sent": "I almost got the warning so you.",
                    "label": 0
                },
                {
                    "sent": "More about this when you see the tutorial on Torch and Theano and Tensorflow for Theano, it is handled with the scan Operation Torch autograph as a different way of doing it, where you can actually just write your for loop and it will essentially construct a tape that corresponds essentially to the these operations and then go towards the tape through the tape backwards.",
                    "label": 0
                },
                {
                    "sent": "Alex Wesco will describe that a bit more in the torch tutorial, and I think Tensorflow as.",
                    "label": 0
                },
                {
                    "sent": "Or will have a version of scan at some point that I'm less familiar with, but this will be, I think, covered in the practical tutorials.",
                    "label": 0
                },
                {
                    "sent": "And you can explicitly.",
                    "label": 0
                },
                {
                    "sent": "That's a good point.",
                    "label": 0
                },
                {
                    "sent": "So you also mentioned you can also just explicitly unfold in time the four loop.",
                    "label": 0
                },
                {
                    "sent": "Which is another way of doing it.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so if.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so if there usually well one approach is to have essentially these components that correspond to computing the forward pass for the Relu, and then within that component you also define how you compute gradients and it's within that component that you will make the assumption.",
                    "label": 0
                },
                {
                    "sent": "For instance here what I've sort of men barely mentioned is that for the point at zero I was assuming I'll use the gradient or the narrative to be set to 0 and that you would implement that within that component for the.",
                    "label": 0
                },
                {
                    "sent": "Drop method or the backdrop pass.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Often times.",
                    "label": 0
                },
                {
                    "sent": "It's just much faster to do it from like.",
                    "label": 0
                },
                {
                    "sent": "Objects.",
                    "label": 0
                },
                {
                    "sent": "Just like everyone compares all the time benchmarks.",
                    "label": 0
                },
                {
                    "sent": "The fundamental idea, which is faster in practice?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So I guess the question is amongst all these different packages, Theano to Grafton.",
                    "label": 0
                },
                {
                    "sent": "So which one is faster and why?",
                    "label": 0
                },
                {
                    "sent": "I guess or in which cases I mean?",
                    "label": 0
                },
                {
                    "sent": "I so just maybe do not take too much time.",
                    "label": 0
                },
                {
                    "sent": "I would prefer you know, pushing that question when you see the practicals because I think this is a fairly sophisticated like it requires sophisticated answer and understanding the specifics of each case.",
                    "label": 0
                },
                {
                    "sent": "I can say from my limited knowledge, which is mostly focused around torch and torch.",
                    "label": 0
                },
                {
                    "sent": "Undergrad is that.",
                    "label": 0
                },
                {
                    "sent": "Ticular will be pretty convenient when you have a computation graph that tends to vary a lot for a given input, whereas I think the annual intensive deals with that a bit less well.",
                    "label": 0
                },
                {
                    "sent": "Torch autograph will have a certain overhead in certain situations, over that you know will not have with tensor flow will not have.",
                    "label": 0
                },
                {
                    "sent": "This will be covered later this week, but those are.",
                    "label": 0
                },
                {
                    "sent": "Thanks to this is, I think, partly why you have all of these different presentations this week is that there's no one single package for every situation, at least as we speak right now.",
                    "label": 1
                },
                {
                    "sent": "Would argue, and it's good to know all these different approaches because behind the scene there are sometimes substantially different, and there's a whole literature on automatic differentiation that actually goes much beyond what is currently implemented in some of these packages, and I think I suspect in.",
                    "label": 0
                },
                {
                    "sent": "Future years of research and developments in deep learning will see us taking more and more advantage of some of these tricks.",
                    "label": 0
                },
                {
                    "sent": "Is that if you do similar differentiation automatic differentiation, then in principle you can have it is done in Theano.",
                    "label": 0
                },
                {
                    "sent": "Once you've done the differentiation, what you have is a symbolic graph that describes the computation.",
                    "label": 0
                },
                {
                    "sent": "So now you can apply compiler tricks to make that the computation where efficient are more likely more stable.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you have something that the standard torch way where you know you just go and do those.",
                    "label": 0
                },
                {
                    "sent": "Calculate the backdrops of each module.",
                    "label": 0
                },
                {
                    "sent": "You can't do that.",
                    "label": 0
                },
                {
                    "sent": "Also, there is some computations, derivative computations that you will be able to do with things like you know, for example, imagine second derivatives you apply twice, the first ever.",
                    "label": 0
                },
                {
                    "sent": "You can't do that with the porch approach, but of course this these are a bit exotic in practice in terms of speed, it's more less the same thing.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "What's OK, I worked hard this animation animation.",
                    "label": 0
                },
                {
                    "sent": "So let's look at it again, OK?",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, and that's it.",
                    "label": 0
                },
                {
                    "sent": "So yeah, so later on finish the training algorithm and talk about deep learning after lunch.",
                    "label": 0
                }
            ]
        }
    }
}