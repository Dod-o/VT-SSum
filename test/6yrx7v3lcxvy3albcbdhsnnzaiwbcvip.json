{
    "id": "6yrx7v3lcxvy3albcbdhsnnzaiwbcvip",
    "title": "Normalized Semantic Web Distance",
    "info": {
        "author": [
            "Tom de Nies, Ghent University"
        ],
        "published": "July 28, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2016_de_nies_web_distance/",
    "segmentation": [
        [
            "Here's what I'll be talking about today.",
            "I'll introduce the problem that we're trying to solve, of course, and then I'll talk about how this is not really a new ID, because the ideas behind my approach are already implemented in something called the normalized web distance by Google.",
            "But what we did do what is new is we applied at on the semantic web, and the way that we applied it and evaluated.",
            "That is that is new, and that's also what will discuss today.",
            "Alright."
        ],
        [
            "So what we want to do is we want to quantify the dissimilarity or the distance between things in the world, right?",
            "We want to do this for indexing, retrieval, clustering, whatever.",
            "Basically because we can do it and for human it's easy to make a relative comparison.",
            "Basically, you can say an Apple and a pear are closer than an Apple and a cucumber intuitively, but it's much harder to put an actual number on that, right?",
            "But if you're a machine and you may need to make some recommendations, or you need to look something up, you need that actual number 2.",
            "Make that relevant relative comparison.",
            "So."
        ],
        [
            "You have this thing called the normalized information distance, which theoretically does this.",
            "This estimates the distance between 2 pieces of information.",
            "Now unfortunately it's non computable because you need an ideal compression mechanism for it.",
            "Basically you need to ideally represent the information and then measure the distance between those representations, which is well, only theoretical.",
            "Now Google came up with something very clever and disregard they said well we have this huge thing called the web and we index quite a bit of it.",
            "Can we use our statistics of our index of our search engine to actually measure distance between concepts?",
            "And that's what they did with the normalized web distance.",
            "Basically they gather these statistics to their search engine.",
            "They indexed large part of the web.",
            "Basically they said OK, this kind of compresses all the information to a single number, namely a page count, right?",
            "And this represents the number of occurrences that have certain term has on the web and the theory behind it is if two of these concepts occur frequently together and their distance is going to be smaller than when they don't appear frequently together.",
            "So."
        ],
        [
            "To actually implement that, you need something called frequency functions.",
            "Basically this frequency function F you calculated by looking at the number of search results for a certain concept, and fxy is basically the number of search results for two concepts together.",
            "Now to normalize this to make sure that you can compare different distances, you need to normalize it somehow and the ideal way that we do that would be to have a total number of web pages indexed by the search engine, but as you will obviously know this is very hard to do and very hard to know because it's.",
            "Changes all the time.",
            "Right, so basically what they do is they usually take a very big number for that and as long as you take that big number your results are compatible with each other.",
            "So the formula for that is basically the formula for normalized information distance instantiated with these frequency functions, right?",
            "There are much more details about how you get to that formula in the paper that I won't bore you with today."
        ],
        [
            "What I will show you is an example of how you do it.",
            "So say you enter bass and guitar and Google both separately.",
            "You get a number of results and then you enter them both together.",
            "The idea is that something called the normalized Google distance in this case between bass and guitar.",
            "You instantiate it with those numbers and you get something like 41%, which if you think about it sounds about right.",
            "It's something that your intuition can can comprehend."
        ],
        [
            "Now this does have some drawbacks, namely that can tell the difference between a fish and a guitar, because you're looking at syntactic similarity, you're not looking at semantic similarity 'cause you have this ambiguity between things that have the same label.",
            "You also have problems with availability, because Google might always be there, or more importantly, it might change.",
            "There are no standards governing regular text search engine's.",
            "You don't have a sparkle for it, you have.",
            "Basically, they determine whatever they think is important.",
            "To implement and in the case of Google can't even implement the normalized Google Distance anymore, because the plus operator changed.",
            "It's well, the meaning of the plus operator changed since they wrote the paper, so you can't implement it anymore.",
            "You need a different search engines such as Bing, which is what we used for our evaluation."
        ],
        [
            "And if you look at the result of distance between two concepts today, tomorrow, it might be very.",
            "It might be different because you have a different number of pages, because the web is constantly changing."
        ],
        [
            "So we thought OK. What if we take this ID of compressing that information using the knowledge that is somehow structured on the web?",
            "How about taking that idea and applying it to the semantic web?",
            "Actually, using the semantic awareness that we have in a knowledge graph actually using the disambiguated concepts that we have on the semantic web and exploiting the structure of it and the similar way that normalized web distance does.",
            "And that's how we came up with the normalized semantic web distance and the core principle, is very similar.",
            "That is, if two concepts are used to describe the same things, then their distance is going to be smaller than when they're not used to describe the same things.",
            "That's the core principle behind it."
        ],
        [
            "So again, you do this.",
            "You need these frequency functions and that's the cool thing with the knowledge graphs is you have lots of options.",
            "In this case you can say, well, you can represent the Knowledge Graph as a set of nodes instead of triples in a set of predicates, right?",
            "So if we take the number of nodes, for example, linking 2X, look at only the incoming links and we can write a frequency function for that, just the cardinality of that set.",
            "We can also look at the outgoing links or both of them and the important thing here is that we count the number of nodes, not the number of links, because we treat a node that is linked to a certain concept.",
            "Basically we treat that as an occurrence on the semantic web.",
            "That's the thing in the back of your mind."
        ],
        [
            "Alright, so if we instantiate the same formula with that, but those cardinalities as a frequency function, basically you get to normalized semantic web distance and what's cool about it is that it's easily implemented using sparkle count distinct queries.",
            "You can easily count the number of nodes, and it's something that will not change because it's a standard.",
            "As long as you conform to the standard, you know that your distance calculation will work.",
            "Now you will notice that here we have this log V thing.",
            "That's the total number of nodes in the graph.",
            "Now that's something that's harder to calculate because basically we're dealing with an open world assumption.",
            "We can estimate it much more accurately than we can on the human readable web, but still, this number is still the hardest part to compute here.",
            "Basically, if you want to count the number of distinct nodes in a knowledge graph, it's not an easy query for a sparkle endpoint to solve, But you can optimize for it."
        ],
        [
            "Alright, so I think it's high time for an example, right?",
            "Let's look at a graph that we have a little toy example.",
            "It also shows the interplay between labeled graphs and unlabeled graphs.",
            "Jim Hendler was talking about this morning.",
            "Well, basically we work with non labeled graphs, right?",
            "So say we want to calculate the distance between concept X and concept Y here.",
            "Let's say we have a knowledge graph about 1000 nodes.",
            "This is a part of it.",
            "These are the nodes that link to X&Y.",
            "If we look at the number of incoming links to concept X, we look at all the red nodes.",
            "Basically, it's five of them.",
            "Same thing for why you have no ABC and F. They all linked to Y and if we look at.",
            "The nodes that link to both of them, we only have node A&F that do that.",
            "So basically we have our frequency functions and we can instantiate or formula with that and we get can calculate the distance using the total number of nodes that we have.",
            "We said it was 1000 in this toy example.",
            "So we have about 16% distance between these two nodes.",
            "Alright."
        ],
        [
            "So what's really nice now is that this was totally generic.",
            "You don't use the actual labels yet.",
            "You can use that.",
            "We can use that to customized the edges that you're going to consider.",
            "But basically you can use this on any graph you can say OK, if we're going to apply this on a very generic use case, we can use Freebase or DB pedia, which is used to present represent knowledge in general.",
            "Or we can go more domain specific.",
            "For example, if you want to calculate the distance between concepts in music or different songs, we can use Musicbrainz.",
            "Or even look at the social graph for all that matters, as long as you have this unlabeled graph, you'll be fine and a way to query it to get those numbers."
        ],
        [
            "Alright, so that's the basic principle now then, the problem was how are we going to evaluate this?",
            "Because we've already presented something similar on the the Freebase implementation we presented last year or two years ago at poster session here.",
            "But we didn't have a good way of evaluating it yet.",
            "Now we looked at literature in the mean time, and we saw that OK, this one recurring thing that we see is the Miller Charles data set is basically this set of 30 concept pairs that were created by Miller and Charles and they gave it to 38 grad students and they said OK, look, you have terms like car and automobile Gem, Jewel Cemetery, Woodland and so on.",
            "OK give us your intuition of the distance between these two concepts and then they took the average of all those grad students.",
            "The numbers they gave them and the idea is that you measure the color relation that your machine generated distance has with this human based assessment.",
            "Now our problem here is that we need to disambiguate these terms somehow, because we're dealing with concepts on the semantic web.",
            "Now to do that.",
            "This is going to impact the calculation of the distance in a great great manner, because of course if you're looking at two different concepts, you're going to have two different sets of links and nodes, so it's going to have a huge impact.",
            "So we decided to test different disambiguation strategies.",
            "The first one was manual, so that, OK, let's try and give this distance the best chance it can have to assess the human judgment by also.",
            "Letting a human says the disambiguation process.",
            "Then we use the count based disambiguation strategy where we looked at the highest number of nodes that were connected to these concepts, basically giving the distance the best chance of getting a fine grained.",
            "Calculation and then we looked at similarity based disambiguation, which is basically what we wanted to do manually but automated.",
            "Basically we looked at different concepts and calculated distance between all of the disambiguation options and took the best one, the one with the lowest distance.",
            "Alright.",
            "Now.",
            "As you might have noticed, the Miller Charles data set is actually based on similarity and not on distance, so there we had a pro."
        ],
        [
            "OK, we need to convert our distance similarity somehow.",
            "Now there's naive ways of doing it, such as the one I put on slide saying OK, let's look at the maximum value we can have for our distance, or at least some value larger than that.",
            "Divide the actual distance with that and then subtracted from one right?",
            "We actually use a little more complex scaling that better distributed the distances or the similarities between zero and one, but the basic principle is the same.",
            "This was linearly scales, we use something else.",
            "You can use exponential scaling.",
            "It doesn't really matter that much what was important for it was that maximum value, because that's something where it again differs from the normalized web distance for the normalized web distance.",
            "You cannot calculate this maximum value because you don't know how many pages there on the web or how many pages Google indexes.",
            "If you know the number of nodes that are in a knowledge graph, you can easily calculate the maximum value that could ever have a distance that means.",
            "The maximum distance that can be between two concepts in your knowledge graph.",
            "It's not going to be infinite, it's going to be that basically it only depends on the number of nodes in the knowledge graph.",
            "The theory behind that is also in the paper.",
            "So for example, if you have a knowledge graph for the 1000 nodes, you know that the maximum distance that you can possibly get is about 8.9, and it grows with the number of nodes.",
            "I think it's logarithmically.",
            "So basically if you have knowledge graph above about four million, I think it.",
            "Adds up to about 20 so it logarithmically grows."
        ],
        [
            "Now.",
            "When we did that, so we converted distances to similarity.",
            "We had the disambiguation strategies we evaluated on using the correlation with the Miller Charles data set.",
            "So first we did that with Freebase.",
            "These results that were already presented at a previous conference.",
            "And you see that, OK, we have clear positive correlation.",
            "It's not that big, but we clearly outperformed the baseline if we calculated the normalized web distance with Bing.",
            "We clearly saw, OK, they don't have the correlation.",
            "Any of the disambiguation strategies that we have.",
            "You also see that in this case, count based disambiguation did not perform that well.",
            "You'll see in DP, audit was very different.",
            "I don't really have an explanation for that.",
            "What we do see is that if we look at.",
            "Both the incoming and outgoing links.",
            "Then we always get the best results, or at least more consistent results.",
            "Which is a good thing to realize.",
            "It's also logical because we use more of the structure of the Knowledge graph."
        ],
        [
            "If we look at the pedia we see we get better results.",
            "Even though the Knowledge Graph is sparser, which was surprising to us an again we see that if we look at both incoming and outgoing links, we get a better correlation.",
            "Again, we beat the baseline by far, but then we said, OK, how does this compare to other approaches?",
            "That was the whole point of evaluating on the Miller Charles data set."
        ],
        [
            "So we looked at other approaches that were relevant to us and these are the three classes that we selected.",
            "The first one is the most related 1 two hours.",
            "It's called the Wikipedia link based measure.",
            "An basically it combines also the principle of the normalized web distance with traditional TF IDF weights and cosine similarity.",
            "Makes a very complex measure using that an basically measures the distance between pages on Wikipedia using the links also.",
            "So the structure of knowledge graph only there, not the machine readable version.",
            "Then you have a whole bunch of ontology based approaches.",
            "There's two surveys over there that summarized the results on that.",
            "Miller Charles data set for these approaches and the results vary all over the place.",
            "Place I'll show it in the next slide.",
            "Basically this is the intuitive thing that distance is measured via the concept hierarchy, basically.",
            "If you have to measure distance between Apple and a cucumber, you say OK, Apple is a fruit computers vegetable.",
            "They're both plants, and so on.",
            "Something like that.",
            "And then there is the actually the simplest 1 digit card distance which just measures the shared nodes instead of the related to the total nodes that are linked to an that was measured in the study that cited there."
        ],
        [
            "So if we compare it, we see that if you look at the most related measured Wikipedia based one, we see that we just don't beat it by 1.1.",
            "On the correlation scale.",
            "So we're very close there.",
            "Definitely in the ballpark.",
            "At the ontology based one, the lowest result that was reported that was correlation of 0.59.",
            "So we beat that one, but we don't beat the highest result that was reported in both of the surveys which was in the lower 80s, which is actually much better.",
            "Correlation and what we have.",
            "If we then look at the simplest approach, surprisingly, it's also the most accurate one.",
            "Now I don't know how accurate that claim I just made was because we can't really look.",
            "They didn't really give out the disambiguations that they used to evaluate here.",
            "They also used a mapping to concepts in a knowledge graph, but it wasn't made public so I have no idea whether we use the same disambiguation or not, and whether the results are compatible.",
            "But in any case, the reported a very much higher correlation.",
            "Alright."
        ],
        [
            "So we did not beat the existing approaches, but we did achieve a clear positive correlation, which means which means that at least our core assumption is in the right direction.",
            "And we outperformed the syntactic similarity of the normalized web distance.",
            "On this data set, because this data set.",
            "Really is not the most suitable one for our approach, but we chose it because it's highly popular in literature, right?",
            "So what we now need to do is we need to."
        ],
        [
            "Look for a different data set.",
            "OK, what we also need to realize is that the quality of the knowledge graph and the ambiguity of the Miller Charles data set have very high impact on what we're trying to do here.",
            "For example, we had concepts journey and voyage that we had to disambiguate, and we looked at the descriptions on DB pedia.",
            "We saw that.",
            "OK, there are many disambiguation options for these terms, but none of them actually captured the intended meaning.",
            "Actually, they mean the pedia travel.",
            "Same thing for Latin mad house.",
            "You have no equivalent of Freebase.",
            "For that an all of these these inconsistencies, all of these gaps in the Knowledge Graph really lead to variations and gaps in the in the results of our evaluation.",
            "So we wondered, can we measure something else to actually show that we.",
            "Achieve what we want to do."
        ],
        [
            "And what we want to do is we want to get this semantic awareness right.",
            "So we want to be able to distinguish fish from a guitar.",
            "Basically.",
            "So to illustrate that, we created a set of 20 concept pairs that we said OK, these."
        ],
        [
            "Really illustrate that things that like pear and Apple, both fruits.",
            "You need a lower distance there than between pair an Apple, the company and we created ten of those those cases, so 20 concepts in total.",
            "And we measured the distance in all of these cases.",
            "What we saw was that."
        ],
        [
            "When we did it on DB pedia, everything was fine or claim was actually supported here.",
            "We always had a lower distance for the first 2 columns then between the first column in the last column.",
            "Great we."
        ],
        [
            "One minor anomaly when we evaluated on Freebase here between automobile an bus, both the transportation mechanism and automobile and bus in the computing context, which was really weird.",
            "But it's also very minor difference there.",
            "So."
        ],
        [
            "So that led us to believe, OK, we're still on the right track.",
            "So as a next step we really need to find a good evaluation corpus that has unambiguous concepts related to human assessments.",
            "And one of the options that we're exploring now is this is the one that I mentioned on the slide there an we also want to look at the aspects of the Knowledge Graph that actually play a part in influencing these results.",
            "Things like the Knowledge graph quality, the connectivity this size, maybe even the semantic richness which previous speaker.",
            "Talks about and importantly, domain specific T because if it's a knowledge graph is more domain specific theory is that you get better results for your more accurate results for your distance calculation that let us to think OK, maybe we can even use the normalized semantic web distance to kind of evaluate knowledge graphs.",
            "You know if you have a good ground truth for this human assessment.",
            "And it's very domain specific.",
            "Maybe you can assess the applicability of this knowledge graph using this ground truth and the normalized semantic web distance like you should at least get a very high correlation if your knowledge graph is really that domain specific and rich.",
            "That's something that we're looking into in the future.",
            "We're also looking into together with the University of Akron is basically to include this measure in an adaptive dissimilarity measure for documents on the document level.",
            "So I actually look at different entities that are current document and measure distance between them and stuff like the earth movers, distance, square, quadratic form distance, and so on.",
            "Alright."
        ],
        [
            "So that's the conclusion we introduced this new distance in knowledge graph, advancing the idea of the normalized with semantic web distance.",
            "We didn't invent the warm water here, but we did evaluated on Freebase and Pedia within established benchmark outperformed the normalized web distance using Bing but not the other approaches from literature.",
            "But we did achieve a good correlation positively and we illustrated that it does exhibit some semantic awareness.",
            "So thank you very much and if you have any questions, I'm thinking now."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here's what I'll be talking about today.",
                    "label": 0
                },
                {
                    "sent": "I'll introduce the problem that we're trying to solve, of course, and then I'll talk about how this is not really a new ID, because the ideas behind my approach are already implemented in something called the normalized web distance by Google.",
                    "label": 1
                },
                {
                    "sent": "But what we did do what is new is we applied at on the semantic web, and the way that we applied it and evaluated.",
                    "label": 0
                },
                {
                    "sent": "That is that is new, and that's also what will discuss today.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we want to do is we want to quantify the dissimilarity or the distance between things in the world, right?",
                    "label": 1
                },
                {
                    "sent": "We want to do this for indexing, retrieval, clustering, whatever.",
                    "label": 1
                },
                {
                    "sent": "Basically because we can do it and for human it's easy to make a relative comparison.",
                    "label": 0
                },
                {
                    "sent": "Basically, you can say an Apple and a pear are closer than an Apple and a cucumber intuitively, but it's much harder to put an actual number on that, right?",
                    "label": 0
                },
                {
                    "sent": "But if you're a machine and you may need to make some recommendations, or you need to look something up, you need that actual number 2.",
                    "label": 0
                },
                {
                    "sent": "Make that relevant relative comparison.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You have this thing called the normalized information distance, which theoretically does this.",
                    "label": 1
                },
                {
                    "sent": "This estimates the distance between 2 pieces of information.",
                    "label": 0
                },
                {
                    "sent": "Now unfortunately it's non computable because you need an ideal compression mechanism for it.",
                    "label": 0
                },
                {
                    "sent": "Basically you need to ideally represent the information and then measure the distance between those representations, which is well, only theoretical.",
                    "label": 0
                },
                {
                    "sent": "Now Google came up with something very clever and disregard they said well we have this huge thing called the web and we index quite a bit of it.",
                    "label": 0
                },
                {
                    "sent": "Can we use our statistics of our index of our search engine to actually measure distance between concepts?",
                    "label": 0
                },
                {
                    "sent": "And that's what they did with the normalized web distance.",
                    "label": 1
                },
                {
                    "sent": "Basically they gather these statistics to their search engine.",
                    "label": 0
                },
                {
                    "sent": "They indexed large part of the web.",
                    "label": 0
                },
                {
                    "sent": "Basically they said OK, this kind of compresses all the information to a single number, namely a page count, right?",
                    "label": 0
                },
                {
                    "sent": "And this represents the number of occurrences that have certain term has on the web and the theory behind it is if two of these concepts occur frequently together and their distance is going to be smaller than when they don't appear frequently together.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To actually implement that, you need something called frequency functions.",
                    "label": 0
                },
                {
                    "sent": "Basically this frequency function F you calculated by looking at the number of search results for a certain concept, and fxy is basically the number of search results for two concepts together.",
                    "label": 1
                },
                {
                    "sent": "Now to normalize this to make sure that you can compare different distances, you need to normalize it somehow and the ideal way that we do that would be to have a total number of web pages indexed by the search engine, but as you will obviously know this is very hard to do and very hard to know because it's.",
                    "label": 0
                },
                {
                    "sent": "Changes all the time.",
                    "label": 1
                },
                {
                    "sent": "Right, so basically what they do is they usually take a very big number for that and as long as you take that big number your results are compatible with each other.",
                    "label": 0
                },
                {
                    "sent": "So the formula for that is basically the formula for normalized information distance instantiated with these frequency functions, right?",
                    "label": 0
                },
                {
                    "sent": "There are much more details about how you get to that formula in the paper that I won't bore you with today.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What I will show you is an example of how you do it.",
                    "label": 0
                },
                {
                    "sent": "So say you enter bass and guitar and Google both separately.",
                    "label": 0
                },
                {
                    "sent": "You get a number of results and then you enter them both together.",
                    "label": 0
                },
                {
                    "sent": "The idea is that something called the normalized Google distance in this case between bass and guitar.",
                    "label": 1
                },
                {
                    "sent": "You instantiate it with those numbers and you get something like 41%, which if you think about it sounds about right.",
                    "label": 0
                },
                {
                    "sent": "It's something that your intuition can can comprehend.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now this does have some drawbacks, namely that can tell the difference between a fish and a guitar, because you're looking at syntactic similarity, you're not looking at semantic similarity 'cause you have this ambiguity between things that have the same label.",
                    "label": 0
                },
                {
                    "sent": "You also have problems with availability, because Google might always be there, or more importantly, it might change.",
                    "label": 1
                },
                {
                    "sent": "There are no standards governing regular text search engine's.",
                    "label": 0
                },
                {
                    "sent": "You don't have a sparkle for it, you have.",
                    "label": 0
                },
                {
                    "sent": "Basically, they determine whatever they think is important.",
                    "label": 0
                },
                {
                    "sent": "To implement and in the case of Google can't even implement the normalized Google Distance anymore, because the plus operator changed.",
                    "label": 0
                },
                {
                    "sent": "It's well, the meaning of the plus operator changed since they wrote the paper, so you can't implement it anymore.",
                    "label": 0
                },
                {
                    "sent": "You need a different search engines such as Bing, which is what we used for our evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you look at the result of distance between two concepts today, tomorrow, it might be very.",
                    "label": 0
                },
                {
                    "sent": "It might be different because you have a different number of pages, because the web is constantly changing.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we thought OK. What if we take this ID of compressing that information using the knowledge that is somehow structured on the web?",
                    "label": 0
                },
                {
                    "sent": "How about taking that idea and applying it to the semantic web?",
                    "label": 0
                },
                {
                    "sent": "Actually, using the semantic awareness that we have in a knowledge graph actually using the disambiguated concepts that we have on the semantic web and exploiting the structure of it and the similar way that normalized web distance does.",
                    "label": 0
                },
                {
                    "sent": "And that's how we came up with the normalized semantic web distance and the core principle, is very similar.",
                    "label": 1
                },
                {
                    "sent": "That is, if two concepts are used to describe the same things, then their distance is going to be smaller than when they're not used to describe the same things.",
                    "label": 1
                },
                {
                    "sent": "That's the core principle behind it.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, you do this.",
                    "label": 0
                },
                {
                    "sent": "You need these frequency functions and that's the cool thing with the knowledge graphs is you have lots of options.",
                    "label": 1
                },
                {
                    "sent": "In this case you can say, well, you can represent the Knowledge Graph as a set of nodes instead of triples in a set of predicates, right?",
                    "label": 0
                },
                {
                    "sent": "So if we take the number of nodes, for example, linking 2X, look at only the incoming links and we can write a frequency function for that, just the cardinality of that set.",
                    "label": 1
                },
                {
                    "sent": "We can also look at the outgoing links or both of them and the important thing here is that we count the number of nodes, not the number of links, because we treat a node that is linked to a certain concept.",
                    "label": 1
                },
                {
                    "sent": "Basically we treat that as an occurrence on the semantic web.",
                    "label": 0
                },
                {
                    "sent": "That's the thing in the back of your mind.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so if we instantiate the same formula with that, but those cardinalities as a frequency function, basically you get to normalized semantic web distance and what's cool about it is that it's easily implemented using sparkle count distinct queries.",
                    "label": 1
                },
                {
                    "sent": "You can easily count the number of nodes, and it's something that will not change because it's a standard.",
                    "label": 0
                },
                {
                    "sent": "As long as you conform to the standard, you know that your distance calculation will work.",
                    "label": 0
                },
                {
                    "sent": "Now you will notice that here we have this log V thing.",
                    "label": 0
                },
                {
                    "sent": "That's the total number of nodes in the graph.",
                    "label": 0
                },
                {
                    "sent": "Now that's something that's harder to calculate because basically we're dealing with an open world assumption.",
                    "label": 0
                },
                {
                    "sent": "We can estimate it much more accurately than we can on the human readable web, but still, this number is still the hardest part to compute here.",
                    "label": 0
                },
                {
                    "sent": "Basically, if you want to count the number of distinct nodes in a knowledge graph, it's not an easy query for a sparkle endpoint to solve, But you can optimize for it.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so I think it's high time for an example, right?",
                    "label": 0
                },
                {
                    "sent": "Let's look at a graph that we have a little toy example.",
                    "label": 0
                },
                {
                    "sent": "It also shows the interplay between labeled graphs and unlabeled graphs.",
                    "label": 0
                },
                {
                    "sent": "Jim Hendler was talking about this morning.",
                    "label": 0
                },
                {
                    "sent": "Well, basically we work with non labeled graphs, right?",
                    "label": 0
                },
                {
                    "sent": "So say we want to calculate the distance between concept X and concept Y here.",
                    "label": 0
                },
                {
                    "sent": "Let's say we have a knowledge graph about 1000 nodes.",
                    "label": 1
                },
                {
                    "sent": "This is a part of it.",
                    "label": 0
                },
                {
                    "sent": "These are the nodes that link to X&Y.",
                    "label": 1
                },
                {
                    "sent": "If we look at the number of incoming links to concept X, we look at all the red nodes.",
                    "label": 0
                },
                {
                    "sent": "Basically, it's five of them.",
                    "label": 0
                },
                {
                    "sent": "Same thing for why you have no ABC and F. They all linked to Y and if we look at.",
                    "label": 0
                },
                {
                    "sent": "The nodes that link to both of them, we only have node A&F that do that.",
                    "label": 0
                },
                {
                    "sent": "So basically we have our frequency functions and we can instantiate or formula with that and we get can calculate the distance using the total number of nodes that we have.",
                    "label": 0
                },
                {
                    "sent": "We said it was 1000 in this toy example.",
                    "label": 0
                },
                {
                    "sent": "So we have about 16% distance between these two nodes.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what's really nice now is that this was totally generic.",
                    "label": 0
                },
                {
                    "sent": "You don't use the actual labels yet.",
                    "label": 0
                },
                {
                    "sent": "You can use that.",
                    "label": 0
                },
                {
                    "sent": "We can use that to customized the edges that you're going to consider.",
                    "label": 0
                },
                {
                    "sent": "But basically you can use this on any graph you can say OK, if we're going to apply this on a very generic use case, we can use Freebase or DB pedia, which is used to present represent knowledge in general.",
                    "label": 0
                },
                {
                    "sent": "Or we can go more domain specific.",
                    "label": 0
                },
                {
                    "sent": "For example, if you want to calculate the distance between concepts in music or different songs, we can use Musicbrainz.",
                    "label": 0
                },
                {
                    "sent": "Or even look at the social graph for all that matters, as long as you have this unlabeled graph, you'll be fine and a way to query it to get those numbers.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so that's the basic principle now then, the problem was how are we going to evaluate this?",
                    "label": 0
                },
                {
                    "sent": "Because we've already presented something similar on the the Freebase implementation we presented last year or two years ago at poster session here.",
                    "label": 0
                },
                {
                    "sent": "But we didn't have a good way of evaluating it yet.",
                    "label": 0
                },
                {
                    "sent": "Now we looked at literature in the mean time, and we saw that OK, this one recurring thing that we see is the Miller Charles data set is basically this set of 30 concept pairs that were created by Miller and Charles and they gave it to 38 grad students and they said OK, look, you have terms like car and automobile Gem, Jewel Cemetery, Woodland and so on.",
                    "label": 0
                },
                {
                    "sent": "OK give us your intuition of the distance between these two concepts and then they took the average of all those grad students.",
                    "label": 0
                },
                {
                    "sent": "The numbers they gave them and the idea is that you measure the color relation that your machine generated distance has with this human based assessment.",
                    "label": 0
                },
                {
                    "sent": "Now our problem here is that we need to disambiguate these terms somehow, because we're dealing with concepts on the semantic web.",
                    "label": 0
                },
                {
                    "sent": "Now to do that.",
                    "label": 0
                },
                {
                    "sent": "This is going to impact the calculation of the distance in a great great manner, because of course if you're looking at two different concepts, you're going to have two different sets of links and nodes, so it's going to have a huge impact.",
                    "label": 0
                },
                {
                    "sent": "So we decided to test different disambiguation strategies.",
                    "label": 0
                },
                {
                    "sent": "The first one was manual, so that, OK, let's try and give this distance the best chance it can have to assess the human judgment by also.",
                    "label": 0
                },
                {
                    "sent": "Letting a human says the disambiguation process.",
                    "label": 0
                },
                {
                    "sent": "Then we use the count based disambiguation strategy where we looked at the highest number of nodes that were connected to these concepts, basically giving the distance the best chance of getting a fine grained.",
                    "label": 0
                },
                {
                    "sent": "Calculation and then we looked at similarity based disambiguation, which is basically what we wanted to do manually but automated.",
                    "label": 0
                },
                {
                    "sent": "Basically we looked at different concepts and calculated distance between all of the disambiguation options and took the best one, the one with the lowest distance.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "As you might have noticed, the Miller Charles data set is actually based on similarity and not on distance, so there we had a pro.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, we need to convert our distance similarity somehow.",
                    "label": 0
                },
                {
                    "sent": "Now there's naive ways of doing it, such as the one I put on slide saying OK, let's look at the maximum value we can have for our distance, or at least some value larger than that.",
                    "label": 0
                },
                {
                    "sent": "Divide the actual distance with that and then subtracted from one right?",
                    "label": 0
                },
                {
                    "sent": "We actually use a little more complex scaling that better distributed the distances or the similarities between zero and one, but the basic principle is the same.",
                    "label": 1
                },
                {
                    "sent": "This was linearly scales, we use something else.",
                    "label": 0
                },
                {
                    "sent": "You can use exponential scaling.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really matter that much what was important for it was that maximum value, because that's something where it again differs from the normalized web distance for the normalized web distance.",
                    "label": 0
                },
                {
                    "sent": "You cannot calculate this maximum value because you don't know how many pages there on the web or how many pages Google indexes.",
                    "label": 0
                },
                {
                    "sent": "If you know the number of nodes that are in a knowledge graph, you can easily calculate the maximum value that could ever have a distance that means.",
                    "label": 0
                },
                {
                    "sent": "The maximum distance that can be between two concepts in your knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "It's not going to be infinite, it's going to be that basically it only depends on the number of nodes in the knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "The theory behind that is also in the paper.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you have a knowledge graph for the 1000 nodes, you know that the maximum distance that you can possibly get is about 8.9, and it grows with the number of nodes.",
                    "label": 1
                },
                {
                    "sent": "I think it's logarithmically.",
                    "label": 0
                },
                {
                    "sent": "So basically if you have knowledge graph above about four million, I think it.",
                    "label": 0
                },
                {
                    "sent": "Adds up to about 20 so it logarithmically grows.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "When we did that, so we converted distances to similarity.",
                    "label": 0
                },
                {
                    "sent": "We had the disambiguation strategies we evaluated on using the correlation with the Miller Charles data set.",
                    "label": 0
                },
                {
                    "sent": "So first we did that with Freebase.",
                    "label": 0
                },
                {
                    "sent": "These results that were already presented at a previous conference.",
                    "label": 0
                },
                {
                    "sent": "And you see that, OK, we have clear positive correlation.",
                    "label": 0
                },
                {
                    "sent": "It's not that big, but we clearly outperformed the baseline if we calculated the normalized web distance with Bing.",
                    "label": 0
                },
                {
                    "sent": "We clearly saw, OK, they don't have the correlation.",
                    "label": 0
                },
                {
                    "sent": "Any of the disambiguation strategies that we have.",
                    "label": 0
                },
                {
                    "sent": "You also see that in this case, count based disambiguation did not perform that well.",
                    "label": 0
                },
                {
                    "sent": "You'll see in DP, audit was very different.",
                    "label": 0
                },
                {
                    "sent": "I don't really have an explanation for that.",
                    "label": 0
                },
                {
                    "sent": "What we do see is that if we look at.",
                    "label": 0
                },
                {
                    "sent": "Both the incoming and outgoing links.",
                    "label": 0
                },
                {
                    "sent": "Then we always get the best results, or at least more consistent results.",
                    "label": 0
                },
                {
                    "sent": "Which is a good thing to realize.",
                    "label": 0
                },
                {
                    "sent": "It's also logical because we use more of the structure of the Knowledge graph.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we look at the pedia we see we get better results.",
                    "label": 0
                },
                {
                    "sent": "Even though the Knowledge Graph is sparser, which was surprising to us an again we see that if we look at both incoming and outgoing links, we get a better correlation.",
                    "label": 0
                },
                {
                    "sent": "Again, we beat the baseline by far, but then we said, OK, how does this compare to other approaches?",
                    "label": 0
                },
                {
                    "sent": "That was the whole point of evaluating on the Miller Charles data set.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we looked at other approaches that were relevant to us and these are the three classes that we selected.",
                    "label": 1
                },
                {
                    "sent": "The first one is the most related 1 two hours.",
                    "label": 0
                },
                {
                    "sent": "It's called the Wikipedia link based measure.",
                    "label": 0
                },
                {
                    "sent": "An basically it combines also the principle of the normalized web distance with traditional TF IDF weights and cosine similarity.",
                    "label": 0
                },
                {
                    "sent": "Makes a very complex measure using that an basically measures the distance between pages on Wikipedia using the links also.",
                    "label": 0
                },
                {
                    "sent": "So the structure of knowledge graph only there, not the machine readable version.",
                    "label": 0
                },
                {
                    "sent": "Then you have a whole bunch of ontology based approaches.",
                    "label": 0
                },
                {
                    "sent": "There's two surveys over there that summarized the results on that.",
                    "label": 0
                },
                {
                    "sent": "Miller Charles data set for these approaches and the results vary all over the place.",
                    "label": 0
                },
                {
                    "sent": "Place I'll show it in the next slide.",
                    "label": 0
                },
                {
                    "sent": "Basically this is the intuitive thing that distance is measured via the concept hierarchy, basically.",
                    "label": 1
                },
                {
                    "sent": "If you have to measure distance between Apple and a cucumber, you say OK, Apple is a fruit computers vegetable.",
                    "label": 0
                },
                {
                    "sent": "They're both plants, and so on.",
                    "label": 0
                },
                {
                    "sent": "Something like that.",
                    "label": 0
                },
                {
                    "sent": "And then there is the actually the simplest 1 digit card distance which just measures the shared nodes instead of the related to the total nodes that are linked to an that was measured in the study that cited there.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we compare it, we see that if you look at the most related measured Wikipedia based one, we see that we just don't beat it by 1.1.",
                    "label": 0
                },
                {
                    "sent": "On the correlation scale.",
                    "label": 0
                },
                {
                    "sent": "So we're very close there.",
                    "label": 0
                },
                {
                    "sent": "Definitely in the ballpark.",
                    "label": 0
                },
                {
                    "sent": "At the ontology based one, the lowest result that was reported that was correlation of 0.59.",
                    "label": 0
                },
                {
                    "sent": "So we beat that one, but we don't beat the highest result that was reported in both of the surveys which was in the lower 80s, which is actually much better.",
                    "label": 0
                },
                {
                    "sent": "Correlation and what we have.",
                    "label": 0
                },
                {
                    "sent": "If we then look at the simplest approach, surprisingly, it's also the most accurate one.",
                    "label": 0
                },
                {
                    "sent": "Now I don't know how accurate that claim I just made was because we can't really look.",
                    "label": 0
                },
                {
                    "sent": "They didn't really give out the disambiguations that they used to evaluate here.",
                    "label": 0
                },
                {
                    "sent": "They also used a mapping to concepts in a knowledge graph, but it wasn't made public so I have no idea whether we use the same disambiguation or not, and whether the results are compatible.",
                    "label": 0
                },
                {
                    "sent": "But in any case, the reported a very much higher correlation.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we did not beat the existing approaches, but we did achieve a clear positive correlation, which means which means that at least our core assumption is in the right direction.",
                    "label": 1
                },
                {
                    "sent": "And we outperformed the syntactic similarity of the normalized web distance.",
                    "label": 0
                },
                {
                    "sent": "On this data set, because this data set.",
                    "label": 0
                },
                {
                    "sent": "Really is not the most suitable one for our approach, but we chose it because it's highly popular in literature, right?",
                    "label": 0
                },
                {
                    "sent": "So what we now need to do is we need to.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look for a different data set.",
                    "label": 0
                },
                {
                    "sent": "OK, what we also need to realize is that the quality of the knowledge graph and the ambiguity of the Miller Charles data set have very high impact on what we're trying to do here.",
                    "label": 0
                },
                {
                    "sent": "For example, we had concepts journey and voyage that we had to disambiguate, and we looked at the descriptions on DB pedia.",
                    "label": 0
                },
                {
                    "sent": "We saw that.",
                    "label": 0
                },
                {
                    "sent": "OK, there are many disambiguation options for these terms, but none of them actually captured the intended meaning.",
                    "label": 1
                },
                {
                    "sent": "Actually, they mean the pedia travel.",
                    "label": 0
                },
                {
                    "sent": "Same thing for Latin mad house.",
                    "label": 0
                },
                {
                    "sent": "You have no equivalent of Freebase.",
                    "label": 1
                },
                {
                    "sent": "For that an all of these these inconsistencies, all of these gaps in the Knowledge Graph really lead to variations and gaps in the in the results of our evaluation.",
                    "label": 0
                },
                {
                    "sent": "So we wondered, can we measure something else to actually show that we.",
                    "label": 0
                },
                {
                    "sent": "Achieve what we want to do.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And what we want to do is we want to get this semantic awareness right.",
                    "label": 0
                },
                {
                    "sent": "So we want to be able to distinguish fish from a guitar.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "So to illustrate that, we created a set of 20 concept pairs that we said OK, these.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really illustrate that things that like pear and Apple, both fruits.",
                    "label": 0
                },
                {
                    "sent": "You need a lower distance there than between pair an Apple, the company and we created ten of those those cases, so 20 concepts in total.",
                    "label": 0
                },
                {
                    "sent": "And we measured the distance in all of these cases.",
                    "label": 0
                },
                {
                    "sent": "What we saw was that.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we did it on DB pedia, everything was fine or claim was actually supported here.",
                    "label": 0
                },
                {
                    "sent": "We always had a lower distance for the first 2 columns then between the first column in the last column.",
                    "label": 0
                },
                {
                    "sent": "Great we.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One minor anomaly when we evaluated on Freebase here between automobile an bus, both the transportation mechanism and automobile and bus in the computing context, which was really weird.",
                    "label": 0
                },
                {
                    "sent": "But it's also very minor difference there.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that led us to believe, OK, we're still on the right track.",
                    "label": 0
                },
                {
                    "sent": "So as a next step we really need to find a good evaluation corpus that has unambiguous concepts related to human assessments.",
                    "label": 0
                },
                {
                    "sent": "And one of the options that we're exploring now is this is the one that I mentioned on the slide there an we also want to look at the aspects of the Knowledge Graph that actually play a part in influencing these results.",
                    "label": 1
                },
                {
                    "sent": "Things like the Knowledge graph quality, the connectivity this size, maybe even the semantic richness which previous speaker.",
                    "label": 0
                },
                {
                    "sent": "Talks about and importantly, domain specific T because if it's a knowledge graph is more domain specific theory is that you get better results for your more accurate results for your distance calculation that let us to think OK, maybe we can even use the normalized semantic web distance to kind of evaluate knowledge graphs.",
                    "label": 0
                },
                {
                    "sent": "You know if you have a good ground truth for this human assessment.",
                    "label": 0
                },
                {
                    "sent": "And it's very domain specific.",
                    "label": 0
                },
                {
                    "sent": "Maybe you can assess the applicability of this knowledge graph using this ground truth and the normalized semantic web distance like you should at least get a very high correlation if your knowledge graph is really that domain specific and rich.",
                    "label": 0
                },
                {
                    "sent": "That's something that we're looking into in the future.",
                    "label": 0
                },
                {
                    "sent": "We're also looking into together with the University of Akron is basically to include this measure in an adaptive dissimilarity measure for documents on the document level.",
                    "label": 0
                },
                {
                    "sent": "So I actually look at different entities that are current document and measure distance between them and stuff like the earth movers, distance, square, quadratic form distance, and so on.",
                    "label": 0
                },
                {
                    "sent": "Alright.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's the conclusion we introduced this new distance in knowledge graph, advancing the idea of the normalized with semantic web distance.",
                    "label": 1
                },
                {
                    "sent": "We didn't invent the warm water here, but we did evaluated on Freebase and Pedia within established benchmark outperformed the normalized web distance using Bing but not the other approaches from literature.",
                    "label": 0
                },
                {
                    "sent": "But we did achieve a good correlation positively and we illustrated that it does exhibit some semantic awareness.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much and if you have any questions, I'm thinking now.",
                    "label": 0
                }
            ]
        }
    }
}