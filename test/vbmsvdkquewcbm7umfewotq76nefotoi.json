{
    "id": "vbmsvdkquewcbm7umfewotq76nefotoi",
    "title": "3D head pose estimation from multiple distant views",
    "info": {
        "author": [
            "Xenophon Zabulis, Foundation for Research and Technology - Hellas"
        ],
        "published": "Dec. 1, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Computer Vision->Face & Gesture Analysis"
        ]
    },
    "url": "http://videolectures.net/bmvc09_zabulis_3dhp/",
    "segmentation": [
        [
            "Hello, good morning, I'm going to talk about 3D head pose estimation from multiple distant views.",
            "That is, we will be estimating the location and the three rotational components of pose from multiple views and producing."
        ],
        [
            "A result like this one, we are using multiple views to cover relatively large area and we want to have subject move freely in the room and estimate the pose while also having possible occlusions.",
            "So the trouble is that because we want to cover a large area, the subject is imaged in a low resolution so we have to do something about that.",
            "And this is basically the focus of this talk.",
            "So we're going to be based like many other methods based on face detection.",
            "But we have to note that in this resolution, face detection is a difficult problem becausw not only because of the low resolution, not only because of the post variability, but also because of the multiple false positives that face detectors reply within sets images."
        ],
        [
            "So other approaches in the literature.",
            "Mainly focused in single view approaches, where the subject is imaged in a relatively high resolution so you can track rigid formation of landmarks.",
            "Or you can use the pixels of the face to vectorize them and manifold and embed them or do post classification or even use templates and detector arrays.",
            "However, in our case this is very difficult to do due to the low resolution and occlusions, so in the context of distal viewing there are several methods.",
            "About five that use multiple cameras and what they do is that they basically perform a single view pose estimation in each image, and in its view, and then they fuse the results, attempting for a higher accuracy.",
            "However, as we see in the literature review, such methods reply only."
        ],
        [
            "The course pose estimation result and also most of them reply only with two out of the three rotational components of pose.",
            "So in a nutshell, what we're going to do is that we're going to use the visual Hull of the subject to assist face detection.",
            "We are going to use a hypothetical sphere around the head where we are going to unfold the texture of the head on this fear, and we're going to perform face detection in this spherical image.",
            "This will simplify face detection and will directly he'll pose estimation, and I would like to comment that in this spherical image we are able to assemble an gather facial texture fragments that are collected from multiple views and we are able to utilize even small.",
            "Pieces and partial views of the face that are available to any view.",
            "So we started like this.",
            "We have multiple."
        ],
        [
            "Calibrated images we have a model of the background.",
            "We perform background subtraction and we extract the visual hole in the paper.",
            "We have the details of how we can do this in a time efficient manner and also be robust to errors of background subtraction and also be able to extract a smooth visual Hull by fitting an isosurface.",
            "So next we want to texture the model.",
            "Maybe this is not see."
        ],
        [
            "Very well, but what we're doing is that for each view we use as Ed buffer in order to compute visibility and reason which triangle of the visual Hull is visible to which camera or cameras.",
            "So in this using this information we are able to take the texture from the original images and paste it onto the visual Hull and have texture 3D model of the person.",
            "So the next step we're going to do is."
        ],
        [
            "To estimate the location of the head, basically the center of the head.",
            "In order to do this, we are using a 3D variant of the mean shift algorithm.",
            "We're using a spherical kernel which we register with the head so that the center of the kernel coincides with the center of the head.",
            "So we're driving this kernel, basically by minimizing the distances from the points on the surface of the head to this spherical kernel around the head, and we initialize.",
            "This operation basically by using the location estimate of the head in the premium in the previous frame and for the 1st frame.",
            "We basically do face detection in the original images and shoot Rays that intersect the visual Hull and when we have such a successful face detection, this event triggers automatically the whole pose estimation process.",
            "So before we jump into the."
        ],
        [
            "Essence of the method.",
            "We are doing the preparatory step, which is basically the course estimation, course orientation estimation and by orientation estimation I mean just pitch in your we margin orientation like a vector coming normal to the head, pointing wherever the head, headset and so we're doing the following.",
            "We have this spherical Cornell around the head and we project it to all images.",
            "Say that we had date images and we and of course this spherical kernel would project to a circle in all images and we go and do face detection in this small circular areas of the original images.",
            "And we also know the size that we expect the phase to occur in these images because the images are calibrated.",
            "So say that out of the 8 images we found faces in these three.",
            "So what we do is that we follow the imaginary lines from the camera centers.",
            "Through the face detection in the images finally intersecting the spherical Kernell in, say these three points, what we're going to do is that we find robust median of these three points on the sphere, and we're going to connect the center of the sphere to this robust medium to find the course orientation vector.",
            "So basically what we know now is a rough estimation of where the phase will occur on this sphere.",
            "Nothing more for the moment.",
            "So in this slide this slide summarizes."
        ],
        [
            "The whole method and this is the essence of the method.",
            "What we have now is a sphere around the head.",
            "We center project the texture of the visual hole on the surface of the sphere.",
            "So what we get is a globe that is painted with the texture of the human head.",
            "So if we unfold this texture this spherical texture, we get a spherical image that looks like that, and we say that if we are able to detect the center of the head and the two D pose of the face.",
            "In this image we can find posts of the whole head becausw the center of the face.",
            "Provides us this red boxes which is pitzen your and the two depots of the face in this image is in.",
            "This image provides us the green axis which is there all component.",
            "So two important observations about this image is first that this image will contain exactly one phase by construction and 2nd that the phase will occur as frontal in this image.",
            "Because if you project the texture from the center of the head towards the surface.",
            "Of this vertical kernel, no matter how the head is in the sphere, it always projects Frontale.",
            "So we are going to detect the face in this image in order to estimate pose, and since we know that."
        ],
        [
            "The faces occurs frontali in this image is we are OK by just using a generic plane frontal face detector.",
            "We're using the Viola Jones face detector from open CV and but we don't know the two depots that would be orientation of the face in this email.",
            "So we evaluate all poses and get of course multiple detections we get sometimes spurious detections, but there are very few of the main corpus of detections is clustered around the true face.",
            "And now we have to find the centroid of these detections.",
            "We find the centroid.",
            "This recall the face center.",
            "We have to also find the two deposal.",
            "To do this.",
            "We are exploiting the vertical symmetry of the face.",
            "We are using a simple normalized cross correlation operator that compares the symmetry of the two counterparts of the face and we optimize this operator to find the axis that symmetry is maximized through this criterion.",
            "So we eventually find an orientation where the symmetry is much, much.",
            "So supposedly we're done, we have.",
            "The center of the face.",
            "We have the 2D orientation were able to estimate the access as shown in the previous figure like the.",
            "The red and the green.",
            "The blue is just the cross product for a visual communication.",
            "So, but actually this is not the whole story becausw.",
            "Actually."
        ],
        [
            "If the face projects near the poles.",
            "What is going to happen is that the face is going to secure highly distorted.",
            "In this spherical image, and we're not going to be able to detect it.",
            "So what we're doing is that we use the course orientation estimate that we compute that earlier on so that.",
            "The computed course orientation."
        ],
        [
            "Right here, right there."
        ],
        [
            "Vector, So what we're doing is that we sent to rotate the sphere in place so that the face will project always on the equator of the sphere, and therefore the spherical rotations are going to be minimized.",
            "This is actually the same frame where we here we did not send to rotate the sphere, and here we sent a rotated so that the face occurs on the equator and therefore we can easily detect it.",
            "Actually we iterate this process at detecting.",
            "We rotate two or three times to gain higher accuracy so.",
            "Next is a valuation."
        ],
        [
            "We created the data set annotated with ground truth.",
            "This is a mannequin's head rotating on a tripod with marked rotational grading so that we know exactly where the had occurred and we sampled the hemisphere of poses.",
            "Additional roll and pitch rotations, and this data set is available online for public use.",
            "You can find the URL in the paper and in this way we are able to perform ground truth experiments.",
            "So comparing what our findings with ground truth, I think that's it compared to what we found by comparison comparing."
        ],
        [
            "Our estimations with ground truth is that we have another juror of about 3 degrees.",
            "We improve state of the art accuracy about 8 to 10 degrees depending on the method.",
            "In the context of distal viewing.",
            "And we also find out by doing a detailed error analysis that you can find in the paper that the accuracy of the method gracefully degrades with lack of coverage.",
            "So I'm going to show you small samples of the experiments.",
            "The full experiments are in the video and also in the page that I showed before and indicate some results.",
            "What you're going to see.",
            "Like such experiments in this experiment, there is absolutely no."
        ],
        [
            "Tracking or or filtering.",
            "You're saying the wrong output of the estimator so that we can evaluate the method without the benefit of tracking.",
            "And of course in the ground truth experiment there was no tracking involved, and here we it's a baseline experiment so that we know that we operate sufficiently well in order in all directions, despite the different mappings of textures in its direction.",
            "So in the next experiment we wanted to see a common."
        ],
        [
            "Gesture that would people do if you touch the head.",
            "If this affects the method.",
            "If we have errors in localization and so on.",
            "So we see that the method plays sufficiently well despite the distortion that visual Hull has.",
            "And in the next experiment we wish to see if we can utilize actually all cameras by having a person move freely in the room."
        ],
        [
            "Do and the sample the whole volume of the room and walk and go up and down so that we can see that there is no sweet spot for our method.",
            "But we can simply walk around the room and continue.",
            "Estimating pose, actually you saw this in the first video with a dancer, which is actually a reference data data set from in area, which is the rest of the datasets around, But the first one is from in real."
        ],
        [
            "So the next experiment I find interesting becausw where the subject is going to severely occlude its face from the frontal camera.",
            "You're going to sit in a minute and what the subject is doing is hiding his face from the camera.",
            "That is seeing the face frontally.",
            "So basically the algorithm.",
            "What it does is that it collects texture fragments from the lateral views.",
            "See this camera here.",
            "So the algorithm collects textual fragments from the lateral views in order to assemble the spherical image and continue estimating pose.",
            "Despite that, the best view is rendered useless.",
            "So.",
            "And in the last.",
            "You must here we just apply the method in two Subs."
        ],
        [
            "So in aroma because we are interested in using this in smart environments and be able to estimate where people are heading, or if two people are facing each other, and possibly if we.",
            "Augment this with the audio.",
            "Make hypothesis of if these two people are talking to each other or not.",
            "We of course.",
            "Understand when one person faces the other by when the access code inside of the pose estimation.",
            "So work in progress.",
            "It's future work in the paper, but we are continuing to work on this."
        ],
        [
            "Basically we want to study and apply robust tracking framework and see which is the most appropriate framework for our case to do tracking and have a prediction.",
            "And also we are optimized the code.",
            "Currently we have the first version of a GPU implementation which runs at approximately 4.5 frames per second and this is just the first implementation we're going to optimize it more not only by better GPU programming.",
            "But not only by getting a better GPU card, but also by computing only what we actually need.",
            "Because currently in our implementation we are computing the whole visual how the whole texture for the whole body queries.",
            "In fact, we need just the face, the texture of the face and of course in this task having a robust tracking framework that will provide the separate addiction would help a lot.",
            "So.",
            "Thank you for your attention."
        ],
        [
            "OK, so basically this is a function of several things.",
            "Basically the most important thing is the size of the room you want to.",
            "Track, so if you have a big rooms you need more camera.",
            "So in our room it is about 5 by 5 meters.",
            "We did this successfully with four cameras and we got an average error of about 5 to 6 degrees and the error dropped to three degrees when we used eight cameras.",
            "It also helps if you have a camera in the ceiling and.",
            "Also.",
            "Yeah, and also comment is that basically at any moment if you place the camera's peripherical in the room.",
            "Basically it's just two or three cameras that see the face.",
            "It's time, but you need lots of cameras depending on the coverage you wish to have in the room.",
            "So for example, if you want to cover the corner you need to place a car camera in the corner to cover this space, so.",
            "Yes, two or three cameras to see.",
            "Basically you need to have a rough estimation of the visual health, so you need two or three cameras basically.",
            "In these images is about 10:15 by 15 pixels in its image.",
            "But because we have partial pixels, partial viewings from the lateral cameras, we are able to create a spherical image that has more pixels.",
            "That it's.",
            "Yes, I we cope with the distortion by rotating the sphere to go to so that the face projects on there.",
            "Equator, but just.",
            "If we just start.",
            "Face detecting from multiple views we have to do many face detections and this basically more computational time.",
            "And also it's this.",
            "Fear is a nice way to formulate so that you go we can go around on any direction and having a spherical image, you just detect the face in one image.",
            "You don't have to evaluate men images."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello, good morning, I'm going to talk about 3D head pose estimation from multiple distant views.",
                    "label": 0
                },
                {
                    "sent": "That is, we will be estimating the location and the three rotational components of pose from multiple views and producing.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A result like this one, we are using multiple views to cover relatively large area and we want to have subject move freely in the room and estimate the pose while also having possible occlusions.",
                    "label": 0
                },
                {
                    "sent": "So the trouble is that because we want to cover a large area, the subject is imaged in a low resolution so we have to do something about that.",
                    "label": 0
                },
                {
                    "sent": "And this is basically the focus of this talk.",
                    "label": 0
                },
                {
                    "sent": "So we're going to be based like many other methods based on face detection.",
                    "label": 1
                },
                {
                    "sent": "But we have to note that in this resolution, face detection is a difficult problem becausw not only because of the low resolution, not only because of the post variability, but also because of the multiple false positives that face detectors reply within sets images.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So other approaches in the literature.",
                    "label": 0
                },
                {
                    "sent": "Mainly focused in single view approaches, where the subject is imaged in a relatively high resolution so you can track rigid formation of landmarks.",
                    "label": 1
                },
                {
                    "sent": "Or you can use the pixels of the face to vectorize them and manifold and embed them or do post classification or even use templates and detector arrays.",
                    "label": 0
                },
                {
                    "sent": "However, in our case this is very difficult to do due to the low resolution and occlusions, so in the context of distal viewing there are several methods.",
                    "label": 0
                },
                {
                    "sent": "About five that use multiple cameras and what they do is that they basically perform a single view pose estimation in each image, and in its view, and then they fuse the results, attempting for a higher accuracy.",
                    "label": 1
                },
                {
                    "sent": "However, as we see in the literature review, such methods reply only.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The course pose estimation result and also most of them reply only with two out of the three rotational components of pose.",
                    "label": 0
                },
                {
                    "sent": "So in a nutshell, what we're going to do is that we're going to use the visual Hull of the subject to assist face detection.",
                    "label": 1
                },
                {
                    "sent": "We are going to use a hypothetical sphere around the head where we are going to unfold the texture of the head on this fear, and we're going to perform face detection in this spherical image.",
                    "label": 1
                },
                {
                    "sent": "This will simplify face detection and will directly he'll pose estimation, and I would like to comment that in this spherical image we are able to assemble an gather facial texture fragments that are collected from multiple views and we are able to utilize even small.",
                    "label": 0
                },
                {
                    "sent": "Pieces and partial views of the face that are available to any view.",
                    "label": 0
                },
                {
                    "sent": "So we started like this.",
                    "label": 0
                },
                {
                    "sent": "We have multiple.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Calibrated images we have a model of the background.",
                    "label": 0
                },
                {
                    "sent": "We perform background subtraction and we extract the visual hole in the paper.",
                    "label": 0
                },
                {
                    "sent": "We have the details of how we can do this in a time efficient manner and also be robust to errors of background subtraction and also be able to extract a smooth visual Hull by fitting an isosurface.",
                    "label": 0
                },
                {
                    "sent": "So next we want to texture the model.",
                    "label": 0
                },
                {
                    "sent": "Maybe this is not see.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very well, but what we're doing is that for each view we use as Ed buffer in order to compute visibility and reason which triangle of the visual Hull is visible to which camera or cameras.",
                    "label": 1
                },
                {
                    "sent": "So in this using this information we are able to take the texture from the original images and paste it onto the visual Hull and have texture 3D model of the person.",
                    "label": 0
                },
                {
                    "sent": "So the next step we're going to do is.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To estimate the location of the head, basically the center of the head.",
                    "label": 0
                },
                {
                    "sent": "In order to do this, we are using a 3D variant of the mean shift algorithm.",
                    "label": 1
                },
                {
                    "sent": "We're using a spherical kernel which we register with the head so that the center of the kernel coincides with the center of the head.",
                    "label": 0
                },
                {
                    "sent": "So we're driving this kernel, basically by minimizing the distances from the points on the surface of the head to this spherical kernel around the head, and we initialize.",
                    "label": 0
                },
                {
                    "sent": "This operation basically by using the location estimate of the head in the premium in the previous frame and for the 1st frame.",
                    "label": 1
                },
                {
                    "sent": "We basically do face detection in the original images and shoot Rays that intersect the visual Hull and when we have such a successful face detection, this event triggers automatically the whole pose estimation process.",
                    "label": 0
                },
                {
                    "sent": "So before we jump into the.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Essence of the method.",
                    "label": 0
                },
                {
                    "sent": "We are doing the preparatory step, which is basically the course estimation, course orientation estimation and by orientation estimation I mean just pitch in your we margin orientation like a vector coming normal to the head, pointing wherever the head, headset and so we're doing the following.",
                    "label": 0
                },
                {
                    "sent": "We have this spherical Cornell around the head and we project it to all images.",
                    "label": 1
                },
                {
                    "sent": "Say that we had date images and we and of course this spherical kernel would project to a circle in all images and we go and do face detection in this small circular areas of the original images.",
                    "label": 0
                },
                {
                    "sent": "And we also know the size that we expect the phase to occur in these images because the images are calibrated.",
                    "label": 0
                },
                {
                    "sent": "So say that out of the 8 images we found faces in these three.",
                    "label": 0
                },
                {
                    "sent": "So what we do is that we follow the imaginary lines from the camera centers.",
                    "label": 0
                },
                {
                    "sent": "Through the face detection in the images finally intersecting the spherical Kernell in, say these three points, what we're going to do is that we find robust median of these three points on the sphere, and we're going to connect the center of the sphere to this robust medium to find the course orientation vector.",
                    "label": 0
                },
                {
                    "sent": "So basically what we know now is a rough estimation of where the phase will occur on this sphere.",
                    "label": 0
                },
                {
                    "sent": "Nothing more for the moment.",
                    "label": 0
                },
                {
                    "sent": "So in this slide this slide summarizes.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The whole method and this is the essence of the method.",
                    "label": 0
                },
                {
                    "sent": "What we have now is a sphere around the head.",
                    "label": 1
                },
                {
                    "sent": "We center project the texture of the visual hole on the surface of the sphere.",
                    "label": 0
                },
                {
                    "sent": "So what we get is a globe that is painted with the texture of the human head.",
                    "label": 0
                },
                {
                    "sent": "So if we unfold this texture this spherical texture, we get a spherical image that looks like that, and we say that if we are able to detect the center of the head and the two D pose of the face.",
                    "label": 0
                },
                {
                    "sent": "In this image we can find posts of the whole head becausw the center of the face.",
                    "label": 1
                },
                {
                    "sent": "Provides us this red boxes which is pitzen your and the two depots of the face in this image is in.",
                    "label": 0
                },
                {
                    "sent": "This image provides us the green axis which is there all component.",
                    "label": 0
                },
                {
                    "sent": "So two important observations about this image is first that this image will contain exactly one phase by construction and 2nd that the phase will occur as frontal in this image.",
                    "label": 0
                },
                {
                    "sent": "Because if you project the texture from the center of the head towards the surface.",
                    "label": 0
                },
                {
                    "sent": "Of this vertical kernel, no matter how the head is in the sphere, it always projects Frontale.",
                    "label": 0
                },
                {
                    "sent": "So we are going to detect the face in this image in order to estimate pose, and since we know that.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The faces occurs frontali in this image is we are OK by just using a generic plane frontal face detector.",
                    "label": 1
                },
                {
                    "sent": "We're using the Viola Jones face detector from open CV and but we don't know the two depots that would be orientation of the face in this email.",
                    "label": 0
                },
                {
                    "sent": "So we evaluate all poses and get of course multiple detections we get sometimes spurious detections, but there are very few of the main corpus of detections is clustered around the true face.",
                    "label": 1
                },
                {
                    "sent": "And now we have to find the centroid of these detections.",
                    "label": 0
                },
                {
                    "sent": "We find the centroid.",
                    "label": 1
                },
                {
                    "sent": "This recall the face center.",
                    "label": 0
                },
                {
                    "sent": "We have to also find the two deposal.",
                    "label": 0
                },
                {
                    "sent": "To do this.",
                    "label": 0
                },
                {
                    "sent": "We are exploiting the vertical symmetry of the face.",
                    "label": 0
                },
                {
                    "sent": "We are using a simple normalized cross correlation operator that compares the symmetry of the two counterparts of the face and we optimize this operator to find the axis that symmetry is maximized through this criterion.",
                    "label": 0
                },
                {
                    "sent": "So we eventually find an orientation where the symmetry is much, much.",
                    "label": 0
                },
                {
                    "sent": "So supposedly we're done, we have.",
                    "label": 0
                },
                {
                    "sent": "The center of the face.",
                    "label": 0
                },
                {
                    "sent": "We have the 2D orientation were able to estimate the access as shown in the previous figure like the.",
                    "label": 0
                },
                {
                    "sent": "The red and the green.",
                    "label": 0
                },
                {
                    "sent": "The blue is just the cross product for a visual communication.",
                    "label": 0
                },
                {
                    "sent": "So, but actually this is not the whole story becausw.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If the face projects near the poles.",
                    "label": 1
                },
                {
                    "sent": "What is going to happen is that the face is going to secure highly distorted.",
                    "label": 0
                },
                {
                    "sent": "In this spherical image, and we're not going to be able to detect it.",
                    "label": 0
                },
                {
                    "sent": "So what we're doing is that we use the course orientation estimate that we compute that earlier on so that.",
                    "label": 0
                },
                {
                    "sent": "The computed course orientation.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right here, right there.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Vector, So what we're doing is that we sent to rotate the sphere in place so that the face will project always on the equator of the sphere, and therefore the spherical rotations are going to be minimized.",
                    "label": 1
                },
                {
                    "sent": "This is actually the same frame where we here we did not send to rotate the sphere, and here we sent a rotated so that the face occurs on the equator and therefore we can easily detect it.",
                    "label": 0
                },
                {
                    "sent": "Actually we iterate this process at detecting.",
                    "label": 0
                },
                {
                    "sent": "We rotate two or three times to gain higher accuracy so.",
                    "label": 0
                },
                {
                    "sent": "Next is a valuation.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We created the data set annotated with ground truth.",
                    "label": 1
                },
                {
                    "sent": "This is a mannequin's head rotating on a tripod with marked rotational grading so that we know exactly where the had occurred and we sampled the hemisphere of poses.",
                    "label": 1
                },
                {
                    "sent": "Additional roll and pitch rotations, and this data set is available online for public use.",
                    "label": 0
                },
                {
                    "sent": "You can find the URL in the paper and in this way we are able to perform ground truth experiments.",
                    "label": 0
                },
                {
                    "sent": "So comparing what our findings with ground truth, I think that's it compared to what we found by comparison comparing.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our estimations with ground truth is that we have another juror of about 3 degrees.",
                    "label": 1
                },
                {
                    "sent": "We improve state of the art accuracy about 8 to 10 degrees depending on the method.",
                    "label": 1
                },
                {
                    "sent": "In the context of distal viewing.",
                    "label": 0
                },
                {
                    "sent": "And we also find out by doing a detailed error analysis that you can find in the paper that the accuracy of the method gracefully degrades with lack of coverage.",
                    "label": 1
                },
                {
                    "sent": "So I'm going to show you small samples of the experiments.",
                    "label": 0
                },
                {
                    "sent": "The full experiments are in the video and also in the page that I showed before and indicate some results.",
                    "label": 0
                },
                {
                    "sent": "What you're going to see.",
                    "label": 0
                },
                {
                    "sent": "Like such experiments in this experiment, there is absolutely no.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tracking or or filtering.",
                    "label": 0
                },
                {
                    "sent": "You're saying the wrong output of the estimator so that we can evaluate the method without the benefit of tracking.",
                    "label": 0
                },
                {
                    "sent": "And of course in the ground truth experiment there was no tracking involved, and here we it's a baseline experiment so that we know that we operate sufficiently well in order in all directions, despite the different mappings of textures in its direction.",
                    "label": 0
                },
                {
                    "sent": "So in the next experiment we wanted to see a common.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gesture that would people do if you touch the head.",
                    "label": 0
                },
                {
                    "sent": "If this affects the method.",
                    "label": 0
                },
                {
                    "sent": "If we have errors in localization and so on.",
                    "label": 0
                },
                {
                    "sent": "So we see that the method plays sufficiently well despite the distortion that visual Hull has.",
                    "label": 0
                },
                {
                    "sent": "And in the next experiment we wish to see if we can utilize actually all cameras by having a person move freely in the room.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do and the sample the whole volume of the room and walk and go up and down so that we can see that there is no sweet spot for our method.",
                    "label": 0
                },
                {
                    "sent": "But we can simply walk around the room and continue.",
                    "label": 0
                },
                {
                    "sent": "Estimating pose, actually you saw this in the first video with a dancer, which is actually a reference data data set from in area, which is the rest of the datasets around, But the first one is from in real.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the next experiment I find interesting becausw where the subject is going to severely occlude its face from the frontal camera.",
                    "label": 0
                },
                {
                    "sent": "You're going to sit in a minute and what the subject is doing is hiding his face from the camera.",
                    "label": 0
                },
                {
                    "sent": "That is seeing the face frontally.",
                    "label": 0
                },
                {
                    "sent": "So basically the algorithm.",
                    "label": 0
                },
                {
                    "sent": "What it does is that it collects texture fragments from the lateral views.",
                    "label": 0
                },
                {
                    "sent": "See this camera here.",
                    "label": 0
                },
                {
                    "sent": "So the algorithm collects textual fragments from the lateral views in order to assemble the spherical image and continue estimating pose.",
                    "label": 0
                },
                {
                    "sent": "Despite that, the best view is rendered useless.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And in the last.",
                    "label": 0
                },
                {
                    "sent": "You must here we just apply the method in two Subs.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in aroma because we are interested in using this in smart environments and be able to estimate where people are heading, or if two people are facing each other, and possibly if we.",
                    "label": 0
                },
                {
                    "sent": "Augment this with the audio.",
                    "label": 0
                },
                {
                    "sent": "Make hypothesis of if these two people are talking to each other or not.",
                    "label": 0
                },
                {
                    "sent": "We of course.",
                    "label": 0
                },
                {
                    "sent": "Understand when one person faces the other by when the access code inside of the pose estimation.",
                    "label": 0
                },
                {
                    "sent": "So work in progress.",
                    "label": 0
                },
                {
                    "sent": "It's future work in the paper, but we are continuing to work on this.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically we want to study and apply robust tracking framework and see which is the most appropriate framework for our case to do tracking and have a prediction.",
                    "label": 1
                },
                {
                    "sent": "And also we are optimized the code.",
                    "label": 0
                },
                {
                    "sent": "Currently we have the first version of a GPU implementation which runs at approximately 4.5 frames per second and this is just the first implementation we're going to optimize it more not only by better GPU programming.",
                    "label": 0
                },
                {
                    "sent": "But not only by getting a better GPU card, but also by computing only what we actually need.",
                    "label": 0
                },
                {
                    "sent": "Because currently in our implementation we are computing the whole visual how the whole texture for the whole body queries.",
                    "label": 0
                },
                {
                    "sent": "In fact, we need just the face, the texture of the face and of course in this task having a robust tracking framework that will provide the separate addiction would help a lot.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so basically this is a function of several things.",
                    "label": 0
                },
                {
                    "sent": "Basically the most important thing is the size of the room you want to.",
                    "label": 0
                },
                {
                    "sent": "Track, so if you have a big rooms you need more camera.",
                    "label": 0
                },
                {
                    "sent": "So in our room it is about 5 by 5 meters.",
                    "label": 0
                },
                {
                    "sent": "We did this successfully with four cameras and we got an average error of about 5 to 6 degrees and the error dropped to three degrees when we used eight cameras.",
                    "label": 0
                },
                {
                    "sent": "It also helps if you have a camera in the ceiling and.",
                    "label": 0
                },
                {
                    "sent": "Also.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and also comment is that basically at any moment if you place the camera's peripherical in the room.",
                    "label": 0
                },
                {
                    "sent": "Basically it's just two or three cameras that see the face.",
                    "label": 0
                },
                {
                    "sent": "It's time, but you need lots of cameras depending on the coverage you wish to have in the room.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you want to cover the corner you need to place a car camera in the corner to cover this space, so.",
                    "label": 0
                },
                {
                    "sent": "Yes, two or three cameras to see.",
                    "label": 0
                },
                {
                    "sent": "Basically you need to have a rough estimation of the visual health, so you need two or three cameras basically.",
                    "label": 0
                },
                {
                    "sent": "In these images is about 10:15 by 15 pixels in its image.",
                    "label": 0
                },
                {
                    "sent": "But because we have partial pixels, partial viewings from the lateral cameras, we are able to create a spherical image that has more pixels.",
                    "label": 0
                },
                {
                    "sent": "That it's.",
                    "label": 0
                },
                {
                    "sent": "Yes, I we cope with the distortion by rotating the sphere to go to so that the face projects on there.",
                    "label": 0
                },
                {
                    "sent": "Equator, but just.",
                    "label": 0
                },
                {
                    "sent": "If we just start.",
                    "label": 0
                },
                {
                    "sent": "Face detecting from multiple views we have to do many face detections and this basically more computational time.",
                    "label": 0
                },
                {
                    "sent": "And also it's this.",
                    "label": 0
                },
                {
                    "sent": "Fear is a nice way to formulate so that you go we can go around on any direction and having a spherical image, you just detect the face in one image.",
                    "label": 0
                },
                {
                    "sent": "You don't have to evaluate men images.",
                    "label": 0
                }
            ]
        }
    }
}