{
    "id": "26hv3knh7kjyx3thwqclhttqk2scnisg",
    "title": "Net2Net: Accelerating Learning via Knowledge Transfer",
    "info": {
        "author": [
            "Tianqi Chen, Department of Computer Science and Engineering, University of Washington"
        ],
        "published": "May 27, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/iclr2016_chen_net2net/",
    "segmentation": [
        [
            "Today I'm going to talk about network.",
            "Now.",
            "This is a collaboration with my intern host Ian, who works at Google and Junctions."
        ],
        [
            "OK.",
            "So."
        ],
        [
            "If."
        ],
        [
            "Let me make a not very appropriate analogy between neural network and human networks in a sense that if we took take a look at deep learning problems today, usually if you if you go back to the to the to the biological neural networks, you will find that the monuments you'll get the more intelligent tend to be.",
            "And I think this kind of analogy kind of holds similarly true in in artificial neural net as well as you get more later, you will need a much larger model eventually.",
            "However, training those large large models usually correspond to a longer training time, and this will list already created a huge problem here.",
            "And if you think about neural networking nowadays, it takes like two days a week to train.",
            "Eventually it will be even harder if you want to really reach lose more high level as you get more data."
        ],
        [
            "So if you think about Dick planning workflow today, there is this very ideal world that you have your problems.",
            "You design a very perfect neural network that solve your problem.",
            "You'll give enough data and bam, that you get the perfect weight and your job is done.",
            "However, if you if you come back and think about reality, what usually happens is that you will normally have some initial designs of neural networks that possibly work OK and then you will iterate over these designs to come up.",
            "Small innovations of neural networks.",
            "And this loops continue on over and over until you find something that says satisfy your need and OK, that you can go with that.",
            "But potentially you will need even more, more different architectures.",
            "And and we see that there are a lot of different architectures pop up every every year.",
            "I would say maybe even even more frequently every year."
        ],
        [
            "So if you put it into a more broader context, you will find that this kind of loop of experiment is happens everywhere.",
            "It happens in deep learning in a sense that usually you want to grow your model when you get more data.",
            "It also happens in machine learning, while you do model selections or you want to select from wider range of data.",
            "And this this is quite natural need as you know it's important to grow the model complexity to match the match.",
            "The data science that I said you get.",
            "And I think there's ultimate dream here that we can build intelligent systems and continuously learn from word and being under one of the important fact problem there is that as you even getting more data you really need to grow and evolve your model and this kind of redesign of Model designer experiment loop is happening.",
            "Will be more crucial on that setting actually."
        ],
        [
            "OK, so if we tickle tickle?"
        ],
        [
            "Look at the.",
            "Neural network training today.",
            "So assume now that we have already trained neural network.",
            "There are several ways that we can deal with this, and what people usually follow today is the most dumbest way I will cause a zoom anyway.",
            "So what you will do is that you will break the already train your network into proteins and retrain from scratch, and this maybe not a very appropriate analogy, but I will say this is usually what we do an.",
            "We really think we really want to find ways to do better than this simple approach.",
            "A second way that that is usually what people can do, is trying to use the already train your network as a teacher and try to build a supervision signal to kind of supervised the new net that you want to train.",
            "So hopefully you the teacher again help to get the student to result in a faster training."
        ],
        [
            "We have tried this approach actually when I started my intern I tried for like first in the first month.",
            "I tried several kind of this approach and unfortunately this approach does not work very well in the sense of like it does not gives you very fast convergence.",
            "However, I think this is a still important direction in a sense that this is a generic approach that allows you to transfer knowledge from arbitrary neural network to another arbitrary architecture.",
            "So to briefly describe what.",
            "I tried, basically we derived supervision signal that tries to decode every layers of the already trained network to the new network in a sense that want the new network to quickly have the ability to have the same ability as the as the teacher model, and it turns out that it does not converge.",
            "Is it does not help the convergence in our in our very order of magnitude manner."
        ],
        [
            "So.",
            "We so then what in this talk we will resort to a much simpler approach which we call Network tab.",
            "That actually helps us in this process, and I think this is a very tiny step towards this more general goal of continuous model evolution."
        ],
        [
            "So to give you a general overview, if you would think about traditional traditional neural network neural network flow, which will normally do is you will do this re experiment loop where you will discard your already trained model every time in the network.",
            "What we're doing instead is that we try to reuse already trained model, you some transformations to transform that already trained model to our kind of equivalent new models and continue training from the from the transfer form transform the model."
        ],
        [
            "So the reason that we do this is actually actually from some observation from experiment.",
            "So we find that.",
            "Actually I think this is kind of a common sense in a sense that the more randomized components you have in your neural network, the harder or the longer time it will take to train the neural network.",
            "So in this experiment we basically what we do is that we take already trained neural network and randomly reinitialize some of the layers and you will find that if you randomly initialize model.",
            "Half of layers, actually the convergence speed is kind of already bad, so the key observation here is that we really want to get rid of the random randomly initialized components and trying to fully utilize the original knowledge as as much as possible."
        ],
        [
            "So this motivates us to this simple solution we call, which we call function preserving transformations where we want to take already trained neural network, do some transformation such that the bigger net is kind of functional equivalent in the sense that for any given input they give the equivalent output, then we will do a bit perturbation to the transform neural network and continue training from the from the Internet from the network."
        ],
        [
            "So we started two specific ways to actually expand model capacity.",
            "The 1st Way is we call net wider net which we where we want to increase the number of channels in each of the in each of the layers.",
            "The 2nd way we we call it not to deeper net which is we want to increase the depth of neural network.",
            "These two operations can be composed in arbitrary order so that we can basically grow both in terms of width as well as steps.",
            "And the question is that whether we can find effective functional preserving transformations to do this to cancel operation."
        ],
        [
            "For the wider transformation, what will what we propose to do is basically randomly duplicates all the channels that you want to widen.",
            "Then after the random duplication we want to we want to reconnect the ways but but divide the edges by the duplication factor so the solar transforming network is exactly equivalent to our original network.",
            "And this also applies for other for the general settings such as convolution neural networks, where all these colors are correspond to each of the individual channels.",
            "Anne."
        ],
        [
            "The key takeaway here is that basically after transformation, if you don't add any noise, then the new network is exactly equivalent to the original neural networks.",
            "We still need to add a bit of noise to product perturbing to continue, because otherwise it cannot break the symmetry."
        ],
        [
            "To grow the noon, noon, noon, and network deeper what we what we can do is that we can take our existing layer and use some kind of factorization that factorized existing layer into two layers and basically useless factorization as operator.",
            "So that we can we can enlarge the depth of a neural network and the question is that how we can come up with effective factorization for for the general architectures that we're interested in.",
            "It turns out we use a very simple idea that.",
            "Turns out to work.",
            "The ID."
        ],
        [
            "Is that we basically?",
            "Add add I identity mapping on top of the existing layer.",
            "So basically a factorization of trivial factorization of a matrix can be identity times that comes that original matrix.",
            "Of course there can be other account affect relations if if can be applied here.",
            "If if they are, they can be faster computed.",
            "So this kind of natural deeper net approach by adding identities help help us to generalize to general modules, including the piecewise linear functions in the in the activation function.",
            "And it also works for other cases like batch normalization.",
            "So it basically works for the general architecture that we care about today.",
            "And there are follow-up works that handles possible other possible works of nonlinearity by researchers from Microsoft Microsoft Research this year."
        ],
        [
            "So.",
            "So you know I have already described this 222 kind of methods that one widen the network and another approach is too dependent network.",
            "The question is that whether we can, we can see we can do some experiment to show whether it can really help us to speed up our experiment cycle.",
            "So."
        ],
        [
            "So you know what experiment will conduct all experiment on the four image data set using the stand using the standard inception model and one of the one of the important thing when we continue training here is that we want to use a smaller learning rate to reflect the end of the learning rate schedule as opposed to use the beginning of limited cause because when you get all ready to get to the part of the of the of the training of a teacher, now the limit is already quite small.",
            "So."
        ],
        [
            "So the first experiment we do is that we will take actually a smaller inception, that is, that is sqrt 2 capacity internal channels, each in each of layers.",
            "So it corresponds to like half of a perimetre as our original inception.",
            "So we will start from a small exception already trained.",
            "Smaller inception as a source model and try to use the net to item wider net to continue training to continue trying the original inception model.",
            "Here all the.",
            "All the all the solid lines are the natural natural net net.",
            "Wider net net approach.",
            "And we also compared to a baseline that randomly that copies part of filters over at randomly initialize the rest of filters.",
            "And finally the green line on the on the bottom on the bottom part is there is a line that correspond to what will happen if we re trying the entire model from scratch.",
            "So there are several takeaways here.",
            "The first takeaway is that we can find by using the natural net approach.",
            "Actually all the models kind of get to similar similar performers.",
            "Also, this is an empirical same cause.",
            "These are local minimums, so there's no guarantee that converge to the same global minima.",
            "But empirically, we can find that by initialize from from using natural net, we can't get too similar performers.",
            "The second takeaway messages that by using that to widen and we kind of get faster convergence ramp up speed.",
            "And you can find that this green line is the line is there is a training.",
            "Accuracy is a test accuracy of the original of the original small inception model and we can find that.",
            "Using that weapon that we can quickly get to that performance and improve from there.",
            "It will translate to around like three to four times speedup compared to directly train from scratch."
        ],
        [
            "We also compared to a net to deeper net approach and here we run a simple experiment that takes a standard exception, an ad for layers of convolution layer to each of our issue of Inception Tower, and we compared against that range from scratch approach.",
            "We gotta find that we can observe similar similar things in terms of faster convergence.",
            "Ann and the good and consensus in the final final validation accuracy.",
            "So we're seeing the experiment results for these two types of approaches.",
            "One that helps you to grow network deeper and another one that helps you to grow network weather and one thing one of interesting things we can think about is that if we can take all these approaches and design wider experiments and try to explore new model designs so that basically we take already trained the state of art neural networks and apply these transformations and quickly see if these network design is good or not.",
            "And."
        ],
        [
            "So, so we do that.",
            "We took a standard inception model actually and we apply both net wider net transformation as well as the net to deeper net transformation so we can find that by applying net wider that actually we get a much better get.",
            "A slight better performance than the state of another state of art model we had there.",
            "We didn't get because of the limited resources and time we didn't get chance to retrieve those bigger models from scratch.",
            "So but however we we we put a curve of the convergence of the original inception model in the Dash line here.",
            "So in theory the convergence of this bigger model should be even slower than this than this dashed lines.",
            "So we can find that like it's actually by using our method, we can quickly quickly start exploration even long before there's things the things converge if you do things from scratch.",
            "This kind of confirms availability of using natural net approaches to do more exploration, and I think this is the small small thing that small contribution that we can do to help us to explore the models faster in the future."
        ],
        [
            "So.",
            "To conclude my talk.",
            "I think there are several takeaways, though.",
            "First takeaway is that we need to do better than the dump approach that directly dumps already.",
            "Directly dump all the old models and retrain from scratch.",
            "It's possible to reuse already trained model and had to help training bigger models.",
            "And one thing important thing when we doing this kind of approach here is they want to avoid random components and we use functional preserving transformations to to achieve that goal and.",
            "In the in, the more general bigger picture, we always want to think about continuous training beyond beyond single shot, and I think there are more interesting space for exploration here.",
            "This is this is a simple approach that scratch the surface of of this.",
            "Of these problems in the bigger context.",
            "So."
        ],
        [
            "I will conclude my talk and I will be happy to take any questions.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Today I'm going to talk about network.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This is a collaboration with my intern host Ian, who works at Google and Junctions.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me make a not very appropriate analogy between neural network and human networks in a sense that if we took take a look at deep learning problems today, usually if you if you go back to the to the to the biological neural networks, you will find that the monuments you'll get the more intelligent tend to be.",
                    "label": 0
                },
                {
                    "sent": "And I think this kind of analogy kind of holds similarly true in in artificial neural net as well as you get more later, you will need a much larger model eventually.",
                    "label": 0
                },
                {
                    "sent": "However, training those large large models usually correspond to a longer training time, and this will list already created a huge problem here.",
                    "label": 1
                },
                {
                    "sent": "And if you think about neural networking nowadays, it takes like two days a week to train.",
                    "label": 0
                },
                {
                    "sent": "Eventually it will be even harder if you want to really reach lose more high level as you get more data.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you think about Dick planning workflow today, there is this very ideal world that you have your problems.",
                    "label": 0
                },
                {
                    "sent": "You design a very perfect neural network that solve your problem.",
                    "label": 0
                },
                {
                    "sent": "You'll give enough data and bam, that you get the perfect weight and your job is done.",
                    "label": 0
                },
                {
                    "sent": "However, if you if you come back and think about reality, what usually happens is that you will normally have some initial designs of neural networks that possibly work OK and then you will iterate over these designs to come up.",
                    "label": 0
                },
                {
                    "sent": "Small innovations of neural networks.",
                    "label": 0
                },
                {
                    "sent": "And this loops continue on over and over until you find something that says satisfy your need and OK, that you can go with that.",
                    "label": 0
                },
                {
                    "sent": "But potentially you will need even more, more different architectures.",
                    "label": 0
                },
                {
                    "sent": "And and we see that there are a lot of different architectures pop up every every year.",
                    "label": 0
                },
                {
                    "sent": "I would say maybe even even more frequently every year.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if you put it into a more broader context, you will find that this kind of loop of experiment is happens everywhere.",
                    "label": 0
                },
                {
                    "sent": "It happens in deep learning in a sense that usually you want to grow your model when you get more data.",
                    "label": 1
                },
                {
                    "sent": "It also happens in machine learning, while you do model selections or you want to select from wider range of data.",
                    "label": 0
                },
                {
                    "sent": "And this this is quite natural need as you know it's important to grow the model complexity to match the match.",
                    "label": 1
                },
                {
                    "sent": "The data science that I said you get.",
                    "label": 0
                },
                {
                    "sent": "And I think there's ultimate dream here that we can build intelligent systems and continuously learn from word and being under one of the important fact problem there is that as you even getting more data you really need to grow and evolve your model and this kind of redesign of Model designer experiment loop is happening.",
                    "label": 0
                },
                {
                    "sent": "Will be more crucial on that setting actually.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so if we tickle tickle?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Look at the.",
                    "label": 0
                },
                {
                    "sent": "Neural network training today.",
                    "label": 0
                },
                {
                    "sent": "So assume now that we have already trained neural network.",
                    "label": 0
                },
                {
                    "sent": "There are several ways that we can deal with this, and what people usually follow today is the most dumbest way I will cause a zoom anyway.",
                    "label": 0
                },
                {
                    "sent": "So what you will do is that you will break the already train your network into proteins and retrain from scratch, and this maybe not a very appropriate analogy, but I will say this is usually what we do an.",
                    "label": 1
                },
                {
                    "sent": "We really think we really want to find ways to do better than this simple approach.",
                    "label": 0
                },
                {
                    "sent": "A second way that that is usually what people can do, is trying to use the already train your network as a teacher and try to build a supervision signal to kind of supervised the new net that you want to train.",
                    "label": 0
                },
                {
                    "sent": "So hopefully you the teacher again help to get the student to result in a faster training.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have tried this approach actually when I started my intern I tried for like first in the first month.",
                    "label": 0
                },
                {
                    "sent": "I tried several kind of this approach and unfortunately this approach does not work very well in the sense of like it does not gives you very fast convergence.",
                    "label": 1
                },
                {
                    "sent": "However, I think this is a still important direction in a sense that this is a generic approach that allows you to transfer knowledge from arbitrary neural network to another arbitrary architecture.",
                    "label": 0
                },
                {
                    "sent": "So to briefly describe what.",
                    "label": 0
                },
                {
                    "sent": "I tried, basically we derived supervision signal that tries to decode every layers of the already trained network to the new network in a sense that want the new network to quickly have the ability to have the same ability as the as the teacher model, and it turns out that it does not converge.",
                    "label": 1
                },
                {
                    "sent": "Is it does not help the convergence in our in our very order of magnitude manner.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We so then what in this talk we will resort to a much simpler approach which we call Network tab.",
                    "label": 0
                },
                {
                    "sent": "That actually helps us in this process, and I think this is a very tiny step towards this more general goal of continuous model evolution.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to give you a general overview, if you would think about traditional traditional neural network neural network flow, which will normally do is you will do this re experiment loop where you will discard your already trained model every time in the network.",
                    "label": 0
                },
                {
                    "sent": "What we're doing instead is that we try to reuse already trained model, you some transformations to transform that already trained model to our kind of equivalent new models and continue training from the from the transfer form transform the model.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the reason that we do this is actually actually from some observation from experiment.",
                    "label": 0
                },
                {
                    "sent": "So we find that.",
                    "label": 0
                },
                {
                    "sent": "Actually I think this is kind of a common sense in a sense that the more randomized components you have in your neural network, the harder or the longer time it will take to train the neural network.",
                    "label": 0
                },
                {
                    "sent": "So in this experiment we basically what we do is that we take already trained neural network and randomly reinitialize some of the layers and you will find that if you randomly initialize model.",
                    "label": 0
                },
                {
                    "sent": "Half of layers, actually the convergence speed is kind of already bad, so the key observation here is that we really want to get rid of the random randomly initialized components and trying to fully utilize the original knowledge as as much as possible.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this motivates us to this simple solution we call, which we call function preserving transformations where we want to take already trained neural network, do some transformation such that the bigger net is kind of functional equivalent in the sense that for any given input they give the equivalent output, then we will do a bit perturbation to the transform neural network and continue training from the from the Internet from the network.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we started two specific ways to actually expand model capacity.",
                    "label": 1
                },
                {
                    "sent": "The 1st Way is we call net wider net which we where we want to increase the number of channels in each of the in each of the layers.",
                    "label": 0
                },
                {
                    "sent": "The 2nd way we we call it not to deeper net which is we want to increase the depth of neural network.",
                    "label": 0
                },
                {
                    "sent": "These two operations can be composed in arbitrary order so that we can basically grow both in terms of width as well as steps.",
                    "label": 0
                },
                {
                    "sent": "And the question is that whether we can find effective functional preserving transformations to do this to cancel operation.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the wider transformation, what will what we propose to do is basically randomly duplicates all the channels that you want to widen.",
                    "label": 0
                },
                {
                    "sent": "Then after the random duplication we want to we want to reconnect the ways but but divide the edges by the duplication factor so the solar transforming network is exactly equivalent to our original network.",
                    "label": 0
                },
                {
                    "sent": "And this also applies for other for the general settings such as convolution neural networks, where all these colors are correspond to each of the individual channels.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The key takeaway here is that basically after transformation, if you don't add any noise, then the new network is exactly equivalent to the original neural networks.",
                    "label": 0
                },
                {
                    "sent": "We still need to add a bit of noise to product perturbing to continue, because otherwise it cannot break the symmetry.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To grow the noon, noon, noon, and network deeper what we what we can do is that we can take our existing layer and use some kind of factorization that factorized existing layer into two layers and basically useless factorization as operator.",
                    "label": 0
                },
                {
                    "sent": "So that we can we can enlarge the depth of a neural network and the question is that how we can come up with effective factorization for for the general architectures that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "It turns out we use a very simple idea that.",
                    "label": 0
                },
                {
                    "sent": "Turns out to work.",
                    "label": 0
                },
                {
                    "sent": "The ID.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that we basically?",
                    "label": 0
                },
                {
                    "sent": "Add add I identity mapping on top of the existing layer.",
                    "label": 0
                },
                {
                    "sent": "So basically a factorization of trivial factorization of a matrix can be identity times that comes that original matrix.",
                    "label": 0
                },
                {
                    "sent": "Of course there can be other account affect relations if if can be applied here.",
                    "label": 0
                },
                {
                    "sent": "If if they are, they can be faster computed.",
                    "label": 0
                },
                {
                    "sent": "So this kind of natural deeper net approach by adding identities help help us to generalize to general modules, including the piecewise linear functions in the in the activation function.",
                    "label": 0
                },
                {
                    "sent": "And it also works for other cases like batch normalization.",
                    "label": 0
                },
                {
                    "sent": "So it basically works for the general architecture that we care about today.",
                    "label": 0
                },
                {
                    "sent": "And there are follow-up works that handles possible other possible works of nonlinearity by researchers from Microsoft Microsoft Research this year.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So you know I have already described this 222 kind of methods that one widen the network and another approach is too dependent network.",
                    "label": 0
                },
                {
                    "sent": "The question is that whether we can, we can see we can do some experiment to show whether it can really help us to speed up our experiment cycle.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you know what experiment will conduct all experiment on the four image data set using the stand using the standard inception model and one of the one of the important thing when we continue training here is that we want to use a smaller learning rate to reflect the end of the learning rate schedule as opposed to use the beginning of limited cause because when you get all ready to get to the part of the of the of the training of a teacher, now the limit is already quite small.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first experiment we do is that we will take actually a smaller inception, that is, that is sqrt 2 capacity internal channels, each in each of layers.",
                    "label": 0
                },
                {
                    "sent": "So it corresponds to like half of a perimetre as our original inception.",
                    "label": 1
                },
                {
                    "sent": "So we will start from a small exception already trained.",
                    "label": 0
                },
                {
                    "sent": "Smaller inception as a source model and try to use the net to item wider net to continue training to continue trying the original inception model.",
                    "label": 0
                },
                {
                    "sent": "Here all the.",
                    "label": 0
                },
                {
                    "sent": "All the all the solid lines are the natural natural net net.",
                    "label": 0
                },
                {
                    "sent": "Wider net net approach.",
                    "label": 0
                },
                {
                    "sent": "And we also compared to a baseline that randomly that copies part of filters over at randomly initialize the rest of filters.",
                    "label": 1
                },
                {
                    "sent": "And finally the green line on the on the bottom on the bottom part is there is a line that correspond to what will happen if we re trying the entire model from scratch.",
                    "label": 0
                },
                {
                    "sent": "So there are several takeaways here.",
                    "label": 0
                },
                {
                    "sent": "The first takeaway is that we can find by using the natural net approach.",
                    "label": 0
                },
                {
                    "sent": "Actually all the models kind of get to similar similar performers.",
                    "label": 0
                },
                {
                    "sent": "Also, this is an empirical same cause.",
                    "label": 0
                },
                {
                    "sent": "These are local minimums, so there's no guarantee that converge to the same global minima.",
                    "label": 0
                },
                {
                    "sent": "But empirically, we can find that by initialize from from using natural net, we can't get too similar performers.",
                    "label": 0
                },
                {
                    "sent": "The second takeaway messages that by using that to widen and we kind of get faster convergence ramp up speed.",
                    "label": 0
                },
                {
                    "sent": "And you can find that this green line is the line is there is a training.",
                    "label": 0
                },
                {
                    "sent": "Accuracy is a test accuracy of the original of the original small inception model and we can find that.",
                    "label": 0
                },
                {
                    "sent": "Using that weapon that we can quickly get to that performance and improve from there.",
                    "label": 0
                },
                {
                    "sent": "It will translate to around like three to four times speedup compared to directly train from scratch.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also compared to a net to deeper net approach and here we run a simple experiment that takes a standard exception, an ad for layers of convolution layer to each of our issue of Inception Tower, and we compared against that range from scratch approach.",
                    "label": 1
                },
                {
                    "sent": "We gotta find that we can observe similar similar things in terms of faster convergence.",
                    "label": 0
                },
                {
                    "sent": "Ann and the good and consensus in the final final validation accuracy.",
                    "label": 0
                },
                {
                    "sent": "So we're seeing the experiment results for these two types of approaches.",
                    "label": 1
                },
                {
                    "sent": "One that helps you to grow network deeper and another one that helps you to grow network weather and one thing one of interesting things we can think about is that if we can take all these approaches and design wider experiments and try to explore new model designs so that basically we take already trained the state of art neural networks and apply these transformations and quickly see if these network design is good or not.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, so we do that.",
                    "label": 0
                },
                {
                    "sent": "We took a standard inception model actually and we apply both net wider net transformation as well as the net to deeper net transformation so we can find that by applying net wider that actually we get a much better get.",
                    "label": 0
                },
                {
                    "sent": "A slight better performance than the state of another state of art model we had there.",
                    "label": 0
                },
                {
                    "sent": "We didn't get because of the limited resources and time we didn't get chance to retrieve those bigger models from scratch.",
                    "label": 0
                },
                {
                    "sent": "So but however we we we put a curve of the convergence of the original inception model in the Dash line here.",
                    "label": 0
                },
                {
                    "sent": "So in theory the convergence of this bigger model should be even slower than this than this dashed lines.",
                    "label": 0
                },
                {
                    "sent": "So we can find that like it's actually by using our method, we can quickly quickly start exploration even long before there's things the things converge if you do things from scratch.",
                    "label": 0
                },
                {
                    "sent": "This kind of confirms availability of using natural net approaches to do more exploration, and I think this is the small small thing that small contribution that we can do to help us to explore the models faster in the future.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "To conclude my talk.",
                    "label": 0
                },
                {
                    "sent": "I think there are several takeaways, though.",
                    "label": 0
                },
                {
                    "sent": "First takeaway is that we need to do better than the dump approach that directly dumps already.",
                    "label": 0
                },
                {
                    "sent": "Directly dump all the old models and retrain from scratch.",
                    "label": 0
                },
                {
                    "sent": "It's possible to reuse already trained model and had to help training bigger models.",
                    "label": 0
                },
                {
                    "sent": "And one thing important thing when we doing this kind of approach here is they want to avoid random components and we use functional preserving transformations to to achieve that goal and.",
                    "label": 0
                },
                {
                    "sent": "In the in, the more general bigger picture, we always want to think about continuous training beyond beyond single shot, and I think there are more interesting space for exploration here.",
                    "label": 0
                },
                {
                    "sent": "This is this is a simple approach that scratch the surface of of this.",
                    "label": 0
                },
                {
                    "sent": "Of these problems in the bigger context.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will conclude my talk and I will be happy to take any questions.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}