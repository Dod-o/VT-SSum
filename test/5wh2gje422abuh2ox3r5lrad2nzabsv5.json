{
    "id": "5wh2gje422abuh2ox3r5lrad2nzabsv5",
    "title": "Directed Graphical Models",
    "info": {
        "author": [
            "Cedric Archambeau, University College London"
        ],
        "published": "March 31, 2011",
        "recorded": "February 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering",
            "Top->Computer Science->Machine Learning->Graphical Models"
        ]
    },
    "url": "http://videolectures.net/aibootcamp2011_archambeau_cgm/",
    "segmentation": [
        [
            "For the morning.",
            "So yes, so don't worry, the title is different from what was announced, but the content is roughly the same, so I decided to focus on directed graphical models.",
            "And not like graphical models in general.",
            "But clustering will be part of the.",
            "The presentation.",
            "And then some other techniques as well.",
            "If you want to know a bit more, just like Colin said, I'm not working academia anymore.",
            "Move like not so long ago to to Xerox so we can talk a bit more.",
            "If you want to ask questions about it, but roughly this is like the only Research Center.",
            "Xerox in Europe.",
            "It's like it's mainly focused on machine learning, so that's different different groups.",
            "It's mainly machine learning, it's quite small.",
            "It's like about 45 permanent researchers and then you have like G students and postdocs that are there as well.",
            "So I would say it's like slightly, it's like it's very similar to academia, except that you have to.",
            "You have to be some constraint on the business and so there you have to address some problems that are related to this form of Xerox which are mainly.",
            "Direction of services and not like on printers as such, because they kind of diversified and that the Xerox recently kind of looked at different types of problems and just printer.",
            "So they're based on everything that is related to document document processing.",
            "A lot of natural language processing machine translation so."
        ],
        [
            "So.",
            "So in fact this morning will be divided into 3 parts.",
            "So first I want to kind of introduce a couple of basic elements on elements of Bayesian networks, so directed graphical models.",
            "So kind of the basic.",
            "Techniques, at least like principles that are important.",
            "And how to reason on graphs relating to probability theory?",
            "And then I will the two other parts.",
            "I will spend the second part on latent variable models.",
            "I would just basically unsupervised learning and that is includes clustering as one of the type of techniques I will discuss.",
            "And the third part will be on conditional models for supervised learning, and I will focus only on regressions.",
            "I will not have the time to do classification, but I will introduce a couple of techniques and especially like maximum likelihood map and am in that context.",
            "And once you have those basics you should be able to read and progress myself for other more advanced techniques and look at the classification as well.",
            "So along the way I will also.",
            "So I prefer not to do that in the beginning because it's a bit painful, so I introduce a couple of elements of probability, probability theory and statistics.",
            "So recalling some basic distributions and some of their properties, but they decide to do that during the during the talk and not at the beginning.",
            "So don't hesitate to interrupt if you if you have questions.",
            "And the practical session this afternoon will be mainly focusing on on the part Part 2 and on clustering.",
            "Models."
        ],
        [
            "So here are a couple of references that are on which the slides are based.",
            "Many of the slides are using Chris Bishop's pattern recognition machine learning book.",
            "Many of the inspector figures are used, the ones that are used in his book.",
            "But there are other.",
            "There are many other books and this is like one of the most recent ones that appeared, which is like a very thick one.",
            "So you have also dislike more older reference books on graphical models themselves, so one of the the the one of the breakthrough books in machine learning, at least from approximate inference, was this one.",
            "Michael Jordan, like a good synthesis of the techniques we started looking at.",
            "So yeah, if I have time, so the last the last part is, so I've decided to work on three parts this morning.",
            "If I have time.",
            "The very last part I would like to talk a bit about Gaussian processes, and in that case you have.",
            "Those are books that are related to Gaussian processes.",
            "This one is a very extensive book.",
            "You have the link.",
            "In fact you can download the book.",
            "It's a it's available online.",
            "Same for David Mckay's book, where you have like interesting models, not only Gaussian processes, but other techniques, and again, this one is available online.",
            "And then finally I just want to.",
            "Recommend like one or two little.",
            "One interesting note is the Matrix Cookbook.",
            "So if you have like those like a little document of like dirty dirty bit more than 30 pages, that kind of augment all the different identities and so it's like a whole list of interesting properties and and and characteristic of matrix matrix computation particular but also applied to probability theory.",
            "So you find a lot of identities that could be useful too.",
            "If you want to make some computer if you want to double check some some some some of the results and then of course you have.",
            "All the tutorials and talks and video lectures.",
            "There are plenty.",
            "There are some very very good.",
            "Among all those one, if you're interested in graphical models, the one of somewhere away so very, very good.",
            "Summarize.",
            "SMR Summoner OSROWES."
        ],
        [
            "OK.",
            "So you've seen yesterday already.",
            "Statistics is quite important, so I will be talking about statistical machine learning.",
            "And I guess one of the main trends in machine learning is now is that?",
            "It's like it's really interaction between static statistics and computer science.",
            "So the main reason that data is kind of increasing all over the, especially on the web, you have images.",
            "You have sound sensor.",
            "We kind of have a lot of sensors.",
            "Everything is recorded to at least a lot of digital information is is being available and so you have like an over kind of becoming difficult to process that information to get you have too much information.",
            "So you want to extract what is really important.",
            "And the second, so that's the first thing.",
            "The second thing is that the data that we record this usually noisy like and unreliable.",
            "So you have like recording errors you have like a machine folds, so there all sorts of reasons when you kind of automatically record data that makes that the data is noisy, and so you have to be, you have to take uncertainty into account and you have to be robust to those fault.",
            "And so today I will focus on strategy.",
            "So one specific way to look at machine learning, which is that you assume you have like some generation process of the data.",
            "So which is kind of the model that you kind of device.",
            "And this is like under the form of probability distributions, so it's easier hierarchy of probability distributions and so you will have like lots of examples in the 2nd and the 3rd part.",
            "And so I hope this will become clear.",
            "So it's really it's like.",
            "How different variables are depending on each other and how they form?",
            "How is the joint distribution structured?",
            "Anne.",
            "So I guess one specificity of machine learning compared to statistics because machine learning is like borrowing a lot of from statistics is that?",
            "We mainly looking at very large scale problems where you have like a lot of data and so having an interesting model which is like we just make sense is not sufficient.",
            "You want something which is computational tractable.",
            "You want to be so the computation complexity is a very important, and it's like a very strong constraint on the models you want to investigate and develop."
        ],
        [
            "So today I will be focused on graphical models and so graphical models is.",
            "Is that the intersection of probability theory and graph theory?",
            "So in fact it's like you're looking at statistical machine learning and then you also take on board graph theory just to represent the type of models you kind of consider an based on some interesting properties you able to reason directly on the graphs instead of having to look at the distributions themselves.",
            "So you can already make some conclusion or make some some.",
            "The reason on the graph themselves.",
            "So it's also interesting because based on those properties you kind of able.",
            "If you have like a very complex model, you able to separate them in different parts and then only look at those specific parts, do the inference onto the learning and then you have like given given that you're looking at probability theory, that's kind of the way.",
            "How do you combine those different parts in a consistent way?",
            "And in fact, the main point is that when you're looking at.",
            "Graphical models or those those joint distributions, is that they're going to.",
            "They're going to represent conditional independence ease, so that's so.",
            "This is giving a structure on those joint distributions, so I think it would be."
        ],
        [
            "Become more clear in a couple of slides.",
            "So before I start into the real real stuff, I just want to just go through one or two applications.",
            "So graphical models are applied in lots of lots of situations.",
            "It's going from bad formatics as I said, to document processing, speech processing, image processing.",
            "Physics and social Sciences or time series.",
            "And so you have like a whole range of applications that are possible to be constantly learning and some are bit more more related to tradition is called statistic maybe.",
            "But you have a lot of it's like widely."
        ],
        [
            "So I want to consider factory specific applications and the first one is.",
            "This topic modeling, and so it wasn't planned but so yesterday during the discussion John mentioned topic models.",
            "So I just wanted to give you an idea of what topic models are doing so roughly.",
            "Uh.",
            "Joke.",
            "A topic model is kind of usually formalized as a graphical model.",
            "And the idea is that you kind of assume that.",
            "You kind of going to.",
            "You have like a whole range of documents, so we have a large corpus, large number of documents that you want to structure that in a certain you want to capture some semantic so so the meaning of what do the documents talk about?",
            "And so the idea that you want to really cluster those documents in different groups and the clusters should be should capture some what we call topics, which are sort of.",
            "This kind of semantically capturing some meaning of what is discussed in that specific document.",
            "And so.",
            "In fact, it makes a very strong assumption about documents that it assumes that the words are generated.",
            "Instead of a backup port.",
            "You make a backward assumption, so that means that you ignored the sequential structure of documents.",
            "You assume all the words are generated and have no connection in the sequence.",
            "And so that that seems to be like a rather strong assumption, but that is sufficient at least two to capture what we call topics.",
            "And so here you have like the typical topics that are could be extracted specific document.",
            "So you have so topics are defined as.",
            "Distributions over vocabulary words.",
            "And So what you have is that this is like a ranked list of those words.",
            "And those are the four different topics.",
            "Usually you don't mean in general, you don't know what is the label of that topic.",
            "But once you look at you look at the different words that are identified.",
            "You usually quite easily can put like a sort of the actual topic.",
            "What is it talking about?",
            "But is that top capturing?",
            "And so for example.",
            "Then when you look at a specific text, you can say OK every word.",
            "So when you assume you look at how documents generated, so your model is saying that every time you have a new document, you draw a specific number of topics with some probability.",
            "And then once you pick one topic, draw, you're going to draw some word according to the probability over over the vocabulary words assisted the topic.",
            "And so, in fact, you can assign every if you look at the document, you can assign every word to a specific topic.",
            "You can say OK, oh this so you don't see it very well, but when you look at the slides you will so you have like this is like a grayscale and you see that.",
            "So depending on which.",
            "So you can really see which are the words that are associated with some specific topics and which are not."
        ],
        [
            "So the second example where graphical must work quite successfully using image denoising.",
            "And roughly, you impose some graphical so.",
            "People use in general Markov random fields.",
            "In this case I will not discuss it today, but I think it's like a very lucrative.",
            "So in this case you have.",
            "This is like an original.",
            "Those images are original images and then you have some some noisy images and what you want to recover is this one, but you've never never observed that that image.",
            "So the question is how do you are you going to dinner?",
            "Is that image and try to reconstruct something which is this kind and so.",
            "Basically the idea is that you.",
            "You're going to impose some some distribution over the pixels.",
            "And you're going to look localized.",
            "Look at local information around the specific pixel.",
            "And so, in fact, here you have like 2 techniques to different ones where the first one you kind of.",
            "You don't look at local information, you kind of the noise.",
            "The basic statistics were in this case you really take into account what are the neighboring pixels telling you, and that information is usually giving you more more.",
            "Improves the denoising process.",
            "So I guess.",
            "I'm just wondering if you see a difference between those two from where you sit.",
            "But if you look closer you will definitely see a different, so this one is more blurry in fact.",
            "Yeah there is.",
            "So if you look closely, you see this one is a.",
            "Is sharper but.",
            "Yeah."
        ],
        [
            "And then the third example, which in Sam from Xerox I have to show you an example about printers.",
            "And so people have kind of looked at.",
            "Graphical models as well in this context where you have like a sort of large printer.",
            "Printers that are managed by Xerox for some client, and so you have like hundreds of printers in some other companies, and so you want to monitor them from a remote basis.",
            "And So what you what people have done is that they kind of look at some graphical representation of.",
            "How is how so that is like you want to, for example, look how are the printers used?",
            "Who is using which printer and where there located, for example.",
            "So you want to identify the fault automatically and that can be for example related to the usage of.",
            "Of people using specific printers.",
            "So if you are always using that specific printer at some point you just stop and you go and use the one on the other side of the building, then there's likely that there is something wrong with that printer and so you can kind of infer from the behavior of people that.",
            "Is something wrong?",
            "And so in that case, when this is really so, you have like the different colors are different printers and the dotted line, just like how many times every time there is a dot, it's someone every line is like someone somewhere else.",
            "So every time reserved out.",
            "That means there is a printing job by that person and then you have like you see for example in this case you see that person was printing at some point, it just stopped printing.",
            "So you kind of dislike though.",
            "Greyscales is like how probable it was that the printer is like there's some default.",
            "With the printer.",
            "Based also on the usage of those printers.",
            "You can also see if so when you have to manage printers from remote you want, so it's like device management.",
            "Based on the usage you can also see OK, this printer has moved.",
            "For example, if at some point people start looking so we kind of there's a printer here.",
            "We kind of using that printer and then like later.",
            "You get started using that printer.",
            "It's not unlikely that people have moved the printer, and since you're matching it remotely, it's very difficult to retrace.",
            "You don't want to send somebody to check out what happened with that printer, so this is kind of general and can be applied to any type of devices when you want to mention you have like a large number of devices you want to monitor."
        ],
        [
            "Right?",
            "So let's start.",
            "So those are just like 3.",
            "Examples to illustrate.",
            "Where graphical models were used to do do some learning.",
            "Think you had like many examples on the John.",
            "Anne.",
            "When he discussed about introduction to machine learning.",
            "So the next three parts is will be.",
            "Will you, looking into the techniques?",
            "So unfortunately not be much, many applications or like examples or examples, but I think it's important also to local, so those slides are more like sort of.",
            "So does every lecture notes you have, like maybe too much material and there's like a lot of text.",
            "It's not like like in between like nodes and it's like.",
            "So the first thing I would like to discuss our Bayesian networks so directed graphical models.",
            "And.",
            "The four points is like the first one is conditional independence.",
            "You had already a little bit of that yesterday.",
            "So we focused a little bit more today.",
            "Then some interesting properties like this separation and Markov blanket that help you reason on on graphs directly and then finally I will spend some time how to do learning in Bayesian networks so.",
            "And.",
            "In a more general case, and then we will see how this can be applied in more concrete examples in the 2nd and 3rd and the 3rd part.",
            "So yeah, I forgot to say that the idea is that to spend like about one hour on every part this morning.",
            "So free."
        ],
        [
            "That would be reasonable.",
            "So the basics are so, so just like to show how we represent.",
            "So we will be looking at directed graphical.",
            "Also, there's a graphical models of this kind.",
            "Those are undirected ones, just to introduce like the formalism you have nodes.",
            "And then some links between those nodes which are directed in our case today.",
            "And those are those indicate some form of dependency between random variables.",
            "So nodes are random variables.",
            "Our dependencies and then you have also so when we color one of the nodes, that means that the variable is observed and when it's not that means it's late and not observed.",
            "So it's usually you introduced because there is really something.",
            "It's usually because of the modeling purpose, so you assume some other random variables that are not directly observed.",
            "You also have plates and please just mean that this node is repeated.",
            "So you have this notice repeated N times just like this is for convenience and not to have to draw too many to many of the errors and.",
            "So directed graph directed graphs are called Bayesian networks or basic base.",
            "That's also the cause, sometimes called belief networks and so on.",
            "So you have like different names for the same thing.",
            "On the other hand, undirected graphical graphical models, which I will not discuss today, you have Markov networks, Markov, random fields and so on.",
            "And then if you combine the two types of graphical models and you get something which is called chain graphs.",
            "Just like."
        ],
        [
            "First, with the conditional independence property.",
            "So I think you might have discussed that also on the first day with Frances at least yesterday a little bit.",
            "So there are two things that I would like to start this like statistical independence.",
            "So we say that some random variable X is independent of Y.",
            "If in fact the joint probability is factorizing.",
            "So if you use the standard.",
            "Can I forgot to take?",
            "Bored.",
            "Yeah.",
            "Anyway, so the standard rule for it if you want to decompose.",
            "The joint of XY.",
            "This is like P of X given y * Y.",
            "So when you have independence dependency on why Vanisher so so you have the joint factorizes into the distribution and the product of the marginals of X times be applied.",
            "Now conditional independence ease is so we know this way, so X is independent of Y given Z.",
            "Means that there is some factorization when you condition on some other random variable.",
            "So if you look at the the joint here P of XY given some other variable Z, if you apply the rules of probability you have like you can decompose in this way.",
            "So we have X given Y&Z times the probability of a given given Z.",
            "And since X&Y are independent given Z device that X is going to take, if you condition on Z is, so the value of Y, sorry, it's not going to impact the value that X can take.",
            "So the dependencies why vanish is so expect rising again, like in statistic independence, but only when you conditional some variables.",
            "You have this factorization.",
            "So put it in another way.",
            "You can also look at the probably of X given Y&Z values taken.",
            "So if you condition values of Y this has no impact on the bands of X, so you can just drop the dependency on on why in this case as well, and the reverse is also true.",
            "So condition dependencies quite.",
            "It's quite natural you have like.",
            "Here's some examples, but so from real life I would say so if you can.",
            "For example, imagine that my genome is going to be independent.",
            "My grandmother, genome condition on my mother's genome.",
            "So that's the intuition between of condition independence.",
            "Yes dear.",
            "So did you, did you?",
            "Did you discuss that first with Frances or not?"
        ],
        [
            "OK."
        ],
        [
            "You decompose.",
            "The joint here into this type of factorization.",
            "So you have like probably have given Y times of the probability of why so when you have like in statistical independence?",
            "X has no impact on on device taken by sorry.",
            "Why has no impact on voice taken by X?",
            "So this is so if you have some statistic independence, you have like the factorization of this form and the conditional independence of the same form, except that you're going to condition on some other variables, which is which I called here Z.",
            "So again you have like this factorization and then independence when you condition or that that specific problem.",
            "Right?"
        ],
        [
            "So.",
            "If you look at the so going to define like a graph, what's the probability graphical model we consider some set of random variables?",
            "And in fact, those variables are related to each other to joint distribution.",
            "And.",
            "The graphical itself is going to materialize.",
            "Some conditional independence is between those variables.",
            "So in fact this is going to introduce some structure in the joint distribution of this genre.",
            "Solution is not going to be going to factorize in some specific way.",
            "Because of those condition independence dependencies.",
            "And so this has two impacts.",
            "One, it gives you more interpretation about what's happening.",
            "Usually the dependencies depend on on prior knowledge about the problem, but also it's going to have an impact on the computation and storage number of parameters that you will use to represent that joined distribution.",
            "So people like to stay.",
            "So if you have like the joint the full joint.",
            "If you have like all the random variables there there.",
            "If you don't look at the condition dependency that you have like the full joint you have this directed graphical model that is kind of sort of computer sort of filter and then you get like a specific factorized joint.",
            "So roughly you have like some.",
            "Dependencies between some random variables.",
            "People like to call that.",
            "Factorization filtering, but it doesn't matter."
        ],
        [
            "So more precisely, if you.",
            "This is 1 example of Bayesian network.",
            "And So what we call the base network is a set of probability distributions associated with a directed acyclic graph.",
            "So this is a director.",
            "Basically graph.",
            "That means that all edges are directed and there are no cycles.",
            "If you go follow the arrows, you will never be able to make loops into the graph.",
            "So in fact, if you look at this specific.",
            "Graph.",
            "You can have, so this is going to do some specific factorization of the joint, but that doesn't tell you you don't have to impose any specific distributions of over different nodes, so it's just like looking in an abstract way of the joint and how the different variables depend on each other.",
            "So we say that a node, so not a.",
            "For example in this case is a parent of node B because there's a direct link from A to B and vice versa.",
            "We talk about children of specific nodes.",
            "And typically so in based networks unload specific node here is independent of its ancestor.",
            "If you condition on its parents condition.",
            "All these parents.",
            "And so those run.",
            "The verbs can be either continuous or discrete, and so that's why I would like to focus on the so in the latent variable model part.",
            "So the second part I will discuss on the one side some discrete latent variable models, and continuously variable amounts.",
            "Something.",
            "It's so roughly.",
            "So if you have the.",
            "So if you don't know anything about about your own variables, then you can see that you have a joint of those three variables and hours here.",
            "They're going to tell you how those variables depend on each other, and so in fact, what's happening is you have.",
            "PA doesn't depend on anything, so you have like some prior probability on A and then you can say that.",
            "FP of B given a.",
            "Because B depends on depends on F and times P of C depending on NB.",
            "So it's just like giving you an idea of how the variables depend on one another.",
            "So it's just so because so.",
            "So when I was talking about filtering roughly, this is like saying, OK, the joint, you can rewrite it in some specific factorized form, depending on when you look directly at the graph.",
            "Then we had waited.",
            "In fact, in the case of Bayesian networks, weights wouldn't represent.",
            "Something, maybe extension?",
            "I don't know if any, but so the probability.",
            "So everyone overtakes the specific value of some probability.",
            "The waiting here is.",
            "It would, for example, if you have like this is defining a specific relation between variables.",
            "If you introduce which kind of if you want to reflect the factorized and then you will not normalize properly.",
            "So you know probability needs always to factorize too.",
            "Sorry to sum up to one.",
            "So the weights might.",
            "I don't know how you would use a weight in that case.",
            "But you have to be consistent from probability theory."
        ],
        [
            "So it's right to the question by the so.",
            "The graph is going to so because of the condition dependencies.",
            "You will have specific to risation of the joint distribution, so that's that's one example.",
            "And in general you can write it in this.",
            "In this way you have like for a graph of nodes.",
            "The joint will decompose in this specific way, so you have the product over so you have like big and nodes and so you can write this.",
            "This decompose this joint into a product of the probability of a specific node given the parents.",
            "Spent.",
            "So those are all PN are all the random variables that are the parents of XN.",
            "And so in some way this can.",
            "What is important here is that this is like a local decomposition.",
            "You only condition on some variables that are close to the extent, so that touching with some.",
            "The right links to that specific variable, so you have like a local decomposition of the joint and that is the main interesting bit about graphical models that it shows.",
            "That tells you how to factorize how to look at local condition distributions.",
            "Now, in the case of the Bayesian networks.",
            "So this is properly normalized, so that's a little complication.",
            "When you look at undirected graphical models, you also decompose a joint into some product of factors, but those are usually not normalized, so you have to normalize them.",
            "So that probably is like if you sum over all the values possible, you should submit to one."
        ],
        [
            "So here, here in the case of this network, so you don't have to normalize.",
            "This is because every factor is properly normalized.",
            "If you have an directed graphical models you don't condition and you have like just like a function of a certain number of random variables, and so that's not the probability function is not probably just like you just imposes positive, but it's not so.",
            "So if you want to normalize, have to sum over all possible possibilities so that you're probably probably is like something up to one.",
            "That's, but in case you're based networks, you don't, you don't need to normalize.",
            "So about the factorization.",
            "So is this factorization useful?",
            "So I have a little example, so you have this.",
            "A sequence of.",
            "So linear sequence of random variables.",
            "So here and and variables.",
            "So if you consider just semigroups tree, so you have like just three of those nodes.",
            "You can apply so you look at the condition dependencies assumption so you know that you're going to.",
            "In the case of tree.",
            "You have like X1.",
            "Depends on nothing.",
            "So you have probably over X one times the probability of X2 given X one times the property of extra given next to.",
            "So you have that extremes condition independent of X1 given X2.",
            "Right?",
            "So here you have fun dependencies like dropping, so if you assume that every random variable here XM is taking discrete values, so K discrete values can take values, so it's not like continuous random variables are discrete random variables.",
            "And then you look at the factorization.",
            "So if you.",
            "If you, for example, want to compute the marginal over of X2.",
            "Then you have to sum over all values of X1 and all values of X tree.",
            "And so if you look at the joint here, every variable can take K values.",
            "So that means that if you want to compute this.",
            "This marginal for every of the K values of X2.",
            "You have to do you have to sum over the key value X one in the caves xtree so you have like something which is of complexity K ^3.",
            "Now, if you.",
            "I want to compute this this marginal using the condition dependencies assumption.",
            "So you use this factorization.",
            "Then you can rewrite.",
            "You have to sum of X1 and X3, but in fact if you look at if you bring those together, we have the joint X one X2 and that doesn't depend on X3.",
            "So you can.",
            "You can use distributive law and you can.",
            "You can just look at here you can first come over X3 and then you can some second step, some over X1.",
            "So here again you have here only have like two of those random variables.",
            "So for every value of K you have to send over K values of X3.",
            "So you have like something which is complexity K square.",
            "And once you've done that, you again you have like only two random variables.",
            "So again you have like a complexity of squared, so that means you move from K cube two 2 * K ^2.",
            "So this is very important when you have like very large graphs where and when when variables can take large number of.",
            "Values.",
            "Because this is usually something like that is just like in practice for large scale is just like impossible to deal with.",
            "It also leads to, so that's for the computations.",
            "So, so when you want to compute some marginals of some probability for some specific nodes.",
            "It's also interesting when you want to store.",
            "There are the parameters of the model.",
            "So if you look at so without the factorization.",
            "You have, like I said, you have like every entry.",
            "Every variable can take values you want to need to be normalized or there's one.",
            "So you're using one dependencies, so that's why you have like K -- 1 parameters because there's one constraint.",
            "So because all the everything needs to sum up to one.",
            "And now if you look at the factorized version, then.",
            "You have here K -- 1 parameters so you have like a parameters, but they have to submit to one so everyone so you're losing one.",
            "You have like 1 parameter system and all the others.",
            "So here you have lucky minus one parameters and for this one you have for the K values of X1 you have again like a -- 1 parameter to determine X2.",
            "And so this is repeated in minus one times.",
            "So you have something which is again K. M * K ^2 in complexity where this is KM in complexity."
        ],
        [
            "So now the question is.",
            "Do we do some additional in dependencies in the graph so we have we so typically you have some expert.",
            "He like a specific problem, we want to model, you know some.",
            "You assume you know that there are some conditional dependencies between some random variables and so someone is going to draw the graph.",
            "The graphical modeler obsessed with this distribution.",
            "And so roughly, if you have like the full graph, the condition dependencies correspond to removing some links in the graph.",
            "Those are like representing those condition dependencies.",
            "So the question is whether we do some additional in dependencies by doing that.",
            "So for example, if you look at this this graph.",
            "Is there is a, for example independent of B?",
            "So can we directly deduce that by looking at the graph?",
            "And similarly, if you condition on some variable, F is a condition independent to be given given F. So we would like to find some ways to by just looking at the graph, can we conclude something interesting about the joint and the button dependencies between random variables?"
        ],
        [
            "And so I will go through a couple of examples.",
            "I will not probably not do all of them because I think I'm running.",
            "Already out of time.",
            "But so there are three people.",
            "There are two types of nodes in a basic network you have like head to tail nodes.",
            "So see here is a head to tail node you like tail to 10 nodes were in fact."
        ],
        [
            "Did you take notes out of this form?",
            "So see here is a data node that means that there are only hours going, so going out of that node.",
            "And then you have all."
        ],
        [
            "So."
        ],
        [
            "I.",
            "Head to head notes, which is like the opposite, so see here.",
            "In this case we had to have known because ours are going to that node, so will consider those three situations."
        ],
        [
            "And now will try to.",
            "Show you what, so if you discuss like statistical independence and conditional independence in those three cases.",
            "First question, if you have this.",
            "This graph so is RA&B independent, so you have like a head to tail node C here, and so the question is whether the joint of A&B Factorizing P of property of a terms of probability of B.",
            "So from the same rule of probabilities, you can write this joint as the marginal over the joint of the tree of random variables.",
            "So you send over, see.",
            "And then you can look at the graph and you can just apply the factorized.",
            "So based on the condition dependencies you can rewrite this disjoint in this specific way.",
            "Right, so P of eight times Pfc, given a times P of given C&B is independent of a given even see, so that's why you don't have dependency there.",
            "You some overseas, so this term does not depend on this factor does not depend on C, so you can remove it from the sum and so you have like those two remaining factors that are there.",
            "So you can write a joint so this time this is going to be the joint of PB, probably of B&C given a.",
            "Into some oversee and so you end up with this.",
            "This product or FP of eight times of be given a so that tells you that in fact, in this specific case, in general there is no independence between B&A.",
            "Because a depends on depends on the on P."
        ],
        [
            "Now the other situation is where you have ahead.",
            "Do you want to test for condition independence?",
            "So you can look at.",
            "You assume that sees observed.",
            "And you want to check whether this joint conditional C is going to factorize in this way.",
            "So again, you look at you look at some you can look for example at the probability of A&B given C. Which you can rewrite using Bayes rule in this way, so you have like the joint of the tree renovables divided by bad, probably of the marginal C. Which then can decompose according to the graph above.",
            "So you have like.",
            "Again, it's exactly the same expression as before and then you can recognize this again as.",
            "As a base rule.",
            "So we can discuss this situation correspond to probability of a given given C. And so you end up with this factorized form.",
            "So here in this case you can.",
            "You can conclude that.",
            "You always have condition dependencies between A&B.",
            "When I had to tell, know this condition of inherited mode.",
            "Between the two.",
            "Neighborhood.",
            "So what is interesting is that in this case.",
            "You were you had this probability of C given a.",
            "And by applying base rule, in fact, you get something it's like you get like you've reverting the link, like reverting the link in the in the Bayesian network because you get this probably probability of a given C. Right?"
        ],
        [
            "So there are four other cases, so I will not go through them because because of time, but I think it's what if you want to go through to it by yourself.",
            "It's like using the same types of properties on basics, basic rules of probability, like like looking at the graph, how it decomposes based on conditional dependencies using base rule using the sum rule of probabilities and the product rule of probabilities.",
            "So just about the results.",
            "So when you look at tail to tail node, so in this case if you want to check statistic independence then if you go through the math you will see that there is no statistic independence.",
            "In general, so when you have like a tail to tail node then P of A&B are not always independent, not necessarily independent."
        ],
        [
            "And when you condition on C then you have like condition dependence.",
            "So this case A&B are condition independent.",
            "So that was in fact in those two cases in the tail to tail nodes and in the head."
        ],
        [
            "Turns out it was.",
            "You had like similar."
        ],
        [
            "Concluir"
        ],
        [
            "Asians?"
        ],
        [
            "Anne.",
            "In the head to tail notice slightly different there you get the opposite, so see again here is a head to head to head to head node.",
            "You want to check about independence between A&B and if you go through Marta get that is independent of B.",
            "General."
        ],
        [
            "But when you condition on that variable then you lose so independent so you don't have conditional dependencies.",
            "If you condition on.",
            "See in this type of.",
            "Astructur"
        ],
        [
            "Right?",
            "So that's.",
            "Leads us to the concept of the separation.",
            "And so roughly the idea of the separation you want to have a rule that tells you OK, if I have a large graphical model which are the parts that are independent from one another.",
            "And so the what we defined it as a block path is one that contains at least observed head to tail or tail to tail nodes.",
            "So if you remember, in those cases you had conditional independency between the different parts and or you have an observed head to head note of which number descendants are observed.",
            "So again in this case you had if you had like statistic independency between the parts.",
            "So when you have, once you have like so this is what we call the block path and so if you have like 2 parts of the graph, you have like a part AB and C3 part.",
            "Sorry that are non intersecting so they have no nodes in common.",
            "Then you can say that is condition dependent on be given, see if all the possible path from A to B are blocked.",
            "Now that is interesting because you don't need to assume anything else about the distributions, just based on the structure of the graphic and circuit.",
            "Those servers are independent and those are or not.",
            "So that helps you to reason on the graph directly.",
            "And.",
            "So can someone tell me if is conditional independent of F given B?",
            "What?",
            "So is there a block a blocked path between A&B?",
            "So this this node is a is a tail to tail node.",
            "Which is observed.",
            "And so you have like NBR, condition dependent.",
            "Give an F. In fact, you have a. Ha yeah.",
            "So in in this, in this specific case for F. So you can double check.",
            "OK, so."
        ],
        [
            "Another important properties important notion is the Markov blanket.",
            "So the Markov blanket is is what in Bayesian networks?",
            "I mean, in general, is the minimal set of nodes you have to you have to take into account to isolate one specific variable from the rest of the graph?",
            "So that means that once you condition all those variables that variable, the probability of variable is independent of all the rest.",
            "So if you look at the property of X specific run variable XI, given all the other random variables, then you can use base rule too, right?",
            "The joint.",
            "Of all the random variables, you know that this Union based network.",
            "So you can factorize in this way.",
            "And here you have like the same.",
            "You have to join but you have to sum over XI.",
            "And so if you look at those, the two, the two, the two factors here, well, all the ones that are not appearing.",
            "In the button at the bottom of the distraction are the ones.",
            "Since you have some other xir, the factor P of XI given.",
            "So I.",
            "Dimes although the the conditional probabilities where XI either except using in the parents.",
            "So just like in the simple, just to remind you, see how we obtain.",
            "If you have the joint.",
            "If you have the condition of XY given X of X given Y.",
            "You can write it in.",
            "In this, in this form, right?",
            "This comes directly from the fact that you have X given Y times.",
            "Sorry.",
            "So that's so.",
            "You know this is the this condition property can be written in the form of this based on this form.",
            "And you can just like write this in this simplified form where you just like bring that into this by writing.",
            "This is like equal to the joint of X&Y, so that's what I used here in this case, and in fact you can see that this is.",
            "For that we did in the second equation.",
            "Here you have like the joint on the top and you have like if you sum over all values of X and you have the joint here.",
            "Right, this is some similar properties, right?",
            "So if you work it out you will see that you have those types of factors that are not simplifying.",
            "And So what?",
            "It tells us is that.",
            "The variables that excite depends on R. It's parents, so those two random variables and then.",
            "Here the X axis here are the variables, where X is one of the parents of those other children, so those are those two, those random variables, and in this conditioning you have also all the other parents of those children, so we should call this parses so those running rabbits.",
            "So those are the ones that are forming the Markov blanket directed graphical models.",
            "OK."
        ],
        [
            "So next.",
            "Couple of slides I'd like to kind of discuss little bit you can do learning in basic networks from a general perspective.",
            "So we consider that there are some we have some some data that are drawn IID.",
            "So what does it mean?",
            "Is that the different observations are drawn from the same distribution.",
            "So, so the identical assumption and their independence, meaning that every sample is independent of a fan.",
            "Another neutron, not condition dependent on another.",
            "So if we consider some specific specific statistical model of the data which you can write in this in this form, you have like a product of factors.",
            "In the case of Bayes networks you don't have the normalizing constant, so you just have this this expression here from before.",
            "So.",
            "That simplifies to."
        ],
        [
            "To this expression we have this product of factors collected factor, but the probabilities that are normalized indicator based networks.",
            "And the quality of the model.",
            "So how good the model fits?",
            "The."
        ],
        [
            "Data depends on the parameters.",
            "How you you estimate those parameters.",
            "So the goal here in the learning which is how do we.",
            "How can we estimate parameters?",
            "And I will discuss three approaches.",
            "One, this maximum likelihood.",
            "Maximum security and also some general principle of Bayesian inference.",
            "Not going too much detail, but I just give you an intuition about the idea behind that.",
            "And so for the moment, we will not assume there are any unobserved latent variables.",
            "So every all the variables are observed and then we have some parameters."
        ],
        [
            "Other.",
            "Defining the model itself.",
            "Right?",
            "So the first purchase maximum likelihood estimation, so you want to estimate the parameter of maximum likelihood, and So what people do is that you look at what is the likelihood.",
            "Likelihood is joint probability of the observations.",
            "So since the independent, you know that it's there are factorizing so you have like the probability of every observation is independent of the other observation.",
            "So you have a product of all the probabilities of the observations you looking at.",
            "We're looking at the log likelihood, so in fact, that's why is the here in front.",
            "So so.",
            "So because you want to maximize the likelihood, so maximizing the logarithm of the this value is not going to change the problem that you want to want to tackle.",
            "So if you just like.",
            "Plug into the expression on the previous slide of the joint.",
            "Then you can rewrite that.",
            "In this way you can just isolate.",
            "Since you have a log of a product, this is sum of a log.",
            "When did that take the product art converted to a sum?",
            "And so the goal in maximum likelihood is you have this log likelihood function.",
            "And you want to find the parameters as going to maximize the log likelihood function.",
            "So you want to find the Theta.",
            "This maximizing this expression in this expression.",
            "So when you have like a factorized form simply you can just maximizing directly, so you maximize, you want the gradient restricted parameters equal to 0.",
            "When you factorize form, you can do that in depending on the different parts of the graph.",
            "So let me separate the graph in different.",
            "Different parts.",
            "Of course, you can also minimize the negative people sometimes minimize negative black suit exactly the same.",
            "The nice property about maximum likelihood is that it's consistent, meaning that when number observations tends to Infinity, you will have your parameter is going to tend to the true value of your parameter.",
            "Now that is very."
        ],
        [
            "Nice, except that it's not working in practice.",
            "Because the likelihood or is unbounded.",
            "So you can have from above.",
            "That means that we can maximize the like with respect to.",
            "You can have like an infinite likelihood if you go perfectly to some specific data point.",
            "And so that is what this form of overfitting and so.",
            "Of course, you don't.",
            "You're not going to generalize well, so you want to avoid.",
            "You want to avoid that.",
            "Anne.",
            "And that is.",
            "I mean, I said this is true for small datasets.",
            "This is true in general.",
            "This is even worse.",
            "For small data set that happens much more easily, So what what the typical approach is to look at.",
            "The first step is to look at maximum posterior.",
            "Then the the idea here is just you want to penalize unreasonable values of the parameters, so you want to keep them in some some registration to do some so it's called regularization or some smoothness on your estimator for distribution.",
            "And So what?",
            "What you can do is like impose some prior distribution over to.",
            "So.",
            "It was discussed yesterday in a specific context.",
            "By that, but roughly so this sign here means a proportional to, so this is just applying Bayes rule, so you want to maximize the possibility of your parameters given the data.",
            "So this is just basic rules and just dropped the normalizing constant.",
            "And So what you maximize now is you maximize.",
            "The.",
            "The log likelihood.",
            "Of the data given the parameters plus the prior.",
            "So this is just like applying, so this is.",
            "This corresponds to the likelihood.",
            "Before, so if you just take the log, you end up with.",
            "This is this term and this is the sum over all the local parameters.",
            "Talk of the local parameters.",
            "And it's just working the same way as before.",
            "You just maximize this.",
            "This expression I'm not going to discuss how you maximize it.",
            "You can.",
            "This is usually using any type of optimization technique, usually nonlinear optimization techniques, but in some cases could be.",
            "You could have like some convex properties anyway, whether you prefer to gradient descent or greater.",
            "In this case, Britain accent technique is fine.",
            "Then depending on the difficulty of the problem, you have to do some more fancy things.",
            "But if your posterior is very broad, that means that you have a lot of uncertainty about this specific value and just taking into account might be might be risky.",
            "So that's why I usually.",
            "Business decisions don't view map as a.",
            "Based approach."
        ],
        [
            "So based inference asset is interested in the full posterior, so you want to view teacher not as a parameter.",
            "If you want this more anymore, you want it as a random variable.",
            "You want to infer the probability of that given given the data instead of a point estimate.",
            "And so again, it's it's based on.",
            "On on base rule when you have like.",
            "You're interested in the probability of Detective and the data, and then here you have likely term if the prior as you want to.",
            "If you have some price usually used for some form of regularization, you want to update that prior into Star Distribution, Bank base room and then there is this specific term which is called here.",
            "The evidence or the marginal likelihood which is.",
            "Obtained by just like integrating over details or summing over to.",
            "Right, so the reason why you're interested in so when you want to make prediction, then if you don't have like a point estimate of cheetah.",
            "This is that you want to predict some.",
            "If you have like a new value X star.",
            "Given the data, that's the actual thing you want to predict.",
            "You want to predict the probability of a new point given what you've observed that can be written in this specific form.",
            "So you have like the likelihood of.",
            "Then your new points are given the parameters.",
            "You want to wait every possible value of the parameter by the posterior probability of that parameter given the data.",
            "So in fact, when you make prediction in Bayesian inference, you're going to look at all possible models, so all values possible to go to some all over them and wait every model by the posterior probability of that parameter given the data.",
            "So why are people called sometimes ensemble learning?",
            "Anne.",
            "Whereas if you would look at maximum likelihood or or map, you would just forget about all this.",
            "You would just look at this specific probability and you would just plug the most the map value or the maximum likely value.",
            "So the nice thing about that is that if you are looking at, you don't.",
            "If you look at this this this way.",
            "Usually what you gain by doing that is that you have not only.",
            "The expected value of the parameter, but you have also some uncertainties have different source on the deviation of the parameters, so you have like confidence measures in the parameter estimation and the predictions you want to make.",
            "Now the problem, and that's what I'm not going to discuss because I won't have the time is that.",
            "Computing all those quantities at those two quantities.",
            "So or like integrals in the continuous case, or something unconscious cases intractable in practice.",
            "You cannot find like closed form expressions, so meaning that you have to do some approximations so people either do approximate Bayesian inference.",
            "And I will discuss them afterwards.",
            "And M is kind of the first step toward separation inference.",
            "There are other approaches.",
            "Which are very popular in statistics in any case, which are sampling techniques, Markov chain Monte Carlo, where in fact the idea is that instead of.",
            "Just trying to write a closed form solution for the posterior.",
            "You're going to sample.",
            "Values and so you kind of reconstruct the posterior by looking at the histogram of divide that you obtain."
        ],
        [
            "Alright, let's stop here for.",
            "For five minutes.",
            "And if you have any questions.",
            "Don't touch it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the morning.",
                    "label": 0
                },
                {
                    "sent": "So yes, so don't worry, the title is different from what was announced, but the content is roughly the same, so I decided to focus on directed graphical models.",
                    "label": 0
                },
                {
                    "sent": "And not like graphical models in general.",
                    "label": 1
                },
                {
                    "sent": "But clustering will be part of the.",
                    "label": 0
                },
                {
                    "sent": "The presentation.",
                    "label": 0
                },
                {
                    "sent": "And then some other techniques as well.",
                    "label": 0
                },
                {
                    "sent": "If you want to know a bit more, just like Colin said, I'm not working academia anymore.",
                    "label": 0
                },
                {
                    "sent": "Move like not so long ago to to Xerox so we can talk a bit more.",
                    "label": 0
                },
                {
                    "sent": "If you want to ask questions about it, but roughly this is like the only Research Center.",
                    "label": 0
                },
                {
                    "sent": "Xerox in Europe.",
                    "label": 0
                },
                {
                    "sent": "It's like it's mainly focused on machine learning, so that's different different groups.",
                    "label": 0
                },
                {
                    "sent": "It's mainly machine learning, it's quite small.",
                    "label": 0
                },
                {
                    "sent": "It's like about 45 permanent researchers and then you have like G students and postdocs that are there as well.",
                    "label": 0
                },
                {
                    "sent": "So I would say it's like slightly, it's like it's very similar to academia, except that you have to.",
                    "label": 0
                },
                {
                    "sent": "You have to be some constraint on the business and so there you have to address some problems that are related to this form of Xerox which are mainly.",
                    "label": 0
                },
                {
                    "sent": "Direction of services and not like on printers as such, because they kind of diversified and that the Xerox recently kind of looked at different types of problems and just printer.",
                    "label": 0
                },
                {
                    "sent": "So they're based on everything that is related to document document processing.",
                    "label": 0
                },
                {
                    "sent": "A lot of natural language processing machine translation so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So in fact this morning will be divided into 3 parts.",
                    "label": 0
                },
                {
                    "sent": "So first I want to kind of introduce a couple of basic elements on elements of Bayesian networks, so directed graphical models.",
                    "label": 1
                },
                {
                    "sent": "So kind of the basic.",
                    "label": 0
                },
                {
                    "sent": "Techniques, at least like principles that are important.",
                    "label": 0
                },
                {
                    "sent": "And how to reason on graphs relating to probability theory?",
                    "label": 0
                },
                {
                    "sent": "And then I will the two other parts.",
                    "label": 0
                },
                {
                    "sent": "I will spend the second part on latent variable models.",
                    "label": 1
                },
                {
                    "sent": "I would just basically unsupervised learning and that is includes clustering as one of the type of techniques I will discuss.",
                    "label": 0
                },
                {
                    "sent": "And the third part will be on conditional models for supervised learning, and I will focus only on regressions.",
                    "label": 1
                },
                {
                    "sent": "I will not have the time to do classification, but I will introduce a couple of techniques and especially like maximum likelihood map and am in that context.",
                    "label": 0
                },
                {
                    "sent": "And once you have those basics you should be able to read and progress myself for other more advanced techniques and look at the classification as well.",
                    "label": 0
                },
                {
                    "sent": "So along the way I will also.",
                    "label": 0
                },
                {
                    "sent": "So I prefer not to do that in the beginning because it's a bit painful, so I introduce a couple of elements of probability, probability theory and statistics.",
                    "label": 1
                },
                {
                    "sent": "So recalling some basic distributions and some of their properties, but they decide to do that during the during the talk and not at the beginning.",
                    "label": 0
                },
                {
                    "sent": "So don't hesitate to interrupt if you if you have questions.",
                    "label": 0
                },
                {
                    "sent": "And the practical session this afternoon will be mainly focusing on on the part Part 2 and on clustering.",
                    "label": 0
                },
                {
                    "sent": "Models.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are a couple of references that are on which the slides are based.",
                    "label": 0
                },
                {
                    "sent": "Many of the slides are using Chris Bishop's pattern recognition machine learning book.",
                    "label": 1
                },
                {
                    "sent": "Many of the inspector figures are used, the ones that are used in his book.",
                    "label": 0
                },
                {
                    "sent": "But there are other.",
                    "label": 0
                },
                {
                    "sent": "There are many other books and this is like one of the most recent ones that appeared, which is like a very thick one.",
                    "label": 0
                },
                {
                    "sent": "So you have also dislike more older reference books on graphical models themselves, so one of the the the one of the breakthrough books in machine learning, at least from approximate inference, was this one.",
                    "label": 0
                },
                {
                    "sent": "Michael Jordan, like a good synthesis of the techniques we started looking at.",
                    "label": 0
                },
                {
                    "sent": "So yeah, if I have time, so the last the last part is, so I've decided to work on three parts this morning.",
                    "label": 0
                },
                {
                    "sent": "If I have time.",
                    "label": 0
                },
                {
                    "sent": "The very last part I would like to talk a bit about Gaussian processes, and in that case you have.",
                    "label": 0
                },
                {
                    "sent": "Those are books that are related to Gaussian processes.",
                    "label": 0
                },
                {
                    "sent": "This one is a very extensive book.",
                    "label": 0
                },
                {
                    "sent": "You have the link.",
                    "label": 0
                },
                {
                    "sent": "In fact you can download the book.",
                    "label": 0
                },
                {
                    "sent": "It's a it's available online.",
                    "label": 0
                },
                {
                    "sent": "Same for David Mckay's book, where you have like interesting models, not only Gaussian processes, but other techniques, and again, this one is available online.",
                    "label": 0
                },
                {
                    "sent": "And then finally I just want to.",
                    "label": 0
                },
                {
                    "sent": "Recommend like one or two little.",
                    "label": 1
                },
                {
                    "sent": "One interesting note is the Matrix Cookbook.",
                    "label": 0
                },
                {
                    "sent": "So if you have like those like a little document of like dirty dirty bit more than 30 pages, that kind of augment all the different identities and so it's like a whole list of interesting properties and and and characteristic of matrix matrix computation particular but also applied to probability theory.",
                    "label": 0
                },
                {
                    "sent": "So you find a lot of identities that could be useful too.",
                    "label": 0
                },
                {
                    "sent": "If you want to make some computer if you want to double check some some some some of the results and then of course you have.",
                    "label": 0
                },
                {
                    "sent": "All the tutorials and talks and video lectures.",
                    "label": 0
                },
                {
                    "sent": "There are plenty.",
                    "label": 0
                },
                {
                    "sent": "There are some very very good.",
                    "label": 0
                },
                {
                    "sent": "Among all those one, if you're interested in graphical models, the one of somewhere away so very, very good.",
                    "label": 0
                },
                {
                    "sent": "Summarize.",
                    "label": 0
                },
                {
                    "sent": "SMR Summoner OSROWES.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So you've seen yesterday already.",
                    "label": 0
                },
                {
                    "sent": "Statistics is quite important, so I will be talking about statistical machine learning.",
                    "label": 1
                },
                {
                    "sent": "And I guess one of the main trends in machine learning is now is that?",
                    "label": 0
                },
                {
                    "sent": "It's like it's really interaction between static statistics and computer science.",
                    "label": 1
                },
                {
                    "sent": "So the main reason that data is kind of increasing all over the, especially on the web, you have images.",
                    "label": 0
                },
                {
                    "sent": "You have sound sensor.",
                    "label": 0
                },
                {
                    "sent": "We kind of have a lot of sensors.",
                    "label": 0
                },
                {
                    "sent": "Everything is recorded to at least a lot of digital information is is being available and so you have like an over kind of becoming difficult to process that information to get you have too much information.",
                    "label": 0
                },
                {
                    "sent": "So you want to extract what is really important.",
                    "label": 1
                },
                {
                    "sent": "And the second, so that's the first thing.",
                    "label": 0
                },
                {
                    "sent": "The second thing is that the data that we record this usually noisy like and unreliable.",
                    "label": 0
                },
                {
                    "sent": "So you have like recording errors you have like a machine folds, so there all sorts of reasons when you kind of automatically record data that makes that the data is noisy, and so you have to be, you have to take uncertainty into account and you have to be robust to those fault.",
                    "label": 0
                },
                {
                    "sent": "And so today I will focus on strategy.",
                    "label": 1
                },
                {
                    "sent": "So one specific way to look at machine learning, which is that you assume you have like some generation process of the data.",
                    "label": 0
                },
                {
                    "sent": "So which is kind of the model that you kind of device.",
                    "label": 0
                },
                {
                    "sent": "And this is like under the form of probability distributions, so it's easier hierarchy of probability distributions and so you will have like lots of examples in the 2nd and the 3rd part.",
                    "label": 0
                },
                {
                    "sent": "And so I hope this will become clear.",
                    "label": 0
                },
                {
                    "sent": "So it's really it's like.",
                    "label": 0
                },
                {
                    "sent": "How different variables are depending on each other and how they form?",
                    "label": 0
                },
                {
                    "sent": "How is the joint distribution structured?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So I guess one specificity of machine learning compared to statistics because machine learning is like borrowing a lot of from statistics is that?",
                    "label": 0
                },
                {
                    "sent": "We mainly looking at very large scale problems where you have like a lot of data and so having an interesting model which is like we just make sense is not sufficient.",
                    "label": 0
                },
                {
                    "sent": "You want something which is computational tractable.",
                    "label": 0
                },
                {
                    "sent": "You want to be so the computation complexity is a very important, and it's like a very strong constraint on the models you want to investigate and develop.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So today I will be focused on graphical models and so graphical models is.",
                    "label": 0
                },
                {
                    "sent": "Is that the intersection of probability theory and graph theory?",
                    "label": 1
                },
                {
                    "sent": "So in fact it's like you're looking at statistical machine learning and then you also take on board graph theory just to represent the type of models you kind of consider an based on some interesting properties you able to reason directly on the graphs instead of having to look at the distributions themselves.",
                    "label": 0
                },
                {
                    "sent": "So you can already make some conclusion or make some some.",
                    "label": 1
                },
                {
                    "sent": "The reason on the graph themselves.",
                    "label": 0
                },
                {
                    "sent": "So it's also interesting because based on those properties you kind of able.",
                    "label": 1
                },
                {
                    "sent": "If you have like a very complex model, you able to separate them in different parts and then only look at those specific parts, do the inference onto the learning and then you have like given given that you're looking at probability theory, that's kind of the way.",
                    "label": 1
                },
                {
                    "sent": "How do you combine those different parts in a consistent way?",
                    "label": 0
                },
                {
                    "sent": "And in fact, the main point is that when you're looking at.",
                    "label": 1
                },
                {
                    "sent": "Graphical models or those those joint distributions, is that they're going to.",
                    "label": 0
                },
                {
                    "sent": "They're going to represent conditional independence ease, so that's so.",
                    "label": 0
                },
                {
                    "sent": "This is giving a structure on those joint distributions, so I think it would be.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Become more clear in a couple of slides.",
                    "label": 0
                },
                {
                    "sent": "So before I start into the real real stuff, I just want to just go through one or two applications.",
                    "label": 0
                },
                {
                    "sent": "So graphical models are applied in lots of lots of situations.",
                    "label": 1
                },
                {
                    "sent": "It's going from bad formatics as I said, to document processing, speech processing, image processing.",
                    "label": 1
                },
                {
                    "sent": "Physics and social Sciences or time series.",
                    "label": 0
                },
                {
                    "sent": "And so you have like a whole range of applications that are possible to be constantly learning and some are bit more more related to tradition is called statistic maybe.",
                    "label": 0
                },
                {
                    "sent": "But you have a lot of it's like widely.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I want to consider factory specific applications and the first one is.",
                    "label": 0
                },
                {
                    "sent": "This topic modeling, and so it wasn't planned but so yesterday during the discussion John mentioned topic models.",
                    "label": 0
                },
                {
                    "sent": "So I just wanted to give you an idea of what topic models are doing so roughly.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Joke.",
                    "label": 0
                },
                {
                    "sent": "A topic model is kind of usually formalized as a graphical model.",
                    "label": 0
                },
                {
                    "sent": "And the idea is that you kind of assume that.",
                    "label": 0
                },
                {
                    "sent": "You kind of going to.",
                    "label": 0
                },
                {
                    "sent": "You have like a whole range of documents, so we have a large corpus, large number of documents that you want to structure that in a certain you want to capture some semantic so so the meaning of what do the documents talk about?",
                    "label": 0
                },
                {
                    "sent": "And so the idea that you want to really cluster those documents in different groups and the clusters should be should capture some what we call topics, which are sort of.",
                    "label": 0
                },
                {
                    "sent": "This kind of semantically capturing some meaning of what is discussed in that specific document.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "In fact, it makes a very strong assumption about documents that it assumes that the words are generated.",
                    "label": 0
                },
                {
                    "sent": "Instead of a backup port.",
                    "label": 0
                },
                {
                    "sent": "You make a backward assumption, so that means that you ignored the sequential structure of documents.",
                    "label": 0
                },
                {
                    "sent": "You assume all the words are generated and have no connection in the sequence.",
                    "label": 0
                },
                {
                    "sent": "And so that that seems to be like a rather strong assumption, but that is sufficient at least two to capture what we call topics.",
                    "label": 0
                },
                {
                    "sent": "And so here you have like the typical topics that are could be extracted specific document.",
                    "label": 0
                },
                {
                    "sent": "So you have so topics are defined as.",
                    "label": 0
                },
                {
                    "sent": "Distributions over vocabulary words.",
                    "label": 0
                },
                {
                    "sent": "And So what you have is that this is like a ranked list of those words.",
                    "label": 0
                },
                {
                    "sent": "And those are the four different topics.",
                    "label": 0
                },
                {
                    "sent": "Usually you don't mean in general, you don't know what is the label of that topic.",
                    "label": 0
                },
                {
                    "sent": "But once you look at you look at the different words that are identified.",
                    "label": 0
                },
                {
                    "sent": "You usually quite easily can put like a sort of the actual topic.",
                    "label": 0
                },
                {
                    "sent": "What is it talking about?",
                    "label": 0
                },
                {
                    "sent": "But is that top capturing?",
                    "label": 0
                },
                {
                    "sent": "And so for example.",
                    "label": 0
                },
                {
                    "sent": "Then when you look at a specific text, you can say OK every word.",
                    "label": 0
                },
                {
                    "sent": "So when you assume you look at how documents generated, so your model is saying that every time you have a new document, you draw a specific number of topics with some probability.",
                    "label": 0
                },
                {
                    "sent": "And then once you pick one topic, draw, you're going to draw some word according to the probability over over the vocabulary words assisted the topic.",
                    "label": 0
                },
                {
                    "sent": "And so, in fact, you can assign every if you look at the document, you can assign every word to a specific topic.",
                    "label": 0
                },
                {
                    "sent": "You can say OK, oh this so you don't see it very well, but when you look at the slides you will so you have like this is like a grayscale and you see that.",
                    "label": 0
                },
                {
                    "sent": "So depending on which.",
                    "label": 0
                },
                {
                    "sent": "So you can really see which are the words that are associated with some specific topics and which are not.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the second example where graphical must work quite successfully using image denoising.",
                    "label": 0
                },
                {
                    "sent": "And roughly, you impose some graphical so.",
                    "label": 0
                },
                {
                    "sent": "People use in general Markov random fields.",
                    "label": 1
                },
                {
                    "sent": "In this case I will not discuss it today, but I think it's like a very lucrative.",
                    "label": 0
                },
                {
                    "sent": "So in this case you have.",
                    "label": 0
                },
                {
                    "sent": "This is like an original.",
                    "label": 0
                },
                {
                    "sent": "Those images are original images and then you have some some noisy images and what you want to recover is this one, but you've never never observed that that image.",
                    "label": 0
                },
                {
                    "sent": "So the question is how do you are you going to dinner?",
                    "label": 0
                },
                {
                    "sent": "Is that image and try to reconstruct something which is this kind and so.",
                    "label": 0
                },
                {
                    "sent": "Basically the idea is that you.",
                    "label": 0
                },
                {
                    "sent": "You're going to impose some some distribution over the pixels.",
                    "label": 0
                },
                {
                    "sent": "And you're going to look localized.",
                    "label": 0
                },
                {
                    "sent": "Look at local information around the specific pixel.",
                    "label": 0
                },
                {
                    "sent": "And so, in fact, here you have like 2 techniques to different ones where the first one you kind of.",
                    "label": 0
                },
                {
                    "sent": "You don't look at local information, you kind of the noise.",
                    "label": 0
                },
                {
                    "sent": "The basic statistics were in this case you really take into account what are the neighboring pixels telling you, and that information is usually giving you more more.",
                    "label": 0
                },
                {
                    "sent": "Improves the denoising process.",
                    "label": 0
                },
                {
                    "sent": "So I guess.",
                    "label": 0
                },
                {
                    "sent": "I'm just wondering if you see a difference between those two from where you sit.",
                    "label": 0
                },
                {
                    "sent": "But if you look closer you will definitely see a different, so this one is more blurry in fact.",
                    "label": 0
                },
                {
                    "sent": "Yeah there is.",
                    "label": 0
                },
                {
                    "sent": "So if you look closely, you see this one is a.",
                    "label": 0
                },
                {
                    "sent": "Is sharper but.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the third example, which in Sam from Xerox I have to show you an example about printers.",
                    "label": 0
                },
                {
                    "sent": "And so people have kind of looked at.",
                    "label": 0
                },
                {
                    "sent": "Graphical models as well in this context where you have like a sort of large printer.",
                    "label": 0
                },
                {
                    "sent": "Printers that are managed by Xerox for some client, and so you have like hundreds of printers in some other companies, and so you want to monitor them from a remote basis.",
                    "label": 0
                },
                {
                    "sent": "And So what you what people have done is that they kind of look at some graphical representation of.",
                    "label": 0
                },
                {
                    "sent": "How is how so that is like you want to, for example, look how are the printers used?",
                    "label": 0
                },
                {
                    "sent": "Who is using which printer and where there located, for example.",
                    "label": 0
                },
                {
                    "sent": "So you want to identify the fault automatically and that can be for example related to the usage of.",
                    "label": 0
                },
                {
                    "sent": "Of people using specific printers.",
                    "label": 0
                },
                {
                    "sent": "So if you are always using that specific printer at some point you just stop and you go and use the one on the other side of the building, then there's likely that there is something wrong with that printer and so you can kind of infer from the behavior of people that.",
                    "label": 0
                },
                {
                    "sent": "Is something wrong?",
                    "label": 0
                },
                {
                    "sent": "And so in that case, when this is really so, you have like the different colors are different printers and the dotted line, just like how many times every time there is a dot, it's someone every line is like someone somewhere else.",
                    "label": 0
                },
                {
                    "sent": "So every time reserved out.",
                    "label": 0
                },
                {
                    "sent": "That means there is a printing job by that person and then you have like you see for example in this case you see that person was printing at some point, it just stopped printing.",
                    "label": 0
                },
                {
                    "sent": "So you kind of dislike though.",
                    "label": 0
                },
                {
                    "sent": "Greyscales is like how probable it was that the printer is like there's some default.",
                    "label": 0
                },
                {
                    "sent": "With the printer.",
                    "label": 0
                },
                {
                    "sent": "Based also on the usage of those printers.",
                    "label": 0
                },
                {
                    "sent": "You can also see if so when you have to manage printers from remote you want, so it's like device management.",
                    "label": 0
                },
                {
                    "sent": "Based on the usage you can also see OK, this printer has moved.",
                    "label": 0
                },
                {
                    "sent": "For example, if at some point people start looking so we kind of there's a printer here.",
                    "label": 0
                },
                {
                    "sent": "We kind of using that printer and then like later.",
                    "label": 0
                },
                {
                    "sent": "You get started using that printer.",
                    "label": 0
                },
                {
                    "sent": "It's not unlikely that people have moved the printer, and since you're matching it remotely, it's very difficult to retrace.",
                    "label": 0
                },
                {
                    "sent": "You don't want to send somebody to check out what happened with that printer, so this is kind of general and can be applied to any type of devices when you want to mention you have like a large number of devices you want to monitor.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So let's start.",
                    "label": 0
                },
                {
                    "sent": "So those are just like 3.",
                    "label": 0
                },
                {
                    "sent": "Examples to illustrate.",
                    "label": 0
                },
                {
                    "sent": "Where graphical models were used to do do some learning.",
                    "label": 0
                },
                {
                    "sent": "Think you had like many examples on the John.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "When he discussed about introduction to machine learning.",
                    "label": 0
                },
                {
                    "sent": "So the next three parts is will be.",
                    "label": 0
                },
                {
                    "sent": "Will you, looking into the techniques?",
                    "label": 0
                },
                {
                    "sent": "So unfortunately not be much, many applications or like examples or examples, but I think it's important also to local, so those slides are more like sort of.",
                    "label": 0
                },
                {
                    "sent": "So does every lecture notes you have, like maybe too much material and there's like a lot of text.",
                    "label": 0
                },
                {
                    "sent": "It's not like like in between like nodes and it's like.",
                    "label": 0
                },
                {
                    "sent": "So the first thing I would like to discuss our Bayesian networks so directed graphical models.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The four points is like the first one is conditional independence.",
                    "label": 0
                },
                {
                    "sent": "You had already a little bit of that yesterday.",
                    "label": 0
                },
                {
                    "sent": "So we focused a little bit more today.",
                    "label": 0
                },
                {
                    "sent": "Then some interesting properties like this separation and Markov blanket that help you reason on on graphs directly and then finally I will spend some time how to do learning in Bayesian networks so.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "In a more general case, and then we will see how this can be applied in more concrete examples in the 2nd and 3rd and the 3rd part.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I forgot to say that the idea is that to spend like about one hour on every part this morning.",
                    "label": 0
                },
                {
                    "sent": "So free.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That would be reasonable.",
                    "label": 0
                },
                {
                    "sent": "So the basics are so, so just like to show how we represent.",
                    "label": 0
                },
                {
                    "sent": "So we will be looking at directed graphical.",
                    "label": 0
                },
                {
                    "sent": "Also, there's a graphical models of this kind.",
                    "label": 0
                },
                {
                    "sent": "Those are undirected ones, just to introduce like the formalism you have nodes.",
                    "label": 0
                },
                {
                    "sent": "And then some links between those nodes which are directed in our case today.",
                    "label": 0
                },
                {
                    "sent": "And those are those indicate some form of dependency between random variables.",
                    "label": 0
                },
                {
                    "sent": "So nodes are random variables.",
                    "label": 0
                },
                {
                    "sent": "Our dependencies and then you have also so when we color one of the nodes, that means that the variable is observed and when it's not that means it's late and not observed.",
                    "label": 0
                },
                {
                    "sent": "So it's usually you introduced because there is really something.",
                    "label": 0
                },
                {
                    "sent": "It's usually because of the modeling purpose, so you assume some other random variables that are not directly observed.",
                    "label": 0
                },
                {
                    "sent": "You also have plates and please just mean that this node is repeated.",
                    "label": 0
                },
                {
                    "sent": "So you have this notice repeated N times just like this is for convenience and not to have to draw too many to many of the errors and.",
                    "label": 0
                },
                {
                    "sent": "So directed graph directed graphs are called Bayesian networks or basic base.",
                    "label": 1
                },
                {
                    "sent": "That's also the cause, sometimes called belief networks and so on.",
                    "label": 0
                },
                {
                    "sent": "So you have like different names for the same thing.",
                    "label": 1
                },
                {
                    "sent": "On the other hand, undirected graphical graphical models, which I will not discuss today, you have Markov networks, Markov, random fields and so on.",
                    "label": 0
                },
                {
                    "sent": "And then if you combine the two types of graphical models and you get something which is called chain graphs.",
                    "label": 0
                },
                {
                    "sent": "Just like.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, with the conditional independence property.",
                    "label": 1
                },
                {
                    "sent": "So I think you might have discussed that also on the first day with Frances at least yesterday a little bit.",
                    "label": 1
                },
                {
                    "sent": "So there are two things that I would like to start this like statistical independence.",
                    "label": 0
                },
                {
                    "sent": "So we say that some random variable X is independent of Y.",
                    "label": 0
                },
                {
                    "sent": "If in fact the joint probability is factorizing.",
                    "label": 0
                },
                {
                    "sent": "So if you use the standard.",
                    "label": 0
                },
                {
                    "sent": "Can I forgot to take?",
                    "label": 0
                },
                {
                    "sent": "Bored.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so the standard rule for it if you want to decompose.",
                    "label": 0
                },
                {
                    "sent": "The joint of XY.",
                    "label": 0
                },
                {
                    "sent": "This is like P of X given y * Y.",
                    "label": 1
                },
                {
                    "sent": "So when you have independence dependency on why Vanisher so so you have the joint factorizes into the distribution and the product of the marginals of X times be applied.",
                    "label": 0
                },
                {
                    "sent": "Now conditional independence ease is so we know this way, so X is independent of Y given Z.",
                    "label": 0
                },
                {
                    "sent": "Means that there is some factorization when you condition on some other random variable.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the the joint here P of XY given some other variable Z, if you apply the rules of probability you have like you can decompose in this way.",
                    "label": 1
                },
                {
                    "sent": "So we have X given Y&Z times the probability of a given given Z.",
                    "label": 0
                },
                {
                    "sent": "And since X&Y are independent given Z device that X is going to take, if you condition on Z is, so the value of Y, sorry, it's not going to impact the value that X can take.",
                    "label": 0
                },
                {
                    "sent": "So the dependencies why vanish is so expect rising again, like in statistic independence, but only when you conditional some variables.",
                    "label": 0
                },
                {
                    "sent": "You have this factorization.",
                    "label": 0
                },
                {
                    "sent": "So put it in another way.",
                    "label": 0
                },
                {
                    "sent": "You can also look at the probably of X given Y&Z values taken.",
                    "label": 0
                },
                {
                    "sent": "So if you condition values of Y this has no impact on the bands of X, so you can just drop the dependency on on why in this case as well, and the reverse is also true.",
                    "label": 0
                },
                {
                    "sent": "So condition dependencies quite.",
                    "label": 0
                },
                {
                    "sent": "It's quite natural you have like.",
                    "label": 0
                },
                {
                    "sent": "Here's some examples, but so from real life I would say so if you can.",
                    "label": 1
                },
                {
                    "sent": "For example, imagine that my genome is going to be independent.",
                    "label": 0
                },
                {
                    "sent": "My grandmother, genome condition on my mother's genome.",
                    "label": 1
                },
                {
                    "sent": "So that's the intuition between of condition independence.",
                    "label": 0
                },
                {
                    "sent": "Yes dear.",
                    "label": 0
                },
                {
                    "sent": "So did you, did you?",
                    "label": 0
                },
                {
                    "sent": "Did you discuss that first with Frances or not?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You decompose.",
                    "label": 0
                },
                {
                    "sent": "The joint here into this type of factorization.",
                    "label": 0
                },
                {
                    "sent": "So you have like probably have given Y times of the probability of why so when you have like in statistical independence?",
                    "label": 0
                },
                {
                    "sent": "X has no impact on on device taken by sorry.",
                    "label": 0
                },
                {
                    "sent": "Why has no impact on voice taken by X?",
                    "label": 0
                },
                {
                    "sent": "So this is so if you have some statistic independence, you have like the factorization of this form and the conditional independence of the same form, except that you're going to condition on some other variables, which is which I called here Z.",
                    "label": 0
                },
                {
                    "sent": "So again you have like this factorization and then independence when you condition or that that specific problem.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If you look at the so going to define like a graph, what's the probability graphical model we consider some set of random variables?",
                    "label": 1
                },
                {
                    "sent": "And in fact, those variables are related to each other to joint distribution.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "The graphical itself is going to materialize.",
                    "label": 0
                },
                {
                    "sent": "Some conditional independence is between those variables.",
                    "label": 0
                },
                {
                    "sent": "So in fact this is going to introduce some structure in the joint distribution of this genre.",
                    "label": 1
                },
                {
                    "sent": "Solution is not going to be going to factorize in some specific way.",
                    "label": 0
                },
                {
                    "sent": "Because of those condition independence dependencies.",
                    "label": 0
                },
                {
                    "sent": "And so this has two impacts.",
                    "label": 0
                },
                {
                    "sent": "One, it gives you more interpretation about what's happening.",
                    "label": 0
                },
                {
                    "sent": "Usually the dependencies depend on on prior knowledge about the problem, but also it's going to have an impact on the computation and storage number of parameters that you will use to represent that joined distribution.",
                    "label": 0
                },
                {
                    "sent": "So people like to stay.",
                    "label": 0
                },
                {
                    "sent": "So if you have like the joint the full joint.",
                    "label": 0
                },
                {
                    "sent": "If you have like all the random variables there there.",
                    "label": 0
                },
                {
                    "sent": "If you don't look at the condition dependency that you have like the full joint you have this directed graphical model that is kind of sort of computer sort of filter and then you get like a specific factorized joint.",
                    "label": 0
                },
                {
                    "sent": "So roughly you have like some.",
                    "label": 0
                },
                {
                    "sent": "Dependencies between some random variables.",
                    "label": 0
                },
                {
                    "sent": "People like to call that.",
                    "label": 0
                },
                {
                    "sent": "Factorization filtering, but it doesn't matter.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So more precisely, if you.",
                    "label": 0
                },
                {
                    "sent": "This is 1 example of Bayesian network.",
                    "label": 0
                },
                {
                    "sent": "And So what we call the base network is a set of probability distributions associated with a directed acyclic graph.",
                    "label": 1
                },
                {
                    "sent": "So this is a director.",
                    "label": 0
                },
                {
                    "sent": "Basically graph.",
                    "label": 0
                },
                {
                    "sent": "That means that all edges are directed and there are no cycles.",
                    "label": 0
                },
                {
                    "sent": "If you go follow the arrows, you will never be able to make loops into the graph.",
                    "label": 0
                },
                {
                    "sent": "So in fact, if you look at this specific.",
                    "label": 0
                },
                {
                    "sent": "Graph.",
                    "label": 0
                },
                {
                    "sent": "You can have, so this is going to do some specific factorization of the joint, but that doesn't tell you you don't have to impose any specific distributions of over different nodes, so it's just like looking in an abstract way of the joint and how the different variables depend on each other.",
                    "label": 0
                },
                {
                    "sent": "So we say that a node, so not a.",
                    "label": 1
                },
                {
                    "sent": "For example in this case is a parent of node B because there's a direct link from A to B and vice versa.",
                    "label": 0
                },
                {
                    "sent": "We talk about children of specific nodes.",
                    "label": 0
                },
                {
                    "sent": "And typically so in based networks unload specific node here is independent of its ancestor.",
                    "label": 0
                },
                {
                    "sent": "If you condition on its parents condition.",
                    "label": 0
                },
                {
                    "sent": "All these parents.",
                    "label": 0
                },
                {
                    "sent": "And so those run.",
                    "label": 0
                },
                {
                    "sent": "The verbs can be either continuous or discrete, and so that's why I would like to focus on the so in the latent variable model part.",
                    "label": 0
                },
                {
                    "sent": "So the second part I will discuss on the one side some discrete latent variable models, and continuously variable amounts.",
                    "label": 0
                },
                {
                    "sent": "Something.",
                    "label": 0
                },
                {
                    "sent": "It's so roughly.",
                    "label": 0
                },
                {
                    "sent": "So if you have the.",
                    "label": 0
                },
                {
                    "sent": "So if you don't know anything about about your own variables, then you can see that you have a joint of those three variables and hours here.",
                    "label": 0
                },
                {
                    "sent": "They're going to tell you how those variables depend on each other, and so in fact, what's happening is you have.",
                    "label": 0
                },
                {
                    "sent": "PA doesn't depend on anything, so you have like some prior probability on A and then you can say that.",
                    "label": 0
                },
                {
                    "sent": "FP of B given a.",
                    "label": 0
                },
                {
                    "sent": "Because B depends on depends on F and times P of C depending on NB.",
                    "label": 0
                },
                {
                    "sent": "So it's just like giving you an idea of how the variables depend on one another.",
                    "label": 0
                },
                {
                    "sent": "So it's just so because so.",
                    "label": 0
                },
                {
                    "sent": "So when I was talking about filtering roughly, this is like saying, OK, the joint, you can rewrite it in some specific factorized form, depending on when you look directly at the graph.",
                    "label": 0
                },
                {
                    "sent": "Then we had waited.",
                    "label": 0
                },
                {
                    "sent": "In fact, in the case of Bayesian networks, weights wouldn't represent.",
                    "label": 0
                },
                {
                    "sent": "Something, maybe extension?",
                    "label": 0
                },
                {
                    "sent": "I don't know if any, but so the probability.",
                    "label": 0
                },
                {
                    "sent": "So everyone overtakes the specific value of some probability.",
                    "label": 0
                },
                {
                    "sent": "The waiting here is.",
                    "label": 0
                },
                {
                    "sent": "It would, for example, if you have like this is defining a specific relation between variables.",
                    "label": 0
                },
                {
                    "sent": "If you introduce which kind of if you want to reflect the factorized and then you will not normalize properly.",
                    "label": 0
                },
                {
                    "sent": "So you know probability needs always to factorize too.",
                    "label": 0
                },
                {
                    "sent": "Sorry to sum up to one.",
                    "label": 0
                },
                {
                    "sent": "So the weights might.",
                    "label": 0
                },
                {
                    "sent": "I don't know how you would use a weight in that case.",
                    "label": 0
                },
                {
                    "sent": "But you have to be consistent from probability theory.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it's right to the question by the so.",
                    "label": 0
                },
                {
                    "sent": "The graph is going to so because of the condition dependencies.",
                    "label": 0
                },
                {
                    "sent": "You will have specific to risation of the joint distribution, so that's that's one example.",
                    "label": 1
                },
                {
                    "sent": "And in general you can write it in this.",
                    "label": 0
                },
                {
                    "sent": "In this way you have like for a graph of nodes.",
                    "label": 1
                },
                {
                    "sent": "The joint will decompose in this specific way, so you have the product over so you have like big and nodes and so you can write this.",
                    "label": 0
                },
                {
                    "sent": "This decompose this joint into a product of the probability of a specific node given the parents.",
                    "label": 0
                },
                {
                    "sent": "Spent.",
                    "label": 0
                },
                {
                    "sent": "So those are all PN are all the random variables that are the parents of XN.",
                    "label": 1
                },
                {
                    "sent": "And so in some way this can.",
                    "label": 0
                },
                {
                    "sent": "What is important here is that this is like a local decomposition.",
                    "label": 0
                },
                {
                    "sent": "You only condition on some variables that are close to the extent, so that touching with some.",
                    "label": 1
                },
                {
                    "sent": "The right links to that specific variable, so you have like a local decomposition of the joint and that is the main interesting bit about graphical models that it shows.",
                    "label": 0
                },
                {
                    "sent": "That tells you how to factorize how to look at local condition distributions.",
                    "label": 0
                },
                {
                    "sent": "Now, in the case of the Bayesian networks.",
                    "label": 0
                },
                {
                    "sent": "So this is properly normalized, so that's a little complication.",
                    "label": 0
                },
                {
                    "sent": "When you look at undirected graphical models, you also decompose a joint into some product of factors, but those are usually not normalized, so you have to normalize them.",
                    "label": 0
                },
                {
                    "sent": "So that probably is like if you sum over all the values possible, you should submit to one.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here, here in the case of this network, so you don't have to normalize.",
                    "label": 0
                },
                {
                    "sent": "This is because every factor is properly normalized.",
                    "label": 0
                },
                {
                    "sent": "If you have an directed graphical models you don't condition and you have like just like a function of a certain number of random variables, and so that's not the probability function is not probably just like you just imposes positive, but it's not so.",
                    "label": 0
                },
                {
                    "sent": "So if you want to normalize, have to sum over all possible possibilities so that you're probably probably is like something up to one.",
                    "label": 0
                },
                {
                    "sent": "That's, but in case you're based networks, you don't, you don't need to normalize.",
                    "label": 0
                },
                {
                    "sent": "So about the factorization.",
                    "label": 0
                },
                {
                    "sent": "So is this factorization useful?",
                    "label": 0
                },
                {
                    "sent": "So I have a little example, so you have this.",
                    "label": 0
                },
                {
                    "sent": "A sequence of.",
                    "label": 0
                },
                {
                    "sent": "So linear sequence of random variables.",
                    "label": 0
                },
                {
                    "sent": "So here and and variables.",
                    "label": 0
                },
                {
                    "sent": "So if you consider just semigroups tree, so you have like just three of those nodes.",
                    "label": 0
                },
                {
                    "sent": "You can apply so you look at the condition dependencies assumption so you know that you're going to.",
                    "label": 0
                },
                {
                    "sent": "In the case of tree.",
                    "label": 0
                },
                {
                    "sent": "You have like X1.",
                    "label": 0
                },
                {
                    "sent": "Depends on nothing.",
                    "label": 0
                },
                {
                    "sent": "So you have probably over X one times the probability of X2 given X one times the property of extra given next to.",
                    "label": 0
                },
                {
                    "sent": "So you have that extremes condition independent of X1 given X2.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So here you have fun dependencies like dropping, so if you assume that every random variable here XM is taking discrete values, so K discrete values can take values, so it's not like continuous random variables are discrete random variables.",
                    "label": 0
                },
                {
                    "sent": "And then you look at the factorization.",
                    "label": 0
                },
                {
                    "sent": "So if you.",
                    "label": 0
                },
                {
                    "sent": "If you, for example, want to compute the marginal over of X2.",
                    "label": 0
                },
                {
                    "sent": "Then you have to sum over all values of X1 and all values of X tree.",
                    "label": 0
                },
                {
                    "sent": "And so if you look at the joint here, every variable can take K values.",
                    "label": 0
                },
                {
                    "sent": "So that means that if you want to compute this.",
                    "label": 0
                },
                {
                    "sent": "This marginal for every of the K values of X2.",
                    "label": 0
                },
                {
                    "sent": "You have to do you have to sum over the key value X one in the caves xtree so you have like something which is of complexity K ^3.",
                    "label": 0
                },
                {
                    "sent": "Now, if you.",
                    "label": 0
                },
                {
                    "sent": "I want to compute this this marginal using the condition dependencies assumption.",
                    "label": 0
                },
                {
                    "sent": "So you use this factorization.",
                    "label": 0
                },
                {
                    "sent": "Then you can rewrite.",
                    "label": 0
                },
                {
                    "sent": "You have to sum of X1 and X3, but in fact if you look at if you bring those together, we have the joint X one X2 and that doesn't depend on X3.",
                    "label": 0
                },
                {
                    "sent": "So you can.",
                    "label": 0
                },
                {
                    "sent": "You can use distributive law and you can.",
                    "label": 0
                },
                {
                    "sent": "You can just look at here you can first come over X3 and then you can some second step, some over X1.",
                    "label": 0
                },
                {
                    "sent": "So here again you have here only have like two of those random variables.",
                    "label": 0
                },
                {
                    "sent": "So for every value of K you have to send over K values of X3.",
                    "label": 0
                },
                {
                    "sent": "So you have like something which is complexity K square.",
                    "label": 0
                },
                {
                    "sent": "And once you've done that, you again you have like only two random variables.",
                    "label": 0
                },
                {
                    "sent": "So again you have like a complexity of squared, so that means you move from K cube two 2 * K ^2.",
                    "label": 0
                },
                {
                    "sent": "So this is very important when you have like very large graphs where and when when variables can take large number of.",
                    "label": 0
                },
                {
                    "sent": "Values.",
                    "label": 0
                },
                {
                    "sent": "Because this is usually something like that is just like in practice for large scale is just like impossible to deal with.",
                    "label": 0
                },
                {
                    "sent": "It also leads to, so that's for the computations.",
                    "label": 0
                },
                {
                    "sent": "So, so when you want to compute some marginals of some probability for some specific nodes.",
                    "label": 0
                },
                {
                    "sent": "It's also interesting when you want to store.",
                    "label": 0
                },
                {
                    "sent": "There are the parameters of the model.",
                    "label": 0
                },
                {
                    "sent": "So if you look at so without the factorization.",
                    "label": 0
                },
                {
                    "sent": "You have, like I said, you have like every entry.",
                    "label": 0
                },
                {
                    "sent": "Every variable can take values you want to need to be normalized or there's one.",
                    "label": 0
                },
                {
                    "sent": "So you're using one dependencies, so that's why you have like K -- 1 parameters because there's one constraint.",
                    "label": 1
                },
                {
                    "sent": "So because all the everything needs to sum up to one.",
                    "label": 0
                },
                {
                    "sent": "And now if you look at the factorized version, then.",
                    "label": 1
                },
                {
                    "sent": "You have here K -- 1 parameters so you have like a parameters, but they have to submit to one so everyone so you're losing one.",
                    "label": 0
                },
                {
                    "sent": "You have like 1 parameter system and all the others.",
                    "label": 0
                },
                {
                    "sent": "So here you have lucky minus one parameters and for this one you have for the K values of X1 you have again like a -- 1 parameter to determine X2.",
                    "label": 0
                },
                {
                    "sent": "And so this is repeated in minus one times.",
                    "label": 0
                },
                {
                    "sent": "So you have something which is again K. M * K ^2 in complexity where this is KM in complexity.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now the question is.",
                    "label": 0
                },
                {
                    "sent": "Do we do some additional in dependencies in the graph so we have we so typically you have some expert.",
                    "label": 0
                },
                {
                    "sent": "He like a specific problem, we want to model, you know some.",
                    "label": 0
                },
                {
                    "sent": "You assume you know that there are some conditional dependencies between some random variables and so someone is going to draw the graph.",
                    "label": 0
                },
                {
                    "sent": "The graphical modeler obsessed with this distribution.",
                    "label": 0
                },
                {
                    "sent": "And so roughly, if you have like the full graph, the condition dependencies correspond to removing some links in the graph.",
                    "label": 0
                },
                {
                    "sent": "Those are like representing those condition dependencies.",
                    "label": 0
                },
                {
                    "sent": "So the question is whether we do some additional in dependencies by doing that.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you look at this this graph.",
                    "label": 0
                },
                {
                    "sent": "Is there is a, for example independent of B?",
                    "label": 0
                },
                {
                    "sent": "So can we directly deduce that by looking at the graph?",
                    "label": 0
                },
                {
                    "sent": "And similarly, if you condition on some variable, F is a condition independent to be given given F. So we would like to find some ways to by just looking at the graph, can we conclude something interesting about the joint and the button dependencies between random variables?",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so I will go through a couple of examples.",
                    "label": 0
                },
                {
                    "sent": "I will not probably not do all of them because I think I'm running.",
                    "label": 0
                },
                {
                    "sent": "Already out of time.",
                    "label": 0
                },
                {
                    "sent": "But so there are three people.",
                    "label": 0
                },
                {
                    "sent": "There are two types of nodes in a basic network you have like head to tail nodes.",
                    "label": 0
                },
                {
                    "sent": "So see here is a head to tail node you like tail to 10 nodes were in fact.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Did you take notes out of this form?",
                    "label": 0
                },
                {
                    "sent": "So see here is a data node that means that there are only hours going, so going out of that node.",
                    "label": 0
                },
                {
                    "sent": "And then you have all.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Head to head notes, which is like the opposite, so see here.",
                    "label": 0
                },
                {
                    "sent": "In this case we had to have known because ours are going to that node, so will consider those three situations.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now will try to.",
                    "label": 0
                },
                {
                    "sent": "Show you what, so if you discuss like statistical independence and conditional independence in those three cases.",
                    "label": 0
                },
                {
                    "sent": "First question, if you have this.",
                    "label": 0
                },
                {
                    "sent": "This graph so is RA&B independent, so you have like a head to tail node C here, and so the question is whether the joint of A&B Factorizing P of property of a terms of probability of B.",
                    "label": 0
                },
                {
                    "sent": "So from the same rule of probabilities, you can write this joint as the marginal over the joint of the tree of random variables.",
                    "label": 0
                },
                {
                    "sent": "So you send over, see.",
                    "label": 0
                },
                {
                    "sent": "And then you can look at the graph and you can just apply the factorized.",
                    "label": 0
                },
                {
                    "sent": "So based on the condition dependencies you can rewrite this disjoint in this specific way.",
                    "label": 0
                },
                {
                    "sent": "Right, so P of eight times Pfc, given a times P of given C&B is independent of a given even see, so that's why you don't have dependency there.",
                    "label": 0
                },
                {
                    "sent": "You some overseas, so this term does not depend on this factor does not depend on C, so you can remove it from the sum and so you have like those two remaining factors that are there.",
                    "label": 0
                },
                {
                    "sent": "So you can write a joint so this time this is going to be the joint of PB, probably of B&C given a.",
                    "label": 0
                },
                {
                    "sent": "Into some oversee and so you end up with this.",
                    "label": 0
                },
                {
                    "sent": "This product or FP of eight times of be given a so that tells you that in fact, in this specific case, in general there is no independence between B&A.",
                    "label": 0
                },
                {
                    "sent": "Because a depends on depends on the on P.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the other situation is where you have ahead.",
                    "label": 0
                },
                {
                    "sent": "Do you want to test for condition independence?",
                    "label": 0
                },
                {
                    "sent": "So you can look at.",
                    "label": 0
                },
                {
                    "sent": "You assume that sees observed.",
                    "label": 0
                },
                {
                    "sent": "And you want to check whether this joint conditional C is going to factorize in this way.",
                    "label": 0
                },
                {
                    "sent": "So again, you look at you look at some you can look for example at the probability of A&B given C. Which you can rewrite using Bayes rule in this way, so you have like the joint of the tree renovables divided by bad, probably of the marginal C. Which then can decompose according to the graph above.",
                    "label": 0
                },
                {
                    "sent": "So you have like.",
                    "label": 0
                },
                {
                    "sent": "Again, it's exactly the same expression as before and then you can recognize this again as.",
                    "label": 0
                },
                {
                    "sent": "As a base rule.",
                    "label": 0
                },
                {
                    "sent": "So we can discuss this situation correspond to probability of a given given C. And so you end up with this factorized form.",
                    "label": 0
                },
                {
                    "sent": "So here in this case you can.",
                    "label": 0
                },
                {
                    "sent": "You can conclude that.",
                    "label": 0
                },
                {
                    "sent": "You always have condition dependencies between A&B.",
                    "label": 0
                },
                {
                    "sent": "When I had to tell, know this condition of inherited mode.",
                    "label": 0
                },
                {
                    "sent": "Between the two.",
                    "label": 0
                },
                {
                    "sent": "Neighborhood.",
                    "label": 0
                },
                {
                    "sent": "So what is interesting is that in this case.",
                    "label": 0
                },
                {
                    "sent": "You were you had this probability of C given a.",
                    "label": 0
                },
                {
                    "sent": "And by applying base rule, in fact, you get something it's like you get like you've reverting the link, like reverting the link in the in the Bayesian network because you get this probably probability of a given C. Right?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are four other cases, so I will not go through them because because of time, but I think it's what if you want to go through to it by yourself.",
                    "label": 0
                },
                {
                    "sent": "It's like using the same types of properties on basics, basic rules of probability, like like looking at the graph, how it decomposes based on conditional dependencies using base rule using the sum rule of probabilities and the product rule of probabilities.",
                    "label": 0
                },
                {
                    "sent": "So just about the results.",
                    "label": 0
                },
                {
                    "sent": "So when you look at tail to tail node, so in this case if you want to check statistic independence then if you go through the math you will see that there is no statistic independence.",
                    "label": 0
                },
                {
                    "sent": "In general, so when you have like a tail to tail node then P of A&B are not always independent, not necessarily independent.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when you condition on C then you have like condition dependence.",
                    "label": 0
                },
                {
                    "sent": "So this case A&B are condition independent.",
                    "label": 0
                },
                {
                    "sent": "So that was in fact in those two cases in the tail to tail nodes and in the head.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Turns out it was.",
                    "label": 0
                },
                {
                    "sent": "You had like similar.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Concluir",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Asians?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "In the head to tail notice slightly different there you get the opposite, so see again here is a head to head to head to head node.",
                    "label": 0
                },
                {
                    "sent": "You want to check about independence between A&B and if you go through Marta get that is independent of B.",
                    "label": 0
                },
                {
                    "sent": "General.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But when you condition on that variable then you lose so independent so you don't have conditional dependencies.",
                    "label": 0
                },
                {
                    "sent": "If you condition on.",
                    "label": 0
                },
                {
                    "sent": "See in this type of.",
                    "label": 0
                },
                {
                    "sent": "Astructur",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So that's.",
                    "label": 0
                },
                {
                    "sent": "Leads us to the concept of the separation.",
                    "label": 1
                },
                {
                    "sent": "And so roughly the idea of the separation you want to have a rule that tells you OK, if I have a large graphical model which are the parts that are independent from one another.",
                    "label": 1
                },
                {
                    "sent": "And so the what we defined it as a block path is one that contains at least observed head to tail or tail to tail nodes.",
                    "label": 0
                },
                {
                    "sent": "So if you remember, in those cases you had conditional independency between the different parts and or you have an observed head to head note of which number descendants are observed.",
                    "label": 1
                },
                {
                    "sent": "So again in this case you had if you had like statistic independency between the parts.",
                    "label": 0
                },
                {
                    "sent": "So when you have, once you have like so this is what we call the block path and so if you have like 2 parts of the graph, you have like a part AB and C3 part.",
                    "label": 0
                },
                {
                    "sent": "Sorry that are non intersecting so they have no nodes in common.",
                    "label": 0
                },
                {
                    "sent": "Then you can say that is condition dependent on be given, see if all the possible path from A to B are blocked.",
                    "label": 1
                },
                {
                    "sent": "Now that is interesting because you don't need to assume anything else about the distributions, just based on the structure of the graphic and circuit.",
                    "label": 0
                },
                {
                    "sent": "Those servers are independent and those are or not.",
                    "label": 1
                },
                {
                    "sent": "So that helps you to reason on the graph directly.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So can someone tell me if is conditional independent of F given B?",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "So is there a block a blocked path between A&B?",
                    "label": 0
                },
                {
                    "sent": "So this this node is a is a tail to tail node.",
                    "label": 0
                },
                {
                    "sent": "Which is observed.",
                    "label": 0
                },
                {
                    "sent": "And so you have like NBR, condition dependent.",
                    "label": 0
                },
                {
                    "sent": "Give an F. In fact, you have a. Ha yeah.",
                    "label": 0
                },
                {
                    "sent": "So in in this, in this specific case for F. So you can double check.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another important properties important notion is the Markov blanket.",
                    "label": 1
                },
                {
                    "sent": "So the Markov blanket is is what in Bayesian networks?",
                    "label": 0
                },
                {
                    "sent": "I mean, in general, is the minimal set of nodes you have to you have to take into account to isolate one specific variable from the rest of the graph?",
                    "label": 1
                },
                {
                    "sent": "So that means that once you condition all those variables that variable, the probability of variable is independent of all the rest.",
                    "label": 0
                },
                {
                    "sent": "So if you look at the property of X specific run variable XI, given all the other random variables, then you can use base rule too, right?",
                    "label": 0
                },
                {
                    "sent": "The joint.",
                    "label": 0
                },
                {
                    "sent": "Of all the random variables, you know that this Union based network.",
                    "label": 0
                },
                {
                    "sent": "So you can factorize in this way.",
                    "label": 0
                },
                {
                    "sent": "And here you have like the same.",
                    "label": 0
                },
                {
                    "sent": "You have to join but you have to sum over XI.",
                    "label": 0
                },
                {
                    "sent": "And so if you look at those, the two, the two, the two factors here, well, all the ones that are not appearing.",
                    "label": 0
                },
                {
                    "sent": "In the button at the bottom of the distraction are the ones.",
                    "label": 0
                },
                {
                    "sent": "Since you have some other xir, the factor P of XI given.",
                    "label": 0
                },
                {
                    "sent": "So I.",
                    "label": 0
                },
                {
                    "sent": "Dimes although the the conditional probabilities where XI either except using in the parents.",
                    "label": 0
                },
                {
                    "sent": "So just like in the simple, just to remind you, see how we obtain.",
                    "label": 0
                },
                {
                    "sent": "If you have the joint.",
                    "label": 0
                },
                {
                    "sent": "If you have the condition of XY given X of X given Y.",
                    "label": 0
                },
                {
                    "sent": "You can write it in.",
                    "label": 0
                },
                {
                    "sent": "In this, in this form, right?",
                    "label": 0
                },
                {
                    "sent": "This comes directly from the fact that you have X given Y times.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "So that's so.",
                    "label": 0
                },
                {
                    "sent": "You know this is the this condition property can be written in the form of this based on this form.",
                    "label": 0
                },
                {
                    "sent": "And you can just like write this in this simplified form where you just like bring that into this by writing.",
                    "label": 0
                },
                {
                    "sent": "This is like equal to the joint of X&Y, so that's what I used here in this case, and in fact you can see that this is.",
                    "label": 0
                },
                {
                    "sent": "For that we did in the second equation.",
                    "label": 0
                },
                {
                    "sent": "Here you have like the joint on the top and you have like if you sum over all values of X and you have the joint here.",
                    "label": 0
                },
                {
                    "sent": "Right, this is some similar properties, right?",
                    "label": 0
                },
                {
                    "sent": "So if you work it out you will see that you have those types of factors that are not simplifying.",
                    "label": 0
                },
                {
                    "sent": "And So what?",
                    "label": 0
                },
                {
                    "sent": "It tells us is that.",
                    "label": 0
                },
                {
                    "sent": "The variables that excite depends on R. It's parents, so those two random variables and then.",
                    "label": 0
                },
                {
                    "sent": "Here the X axis here are the variables, where X is one of the parents of those other children, so those are those two, those random variables, and in this conditioning you have also all the other parents of those children, so we should call this parses so those running rabbits.",
                    "label": 0
                },
                {
                    "sent": "So those are the ones that are forming the Markov blanket directed graphical models.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So next.",
                    "label": 0
                },
                {
                    "sent": "Couple of slides I'd like to kind of discuss little bit you can do learning in basic networks from a general perspective.",
                    "label": 0
                },
                {
                    "sent": "So we consider that there are some we have some some data that are drawn IID.",
                    "label": 1
                },
                {
                    "sent": "So what does it mean?",
                    "label": 0
                },
                {
                    "sent": "Is that the different observations are drawn from the same distribution.",
                    "label": 1
                },
                {
                    "sent": "So, so the identical assumption and their independence, meaning that every sample is independent of a fan.",
                    "label": 0
                },
                {
                    "sent": "Another neutron, not condition dependent on another.",
                    "label": 0
                },
                {
                    "sent": "So if we consider some specific specific statistical model of the data which you can write in this in this form, you have like a product of factors.",
                    "label": 0
                },
                {
                    "sent": "In the case of Bayes networks you don't have the normalizing constant, so you just have this this expression here from before.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "That simplifies to.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To this expression we have this product of factors collected factor, but the probabilities that are normalized indicator based networks.",
                    "label": 0
                },
                {
                    "sent": "And the quality of the model.",
                    "label": 0
                },
                {
                    "sent": "So how good the model fits?",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Data depends on the parameters.",
                    "label": 1
                },
                {
                    "sent": "How you you estimate those parameters.",
                    "label": 0
                },
                {
                    "sent": "So the goal here in the learning which is how do we.",
                    "label": 0
                },
                {
                    "sent": "How can we estimate parameters?",
                    "label": 0
                },
                {
                    "sent": "And I will discuss three approaches.",
                    "label": 0
                },
                {
                    "sent": "One, this maximum likelihood.",
                    "label": 1
                },
                {
                    "sent": "Maximum security and also some general principle of Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "Not going too much detail, but I just give you an intuition about the idea behind that.",
                    "label": 0
                },
                {
                    "sent": "And so for the moment, we will not assume there are any unobserved latent variables.",
                    "label": 0
                },
                {
                    "sent": "So every all the variables are observed and then we have some parameters.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Other.",
                    "label": 0
                },
                {
                    "sent": "Defining the model itself.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So the first purchase maximum likelihood estimation, so you want to estimate the parameter of maximum likelihood, and So what people do is that you look at what is the likelihood.",
                    "label": 0
                },
                {
                    "sent": "Likelihood is joint probability of the observations.",
                    "label": 1
                },
                {
                    "sent": "So since the independent, you know that it's there are factorizing so you have like the probability of every observation is independent of the other observation.",
                    "label": 0
                },
                {
                    "sent": "So you have a product of all the probabilities of the observations you looking at.",
                    "label": 0
                },
                {
                    "sent": "We're looking at the log likelihood, so in fact, that's why is the here in front.",
                    "label": 0
                },
                {
                    "sent": "So so.",
                    "label": 0
                },
                {
                    "sent": "So because you want to maximize the likelihood, so maximizing the logarithm of the this value is not going to change the problem that you want to want to tackle.",
                    "label": 0
                },
                {
                    "sent": "So if you just like.",
                    "label": 1
                },
                {
                    "sent": "Plug into the expression on the previous slide of the joint.",
                    "label": 0
                },
                {
                    "sent": "Then you can rewrite that.",
                    "label": 0
                },
                {
                    "sent": "In this way you can just isolate.",
                    "label": 0
                },
                {
                    "sent": "Since you have a log of a product, this is sum of a log.",
                    "label": 0
                },
                {
                    "sent": "When did that take the product art converted to a sum?",
                    "label": 0
                },
                {
                    "sent": "And so the goal in maximum likelihood is you have this log likelihood function.",
                    "label": 1
                },
                {
                    "sent": "And you want to find the parameters as going to maximize the log likelihood function.",
                    "label": 0
                },
                {
                    "sent": "So you want to find the Theta.",
                    "label": 0
                },
                {
                    "sent": "This maximizing this expression in this expression.",
                    "label": 0
                },
                {
                    "sent": "So when you have like a factorized form simply you can just maximizing directly, so you maximize, you want the gradient restricted parameters equal to 0.",
                    "label": 0
                },
                {
                    "sent": "When you factorize form, you can do that in depending on the different parts of the graph.",
                    "label": 0
                },
                {
                    "sent": "So let me separate the graph in different.",
                    "label": 0
                },
                {
                    "sent": "Different parts.",
                    "label": 0
                },
                {
                    "sent": "Of course, you can also minimize the negative people sometimes minimize negative black suit exactly the same.",
                    "label": 0
                },
                {
                    "sent": "The nice property about maximum likelihood is that it's consistent, meaning that when number observations tends to Infinity, you will have your parameter is going to tend to the true value of your parameter.",
                    "label": 0
                },
                {
                    "sent": "Now that is very.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nice, except that it's not working in practice.",
                    "label": 0
                },
                {
                    "sent": "Because the likelihood or is unbounded.",
                    "label": 1
                },
                {
                    "sent": "So you can have from above.",
                    "label": 0
                },
                {
                    "sent": "That means that we can maximize the like with respect to.",
                    "label": 0
                },
                {
                    "sent": "You can have like an infinite likelihood if you go perfectly to some specific data point.",
                    "label": 0
                },
                {
                    "sent": "And so that is what this form of overfitting and so.",
                    "label": 0
                },
                {
                    "sent": "Of course, you don't.",
                    "label": 0
                },
                {
                    "sent": "You're not going to generalize well, so you want to avoid.",
                    "label": 0
                },
                {
                    "sent": "You want to avoid that.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And that is.",
                    "label": 0
                },
                {
                    "sent": "I mean, I said this is true for small datasets.",
                    "label": 0
                },
                {
                    "sent": "This is true in general.",
                    "label": 0
                },
                {
                    "sent": "This is even worse.",
                    "label": 0
                },
                {
                    "sent": "For small data set that happens much more easily, So what what the typical approach is to look at.",
                    "label": 1
                },
                {
                    "sent": "The first step is to look at maximum posterior.",
                    "label": 0
                },
                {
                    "sent": "Then the the idea here is just you want to penalize unreasonable values of the parameters, so you want to keep them in some some registration to do some so it's called regularization or some smoothness on your estimator for distribution.",
                    "label": 0
                },
                {
                    "sent": "And So what?",
                    "label": 0
                },
                {
                    "sent": "What you can do is like impose some prior distribution over to.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "It was discussed yesterday in a specific context.",
                    "label": 0
                },
                {
                    "sent": "By that, but roughly so this sign here means a proportional to, so this is just applying Bayes rule, so you want to maximize the possibility of your parameters given the data.",
                    "label": 0
                },
                {
                    "sent": "So this is just basic rules and just dropped the normalizing constant.",
                    "label": 0
                },
                {
                    "sent": "And So what you maximize now is you maximize.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "The log likelihood.",
                    "label": 0
                },
                {
                    "sent": "Of the data given the parameters plus the prior.",
                    "label": 0
                },
                {
                    "sent": "So this is just like applying, so this is.",
                    "label": 0
                },
                {
                    "sent": "This corresponds to the likelihood.",
                    "label": 0
                },
                {
                    "sent": "Before, so if you just take the log, you end up with.",
                    "label": 0
                },
                {
                    "sent": "This is this term and this is the sum over all the local parameters.",
                    "label": 0
                },
                {
                    "sent": "Talk of the local parameters.",
                    "label": 0
                },
                {
                    "sent": "And it's just working the same way as before.",
                    "label": 0
                },
                {
                    "sent": "You just maximize this.",
                    "label": 0
                },
                {
                    "sent": "This expression I'm not going to discuss how you maximize it.",
                    "label": 0
                },
                {
                    "sent": "You can.",
                    "label": 0
                },
                {
                    "sent": "This is usually using any type of optimization technique, usually nonlinear optimization techniques, but in some cases could be.",
                    "label": 0
                },
                {
                    "sent": "You could have like some convex properties anyway, whether you prefer to gradient descent or greater.",
                    "label": 0
                },
                {
                    "sent": "In this case, Britain accent technique is fine.",
                    "label": 0
                },
                {
                    "sent": "Then depending on the difficulty of the problem, you have to do some more fancy things.",
                    "label": 1
                },
                {
                    "sent": "But if your posterior is very broad, that means that you have a lot of uncertainty about this specific value and just taking into account might be might be risky.",
                    "label": 0
                },
                {
                    "sent": "So that's why I usually.",
                    "label": 0
                },
                {
                    "sent": "Business decisions don't view map as a.",
                    "label": 0
                },
                {
                    "sent": "Based approach.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So based inference asset is interested in the full posterior, so you want to view teacher not as a parameter.",
                    "label": 1
                },
                {
                    "sent": "If you want this more anymore, you want it as a random variable.",
                    "label": 1
                },
                {
                    "sent": "You want to infer the probability of that given given the data instead of a point estimate.",
                    "label": 1
                },
                {
                    "sent": "And so again, it's it's based on.",
                    "label": 0
                },
                {
                    "sent": "On on base rule when you have like.",
                    "label": 1
                },
                {
                    "sent": "You're interested in the probability of Detective and the data, and then here you have likely term if the prior as you want to.",
                    "label": 0
                },
                {
                    "sent": "If you have some price usually used for some form of regularization, you want to update that prior into Star Distribution, Bank base room and then there is this specific term which is called here.",
                    "label": 0
                },
                {
                    "sent": "The evidence or the marginal likelihood which is.",
                    "label": 0
                },
                {
                    "sent": "Obtained by just like integrating over details or summing over to.",
                    "label": 0
                },
                {
                    "sent": "Right, so the reason why you're interested in so when you want to make prediction, then if you don't have like a point estimate of cheetah.",
                    "label": 0
                },
                {
                    "sent": "This is that you want to predict some.",
                    "label": 0
                },
                {
                    "sent": "If you have like a new value X star.",
                    "label": 0
                },
                {
                    "sent": "Given the data, that's the actual thing you want to predict.",
                    "label": 0
                },
                {
                    "sent": "You want to predict the probability of a new point given what you've observed that can be written in this specific form.",
                    "label": 0
                },
                {
                    "sent": "So you have like the likelihood of.",
                    "label": 0
                },
                {
                    "sent": "Then your new points are given the parameters.",
                    "label": 0
                },
                {
                    "sent": "You want to wait every possible value of the parameter by the posterior probability of that parameter given the data.",
                    "label": 0
                },
                {
                    "sent": "So in fact, when you make prediction in Bayesian inference, you're going to look at all possible models, so all values possible to go to some all over them and wait every model by the posterior probability of that parameter given the data.",
                    "label": 0
                },
                {
                    "sent": "So why are people called sometimes ensemble learning?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Whereas if you would look at maximum likelihood or or map, you would just forget about all this.",
                    "label": 0
                },
                {
                    "sent": "You would just look at this specific probability and you would just plug the most the map value or the maximum likely value.",
                    "label": 0
                },
                {
                    "sent": "So the nice thing about that is that if you are looking at, you don't.",
                    "label": 0
                },
                {
                    "sent": "If you look at this this this way.",
                    "label": 0
                },
                {
                    "sent": "Usually what you gain by doing that is that you have not only.",
                    "label": 0
                },
                {
                    "sent": "The expected value of the parameter, but you have also some uncertainties have different source on the deviation of the parameters, so you have like confidence measures in the parameter estimation and the predictions you want to make.",
                    "label": 0
                },
                {
                    "sent": "Now the problem, and that's what I'm not going to discuss because I won't have the time is that.",
                    "label": 0
                },
                {
                    "sent": "Computing all those quantities at those two quantities.",
                    "label": 0
                },
                {
                    "sent": "So or like integrals in the continuous case, or something unconscious cases intractable in practice.",
                    "label": 0
                },
                {
                    "sent": "You cannot find like closed form expressions, so meaning that you have to do some approximations so people either do approximate Bayesian inference.",
                    "label": 0
                },
                {
                    "sent": "And I will discuss them afterwards.",
                    "label": 0
                },
                {
                    "sent": "And M is kind of the first step toward separation inference.",
                    "label": 0
                },
                {
                    "sent": "There are other approaches.",
                    "label": 0
                },
                {
                    "sent": "Which are very popular in statistics in any case, which are sampling techniques, Markov chain Monte Carlo, where in fact the idea is that instead of.",
                    "label": 0
                },
                {
                    "sent": "Just trying to write a closed form solution for the posterior.",
                    "label": 0
                },
                {
                    "sent": "You're going to sample.",
                    "label": 0
                },
                {
                    "sent": "Values and so you kind of reconstruct the posterior by looking at the histogram of divide that you obtain.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, let's stop here for.",
                    "label": 0
                },
                {
                    "sent": "For five minutes.",
                    "label": 0
                },
                {
                    "sent": "And if you have any questions.",
                    "label": 0
                },
                {
                    "sent": "Don't touch it.",
                    "label": 0
                }
            ]
        }
    }
}