{
    "id": "gugxlrihx4izlyd42hjppftnphggrapt",
    "title": "Differentially Private Feature Selection via Stability Arguments, and the Robustness of the Lasso",
    "info": {
        "author": [
            "Abhradeep Guha Thakurta, Microsoft Research"
        ],
        "published": "Aug. 9, 2013",
        "recorded": "June 2013",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/colt2013_guha_thakurta_lasso/",
    "segmentation": [
        [
            "Hi, I'm really stuck with her and this is a joint work with Adam Smith.",
            "Was my PhD advisor from constricted?",
            "So."
        ],
        [
            "In the initial stage, what I'll do is I'll try to set up the learning problem and hyper how privacy plays a role in there.",
            "So consider a data set which has bunch of individuals in it and each record in the data set corresponds to an individual.",
            "And you can think this information about these individuals to be sensitive.",
            "Something like a bunch of medical tests done on a person and a bit telling that whether he has diabetes or not and there is an algorithm sitting in front of it which is releasing summary statistics about this data set to the external world.",
            "Now the status can be something like putting a classifier with.",
            "Which wants to classify whether a person has diabetes or not.",
            "Now clearly this kind of information is useful to the external world, but there might be an adversary who wants to use this information to breach into the privacy of one of the entries in the data set and our job in this talk is to protect the privacy of dissenters, while giving reasonably useful information.",
            "So so clearly there is.",
            "There are two conflicting goals.",
            "One is that you want to release sufficiently useful information, but at the same time you do not want to give two accurate information which will.",
            "Value The privacy of individuals."
        ],
        [
            "So this is a difficult problem and there has been a series of attacks in the recent past on systems built under this setting.",
            "And I'm just not name of name them right now and this is extremely activity of research in computer science or expansion across all of computer science right now.",
            "So with this high level setting, let me tell you what exact problem we want to solve and how we want to address it privacy."
        ],
        [
            "So the problem we want to solve this model selection, so I assume that you are given a discrete collection of probability distribution families M1M2 and blah blah and your goal is to select a model which best explains the data set or which best fits the data set.",
            "One example could be something like you want to do feature selection.",
            "So in this talk, the concrete model selection example, which will be using through the talk."
        ],
        [
            "Is something called sparse linear regression so sparse linear regression?",
            "Each data point is a couple where the first part of the people is the feature vector in RP and the 2nd is a respons why, which is in R. And the modeling assumption is that why is generated from X via linear transformation.",
            "So you take X multiply with the parameter vector Theta star and add some noise to it and generate wife.",
            "So in terms of sparsity, what we assume is that the model parameter which is generating Y from X is sparse, that is, it has few nonzero entries.",
            "Let's say it has non 0 interest and specifically is smaller than in and Secondly you're interested in the high dimensional region where the dimensionality of the problem is much larger than the number of samples you have.",
            "So the specific model selection problem is we want to recover the coordinates of Theta star or the support of Theta star which are nonzero.",
            "Ugh."
        ],
        [
            "This has been studied from I think 1996.",
            "How to solve this problem and the one very famous estimator for this problem is lost emitter and what it says is that if you have a list squared loss plus correspond to properly scaled L1 regularization, then the support of the minimizer Theta hat is essentially the support of Theta star.",
            "So this has been studied reasonably well in the statistique literature and we want to solve this problem with privacy."
        ],
        [
            "So what is this paper all about?",
            "It's about differentially private model selection will give generic techniques for doing model selection with privacy.",
            "One specific example of this technique will be sparse model, sparse linear regression and in the second part of the top while designing this private algorithms for sparse linear regression will give the first stability analysis for lasso."
        ],
        [
            "So white privacy is a concern, so I told you that I will design model selection algorithms with privacy.",
            "But why is it a concern?",
            "So most model selection problems can be thought of as like on this optimization problems, so the other examples of convex optimization is mean median support vector machines."
        ],
        [
            "No, let's pick one example from this.",
            "Let's take support vector machine and see that I mean what are the privacy implications in there.",
            "So if you think a standard output of a support vector machine, these are like the support vectors and support vectors.",
            "Essentially some of the data points.",
            "And the initial setting when I started, I told you that I want to preserve the privacy of the data points.",
            "Now if I'm out putting the data points is clearly not a good thing, but I'm doing so we want to do something smarter than this."
        ],
        [
            "So the question is, how do you ensure privacy?",
            "And the answer we give here is that we will design algorithms that satisfy differential privacy."
        ],
        [
            "So differential privacy was a notion initially proposed by Doctor Max Init Simmons Smith in 2006.",
            "And then there was a follow up for blood work it out in 2006.",
            "So the high level intuition of differential privacy is that from the output of an algorithm, an adversary will not be able to recognize your presence or absence in the data set, or in other words, it will learn almost the same thing about you irrespective of your presence or absence in the data set.",
            "One crucial thing to remember is it's a restriction on the algorithm, not on the data.",
            "And the second thing is this notion gives good guarantees even in the presence of arbitrary side information.",
            "So the requirement of differential privacy is that for any pair of datasets, D&D prime which differ in one element.",
            "The distribution that the data set induces.",
            "On the space of outputs of the algorithm are closed.",
            "That is, so if you run the algorithm with D, then the distribution over the space of outputs is this red line red curve and the distribution with the prime is this blue curve and this two distributions are closed.",
            "I'm enclosing the sensor statistical distance.",
            "So the closeness is measured by two parameters, epsilon and Delta.",
            "So that I don't miss about this algorithm.",
            "So the closeness is measured by two parameters, epsilon, Delta.",
            "In this talk I will not say how exactly this epsilon, Delta player part.",
            "Only thing you just need to remember is if someone is something as small constant and Delta is something which depends one by Poly in the size of the data set."
        ],
        [
            "So there in the recent past there has been a lot of work on privacy and learning and it's just not possible to name all of them in this talk.",
            "Almost all of these results work in the low dimensional regime where the dimensionality of the problem is smaller than the number of samples you have.",
            "I mentioned 2 results in the context of model selection.",
            "Then there was a result baiza at all in 2009, which studied model selection.",
            "But the results were also in the regime where the dimension is smaller than the number of samples.",
            "Last year we had a result in Caldwell.",
            "We provided algorithms for sparse linear regression with sample complexity, which kills something like the sparsity times, the log of the dimensionality squared.",
            "And so this was the 1st result in the high dimensional regime.",
            "So in this talk what I'll do is I'll first generalize this result and also give title better guarantees compared to this result."
        ],
        [
            "So, So what our contributions are.",
            "We provide new algorithms.",
            "For new connections between stability and learning.",
            "More precisely, what you do is.",
            "We show that a stable non private model model selection algorithm implies a differentially private model selection algorithm and by stability we look at two notions of stability.",
            "One is this subsampling stability and another is a perturbation stability.",
            "Both of these notions have been studied extensively in the literature.",
            "One point I want to mention is that the algorithm based on subsampling stability for model selection is always computationally efficient, and in the rest of the talk when I say efficient, it's I mean computationally efficient, not statistical efficiency.",
            "So.",
            "This in the second part of our contribution.",
            "What we show is that the last two estimator is stable in the sense.",
            "The conditions which imply lasso to be consistent, those other conditions which are sufficient enough to show lasso is stable.",
            "Moreover, what issue is we provide efficient tests for the stability of LASSO, and essentially that allows us to give a private algorithm for sparse linear regression with optimal sample complexity.",
            "Since I will not have time to speak about all of them, so I'll be touching up on two of these things.",
            "First, I will talk about the perturbation stability and in the second part I will talk about how we design the test for stability of lasso."
        ],
        [
            "So.",
            "So I'll first state the results and then I will go into the exact algorithm.",
            "So what our results here is that.",
            "Our results work under deterministic conditions and also under stochastic assumptions, but in the talk I'll just be telling over the stochastic assumption.",
            "So what is the stochastic assumption?",
            "The underlying model parameter Theta star has the Infinity norm bounded by 1?",
            "The support of Theta studies at most is, that is, it has at most S nonzero entries.",
            "And other technical condition is that the non non zero and the smallest nonzero entry of Theta star.",
            "Is is a constant?",
            "And the sticker and the design matrix that is the each entry in the feature vector is drawn IID from normal 01 and the noise in the linear system is from normal zero Sigma squared.",
            "But I want to mention that we have deterministic assumptions in the paper.",
            "So what does known earlier so in our last year's work?",
            "But we showed that we could design algorithm for sparse linear regression, which has sample complexity of A squared log squared P times the privacy parameter.",
            "One way of selling.",
            "So in this talk, what we show is that we can get a sample complexity of sloppy, which by the way, is the optimal sample complexity for sparse linear regression while also and we show that there is a multiplicative dependence of log 1 by Delta on epsilon.",
            "Furthermore, we can show that via this perturbation stability and looking at the exact stability property of the last system later, we can get the sample complexity to S log P plus lower order terms, smaller smallest and the privacy parameter gets multiplied to the lower or items.",
            "So.",
            "Our results are optimal.",
            "In the sense that it matches the Nonprivate sample composition that is known to be optimal in that sense."
        ],
        [
            "So now I'll tell you about perturbation stability."
        ],
        [
            "So doesn't you're sensing matrix?",
            "I mean doesn't it have for IP?",
            "Yeah, but I will not test for IP.",
            "Will test for something slightly weaker than that first ability.",
            "See why wouldn't follow from?",
            "I mean, you know, privacy issue aside, the sample complexity within follow up tomorrow.",
            "Yeah, so I'm saying like with privacy get sloppy.",
            "So I'm just saying that the unknown private I just wanted to mention the non private is sloppy and we get sloppy even with privacy.",
            "So to talk about perturbation stability will define something called as a distance to instability framework.",
            "So consider a function for each Maps from the space of datasets to some arbitrary range.",
            "And we call this function to be castable if by K stable at upset D if by changing K entries in the data set D the output of the function does not change.",
            "So this is a definition of K stability and our objective is to output FD while preserving privacy.",
            "So to keep a mental picture in mind, you have the space of all datasets.",
            "This red region is a set of unstable datasets where changing one entry changes the output of FD and this D we want to output.",
            "Everybody lies like sufficiently far away from the boundary."
        ],
        [
            "So how does the high level algorithm work?",
            "You take the forgiven if Andy you find out the minimum number of entries you need to change.",
            "Indeed, to make the function unstable on that data set.",
            "So that's your disk.",
            "Then you add some doubly exponential noise, which is the Laplace distribution from the level of distribution whose standard deviation is constant.",
            "That is like 1 by epsilon, and you test if this noisy value is greater than log 1 by Delta.",
            "Excellent if it is then just output FD.",
            "Otherwise output about and fail."
        ],
        [
            "It was shown some variants of this algorithm has appeared earlier in the literature, and it was shown that this algorithm is epsilon Delta, differentially private, and I'll not get into the proof.",
            "Moreover, in terms of utility to assume that if.",
            "At a data set, F is 2 log 1 by Delta on epsilon stable.",
            "Then the algorithm outputs FT exactly.",
            "So that's important to know."
        ],
        [
            "So we might think that we had done by now because I've given you the.",
            "Best possibility and.",
            "And yeah, I'm giving notice for stability and we might plug in the last estimate, and we're done.",
            "But the issue is that.",
            "It is hard to compute the stability of the function efficiently.",
            "So one way to fix it is will find it proxy for the distance to instability.",
            "With the following properties, first the proxy should lower bound the distance to instability.",
            "And Moreover, it should have the safety property, meaning that for any data set the any pair of datasets, D&D prime changing one entry, the value of D does not change."
        ],
        [
            "So now it brings us to the.",
            "Test for perturbation stability.",
            "Of lasso, and I'll see that I'll tell you how it fits into this distance to instability framework."
        ],
        [
            "So we called the last Test emitter.",
            "It is everyone.",
            "It is squared loss plus.",
            "They learn regularization.",
            "But your series of work can also be served in one day 2006, and I've shown that under the stochastic assumption and with the proper choice of Lambda.",
            "If you allow your sample size to grow as a sloppy, then the support of the minimizer Theta hat is equal to the support of Theta star."
        ],
        [
            "But we show is the consistency assumptions for loss over sufficient to guarantee the stability of the minimizer Theta had by stability I mean the support of this minimizer.",
            "Theta Hat will not change if I change a few of the entries.",
            "And the proof at a high level goes.",
            "We're analyzing the Kitty condition of the minimizer at Pizza Hut, and we show that the support of the support of Theta hat.",
            "Snippet by constructing a dual certificate for nearby instances of the data set, and we show that the support does not change."
        ],
        [
            "So now I told you that I will test for this property so wanted to test for this property is while generating to proxy proxy conditions.",
            "First worked for something or less restrictive strong convexity.",
            "So what it says is if I look at the support of Theta hat, let's call the gamma is the support and gamma complement is other coordinates then will look will test for the strong convexity of the least squared loss restricted to the support of Theta hat?",
            "And one thing to note here is that we are only testing for strong convexity on this support.",
            "We are not testing for all possible choices of the support and this allows us to bypass this NP hardness for testing for our IP address in general.",
            "And the second thing we need to test is for something called a strong stability, meaning that on gamma gamma complement, the values in the gradient is sufficiently smaller than Lambda.",
            "So meaning changing a few entries should not basically move my support.",
            "And one thing to mention is that these conditions hold with high probability.",
            "In the stochastic setting."
        ],
        [
            "So I'll just give you an intuition of why such a thing works.",
            "So look at the least squared loss along the coordinates where the support is non zero.",
            "The minimizer cannot move because of strong convexity."
        ],
        [
            "On the complementary direction, you have element regularization, and this minimizer is sitting somewhere at 0.",
            "So to move from zero you have to give a slope of plus Lambda minus Lambda, so that the minimizer moves so the L1 regularization gives you stability in the complementary direction."
        ],
        [
            "I think I'll skip how this conditions.",
            "Essentially, we cannot describe the exact functions for it, but essentially describe two functions G1 and G2, and what we show is that.",
            "If you wanna do a large then we can privately test for G1 and G2.",
            "And this serves as a proxy distance to instabile."
        ],
        [
            "And now putting this pieces together will constructed function D hat, which is composed of this function G1 and G2, which is the proxy distances.",
            "And we'll add Laplace, one, webcal and noise and check if that is sufficiently large, if sufficiently large and I'll output the support of the minimizer otherwise allowed.",
            "Put about and I've already stated you the privacy guarantee and also the utility guarantees that if the hat is at least two, log on by Delta upon epsilon and then with high probability will output the minimizer support of the underlying parameter vector Theta star."
        ],
        [
            "Just to review the contribution, what I told you.",
            "Stable non private model selection algorithms imply private model selection algorithm and I give you a kind of a generic reduction.",
            "Then I told you about how to test for stability.",
            "For this specific problem of lasso and how it fits into the private sparse linear regression framework."
        ],
        [
            "In terms of future work, what you want to analyze this solasso is like one step solution.",
            "You just optimizing function and you are done.",
            "So there are other techniques like least angle regression which are more iterative.",
            "Can we test for privacy and stability properties of those algorithms?",
            "And finally we want to analyze other classes of problems like low rank matrix approximation and something like sparse representation with many good feature sparse vectors, and we want to do these things with privacy and that's it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi, I'm really stuck with her and this is a joint work with Adam Smith.",
                    "label": 1
                },
                {
                    "sent": "Was my PhD advisor from constricted?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the initial stage, what I'll do is I'll try to set up the learning problem and hyper how privacy plays a role in there.",
                    "label": 0
                },
                {
                    "sent": "So consider a data set which has bunch of individuals in it and each record in the data set corresponds to an individual.",
                    "label": 0
                },
                {
                    "sent": "And you can think this information about these individuals to be sensitive.",
                    "label": 0
                },
                {
                    "sent": "Something like a bunch of medical tests done on a person and a bit telling that whether he has diabetes or not and there is an algorithm sitting in front of it which is releasing summary statistics about this data set to the external world.",
                    "label": 0
                },
                {
                    "sent": "Now the status can be something like putting a classifier with.",
                    "label": 0
                },
                {
                    "sent": "Which wants to classify whether a person has diabetes or not.",
                    "label": 0
                },
                {
                    "sent": "Now clearly this kind of information is useful to the external world, but there might be an adversary who wants to use this information to breach into the privacy of one of the entries in the data set and our job in this talk is to protect the privacy of dissenters, while giving reasonably useful information.",
                    "label": 0
                },
                {
                    "sent": "So so clearly there is.",
                    "label": 0
                },
                {
                    "sent": "There are two conflicting goals.",
                    "label": 1
                },
                {
                    "sent": "One is that you want to release sufficiently useful information, but at the same time you do not want to give two accurate information which will.",
                    "label": 1
                },
                {
                    "sent": "Value The privacy of individuals.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is a difficult problem and there has been a series of attacks in the recent past on systems built under this setting.",
                    "label": 0
                },
                {
                    "sent": "And I'm just not name of name them right now and this is extremely activity of research in computer science or expansion across all of computer science right now.",
                    "label": 0
                },
                {
                    "sent": "So with this high level setting, let me tell you what exact problem we want to solve and how we want to address it privacy.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem we want to solve this model selection, so I assume that you are given a discrete collection of probability distribution families M1M2 and blah blah and your goal is to select a model which best explains the data set or which best fits the data set.",
                    "label": 1
                },
                {
                    "sent": "One example could be something like you want to do feature selection.",
                    "label": 0
                },
                {
                    "sent": "So in this talk, the concrete model selection example, which will be using through the talk.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is something called sparse linear regression so sparse linear regression?",
                    "label": 1
                },
                {
                    "sent": "Each data point is a couple where the first part of the people is the feature vector in RP and the 2nd is a respons why, which is in R. And the modeling assumption is that why is generated from X via linear transformation.",
                    "label": 0
                },
                {
                    "sent": "So you take X multiply with the parameter vector Theta star and add some noise to it and generate wife.",
                    "label": 0
                },
                {
                    "sent": "So in terms of sparsity, what we assume is that the model parameter which is generating Y from X is sparse, that is, it has few nonzero entries.",
                    "label": 0
                },
                {
                    "sent": "Let's say it has non 0 interest and specifically is smaller than in and Secondly you're interested in the high dimensional region where the dimensionality of the problem is much larger than the number of samples you have.",
                    "label": 0
                },
                {
                    "sent": "So the specific model selection problem is we want to recover the coordinates of Theta star or the support of Theta star which are nonzero.",
                    "label": 1
                },
                {
                    "sent": "Ugh.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This has been studied from I think 1996.",
                    "label": 0
                },
                {
                    "sent": "How to solve this problem and the one very famous estimator for this problem is lost emitter and what it says is that if you have a list squared loss plus correspond to properly scaled L1 regularization, then the support of the minimizer Theta hat is essentially the support of Theta star.",
                    "label": 0
                },
                {
                    "sent": "So this has been studied reasonably well in the statistique literature and we want to solve this problem with privacy.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is this paper all about?",
                    "label": 1
                },
                {
                    "sent": "It's about differentially private model selection will give generic techniques for doing model selection with privacy.",
                    "label": 1
                },
                {
                    "sent": "One specific example of this technique will be sparse model, sparse linear regression and in the second part of the top while designing this private algorithms for sparse linear regression will give the first stability analysis for lasso.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So white privacy is a concern, so I told you that I will design model selection algorithms with privacy.",
                    "label": 0
                },
                {
                    "sent": "But why is it a concern?",
                    "label": 1
                },
                {
                    "sent": "So most model selection problems can be thought of as like on this optimization problems, so the other examples of convex optimization is mean median support vector machines.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, let's pick one example from this.",
                    "label": 0
                },
                {
                    "sent": "Let's take support vector machine and see that I mean what are the privacy implications in there.",
                    "label": 0
                },
                {
                    "sent": "So if you think a standard output of a support vector machine, these are like the support vectors and support vectors.",
                    "label": 1
                },
                {
                    "sent": "Essentially some of the data points.",
                    "label": 0
                },
                {
                    "sent": "And the initial setting when I started, I told you that I want to preserve the privacy of the data points.",
                    "label": 0
                },
                {
                    "sent": "Now if I'm out putting the data points is clearly not a good thing, but I'm doing so we want to do something smarter than this.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the question is, how do you ensure privacy?",
                    "label": 0
                },
                {
                    "sent": "And the answer we give here is that we will design algorithms that satisfy differential privacy.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So differential privacy was a notion initially proposed by Doctor Max Init Simmons Smith in 2006.",
                    "label": 0
                },
                {
                    "sent": "And then there was a follow up for blood work it out in 2006.",
                    "label": 0
                },
                {
                    "sent": "So the high level intuition of differential privacy is that from the output of an algorithm, an adversary will not be able to recognize your presence or absence in the data set, or in other words, it will learn almost the same thing about you irrespective of your presence or absence in the data set.",
                    "label": 0
                },
                {
                    "sent": "One crucial thing to remember is it's a restriction on the algorithm, not on the data.",
                    "label": 1
                },
                {
                    "sent": "And the second thing is this notion gives good guarantees even in the presence of arbitrary side information.",
                    "label": 1
                },
                {
                    "sent": "So the requirement of differential privacy is that for any pair of datasets, D&D prime which differ in one element.",
                    "label": 0
                },
                {
                    "sent": "The distribution that the data set induces.",
                    "label": 0
                },
                {
                    "sent": "On the space of outputs of the algorithm are closed.",
                    "label": 1
                },
                {
                    "sent": "That is, so if you run the algorithm with D, then the distribution over the space of outputs is this red line red curve and the distribution with the prime is this blue curve and this two distributions are closed.",
                    "label": 0
                },
                {
                    "sent": "I'm enclosing the sensor statistical distance.",
                    "label": 0
                },
                {
                    "sent": "So the closeness is measured by two parameters, epsilon and Delta.",
                    "label": 0
                },
                {
                    "sent": "So that I don't miss about this algorithm.",
                    "label": 0
                },
                {
                    "sent": "So the closeness is measured by two parameters, epsilon, Delta.",
                    "label": 0
                },
                {
                    "sent": "In this talk I will not say how exactly this epsilon, Delta player part.",
                    "label": 0
                },
                {
                    "sent": "Only thing you just need to remember is if someone is something as small constant and Delta is something which depends one by Poly in the size of the data set.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there in the recent past there has been a lot of work on privacy and learning and it's just not possible to name all of them in this talk.",
                    "label": 1
                },
                {
                    "sent": "Almost all of these results work in the low dimensional regime where the dimensionality of the problem is smaller than the number of samples you have.",
                    "label": 0
                },
                {
                    "sent": "I mentioned 2 results in the context of model selection.",
                    "label": 1
                },
                {
                    "sent": "Then there was a result baiza at all in 2009, which studied model selection.",
                    "label": 0
                },
                {
                    "sent": "But the results were also in the regime where the dimension is smaller than the number of samples.",
                    "label": 0
                },
                {
                    "sent": "Last year we had a result in Caldwell.",
                    "label": 0
                },
                {
                    "sent": "We provided algorithms for sparse linear regression with sample complexity, which kills something like the sparsity times, the log of the dimensionality squared.",
                    "label": 0
                },
                {
                    "sent": "And so this was the 1st result in the high dimensional regime.",
                    "label": 0
                },
                {
                    "sent": "So in this talk what I'll do is I'll first generalize this result and also give title better guarantees compared to this result.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, So what our contributions are.",
                    "label": 1
                },
                {
                    "sent": "We provide new algorithms.",
                    "label": 0
                },
                {
                    "sent": "For new connections between stability and learning.",
                    "label": 1
                },
                {
                    "sent": "More precisely, what you do is.",
                    "label": 1
                },
                {
                    "sent": "We show that a stable non private model model selection algorithm implies a differentially private model selection algorithm and by stability we look at two notions of stability.",
                    "label": 1
                },
                {
                    "sent": "One is this subsampling stability and another is a perturbation stability.",
                    "label": 0
                },
                {
                    "sent": "Both of these notions have been studied extensively in the literature.",
                    "label": 0
                },
                {
                    "sent": "One point I want to mention is that the algorithm based on subsampling stability for model selection is always computationally efficient, and in the rest of the talk when I say efficient, it's I mean computationally efficient, not statistical efficiency.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This in the second part of our contribution.",
                    "label": 0
                },
                {
                    "sent": "What we show is that the last two estimator is stable in the sense.",
                    "label": 1
                },
                {
                    "sent": "The conditions which imply lasso to be consistent, those other conditions which are sufficient enough to show lasso is stable.",
                    "label": 0
                },
                {
                    "sent": "Moreover, what issue is we provide efficient tests for the stability of LASSO, and essentially that allows us to give a private algorithm for sparse linear regression with optimal sample complexity.",
                    "label": 0
                },
                {
                    "sent": "Since I will not have time to speak about all of them, so I'll be touching up on two of these things.",
                    "label": 0
                },
                {
                    "sent": "First, I will talk about the perturbation stability and in the second part I will talk about how we design the test for stability of lasso.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So I'll first state the results and then I will go into the exact algorithm.",
                    "label": 0
                },
                {
                    "sent": "So what our results here is that.",
                    "label": 1
                },
                {
                    "sent": "Our results work under deterministic conditions and also under stochastic assumptions, but in the talk I'll just be telling over the stochastic assumption.",
                    "label": 0
                },
                {
                    "sent": "So what is the stochastic assumption?",
                    "label": 0
                },
                {
                    "sent": "The underlying model parameter Theta star has the Infinity norm bounded by 1?",
                    "label": 0
                },
                {
                    "sent": "The support of Theta studies at most is, that is, it has at most S nonzero entries.",
                    "label": 0
                },
                {
                    "sent": "And other technical condition is that the non non zero and the smallest nonzero entry of Theta star.",
                    "label": 1
                },
                {
                    "sent": "Is is a constant?",
                    "label": 0
                },
                {
                    "sent": "And the sticker and the design matrix that is the each entry in the feature vector is drawn IID from normal 01 and the noise in the linear system is from normal zero Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "But I want to mention that we have deterministic assumptions in the paper.",
                    "label": 0
                },
                {
                    "sent": "So what does known earlier so in our last year's work?",
                    "label": 0
                },
                {
                    "sent": "But we showed that we could design algorithm for sparse linear regression, which has sample complexity of A squared log squared P times the privacy parameter.",
                    "label": 0
                },
                {
                    "sent": "One way of selling.",
                    "label": 0
                },
                {
                    "sent": "So in this talk, what we show is that we can get a sample complexity of sloppy, which by the way, is the optimal sample complexity for sparse linear regression while also and we show that there is a multiplicative dependence of log 1 by Delta on epsilon.",
                    "label": 1
                },
                {
                    "sent": "Furthermore, we can show that via this perturbation stability and looking at the exact stability property of the last system later, we can get the sample complexity to S log P plus lower order terms, smaller smallest and the privacy parameter gets multiplied to the lower or items.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Our results are optimal.",
                    "label": 0
                },
                {
                    "sent": "In the sense that it matches the Nonprivate sample composition that is known to be optimal in that sense.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now I'll tell you about perturbation stability.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So doesn't you're sensing matrix?",
                    "label": 0
                },
                {
                    "sent": "I mean doesn't it have for IP?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but I will not test for IP.",
                    "label": 0
                },
                {
                    "sent": "Will test for something slightly weaker than that first ability.",
                    "label": 0
                },
                {
                    "sent": "See why wouldn't follow from?",
                    "label": 0
                },
                {
                    "sent": "I mean, you know, privacy issue aside, the sample complexity within follow up tomorrow.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I'm saying like with privacy get sloppy.",
                    "label": 0
                },
                {
                    "sent": "So I'm just saying that the unknown private I just wanted to mention the non private is sloppy and we get sloppy even with privacy.",
                    "label": 0
                },
                {
                    "sent": "So to talk about perturbation stability will define something called as a distance to instability framework.",
                    "label": 1
                },
                {
                    "sent": "So consider a function for each Maps from the space of datasets to some arbitrary range.",
                    "label": 0
                },
                {
                    "sent": "And we call this function to be castable if by K stable at upset D if by changing K entries in the data set D the output of the function does not change.",
                    "label": 1
                },
                {
                    "sent": "So this is a definition of K stability and our objective is to output FD while preserving privacy.",
                    "label": 0
                },
                {
                    "sent": "So to keep a mental picture in mind, you have the space of all datasets.",
                    "label": 0
                },
                {
                    "sent": "This red region is a set of unstable datasets where changing one entry changes the output of FD and this D we want to output.",
                    "label": 0
                },
                {
                    "sent": "Everybody lies like sufficiently far away from the boundary.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how does the high level algorithm work?",
                    "label": 0
                },
                {
                    "sent": "You take the forgiven if Andy you find out the minimum number of entries you need to change.",
                    "label": 0
                },
                {
                    "sent": "Indeed, to make the function unstable on that data set.",
                    "label": 0
                },
                {
                    "sent": "So that's your disk.",
                    "label": 0
                },
                {
                    "sent": "Then you add some doubly exponential noise, which is the Laplace distribution from the level of distribution whose standard deviation is constant.",
                    "label": 0
                },
                {
                    "sent": "That is like 1 by epsilon, and you test if this noisy value is greater than log 1 by Delta.",
                    "label": 0
                },
                {
                    "sent": "Excellent if it is then just output FD.",
                    "label": 0
                },
                {
                    "sent": "Otherwise output about and fail.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It was shown some variants of this algorithm has appeared earlier in the literature, and it was shown that this algorithm is epsilon Delta, differentially private, and I'll not get into the proof.",
                    "label": 0
                },
                {
                    "sent": "Moreover, in terms of utility to assume that if.",
                    "label": 0
                },
                {
                    "sent": "At a data set, F is 2 log 1 by Delta on epsilon stable.",
                    "label": 1
                },
                {
                    "sent": "Then the algorithm outputs FT exactly.",
                    "label": 1
                },
                {
                    "sent": "So that's important to know.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we might think that we had done by now because I've given you the.",
                    "label": 0
                },
                {
                    "sent": "Best possibility and.",
                    "label": 0
                },
                {
                    "sent": "And yeah, I'm giving notice for stability and we might plug in the last estimate, and we're done.",
                    "label": 0
                },
                {
                    "sent": "But the issue is that.",
                    "label": 0
                },
                {
                    "sent": "It is hard to compute the stability of the function efficiently.",
                    "label": 0
                },
                {
                    "sent": "So one way to fix it is will find it proxy for the distance to instability.",
                    "label": 1
                },
                {
                    "sent": "With the following properties, first the proxy should lower bound the distance to instability.",
                    "label": 1
                },
                {
                    "sent": "And Moreover, it should have the safety property, meaning that for any data set the any pair of datasets, D&D prime changing one entry, the value of D does not change.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now it brings us to the.",
                    "label": 0
                },
                {
                    "sent": "Test for perturbation stability.",
                    "label": 0
                },
                {
                    "sent": "Of lasso, and I'll see that I'll tell you how it fits into this distance to instability framework.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we called the last Test emitter.",
                    "label": 0
                },
                {
                    "sent": "It is everyone.",
                    "label": 0
                },
                {
                    "sent": "It is squared loss plus.",
                    "label": 0
                },
                {
                    "sent": "They learn regularization.",
                    "label": 0
                },
                {
                    "sent": "But your series of work can also be served in one day 2006, and I've shown that under the stochastic assumption and with the proper choice of Lambda.",
                    "label": 1
                },
                {
                    "sent": "If you allow your sample size to grow as a sloppy, then the support of the minimizer Theta hat is equal to the support of Theta star.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But we show is the consistency assumptions for loss over sufficient to guarantee the stability of the minimizer Theta had by stability I mean the support of this minimizer.",
                    "label": 1
                },
                {
                    "sent": "Theta Hat will not change if I change a few of the entries.",
                    "label": 0
                },
                {
                    "sent": "And the proof at a high level goes.",
                    "label": 0
                },
                {
                    "sent": "We're analyzing the Kitty condition of the minimizer at Pizza Hut, and we show that the support of the support of Theta hat.",
                    "label": 0
                },
                {
                    "sent": "Snippet by constructing a dual certificate for nearby instances of the data set, and we show that the support does not change.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I told you that I will test for this property so wanted to test for this property is while generating to proxy proxy conditions.",
                    "label": 0
                },
                {
                    "sent": "First worked for something or less restrictive strong convexity.",
                    "label": 1
                },
                {
                    "sent": "So what it says is if I look at the support of Theta hat, let's call the gamma is the support and gamma complement is other coordinates then will look will test for the strong convexity of the least squared loss restricted to the support of Theta hat?",
                    "label": 1
                },
                {
                    "sent": "And one thing to note here is that we are only testing for strong convexity on this support.",
                    "label": 0
                },
                {
                    "sent": "We are not testing for all possible choices of the support and this allows us to bypass this NP hardness for testing for our IP address in general.",
                    "label": 1
                },
                {
                    "sent": "And the second thing we need to test is for something called a strong stability, meaning that on gamma gamma complement, the values in the gradient is sufficiently smaller than Lambda.",
                    "label": 0
                },
                {
                    "sent": "So meaning changing a few entries should not basically move my support.",
                    "label": 0
                },
                {
                    "sent": "And one thing to mention is that these conditions hold with high probability.",
                    "label": 0
                },
                {
                    "sent": "In the stochastic setting.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'll just give you an intuition of why such a thing works.",
                    "label": 0
                },
                {
                    "sent": "So look at the least squared loss along the coordinates where the support is non zero.",
                    "label": 0
                },
                {
                    "sent": "The minimizer cannot move because of strong convexity.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On the complementary direction, you have element regularization, and this minimizer is sitting somewhere at 0.",
                    "label": 0
                },
                {
                    "sent": "So to move from zero you have to give a slope of plus Lambda minus Lambda, so that the minimizer moves so the L1 regularization gives you stability in the complementary direction.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think I'll skip how this conditions.",
                    "label": 0
                },
                {
                    "sent": "Essentially, we cannot describe the exact functions for it, but essentially describe two functions G1 and G2, and what we show is that.",
                    "label": 0
                },
                {
                    "sent": "If you wanna do a large then we can privately test for G1 and G2.",
                    "label": 0
                },
                {
                    "sent": "And this serves as a proxy distance to instabile.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And now putting this pieces together will constructed function D hat, which is composed of this function G1 and G2, which is the proxy distances.",
                    "label": 0
                },
                {
                    "sent": "And we'll add Laplace, one, webcal and noise and check if that is sufficiently large, if sufficiently large and I'll output the support of the minimizer otherwise allowed.",
                    "label": 1
                },
                {
                    "sent": "Put about and I've already stated you the privacy guarantee and also the utility guarantees that if the hat is at least two, log on by Delta upon epsilon and then with high probability will output the minimizer support of the underlying parameter vector Theta star.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to review the contribution, what I told you.",
                    "label": 0
                },
                {
                    "sent": "Stable non private model selection algorithms imply private model selection algorithm and I give you a kind of a generic reduction.",
                    "label": 1
                },
                {
                    "sent": "Then I told you about how to test for stability.",
                    "label": 1
                },
                {
                    "sent": "For this specific problem of lasso and how it fits into the private sparse linear regression framework.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In terms of future work, what you want to analyze this solasso is like one step solution.",
                    "label": 0
                },
                {
                    "sent": "You just optimizing function and you are done.",
                    "label": 0
                },
                {
                    "sent": "So there are other techniques like least angle regression which are more iterative.",
                    "label": 1
                },
                {
                    "sent": "Can we test for privacy and stability properties of those algorithms?",
                    "label": 0
                },
                {
                    "sent": "And finally we want to analyze other classes of problems like low rank matrix approximation and something like sparse representation with many good feature sparse vectors, and we want to do these things with privacy and that's it.",
                    "label": 1
                }
            ]
        }
    }
}