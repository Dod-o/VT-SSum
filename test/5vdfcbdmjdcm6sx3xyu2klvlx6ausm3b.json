{
    "id": "5vdfcbdmjdcm6sx3xyu2klvlx6ausm3b",
    "title": "Training and Testing of Recommender Systems on Data Missing Not at Random",
    "info": {
        "author": [
            "Harald Steck, Bell Labs, Alcatel-Lucent"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Databases"
        ]
    },
    "url": "http://videolectures.net/kdd2010_steck_ttrs/",
    "segmentation": [
        [
            "So let's, uh, consider."
        ],
        [
            "The following real world problem.",
            "So from all items that I have some movies in my store, I want to pick a few items for each user and the goal is of course that each user should find the recommended items relevant relevant.",
            "I mean that they like it and in the Netflix data that means that they gave a five star rating or would give a five star rating to these items.",
            "So then to translate it into our data mining problem I have to 1st define a goal how to actually test that I have a good recommender system.",
            "If someone gives me a recommender system so I do the usual thing.",
            "Offline tests with historical rating data that's easy and cheap and I want to maximize the accuracy.",
            "And like Netflix competition, right?",
            "A lot of people have done optimization of the root mean squared error on the observed ratings, so that's probably the most popular accuracy measure in the literature in recent years.",
            "But also there's something about ranking measures like N DCG on observed ratings.",
            "Once we have defined our goal, then we can do the second approximation, which is we have some training data and we try to develop a good model for it and the good training objective function and we optimize it and hopefully get a good approximation to our.",
            "Performance measures on the test data, so given that in the Netflix competition the goal was fixed with the root mean squared error on the observed ratings, a lot of work and very impressive work has been done on this second approximation down here to develop a lot of models and objective functions and so."
        ],
        [
            "This talk is now about the other approximation.",
            "How could actually is this root mean squared error on the observed ratings in the first place?",
            "OK, to a quick."
        ],
        [
            "Review so the data looks like the following, so we have a rating matrix given with users and items for the two dimensions.",
            "And conceptually we can sync that we have unknown complete rating matrix.",
            "And we obv."
        ],
        [
            "If only a small fraction of the ratings for Netflix data, it is 1% of the data and."
        ],
        [
            "There's a missing data mechanism that determines actually what we observe, and in general it's well known that we cannot ignore that missing data mechanism, but there's one nice special case which is missing at random.",
            "And that case is true if a rating value has no effect on the probability that it is actually missing in the observed data, and if that's the case, then the correct results are obtained by ignoring the missing ratings.",
            "So you can only work on the observed ones.",
            "Like in the Netflix competition for example."
        ],
        [
            "OK, so now Yahoo did.",
            "Actually a nice experiment.",
            "They collected data to see if this is actually true, that the data is missing at random.",
            "It has been published by Marlin last year and so you could give so users could give five star rating.",
            "So between one and five to the songs in Yahoo launch cut cost data, 5 is the best and one is that they don't like it and they collected the data in two different ways.",
            "The first one is that they did a survey so.",
            "Yahoo randomly created a list for every user and ask them to rate the songs, and that's the distribution we get here on the left so we can see.",
            "Most people don't like most of the songs and that's I think a good approximation to the distribution we would get from this unknown complete data.",
            "And then they also collected the data in the usual way where they gave the users the freedom to choose what they actually want to rate, and we can clearly see that this distribution looks very different.",
            "So the low ratings have now a much lower probability.",
            "Obviously this data is missing not at random, and what we can see is instead of giving low ratings, users tend to not give a rating at all."
        ],
        [
            "OK, so now that we see that it's actually not missing at random.",
            "The question is now in the real world, we actually want to pick from all the items a few, but now we actually only test on the observed ratings, right?",
            "So that's only guaranteed to be correct if the data is really missing at random, which is not true.",
            "So that motivates.",
            "Now actually do a different test that we actually test on all the items.",
            "So we have observed ratings and also missing ratings and so on.",
            "The question is now what measure can be used there?",
            "And a very simple measure is the top K hit rate that we can use for that.",
            "So let's."
        ],
        [
            "We look at the top gate rate on all the items so you can define it now as recall or as precision recall means.",
            "Now that we divide the number of relevant items in the top K. So the number of items the user likes in the top K divided by all the items that the user likes and precision is the number of items the user likes in the top K / K. And what I do is now IP."
        ],
        [
            "At the top, K to be the recall, not the precision.",
            "For two reasons.",
            "One reason is when I want to compare different recommender systems on fixed data.",
            "And on a fixed key, right?",
            "Let's say top 10.",
            "Then recall is actually proportional to precision.",
            "Hot because downstairs these two numbers are actually fixed, so this is constant for constant data and this is constant for constant K. Upstairs we have the same number.",
            "In both cases.",
            "The second important property is that under my assumption.",
            "We can calculate the recall from the mnar data and it actually provides us an unbiased estimate for the recall.",
            "We would have computed from this unknown complete data and this is important because this unknown complete data refers to all the items, which is the original real world problem that we want to pick from all items and it actually gives us an idea about user experience, right?",
            "So that's the user experience is related to the complete data and not to the.",
            "I mean I mnar data and the assumption for this to hold is actually pretty simple.",
            "I assume that we observe a random subset of the relevant ratings which is in this case the Five Star rating.",
            "So I assume for this little fraction that it's missing at random and for the other rating values that can be anything going on missing, not at random whatever.",
            "I don't care, and if that's the case, then what happens is non recall.",
            "Basically we just observed in our random fraction upstairs and downstairs, and because it's the same fraction.",
            "The ratio is the same for both datasets, OK?"
        ],
        [
            "I'm not op.",
            "Kate rate obviously depends on K, which we have to pick right top five, top 10 or whatever and it also ignores the ranking of the items.",
            "So here this graph depicts how what the top K looks like, possibly as a function of K. And here I have normalized K to range from zero to 1 instead of from one to N where N is the number of items.",
            "And as we have basically a longer list instead of top five, we have top ten.",
            "Of course chances are.",
            "Higher that relevant items are in that list and that's why this curve increases with K. And."
        ],
        [
            "Now we can also define a different measure, which is just the area under that curve.",
            "So I called it the area under the top K curve and this is obviously independent K. It's the numbers between zero and one larger is better and.",
            "It captures now the ranking of all the items and it agrees with the area under the arosi curve that you might be familiar with in the leading order.",
            "If the number of relevant items is much smaller than all the items.",
            "And again, it's an unbiased estimate that we can calculate from the mnar data like the top gate rate.",
            "So that's now."
        ],
        [
            "Anne.",
            "OK, now that I'm motivated to use actually a test performance measure on all the items, right like top K or a top.",
            "If you don't want to specify OK Now let's quickly see how it can be actually efficiently train a model to get some good results here."
        ],
        [
            "I use a very simple approach here.",
            "Matrix factorization model with two low rank matrices P&Q.",
            "And and offset are M and I use an experiments later than the latent space dimension is 50, so the rank 50."
        ],
        [
            "OK, now computational efficiency is really important for recommender systems, so that's why I just use the familiar least squares objective function.",
            "But I make small but important change to it.",
            "As you remember, we in the real world problem we have to pick from all items are few that we want to recommend.",
            "So that's why now I actually have a summation here over all the users and then all the items.",
            "So this is a big difference.",
            "Now to the approaches in Netflix competition where you had just to sum over the items within.",
            "Absoft rating.",
            "If I sum over all items now I have the problem that this squared error term here, where these are the ratings.",
            "This is basically undefined for the missing ratings obviously.",
            "So what I do is I impute value little RM for all the missing values, which makes it well defined and now the problem is that it's a very imbalanced data set.",
            "Becausw in Netflix 99% of the data is missing which now dominates this some.",
            "So I also put some.",
            "Wait here, which is 1 for the observed ratings and it is a small little number.",
            "Little WM for the missing ones.",
            "So that way I can make it more balanced if I want to.",
            "And then the third part is this usual regularization term, the L2 norm and a half a number Lambda here.",
            "To determine how big this regularization should be so the nice thing is that this equation now, even though it's now over all the items.",
            "So it's basically now.",
            "About 100 times more items that I have to sum over but with alternating least squares I can do it almost as efficiently as if I just had a sum over the observed items.",
            "Observed ratings here, so that's nice.",
            "And now let's look at these tuning parameters."
        ],
        [
            "So the imputed value rating value RM the wait for the missing ones and also Lambda for the regularization.",
            "So concerning the imputed rating value, what I find is now for the Netflix data, so these are now the test results on the health out probe set.",
            "So the ratings range from one to five and what I see is now the optimal value is actually about two, which makes sense because this is kind of a small value, much smaller than the observed mean, which is 3.6 and but it's also not a smallest one, so in some sense one can interpret it maybe as the mean of the missing ratings."
        ],
        [
            "The more interesting is now actually the way that we have to choose for the missing ratings, and so the maximum we can choose this one, which means that the missing end up soft ratings all have the same weight of 1, which is a standard SVT then which has been used successfully in latent semantic analysis for over 10 years now for text analysis.",
            "And we get some measure here for the area under the top curve .91 something like that.",
            "That's the green line.",
            "So now basically as you make the data set more balanced by reducing the weight for the missing ones, you get better, better results and we get an optimum at value of 0.005, which looks almost like 0.",
            "But remember that the fraction of observed ratings in the Netflix data is 0.01, so this means now that all the missing ratings together have half the weight.",
            "Compared to all the observed ratings, so it's about balanced training set.",
            "Now another dramatic thing is that as we go now to a weight of zero, it drops off a Cliff here and actually in ends.",
            "It ends up here below .9, which is basically the lowest score that I got here for any weight.",
            "And that's basically when you ignore the missing ratings.",
            "That's kind of the Netflix competition."
        ],
        [
            "So eventually we actually interested in the top K hit rate rather than the area under it, and so he is actually the curve for different values of K. Again, so normalized from zero to 1 instead of 1 to the number of items, which is 17,000 for Netflix.",
            "So the red curve is now the one that I get by accounting for the missing ratings, and Dash 1 is when I ignore the missing ratings.",
            "So that's the weight of zero, and in reality we are interested only in very.",
            "Small case right top five."
        ],
        [
            "Top 10 so let's zoom in to the front and what we can see is still the red curve is way above the dashed one.",
            "And more Interestingly, is."
        ],
        [
            "There is a squeeze out now more out of this root mean squared error, so this test curve actually as a root mean squared error of .9 two at standard matrix factorization model.",
            "The Red one is really bad with one point 1, but if we go to a really good model with .887 which was reported in Yehuda Koren's paper in 08, the integrated model trained to optimize our MSE.",
            "He also reported results concerning the topic hit rate, which are basically these three dots.",
            "So there's one here.",
            "One here and one here.",
            "You can see that you get a little bit of an improvement over the dashed line, but you're way below the red line.",
            "So in fact here you get 39% improvement and here you get a 50% improvement over the integrated model."
        ],
        [
            "Which shows that accounting for the missing ratings is really important in practice to optimize the.",
            "The top K hit rate.",
            "And."
        ],
        [
            "So that's also some related work.",
            "Most work in recent years has been, of course about the explicit feedback data ratings and Netflix competition.",
            "And as a just showed.",
            "So in the hood occurrence paper, he showed that improving the MSE on the observed data.",
            "It also increases the top K hit rate on all items.",
            "But as we've just seen now, only up to some limit, right?",
            "And it also has been noticed that ratings are missing.",
            "Not at random in different contexts, so different models have been proposed like conditional RBM and SVD.",
            "One and two SVD plus plus.",
            "Account for what has been rated, but in some sense this model actually more tend to focus on then ignoring the not rated items rather than now.",
            "Sort of including the mini analysis.",
            "Most important, in this context, I think is the work of Marlin.",
            "He pointed out that this ratings are not missing at random and what he did, what is different to my approach is that he tested on the sort of complete data which is the survey data.",
            "If you remember from the beginning and he trained a multinomial mixture model on the mnar data to take account of that, which is conceptually very nice, but unfortunately computationally doesn't really scale up to the.",
            "Large datasets that we have.",
            "There's also some work in the implicit feedback data community about Clickstream data, TV consumption tags, bookmarks, purchases, and that is actually much more similar to my work than the one in the explicit feedback data.",
            "As I found out after the fact and what they do is they trained also matrix factorization model with weighted least squares, objective function so very similar to mine.",
            "The difference was that they had now of course binary data.",
            "Because they only had a yes no.",
            "If someone clicked and they didn't have an oh right, they only observe the positives.",
            "And so if you only have positives, you have to somehow train your model value.",
            "Assume that all the other ones are missing.",
            "The missing ones are all the negatives, right?",
            "And so that's how they.",
            "Then impute the missing ones and, but at the same time they pointed out that the difference to the explicit feedback data is that in the latter case you actually have positive and negative observations, so you don't have to do anything like that.",
            "But now that we've seen is actually that both cases are very similar and in both cases the negatives are underrepresented or missing completely so.",
            "It's actually pretty similar in the end."
        ],
        [
            "So conclusion, so I considered explicit feedback data missing not at random, and hopefully now I convinced you that it is really important to define a very good test performance measure that is very close to your real world problem.",
            "It's very nice if it's unbiased on the mnar data under.",
            "Hopefully my assumptions because then it also tells you something about the user experience you can expect on the unknown complete data, and I showed that the area on the top K hit rate curve and also top the hit rate itself is a good measure for that.",
            "And I also showed a computationally efficient approach and objective function for training are very simple model.",
            "I call it all rank because it's concerned with all items and what we've seen is accounting for the missing ratings is really essential to get and we get at the same time a very large improvement.",
            "And in the top gate rate.",
            "So future work is of course to come up with maybe better test performance measures.",
            "And then also, of course, maybe to have better training, objective, objective functions and develop better models for that.",
            "And also one thing that I want to point out is that we have obtained a lot of interesting results were concerning the root mean squared error.",
            "But all these results may not necessarily carry over.",
            "Now to some other performance measures like the top K hit rate, because we've seen that they are actually very different, right?",
            "A model that is equipped with RMS is not good with top hit rate and so one thing is, for example, maybe to reexamine?",
            "Collaborative filtering versus content based methods.",
            "We are at the moment it seems collaborative filtering is much better, but maybe it's only pharmacy and not bother me.",
            "OK, thank you.",
            "Yes, I like this.",
            "To optimize the default rate rating and the weight based on Building 1 particular model right and then you found the way that gave you the highest score.",
            "And presumably you would use that way in the same competition or practice, but it's part of getting these weights after rolling papers for model with his own biases.",
            "What do you mean now that it is funny to get this way?",
            "Sorry, so hopefully you could hear me doing the talk.",
            "So what you mean?",
            "It is funny that I get this weight.",
            "1st and then.",
            "Wait, yes I have some right right?",
            "Before we make any models and then we.",
            "That is true.",
            "I mean, you can think about the objective function without the model, and then once you have the model, then basically this just a tuning parameter like the Lambda for the regularization.",
            "It's just one more tuning parameter and you do cross validation to optimize it normal approach.",
            "Your tuning in general, which is odd.",
            "So I determine Lambda right and which is also tuned by everyone.",
            "I mean, that's also part of the objective function, so it's no different.",
            "Sizing function is fixed.",
            "The top case.",
            "So OK, so basically I have.",
            "If I use a fixed objective function I can optimize it.",
            "Then I see what I get right and then basically what I do is then I do a cross validation with my test data to optimize the tuning parameters.",
            "Now I don't have only lambdas attuning prior to path.",
            "Also have the weight and the imputed rating value so so and I determined based on the area under the top kicker on cross validation.",
            "So just the usual procedure.",
            "OK so we need to move to the next speaker.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's, uh, consider.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The following real world problem.",
                    "label": 0
                },
                {
                    "sent": "So from all items that I have some movies in my store, I want to pick a few items for each user and the goal is of course that each user should find the recommended items relevant relevant.",
                    "label": 1
                },
                {
                    "sent": "I mean that they like it and in the Netflix data that means that they gave a five star rating or would give a five star rating to these items.",
                    "label": 1
                },
                {
                    "sent": "So then to translate it into our data mining problem I have to 1st define a goal how to actually test that I have a good recommender system.",
                    "label": 0
                },
                {
                    "sent": "If someone gives me a recommender system so I do the usual thing.",
                    "label": 1
                },
                {
                    "sent": "Offline tests with historical rating data that's easy and cheap and I want to maximize the accuracy.",
                    "label": 0
                },
                {
                    "sent": "And like Netflix competition, right?",
                    "label": 1
                },
                {
                    "sent": "A lot of people have done optimization of the root mean squared error on the observed ratings, so that's probably the most popular accuracy measure in the literature in recent years.",
                    "label": 0
                },
                {
                    "sent": "But also there's something about ranking measures like N DCG on observed ratings.",
                    "label": 0
                },
                {
                    "sent": "Once we have defined our goal, then we can do the second approximation, which is we have some training data and we try to develop a good model for it and the good training objective function and we optimize it and hopefully get a good approximation to our.",
                    "label": 0
                },
                {
                    "sent": "Performance measures on the test data, so given that in the Netflix competition the goal was fixed with the root mean squared error on the observed ratings, a lot of work and very impressive work has been done on this second approximation down here to develop a lot of models and objective functions and so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This talk is now about the other approximation.",
                    "label": 0
                },
                {
                    "sent": "How could actually is this root mean squared error on the observed ratings in the first place?",
                    "label": 0
                },
                {
                    "sent": "OK, to a quick.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Review so the data looks like the following, so we have a rating matrix given with users and items for the two dimensions.",
                    "label": 0
                },
                {
                    "sent": "And conceptually we can sync that we have unknown complete rating matrix.",
                    "label": 1
                },
                {
                    "sent": "And we obv.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If only a small fraction of the ratings for Netflix data, it is 1% of the data and.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a missing data mechanism that determines actually what we observe, and in general it's well known that we cannot ignore that missing data mechanism, but there's one nice special case which is missing at random.",
                    "label": 0
                },
                {
                    "sent": "And that case is true if a rating value has no effect on the probability that it is actually missing in the observed data, and if that's the case, then the correct results are obtained by ignoring the missing ratings.",
                    "label": 1
                },
                {
                    "sent": "So you can only work on the observed ones.",
                    "label": 0
                },
                {
                    "sent": "Like in the Netflix competition for example.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now Yahoo did.",
                    "label": 0
                },
                {
                    "sent": "Actually a nice experiment.",
                    "label": 0
                },
                {
                    "sent": "They collected data to see if this is actually true, that the data is missing at random.",
                    "label": 0
                },
                {
                    "sent": "It has been published by Marlin last year and so you could give so users could give five star rating.",
                    "label": 0
                },
                {
                    "sent": "So between one and five to the songs in Yahoo launch cut cost data, 5 is the best and one is that they don't like it and they collected the data in two different ways.",
                    "label": 0
                },
                {
                    "sent": "The first one is that they did a survey so.",
                    "label": 0
                },
                {
                    "sent": "Yahoo randomly created a list for every user and ask them to rate the songs, and that's the distribution we get here on the left so we can see.",
                    "label": 0
                },
                {
                    "sent": "Most people don't like most of the songs and that's I think a good approximation to the distribution we would get from this unknown complete data.",
                    "label": 0
                },
                {
                    "sent": "And then they also collected the data in the usual way where they gave the users the freedom to choose what they actually want to rate, and we can clearly see that this distribution looks very different.",
                    "label": 0
                },
                {
                    "sent": "So the low ratings have now a much lower probability.",
                    "label": 0
                },
                {
                    "sent": "Obviously this data is missing not at random, and what we can see is instead of giving low ratings, users tend to not give a rating at all.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now that we see that it's actually not missing at random.",
                    "label": 0
                },
                {
                    "sent": "The question is now in the real world, we actually want to pick from all the items a few, but now we actually only test on the observed ratings, right?",
                    "label": 0
                },
                {
                    "sent": "So that's only guaranteed to be correct if the data is really missing at random, which is not true.",
                    "label": 0
                },
                {
                    "sent": "So that motivates.",
                    "label": 0
                },
                {
                    "sent": "Now actually do a different test that we actually test on all the items.",
                    "label": 0
                },
                {
                    "sent": "So we have observed ratings and also missing ratings and so on.",
                    "label": 0
                },
                {
                    "sent": "The question is now what measure can be used there?",
                    "label": 0
                },
                {
                    "sent": "And a very simple measure is the top K hit rate that we can use for that.",
                    "label": 0
                },
                {
                    "sent": "So let's.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We look at the top gate rate on all the items so you can define it now as recall or as precision recall means.",
                    "label": 0
                },
                {
                    "sent": "Now that we divide the number of relevant items in the top K. So the number of items the user likes in the top K divided by all the items that the user likes and precision is the number of items the user likes in the top K / K. And what I do is now IP.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At the top, K to be the recall, not the precision.",
                    "label": 0
                },
                {
                    "sent": "For two reasons.",
                    "label": 0
                },
                {
                    "sent": "One reason is when I want to compare different recommender systems on fixed data.",
                    "label": 1
                },
                {
                    "sent": "And on a fixed key, right?",
                    "label": 0
                },
                {
                    "sent": "Let's say top 10.",
                    "label": 0
                },
                {
                    "sent": "Then recall is actually proportional to precision.",
                    "label": 0
                },
                {
                    "sent": "Hot because downstairs these two numbers are actually fixed, so this is constant for constant data and this is constant for constant K. Upstairs we have the same number.",
                    "label": 0
                },
                {
                    "sent": "In both cases.",
                    "label": 0
                },
                {
                    "sent": "The second important property is that under my assumption.",
                    "label": 1
                },
                {
                    "sent": "We can calculate the recall from the mnar data and it actually provides us an unbiased estimate for the recall.",
                    "label": 0
                },
                {
                    "sent": "We would have computed from this unknown complete data and this is important because this unknown complete data refers to all the items, which is the original real world problem that we want to pick from all items and it actually gives us an idea about user experience, right?",
                    "label": 0
                },
                {
                    "sent": "So that's the user experience is related to the complete data and not to the.",
                    "label": 1
                },
                {
                    "sent": "I mean I mnar data and the assumption for this to hold is actually pretty simple.",
                    "label": 1
                },
                {
                    "sent": "I assume that we observe a random subset of the relevant ratings which is in this case the Five Star rating.",
                    "label": 0
                },
                {
                    "sent": "So I assume for this little fraction that it's missing at random and for the other rating values that can be anything going on missing, not at random whatever.",
                    "label": 0
                },
                {
                    "sent": "I don't care, and if that's the case, then what happens is non recall.",
                    "label": 0
                },
                {
                    "sent": "Basically we just observed in our random fraction upstairs and downstairs, and because it's the same fraction.",
                    "label": 0
                },
                {
                    "sent": "The ratio is the same for both datasets, OK?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm not op.",
                    "label": 0
                },
                {
                    "sent": "Kate rate obviously depends on K, which we have to pick right top five, top 10 or whatever and it also ignores the ranking of the items.",
                    "label": 1
                },
                {
                    "sent": "So here this graph depicts how what the top K looks like, possibly as a function of K. And here I have normalized K to range from zero to 1 instead of from one to N where N is the number of items.",
                    "label": 0
                },
                {
                    "sent": "And as we have basically a longer list instead of top five, we have top ten.",
                    "label": 0
                },
                {
                    "sent": "Of course chances are.",
                    "label": 0
                },
                {
                    "sent": "Higher that relevant items are in that list and that's why this curve increases with K. And.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we can also define a different measure, which is just the area under that curve.",
                    "label": 0
                },
                {
                    "sent": "So I called it the area under the top K curve and this is obviously independent K. It's the numbers between zero and one larger is better and.",
                    "label": 1
                },
                {
                    "sent": "It captures now the ranking of all the items and it agrees with the area under the arosi curve that you might be familiar with in the leading order.",
                    "label": 1
                },
                {
                    "sent": "If the number of relevant items is much smaller than all the items.",
                    "label": 0
                },
                {
                    "sent": "And again, it's an unbiased estimate that we can calculate from the mnar data like the top gate rate.",
                    "label": 0
                },
                {
                    "sent": "So that's now.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "OK, now that I'm motivated to use actually a test performance measure on all the items, right like top K or a top.",
                    "label": 0
                },
                {
                    "sent": "If you don't want to specify OK Now let's quickly see how it can be actually efficiently train a model to get some good results here.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I use a very simple approach here.",
                    "label": 0
                },
                {
                    "sent": "Matrix factorization model with two low rank matrices P&Q.",
                    "label": 1
                },
                {
                    "sent": "And and offset are M and I use an experiments later than the latent space dimension is 50, so the rank 50.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now computational efficiency is really important for recommender systems, so that's why I just use the familiar least squares objective function.",
                    "label": 1
                },
                {
                    "sent": "But I make small but important change to it.",
                    "label": 0
                },
                {
                    "sent": "As you remember, we in the real world problem we have to pick from all items are few that we want to recommend.",
                    "label": 0
                },
                {
                    "sent": "So that's why now I actually have a summation here over all the users and then all the items.",
                    "label": 0
                },
                {
                    "sent": "So this is a big difference.",
                    "label": 0
                },
                {
                    "sent": "Now to the approaches in Netflix competition where you had just to sum over the items within.",
                    "label": 0
                },
                {
                    "sent": "Absoft rating.",
                    "label": 0
                },
                {
                    "sent": "If I sum over all items now I have the problem that this squared error term here, where these are the ratings.",
                    "label": 1
                },
                {
                    "sent": "This is basically undefined for the missing ratings obviously.",
                    "label": 0
                },
                {
                    "sent": "So what I do is I impute value little RM for all the missing values, which makes it well defined and now the problem is that it's a very imbalanced data set.",
                    "label": 0
                },
                {
                    "sent": "Becausw in Netflix 99% of the data is missing which now dominates this some.",
                    "label": 0
                },
                {
                    "sent": "So I also put some.",
                    "label": 0
                },
                {
                    "sent": "Wait here, which is 1 for the observed ratings and it is a small little number.",
                    "label": 0
                },
                {
                    "sent": "Little WM for the missing ones.",
                    "label": 0
                },
                {
                    "sent": "So that way I can make it more balanced if I want to.",
                    "label": 0
                },
                {
                    "sent": "And then the third part is this usual regularization term, the L2 norm and a half a number Lambda here.",
                    "label": 0
                },
                {
                    "sent": "To determine how big this regularization should be so the nice thing is that this equation now, even though it's now over all the items.",
                    "label": 0
                },
                {
                    "sent": "So it's basically now.",
                    "label": 1
                },
                {
                    "sent": "About 100 times more items that I have to sum over but with alternating least squares I can do it almost as efficiently as if I just had a sum over the observed items.",
                    "label": 1
                },
                {
                    "sent": "Observed ratings here, so that's nice.",
                    "label": 0
                },
                {
                    "sent": "And now let's look at these tuning parameters.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the imputed value rating value RM the wait for the missing ones and also Lambda for the regularization.",
                    "label": 1
                },
                {
                    "sent": "So concerning the imputed rating value, what I find is now for the Netflix data, so these are now the test results on the health out probe set.",
                    "label": 1
                },
                {
                    "sent": "So the ratings range from one to five and what I see is now the optimal value is actually about two, which makes sense because this is kind of a small value, much smaller than the observed mean, which is 3.6 and but it's also not a smallest one, so in some sense one can interpret it maybe as the mean of the missing ratings.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The more interesting is now actually the way that we have to choose for the missing ratings, and so the maximum we can choose this one, which means that the missing end up soft ratings all have the same weight of 1, which is a standard SVT then which has been used successfully in latent semantic analysis for over 10 years now for text analysis.",
                    "label": 1
                },
                {
                    "sent": "And we get some measure here for the area under the top curve .91 something like that.",
                    "label": 0
                },
                {
                    "sent": "That's the green line.",
                    "label": 0
                },
                {
                    "sent": "So now basically as you make the data set more balanced by reducing the weight for the missing ones, you get better, better results and we get an optimum at value of 0.005, which looks almost like 0.",
                    "label": 0
                },
                {
                    "sent": "But remember that the fraction of observed ratings in the Netflix data is 0.01, so this means now that all the missing ratings together have half the weight.",
                    "label": 1
                },
                {
                    "sent": "Compared to all the observed ratings, so it's about balanced training set.",
                    "label": 0
                },
                {
                    "sent": "Now another dramatic thing is that as we go now to a weight of zero, it drops off a Cliff here and actually in ends.",
                    "label": 0
                },
                {
                    "sent": "It ends up here below .9, which is basically the lowest score that I got here for any weight.",
                    "label": 0
                },
                {
                    "sent": "And that's basically when you ignore the missing ratings.",
                    "label": 0
                },
                {
                    "sent": "That's kind of the Netflix competition.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So eventually we actually interested in the top K hit rate rather than the area under it, and so he is actually the curve for different values of K. Again, so normalized from zero to 1 instead of 1 to the number of items, which is 17,000 for Netflix.",
                    "label": 0
                },
                {
                    "sent": "So the red curve is now the one that I get by accounting for the missing ratings, and Dash 1 is when I ignore the missing ratings.",
                    "label": 0
                },
                {
                    "sent": "So that's the weight of zero, and in reality we are interested only in very.",
                    "label": 0
                },
                {
                    "sent": "Small case right top five.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Top 10 so let's zoom in to the front and what we can see is still the red curve is way above the dashed one.",
                    "label": 0
                },
                {
                    "sent": "And more Interestingly, is.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is a squeeze out now more out of this root mean squared error, so this test curve actually as a root mean squared error of .9 two at standard matrix factorization model.",
                    "label": 0
                },
                {
                    "sent": "The Red one is really bad with one point 1, but if we go to a really good model with .887 which was reported in Yehuda Koren's paper in 08, the integrated model trained to optimize our MSE.",
                    "label": 1
                },
                {
                    "sent": "He also reported results concerning the topic hit rate, which are basically these three dots.",
                    "label": 0
                },
                {
                    "sent": "So there's one here.",
                    "label": 0
                },
                {
                    "sent": "One here and one here.",
                    "label": 0
                },
                {
                    "sent": "You can see that you get a little bit of an improvement over the dashed line, but you're way below the red line.",
                    "label": 1
                },
                {
                    "sent": "So in fact here you get 39% improvement and here you get a 50% improvement over the integrated model.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which shows that accounting for the missing ratings is really important in practice to optimize the.",
                    "label": 0
                },
                {
                    "sent": "The top K hit rate.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's also some related work.",
                    "label": 0
                },
                {
                    "sent": "Most work in recent years has been, of course about the explicit feedback data ratings and Netflix competition.",
                    "label": 0
                },
                {
                    "sent": "And as a just showed.",
                    "label": 0
                },
                {
                    "sent": "So in the hood occurrence paper, he showed that improving the MSE on the observed data.",
                    "label": 0
                },
                {
                    "sent": "It also increases the top K hit rate on all items.",
                    "label": 1
                },
                {
                    "sent": "But as we've just seen now, only up to some limit, right?",
                    "label": 0
                },
                {
                    "sent": "And it also has been noticed that ratings are missing.",
                    "label": 1
                },
                {
                    "sent": "Not at random in different contexts, so different models have been proposed like conditional RBM and SVD.",
                    "label": 0
                },
                {
                    "sent": "One and two SVD plus plus.",
                    "label": 0
                },
                {
                    "sent": "Account for what has been rated, but in some sense this model actually more tend to focus on then ignoring the not rated items rather than now.",
                    "label": 0
                },
                {
                    "sent": "Sort of including the mini analysis.",
                    "label": 0
                },
                {
                    "sent": "Most important, in this context, I think is the work of Marlin.",
                    "label": 0
                },
                {
                    "sent": "He pointed out that this ratings are not missing at random and what he did, what is different to my approach is that he tested on the sort of complete data which is the survey data.",
                    "label": 0
                },
                {
                    "sent": "If you remember from the beginning and he trained a multinomial mixture model on the mnar data to take account of that, which is conceptually very nice, but unfortunately computationally doesn't really scale up to the.",
                    "label": 0
                },
                {
                    "sent": "Large datasets that we have.",
                    "label": 0
                },
                {
                    "sent": "There's also some work in the implicit feedback data community about Clickstream data, TV consumption tags, bookmarks, purchases, and that is actually much more similar to my work than the one in the explicit feedback data.",
                    "label": 1
                },
                {
                    "sent": "As I found out after the fact and what they do is they trained also matrix factorization model with weighted least squares, objective function so very similar to mine.",
                    "label": 0
                },
                {
                    "sent": "The difference was that they had now of course binary data.",
                    "label": 0
                },
                {
                    "sent": "Because they only had a yes no.",
                    "label": 0
                },
                {
                    "sent": "If someone clicked and they didn't have an oh right, they only observe the positives.",
                    "label": 0
                },
                {
                    "sent": "And so if you only have positives, you have to somehow train your model value.",
                    "label": 0
                },
                {
                    "sent": "Assume that all the other ones are missing.",
                    "label": 0
                },
                {
                    "sent": "The missing ones are all the negatives, right?",
                    "label": 0
                },
                {
                    "sent": "And so that's how they.",
                    "label": 1
                },
                {
                    "sent": "Then impute the missing ones and, but at the same time they pointed out that the difference to the explicit feedback data is that in the latter case you actually have positive and negative observations, so you don't have to do anything like that.",
                    "label": 0
                },
                {
                    "sent": "But now that we've seen is actually that both cases are very similar and in both cases the negatives are underrepresented or missing completely so.",
                    "label": 0
                },
                {
                    "sent": "It's actually pretty similar in the end.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So conclusion, so I considered explicit feedback data missing not at random, and hopefully now I convinced you that it is really important to define a very good test performance measure that is very close to your real world problem.",
                    "label": 1
                },
                {
                    "sent": "It's very nice if it's unbiased on the mnar data under.",
                    "label": 1
                },
                {
                    "sent": "Hopefully my assumptions because then it also tells you something about the user experience you can expect on the unknown complete data, and I showed that the area on the top K hit rate curve and also top the hit rate itself is a good measure for that.",
                    "label": 0
                },
                {
                    "sent": "And I also showed a computationally efficient approach and objective function for training are very simple model.",
                    "label": 1
                },
                {
                    "sent": "I call it all rank because it's concerned with all items and what we've seen is accounting for the missing ratings is really essential to get and we get at the same time a very large improvement.",
                    "label": 0
                },
                {
                    "sent": "And in the top gate rate.",
                    "label": 1
                },
                {
                    "sent": "So future work is of course to come up with maybe better test performance measures.",
                    "label": 0
                },
                {
                    "sent": "And then also, of course, maybe to have better training, objective, objective functions and develop better models for that.",
                    "label": 0
                },
                {
                    "sent": "And also one thing that I want to point out is that we have obtained a lot of interesting results were concerning the root mean squared error.",
                    "label": 0
                },
                {
                    "sent": "But all these results may not necessarily carry over.",
                    "label": 0
                },
                {
                    "sent": "Now to some other performance measures like the top K hit rate, because we've seen that they are actually very different, right?",
                    "label": 1
                },
                {
                    "sent": "A model that is equipped with RMS is not good with top hit rate and so one thing is, for example, maybe to reexamine?",
                    "label": 0
                },
                {
                    "sent": "Collaborative filtering versus content based methods.",
                    "label": 0
                },
                {
                    "sent": "We are at the moment it seems collaborative filtering is much better, but maybe it's only pharmacy and not bother me.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Yes, I like this.",
                    "label": 0
                },
                {
                    "sent": "To optimize the default rate rating and the weight based on Building 1 particular model right and then you found the way that gave you the highest score.",
                    "label": 0
                },
                {
                    "sent": "And presumably you would use that way in the same competition or practice, but it's part of getting these weights after rolling papers for model with his own biases.",
                    "label": 0
                },
                {
                    "sent": "What do you mean now that it is funny to get this way?",
                    "label": 0
                },
                {
                    "sent": "Sorry, so hopefully you could hear me doing the talk.",
                    "label": 0
                },
                {
                    "sent": "So what you mean?",
                    "label": 0
                },
                {
                    "sent": "It is funny that I get this weight.",
                    "label": 0
                },
                {
                    "sent": "1st and then.",
                    "label": 0
                },
                {
                    "sent": "Wait, yes I have some right right?",
                    "label": 0
                },
                {
                    "sent": "Before we make any models and then we.",
                    "label": 0
                },
                {
                    "sent": "That is true.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can think about the objective function without the model, and then once you have the model, then basically this just a tuning parameter like the Lambda for the regularization.",
                    "label": 0
                },
                {
                    "sent": "It's just one more tuning parameter and you do cross validation to optimize it normal approach.",
                    "label": 0
                },
                {
                    "sent": "Your tuning in general, which is odd.",
                    "label": 0
                },
                {
                    "sent": "So I determine Lambda right and which is also tuned by everyone.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's also part of the objective function, so it's no different.",
                    "label": 0
                },
                {
                    "sent": "Sizing function is fixed.",
                    "label": 0
                },
                {
                    "sent": "The top case.",
                    "label": 0
                },
                {
                    "sent": "So OK, so basically I have.",
                    "label": 0
                },
                {
                    "sent": "If I use a fixed objective function I can optimize it.",
                    "label": 0
                },
                {
                    "sent": "Then I see what I get right and then basically what I do is then I do a cross validation with my test data to optimize the tuning parameters.",
                    "label": 0
                },
                {
                    "sent": "Now I don't have only lambdas attuning prior to path.",
                    "label": 0
                },
                {
                    "sent": "Also have the weight and the imputed rating value so so and I determined based on the area under the top kicker on cross validation.",
                    "label": 0
                },
                {
                    "sent": "So just the usual procedure.",
                    "label": 0
                },
                {
                    "sent": "OK so we need to move to the next speaker.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}