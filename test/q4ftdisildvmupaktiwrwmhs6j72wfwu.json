{
    "id": "q4ftdisildvmupaktiwrwmhs6j72wfwu",
    "title": "WORQ: Workload-Driven RDF Query Processing",
    "info": {
        "author": [
            "Amgad Madkour, Department of Computer Science, Purdue University"
        ],
        "published": "Nov. 22, 2018",
        "recorded": "October 2018",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2018_madkour_woqr_workload_driven/",
    "segmentation": [
        [
            "So today I'll be discussing a workflow driven RDF preprocessing technique named work.",
            "This work was done in collaboration with Ahmed Ali, my colleague from Google and my advisor professor will dive."
        ],
        [
            "So let's just get into business.",
            "Basically, RDF is everywhere.",
            "I don't need to sell the idea of using RDF in industry or in academia.",
            "It's a fact we are using RDF both in academia and academic setups.",
            "An industrial setups.",
            "Our main problem is always scale.",
            "How can we utilize the information that we have given the large scale of RDF data that we have everywhere?",
            "One particular medium where we believe is appropriate for both storing and processing RDF data is the cloud.",
            "And this will be the focus of this talk.",
            "How can we actually do RDF preprocessing over cloud based systems having?"
        ],
        [
            "Add that we are actually as database researchers.",
            "We know that we are faced with specific challenges related to scale.",
            "When you put your RDF data over a cloud, set up one of the main problems that you have is the network shuffling problem.",
            "In essence, when we look at it from a database perspective, you have the problem of intermediate results that are actually generated when you are actually trying to figure out what data joins with what, so the intermediate results is definitely our nightmare and our objective is to find ways.",
            "Of how to reduce this intermediate results footprint throughout our network if we succeed in that, we decrease the network shuffling and at that point in time we would have better query processing performance.",
            "Another very important issue is when we have our RDF data.",
            "How are you going to partition it across all the machines that you have?",
            "So we are faced with all of these dilemmas.",
            "If we want to summarize those."
        ],
        [
            "Into three basic problem statements, we believe the first one is the data partitioning scheme.",
            "What is the best way to partition RDF data in a cloud based set?",
            "Having said that, most of the systems that we see actually over a cloud based setups that you have tend to store the data in a mostly relational form, and I'm going to go into that in just a bit.",
            "Second point is intermediate results.",
            "How are we going to reduce the footprint we when we are actually joining RDF triples from across machines?",
            "Third is that actually the aspect of caching, so might some people might argue if you cash your data or if you catch some results.",
            "Be able to increase the query processing performance.",
            "We argue otherwise caching is a good mechanism, however caching results.",
            "Final results is really not practical, so we want to go and see how."
        ],
        [
            "We can actually talk at that point, so we proposed four points, the first one being we want to basically see how can we reduce this intermediate result footprint by creating what is called reductions.",
            "So essentially when we are we are joining tables, can we in advance before doing the joint process?",
            "Figure out that there are specific entries.",
            "RDF triples that will not join and hence avoid pushing them on the network to start off with.",
            "So that's the first objective that we had and we want to do this in an online fashion.",
            "Might do that.",
            "Because there actually are approaches in the literature where we we see people computing reductions before hand before the system starts.",
            "Our challenge is how to do that in online fashion online meaning when I see the query I performed the reductions.",
            "I answered the query so this is 1, the second part is we tend to actually adopt more more or less aesthetic partitioning criteria when we have RDF data.",
            "For example, we say we're going to partition all the data based on the subject.",
            "We do hash partitioning for other resources, hash hashing for other resources.",
            "And we just distributed on the machines.",
            "We argue that maybe a more workload driven oriented partitioning criteria might be better and it would pay off when we are actually investigating decrease that.",
            "We have.",
            "Third Point is caching of reductions instead of catching the final results.",
            "And this has a very big benefit.",
            "What if we can actually create reductions that are represented representable enough that we can reuse them across different queries?",
            "And if we cache them in memory and use them as a reusable components, we would actually be able to make best use of.",
            "Our cash instead of cashing all the final results of the queries that you are processing.",
            "Finally, since we.",
            "Began our argument with the fact that maybe one way of representing RDF data is by adopting a relational approach well with the relation approaches that we see nowadays, one of the main problems that they have is how can you answer unbound queries using, let's say, a certain relational partition?"
        ],
        [
            "Let's take this just some example.",
            "First of all, on the online reductions of RDF data, so we know I'm not going to go into details about what sparkle queries look like, but we know that the main building block of any sparkle query is the BGP basic graph pattern.",
            "OK, and what's interesting about the BPCI is they can basically infer a set of join patterns out of each BGP.",
            "For example, if we have a query that has three triple patterns like this, the number of triple the number of joint correlations.",
            "That we can infer off the out of the query are as follows.",
            "For example, I can join if we're assuming today that tweet data is stored on a separate in one table mentioned in a second uncertain likes, and I'm going to over example there is this one.",
            "The joint correlations are you can join tweet on subject with mention on subject.",
            "So that's one pattern.",
            "Or you can join mentioned subject with tweets subject while you believe that this is not the same thing, it's basically saying that the second pattern second table would join with the first one.",
            "Basically, over the subject when we have mentioned and tweet, this is the driving force for our technique.",
            "If we are able to analyze a query and figure out joint patterns that can actually Co occur in the workload, we can precompute reductions for those.",
            "And we're going to go into an example for that one."
        ],
        [
            "OK, so the first one is basically one.",
            "I have a query like this that has mentioned and tweet.",
            "What's the joint pattern that we're going to have so the joint pattern here is that you are saying that you have mentioned subjects joins with tweet subject, right?",
            "And this is the joint problem that you have.",
            "So essentially, we're saying that we're going to call 2 tables and what you're seeing here is the relational representation of the of the RDF data that we had.",
            "We split the data by the predicate name so we have a table for mention and table for tweets and what you have here is the subject and object for each and every predicate table.",
            "That's nice.",
            "The proposal we have is when you have those tables.",
            "The first thing you do during preprocessing phase is you create bloom filters for each one of them.",
            "So for example here I can.",
            "I will have a Bloom filter that represents that.",
            "The subject column and another one for the object column and the third one.",
            "Here for subject and one for the object.",
            "But since I'm just interested in the subject column, I'm going to utilize the Bloom filter represented the subject column in both dimension and the tweets.",
            "How are we going to use those in order to actually to infer the reductions?",
            "Well, we can do that in an online fashion by saying, for example, if I want to find an instance of mention, a reduction of mention given a join with tweet, you just probe the Bloom filter of tweet.",
            "Ask it for each entry inside dimension table.",
            "Do you have this resource?",
            "Would qualify your Bloom filter or not, and that's the question that we keep asking.",
            "So for example here what will happen is we will find that there are only three entries for the mentioned table that would qualify a join with tweets.",
            "This is the critical point.",
            "This is the most important point that we want to emphasize on.",
            "Having said that, you'll be able to have a new instance of dimension table with only the triple patterns that qualify a join on subject with tweet on subject.",
            "Same idea could actually also be applied for tweet.",
            "I can probe the Bloom filter for dimension an, infer that there is actually another reduction.",
            "OK, that sounds good.",
            "However, the query also has Mary.",
            "As an object, so we now have filtered that and I was arguing the beginning that the reductions.",
            "Could be reusable across queries, right?",
            "Well, that's the thing we're going to retain the reductions, and we can actually apply Mary directly over dimension reduction.",
            "So we do the selection over that particular table and then we proceed with our joint and get our results.",
            "What does this mean?",
            "This means that these reductions now are reusable for this specific pattern, and we can apply whatever predicate other other selection operators over your remaining parts of your query.",
            "That's basically the gist of the idea that we have.",
            "OK, so this is nice for let's say 2 two triple patterns.",
            "Can we go beyond that?"
        ],
        [
            "Yes, we are actually we have something proposed.",
            "It's called the energy join.",
            "The energy join is essentially the same idea.",
            "Let's say you have three triple patterns, so you have your 3 tables right.",
            "But now instead of probing only one filter for each table, you probe all the filters that you're connected to.",
            "So now I have for example, here I have a joint pattern between the three tables on X, so for each one you're going to probe this one and this one.",
            "So every entry that you have in your reduction needs to bypass.",
            "Two filters, So what essentially you end up having is an aggressive reduction of the input data that you were going there going to use for your query.",
            "That's the main.",
            "The idea that you have, and finally that you have you have your result set."
        ],
        [
            "So for the caching part, well, since we were arguing that we can reuse the reductions.",
            "What are we going to cache here?",
            "From this figure, we're basically going to catch the reductions now instead of the original tables, and we apply the selection directly over the reductions instead of actually looking at the original data.",
            "This is definitely a big win for us."
        ],
        [
            "What about workload driven partitioning?",
            "Well, basically in regular partitioning, if you have three machines today and you want to partition both tables and we take one static criteria based on subjects.",
            "OK, so we're going to take John all entries of John in the first machine, put it on machine one, and for tweet the same thing Alex Emery Machine 2 Alex from Tweet Sally from from mention and Mike and Mike.",
            "Entries of tweets are going to be in this in this machine tree.",
            "What did we do now?",
            "We essentially selected a static criteria for partitioning the data before we start."
        ],
        [
            "You better we basically can if you're using the same idea of reductions, we can look at the reduction that are generated based on the query that we have, and for each table we apply a different partitioning criteria based on how it's joint.",
            "So for example, I can have one reduction partition by subject and another based on object, so we're not taking a predefined decision of how to partition our data.",
            "So the same idea will apply with partition.",
            "The data for R1R2 and R3.",
            "Again, we're not tight to specifics."
        ],
        [
            "What about the last point, which is the queries of unbound properties.",
            "We, we argued in the beginning that an unbound property here is.",
            "You don't know what the table name would be, right?",
            "So how are we going to figure out which table are we going to query?",
            "So in the in the in the regular case, what will happen is we'll have to scale all the tables right?",
            "To figure out what bounds with Mike.",
            "So what the solution for that we have?",
            "Basically a very small solution.",
            "I'm sorry about that.",
            "Yeah, this one.",
            "So essentially what the proposal is?",
            "What if I have Mary is abound is abound Val."
        ],
        [
            "What's going to happen is I can query the bloom filters, not the original tables and ask do you have Mary in your Bloom filter?",
            "If yes, it's going to be a match.",
            "If no, that does not match.",
            "That means table Tweet will not be part of the unbound result.",
            "And since we're using Bloom filters, let's take for the sake of clarity here that there is a false positive match.",
            "We call this whole phase that an identification phase where we identify what are the unbound predicates that we can use in order to say that this is the result set.",
            "So for this false positive, what are we going to do about that?",
            "We go into something called the verification page, which is very easy.",
            "Use the original data.",
            "Apply the filters if it qualifies.",
            "Therefore it's a match.",
            "If it does not qualify.",
            "Therefore it was an incorrect match.",
            "So."
        ],
        [
            "We did the error experiments.",
            "We implemented the system over work technique in a system called knowledge cubes.",
            "It's basically available online.",
            "You can look at it and we had three benchmark systems that we tested what against what LBM and ego."
        ],
        [
            "Basically we tested against assist."
        ],
        [
            "S2 RDF, which actually is an Apple Apple to Apple comparison as to RDF.",
            "Adopts the concept of reductions and they essentially try to precompute all reductions before hand, so the results that you're going to have is."
        ],
        [
            "Follows in terms of number of files.",
            "We almost have an order of magnitude less footprint then S2 RDF because we do not precompute all the reductions."
        ],
        [
            "The data size on HTFS almost is also an order of magnitude less."
        ],
        [
            "In terms of pre processing, time is very very low.",
            "The reason is simply because we don't do reductions before hand and as to RDF he has to compute all reductions across all the predicates that he has.",
            "So we're not doing that, so that's why we have a very lower preprocessing."
        ],
        [
            "But in terms of mean execution time and total execution time, we're doing definitely much better because our signature, our reductions, are much smaller than S2 RDF and actually can attribute through the energy join.",
            "We're not actually an S2 RDF the you have to do it on a binary and binary form for each predicate you do a reduction with the other predicate, so this actually paid off during query execution performance when we actually are doing the energy joins."
        ],
        [
            "Same here for query patterns.",
            "This was an interesting study for the 5000 queries that we had.",
            "We analyze what are the patterns we have found 20 patterns for this one and 20 patterns for this one, and we consistently get good results overall.",
            "Overall, the datasets that we."
        ],
        [
            "As for the workload driven partitioning, basically what Dave is a for.",
            "What if an LBM?",
            "The main idea here is that we showcase that the workload driven partitioning part achieves better performance than adopting a static criteria and same was also apparent in LBM.",
            "Finally.",
            "That's just.",
            "So.",
            "My machine froze, that's not good.",
            "But anyway, so yeah, so basic."
        ],
        [
            "So just in conclusion, and the same idea was actually for the unbound properties, we have almost the double."
        ],
        [
            "The performance with double the performance enhancement when compared to using and not using Unbound property.",
            "In conclusion, we presented an online partitioning.",
            "We presented an online way for computing reductions and workload driven.",
            "RDF workload driven RTF partitioning criteria.",
            "A caching of reduction process where we avoid caching the final results.",
            "And finally we presented a simple technique for identifying the bound properties efficiently.",
            "Thank you, thank."
        ],
        [
            "You"
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So today I'll be discussing a workflow driven RDF preprocessing technique named work.",
                    "label": 0
                },
                {
                    "sent": "This work was done in collaboration with Ahmed Ali, my colleague from Google and my advisor professor will dive.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's just get into business.",
                    "label": 0
                },
                {
                    "sent": "Basically, RDF is everywhere.",
                    "label": 0
                },
                {
                    "sent": "I don't need to sell the idea of using RDF in industry or in academia.",
                    "label": 0
                },
                {
                    "sent": "It's a fact we are using RDF both in academia and academic setups.",
                    "label": 0
                },
                {
                    "sent": "An industrial setups.",
                    "label": 0
                },
                {
                    "sent": "Our main problem is always scale.",
                    "label": 0
                },
                {
                    "sent": "How can we utilize the information that we have given the large scale of RDF data that we have everywhere?",
                    "label": 0
                },
                {
                    "sent": "One particular medium where we believe is appropriate for both storing and processing RDF data is the cloud.",
                    "label": 0
                },
                {
                    "sent": "And this will be the focus of this talk.",
                    "label": 0
                },
                {
                    "sent": "How can we actually do RDF preprocessing over cloud based systems having?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Add that we are actually as database researchers.",
                    "label": 0
                },
                {
                    "sent": "We know that we are faced with specific challenges related to scale.",
                    "label": 0
                },
                {
                    "sent": "When you put your RDF data over a cloud, set up one of the main problems that you have is the network shuffling problem.",
                    "label": 1
                },
                {
                    "sent": "In essence, when we look at it from a database perspective, you have the problem of intermediate results that are actually generated when you are actually trying to figure out what data joins with what, so the intermediate results is definitely our nightmare and our objective is to find ways.",
                    "label": 0
                },
                {
                    "sent": "Of how to reduce this intermediate results footprint throughout our network if we succeed in that, we decrease the network shuffling and at that point in time we would have better query processing performance.",
                    "label": 0
                },
                {
                    "sent": "Another very important issue is when we have our RDF data.",
                    "label": 0
                },
                {
                    "sent": "How are you going to partition it across all the machines that you have?",
                    "label": 0
                },
                {
                    "sent": "So we are faced with all of these dilemmas.",
                    "label": 0
                },
                {
                    "sent": "If we want to summarize those.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Into three basic problem statements, we believe the first one is the data partitioning scheme.",
                    "label": 1
                },
                {
                    "sent": "What is the best way to partition RDF data in a cloud based set?",
                    "label": 0
                },
                {
                    "sent": "Having said that, most of the systems that we see actually over a cloud based setups that you have tend to store the data in a mostly relational form, and I'm going to go into that in just a bit.",
                    "label": 0
                },
                {
                    "sent": "Second point is intermediate results.",
                    "label": 1
                },
                {
                    "sent": "How are we going to reduce the footprint we when we are actually joining RDF triples from across machines?",
                    "label": 0
                },
                {
                    "sent": "Third is that actually the aspect of caching, so might some people might argue if you cash your data or if you catch some results.",
                    "label": 1
                },
                {
                    "sent": "Be able to increase the query processing performance.",
                    "label": 0
                },
                {
                    "sent": "We argue otherwise caching is a good mechanism, however caching results.",
                    "label": 0
                },
                {
                    "sent": "Final results is really not practical, so we want to go and see how.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can actually talk at that point, so we proposed four points, the first one being we want to basically see how can we reduce this intermediate result footprint by creating what is called reductions.",
                    "label": 0
                },
                {
                    "sent": "So essentially when we are we are joining tables, can we in advance before doing the joint process?",
                    "label": 0
                },
                {
                    "sent": "Figure out that there are specific entries.",
                    "label": 0
                },
                {
                    "sent": "RDF triples that will not join and hence avoid pushing them on the network to start off with.",
                    "label": 1
                },
                {
                    "sent": "So that's the first objective that we had and we want to do this in an online fashion.",
                    "label": 0
                },
                {
                    "sent": "Might do that.",
                    "label": 0
                },
                {
                    "sent": "Because there actually are approaches in the literature where we we see people computing reductions before hand before the system starts.",
                    "label": 0
                },
                {
                    "sent": "Our challenge is how to do that in online fashion online meaning when I see the query I performed the reductions.",
                    "label": 0
                },
                {
                    "sent": "I answered the query so this is 1, the second part is we tend to actually adopt more more or less aesthetic partitioning criteria when we have RDF data.",
                    "label": 0
                },
                {
                    "sent": "For example, we say we're going to partition all the data based on the subject.",
                    "label": 0
                },
                {
                    "sent": "We do hash partitioning for other resources, hash hashing for other resources.",
                    "label": 0
                },
                {
                    "sent": "And we just distributed on the machines.",
                    "label": 0
                },
                {
                    "sent": "We argue that maybe a more workload driven oriented partitioning criteria might be better and it would pay off when we are actually investigating decrease that.",
                    "label": 0
                },
                {
                    "sent": "We have.",
                    "label": 0
                },
                {
                    "sent": "Third Point is caching of reductions instead of catching the final results.",
                    "label": 0
                },
                {
                    "sent": "And this has a very big benefit.",
                    "label": 0
                },
                {
                    "sent": "What if we can actually create reductions that are represented representable enough that we can reuse them across different queries?",
                    "label": 0
                },
                {
                    "sent": "And if we cache them in memory and use them as a reusable components, we would actually be able to make best use of.",
                    "label": 0
                },
                {
                    "sent": "Our cash instead of cashing all the final results of the queries that you are processing.",
                    "label": 0
                },
                {
                    "sent": "Finally, since we.",
                    "label": 0
                },
                {
                    "sent": "Began our argument with the fact that maybe one way of representing RDF data is by adopting a relational approach well with the relation approaches that we see nowadays, one of the main problems that they have is how can you answer unbound queries using, let's say, a certain relational partition?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's take this just some example.",
                    "label": 0
                },
                {
                    "sent": "First of all, on the online reductions of RDF data, so we know I'm not going to go into details about what sparkle queries look like, but we know that the main building block of any sparkle query is the BGP basic graph pattern.",
                    "label": 1
                },
                {
                    "sent": "OK, and what's interesting about the BPCI is they can basically infer a set of join patterns out of each BGP.",
                    "label": 0
                },
                {
                    "sent": "For example, if we have a query that has three triple patterns like this, the number of triple the number of joint correlations.",
                    "label": 0
                },
                {
                    "sent": "That we can infer off the out of the query are as follows.",
                    "label": 0
                },
                {
                    "sent": "For example, I can join if we're assuming today that tweet data is stored on a separate in one table mentioned in a second uncertain likes, and I'm going to over example there is this one.",
                    "label": 0
                },
                {
                    "sent": "The joint correlations are you can join tweet on subject with mention on subject.",
                    "label": 0
                },
                {
                    "sent": "So that's one pattern.",
                    "label": 0
                },
                {
                    "sent": "Or you can join mentioned subject with tweets subject while you believe that this is not the same thing, it's basically saying that the second pattern second table would join with the first one.",
                    "label": 0
                },
                {
                    "sent": "Basically, over the subject when we have mentioned and tweet, this is the driving force for our technique.",
                    "label": 0
                },
                {
                    "sent": "If we are able to analyze a query and figure out joint patterns that can actually Co occur in the workload, we can precompute reductions for those.",
                    "label": 0
                },
                {
                    "sent": "And we're going to go into an example for that one.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the first one is basically one.",
                    "label": 0
                },
                {
                    "sent": "I have a query like this that has mentioned and tweet.",
                    "label": 0
                },
                {
                    "sent": "What's the joint pattern that we're going to have so the joint pattern here is that you are saying that you have mentioned subjects joins with tweet subject, right?",
                    "label": 0
                },
                {
                    "sent": "And this is the joint problem that you have.",
                    "label": 0
                },
                {
                    "sent": "So essentially, we're saying that we're going to call 2 tables and what you're seeing here is the relational representation of the of the RDF data that we had.",
                    "label": 0
                },
                {
                    "sent": "We split the data by the predicate name so we have a table for mention and table for tweets and what you have here is the subject and object for each and every predicate table.",
                    "label": 0
                },
                {
                    "sent": "That's nice.",
                    "label": 0
                },
                {
                    "sent": "The proposal we have is when you have those tables.",
                    "label": 0
                },
                {
                    "sent": "The first thing you do during preprocessing phase is you create bloom filters for each one of them.",
                    "label": 0
                },
                {
                    "sent": "So for example here I can.",
                    "label": 0
                },
                {
                    "sent": "I will have a Bloom filter that represents that.",
                    "label": 0
                },
                {
                    "sent": "The subject column and another one for the object column and the third one.",
                    "label": 0
                },
                {
                    "sent": "Here for subject and one for the object.",
                    "label": 0
                },
                {
                    "sent": "But since I'm just interested in the subject column, I'm going to utilize the Bloom filter represented the subject column in both dimension and the tweets.",
                    "label": 0
                },
                {
                    "sent": "How are we going to use those in order to actually to infer the reductions?",
                    "label": 0
                },
                {
                    "sent": "Well, we can do that in an online fashion by saying, for example, if I want to find an instance of mention, a reduction of mention given a join with tweet, you just probe the Bloom filter of tweet.",
                    "label": 0
                },
                {
                    "sent": "Ask it for each entry inside dimension table.",
                    "label": 0
                },
                {
                    "sent": "Do you have this resource?",
                    "label": 0
                },
                {
                    "sent": "Would qualify your Bloom filter or not, and that's the question that we keep asking.",
                    "label": 0
                },
                {
                    "sent": "So for example here what will happen is we will find that there are only three entries for the mentioned table that would qualify a join with tweets.",
                    "label": 0
                },
                {
                    "sent": "This is the critical point.",
                    "label": 0
                },
                {
                    "sent": "This is the most important point that we want to emphasize on.",
                    "label": 0
                },
                {
                    "sent": "Having said that, you'll be able to have a new instance of dimension table with only the triple patterns that qualify a join on subject with tweet on subject.",
                    "label": 0
                },
                {
                    "sent": "Same idea could actually also be applied for tweet.",
                    "label": 0
                },
                {
                    "sent": "I can probe the Bloom filter for dimension an, infer that there is actually another reduction.",
                    "label": 0
                },
                {
                    "sent": "OK, that sounds good.",
                    "label": 0
                },
                {
                    "sent": "However, the query also has Mary.",
                    "label": 0
                },
                {
                    "sent": "As an object, so we now have filtered that and I was arguing the beginning that the reductions.",
                    "label": 0
                },
                {
                    "sent": "Could be reusable across queries, right?",
                    "label": 0
                },
                {
                    "sent": "Well, that's the thing we're going to retain the reductions, and we can actually apply Mary directly over dimension reduction.",
                    "label": 0
                },
                {
                    "sent": "So we do the selection over that particular table and then we proceed with our joint and get our results.",
                    "label": 0
                },
                {
                    "sent": "What does this mean?",
                    "label": 0
                },
                {
                    "sent": "This means that these reductions now are reusable for this specific pattern, and we can apply whatever predicate other other selection operators over your remaining parts of your query.",
                    "label": 0
                },
                {
                    "sent": "That's basically the gist of the idea that we have.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is nice for let's say 2 two triple patterns.",
                    "label": 0
                },
                {
                    "sent": "Can we go beyond that?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, we are actually we have something proposed.",
                    "label": 0
                },
                {
                    "sent": "It's called the energy join.",
                    "label": 0
                },
                {
                    "sent": "The energy join is essentially the same idea.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have three triple patterns, so you have your 3 tables right.",
                    "label": 0
                },
                {
                    "sent": "But now instead of probing only one filter for each table, you probe all the filters that you're connected to.",
                    "label": 0
                },
                {
                    "sent": "So now I have for example, here I have a joint pattern between the three tables on X, so for each one you're going to probe this one and this one.",
                    "label": 0
                },
                {
                    "sent": "So every entry that you have in your reduction needs to bypass.",
                    "label": 0
                },
                {
                    "sent": "Two filters, So what essentially you end up having is an aggressive reduction of the input data that you were going there going to use for your query.",
                    "label": 0
                },
                {
                    "sent": "That's the main.",
                    "label": 0
                },
                {
                    "sent": "The idea that you have, and finally that you have you have your result set.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the caching part, well, since we were arguing that we can reuse the reductions.",
                    "label": 0
                },
                {
                    "sent": "What are we going to cache here?",
                    "label": 0
                },
                {
                    "sent": "From this figure, we're basically going to catch the reductions now instead of the original tables, and we apply the selection directly over the reductions instead of actually looking at the original data.",
                    "label": 0
                },
                {
                    "sent": "This is definitely a big win for us.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What about workload driven partitioning?",
                    "label": 0
                },
                {
                    "sent": "Well, basically in regular partitioning, if you have three machines today and you want to partition both tables and we take one static criteria based on subjects.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to take John all entries of John in the first machine, put it on machine one, and for tweet the same thing Alex Emery Machine 2 Alex from Tweet Sally from from mention and Mike and Mike.",
                    "label": 0
                },
                {
                    "sent": "Entries of tweets are going to be in this in this machine tree.",
                    "label": 0
                },
                {
                    "sent": "What did we do now?",
                    "label": 0
                },
                {
                    "sent": "We essentially selected a static criteria for partitioning the data before we start.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You better we basically can if you're using the same idea of reductions, we can look at the reduction that are generated based on the query that we have, and for each table we apply a different partitioning criteria based on how it's joint.",
                    "label": 0
                },
                {
                    "sent": "So for example, I can have one reduction partition by subject and another based on object, so we're not taking a predefined decision of how to partition our data.",
                    "label": 0
                },
                {
                    "sent": "So the same idea will apply with partition.",
                    "label": 0
                },
                {
                    "sent": "The data for R1R2 and R3.",
                    "label": 0
                },
                {
                    "sent": "Again, we're not tight to specifics.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What about the last point, which is the queries of unbound properties.",
                    "label": 1
                },
                {
                    "sent": "We, we argued in the beginning that an unbound property here is.",
                    "label": 0
                },
                {
                    "sent": "You don't know what the table name would be, right?",
                    "label": 0
                },
                {
                    "sent": "So how are we going to figure out which table are we going to query?",
                    "label": 0
                },
                {
                    "sent": "So in the in the in the regular case, what will happen is we'll have to scale all the tables right?",
                    "label": 0
                },
                {
                    "sent": "To figure out what bounds with Mike.",
                    "label": 0
                },
                {
                    "sent": "So what the solution for that we have?",
                    "label": 0
                },
                {
                    "sent": "Basically a very small solution.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry about that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, this one.",
                    "label": 0
                },
                {
                    "sent": "So essentially what the proposal is?",
                    "label": 0
                },
                {
                    "sent": "What if I have Mary is abound is abound Val.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What's going to happen is I can query the bloom filters, not the original tables and ask do you have Mary in your Bloom filter?",
                    "label": 0
                },
                {
                    "sent": "If yes, it's going to be a match.",
                    "label": 0
                },
                {
                    "sent": "If no, that does not match.",
                    "label": 1
                },
                {
                    "sent": "That means table Tweet will not be part of the unbound result.",
                    "label": 0
                },
                {
                    "sent": "And since we're using Bloom filters, let's take for the sake of clarity here that there is a false positive match.",
                    "label": 1
                },
                {
                    "sent": "We call this whole phase that an identification phase where we identify what are the unbound predicates that we can use in order to say that this is the result set.",
                    "label": 0
                },
                {
                    "sent": "So for this false positive, what are we going to do about that?",
                    "label": 0
                },
                {
                    "sent": "We go into something called the verification page, which is very easy.",
                    "label": 0
                },
                {
                    "sent": "Use the original data.",
                    "label": 0
                },
                {
                    "sent": "Apply the filters if it qualifies.",
                    "label": 0
                },
                {
                    "sent": "Therefore it's a match.",
                    "label": 0
                },
                {
                    "sent": "If it does not qualify.",
                    "label": 0
                },
                {
                    "sent": "Therefore it was an incorrect match.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We did the error experiments.",
                    "label": 0
                },
                {
                    "sent": "We implemented the system over work technique in a system called knowledge cubes.",
                    "label": 0
                },
                {
                    "sent": "It's basically available online.",
                    "label": 0
                },
                {
                    "sent": "You can look at it and we had three benchmark systems that we tested what against what LBM and ego.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically we tested against assist.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "S2 RDF, which actually is an Apple Apple to Apple comparison as to RDF.",
                    "label": 0
                },
                {
                    "sent": "Adopts the concept of reductions and they essentially try to precompute all reductions before hand, so the results that you're going to have is.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Follows in terms of number of files.",
                    "label": 0
                },
                {
                    "sent": "We almost have an order of magnitude less footprint then S2 RDF because we do not precompute all the reductions.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The data size on HTFS almost is also an order of magnitude less.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In terms of pre processing, time is very very low.",
                    "label": 0
                },
                {
                    "sent": "The reason is simply because we don't do reductions before hand and as to RDF he has to compute all reductions across all the predicates that he has.",
                    "label": 0
                },
                {
                    "sent": "So we're not doing that, so that's why we have a very lower preprocessing.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in terms of mean execution time and total execution time, we're doing definitely much better because our signature, our reductions, are much smaller than S2 RDF and actually can attribute through the energy join.",
                    "label": 0
                },
                {
                    "sent": "We're not actually an S2 RDF the you have to do it on a binary and binary form for each predicate you do a reduction with the other predicate, so this actually paid off during query execution performance when we actually are doing the energy joins.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same here for query patterns.",
                    "label": 0
                },
                {
                    "sent": "This was an interesting study for the 5000 queries that we had.",
                    "label": 0
                },
                {
                    "sent": "We analyze what are the patterns we have found 20 patterns for this one and 20 patterns for this one, and we consistently get good results overall.",
                    "label": 0
                },
                {
                    "sent": "Overall, the datasets that we.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As for the workload driven partitioning, basically what Dave is a for.",
                    "label": 0
                },
                {
                    "sent": "What if an LBM?",
                    "label": 0
                },
                {
                    "sent": "The main idea here is that we showcase that the workload driven partitioning part achieves better performance than adopting a static criteria and same was also apparent in LBM.",
                    "label": 0
                },
                {
                    "sent": "Finally.",
                    "label": 0
                },
                {
                    "sent": "That's just.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "My machine froze, that's not good.",
                    "label": 0
                },
                {
                    "sent": "But anyway, so yeah, so basic.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just in conclusion, and the same idea was actually for the unbound properties, we have almost the double.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The performance with double the performance enhancement when compared to using and not using Unbound property.",
                    "label": 0
                },
                {
                    "sent": "In conclusion, we presented an online partitioning.",
                    "label": 0
                },
                {
                    "sent": "We presented an online way for computing reductions and workload driven.",
                    "label": 1
                },
                {
                    "sent": "RDF workload driven RTF partitioning criteria.",
                    "label": 0
                },
                {
                    "sent": "A caching of reduction process where we avoid caching the final results.",
                    "label": 1
                },
                {
                    "sent": "And finally we presented a simple technique for identifying the bound properties efficiently.",
                    "label": 0
                },
                {
                    "sent": "Thank you, thank.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You",
                    "label": 0
                }
            ]
        }
    }
}