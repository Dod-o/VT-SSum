{
    "id": "z7qpv54wvkoovlz23rs2nq2k4rizacf6",
    "title": "Effects of Stress and Genotype on Exploration-Exploitation Dynamics in Reinforcement Learning",
    "info": {
        "author": [
            "Gedi Luk\u0161ys, \u00c9cole Polytechnique F\u00e9d\u00e9rale de Lausanne"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "December 2006",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/otee06_luksys_esgee/",
    "segmentation": [
        [
            "Actually, it's sort of thing and most others.",
            "So in this talk you won't see much of mass for much of theory, but I'm going to show you how we can actually combine reinforcement learning models with behavioral experiments in order to study how external factors like stress or differential Geno type between different animals could lead to could affect their exploration exploitation."
        ],
        [
            "So first of all, let's have a look at the simple behavioral setup.",
            "So we have a box.",
            "In this box there is a hole where you know an animal can put a nose inside and there is a reserve at the top with the food.",
            "And this food can be delivered down to the box at some point, and the animal is hungry and the idea is that in the beginning lights are off, but then when a trial starts, light is getting on and then at some point.",
            "Animal responds by poking nose into a hole, and then as a result, it receives foot, which it can eat.",
            "And of course it's happy.",
            "And then at other time if, for example, if we don't have trial but in that range weather and the lights are off.",
            "Then animal, if it makes a poke.",
            "In that case it doesn't receive anything.",
            "And of course it doesn't satisfy its goal.",
            "So you say so.",
            "The goal of animal in the stars because the total session times 10 minutes is to actually respond as quickly as possible to the trials when the light is on.",
            "And of course, to waste as little energy as possible, but making posting into try interval.",
            "And our question is how can we model learning and behavior in such task in computational framework?"
        ],
        [
            "And one of the usual ways to model such kind of conditioning and many other experiments is using simple temporal difference reinforcement learning models with discrete States and actions.",
            "So for instance, in this case our states correspond to whether the lights are on, so we have a trial, or where the lights are offered and there's into interval and respect to animal position that corresponds to whether animals outside of this inside and here is just a detail we have to distinguish between starting your continue.",
            "Suppose because in practice, animals need at least some time, which in our case is 2 time steps to actually deliver physically their word.",
            "So for example, when trial start we move from here to here, and then at each point animal can choose whether to stay outside or making you poke, and then when it makes working, it can choose how quickly to return to the outside state.",
            "So we sat at each point.",
            "There is basically a choice of two different actions, and then once you have this formalization what we have.",
            "To learn this so called curious which correspond to the expected future reward.",
            "Said, if understate S action, A has been taken.",
            "So here the reward is discounted with the temporal discount factor garment to future.",
            "So this is standard TD stuff and we learn this curious with the famous temporal difference error which basically subtracts the discounted next value next value with the current value and add it with the observed towards and then.",
            "We used this Delta T tempo difference error by multiplying with learning rate and this is the way how we simply updated kouvelis at each time step."
        ],
        [
            "And then, of course, such TTL models are capable of predicting many qualitative aspects of animal behavior, suggests animals learn based on rewards, or the extinguished learning if rewards are not present anymore.",
            "But now question is, what about realistic aspects of animal behavior?",
            "For example, what if some animals are more hungry than others?",
            "How can we account for that?",
            "Or what if some animals are exposed?",
            "For example a cat?",
            "Before the experiment, how would it affect their learning or in general, even if there are no extra factors?",
            "What if some animals learn faster, others learn slower, so such things you can't really account just using it.",
            "Generic reinforcement learning framework and the question is how can we do this thing?",
            "Will still using our."
        ],
        [
            "Simple live models.",
            "So the idea is that if we have some way how to dynamically control the meta parameters in their enforcement learning, perhaps very possible to achieve those rich class of different behaviors which animals actually produce in nature in practice.",
            "So for example, the most relevant thing for workshop once we have those Q values which correspond to the future award predictions.",
            "I mean, we still May either.",
            "Exploit them more accurately so that, for example, if animals staying outside and it has to choose between action, stay continuously or move and start anew.",
            "Poke for instance if the value of 1, one of the action is a bit higher than the other.",
            "The probability that this action will be chosen is controlled by this expectation factor better.",
            "So if it is higher than even a small differences in the Q values will actually lead to.",
            "Always persistence strategy of choosing the hike valuable if it is low and even larger difference could be could correspond to quite random behavior.",
            "So here is a standard problem of balancing exploration exploitation.",
            "So at this point we have those three meter parameters learning great expectation faction reward this contact, which we interested to learn how we can balance them and how we can adjust them so that our model really."
        ],
        [
            "Sponsored realistic behavior and this is not only the model stuff, but there is there also hypothesis that this meta learning is related to certain parts in the brain, like for instance.",
            "Interactivity of dopamine, serotonin or general in anothers corresponding to those meta parameters in some way like there's article bushels for dopamine which is quite famous and there is a hypothesis by DIA.",
            "For the other new modulators, and certainly the neuromodulators can influence learning by, for example, by influencing heter synaptic plasticity which actually correspond simulate to the learning rules which we have in our enforcement learning.",
            "And then if we have such kind of relation and if you want to know how for example stress or different genetic background influences learning influences brain processes, we can actually use an indirect method and we can try to sort of say see what differences in their enforcement remodel which as closely as possible resembles behavior we observe and then based on those differences we can infer what are actually different senior modulators which we would be able to test experimentally."
        ],
        [
            "And then so let's look at the outline of our approach.",
            "So basically we used to derail models to model animal behavior in two different tasks.",
            "One of them that are insured for you, the whole box and the other one, which is a bit more complicated, so I won't have time to go into detail.",
            "the Morris Water maze where basically animal is dropped into a pool of cold water, usually called, and it has to find the hidden platform at certain point using the outside queue.",
            "So it is.",
            "The standard spatial learning task and here we can also manipulate things like for example the temperature of water as a stressor.",
            "And then we formalize the behavior in these two experiments in some kind of reinforcement learning models.",
            "And then we can compare the performance of these models for the animal behavior.",
            "And of course, our goal is to find ways how we can control meta parameter values in order to have the best fit between these two things."
        ],
        [
            "So let's look more distant to a file box experiments.",
            "So we have hungry myself, two different genotypes.",
            "One of them are the standard used by experimentalists, the Series 7 so called black mice and the other ones are DBA mice, which are very hyperactive, very impulsive and very anxious.",
            "So they really behaving differently.",
            "And then the experiment trial, as you saw, consists of lights getting on and then we wait for the response of the animal.",
            "When it responds, it gets food and the lights go off.",
            "And here we have 15 seconds into try interval and then there is alternation of those during a daily 10 minutes trial and because animals of course are different from models, we cannot teach animal in a single day.",
            "So we have to repeat it our many different days.",
            "So after the initial habitation that exposure to.",
            "Set up we first train like.",
            "Using all groups for all groups of animals using the same type of protocol, and then for the later four days we apply different stress.",
            "Condition says we could we could study the effects of stress and then also we make a break of almost one month to see how much animals would forget or how much they would consolidate the memory and then we can train and begin in the second block at the same time seeing basically how much they remember how much they."
        ],
        [
            "Out of this.",
            "OK, so once we have this experiment then we have to find a way how to evaluate the performance for both the animals and the model.",
            "So a simple way.",
            "A simple way is.",
            "For example, here we have this 10 minutes trial and we can calculate response time for each trial like 12 seconds, 9 seconds, 20 seconds having a calculate our own mean and then you see this corresponds to the value for the first day and then if we draw over this you see a decrease like in the first inning bug.",
            "Then there's a break.",
            "There is a little increase and then it decreases further.",
            "We can also look at other performance measures called PMS as how many trials it made during those 10 minutes.",
            "Any wrong in the trial pokes it made and so on.",
            "And then once we have this, we can actually calculate the same things for the model as well As for the real animal, and then using a variation of stochastic gradient descent algorithm we can estimate the meta parameter sets Alpha, beta, gamma for each session and each animal in a way that minimizes the following Chi Square chi squared goodness fit.",
            "So basically here is a difference between exponential performance measure and model given following parameters.",
            "We were variance and summing up overall and we want to get this as small as possible.",
            "Ideal that it won't be a significant difference between the two.",
            "And here you see one example.",
            "When we estimate here, we get how our model actually performs."
        ],
        [
            "OK, and now we estimated those meta parameters and what is next.",
            "So first is it should be good to check generalization whether our estimation generalizes well.",
            "Also have goodness fit is good enough.",
            "Then it's also to see the stability and reliability of those math parameters.",
            "We can generate artificial behavior, but for example taking certain Department sets, generating behavior and write him back, estimate the parameters and to see if we get the same thing between the two.",
            "So you've seen this graph at the difference actually smaller.",
            "Especially for exploitation factor, you really get quite accurate estimations.",
            "And then we can ask questions like how do meta parameters change with time or what are the differences between diff."
        ],
        [
            "X men to groups.",
            "So today we're just going to have a look at the exploitation factors better, how?"
        ],
        [
            "The change.",
            "And the basic result that we see is that doing a process of learning during a single block.",
            "This exploitation factors clearly increase for both groups of animals.",
            "And there is a decrease and then increase further during 2nd learning block and there is no such thing for learning rates for example, which would be maybe expecting some algorithmic terms.",
            "So we could say that with a decreasing Delta reward prediction errors those expectation factors better increase with time.",
            "So this actually corresponds to theoretical idea which was suggesting many papers with.",
            "To show that animals in fact behave like what theory would suggest should be optimal.",
            "And then in a thing that we see is that those anxious DBA mice show much lower expectation factor consistently than the normal seated 7 mice, and this would actually suggests that the reknown, the impairment in the performance of these mice is probably due to the lack of attention towards particular task, particular similar and not that we cannot learn the things, but they cannot exploit and perform accurately what they have already learned.",
            "And also in a nice thing if we look at the results for the Water maze experiment, we have the same qualitative result.",
            "Basically we have increased in both cases and we have the series.",
            "7 miles have higher expectation factors then the DBA, mice and the same things actually are very similar for other prime."
        ],
        [
            "The parameters well, and now if you want to look finally the effects of stress with stress is not so simple, but we see also in both experiments that those expedition factors better are elevated upon exposure to stress for the normal mice for season seven, but there is not significant no significant increase for the anxious mice.",
            "And if you ask why could that be the case?",
            "One of the biological possible explanations is that there is a so say hi political relation of this expectation factor to.",
            "Sonic noradrenaline levels in the brain.",
            "And we know that these mice are more anxious and their higher stress hormone levels and higher modeling levels.",
            "And there is a hypothesis that actually not generally is related to performance accuracy and that under intermediate levels we can achieve best performance because it leads animals to focus their attention where at low levels are not sufficiently alerted at the high levels.",
            "We have kind of distracted scanning attention.",
            "And so you see, in this case that because black animals on the left side of the curve having additional stress, which helps them to increase their focus of attention, while these animals, they're ready to anchor, the too distracted with anything, and adding additional stress really doesn't help for them.",
            "So this is just example, how can relate it to biological data.",
            "And then of course we can apply, for example, agonist or antagonist self modeling system to study if we would change things and we estimate the parameters again, if we really.",
            "Establish a relation as we think it could be."
        ],
        [
            "So thanks a lot, that's joint work with Jeremy Kinessa.",
            "Initially question my supervisors come in sign language from Gessner, and for those who want to discuss more and the future directions of our work, he should commit post later in the afternoon.",
            "Experience, regret.",
            "Well, I mean I don't know it could be, it could be.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, it's sort of thing and most others.",
                    "label": 0
                },
                {
                    "sent": "So in this talk you won't see much of mass for much of theory, but I'm going to show you how we can actually combine reinforcement learning models with behavioral experiments in order to study how external factors like stress or differential Geno type between different animals could lead to could affect their exploration exploitation.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first of all, let's have a look at the simple behavioral setup.",
                    "label": 0
                },
                {
                    "sent": "So we have a box.",
                    "label": 0
                },
                {
                    "sent": "In this box there is a hole where you know an animal can put a nose inside and there is a reserve at the top with the food.",
                    "label": 0
                },
                {
                    "sent": "And this food can be delivered down to the box at some point, and the animal is hungry and the idea is that in the beginning lights are off, but then when a trial starts, light is getting on and then at some point.",
                    "label": 0
                },
                {
                    "sent": "Animal responds by poking nose into a hole, and then as a result, it receives foot, which it can eat.",
                    "label": 0
                },
                {
                    "sent": "And of course it's happy.",
                    "label": 0
                },
                {
                    "sent": "And then at other time if, for example, if we don't have trial but in that range weather and the lights are off.",
                    "label": 0
                },
                {
                    "sent": "Then animal, if it makes a poke.",
                    "label": 0
                },
                {
                    "sent": "In that case it doesn't receive anything.",
                    "label": 0
                },
                {
                    "sent": "And of course it doesn't satisfy its goal.",
                    "label": 0
                },
                {
                    "sent": "So you say so.",
                    "label": 0
                },
                {
                    "sent": "The goal of animal in the stars because the total session times 10 minutes is to actually respond as quickly as possible to the trials when the light is on.",
                    "label": 0
                },
                {
                    "sent": "And of course, to waste as little energy as possible, but making posting into try interval.",
                    "label": 0
                },
                {
                    "sent": "And our question is how can we model learning and behavior in such task in computational framework?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And one of the usual ways to model such kind of conditioning and many other experiments is using simple temporal difference reinforcement learning models with discrete States and actions.",
                    "label": 1
                },
                {
                    "sent": "So for instance, in this case our states correspond to whether the lights are on, so we have a trial, or where the lights are offered and there's into interval and respect to animal position that corresponds to whether animals outside of this inside and here is just a detail we have to distinguish between starting your continue.",
                    "label": 0
                },
                {
                    "sent": "Suppose because in practice, animals need at least some time, which in our case is 2 time steps to actually deliver physically their word.",
                    "label": 0
                },
                {
                    "sent": "So for example, when trial start we move from here to here, and then at each point animal can choose whether to stay outside or making you poke, and then when it makes working, it can choose how quickly to return to the outside state.",
                    "label": 0
                },
                {
                    "sent": "So we sat at each point.",
                    "label": 0
                },
                {
                    "sent": "There is basically a choice of two different actions, and then once you have this formalization what we have.",
                    "label": 0
                },
                {
                    "sent": "To learn this so called curious which correspond to the expected future reward.",
                    "label": 0
                },
                {
                    "sent": "Said, if understate S action, A has been taken.",
                    "label": 0
                },
                {
                    "sent": "So here the reward is discounted with the temporal discount factor garment to future.",
                    "label": 0
                },
                {
                    "sent": "So this is standard TD stuff and we learn this curious with the famous temporal difference error which basically subtracts the discounted next value next value with the current value and add it with the observed towards and then.",
                    "label": 0
                },
                {
                    "sent": "We used this Delta T tempo difference error by multiplying with learning rate and this is the way how we simply updated kouvelis at each time step.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then, of course, such TTL models are capable of predicting many qualitative aspects of animal behavior, suggests animals learn based on rewards, or the extinguished learning if rewards are not present anymore.",
                    "label": 1
                },
                {
                    "sent": "But now question is, what about realistic aspects of animal behavior?",
                    "label": 0
                },
                {
                    "sent": "For example, what if some animals are more hungry than others?",
                    "label": 1
                },
                {
                    "sent": "How can we account for that?",
                    "label": 0
                },
                {
                    "sent": "Or what if some animals are exposed?",
                    "label": 1
                },
                {
                    "sent": "For example a cat?",
                    "label": 0
                },
                {
                    "sent": "Before the experiment, how would it affect their learning or in general, even if there are no extra factors?",
                    "label": 0
                },
                {
                    "sent": "What if some animals learn faster, others learn slower, so such things you can't really account just using it.",
                    "label": 0
                },
                {
                    "sent": "Generic reinforcement learning framework and the question is how can we do this thing?",
                    "label": 0
                },
                {
                    "sent": "Will still using our.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Simple live models.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that if we have some way how to dynamically control the meta parameters in their enforcement learning, perhaps very possible to achieve those rich class of different behaviors which animals actually produce in nature in practice.",
                    "label": 0
                },
                {
                    "sent": "So for example, the most relevant thing for workshop once we have those Q values which correspond to the future award predictions.",
                    "label": 0
                },
                {
                    "sent": "I mean, we still May either.",
                    "label": 0
                },
                {
                    "sent": "Exploit them more accurately so that, for example, if animals staying outside and it has to choose between action, stay continuously or move and start anew.",
                    "label": 0
                },
                {
                    "sent": "Poke for instance if the value of 1, one of the action is a bit higher than the other.",
                    "label": 0
                },
                {
                    "sent": "The probability that this action will be chosen is controlled by this expectation factor better.",
                    "label": 0
                },
                {
                    "sent": "So if it is higher than even a small differences in the Q values will actually lead to.",
                    "label": 0
                },
                {
                    "sent": "Always persistence strategy of choosing the hike valuable if it is low and even larger difference could be could correspond to quite random behavior.",
                    "label": 0
                },
                {
                    "sent": "So here is a standard problem of balancing exploration exploitation.",
                    "label": 0
                },
                {
                    "sent": "So at this point we have those three meter parameters learning great expectation faction reward this contact, which we interested to learn how we can balance them and how we can adjust them so that our model really.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sponsored realistic behavior and this is not only the model stuff, but there is there also hypothesis that this meta learning is related to certain parts in the brain, like for instance.",
                    "label": 0
                },
                {
                    "sent": "Interactivity of dopamine, serotonin or general in anothers corresponding to those meta parameters in some way like there's article bushels for dopamine which is quite famous and there is a hypothesis by DIA.",
                    "label": 0
                },
                {
                    "sent": "For the other new modulators, and certainly the neuromodulators can influence learning by, for example, by influencing heter synaptic plasticity which actually correspond simulate to the learning rules which we have in our enforcement learning.",
                    "label": 0
                },
                {
                    "sent": "And then if we have such kind of relation and if you want to know how for example stress or different genetic background influences learning influences brain processes, we can actually use an indirect method and we can try to sort of say see what differences in their enforcement remodel which as closely as possible resembles behavior we observe and then based on those differences we can infer what are actually different senior modulators which we would be able to test experimentally.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then so let's look at the outline of our approach.",
                    "label": 1
                },
                {
                    "sent": "So basically we used to derail models to model animal behavior in two different tasks.",
                    "label": 1
                },
                {
                    "sent": "One of them that are insured for you, the whole box and the other one, which is a bit more complicated, so I won't have time to go into detail.",
                    "label": 0
                },
                {
                    "sent": "the Morris Water maze where basically animal is dropped into a pool of cold water, usually called, and it has to find the hidden platform at certain point using the outside queue.",
                    "label": 0
                },
                {
                    "sent": "So it is.",
                    "label": 0
                },
                {
                    "sent": "The standard spatial learning task and here we can also manipulate things like for example the temperature of water as a stressor.",
                    "label": 0
                },
                {
                    "sent": "And then we formalize the behavior in these two experiments in some kind of reinforcement learning models.",
                    "label": 0
                },
                {
                    "sent": "And then we can compare the performance of these models for the animal behavior.",
                    "label": 0
                },
                {
                    "sent": "And of course, our goal is to find ways how we can control meta parameter values in order to have the best fit between these two things.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's look more distant to a file box experiments.",
                    "label": 0
                },
                {
                    "sent": "So we have hungry myself, two different genotypes.",
                    "label": 0
                },
                {
                    "sent": "One of them are the standard used by experimentalists, the Series 7 so called black mice and the other ones are DBA mice, which are very hyperactive, very impulsive and very anxious.",
                    "label": 0
                },
                {
                    "sent": "So they really behaving differently.",
                    "label": 0
                },
                {
                    "sent": "And then the experiment trial, as you saw, consists of lights getting on and then we wait for the response of the animal.",
                    "label": 0
                },
                {
                    "sent": "When it responds, it gets food and the lights go off.",
                    "label": 0
                },
                {
                    "sent": "And here we have 15 seconds into try interval and then there is alternation of those during a daily 10 minutes trial and because animals of course are different from models, we cannot teach animal in a single day.",
                    "label": 0
                },
                {
                    "sent": "So we have to repeat it our many different days.",
                    "label": 0
                },
                {
                    "sent": "So after the initial habitation that exposure to.",
                    "label": 0
                },
                {
                    "sent": "Set up we first train like.",
                    "label": 0
                },
                {
                    "sent": "Using all groups for all groups of animals using the same type of protocol, and then for the later four days we apply different stress.",
                    "label": 0
                },
                {
                    "sent": "Condition says we could we could study the effects of stress and then also we make a break of almost one month to see how much animals would forget or how much they would consolidate the memory and then we can train and begin in the second block at the same time seeing basically how much they remember how much they.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Out of this.",
                    "label": 0
                },
                {
                    "sent": "OK, so once we have this experiment then we have to find a way how to evaluate the performance for both the animals and the model.",
                    "label": 1
                },
                {
                    "sent": "So a simple way.",
                    "label": 0
                },
                {
                    "sent": "A simple way is.",
                    "label": 0
                },
                {
                    "sent": "For example, here we have this 10 minutes trial and we can calculate response time for each trial like 12 seconds, 9 seconds, 20 seconds having a calculate our own mean and then you see this corresponds to the value for the first day and then if we draw over this you see a decrease like in the first inning bug.",
                    "label": 0
                },
                {
                    "sent": "Then there's a break.",
                    "label": 0
                },
                {
                    "sent": "There is a little increase and then it decreases further.",
                    "label": 0
                },
                {
                    "sent": "We can also look at other performance measures called PMS as how many trials it made during those 10 minutes.",
                    "label": 0
                },
                {
                    "sent": "Any wrong in the trial pokes it made and so on.",
                    "label": 0
                },
                {
                    "sent": "And then once we have this, we can actually calculate the same things for the model as well As for the real animal, and then using a variation of stochastic gradient descent algorithm we can estimate the meta parameter sets Alpha, beta, gamma for each session and each animal in a way that minimizes the following Chi Square chi squared goodness fit.",
                    "label": 1
                },
                {
                    "sent": "So basically here is a difference between exponential performance measure and model given following parameters.",
                    "label": 0
                },
                {
                    "sent": "We were variance and summing up overall and we want to get this as small as possible.",
                    "label": 0
                },
                {
                    "sent": "Ideal that it won't be a significant difference between the two.",
                    "label": 0
                },
                {
                    "sent": "And here you see one example.",
                    "label": 0
                },
                {
                    "sent": "When we estimate here, we get how our model actually performs.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, and now we estimated those meta parameters and what is next.",
                    "label": 1
                },
                {
                    "sent": "So first is it should be good to check generalization whether our estimation generalizes well.",
                    "label": 1
                },
                {
                    "sent": "Also have goodness fit is good enough.",
                    "label": 0
                },
                {
                    "sent": "Then it's also to see the stability and reliability of those math parameters.",
                    "label": 1
                },
                {
                    "sent": "We can generate artificial behavior, but for example taking certain Department sets, generating behavior and write him back, estimate the parameters and to see if we get the same thing between the two.",
                    "label": 0
                },
                {
                    "sent": "So you've seen this graph at the difference actually smaller.",
                    "label": 0
                },
                {
                    "sent": "Especially for exploitation factor, you really get quite accurate estimations.",
                    "label": 0
                },
                {
                    "sent": "And then we can ask questions like how do meta parameters change with time or what are the differences between diff.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "X men to groups.",
                    "label": 0
                },
                {
                    "sent": "So today we're just going to have a look at the exploitation factors better, how?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The change.",
                    "label": 0
                },
                {
                    "sent": "And the basic result that we see is that doing a process of learning during a single block.",
                    "label": 0
                },
                {
                    "sent": "This exploitation factors clearly increase for both groups of animals.",
                    "label": 1
                },
                {
                    "sent": "And there is a decrease and then increase further during 2nd learning block and there is no such thing for learning rates for example, which would be maybe expecting some algorithmic terms.",
                    "label": 1
                },
                {
                    "sent": "So we could say that with a decreasing Delta reward prediction errors those expectation factors better increase with time.",
                    "label": 0
                },
                {
                    "sent": "So this actually corresponds to theoretical idea which was suggesting many papers with.",
                    "label": 0
                },
                {
                    "sent": "To show that animals in fact behave like what theory would suggest should be optimal.",
                    "label": 1
                },
                {
                    "sent": "And then in a thing that we see is that those anxious DBA mice show much lower expectation factor consistently than the normal seated 7 mice, and this would actually suggests that the reknown, the impairment in the performance of these mice is probably due to the lack of attention towards particular task, particular similar and not that we cannot learn the things, but they cannot exploit and perform accurately what they have already learned.",
                    "label": 0
                },
                {
                    "sent": "And also in a nice thing if we look at the results for the Water maze experiment, we have the same qualitative result.",
                    "label": 1
                },
                {
                    "sent": "Basically we have increased in both cases and we have the series.",
                    "label": 0
                },
                {
                    "sent": "7 miles have higher expectation factors then the DBA, mice and the same things actually are very similar for other prime.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The parameters well, and now if you want to look finally the effects of stress with stress is not so simple, but we see also in both experiments that those expedition factors better are elevated upon exposure to stress for the normal mice for season seven, but there is not significant no significant increase for the anxious mice.",
                    "label": 1
                },
                {
                    "sent": "And if you ask why could that be the case?",
                    "label": 0
                },
                {
                    "sent": "One of the biological possible explanations is that there is a so say hi political relation of this expectation factor to.",
                    "label": 0
                },
                {
                    "sent": "Sonic noradrenaline levels in the brain.",
                    "label": 0
                },
                {
                    "sent": "And we know that these mice are more anxious and their higher stress hormone levels and higher modeling levels.",
                    "label": 0
                },
                {
                    "sent": "And there is a hypothesis that actually not generally is related to performance accuracy and that under intermediate levels we can achieve best performance because it leads animals to focus their attention where at low levels are not sufficiently alerted at the high levels.",
                    "label": 0
                },
                {
                    "sent": "We have kind of distracted scanning attention.",
                    "label": 0
                },
                {
                    "sent": "And so you see, in this case that because black animals on the left side of the curve having additional stress, which helps them to increase their focus of attention, while these animals, they're ready to anchor, the too distracted with anything, and adding additional stress really doesn't help for them.",
                    "label": 0
                },
                {
                    "sent": "So this is just example, how can relate it to biological data.",
                    "label": 0
                },
                {
                    "sent": "And then of course we can apply, for example, agonist or antagonist self modeling system to study if we would change things and we estimate the parameters again, if we really.",
                    "label": 0
                },
                {
                    "sent": "Establish a relation as we think it could be.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So thanks a lot, that's joint work with Jeremy Kinessa.",
                    "label": 1
                },
                {
                    "sent": "Initially question my supervisors come in sign language from Gessner, and for those who want to discuss more and the future directions of our work, he should commit post later in the afternoon.",
                    "label": 0
                },
                {
                    "sent": "Experience, regret.",
                    "label": 0
                },
                {
                    "sent": "Well, I mean I don't know it could be, it could be.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}