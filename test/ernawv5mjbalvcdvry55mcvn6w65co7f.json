{
    "id": "ernawv5mjbalvcdvry55mcvn6w65co7f",
    "title": "The KL-UCB Algorithm for Bounded Stochastic Bandits and Beyond",
    "info": {
        "author": [
            "Aur\u00e9lien Garivier, CNRS - LTCI UMR 5141 Telecom ParisTech"
        ],
        "published": "Aug. 2, 2011",
        "recorded": "July 2011",
        "category": [
            "Top->Computer Science->Algorithms and Data Structures"
        ]
    },
    "url": "http://videolectures.net/colt2011_garivier_bandits/",
    "segmentation": [
        [
            "Good morning everybody.",
            "I'm already getting.",
            "Yeah will present you a joint work with Olivia Cafe."
        ],
        [
            "And as the first talk in the abandoned session, I will review some basic properties of bandit so that the entire talk should be understandable by everybody, I think."
        ],
        [
            "Actually I will start by detailing the model we consider here.",
            "It's a special mother of reinforcement learning, but it's the simplest you can imagine.",
            "Reinforcement learning is the setting where an agent interacts with an environment in the way that he takes actions every discrete time and he received a certain reward.",
            "Normally in general or enforcement learning, there's also a notion of state, but here in bandit the specificity is that there is no state that just one state.",
            "However, bandit problems already contained this exploration exploitation dilemma that Java exposed yesterday, and as we as we will see, the agent has to try every possibility sufficiently often and yet to take advantage of the knowledge he has accumulated on the performance of each action.",
            "This setting is different from statistical learning in its goal.",
            "In it, same as says.",
            "Goal is not to do estimation, but the goal is to maximize the rewards that you don't want to precisely estimate the quality of interaction you really want to take the best decisions, and it's also different from game theory insofar that in my talk I will only be concerned with stochastic bandits, and so you don't.",
            "We don't play against a devil or nature, we play against an environment which is completely oblivious, completely stochastic."
        ],
        [
            "Alright, precisely, we shall consider finite finite number of arms, and each arm is a sequence of IID random variables.",
            "But we don't observe all of them at each time.",
            "You just choose one of the arms and you get the corresponding reward, the objective being to maximize accumulated receive reward.",
            "Each arm is a characterized by some probability distribution.",
            "And the family of distributions of each arm can be either parametric family or a non parametric family.",
            "In this talk we shall be concerned with two settings.",
            "First, we shall consider general bounded rewards, which is the kind of general modern way things are considered for approximately 10 years, and but we shall also be concerned with the Canonical exponential family as an example of a different setting with possibly non bonded.",
            "Non bolded rewards.",
            "As an example, if you're not familiar with that, you can think of Bernoulli Rewards.",
            "So each time you get you, you choose and, um, you get either one or zero.",
            "You can think of this problem as a.",
            "The emblematic example is maybe clinical trials.",
            "So you have several treatments to treat some patients, and you must attribute them each of them one.",
            "You have the right only to attribute them one treatment, and you see if he lives or if he dies and the objective is of course to save the largest possible number of patients you don't want to evaluate the quality of each treatment, but you really want to save as many as.",
            "Possible.",
            "What can you do?",
            "Well, you can choose a strategy and what we will call a strategy in very simple terms.",
            "It's just a way to react to the past observations.",
            "So you choose at time T and action EI sub T which is just a function of what rewards you you had."
        ],
        [
            "In the past, and how do you evaluate such a strategy?",
            "Actually, you want to maximize accumulated rewards, so you choose piece as to maximize in expectation this expectation and this expectation can easily be rewritten as a convex combination as a combination of the quality of each treatment times the number of the expected number of times that you will play is that you will give.",
            "Each treatment so that the important quantity, as we will see, is the expected number.",
            "If each action is played.",
            "And we will denote in this entire talk.",
            "MU surveys are expected number of the expected reward when you choose Action A.",
            "So as can readily be seen.",
            "Cumulative reward maximization is equivalent to regret minimization.",
            "Well, actually any reasonable strategy will in the long run work as well as the best treatments though right quantity to consider in order to evaluate your strategy is the difference between what you would have done with you, what reward you would have accumulating using always the best treatment.",
            "The difference between that quantity that Oracle quantity and can say and what you really did, and so the quantity we shall have to minimize over P over the strategy P is this strategy.",
            "Is this quantities difference of efficiency times the number of times multiplied by the number of times you played each arm?"
        ],
        [
            "Actually, this is a very simple programs.",
            "In simplest one can imagine in statistical learning, so quite precise things are known about this and well, we shall call consistent strategy, which in the long run works as well as the best treatment, and we shall call it efficient if the regret the quantity I just introduced is a sub polynomial.",
            "Say it's quite easy to design efficient strategies.",
            "And there's a lower bound on the performance of each efficient strategy."
        ],
        [
            "In the long run, this lower bound is initially due to to lie Ann Robbins.",
            "Actually, you should remain.",
            "Keep in mind this quantity the regret is lower bounded as log N times visum.",
            "Visum is the quotient of the difference of efficiency divided by some quantity which appears here and which is the kullback library divergance.",
            "Between distribution P data and PT to prime.",
            "So this result was proved first in a parametric sitting where distributions were one parameter were parameterized by one parameter.",
            "The result was further generalized much later, but you can keep in mind for this example the battery case.",
            "While this function is simply the binary diversions function that everybody knows.",
            "So I first presented a lower bound.",
            "Now I shall present I."
        ],
        [
            "These algorithms, which will try to attain."
        ],
        [
            "This lower bound and among I will focus on algorithms called optimistic optimism.",
            "Is you ristic?",
            "It's a motor which tells you to play as if the environment was the most favorable among all environments that are sufficiently likely given the observations accumulated so far.",
            "So this seems to be a very, very general thing, and actually it can be applied in very different settings, and it appears to give.",
            "Algorithms which are robust, efficient, easy to implement, and often easy to interpret.",
            "So lots of qualities for these algorithms."
        ],
        [
            "What does this give in our setting?",
            "Well, the gives well known algorithm, UCB, which was introduced by people in this conference and which country which constructs an upper bound on the expected reward of each arm.",
            "So it's very easy to understand what it does.",
            "You have the estimated reward using ARM a which you don't choose, which you just do not plugged into the.",
            "Procedure it would be a very bad idea only to trust the estimates, but the good idea is to inflate the estimate with an exploration bonus which is inversely proportional to the number of times you play the you played the arm, and using this strategy.",
            "Actually you can show that.",
            "The regret where we will see that this strategy has a good performance.",
            "So this strategy is called an index strategy and you know by Asian version of this problem index strategies are shown to be optimal.",
            "So actually your decision rule is very easy.",
            "You just look at each are man.",
            "You associate an index to each arm and you choose the arm with highest index.",
            "This is called an index strategy.",
            "It has actually very easily interpretable and intuitive behavior.",
            "Maybe we share."
        ],
        [
            "We can illustrate this behavior or.",
            "On this little video here, for simplicity I I took binary rewards with five arms.",
            "The performance of each arm is the red point, so this one is the best.",
            "We started by playing each time one each arm one time, and then we let the algorithm play.",
            "We show the confidence intervals and as I just said, the algorithm chooses the harm.",
            "The arm with highest author.",
            "With highest with the highest top of the confidence intervals which has the automatic effect to equalize the tops of the intervals, and so you can see that in the long run what happens is that the tops are the same man for the intervals too.",
            "If they always contain the red points, and if the top is always the same, then you have played lots of times the best arm and very rarely the bad ones."
        ],
        [
            "The performance of the algorithm can be analyzed using exactly the kind of ideas that I just that can be seen in this.",
            "In this video, 401 rewards the regret of UCB can be shown to be of upper bounded by quantity of other lock N and more precisely, well, this ones are actually completely a finite time and non aseptic.",
            "But for the ease of the presentation I showed me.",
            "In that in that form, the expectation of the reward if order log N times this quantity."
        ],
        [
            "If you think of the lower bound, well, this is not far, but this is not exactly the good quantity we would like.",
            "This is not the quantity of the lower."
        ],
        [
            "Bound and actually to obtain this coefficient 1/2, here is something not completely easy, yes, so in the Bernoulli case, the right hand side is greater than what is suggested by the lion Urbanspoon.",
            "So this is not optimal in the binary case and one could think this is suboptimal.",
            "Also in the general bounded case, many variants have been proposed to improve on this algorithm and in particular variance sinking into account.",
            "The variance, this second moment of the reward distribution.",
            "Actually this can be readily felt as UCB uses this exploration bonus, which comes from having time inequality.",
            "People thought well we will do better by using Bernstein's inequality.",
            "This is not completely true, actually, for reasons that are a bit explaining the papers, they usually the second term in the bench to inequality is neglectable.",
            "But here we are in the in a very special case when he does not necessarily vanish, and This is why the idea of Bernstein is not is not completely satisfying."
        ],
        [
            "In that in that view."
        ],
        [
            "What we propose for an algorithm is actually a very very old ID.",
            "We propose to compute the upper bound little differently.",
            "We propose to use the KLZ callback library.",
            "Versions of Bernoulli random variables, and in a way to choose the upper bound by minimizing this distance are maybe I will show."
        ],
        [
            "On one slide, and this is the only slide in my presentation.",
            "When you should think a little which contains a little of mathematics but very very elementary.",
            "We show why this upper bounds for boundary variables can be constructed that way.",
            "So if you take IID Bernoulli variables of parameter T to zero, and if you ask what is the probability that the current mean is smaller than X."
        ],
        [
            "Then you can apply this famous channel bound to obtain that this is lower than this exponential quality quantity.",
            "In other words, if you set this quantity to be exponential to be Alpha, then the probability that the estimated mean comes lower than X is upper bounded by by Alpha.",
            "If you look at this inequality in the other direction, if you are given this estimate that you wonder what the real parameter is.",
            "Readily gives you a confidence interval.",
            "The confidence interval in the corresponding confidence interval.",
            "Is to take at least zero.",
            "The upper bound is to take the largest value of you such that the distance between P&U is smaller than the log of the error you would like divided by the number of observations."
        ],
        [
            "So what they did here I did it.",
            "I did the entire reason Mentan boundary viable and what I want at first said is that I didn't want to focus on binary variable, but I wanted to treat all bounded variables bounded means by rescaling you can reschedule them between zero and one.",
            "So why focus on binary variables in a way everyone knows why this is because they maximize divisions among all bounded variables with the given expectation.",
            "This is probably something everyone knows about the variance.",
            "You know that a random variables taking values in 01 has a smaller variance than the corresponding binary variable with the same expectation.",
            "This is actually also true for the exponential moments, and so it is also true for the kind of bounce you obtained by the chroma chernof."
        ],
        [
            "Effort.",
            "So using this and using a special a analysis of the algorithm, you can show that the number of times this KL UCB algorithm plays a suboptimal arm is upper bounded by exactly the right quantities of you.",
            "We obtained the same kind of burns As for the plane UCB algorithm, But this time with the right with the right constant matching the lower bound in the binary case.",
            "But this bound, I repeat, is true for.",
            "All bounded rewards in as interval 01, just it's asymptotically optimal in the binary case."
        ],
        [
            "Maybe 1 slide to show the comparison between the terms that are used in the plane.",
            "Eusebian scale.",
            "UCB algorithm.",
            "This comparison is just an illustration of the Pinsker inequality.",
            "Actually in blue you have the calendar divergance in red you have the.",
            "What you are doing with herding inequality, so the quadratic.",
            "Approximation, as you can see when it comes to high or low levels, the difference can be quite huge.",
            "This scale diversions takes into account in particular the variance of the process and even."
        ],
        [
            "A little more.",
            "The main tool for this proof is division inequality.",
            "For safe normalized samples.",
            "Kind of quantities Chabot talked about, but in a much more general setting yesterday.",
            "Here we have to we have a random number of terms and we want to quantify the distance between U offensive upper bound we constructed using the equal back Libra Divergance and the true parameter.",
            "And while taking care of her victim.",
            "Quite precisely, we obtained that this kind of inequality.",
            "Maybe we use this one in the proof.",
            "Maybe this one is most useful in general, actually it appears to be useful in very different settings.",
            "The probability that the normalized this normalized quantities in normalized KL divergent between the estimating the true parameter.",
            "When I say normalize that I mean multiplied by its order of decreasing to 0.",
            "So multiplied by the number of observations.",
            "So probability that this quantity be greater than some Delta is essentially exponential minus Delta times the term that comes from the fact that you have a random number."
        ],
        [
            "Victims.",
            "Maybe show a few a few, a few results to show that actually the improvement can be quite huge even in simple examples we took.",
            "Most of the well known variants of the Eusebian and other algorithms available for stochastic bandits, and we wanted to do a comparison.",
            "The first setting is a is trivial.",
            "You take just two arms.",
            "One is a little bit better than the other one.",
            "And, well, what you observe is that UCB works well, but quite a lot more can be done in this setting.",
            "The worst the worst algorithm in that case appear to be USV.",
            "UCB for the reason I gave you first, that's the neglectable termine.",
            "Bernstein is not at all neglectable ads on this at this scale, so UCKLUCV appears to do as we expected as we hoped.",
            "Actually, we also introduced in the comparison the algorithm UCB tuned with, which is a variant in the initial paper, for which there is no theoretical guarantees, and this does quite well in expectation, but with very large.",
            "Risk actually.",
            "It's sometimes a bit difficult to evaluate bandit processes, as the regret appears to be very, very skewed, so we had to do quite a lot of replications.",
            "Actually, if you do less replications at the horizon, you have your, your results may be quite subject to controversy.",
            "And this is because, as you can see, so regret is very small in expectation, but appears sometimes to be very very large.",
            "So think of a deviation inequality for variables taking value in in."
        ],
        [
            "Well, you know larger interval you need a lot of observations but then arms scenario with low rewards, low rewards are particularly motivated by applications of this bandit problems in the Internet versus assessments.",
            "For instance, where the difference actually goes larger and larger.",
            "With this slide is to insist on the fact that there is a slight improvement that can be given to these algorithms using the idea of the most algorithm to change the confidence term divided divided by the number of observations.",
            "I."
        ],
        [
            "I also gave an explanation example with the truncated exponential to show that the algorithm is absolutely not specific to binary variables.",
            "Where were algorithms peers to be beaten by use?"
        ],
        [
            "To be tuned but there is a."
        ],
        [
            "Revenge actually what we did for bandori variables can as well be done for other other families of probabilities, say for instance probabilities with a given Canonical exponential model, the algorithm stays exactly the same, just the definition of the Klu function in the algorithm changes.",
            "For instance, if you have exponential rewards, you think this divergent Bregman divergences.",
            "If you have question distributions, you have another.",
            "Bregman divergences anyway, this case algorithms and it gives corresponding bounds.",
            "Or obtaining that is a lower boundary or reaching in the long run."
        ],
        [
            "The lower bounds though these reasons for exponential rewards comparison with UCB tune this time is in our favor because they can be exponential algorithms is variant take adapted for exponential rewards.",
            "Does well in mean and actually it mean you see between does bad, but as you can see most of the time it is very very efficient.",
            "This is the central regions is the central 20%, so regions just deviations become very very huge as the algorithm is not sufficiently present."
        ],
        [
            "So 2 words of conclusion first.",
            "One should use KL, UCB rather than you see one or is to beat you as he has exactly the same range of applications and it does strictly better in every case.",
            "There is a method can be adapted in a family of exponential rewards, but can it be adapted to other families of rewards?",
            "Well, if you want to answer some to have some answer to this question, just listen to the next talk and then can this idea of.",
            "Considering KL deviation bounds be used in useful in other settings, yes it can.",
            "It is actually in very very different settings for Markov memory estimation, but it is also more generally within a reinforcement.",
            "Learning the same kind of findings can be used to estimate to use the kind of to analyze these kind of algorithms that shabba presented yesterday in his second talk in a finite setting then.",
            "Using Kaldahl versions instead of instead of L1 norm in neighborhoods leads to qualitatively different algorithms which have a lot of nice property."
        ],
        [
            "So thank you for your attention.",
            "So can you come back to you deviation inequality?"
        ],
        [
            "OK, So what do you use to prove that do use a maximal Chaffin peeling argument?",
            "Or do you use more than that?",
            "We know that's exactly what what it is.",
            "I gave a first hint of the proof in this slide actually, but then the the difficulty is to obtain something that is true with a random number of summons so to to do this correctly rather than using a union bound directly, you just group the number of occurrences that appeared to be on the same scale together so as to use less union bound.",
            "In a way the user a union bound on less.",
            "Events this is how you obtain here the lock and dam that allows you to obtain the good constants.",
            "Then in the in the analysis.",
            "Any other question?",
            "OK, so let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good morning everybody.",
                    "label": 0
                },
                {
                    "sent": "I'm already getting.",
                    "label": 0
                },
                {
                    "sent": "Yeah will present you a joint work with Olivia Cafe.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as the first talk in the abandoned session, I will review some basic properties of bandit so that the entire talk should be understandable by everybody, I think.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actually I will start by detailing the model we consider here.",
                    "label": 0
                },
                {
                    "sent": "It's a special mother of reinforcement learning, but it's the simplest you can imagine.",
                    "label": 0
                },
                {
                    "sent": "Reinforcement learning is the setting where an agent interacts with an environment in the way that he takes actions every discrete time and he received a certain reward.",
                    "label": 1
                },
                {
                    "sent": "Normally in general or enforcement learning, there's also a notion of state, but here in bandit the specificity is that there is no state that just one state.",
                    "label": 0
                },
                {
                    "sent": "However, bandit problems already contained this exploration exploitation dilemma that Java exposed yesterday, and as we as we will see, the agent has to try every possibility sufficiently often and yet to take advantage of the knowledge he has accumulated on the performance of each action.",
                    "label": 0
                },
                {
                    "sent": "This setting is different from statistical learning in its goal.",
                    "label": 0
                },
                {
                    "sent": "In it, same as says.",
                    "label": 0
                },
                {
                    "sent": "Goal is not to do estimation, but the goal is to maximize the rewards that you don't want to precisely estimate the quality of interaction you really want to take the best decisions, and it's also different from game theory insofar that in my talk I will only be concerned with stochastic bandits, and so you don't.",
                    "label": 0
                },
                {
                    "sent": "We don't play against a devil or nature, we play against an environment which is completely oblivious, completely stochastic.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, precisely, we shall consider finite finite number of arms, and each arm is a sequence of IID random variables.",
                    "label": 0
                },
                {
                    "sent": "But we don't observe all of them at each time.",
                    "label": 0
                },
                {
                    "sent": "You just choose one of the arms and you get the corresponding reward, the objective being to maximize accumulated receive reward.",
                    "label": 0
                },
                {
                    "sent": "Each arm is a characterized by some probability distribution.",
                    "label": 0
                },
                {
                    "sent": "And the family of distributions of each arm can be either parametric family or a non parametric family.",
                    "label": 1
                },
                {
                    "sent": "In this talk we shall be concerned with two settings.",
                    "label": 0
                },
                {
                    "sent": "First, we shall consider general bounded rewards, which is the kind of general modern way things are considered for approximately 10 years, and but we shall also be concerned with the Canonical exponential family as an example of a different setting with possibly non bonded.",
                    "label": 0
                },
                {
                    "sent": "Non bolded rewards.",
                    "label": 0
                },
                {
                    "sent": "As an example, if you're not familiar with that, you can think of Bernoulli Rewards.",
                    "label": 0
                },
                {
                    "sent": "So each time you get you, you choose and, um, you get either one or zero.",
                    "label": 0
                },
                {
                    "sent": "You can think of this problem as a.",
                    "label": 0
                },
                {
                    "sent": "The emblematic example is maybe clinical trials.",
                    "label": 0
                },
                {
                    "sent": "So you have several treatments to treat some patients, and you must attribute them each of them one.",
                    "label": 0
                },
                {
                    "sent": "You have the right only to attribute them one treatment, and you see if he lives or if he dies and the objective is of course to save the largest possible number of patients you don't want to evaluate the quality of each treatment, but you really want to save as many as.",
                    "label": 0
                },
                {
                    "sent": "Possible.",
                    "label": 0
                },
                {
                    "sent": "What can you do?",
                    "label": 0
                },
                {
                    "sent": "Well, you can choose a strategy and what we will call a strategy in very simple terms.",
                    "label": 0
                },
                {
                    "sent": "It's just a way to react to the past observations.",
                    "label": 0
                },
                {
                    "sent": "So you choose at time T and action EI sub T which is just a function of what rewards you you had.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the past, and how do you evaluate such a strategy?",
                    "label": 0
                },
                {
                    "sent": "Actually, you want to maximize accumulated rewards, so you choose piece as to maximize in expectation this expectation and this expectation can easily be rewritten as a convex combination as a combination of the quality of each treatment times the number of the expected number of times that you will play is that you will give.",
                    "label": 1
                },
                {
                    "sent": "Each treatment so that the important quantity, as we will see, is the expected number.",
                    "label": 0
                },
                {
                    "sent": "If each action is played.",
                    "label": 0
                },
                {
                    "sent": "And we will denote in this entire talk.",
                    "label": 0
                },
                {
                    "sent": "MU surveys are expected number of the expected reward when you choose Action A.",
                    "label": 1
                },
                {
                    "sent": "So as can readily be seen.",
                    "label": 1
                },
                {
                    "sent": "Cumulative reward maximization is equivalent to regret minimization.",
                    "label": 0
                },
                {
                    "sent": "Well, actually any reasonable strategy will in the long run work as well as the best treatments though right quantity to consider in order to evaluate your strategy is the difference between what you would have done with you, what reward you would have accumulating using always the best treatment.",
                    "label": 0
                },
                {
                    "sent": "The difference between that quantity that Oracle quantity and can say and what you really did, and so the quantity we shall have to minimize over P over the strategy P is this strategy.",
                    "label": 0
                },
                {
                    "sent": "Is this quantities difference of efficiency times the number of times multiplied by the number of times you played each arm?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, this is a very simple programs.",
                    "label": 0
                },
                {
                    "sent": "In simplest one can imagine in statistical learning, so quite precise things are known about this and well, we shall call consistent strategy, which in the long run works as well as the best treatment, and we shall call it efficient if the regret the quantity I just introduced is a sub polynomial.",
                    "label": 0
                },
                {
                    "sent": "Say it's quite easy to design efficient strategies.",
                    "label": 0
                },
                {
                    "sent": "And there's a lower bound on the performance of each efficient strategy.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the long run, this lower bound is initially due to to lie Ann Robbins.",
                    "label": 0
                },
                {
                    "sent": "Actually, you should remain.",
                    "label": 0
                },
                {
                    "sent": "Keep in mind this quantity the regret is lower bounded as log N times visum.",
                    "label": 1
                },
                {
                    "sent": "Visum is the quotient of the difference of efficiency divided by some quantity which appears here and which is the kullback library divergance.",
                    "label": 0
                },
                {
                    "sent": "Between distribution P data and PT to prime.",
                    "label": 0
                },
                {
                    "sent": "So this result was proved first in a parametric sitting where distributions were one parameter were parameterized by one parameter.",
                    "label": 0
                },
                {
                    "sent": "The result was further generalized much later, but you can keep in mind for this example the battery case.",
                    "label": 0
                },
                {
                    "sent": "While this function is simply the binary diversions function that everybody knows.",
                    "label": 0
                },
                {
                    "sent": "So I first presented a lower bound.",
                    "label": 1
                },
                {
                    "sent": "Now I shall present I.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "These algorithms, which will try to attain.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This lower bound and among I will focus on algorithms called optimistic optimism.",
                    "label": 0
                },
                {
                    "sent": "Is you ristic?",
                    "label": 0
                },
                {
                    "sent": "It's a motor which tells you to play as if the environment was the most favorable among all environments that are sufficiently likely given the observations accumulated so far.",
                    "label": 0
                },
                {
                    "sent": "So this seems to be a very, very general thing, and actually it can be applied in very different settings, and it appears to give.",
                    "label": 0
                },
                {
                    "sent": "Algorithms which are robust, efficient, easy to implement, and often easy to interpret.",
                    "label": 0
                },
                {
                    "sent": "So lots of qualities for these algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What does this give in our setting?",
                    "label": 0
                },
                {
                    "sent": "Well, the gives well known algorithm, UCB, which was introduced by people in this conference and which country which constructs an upper bound on the expected reward of each arm.",
                    "label": 0
                },
                {
                    "sent": "So it's very easy to understand what it does.",
                    "label": 0
                },
                {
                    "sent": "You have the estimated reward using ARM a which you don't choose, which you just do not plugged into the.",
                    "label": 0
                },
                {
                    "sent": "Procedure it would be a very bad idea only to trust the estimates, but the good idea is to inflate the estimate with an exploration bonus which is inversely proportional to the number of times you play the you played the arm, and using this strategy.",
                    "label": 0
                },
                {
                    "sent": "Actually you can show that.",
                    "label": 0
                },
                {
                    "sent": "The regret where we will see that this strategy has a good performance.",
                    "label": 0
                },
                {
                    "sent": "So this strategy is called an index strategy and you know by Asian version of this problem index strategies are shown to be optimal.",
                    "label": 0
                },
                {
                    "sent": "So actually your decision rule is very easy.",
                    "label": 0
                },
                {
                    "sent": "You just look at each are man.",
                    "label": 0
                },
                {
                    "sent": "You associate an index to each arm and you choose the arm with highest index.",
                    "label": 0
                },
                {
                    "sent": "This is called an index strategy.",
                    "label": 0
                },
                {
                    "sent": "It has actually very easily interpretable and intuitive behavior.",
                    "label": 0
                },
                {
                    "sent": "Maybe we share.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can illustrate this behavior or.",
                    "label": 0
                },
                {
                    "sent": "On this little video here, for simplicity I I took binary rewards with five arms.",
                    "label": 0
                },
                {
                    "sent": "The performance of each arm is the red point, so this one is the best.",
                    "label": 1
                },
                {
                    "sent": "We started by playing each time one each arm one time, and then we let the algorithm play.",
                    "label": 0
                },
                {
                    "sent": "We show the confidence intervals and as I just said, the algorithm chooses the harm.",
                    "label": 1
                },
                {
                    "sent": "The arm with highest author.",
                    "label": 1
                },
                {
                    "sent": "With highest with the highest top of the confidence intervals which has the automatic effect to equalize the tops of the intervals, and so you can see that in the long run what happens is that the tops are the same man for the intervals too.",
                    "label": 0
                },
                {
                    "sent": "If they always contain the red points, and if the top is always the same, then you have played lots of times the best arm and very rarely the bad ones.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The performance of the algorithm can be analyzed using exactly the kind of ideas that I just that can be seen in this.",
                    "label": 0
                },
                {
                    "sent": "In this video, 401 rewards the regret of UCB can be shown to be of upper bounded by quantity of other lock N and more precisely, well, this ones are actually completely a finite time and non aseptic.",
                    "label": 0
                },
                {
                    "sent": "But for the ease of the presentation I showed me.",
                    "label": 0
                },
                {
                    "sent": "In that in that form, the expectation of the reward if order log N times this quantity.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you think of the lower bound, well, this is not far, but this is not exactly the good quantity we would like.",
                    "label": 0
                },
                {
                    "sent": "This is not the quantity of the lower.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bound and actually to obtain this coefficient 1/2, here is something not completely easy, yes, so in the Bernoulli case, the right hand side is greater than what is suggested by the lion Urbanspoon.",
                    "label": 1
                },
                {
                    "sent": "So this is not optimal in the binary case and one could think this is suboptimal.",
                    "label": 0
                },
                {
                    "sent": "Also in the general bounded case, many variants have been proposed to improve on this algorithm and in particular variance sinking into account.",
                    "label": 0
                },
                {
                    "sent": "The variance, this second moment of the reward distribution.",
                    "label": 0
                },
                {
                    "sent": "Actually this can be readily felt as UCB uses this exploration bonus, which comes from having time inequality.",
                    "label": 0
                },
                {
                    "sent": "People thought well we will do better by using Bernstein's inequality.",
                    "label": 0
                },
                {
                    "sent": "This is not completely true, actually, for reasons that are a bit explaining the papers, they usually the second term in the bench to inequality is neglectable.",
                    "label": 0
                },
                {
                    "sent": "But here we are in the in a very special case when he does not necessarily vanish, and This is why the idea of Bernstein is not is not completely satisfying.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In that in that view.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we propose for an algorithm is actually a very very old ID.",
                    "label": 0
                },
                {
                    "sent": "We propose to compute the upper bound little differently.",
                    "label": 0
                },
                {
                    "sent": "We propose to use the KLZ callback library.",
                    "label": 0
                },
                {
                    "sent": "Versions of Bernoulli random variables, and in a way to choose the upper bound by minimizing this distance are maybe I will show.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "On one slide, and this is the only slide in my presentation.",
                    "label": 0
                },
                {
                    "sent": "When you should think a little which contains a little of mathematics but very very elementary.",
                    "label": 0
                },
                {
                    "sent": "We show why this upper bounds for boundary variables can be constructed that way.",
                    "label": 0
                },
                {
                    "sent": "So if you take IID Bernoulli variables of parameter T to zero, and if you ask what is the probability that the current mean is smaller than X.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then you can apply this famous channel bound to obtain that this is lower than this exponential quality quantity.",
                    "label": 0
                },
                {
                    "sent": "In other words, if you set this quantity to be exponential to be Alpha, then the probability that the estimated mean comes lower than X is upper bounded by by Alpha.",
                    "label": 1
                },
                {
                    "sent": "If you look at this inequality in the other direction, if you are given this estimate that you wonder what the real parameter is.",
                    "label": 0
                },
                {
                    "sent": "Readily gives you a confidence interval.",
                    "label": 0
                },
                {
                    "sent": "The confidence interval in the corresponding confidence interval.",
                    "label": 0
                },
                {
                    "sent": "Is to take at least zero.",
                    "label": 0
                },
                {
                    "sent": "The upper bound is to take the largest value of you such that the distance between P&U is smaller than the log of the error you would like divided by the number of observations.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what they did here I did it.",
                    "label": 0
                },
                {
                    "sent": "I did the entire reason Mentan boundary viable and what I want at first said is that I didn't want to focus on binary variable, but I wanted to treat all bounded variables bounded means by rescaling you can reschedule them between zero and one.",
                    "label": 0
                },
                {
                    "sent": "So why focus on binary variables in a way everyone knows why this is because they maximize divisions among all bounded variables with the given expectation.",
                    "label": 0
                },
                {
                    "sent": "This is probably something everyone knows about the variance.",
                    "label": 0
                },
                {
                    "sent": "You know that a random variables taking values in 01 has a smaller variance than the corresponding binary variable with the same expectation.",
                    "label": 0
                },
                {
                    "sent": "This is actually also true for the exponential moments, and so it is also true for the kind of bounce you obtained by the chroma chernof.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Effort.",
                    "label": 0
                },
                {
                    "sent": "So using this and using a special a analysis of the algorithm, you can show that the number of times this KL UCB algorithm plays a suboptimal arm is upper bounded by exactly the right quantities of you.",
                    "label": 0
                },
                {
                    "sent": "We obtained the same kind of burns As for the plane UCB algorithm, But this time with the right with the right constant matching the lower bound in the binary case.",
                    "label": 0
                },
                {
                    "sent": "But this bound, I repeat, is true for.",
                    "label": 0
                },
                {
                    "sent": "All bounded rewards in as interval 01, just it's asymptotically optimal in the binary case.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe 1 slide to show the comparison between the terms that are used in the plane.",
                    "label": 0
                },
                {
                    "sent": "Eusebian scale.",
                    "label": 0
                },
                {
                    "sent": "UCB algorithm.",
                    "label": 0
                },
                {
                    "sent": "This comparison is just an illustration of the Pinsker inequality.",
                    "label": 0
                },
                {
                    "sent": "Actually in blue you have the calendar divergance in red you have the.",
                    "label": 0
                },
                {
                    "sent": "What you are doing with herding inequality, so the quadratic.",
                    "label": 0
                },
                {
                    "sent": "Approximation, as you can see when it comes to high or low levels, the difference can be quite huge.",
                    "label": 0
                },
                {
                    "sent": "This scale diversions takes into account in particular the variance of the process and even.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A little more.",
                    "label": 0
                },
                {
                    "sent": "The main tool for this proof is division inequality.",
                    "label": 0
                },
                {
                    "sent": "For safe normalized samples.",
                    "label": 0
                },
                {
                    "sent": "Kind of quantities Chabot talked about, but in a much more general setting yesterday.",
                    "label": 0
                },
                {
                    "sent": "Here we have to we have a random number of terms and we want to quantify the distance between U offensive upper bound we constructed using the equal back Libra Divergance and the true parameter.",
                    "label": 0
                },
                {
                    "sent": "And while taking care of her victim.",
                    "label": 0
                },
                {
                    "sent": "Quite precisely, we obtained that this kind of inequality.",
                    "label": 0
                },
                {
                    "sent": "Maybe we use this one in the proof.",
                    "label": 0
                },
                {
                    "sent": "Maybe this one is most useful in general, actually it appears to be useful in very different settings.",
                    "label": 0
                },
                {
                    "sent": "The probability that the normalized this normalized quantities in normalized KL divergent between the estimating the true parameter.",
                    "label": 0
                },
                {
                    "sent": "When I say normalize that I mean multiplied by its order of decreasing to 0.",
                    "label": 0
                },
                {
                    "sent": "So multiplied by the number of observations.",
                    "label": 0
                },
                {
                    "sent": "So probability that this quantity be greater than some Delta is essentially exponential minus Delta times the term that comes from the fact that you have a random number.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Victims.",
                    "label": 0
                },
                {
                    "sent": "Maybe show a few a few, a few results to show that actually the improvement can be quite huge even in simple examples we took.",
                    "label": 0
                },
                {
                    "sent": "Most of the well known variants of the Eusebian and other algorithms available for stochastic bandits, and we wanted to do a comparison.",
                    "label": 0
                },
                {
                    "sent": "The first setting is a is trivial.",
                    "label": 0
                },
                {
                    "sent": "You take just two arms.",
                    "label": 0
                },
                {
                    "sent": "One is a little bit better than the other one.",
                    "label": 0
                },
                {
                    "sent": "And, well, what you observe is that UCB works well, but quite a lot more can be done in this setting.",
                    "label": 0
                },
                {
                    "sent": "The worst the worst algorithm in that case appear to be USV.",
                    "label": 0
                },
                {
                    "sent": "UCB for the reason I gave you first, that's the neglectable termine.",
                    "label": 0
                },
                {
                    "sent": "Bernstein is not at all neglectable ads on this at this scale, so UCKLUCV appears to do as we expected as we hoped.",
                    "label": 0
                },
                {
                    "sent": "Actually, we also introduced in the comparison the algorithm UCB tuned with, which is a variant in the initial paper, for which there is no theoretical guarantees, and this does quite well in expectation, but with very large.",
                    "label": 0
                },
                {
                    "sent": "Risk actually.",
                    "label": 0
                },
                {
                    "sent": "It's sometimes a bit difficult to evaluate bandit processes, as the regret appears to be very, very skewed, so we had to do quite a lot of replications.",
                    "label": 0
                },
                {
                    "sent": "Actually, if you do less replications at the horizon, you have your, your results may be quite subject to controversy.",
                    "label": 0
                },
                {
                    "sent": "And this is because, as you can see, so regret is very small in expectation, but appears sometimes to be very very large.",
                    "label": 0
                },
                {
                    "sent": "So think of a deviation inequality for variables taking value in in.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, you know larger interval you need a lot of observations but then arms scenario with low rewards, low rewards are particularly motivated by applications of this bandit problems in the Internet versus assessments.",
                    "label": 0
                },
                {
                    "sent": "For instance, where the difference actually goes larger and larger.",
                    "label": 0
                },
                {
                    "sent": "With this slide is to insist on the fact that there is a slight improvement that can be given to these algorithms using the idea of the most algorithm to change the confidence term divided divided by the number of observations.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I also gave an explanation example with the truncated exponential to show that the algorithm is absolutely not specific to binary variables.",
                    "label": 0
                },
                {
                    "sent": "Where were algorithms peers to be beaten by use?",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To be tuned but there is a.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Revenge actually what we did for bandori variables can as well be done for other other families of probabilities, say for instance probabilities with a given Canonical exponential model, the algorithm stays exactly the same, just the definition of the Klu function in the algorithm changes.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you have exponential rewards, you think this divergent Bregman divergences.",
                    "label": 0
                },
                {
                    "sent": "If you have question distributions, you have another.",
                    "label": 0
                },
                {
                    "sent": "Bregman divergences anyway, this case algorithms and it gives corresponding bounds.",
                    "label": 0
                },
                {
                    "sent": "Or obtaining that is a lower boundary or reaching in the long run.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The lower bounds though these reasons for exponential rewards comparison with UCB tune this time is in our favor because they can be exponential algorithms is variant take adapted for exponential rewards.",
                    "label": 1
                },
                {
                    "sent": "Does well in mean and actually it mean you see between does bad, but as you can see most of the time it is very very efficient.",
                    "label": 1
                },
                {
                    "sent": "This is the central regions is the central 20%, so regions just deviations become very very huge as the algorithm is not sufficiently present.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So 2 words of conclusion first.",
                    "label": 0
                },
                {
                    "sent": "One should use KL, UCB rather than you see one or is to beat you as he has exactly the same range of applications and it does strictly better in every case.",
                    "label": 0
                },
                {
                    "sent": "There is a method can be adapted in a family of exponential rewards, but can it be adapted to other families of rewards?",
                    "label": 0
                },
                {
                    "sent": "Well, if you want to answer some to have some answer to this question, just listen to the next talk and then can this idea of.",
                    "label": 0
                },
                {
                    "sent": "Considering KL deviation bounds be used in useful in other settings, yes it can.",
                    "label": 0
                },
                {
                    "sent": "It is actually in very very different settings for Markov memory estimation, but it is also more generally within a reinforcement.",
                    "label": 0
                },
                {
                    "sent": "Learning the same kind of findings can be used to estimate to use the kind of to analyze these kind of algorithms that shabba presented yesterday in his second talk in a finite setting then.",
                    "label": 0
                },
                {
                    "sent": "Using Kaldahl versions instead of instead of L1 norm in neighborhoods leads to qualitatively different algorithms which have a lot of nice property.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thank you for your attention.",
                    "label": 0
                },
                {
                    "sent": "So can you come back to you deviation inequality?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what do you use to prove that do use a maximal Chaffin peeling argument?",
                    "label": 0
                },
                {
                    "sent": "Or do you use more than that?",
                    "label": 0
                },
                {
                    "sent": "We know that's exactly what what it is.",
                    "label": 0
                },
                {
                    "sent": "I gave a first hint of the proof in this slide actually, but then the the difficulty is to obtain something that is true with a random number of summons so to to do this correctly rather than using a union bound directly, you just group the number of occurrences that appeared to be on the same scale together so as to use less union bound.",
                    "label": 0
                },
                {
                    "sent": "In a way the user a union bound on less.",
                    "label": 0
                },
                {
                    "sent": "Events this is how you obtain here the lock and dam that allows you to obtain the good constants.",
                    "label": 0
                },
                {
                    "sent": "Then in the in the analysis.",
                    "label": 0
                },
                {
                    "sent": "Any other question?",
                    "label": 0
                },
                {
                    "sent": "OK, so let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}