{
    "id": "czabbf26puqarbdrxkbjy5olksj57pgv",
    "title": "Textual Entailment",
    "info": {
        "author": [
            "Ido Dagan, Bar-Ilan University"
        ],
        "published": "Feb. 7, 2008",
        "recorded": "January 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/psm08_dagan_te/",
    "segmentation": [
        [
            "We're gonna have today a large number of presentations from pump priming projects in Pascal.",
            "We had a large number of projects in that were.",
            "Funded the in the three calls plus a number of later brokerage of expertise.",
            "Projects that were selected in the last smaller call last year and are aimed more at showcasing and publicizing Pascal results.",
            "So we learn will have this.",
            "This presentations that somehow we are requested by contractual obligation in the sense that.",
            "We asked the old the responsible of funded projects too.",
            "At the end of the project to give a presentation, and this is the event in which this presentation will be collected and then made available.",
            "So before to start, an eloquent would like to say something about the feedback.",
            "Something very simple, I'm sure that.",
            "You will do this anyway.",
            "There are lots of companies today we would like to be able to select a few that you think are more suitable for outreach and dissemination because we have a lot of activities of dissemination this year.",
            "If you have any way to feedback which of these are climbing is more.",
            "Suitable for general public, industrial outreach and so on.",
            "Please vote in a way.",
            "One idea could be just to take the list and give it back to us this simple idea.",
            "Thank you very much.",
            "OK, we start with the first speaker.",
            "If either the gun.",
            "Just add the 25 minutes.",
            "Here."
        ],
        [
            "OK. Good morning.",
            "After yesterday's dinner and the amounts of wine we got, I appreciate the fact that I'm here.",
            "So I definitely appreciate the fact that you are here this time.",
            "OK, I will describe.",
            "The pump priming project that I was coordinating.",
            "It was out of the first call.",
            "Started the first call, so we train in 2000.",
            "Four 2005 the partners additional partners were C NTS.",
            "It unfair volunteered elements at EDF, Sammy Benjo, and Michaela Keller and it Excelsi.",
            "If we go see a serial good and Steven Clenshaw or how to how to pronounce it clecia?",
            "OK, thank you.",
            "So base."
        ],
        [
            "Likely the project, I mean that's obviously will find.",
            "Close relationships to my talk yesterday, so it was also within the paradigm of textual entailment.",
            "It started about the same time.",
            "The challenge and the fun project.",
            "And in particular, it focused the project itself, focused on the lexical level of entailment.",
            "And just the general speech, so I'll just this slide gives a general overview of the roles of the different partners, and then I'll have a section on each partner.",
            "So it by line we define the general framework.",
            "And some developed some lexical entailment models.",
            "Idiot also worked on lexical entailment.",
            "Coming.",
            "Kind of following up on earlier work that deal on text categorization and information retrieval.",
            "C NTS.",
            "It's more language processing group and they provided the common application beyond the different applications that people working at the different groups.",
            "We collaborated several groups to work on the same application, which was a sub titling application.",
            "So generate from transcripts compact subtitles.",
            "For TV so they kind of led on the application setting on which they were working earlier, and we integrated our models and the idiot models to try to perform a certain task.",
            "Servers are described and finally excelsi try to incorporate again related lexical entailment models in information retrieval, so that was the whole setting, so I'm going to buy the partners."
        ],
        [
            "Um?",
            "So this is very long.",
            "So just."
        ],
        [
            "As a brief reminder, for yesterday or for anybody who wasn't there yesterday, we were talking about textual entailment, so the general setting we have a text OK and hypothesis we want to know whether we can infer the meaning of the hypothesis from the text that's useful in several applications.",
            "As I mentioned yesterday, and basically we need to figure out whether one meaning can be inferred from the other, and if I use the slides to just to reflect on the lexical element.",
            "Basically, 11 property that typically has to be satisfied is that.",
            "Every concept or every lexical concept that's in the hypothesis should somehow be entailed from the text, right?",
            "The hypothesis can be true if there's no reference in the tag to the concept of born or to Elizabeth though.",
            "OK, so part of the internal processes to entail the lexical level over lexical items in the hypothesis from the text.",
            "Of course, then you have to figure out it.",
            "Also, the satisfy the same relationships, etc.",
            "So it's full intent will hold, but that's the lexical aspect.",
            "OK, and that's what we were after in this project."
        ],
        [
            "Um so.",
            "I mean like with every new problem, we spent enormous amount of time, abit frustrating for included.",
            "It's part of his PhD research in understanding the problem coming up with various definitions you can work three months on some definition.",
            "Then try this thing.",
            "Whether it's worth something or not, understanding the relation to applications and figuring out how to evaluate this.",
            "And that's also was kind of relationship with our work on the RT, which over and lead.",
            "Technically, at the time."
        ],
        [
            "So.",
            "Beyond the basic definition of textual entailment, because we wanted to work with some probabilistic models, we sought definition of lexical setting for texture entailment and eventually we came with this kind of definition.",
            "I mean, what would it mean that attacks entails hypothesis?",
            "And yesterday I mentioned that we are not seeking just a strict linguistic type of definition, which if you like, the probability that X entails hypothesis is 1.",
            "But we had in the definition yesterday count for most likely kind of notion.",
            "So how can we formalize that in probabilistic terms?",
            "So basically we came up with the following this definition, we would say in some sense the text probabilistically entails hypothesis if the conditional probability of the hypothesis given the text is higher than the prior.",
            "So if after hearing the text after reading the text, your belief in the truth of the hypothesis increases, then in some sense the text entails.",
            "Hypothesis, at least at some minimal sense, and this actually provides from information theoretic perspective.",
            "This corresponds to positive kind of pointwise mutual information.",
            "Suggesting the text provides some information about the hypothesis.",
            "If the text is irrelevant then.",
            "The the probability of the hypothesis being true will remain the same.",
            "If the text contradicts.",
            "I suppose in some sense the probability will decrease, but if it increases, the probability that in some sense it entails.",
            "Right and then and then the second notion would be, you know, just the absolute value of the of this conditional probability would be kind of a confidence, right?",
            "So if you are Detective that tries to figure out whether John was born in France and you know you have let's say or hypothesis, and your text is John was not born in America, it slightly increases the probability that.",
            "He was born in France, so there's some notion of entailment useful for you know, very analytic purposes for common user that would not be useful.",
            "Entailment, right?",
            "So you can add this impairment confidence factor.",
            "So that's a probabilistic notion, and under that perception we try to develop some probabilistic models."
        ],
        [
            "And we came up with two models.",
            "One was based and I want in all.",
            "Groups I won't get deep into the models, so one model was was based on lexical alignment.",
            "So if I want to.",
            "Let's say I want to.",
            "Entail Japanese I would look for a particular word like Japan.",
            "That would entail the word Japanese in the hypothesis, OK?",
            "OK or yeah and similar others OK voter and vote etc.",
            "This is the output of the automated models for some of these.",
            "You may disagree with, but this is the output of the models that will or undeveloped.",
            "And basically this was based on some Co occurrence probability."
        ],
        [
            "Reason you can find here resemblance to early alignment models in statistical machine translation.",
            "So this was actually a very simple model.",
            "However, at the RT first Party, which were all systems were quite weak, it actually was one of the two systems scoring the best with respect to accuracy."
        ],
        [
            "Then we looked at another model.",
            "Where we said, OK, we want to to entail the word, let's say votes in the hypothesis, but we won't look just a particular at a single word, but rather collectively from the entire text.",
            "So does the word.",
            "Vote fits well in the text.",
            "In the whole context of all the words in the text, for instance, the text discusses elections and politicians.",
            "Then maybe the word vote fits in that context better and we defined.",
            "Basically we apply the Bayesian model and then the trick there was to view this problem as a text categorization problem.",
            "So just like acceleration problem, we would say OK, suppose I said the car, but let's use this previous example.",
            "Let's keep with vote.",
            "I want to know whether vote is collectively entailed from the text.",
            "I will view vote as kind of a text categorization as a text category.",
            "Just like index creation.",
            "We want to find texts that discuss politics or sports or medicine.",
            "We will regard each word as a text cutting as a category and would like to know whether the text as a whole.",
            "The whole context of the text, all the words collectively entail the meaning of this target word.",
            "OK, so in this perspective, every word in the lexicon would be regarded.",
            "Is a category from a text colorization perspective.",
            "Now, of course we don't have annotated training data for such a large set of of words of categories, so we use kind of bootstrapping approach where first we take all the words that include the word is positive examples and bootstrap from that.",
            "So that was the general spirit, and I think the nice thing here was this perspective of viewing textual entailment.",
            "Lexical entailment is kind of unsupervised text colorization task.",
            "OK, so that's the basically the most of the work.",
            "We developed it."
        ],
        [
            "At very long at Ed app.",
            "Actually they follow the same intuition that this lexical entailment problem when you try to enter a word from the full text actually resembles text categorization and they developped.",
            "Models for this problem, following on earlier work that they did.",
            "For that we were doing for text colorization.",
            "I.",
            "And they took a.",
            "Explore two different MoD."
        ],
        [
            "One is again, I won't get into this model if someone knows about the work that theme topic mixture model which they used earlier, but they figured out it doesn't scale well.",
            "Well to this case.",
            "Then they developed in a neural network for text representation."
        ],
        [
            "Where in the same spirit of neural networks for language modeling, so actually another way to look at this problem rather than text colorization is to look at it as a language modeling approach, right?",
            "I have the word, let's say vote, and I want to know what's the likelihood of you could think of it as asking the question what's the likelihood of the word votes that comes from the hypothesis, what its likelihood to appear in the text.",
            "So it's not language modeling in the linear sense, like in speech recognition.",
            "Usually language models try to predict the likelihood for word in a particular position.",
            "But this is more like kind of a bag of words approach, which doesn't consider the sequence.",
            "But ask does.",
            "Generally the word vote fits well in the context in the whole context of the text.",
            "OK, so that was the motivations in these in these larger.",
            "In this, in these models they used multilayered perceptrons, constructed a perception for the text and the document, and exchange the next slides.",
            "Illustrates that and and then try to see whether a joint model for the word in the document can estimate the likelihood that the world will appear in this document."
        ],
        [
            "And this was the general architecture.",
            "OK, so they had word MLP, a document MLP and then trying to figure out what's the joint likelihood.",
            "Do you think emerging criterion?",
            "These are the technical details?",
            "And the first step studied also on a retrieval task and filtering task and.",
            "And got improvements.",
            "As you can see in the in.",
            "In the table.",
            "So NTR is the new model which performed better than TF IDF for PSA probabilistic LSA.",
            "And finally, this approach is indeed suitable to assess the lexical entailment probability that was next also integrated in the subtitling project.",
            "So this is."
        ],
        [
            "Regards India, India C NTS.",
            "As I said brought in the subtitle Ng application which also.",
            "Requires certain aspects as part of the problem.",
            "It's a kind of a complex problem, so, but a certain point.",
            "Lexical entailment can be useful and we tried applying the bar illan and idiot models for this application.",
            "Um?",
            "So."
        ],
        [
            "So they had a project Moussa, but in which the goal was to generate subtitles.",
            "Yeah, both monolingual and translation.",
            "That was the whole setting of the project.",
            "And their goals was to take care of this sentence.",
            "Compression.",
            "OK, so the general requirement."
        ],
        [
            "It is that for subtitling, right?",
            "You have some transcript, but you only have a certain space for the subtitle.",
            "So often you have to compress the actual transcript in order to fit in subtitle requirements.",
            "Now, from an entailment perspective, you would like of course, the transcript wouldn't tell the subtitle, right?",
            "You don't want to create a subtitle that is not faithful to the origin.",
            "Ideally would also like the other way around.",
            "So like if you don't have to lose information, that subtitle will also entail the transcript, but sometimes you must lose some information, so the subtitle will give you a little bit less information.",
            "It won't entail all the information in the text, but you'd like to have a minimal loss, so that sentiment perspective about this task."
        ],
        [
            "Is just an example, we're only dilution was applied whereby some kinds of paraphrasing, you know, it was clear that could be replaced with.",
            "Clearly, while the very was dropped, but rather little loss in information.",
            "So basically going for subtitling you have two possibilities, deletion, as we've seen in the previous slide, an that was not part of the project of the Palm project is something they did in the general project, and what we focused is where we focused.",
            "Is an substitution, so you don't want to lose information.",
            "You substitute substitute a longer piece of text with a shorter one that basically has the same meaning, and that's where we focus where we constructed the setting to integrate the lexical entailment models."
        ],
        [
            "Now just to understand the scope of the setting of the problem, usually you need rather little.",
            "Compression, so you have an expression like a word and maybe you look for shorter synonyms or you have like a longer expression.",
            "Look for a shorter expression there if you have several alternatives to compress the original expression in the transcript, you may use lexical entailment models to rank them to choose the one that seems to be most likely entailed in this context.",
            "For instance, a word might be ambiguous.",
            "It may have two shorter synonyms, but one of them corresponds to a sense that.",
            "Fits the context and another to another sense.",
            "OK, so the lexical entailment models can tell you the bar illan and the idiot models.",
            "We're hoping will be able to tell us in which context each substitution is more legitimate, and then choose the most likely one.",
            "So that was that was the idea.",
            "So this was the idea and the."
        ],
        [
            "Setting we had is as you can see here, we would upper table shows it.",
            "We have the original sentence.",
            "We pick a certain word for which we find a shorter synonym in Word net.",
            "So for instance, you have the word answer in these examples and word.",
            "It tells us that reply is a shorter synonym, so we can gain a few characters by making such a replacement and then.",
            "Um?",
            "In each context, the substance may be either correct or incorrect, so we use human judgment to produce these false true assignments.",
            "Saying in this context, this is legitimate or not OK, and then to test the models whenever we had a word for which we found shorter Wordnet synonyms, we applied Alternatively the different lexical entailment models.",
            "They ranked the substitutions, and we picked the best one.",
            "OK, and that was evaluated relative to to.",
            "Two human judgments.",
            "And in this experiment, we compared the two.",
            "The bar down idiot models which you could think of them as kind of posterior models right they.",
            "Test whether the entailment holds in a particular context.",
            "They evaluate the words in the full sentence and see whether, in this context, reply or the word reply fits in well.",
            "The other part is for each were not synonyms is also some prior probability.",
            "So suppose answer would have to synonyms.",
            "One is corresponds to quite frequent usage of answer, while the other two are quite rare.",
            "Usage of of of answer.",
            "In that case the prior probability of some substitution to be correct may be different for different alternatives, so it's useful also to have some estimate for the prior probability.",
            "So in this experiment we also used to prior."
        ],
        [
            "Models, So what we have here is.",
            "These are the very long and the idea of models.",
            "The posterior models and these are two prior models, one using some distributional similarity.",
            "So statistical similarity context independent amongst the two words and the other comes from sensor notations in Word.",
            "Net telling us whether the serial name is infrequent sense or not of the original world and what we found here in this experiment actually that the prior models performed much better.",
            "Then the posterior models and the posterior models didn't provide really statistical significant contribution.",
            "And the major in this application, unlike the other applications in which they were tested, where they did show better results.",
            "And we had some analysis showing that in this sub titling application, local context apparently was much more important.",
            "So the models were not strong enough for this purpose.",
            "The analysis showed that better models that would combine local and global context still have room for improvement.",
            "Also in this application.",
            "So in retrospect.",
            "This application didn't turn out to be the best one for those models, but it gave a nice setting to compare the models.",
            "And then your network performed also a little bit better than Bayesian model, so this was the contribution of the.",
            "DS Group from adverb and."
        ],
        [
            "Finally Xerox, they try to develop similar models.",
            "Also, they started actually by replicating the model that or undeveloped bar illan and trying to adapt it to IR and examining."
        ],
        [
            "Recent improvements.",
            "And basically wanted to see whether this lexical entailment can improve IR.",
            "Anne."
        ],
        [
            "That was embedded within the language modeling approach.",
            "Again, I won't get into the formula.",
            "But basically the idea is if people are familiar with the language modeling approach to IR, there is a particular approach that suitable for lexical expansion, which is like a translation based, so it tries to model whether you can, in some sense translate the query to fit into the document.",
            "So they plugged in the lexical entailment probabilities.",
            "In this translation module.",
            "In this translation component, and again this is they tried three diff."
        ],
        [
            "It approaches three different variants.",
            "You can see based on the Jaccard, some other form of lexical similarity etc and also tried the barrel and model."
        ],
        [
            "And overall they should have seen that the version based on Kelda virgins performs best.",
            "And indeed when you augment the standard language model with lexical entailment probability, it gives you more, gives you better performance.",
            "I refer to that also in the follow up activities, so this was the work at at Excel.",
            "See which interact mostly with Darrell on or and visited them.",
            "Expensive there sometime that was their part."
        ],
        [
            "We had a bunch of meetings, common wiki, mailing lists, etc in some papers.",
            "Published.",
            "The nice thing about the pump that indeed triggers follow."
        ],
        [
            "Activities in most of the sites.",
            "XRC followed up with these models and participated also in Clay 2007.",
            "Domain specific IR task and what Stephanie did found is that that was actually corresponding with them.",
            "Thought this meeting to prepare the data on the follow-up activities to collect it from the others.",
            "And they did find that excellent models do find more relations between terms than the more standard pseudo relevance feedback methods that are common in IR.",
            "For those familiar with it, and then in clay they got improvement from 45 to 50%, which is, you know, this is going to improvement in mapping mean average precision in."
        ],
        [
            "Our terms at C NTS.",
            "It also triggered following word to kind of also made him aware to the internment approach and now they have a project, a joint project with Symbol University on semantic entailment in Dutch, and that's used for the summarization problem which I also mentioned yesterday in summarization, you have two sentences, let's say coming from different documents that mean actually the same.",
            "You want to know that one entails the other so one is redundant and doesn't need to be included in the summary."
        ],
        [
            "And it very long we we had a bunch of activities or and continue that we had a paper that studied the lexical entailment in more depth.",
            "And then we also combine the two prior models that was published in MLP empirical methods for NLP.",
            "It also inspired another work on word, since this ambiguation innocence.",
            "This approach.",
            "Can be viewed as implicitly performing word senses simulation.",
            "Suppose, for instance, you have the world record and you want to replace it with the word disk.",
            "OK, because it's shorter taking just the subtitling example or in IR.",
            "You want to retrieve documents for records and disks and use the word disk as an expanding work if.",
            "The text in which disk occurs is in the musical sense.",
            "Then this could entail record if it's, let's say, the computer sends.",
            "It won't entail record.",
            "So implicitly you are doing kind of word simulation, but without referring to an explicit set of sensors.",
            "So that's kind of a neat way to get around the whole problem of defining explicitly sensors follows the philosophy that I mentioned yesterday, and we had a paper at ACL on that.",
            "Then we have a project taking this approach to IR, and another one for unsupervised exploration.",
            "These are government funded projects in Israel."
        ],
        [
            "Just as a take out, I think the nice thing from this project beyond the follow-up activities it triggered for the partners, was that I think it identifies new, interesting research problems within this.",
            "Under this paradigm of entailment, and if you look at the basically the two first bullet specifies there's 2 interesting tasks.",
            "One is identified prior.",
            "Likelihood of entailments of this kind of corresponds to build to learning this entailment network between.",
            "Between lexical terms, so that could be considered as the prior or network OK, which words couldn't tell the other records, couldn't ale disk, etc.",
            "And the second problem is like this posterior problem.",
            "OK, OK, this can entail record, but does it entail record in this context or in other context is kind of implicitly simulation sense, so I think these are very interesting machine learning problems that come up in this context and you consider them as kind of unsupervised variants of earlier problems.",
            "And I think this can be nicely evaluated in different settings, particularly just to give.",
            "Do I think easy ways to evaluate models for these cases?",
            "One would be one thing that I suggest that it would be nice if there is a challenge on something like that is just to take standard text colorization datasets that are available and they.",
            "Particularly highlighting text colorization, 'cause this was popular in machine learning community to apply models to text categorization, and you have the data sets.",
            "So rather than using the training data in the supervised way, take an unsupervised approach.",
            "You want to classify text to medicine or too low over to acquisitions.",
            "Just try to learn models like we did Bar Illan an idiot that predict entailment of the word law of the word medicine, or the word acquisition OK. And start only with the title of the category, which is a word right?",
            "The title of the country by itself is a language word and try in an unsupervised way to learn models that entail the meaning of this category and then just test them on standard text colorization datasets.",
            "This is projects we have now think it's very useful and very easy to run.",
            "Just use available.",
            "We use the 20 newsgroups in the Reuters and a bunch of taxations.",
            "Another one may use the RT datasets.",
            "I mean just take the RT day to try to predict weather entailment holds, but just using the lexical approach and lexical approaches were actually.",
            "Pretty good so.",
            "Maybe you won't beat methods that use also syntax and other things, but just to develop the lexical models.",
            "It would be very interesting to compare different models and there are some baselines in the art literature.",
            "Some people intentionally just restrict themselves to lexical models to explore how well they can perform.",
            "I think methods of this sort would be very nice to try and develop more rigorous machine learning models for these methods.",
            "OK, that's my take out.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're gonna have today a large number of presentations from pump priming projects in Pascal.",
                    "label": 1
                },
                {
                    "sent": "We had a large number of projects in that were.",
                    "label": 0
                },
                {
                    "sent": "Funded the in the three calls plus a number of later brokerage of expertise.",
                    "label": 0
                },
                {
                    "sent": "Projects that were selected in the last smaller call last year and are aimed more at showcasing and publicizing Pascal results.",
                    "label": 0
                },
                {
                    "sent": "So we learn will have this.",
                    "label": 0
                },
                {
                    "sent": "This presentations that somehow we are requested by contractual obligation in the sense that.",
                    "label": 0
                },
                {
                    "sent": "We asked the old the responsible of funded projects too.",
                    "label": 0
                },
                {
                    "sent": "At the end of the project to give a presentation, and this is the event in which this presentation will be collected and then made available.",
                    "label": 0
                },
                {
                    "sent": "So before to start, an eloquent would like to say something about the feedback.",
                    "label": 0
                },
                {
                    "sent": "Something very simple, I'm sure that.",
                    "label": 0
                },
                {
                    "sent": "You will do this anyway.",
                    "label": 0
                },
                {
                    "sent": "There are lots of companies today we would like to be able to select a few that you think are more suitable for outreach and dissemination because we have a lot of activities of dissemination this year.",
                    "label": 0
                },
                {
                    "sent": "If you have any way to feedback which of these are climbing is more.",
                    "label": 0
                },
                {
                    "sent": "Suitable for general public, industrial outreach and so on.",
                    "label": 0
                },
                {
                    "sent": "Please vote in a way.",
                    "label": 0
                },
                {
                    "sent": "One idea could be just to take the list and give it back to us this simple idea.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "OK, we start with the first speaker.",
                    "label": 0
                },
                {
                    "sent": "If either the gun.",
                    "label": 0
                },
                {
                    "sent": "Just add the 25 minutes.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK. Good morning.",
                    "label": 0
                },
                {
                    "sent": "After yesterday's dinner and the amounts of wine we got, I appreciate the fact that I'm here.",
                    "label": 0
                },
                {
                    "sent": "So I definitely appreciate the fact that you are here this time.",
                    "label": 0
                },
                {
                    "sent": "OK, I will describe.",
                    "label": 0
                },
                {
                    "sent": "The pump priming project that I was coordinating.",
                    "label": 1
                },
                {
                    "sent": "It was out of the first call.",
                    "label": 0
                },
                {
                    "sent": "Started the first call, so we train in 2000.",
                    "label": 0
                },
                {
                    "sent": "Four 2005 the partners additional partners were C NTS.",
                    "label": 0
                },
                {
                    "sent": "It unfair volunteered elements at EDF, Sammy Benjo, and Michaela Keller and it Excelsi.",
                    "label": 0
                },
                {
                    "sent": "If we go see a serial good and Steven Clenshaw or how to how to pronounce it clecia?",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "So base.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Likely the project, I mean that's obviously will find.",
                    "label": 0
                },
                {
                    "sent": "Close relationships to my talk yesterday, so it was also within the paradigm of textual entailment.",
                    "label": 0
                },
                {
                    "sent": "It started about the same time.",
                    "label": 0
                },
                {
                    "sent": "The challenge and the fun project.",
                    "label": 0
                },
                {
                    "sent": "And in particular, it focused the project itself, focused on the lexical level of entailment.",
                    "label": 0
                },
                {
                    "sent": "And just the general speech, so I'll just this slide gives a general overview of the roles of the different partners, and then I'll have a section on each partner.",
                    "label": 0
                },
                {
                    "sent": "So it by line we define the general framework.",
                    "label": 0
                },
                {
                    "sent": "And some developed some lexical entailment models.",
                    "label": 1
                },
                {
                    "sent": "Idiot also worked on lexical entailment.",
                    "label": 0
                },
                {
                    "sent": "Coming.",
                    "label": 0
                },
                {
                    "sent": "Kind of following up on earlier work that deal on text categorization and information retrieval.",
                    "label": 0
                },
                {
                    "sent": "C NTS.",
                    "label": 1
                },
                {
                    "sent": "It's more language processing group and they provided the common application beyond the different applications that people working at the different groups.",
                    "label": 0
                },
                {
                    "sent": "We collaborated several groups to work on the same application, which was a sub titling application.",
                    "label": 0
                },
                {
                    "sent": "So generate from transcripts compact subtitles.",
                    "label": 0
                },
                {
                    "sent": "For TV so they kind of led on the application setting on which they were working earlier, and we integrated our models and the idiot models to try to perform a certain task.",
                    "label": 0
                },
                {
                    "sent": "Servers are described and finally excelsi try to incorporate again related lexical entailment models in information retrieval, so that was the whole setting, so I'm going to buy the partners.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So this is very long.",
                    "label": 0
                },
                {
                    "sent": "So just.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As a brief reminder, for yesterday or for anybody who wasn't there yesterday, we were talking about textual entailment, so the general setting we have a text OK and hypothesis we want to know whether we can infer the meaning of the hypothesis from the text that's useful in several applications.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned yesterday, and basically we need to figure out whether one meaning can be inferred from the other, and if I use the slides to just to reflect on the lexical element.",
                    "label": 0
                },
                {
                    "sent": "Basically, 11 property that typically has to be satisfied is that.",
                    "label": 0
                },
                {
                    "sent": "Every concept or every lexical concept that's in the hypothesis should somehow be entailed from the text, right?",
                    "label": 0
                },
                {
                    "sent": "The hypothesis can be true if there's no reference in the tag to the concept of born or to Elizabeth though.",
                    "label": 0
                },
                {
                    "sent": "OK, so part of the internal processes to entail the lexical level over lexical items in the hypothesis from the text.",
                    "label": 0
                },
                {
                    "sent": "Of course, then you have to figure out it.",
                    "label": 0
                },
                {
                    "sent": "Also, the satisfy the same relationships, etc.",
                    "label": 0
                },
                {
                    "sent": "So it's full intent will hold, but that's the lexical aspect.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's what we were after in this project.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um so.",
                    "label": 0
                },
                {
                    "sent": "I mean like with every new problem, we spent enormous amount of time, abit frustrating for included.",
                    "label": 0
                },
                {
                    "sent": "It's part of his PhD research in understanding the problem coming up with various definitions you can work three months on some definition.",
                    "label": 1
                },
                {
                    "sent": "Then try this thing.",
                    "label": 1
                },
                {
                    "sent": "Whether it's worth something or not, understanding the relation to applications and figuring out how to evaluate this.",
                    "label": 0
                },
                {
                    "sent": "And that's also was kind of relationship with our work on the RT, which over and lead.",
                    "label": 0
                },
                {
                    "sent": "Technically, at the time.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Beyond the basic definition of textual entailment, because we wanted to work with some probabilistic models, we sought definition of lexical setting for texture entailment and eventually we came with this kind of definition.",
                    "label": 1
                },
                {
                    "sent": "I mean, what would it mean that attacks entails hypothesis?",
                    "label": 0
                },
                {
                    "sent": "And yesterday I mentioned that we are not seeking just a strict linguistic type of definition, which if you like, the probability that X entails hypothesis is 1.",
                    "label": 0
                },
                {
                    "sent": "But we had in the definition yesterday count for most likely kind of notion.",
                    "label": 0
                },
                {
                    "sent": "So how can we formalize that in probabilistic terms?",
                    "label": 1
                },
                {
                    "sent": "So basically we came up with the following this definition, we would say in some sense the text probabilistically entails hypothesis if the conditional probability of the hypothesis given the text is higher than the prior.",
                    "label": 0
                },
                {
                    "sent": "So if after hearing the text after reading the text, your belief in the truth of the hypothesis increases, then in some sense the text entails.",
                    "label": 0
                },
                {
                    "sent": "Hypothesis, at least at some minimal sense, and this actually provides from information theoretic perspective.",
                    "label": 0
                },
                {
                    "sent": "This corresponds to positive kind of pointwise mutual information.",
                    "label": 0
                },
                {
                    "sent": "Suggesting the text provides some information about the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "If the text is irrelevant then.",
                    "label": 0
                },
                {
                    "sent": "The the probability of the hypothesis being true will remain the same.",
                    "label": 1
                },
                {
                    "sent": "If the text contradicts.",
                    "label": 1
                },
                {
                    "sent": "I suppose in some sense the probability will decrease, but if it increases, the probability that in some sense it entails.",
                    "label": 0
                },
                {
                    "sent": "Right and then and then the second notion would be, you know, just the absolute value of the of this conditional probability would be kind of a confidence, right?",
                    "label": 0
                },
                {
                    "sent": "So if you are Detective that tries to figure out whether John was born in France and you know you have let's say or hypothesis, and your text is John was not born in America, it slightly increases the probability that.",
                    "label": 0
                },
                {
                    "sent": "He was born in France, so there's some notion of entailment useful for you know, very analytic purposes for common user that would not be useful.",
                    "label": 0
                },
                {
                    "sent": "Entailment, right?",
                    "label": 0
                },
                {
                    "sent": "So you can add this impairment confidence factor.",
                    "label": 0
                },
                {
                    "sent": "So that's a probabilistic notion, and under that perception we try to develop some probabilistic models.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we came up with two models.",
                    "label": 0
                },
                {
                    "sent": "One was based and I want in all.",
                    "label": 0
                },
                {
                    "sent": "Groups I won't get deep into the models, so one model was was based on lexical alignment.",
                    "label": 1
                },
                {
                    "sent": "So if I want to.",
                    "label": 0
                },
                {
                    "sent": "Let's say I want to.",
                    "label": 0
                },
                {
                    "sent": "Entail Japanese I would look for a particular word like Japan.",
                    "label": 0
                },
                {
                    "sent": "That would entail the word Japanese in the hypothesis, OK?",
                    "label": 0
                },
                {
                    "sent": "OK or yeah and similar others OK voter and vote etc.",
                    "label": 0
                },
                {
                    "sent": "This is the output of the automated models for some of these.",
                    "label": 0
                },
                {
                    "sent": "You may disagree with, but this is the output of the models that will or undeveloped.",
                    "label": 0
                },
                {
                    "sent": "And basically this was based on some Co occurrence probability.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reason you can find here resemblance to early alignment models in statistical machine translation.",
                    "label": 0
                },
                {
                    "sent": "So this was actually a very simple model.",
                    "label": 0
                },
                {
                    "sent": "However, at the RT first Party, which were all systems were quite weak, it actually was one of the two systems scoring the best with respect to accuracy.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then we looked at another model.",
                    "label": 0
                },
                {
                    "sent": "Where we said, OK, we want to to entail the word, let's say votes in the hypothesis, but we won't look just a particular at a single word, but rather collectively from the entire text.",
                    "label": 0
                },
                {
                    "sent": "So does the word.",
                    "label": 0
                },
                {
                    "sent": "Vote fits well in the text.",
                    "label": 0
                },
                {
                    "sent": "In the whole context of all the words in the text, for instance, the text discusses elections and politicians.",
                    "label": 0
                },
                {
                    "sent": "Then maybe the word vote fits in that context better and we defined.",
                    "label": 0
                },
                {
                    "sent": "Basically we apply the Bayesian model and then the trick there was to view this problem as a text categorization problem.",
                    "label": 1
                },
                {
                    "sent": "So just like acceleration problem, we would say OK, suppose I said the car, but let's use this previous example.",
                    "label": 0
                },
                {
                    "sent": "Let's keep with vote.",
                    "label": 0
                },
                {
                    "sent": "I want to know whether vote is collectively entailed from the text.",
                    "label": 0
                },
                {
                    "sent": "I will view vote as kind of a text categorization as a text category.",
                    "label": 1
                },
                {
                    "sent": "Just like index creation.",
                    "label": 0
                },
                {
                    "sent": "We want to find texts that discuss politics or sports or medicine.",
                    "label": 0
                },
                {
                    "sent": "We will regard each word as a text cutting as a category and would like to know whether the text as a whole.",
                    "label": 0
                },
                {
                    "sent": "The whole context of the text, all the words collectively entail the meaning of this target word.",
                    "label": 1
                },
                {
                    "sent": "OK, so in this perspective, every word in the lexicon would be regarded.",
                    "label": 1
                },
                {
                    "sent": "Is a category from a text colorization perspective.",
                    "label": 0
                },
                {
                    "sent": "Now, of course we don't have annotated training data for such a large set of of words of categories, so we use kind of bootstrapping approach where first we take all the words that include the word is positive examples and bootstrap from that.",
                    "label": 0
                },
                {
                    "sent": "So that was the general spirit, and I think the nice thing here was this perspective of viewing textual entailment.",
                    "label": 0
                },
                {
                    "sent": "Lexical entailment is kind of unsupervised text colorization task.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's the basically the most of the work.",
                    "label": 0
                },
                {
                    "sent": "We developed it.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At very long at Ed app.",
                    "label": 0
                },
                {
                    "sent": "Actually they follow the same intuition that this lexical entailment problem when you try to enter a word from the full text actually resembles text categorization and they developped.",
                    "label": 0
                },
                {
                    "sent": "Models for this problem, following on earlier work that they did.",
                    "label": 0
                },
                {
                    "sent": "For that we were doing for text colorization.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "And they took a.",
                    "label": 0
                },
                {
                    "sent": "Explore two different MoD.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One is again, I won't get into this model if someone knows about the work that theme topic mixture model which they used earlier, but they figured out it doesn't scale well.",
                    "label": 1
                },
                {
                    "sent": "Well to this case.",
                    "label": 1
                },
                {
                    "sent": "Then they developed in a neural network for text representation.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where in the same spirit of neural networks for language modeling, so actually another way to look at this problem rather than text colorization is to look at it as a language modeling approach, right?",
                    "label": 0
                },
                {
                    "sent": "I have the word, let's say vote, and I want to know what's the likelihood of you could think of it as asking the question what's the likelihood of the word votes that comes from the hypothesis, what its likelihood to appear in the text.",
                    "label": 0
                },
                {
                    "sent": "So it's not language modeling in the linear sense, like in speech recognition.",
                    "label": 0
                },
                {
                    "sent": "Usually language models try to predict the likelihood for word in a particular position.",
                    "label": 0
                },
                {
                    "sent": "But this is more like kind of a bag of words approach, which doesn't consider the sequence.",
                    "label": 0
                },
                {
                    "sent": "But ask does.",
                    "label": 0
                },
                {
                    "sent": "Generally the word vote fits well in the context in the whole context of the text.",
                    "label": 0
                },
                {
                    "sent": "OK, so that was the motivations in these in these larger.",
                    "label": 0
                },
                {
                    "sent": "In this, in these models they used multilayered perceptrons, constructed a perception for the text and the document, and exchange the next slides.",
                    "label": 0
                },
                {
                    "sent": "Illustrates that and and then try to see whether a joint model for the word in the document can estimate the likelihood that the world will appear in this document.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this was the general architecture.",
                    "label": 1
                },
                {
                    "sent": "OK, so they had word MLP, a document MLP and then trying to figure out what's the joint likelihood.",
                    "label": 0
                },
                {
                    "sent": "Do you think emerging criterion?",
                    "label": 0
                },
                {
                    "sent": "These are the technical details?",
                    "label": 1
                },
                {
                    "sent": "And the first step studied also on a retrieval task and filtering task and.",
                    "label": 0
                },
                {
                    "sent": "And got improvements.",
                    "label": 0
                },
                {
                    "sent": "As you can see in the in.",
                    "label": 0
                },
                {
                    "sent": "In the table.",
                    "label": 0
                },
                {
                    "sent": "So NTR is the new model which performed better than TF IDF for PSA probabilistic LSA.",
                    "label": 0
                },
                {
                    "sent": "And finally, this approach is indeed suitable to assess the lexical entailment probability that was next also integrated in the subtitling project.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Regards India, India C NTS.",
                    "label": 0
                },
                {
                    "sent": "As I said brought in the subtitle Ng application which also.",
                    "label": 0
                },
                {
                    "sent": "Requires certain aspects as part of the problem.",
                    "label": 0
                },
                {
                    "sent": "It's a kind of a complex problem, so, but a certain point.",
                    "label": 0
                },
                {
                    "sent": "Lexical entailment can be useful and we tried applying the bar illan and idiot models for this application.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So they had a project Moussa, but in which the goal was to generate subtitles.",
                    "label": 0
                },
                {
                    "sent": "Yeah, both monolingual and translation.",
                    "label": 0
                },
                {
                    "sent": "That was the whole setting of the project.",
                    "label": 0
                },
                {
                    "sent": "And their goals was to take care of this sentence.",
                    "label": 0
                },
                {
                    "sent": "Compression.",
                    "label": 0
                },
                {
                    "sent": "OK, so the general requirement.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is that for subtitling, right?",
                    "label": 0
                },
                {
                    "sent": "You have some transcript, but you only have a certain space for the subtitle.",
                    "label": 0
                },
                {
                    "sent": "So often you have to compress the actual transcript in order to fit in subtitle requirements.",
                    "label": 1
                },
                {
                    "sent": "Now, from an entailment perspective, you would like of course, the transcript wouldn't tell the subtitle, right?",
                    "label": 0
                },
                {
                    "sent": "You don't want to create a subtitle that is not faithful to the origin.",
                    "label": 0
                },
                {
                    "sent": "Ideally would also like the other way around.",
                    "label": 0
                },
                {
                    "sent": "So like if you don't have to lose information, that subtitle will also entail the transcript, but sometimes you must lose some information, so the subtitle will give you a little bit less information.",
                    "label": 1
                },
                {
                    "sent": "It won't entail all the information in the text, but you'd like to have a minimal loss, so that sentiment perspective about this task.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is just an example, we're only dilution was applied whereby some kinds of paraphrasing, you know, it was clear that could be replaced with.",
                    "label": 1
                },
                {
                    "sent": "Clearly, while the very was dropped, but rather little loss in information.",
                    "label": 0
                },
                {
                    "sent": "So basically going for subtitling you have two possibilities, deletion, as we've seen in the previous slide, an that was not part of the project of the Palm project is something they did in the general project, and what we focused is where we focused.",
                    "label": 0
                },
                {
                    "sent": "Is an substitution, so you don't want to lose information.",
                    "label": 0
                },
                {
                    "sent": "You substitute substitute a longer piece of text with a shorter one that basically has the same meaning, and that's where we focus where we constructed the setting to integrate the lexical entailment models.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now just to understand the scope of the setting of the problem, usually you need rather little.",
                    "label": 0
                },
                {
                    "sent": "Compression, so you have an expression like a word and maybe you look for shorter synonyms or you have like a longer expression.",
                    "label": 0
                },
                {
                    "sent": "Look for a shorter expression there if you have several alternatives to compress the original expression in the transcript, you may use lexical entailment models to rank them to choose the one that seems to be most likely entailed in this context.",
                    "label": 0
                },
                {
                    "sent": "For instance, a word might be ambiguous.",
                    "label": 0
                },
                {
                    "sent": "It may have two shorter synonyms, but one of them corresponds to a sense that.",
                    "label": 0
                },
                {
                    "sent": "Fits the context and another to another sense.",
                    "label": 0
                },
                {
                    "sent": "OK, so the lexical entailment models can tell you the bar illan and the idiot models.",
                    "label": 1
                },
                {
                    "sent": "We're hoping will be able to tell us in which context each substitution is more legitimate, and then choose the most likely one.",
                    "label": 0
                },
                {
                    "sent": "So that was that was the idea.",
                    "label": 0
                },
                {
                    "sent": "So this was the idea and the.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Setting we had is as you can see here, we would upper table shows it.",
                    "label": 0
                },
                {
                    "sent": "We have the original sentence.",
                    "label": 0
                },
                {
                    "sent": "We pick a certain word for which we find a shorter synonym in Word net.",
                    "label": 0
                },
                {
                    "sent": "So for instance, you have the word answer in these examples and word.",
                    "label": 0
                },
                {
                    "sent": "It tells us that reply is a shorter synonym, so we can gain a few characters by making such a replacement and then.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "In each context, the substance may be either correct or incorrect, so we use human judgment to produce these false true assignments.",
                    "label": 0
                },
                {
                    "sent": "Saying in this context, this is legitimate or not OK, and then to test the models whenever we had a word for which we found shorter Wordnet synonyms, we applied Alternatively the different lexical entailment models.",
                    "label": 0
                },
                {
                    "sent": "They ranked the substitutions, and we picked the best one.",
                    "label": 0
                },
                {
                    "sent": "OK, and that was evaluated relative to to.",
                    "label": 1
                },
                {
                    "sent": "Two human judgments.",
                    "label": 0
                },
                {
                    "sent": "And in this experiment, we compared the two.",
                    "label": 1
                },
                {
                    "sent": "The bar down idiot models which you could think of them as kind of posterior models right they.",
                    "label": 0
                },
                {
                    "sent": "Test whether the entailment holds in a particular context.",
                    "label": 0
                },
                {
                    "sent": "They evaluate the words in the full sentence and see whether, in this context, reply or the word reply fits in well.",
                    "label": 0
                },
                {
                    "sent": "The other part is for each were not synonyms is also some prior probability.",
                    "label": 0
                },
                {
                    "sent": "So suppose answer would have to synonyms.",
                    "label": 0
                },
                {
                    "sent": "One is corresponds to quite frequent usage of answer, while the other two are quite rare.",
                    "label": 0
                },
                {
                    "sent": "Usage of of of answer.",
                    "label": 0
                },
                {
                    "sent": "In that case the prior probability of some substitution to be correct may be different for different alternatives, so it's useful also to have some estimate for the prior probability.",
                    "label": 0
                },
                {
                    "sent": "So in this experiment we also used to prior.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Models, So what we have here is.",
                    "label": 0
                },
                {
                    "sent": "These are the very long and the idea of models.",
                    "label": 0
                },
                {
                    "sent": "The posterior models and these are two prior models, one using some distributional similarity.",
                    "label": 0
                },
                {
                    "sent": "So statistical similarity context independent amongst the two words and the other comes from sensor notations in Word.",
                    "label": 0
                },
                {
                    "sent": "Net telling us whether the serial name is infrequent sense or not of the original world and what we found here in this experiment actually that the prior models performed much better.",
                    "label": 0
                },
                {
                    "sent": "Then the posterior models and the posterior models didn't provide really statistical significant contribution.",
                    "label": 0
                },
                {
                    "sent": "And the major in this application, unlike the other applications in which they were tested, where they did show better results.",
                    "label": 0
                },
                {
                    "sent": "And we had some analysis showing that in this sub titling application, local context apparently was much more important.",
                    "label": 0
                },
                {
                    "sent": "So the models were not strong enough for this purpose.",
                    "label": 0
                },
                {
                    "sent": "The analysis showed that better models that would combine local and global context still have room for improvement.",
                    "label": 0
                },
                {
                    "sent": "Also in this application.",
                    "label": 0
                },
                {
                    "sent": "So in retrospect.",
                    "label": 0
                },
                {
                    "sent": "This application didn't turn out to be the best one for those models, but it gave a nice setting to compare the models.",
                    "label": 0
                },
                {
                    "sent": "And then your network performed also a little bit better than Bayesian model, so this was the contribution of the.",
                    "label": 0
                },
                {
                    "sent": "DS Group from adverb and.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally Xerox, they try to develop similar models.",
                    "label": 0
                },
                {
                    "sent": "Also, they started actually by replicating the model that or undeveloped bar illan and trying to adapt it to IR and examining.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Recent improvements.",
                    "label": 0
                },
                {
                    "sent": "And basically wanted to see whether this lexical entailment can improve IR.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That was embedded within the language modeling approach.",
                    "label": 0
                },
                {
                    "sent": "Again, I won't get into the formula.",
                    "label": 0
                },
                {
                    "sent": "But basically the idea is if people are familiar with the language modeling approach to IR, there is a particular approach that suitable for lexical expansion, which is like a translation based, so it tries to model whether you can, in some sense translate the query to fit into the document.",
                    "label": 1
                },
                {
                    "sent": "So they plugged in the lexical entailment probabilities.",
                    "label": 0
                },
                {
                    "sent": "In this translation module.",
                    "label": 0
                },
                {
                    "sent": "In this translation component, and again this is they tried three diff.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It approaches three different variants.",
                    "label": 0
                },
                {
                    "sent": "You can see based on the Jaccard, some other form of lexical similarity etc and also tried the barrel and model.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And overall they should have seen that the version based on Kelda virgins performs best.",
                    "label": 0
                },
                {
                    "sent": "And indeed when you augment the standard language model with lexical entailment probability, it gives you more, gives you better performance.",
                    "label": 1
                },
                {
                    "sent": "I refer to that also in the follow up activities, so this was the work at at Excel.",
                    "label": 0
                },
                {
                    "sent": "See which interact mostly with Darrell on or and visited them.",
                    "label": 0
                },
                {
                    "sent": "Expensive there sometime that was their part.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We had a bunch of meetings, common wiki, mailing lists, etc in some papers.",
                    "label": 0
                },
                {
                    "sent": "Published.",
                    "label": 0
                },
                {
                    "sent": "The nice thing about the pump that indeed triggers follow.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Activities in most of the sites.",
                    "label": 0
                },
                {
                    "sent": "XRC followed up with these models and participated also in Clay 2007.",
                    "label": 0
                },
                {
                    "sent": "Domain specific IR task and what Stephanie did found is that that was actually corresponding with them.",
                    "label": 1
                },
                {
                    "sent": "Thought this meeting to prepare the data on the follow-up activities to collect it from the others.",
                    "label": 0
                },
                {
                    "sent": "And they did find that excellent models do find more relations between terms than the more standard pseudo relevance feedback methods that are common in IR.",
                    "label": 0
                },
                {
                    "sent": "For those familiar with it, and then in clay they got improvement from 45 to 50%, which is, you know, this is going to improvement in mapping mean average precision in.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our terms at C NTS.",
                    "label": 0
                },
                {
                    "sent": "It also triggered following word to kind of also made him aware to the internment approach and now they have a project, a joint project with Symbol University on semantic entailment in Dutch, and that's used for the summarization problem which I also mentioned yesterday in summarization, you have two sentences, let's say coming from different documents that mean actually the same.",
                    "label": 1
                },
                {
                    "sent": "You want to know that one entails the other so one is redundant and doesn't need to be included in the summary.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it very long we we had a bunch of activities or and continue that we had a paper that studied the lexical entailment in more depth.",
                    "label": 0
                },
                {
                    "sent": "And then we also combine the two prior models that was published in MLP empirical methods for NLP.",
                    "label": 0
                },
                {
                    "sent": "It also inspired another work on word, since this ambiguation innocence.",
                    "label": 0
                },
                {
                    "sent": "This approach.",
                    "label": 0
                },
                {
                    "sent": "Can be viewed as implicitly performing word senses simulation.",
                    "label": 0
                },
                {
                    "sent": "Suppose, for instance, you have the world record and you want to replace it with the word disk.",
                    "label": 0
                },
                {
                    "sent": "OK, because it's shorter taking just the subtitling example or in IR.",
                    "label": 0
                },
                {
                    "sent": "You want to retrieve documents for records and disks and use the word disk as an expanding work if.",
                    "label": 0
                },
                {
                    "sent": "The text in which disk occurs is in the musical sense.",
                    "label": 0
                },
                {
                    "sent": "Then this could entail record if it's, let's say, the computer sends.",
                    "label": 0
                },
                {
                    "sent": "It won't entail record.",
                    "label": 0
                },
                {
                    "sent": "So implicitly you are doing kind of word simulation, but without referring to an explicit set of sensors.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of a neat way to get around the whole problem of defining explicitly sensors follows the philosophy that I mentioned yesterday, and we had a paper at ACL on that.",
                    "label": 0
                },
                {
                    "sent": "Then we have a project taking this approach to IR, and another one for unsupervised exploration.",
                    "label": 0
                },
                {
                    "sent": "These are government funded projects in Israel.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just as a take out, I think the nice thing from this project beyond the follow-up activities it triggered for the partners, was that I think it identifies new, interesting research problems within this.",
                    "label": 0
                },
                {
                    "sent": "Under this paradigm of entailment, and if you look at the basically the two first bullet specifies there's 2 interesting tasks.",
                    "label": 0
                },
                {
                    "sent": "One is identified prior.",
                    "label": 0
                },
                {
                    "sent": "Likelihood of entailments of this kind of corresponds to build to learning this entailment network between.",
                    "label": 0
                },
                {
                    "sent": "Between lexical terms, so that could be considered as the prior or network OK, which words couldn't tell the other records, couldn't ale disk, etc.",
                    "label": 0
                },
                {
                    "sent": "And the second problem is like this posterior problem.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, this can entail record, but does it entail record in this context or in other context is kind of implicitly simulation sense, so I think these are very interesting machine learning problems that come up in this context and you consider them as kind of unsupervised variants of earlier problems.",
                    "label": 0
                },
                {
                    "sent": "And I think this can be nicely evaluated in different settings, particularly just to give.",
                    "label": 0
                },
                {
                    "sent": "Do I think easy ways to evaluate models for these cases?",
                    "label": 0
                },
                {
                    "sent": "One would be one thing that I suggest that it would be nice if there is a challenge on something like that is just to take standard text colorization datasets that are available and they.",
                    "label": 0
                },
                {
                    "sent": "Particularly highlighting text colorization, 'cause this was popular in machine learning community to apply models to text categorization, and you have the data sets.",
                    "label": 0
                },
                {
                    "sent": "So rather than using the training data in the supervised way, take an unsupervised approach.",
                    "label": 0
                },
                {
                    "sent": "You want to classify text to medicine or too low over to acquisitions.",
                    "label": 0
                },
                {
                    "sent": "Just try to learn models like we did Bar Illan an idiot that predict entailment of the word law of the word medicine, or the word acquisition OK. And start only with the title of the category, which is a word right?",
                    "label": 0
                },
                {
                    "sent": "The title of the country by itself is a language word and try in an unsupervised way to learn models that entail the meaning of this category and then just test them on standard text colorization datasets.",
                    "label": 0
                },
                {
                    "sent": "This is projects we have now think it's very useful and very easy to run.",
                    "label": 0
                },
                {
                    "sent": "Just use available.",
                    "label": 0
                },
                {
                    "sent": "We use the 20 newsgroups in the Reuters and a bunch of taxations.",
                    "label": 0
                },
                {
                    "sent": "Another one may use the RT datasets.",
                    "label": 0
                },
                {
                    "sent": "I mean just take the RT day to try to predict weather entailment holds, but just using the lexical approach and lexical approaches were actually.",
                    "label": 0
                },
                {
                    "sent": "Pretty good so.",
                    "label": 0
                },
                {
                    "sent": "Maybe you won't beat methods that use also syntax and other things, but just to develop the lexical models.",
                    "label": 0
                },
                {
                    "sent": "It would be very interesting to compare different models and there are some baselines in the art literature.",
                    "label": 0
                },
                {
                    "sent": "Some people intentionally just restrict themselves to lexical models to explore how well they can perform.",
                    "label": 0
                },
                {
                    "sent": "I think methods of this sort would be very nice to try and develop more rigorous machine learning models for these methods.",
                    "label": 0
                },
                {
                    "sent": "OK, that's my take out.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}