{
    "id": "vblr34yb4icutpduvqglqws5kxnfuhnz",
    "title": "Large-Scale Deep Unsupervised Learning Using Graphics Processors",
    "info": {
        "author": [
            "Rajat Raina, Computer Science Department, Stanford University"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Unsupervised Learning",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_raina_lsd/",
    "segmentation": [
        [
            "I'm going to be talking about using unlabeled data in machine learning and just a motor."
        ],
        [
            "I ate some of the methods I'll talk about.",
            "Let me start with a simple example of image classification.",
            "So we are given some examples of cars in motorcycle images and the task is to predict whether an image is a car or a motorcycle.",
            "Now, of course, if you represent these images as a set of pixels.",
            "Then this task appears hard because it might be that the cars and the motorcycles are hard to discriminate with simple machine learning classifiers.",
            "So the way you can use unlabeled data is you take a large number of unlabeled examples.",
            "The keyword being large number.",
            "You use that to learn a high level or deep representation of these inputs in such a way that if you look at the final representation, you learn magically now cars and motorcycles are well separated in this space and you can use your favorite classifier.",
            "To perform the classification.",
            "Now in this talk, I'll focus on two particular models for this deep belief networks and sparse coding.",
            "But even irrespective of the model.",
            "It just appears that to map the space of pixels into a space that is selective enough to distinguish between cars and motorcycles, we might need fairly complex or deep models and we might need lots and lots of unlabeled examples to learn these.",
            "Learn such representations."
        ],
        [
            "So the promise of unsupervised learning really is we should be able to use lots and lots of unlabeled data to learn complex or deep models which might have.",
            "Many, many parameters."
        ],
        [
            "If you look at recent work on, say, deep belief networks, it turns out that.",
            "The range of the models that we can learn is actually much smaller than what we would like to learn.",
            "So one of the models we would like to consider over images.",
            "Consists of around 100 million parameters.",
            "If you look at some of the recent work in the literature.",
            "Notice that the number of parameters in those models range in the order of a few millions.",
            "Now it is possible that higher models higher scale models can be learned if you give it sufficient time.",
            "But still the order of magnitude difference between where we'd like to be an where we are appears to be striking.",
            "So this is just about the number of parameters.",
            "What about amount of data we can learn from?",
            "So in machine learning it has been shown again and again that the quality of an algorithm could depend quite a lot on what you train it on.",
            "So here's an example from supervised learning or from a paper by."
        ],
        [
            "And Cohen Brill.",
            "What they show is that on the X axis is the amount of training data you have.",
            "So in leaps of 10.",
            "And on the Y axis is the test accuracy obtained by one of five different learning algorithms.",
            "The task or the learning algorithms themselves are not that important.",
            "What I want to say is that a lot of work in machine learning is focused just on the Y axis.",
            "So for this data set, this algorithm is better than that other algorithm.",
            "But one thing to note is if you start at this point.",
            "And you increase the label data 10 times or increase the data 10 times.",
            "Then it might be that even the worst algorithm earlier is better than the best algorithm with 10 times less data.",
            "And you can do that one more time.",
            "So again the worst algorithm is better than the best algorithm.",
            "OK.",
            "So that seems to illustrate that scale is really important.",
            "Also, one thing to note is eventually when you have enough data, the model stop still stops learning anything new.",
            "But the model that is best at that point.",
            "Might be different from the model that was best at a small scale.",
            "This means that if you are working developing algorithms at a small scale, even though we want to apply them at a large scale.",
            "The results may not match up exactly."
        ],
        [
            "So, just to summarize, current models for deep belief networks or sparse coding can handle on the order of thousands of input dimensions, thousands of hidden units, or about of the order of a million parameters, where we'd like to be is, say, around 100 million parameters.",
            "So that's what I'm going to be focusing on, and there have been recently many, many papers developing ingenious methods for doing this.",
            "For taking this leap.",
            "That makes it seem even more worthwhile to consider advances in hardware.",
            "To go from a million to 100 million parameters in particular, I'm not going to use any special purpose hardware in this talk, I'm just going to use graphics cards that many of you might already have at home on your desktops, and you can even go to Walmart and buy one if you need.",
            "So."
        ],
        [
            "Why graphics cards?",
            "So if you haven't seen this plot before, it's quite informative.",
            "This shows over the last six years or so.",
            "The growth and processing power of GPU's or graphics processors from NVIDIA.",
            "An high end Intel CPUs on the Y axis is the peak number of gigaflops or billions of operations per second.",
            "What you see from here is that.",
            "In the last six or seven years, GPU's have vastly outperformed CPUs and this is understandable because CPU's are optimized for.",
            "The execution speed of a single thread, so they need lots of machinery, caching, control flow and so on just to optimize one thread, but often for many tasks you don't really need one thread to be really fast.",
            "You need the throughput to be really fast, and for that GPU's are much better than CPUs and for that reason I think there can be applicable to many machine learning problems.",
            "Still number some."
        ],
        [
            "Times don't tell the full story, so here's a picture.",
            "This is a picture of the top supercomputer in 2000.",
            "Took about 2 basketball courts of space.",
            "This has about as much peak processing power as 13 graphics cards that you can again buy from Walmart today.",
            "Skin.",
            "So.",
            "Complete introduction to graphics hardware is of course beyond the scope of this talk.",
            "But we really need a small nudge."
        ],
        [
            "I have information from graphics hardware and I'll try to introduce that in the next 2 slides.",
            "The graphics cards that I use look as follows.",
            "They have two levels of parallelism.",
            "At the top you have these things called MPs, Multiprocessors.",
            "Inside each multiprocessor you have a bunch of stream processors, which are the actual cores that run the computation.",
            "So in this case you have 30 * 8 or 240 cores and you have various types of memory.",
            "A small amount of shared memory, an large amount of global memory on the graphics card which can communicate with the RAM on on your desktop.",
            "The main thing I want to focus on for this talk is just the communication with these types of memory.",
            "So at the top this small amount of memory is really really fast.",
            "OK, but you have only a small amount of it.",
            "For the most part, we are stuck with using the global memory.",
            "Now the global memory also is fast and can be accessed in parallel.",
            "The only problem is there are certain restrictions on access which disallow some types of algorithms that might make random access to the data.",
            "I won't go into the technical details, but that's the bottom line and the biggest bottleneck often is getting the data from RAM into the global memory that is the slowest part.",
            "So how do you write a program for this thing?",
            "Well, you have two levels."
        ],
        [
            "Parallelism, so you break up your task into 2 levels of parallelism.",
            "They're called often blocks that fit the multiprocessors and threads that fit the stream processors.",
            "Again, to rephrase, the earlier constraints we had constraints on this axis.",
            "It had to follow certain access patterns.",
            "So what that means is we are restricted to certain types of operations only.",
            "And the second bottleneck was the transfer from Ram.",
            "So we want to minimize the amounts of data we want to transfer into the graphics card or out of it.",
            "Till now it seems like you really need to go deep into the hardware to figure out anything, but turns out there are really good high level libraries that look a lot like C. Which give you high level routines to manipulate GPU memory.",
            "An lots of matrix libraries that can help with many machine learning tasks, so it's not all that bad to program these things.",
            "OK, now I'll attempt to give a general template for."
        ],
        [
            "Um?",
            "Unsupervised learning with GPS, of course, without specifying the algorithm, it's difficult to get too specific.",
            "But the basic idea is we'll keep all the parameters in global memory as much as we can.",
            "So we'll initialize all the parameters there and keep them there.",
            "All will transfer into the global memory is large batches of unlabeled data that will process and then bring in the next batch.",
            "So we just iterate over bringing in a large number of examples into memory and then picking small batches of those examples and computing the updates in parallel.",
            "Those updates have to be computed keeping the specific constraints in mind, so you might have to use GPU matrix libraries or the parallelism in special ways, but that's about all there is to it.",
            "Let's see what what you can do with this specific model."
        ],
        [
            "Like a deep belief net.",
            "So well, the details were introduced in previous talks as well.",
            "It's basically a undirected graphical model with visible layer Anna hidden layer."
        ],
        [
            "All I really, really want to appeal to here is that the parameters are encoded in WC&B.",
            "And contrastive divergent learning can be done very easily.",
            "If you can just compute these operations so these are some simple matrix operations.",
            "Which say that given the visible layer, can you sample a hidden layer and given a hidden layer, can you sample visible layer?",
            "So without going into too much details by maintaining all the parameters in memory and just performing these updates using matrix operations in some other implementation details.",
            "Turns out you can do all this very well and the proof is released in the results, so here."
        ],
        [
            "Is set up.",
            "I'm going to use.",
            "I'm using a single graphics card nowadays.",
            "You can use multiple graphics cards.",
            "The current price as I said is not much just $250 and I'm comparing with a dual core CPU.",
            "This is not state of the art, but.",
            "This is what?",
            "So what I'm showing here is the learning time for a single law."
        ],
        [
            "Large PBM on the X axis is the millions of parameters in the RBM.",
            "On the Y axis is the learning time taken for processing 10,000,000 examples.",
            "So you'll notice that a lot of the work lot of the published recent workin on DBN's is focused on the left half of this graph, because once it starts to take weeks to run single experiments, it's really difficult to make progress, but turns out with GPUs, especially for large models.",
            "You get really massive speedups on what you can get using single CPU's using Dual core CPUs.",
            "OK.",
            "So this is nice.",
            "Another thing that this allows you to do is."
        ],
        [
            "To learn new types of models so well, what I call the overlapping patches DBN is basically hung locks convolutional RBM model without weight sharing.",
            "So you have a large image tiled by patches or receptive fields.",
            "Each of those patches is connected to a set of hidden units with some weights for this little RBM.",
            "An different weights for different patches, of course, because the weights are not tide, we have a large number of free parameters now, and learning in this model can be really really slow.",
            "And again, it's slightly more complicated than a regular PBM, but you can still do it on the GPU and."
        ],
        [
            "Here's an example model, so you could have an input image, say 100 and 44144 pixels.",
            "You could take 24 by 24 pixel receptive fields or patches.",
            "Connect all of them to 128 units, and so on.",
            "An you could build up the model.",
            "This is 1 specific model that ends up having around 110 million parameters and you can learn all of these in about one day.",
            "On the GPU.",
            "OK, let me."
        ],
        [
            "I want to sparse coding, so just."
        ],
        [
            "Briefly, in sparse coding, I will illustrate that with an image example, if you're given an input image.",
            "Sparse coding tries to find a set of patterns or basis vectors.",
            "Such that the input can be represented as a linear combination of only a few of those basis vectors.",
            "So in this case this input is a combination of these three edge like basis vectors.",
            "These weights themselves are called activations.",
            "And the goal of sparse coding is to learn these basis vectors and it turns out if you do that, you often get a decomposition of the input into somewhat high level features.",
            "Edges in this case, and so the activations form a higher level representation for the inputs.",
            "I don't want to get too much into the details or the notation, But basically this is the classical problem that you might solve given lots of unlabeled data.",
            "You might optimize this problem over the B&A variables.",
            "The first term says that.",
            "The exercise should be well represented by the reconstruction and the second term penalizes the L1 norm of the activations, just forcing most of the activations to be at 0, so the activations are sparse.",
            "Now, classical algorithms on the CPU for this method use alternating minimization."
        ],
        [
            "Where you start with random values, you keep a fixed.",
            "Find the optimal B and then you keep be fixed and find the optimal aid.",
            "Both of these are convex problems, so they can be solved.",
            "If you try to apply this to the GPU, well, the first step turns out to be relatively easy.",
            "You can do projected gradient descent and.",
            "Essentially because you don't have this problematic L one term.",
            "You can do it.",
            "The second step is not as straightforward because if you keep be fixed and find the optimal A.",
            "You need to parallelize optimization problems such as this where you have quadratic term in A and a non differentiable term in AL.",
            "One norm turns out CPU algorithms don't apply very well to this because they often require sparse linear algebra or other access patterns that are hard to support on the GPU.",
            "So."
        ],
        [
            "Turns out what you can do is.",
            "You can use the following observation.",
            "It is very easy to optimize for just a single coordinate, keeping the others fixed.",
            "So if you were to optimize this just over, say AJ, keeping everything else it fixed, it's really easy to do that.",
            "So our algorithm will proceed.",
            "Free.",
            "Our algorithm will proceed in the following way.",
            "We'll start with some current estimates for the activations a.",
            "Then, in parallel, we'll compute all the coordinate wise updates.",
            "So even star is the update just for the first activation.",
            "ETA start just for the second activation and this is inherently parallel.",
            "You can really compute that on separate cores.",
            "Of course, once you have all these updates, how do you combine them?",
            "What you can do is you can show that the combination of all these updates is a decent direction for the overall objective.",
            "And because it is a decent direction, you can do a line search along this direction.",
            "To find a new point to move to.",
            "So this is 1 iteration of the algorithm where we went from a in an inherently parallel way to a new set of values.",
            "And just by doing that, turns out you can do much better again.",
            "So on the X axis.",
            "Now I have."
        ],
        [
            "A model parameter that controls the sparsity of the solutions.",
            "And again, you can reduce the time for learning from weeks down to about a day.",
            "Oh, the possibilities seem immense."
        ],
        [
            "So, just to summarize, I would argue that large scale unsupervised learning is important for two reasons.",
            "1st just handling 10 times more data might make an OK algorithm into a good algorithm, which means that scaling up these algorithms appears as important as devising new algorithms.",
            "The second thing is especially for unsupervised learning, we are often motivated by the fact that there are lots and lots of unlabeled examples.",
            "So if you eventually want to apply it to lots and lots of unlabeled examples, working at a smaller scale could confound the effects of the model itself with the effects of scale.",
            "GPU's are powerful tool for machine learning.",
            "They are much easier to program than at least I first thought and they are especially useful for stochastic learning methods where each individual update is actually quite fast.",
            "But there are lots and lots of these updates.",
            "And in this talk I just showed methods for applying GPS to deep belief networks and sparse coding leading to order of magnitude improvements.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to be talking about using unlabeled data in machine learning and just a motor.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I ate some of the methods I'll talk about.",
                    "label": 0
                },
                {
                    "sent": "Let me start with a simple example of image classification.",
                    "label": 0
                },
                {
                    "sent": "So we are given some examples of cars in motorcycle images and the task is to predict whether an image is a car or a motorcycle.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, if you represent these images as a set of pixels.",
                    "label": 0
                },
                {
                    "sent": "Then this task appears hard because it might be that the cars and the motorcycles are hard to discriminate with simple machine learning classifiers.",
                    "label": 0
                },
                {
                    "sent": "So the way you can use unlabeled data is you take a large number of unlabeled examples.",
                    "label": 1
                },
                {
                    "sent": "The keyword being large number.",
                    "label": 0
                },
                {
                    "sent": "You use that to learn a high level or deep representation of these inputs in such a way that if you look at the final representation, you learn magically now cars and motorcycles are well separated in this space and you can use your favorite classifier.",
                    "label": 0
                },
                {
                    "sent": "To perform the classification.",
                    "label": 0
                },
                {
                    "sent": "Now in this talk, I'll focus on two particular models for this deep belief networks and sparse coding.",
                    "label": 1
                },
                {
                    "sent": "But even irrespective of the model.",
                    "label": 0
                },
                {
                    "sent": "It just appears that to map the space of pixels into a space that is selective enough to distinguish between cars and motorcycles, we might need fairly complex or deep models and we might need lots and lots of unlabeled examples to learn these.",
                    "label": 0
                },
                {
                    "sent": "Learn such representations.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the promise of unsupervised learning really is we should be able to use lots and lots of unlabeled data to learn complex or deep models which might have.",
                    "label": 0
                },
                {
                    "sent": "Many, many parameters.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you look at recent work on, say, deep belief networks, it turns out that.",
                    "label": 1
                },
                {
                    "sent": "The range of the models that we can learn is actually much smaller than what we would like to learn.",
                    "label": 1
                },
                {
                    "sent": "So one of the models we would like to consider over images.",
                    "label": 1
                },
                {
                    "sent": "Consists of around 100 million parameters.",
                    "label": 0
                },
                {
                    "sent": "If you look at some of the recent work in the literature.",
                    "label": 0
                },
                {
                    "sent": "Notice that the number of parameters in those models range in the order of a few millions.",
                    "label": 0
                },
                {
                    "sent": "Now it is possible that higher models higher scale models can be learned if you give it sufficient time.",
                    "label": 1
                },
                {
                    "sent": "But still the order of magnitude difference between where we'd like to be an where we are appears to be striking.",
                    "label": 0
                },
                {
                    "sent": "So this is just about the number of parameters.",
                    "label": 0
                },
                {
                    "sent": "What about amount of data we can learn from?",
                    "label": 0
                },
                {
                    "sent": "So in machine learning it has been shown again and again that the quality of an algorithm could depend quite a lot on what you train it on.",
                    "label": 0
                },
                {
                    "sent": "So here's an example from supervised learning or from a paper by.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And Cohen Brill.",
                    "label": 0
                },
                {
                    "sent": "What they show is that on the X axis is the amount of training data you have.",
                    "label": 0
                },
                {
                    "sent": "So in leaps of 10.",
                    "label": 0
                },
                {
                    "sent": "And on the Y axis is the test accuracy obtained by one of five different learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "The task or the learning algorithms themselves are not that important.",
                    "label": 0
                },
                {
                    "sent": "What I want to say is that a lot of work in machine learning is focused just on the Y axis.",
                    "label": 0
                },
                {
                    "sent": "So for this data set, this algorithm is better than that other algorithm.",
                    "label": 0
                },
                {
                    "sent": "But one thing to note is if you start at this point.",
                    "label": 0
                },
                {
                    "sent": "And you increase the label data 10 times or increase the data 10 times.",
                    "label": 0
                },
                {
                    "sent": "Then it might be that even the worst algorithm earlier is better than the best algorithm with 10 times less data.",
                    "label": 0
                },
                {
                    "sent": "And you can do that one more time.",
                    "label": 0
                },
                {
                    "sent": "So again the worst algorithm is better than the best algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that seems to illustrate that scale is really important.",
                    "label": 0
                },
                {
                    "sent": "Also, one thing to note is eventually when you have enough data, the model stop still stops learning anything new.",
                    "label": 0
                },
                {
                    "sent": "But the model that is best at that point.",
                    "label": 0
                },
                {
                    "sent": "Might be different from the model that was best at a small scale.",
                    "label": 0
                },
                {
                    "sent": "This means that if you are working developing algorithms at a small scale, even though we want to apply them at a large scale.",
                    "label": 0
                },
                {
                    "sent": "The results may not match up exactly.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, just to summarize, current models for deep belief networks or sparse coding can handle on the order of thousands of input dimensions, thousands of hidden units, or about of the order of a million parameters, where we'd like to be is, say, around 100 million parameters.",
                    "label": 1
                },
                {
                    "sent": "So that's what I'm going to be focusing on, and there have been recently many, many papers developing ingenious methods for doing this.",
                    "label": 0
                },
                {
                    "sent": "For taking this leap.",
                    "label": 0
                },
                {
                    "sent": "That makes it seem even more worthwhile to consider advances in hardware.",
                    "label": 0
                },
                {
                    "sent": "To go from a million to 100 million parameters in particular, I'm not going to use any special purpose hardware in this talk, I'm just going to use graphics cards that many of you might already have at home on your desktops, and you can even go to Walmart and buy one if you need.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Why graphics cards?",
                    "label": 0
                },
                {
                    "sent": "So if you haven't seen this plot before, it's quite informative.",
                    "label": 0
                },
                {
                    "sent": "This shows over the last six years or so.",
                    "label": 0
                },
                {
                    "sent": "The growth and processing power of GPU's or graphics processors from NVIDIA.",
                    "label": 1
                },
                {
                    "sent": "An high end Intel CPUs on the Y axis is the peak number of gigaflops or billions of operations per second.",
                    "label": 0
                },
                {
                    "sent": "What you see from here is that.",
                    "label": 0
                },
                {
                    "sent": "In the last six or seven years, GPU's have vastly outperformed CPUs and this is understandable because CPU's are optimized for.",
                    "label": 0
                },
                {
                    "sent": "The execution speed of a single thread, so they need lots of machinery, caching, control flow and so on just to optimize one thread, but often for many tasks you don't really need one thread to be really fast.",
                    "label": 0
                },
                {
                    "sent": "You need the throughput to be really fast, and for that GPU's are much better than CPUs and for that reason I think there can be applicable to many machine learning problems.",
                    "label": 0
                },
                {
                    "sent": "Still number some.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Times don't tell the full story, so here's a picture.",
                    "label": 0
                },
                {
                    "sent": "This is a picture of the top supercomputer in 2000.",
                    "label": 0
                },
                {
                    "sent": "Took about 2 basketball courts of space.",
                    "label": 0
                },
                {
                    "sent": "This has about as much peak processing power as 13 graphics cards that you can again buy from Walmart today.",
                    "label": 0
                },
                {
                    "sent": "Skin.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Complete introduction to graphics hardware is of course beyond the scope of this talk.",
                    "label": 0
                },
                {
                    "sent": "But we really need a small nudge.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I have information from graphics hardware and I'll try to introduce that in the next 2 slides.",
                    "label": 0
                },
                {
                    "sent": "The graphics cards that I use look as follows.",
                    "label": 1
                },
                {
                    "sent": "They have two levels of parallelism.",
                    "label": 0
                },
                {
                    "sent": "At the top you have these things called MPs, Multiprocessors.",
                    "label": 0
                },
                {
                    "sent": "Inside each multiprocessor you have a bunch of stream processors, which are the actual cores that run the computation.",
                    "label": 0
                },
                {
                    "sent": "So in this case you have 30 * 8 or 240 cores and you have various types of memory.",
                    "label": 0
                },
                {
                    "sent": "A small amount of shared memory, an large amount of global memory on the graphics card which can communicate with the RAM on on your desktop.",
                    "label": 0
                },
                {
                    "sent": "The main thing I want to focus on for this talk is just the communication with these types of memory.",
                    "label": 0
                },
                {
                    "sent": "So at the top this small amount of memory is really really fast.",
                    "label": 0
                },
                {
                    "sent": "OK, but you have only a small amount of it.",
                    "label": 0
                },
                {
                    "sent": "For the most part, we are stuck with using the global memory.",
                    "label": 0
                },
                {
                    "sent": "Now the global memory also is fast and can be accessed in parallel.",
                    "label": 0
                },
                {
                    "sent": "The only problem is there are certain restrictions on access which disallow some types of algorithms that might make random access to the data.",
                    "label": 0
                },
                {
                    "sent": "I won't go into the technical details, but that's the bottom line and the biggest bottleneck often is getting the data from RAM into the global memory that is the slowest part.",
                    "label": 0
                },
                {
                    "sent": "So how do you write a program for this thing?",
                    "label": 0
                },
                {
                    "sent": "Well, you have two levels.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Parallelism, so you break up your task into 2 levels of parallelism.",
                    "label": 0
                },
                {
                    "sent": "They're called often blocks that fit the multiprocessors and threads that fit the stream processors.",
                    "label": 0
                },
                {
                    "sent": "Again, to rephrase, the earlier constraints we had constraints on this axis.",
                    "label": 0
                },
                {
                    "sent": "It had to follow certain access patterns.",
                    "label": 0
                },
                {
                    "sent": "So what that means is we are restricted to certain types of operations only.",
                    "label": 0
                },
                {
                    "sent": "And the second bottleneck was the transfer from Ram.",
                    "label": 1
                },
                {
                    "sent": "So we want to minimize the amounts of data we want to transfer into the graphics card or out of it.",
                    "label": 0
                },
                {
                    "sent": "Till now it seems like you really need to go deep into the hardware to figure out anything, but turns out there are really good high level libraries that look a lot like C. Which give you high level routines to manipulate GPU memory.",
                    "label": 0
                },
                {
                    "sent": "An lots of matrix libraries that can help with many machine learning tasks, so it's not all that bad to program these things.",
                    "label": 0
                },
                {
                    "sent": "OK, now I'll attempt to give a general template for.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Unsupervised learning with GPS, of course, without specifying the algorithm, it's difficult to get too specific.",
                    "label": 0
                },
                {
                    "sent": "But the basic idea is we'll keep all the parameters in global memory as much as we can.",
                    "label": 0
                },
                {
                    "sent": "So we'll initialize all the parameters there and keep them there.",
                    "label": 0
                },
                {
                    "sent": "All will transfer into the global memory is large batches of unlabeled data that will process and then bring in the next batch.",
                    "label": 0
                },
                {
                    "sent": "So we just iterate over bringing in a large number of examples into memory and then picking small batches of those examples and computing the updates in parallel.",
                    "label": 0
                },
                {
                    "sent": "Those updates have to be computed keeping the specific constraints in mind, so you might have to use GPU matrix libraries or the parallelism in special ways, but that's about all there is to it.",
                    "label": 0
                },
                {
                    "sent": "Let's see what what you can do with this specific model.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like a deep belief net.",
                    "label": 0
                },
                {
                    "sent": "So well, the details were introduced in previous talks as well.",
                    "label": 0
                },
                {
                    "sent": "It's basically a undirected graphical model with visible layer Anna hidden layer.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All I really, really want to appeal to here is that the parameters are encoded in WC&B.",
                    "label": 0
                },
                {
                    "sent": "And contrastive divergent learning can be done very easily.",
                    "label": 0
                },
                {
                    "sent": "If you can just compute these operations so these are some simple matrix operations.",
                    "label": 0
                },
                {
                    "sent": "Which say that given the visible layer, can you sample a hidden layer and given a hidden layer, can you sample visible layer?",
                    "label": 0
                },
                {
                    "sent": "So without going into too much details by maintaining all the parameters in memory and just performing these updates using matrix operations in some other implementation details.",
                    "label": 0
                },
                {
                    "sent": "Turns out you can do all this very well and the proof is released in the results, so here.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is set up.",
                    "label": 0
                },
                {
                    "sent": "I'm going to use.",
                    "label": 0
                },
                {
                    "sent": "I'm using a single graphics card nowadays.",
                    "label": 0
                },
                {
                    "sent": "You can use multiple graphics cards.",
                    "label": 0
                },
                {
                    "sent": "The current price as I said is not much just $250 and I'm comparing with a dual core CPU.",
                    "label": 0
                },
                {
                    "sent": "This is not state of the art, but.",
                    "label": 0
                },
                {
                    "sent": "This is what?",
                    "label": 0
                },
                {
                    "sent": "So what I'm showing here is the learning time for a single law.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Large PBM on the X axis is the millions of parameters in the RBM.",
                    "label": 0
                },
                {
                    "sent": "On the Y axis is the learning time taken for processing 10,000,000 examples.",
                    "label": 0
                },
                {
                    "sent": "So you'll notice that a lot of the work lot of the published recent workin on DBN's is focused on the left half of this graph, because once it starts to take weeks to run single experiments, it's really difficult to make progress, but turns out with GPUs, especially for large models.",
                    "label": 0
                },
                {
                    "sent": "You get really massive speedups on what you can get using single CPU's using Dual core CPUs.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is nice.",
                    "label": 0
                },
                {
                    "sent": "Another thing that this allows you to do is.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To learn new types of models so well, what I call the overlapping patches DBN is basically hung locks convolutional RBM model without weight sharing.",
                    "label": 0
                },
                {
                    "sent": "So you have a large image tiled by patches or receptive fields.",
                    "label": 0
                },
                {
                    "sent": "Each of those patches is connected to a set of hidden units with some weights for this little RBM.",
                    "label": 0
                },
                {
                    "sent": "An different weights for different patches, of course, because the weights are not tide, we have a large number of free parameters now, and learning in this model can be really really slow.",
                    "label": 0
                },
                {
                    "sent": "And again, it's slightly more complicated than a regular PBM, but you can still do it on the GPU and.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example model, so you could have an input image, say 100 and 44144 pixels.",
                    "label": 0
                },
                {
                    "sent": "You could take 24 by 24 pixel receptive fields or patches.",
                    "label": 0
                },
                {
                    "sent": "Connect all of them to 128 units, and so on.",
                    "label": 0
                },
                {
                    "sent": "An you could build up the model.",
                    "label": 0
                },
                {
                    "sent": "This is 1 specific model that ends up having around 110 million parameters and you can learn all of these in about one day.",
                    "label": 0
                },
                {
                    "sent": "On the GPU.",
                    "label": 0
                },
                {
                    "sent": "OK, let me.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to sparse coding, so just.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Briefly, in sparse coding, I will illustrate that with an image example, if you're given an input image.",
                    "label": 0
                },
                {
                    "sent": "Sparse coding tries to find a set of patterns or basis vectors.",
                    "label": 1
                },
                {
                    "sent": "Such that the input can be represented as a linear combination of only a few of those basis vectors.",
                    "label": 0
                },
                {
                    "sent": "So in this case this input is a combination of these three edge like basis vectors.",
                    "label": 0
                },
                {
                    "sent": "These weights themselves are called activations.",
                    "label": 0
                },
                {
                    "sent": "And the goal of sparse coding is to learn these basis vectors and it turns out if you do that, you often get a decomposition of the input into somewhat high level features.",
                    "label": 0
                },
                {
                    "sent": "Edges in this case, and so the activations form a higher level representation for the inputs.",
                    "label": 0
                },
                {
                    "sent": "I don't want to get too much into the details or the notation, But basically this is the classical problem that you might solve given lots of unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "You might optimize this problem over the B&A variables.",
                    "label": 0
                },
                {
                    "sent": "The first term says that.",
                    "label": 0
                },
                {
                    "sent": "The exercise should be well represented by the reconstruction and the second term penalizes the L1 norm of the activations, just forcing most of the activations to be at 0, so the activations are sparse.",
                    "label": 0
                },
                {
                    "sent": "Now, classical algorithms on the CPU for this method use alternating minimization.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Where you start with random values, you keep a fixed.",
                    "label": 1
                },
                {
                    "sent": "Find the optimal B and then you keep be fixed and find the optimal aid.",
                    "label": 0
                },
                {
                    "sent": "Both of these are convex problems, so they can be solved.",
                    "label": 0
                },
                {
                    "sent": "If you try to apply this to the GPU, well, the first step turns out to be relatively easy.",
                    "label": 0
                },
                {
                    "sent": "You can do projected gradient descent and.",
                    "label": 0
                },
                {
                    "sent": "Essentially because you don't have this problematic L one term.",
                    "label": 0
                },
                {
                    "sent": "You can do it.",
                    "label": 0
                },
                {
                    "sent": "The second step is not as straightforward because if you keep be fixed and find the optimal A.",
                    "label": 0
                },
                {
                    "sent": "You need to parallelize optimization problems such as this where you have quadratic term in A and a non differentiable term in AL.",
                    "label": 0
                },
                {
                    "sent": "One norm turns out CPU algorithms don't apply very well to this because they often require sparse linear algebra or other access patterns that are hard to support on the GPU.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Turns out what you can do is.",
                    "label": 0
                },
                {
                    "sent": "You can use the following observation.",
                    "label": 0
                },
                {
                    "sent": "It is very easy to optimize for just a single coordinate, keeping the others fixed.",
                    "label": 1
                },
                {
                    "sent": "So if you were to optimize this just over, say AJ, keeping everything else it fixed, it's really easy to do that.",
                    "label": 1
                },
                {
                    "sent": "So our algorithm will proceed.",
                    "label": 0
                },
                {
                    "sent": "Free.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm will proceed in the following way.",
                    "label": 0
                },
                {
                    "sent": "We'll start with some current estimates for the activations a.",
                    "label": 0
                },
                {
                    "sent": "Then, in parallel, we'll compute all the coordinate wise updates.",
                    "label": 0
                },
                {
                    "sent": "So even star is the update just for the first activation.",
                    "label": 0
                },
                {
                    "sent": "ETA start just for the second activation and this is inherently parallel.",
                    "label": 0
                },
                {
                    "sent": "You can really compute that on separate cores.",
                    "label": 0
                },
                {
                    "sent": "Of course, once you have all these updates, how do you combine them?",
                    "label": 0
                },
                {
                    "sent": "What you can do is you can show that the combination of all these updates is a decent direction for the overall objective.",
                    "label": 0
                },
                {
                    "sent": "And because it is a decent direction, you can do a line search along this direction.",
                    "label": 0
                },
                {
                    "sent": "To find a new point to move to.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 iteration of the algorithm where we went from a in an inherently parallel way to a new set of values.",
                    "label": 0
                },
                {
                    "sent": "And just by doing that, turns out you can do much better again.",
                    "label": 0
                },
                {
                    "sent": "So on the X axis.",
                    "label": 0
                },
                {
                    "sent": "Now I have.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A model parameter that controls the sparsity of the solutions.",
                    "label": 0
                },
                {
                    "sent": "And again, you can reduce the time for learning from weeks down to about a day.",
                    "label": 0
                },
                {
                    "sent": "Oh, the possibilities seem immense.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, just to summarize, I would argue that large scale unsupervised learning is important for two reasons.",
                    "label": 0
                },
                {
                    "sent": "1st just handling 10 times more data might make an OK algorithm into a good algorithm, which means that scaling up these algorithms appears as important as devising new algorithms.",
                    "label": 0
                },
                {
                    "sent": "The second thing is especially for unsupervised learning, we are often motivated by the fact that there are lots and lots of unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "So if you eventually want to apply it to lots and lots of unlabeled examples, working at a smaller scale could confound the effects of the model itself with the effects of scale.",
                    "label": 0
                },
                {
                    "sent": "GPU's are powerful tool for machine learning.",
                    "label": 0
                },
                {
                    "sent": "They are much easier to program than at least I first thought and they are especially useful for stochastic learning methods where each individual update is actually quite fast.",
                    "label": 0
                },
                {
                    "sent": "But there are lots and lots of these updates.",
                    "label": 0
                },
                {
                    "sent": "And in this talk I just showed methods for applying GPS to deep belief networks and sparse coding leading to order of magnitude improvements.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}