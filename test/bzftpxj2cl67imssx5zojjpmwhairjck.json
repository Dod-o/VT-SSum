{
    "id": "bzftpxj2cl67imssx5zojjpmwhairjck",
    "title": "Reinforcement Learning",
    "info": {
        "author": [
            "R\u00e9mi Munos, SequeL lab, INRIA Lille - Nord Europe"
        ],
        "published": "Nov. 21, 2013",
        "recorded": "September 2013",
        "category": [
            "Top->Physics->Statistical Physics",
            "Top->Computer Science->Network Analysis"
        ]
    },
    "url": "http://videolectures.net/netadis2013_munos_reinforcement_learning/",
    "segmentation": [
        [
            "Good morning everybody, thanks for the invitation to give a course on reinforcement learning.",
            "To this summer school.",
            "So so yes, so I am researcher at INRIA Lille in France on an I'm currently on leave Nunca at Microsoft Research in New England lab.",
            "And so so I typed all this, this, this course reinforcement learning, introduction to reinforcement learning.",
            "And I added on multi arm bandits.",
            "So I would like also to give a brief introduction to the field of multi arm bandits.",
            "Because I think it might be interesting and also it's very related to reinforcement learning.",
            "So I'm going to give us 3 three lectures."
        ],
        [
            "And the course is going to follow this this outline, so there will be three parts.",
            "About one is going to be a general introduction to reinforcement learning and dynamic programming will introduce the field of dynamic programming and.",
            "And so introduce several algorithm value iteration policy iteration and also will introduce the algorithm called Q Learning first algorithm in reinforcement learning.",
            "And then so this part basically consists of the problem of.",
            "So small problems where it says space is finite and where you can represent your function in with a very good accuracy, but in actual real problems the space is so large that you cannot represent your interesting functions perfectly everywhere.",
            "So you have to use approximations and so in the second part we will see.",
            "What are the impact of those approximations on the impact the impact on those of those approximations on the performance of the algorithms?",
            "So we're going to look at.",
            "So mentalities of dynamic programming and reinforcement learning when we consider approximations.",
            "And at the end we will do some connections with the field of statistical learning and then the third part will be an introduction to the field of multi arm bandits and so in different settings either in stochastic bandits or adversarial bandits.",
            "And then I will also mention some extensions such as computation of our approximation of Librium in games and also the field of Monte Carlo Tree Search.",
            "So by the way, this is intended to be a interactive course, so please don't hesitate to interrupt me or ask me questions.",
            "Whenever you want, because I guess the goal of this summer school is to, I mean to have interactions and so.",
            "So please ask me any question you wish.",
            "So I'd like to go as fast as possible during the first 2 lectures in order to arrive there and then spend 1 1/2 lecture.",
            "For for actually the last part part three, I would like to spend some time there, so I'm going to go really fast, but so if you don't understand something, just let me know and I will try to adjust to the right speed.",
            "So."
        ],
        [
            "So introduction to reinforcement learning.",
            "Just mention a few references.",
            "Serve old books, so book by Bessica synthetically Neurodynamic programming the book by Sutton and Barto on introduction to Reinforcement learning on Markov decision problems.",
            "There is a book by Martin Puterman and more recently there is a book by a championship is very on algorithm for reinforcement learning.",
            "So if you want to read more about the field.",
            "I will.",
            "I will recommend you to start with this one which is really introduction.",
            "This one is more more material into it, specially into function approximation and in combination with dynamic programming and this one is really kind of an update of reinforcement learning.",
            "So it's specially interesting now.",
            "So what is?"
        ],
        [
            "Interest money.",
            "So the goal of interesting reinforcement learning.",
            "Is to learn to make good decisions in unknown environments.",
            "So we want to learn.",
            "To make good decisions.",
            "So the goal is to make good decisions.",
            "Good decisions means.",
            "Make it take actions or follow some kind of policies or our goal is to follow a policy or strategy which tells us what we should do in each possible situation that we encounter and the way we're going to do that is by learning.",
            "So we're going to learn to act and the type of learning we will consider is learning from experience.",
            "So and more precisely, is kind of.",
            "Learning from trials and errors.",
            "So we're going to try something and then based on our things go, we will get some feedback on our previous actions.",
            "This feedback is called reinforcement or rewards and based on this information, we're going to improve our strategy in order to do better in the later rounds.",
            "So an example that illustrates this type of learning is, for example, a child that learns to ride a bicycle.",
            "So the way the child learns to ride a bicycle.",
            "Is not by writing down the stochastic differential equations of the system and then computing the optimal control by solving the Hamilton Jacobi Melbourne equations.",
            "Well, maybe it's doing that.",
            "If it does, it is kind of a weird child anyway, so the way it does learn to ride a bicycle is just by trying and then depending on the what happens.",
            "Well, you will learn so this is the type of learning that we want to model and so we want to design algorithms, study their convergence properties are the strategy going to converge to an optimal strategy in some sense, and how fast and so on.",
            "There are many applications.",
            "For example, in games we want to learn to play games, for example chess and well based on our previous games.",
            "So we want to improve our strategy if we fail in previous games or if we succeeded as well, applications in robotics so mobile robotics for example, or also a control of whatever robot arm we want to control.",
            "Many applications in operation research in well.",
            "And other fields as well, and so here I mention a few application that has been."
        ],
        [
            "Developed in the field.",
            "Notably the first one.",
            "That was really important, applications was.",
            "The program that learn to play the game of backgammon.",
            "By playing against itself in a repeated way for many several days actually of self playing, and after this these games.",
            "So we achieve a level.",
            "I was among the best backgammon player in the world.",
            "There are many other applications in games also in robotics, packet routing, job shop scheduling, production, manufacturing, optimization.",
            "Basically well, you can apply reinforcement learning in any problem that requires to make sequential decisions.",
            "And where there are some unknown in the.",
            "In the problem in your in your system, whether it's the dynamics or the reward function that you want to optimize.",
            "Whenever there is something that you need to learn and at the same time need to optimize, then you might think of applying reinforcement.",
            "Learning will also say a few words about those two applications, game of poker and the game of Go, but I will mention those one I will discuss about the setting of Milton bandits here you can find.",
            "Website that gathers many recent applications in the field of reinforcement learning.",
            "It's on the web page of JavaScript fatty."
        ],
        [
            "K here just a set of features that it is traded.",
            "Some applications that used reinforcement learning or potentially could use reinforcement learning.",
            "So maybe the best way to illustrate or this technique works is by showing you a demo by a friend of mine whose name is Martin Riedmiller.",
            "And so the video shows an example of robotic application that tries to invert a pendulum.",
            "And so here is a.",
            "His video, so Martin is no treble University in Germany so here is a setting so you have a pendulum attached on the side to a cart and discard moves in a rail an so the computers can control.",
            "The force applied to the court.",
            "And the goal is to swing the pendulum to bring it to the inverted position.",
            "So the way it works, I mean this setting is by several episodes added.",
            "Each episode the computer tries strategy and applies it on the real system.",
            "And see how it performs.",
            "And based on this feedback at the next episode, it tries to improve its strategy.",
            "Anan tried another video so that's trial #29.",
            "So at the beginning, beginning seems to do nothing really interesting.",
            "But actually exploring the state space or trying what happens if you try this action?",
            "And so he learns to.",
            "To see what are the consequences?",
            "So it succeeded, the ones inverted.",
            "But it's not very stable.",
            "After this number of iterations, then he's doing it will be better.",
            "Window.",
            "Final controller.",
            "This thing is the limit.",
            "OK.",
            "So what's interesting, yeah?",
            "The number of possibilities infinite yes.",
            "They selected previously number of sequences of actions and then the number of action is quite small.",
            "It's just right and left, but the state space is huge because the spaces of is continuous and so it's a maybe 4 dimensional state space.",
            "So you have the angle of the of the ball.",
            "This velocity and also the position of the of the carton.",
            "It's and its velocity.",
            "So it's like this faces of dimension 4.",
            "This continues and so.",
            "And so, exploring space of that dimension with this number of states is already difficult, so that's why at the beginning what you see that it seems to do like completely stupid things, but actually doesn't know what else to do, and so he's trying basically randomly actions.",
            "But based on that is learning some part of the dynamics, so not everywhere because he hasn't explore this part of the state space yet is just exploring this.",
            "But once he managed to actually start balancing the ball, then is learning to explore other parts of the state space.",
            "And so, so this is interesting example because you really see the process of learning and so there is this tradeoff in between learning and planning learning.",
            "So we will discuss more later.",
            "But learning means discovering useful information in your search space in your in your state space in terms of dynamics and reward functions and planning means based on your current information.",
            "Then well, you want to maximize.",
            "Find the best possible strategy.",
            "And so at the beginning there is more running an towards the end there is more planning, so he is using what is acquiring in terms of knowledge of his system to.",
            "I mean to do this is controller."
        ],
        [
            "So this is a general picture that we show in reinforcement learning to represent the problem.",
            "So we have an agent that interacts with its environment.",
            "So the agent perceives some state.",
            "Of the environment and then as to make some decisions, make action, choose actions, and so there is a loop here.",
            "Kind of a feedback loop, so we receive some state, then exactions.",
            "Environment returns than you state an this continuum, and once in awhile he received some feedback, so the reward or reinforcement and his goal is to find the actions in order to maximize the rewards that we get.",
            "So that's the general.",
            "Diagram that we have in reinforcement learning.",
            "So the problems of reinforcement learning is that the environment is usually unknown.",
            "In addition to that is usually well.",
            "It may be stochastic, maybe partially observable.",
            "It may be adversarial also could be some opponents that are playing against him.",
            "An the only information that he gets is a reinforcement which may even be delayed, which tells seems how well you performed in the past.",
            "So this information is very poor if you compare it to supervised learning setting where supervised learning at each state you are given what action you should do or what.",
            "What is your level or you value for this specific state.",
            "Here is not told what action it should choose in any state.",
            "We just let him do whatever he wants and then we just evaluate the quality of what he has done.",
            "Based on this, feedback is to improve strategy, so learning is more difficult than in supervised learning.",
            "And the goal is exceeds maximize the expected some future rewards.",
            "So here we see that we not only want to maximize the immediate reward, we just don't want to just find the action that maximizes immediate reward.",
            "But we want to maximize.",
            "Possible rewards in the future as well.",
            "So for example, in chess you may want to sacrifice some material you know to win the game in the end.",
            "So that's the goal, and so one of the problem is really how to sacrifice short-term small reward in order to privilege larger rewards in the long term.",
            "And the way reinforcement learning usually addressed this problem.",
            "Is by learning function.",
            "That represents the best.",
            "The best quality of rewards you can get from any possible states.",
            "So this is cool."
        ],
        [
            "Old value function.",
            "For optimal value function, which is defined as.",
            "The value.",
            "An action agent can obtain from any state if he plays optimally.",
            "So for example, if his goal is to maximize the sum of rewards, then the optimal value function at a given state X is going to be defined as the supremum over all possible strategies of the expected sum of rewards you will get starting from this state and following this this policy.",
            "So let me illustrate this in a non formal way on this example causing a consider this environment where at time T you are in that state XD.",
            "And you have two possible actions.",
            "If you choose this action, then you will move to one of those states with some transition probabilities.",
            "Then if you think about what it means.",
            "To be optimal from that state, it means being optimal immediately at the current state and then on each of the resulting possible next state, continuing to be optimal.",
            "So we see that we can have a link between the optimal value function at the current state and the optimal value function at the next state.",
            "When that I reach when we follow the best immediate action.",
            "And this is, this leads to a property of the optimal value function, which is called the Bellman equation that we will see more in details later but just want to mention this here.",
            "The bellman equation here says exactly whether I've just said which is the optimal value function at this state, is a Max over all possible actions of the immediate reward obtained in that state.",
            "If I choose this action plus the expected optimal value function at the next state?",
            "If I choose this action A.",
            "And then it takes the maximum over all possible actions.",
            "So what it means is that if I am in the state XD, an act optimally, no.",
            "Well, my optimal value function is going to be equal to the.",
            "The immediate optimal action plus the expected optimal value of the next X.",
            "And so in some way, if I learn this optimal.",
            "Value function.",
            "Then well, then I can act optimally, because if I know the optimal value function, those next states well, I will just choose the action that maximizes the.",
            "You know the immediate expected value of the optimal value function at the next state.",
            "So in some ways it's a way to solve the problem I mentioned just before, because if I'm able to learn this optimal value function on the whole state space, then by acting really with respect to this value function, then I'm actually choosing my action in an optimal way in order to maximize the sum of expected rewards.",
            "So let me also mention the term to temporal difference, which is which means which is a difference in evaluations of two successive states.",
            "Up to the reward function, so temporal difference when I'm in XD and I choose action 80 and move to next X + 1 is a different V star of X2 plus one plus the immediate reward minus V star of XT an.",
            "Based on what I've just said before.",
            "We have the property that if we in XT we choose the optimal action.",
            "Then this Delta T in expectation is going to be equal to 0, while because we have that V star of XT is equal to the expectation of the reward plus we start the next state so.",
            "Expectation over all possible next states.",
            "If I over results probabilities here.",
            "So I can rewrite this so if I choose the optimal action.",
            "The action that reaches the maximum.",
            "Here we see that the star of XT is equal to the immediate reward for this action plus this expectation of the store for the next date.",
            "And so this means that if I choose the optimal action here in expectation, my temporal difference is equal to 0.",
            "And that's actually the the temporal difference is going to be.",
            "The signal that is going to allow me to learn the optimal value function cause.",
            "If I have a current evaluation of state such that when I'm in this state and I had to take an action and I moved to another state where somehow there is a mismatch between these two consecutive evaluations.",
            "For example, the next state is overestimated compared to the previous one.",
            "Then it means that there is some incoherence in my evaluations of the States and so I can update.",
            "My evaluation in order to make it more current, more current means that in expectation this should be equal to zero and one this is actually true.",
            "So when I have learned function so that whenever I make the best possible actions I have the property that expectation, those Delta T are equal to zero.",
            "That means that I have just learned the optimal value function.",
            "So this is also called surprise signal.",
            "Whenever there is a surprise inside sense that there is a new evaluation of the next state is not current with my previous expectation.",
            "My previous evaluation, then it means that I'm I can learn from this surprise to modify my value function in order to make it more.",
            "Anne.",
            "2 compatible you know, between my current state and my next dates?",
            "So whenever there is unlike this signal of surprise and then I can update my optimal value function and once I reach that state where in expectation there is no more signal of normal temporal difference, then it means that I've reached a good approximation of my optimal value function.",
            "That's the intuition for whole.",
            "We can learn this optimal value function and the fact that based on this optimal value function, then I will be able to make optimal decisions.",
            "I will make all this more formal in later.",
            "Plants."
        ],
        [
            "But so far so I wanted to say that so the challenges of reinforcement learning as I said, is.",
            "So how problem?",
            "Because information that we are is very poor.",
            "In order to learn something which is quite.",
            "Quite difficult because we want to learn a policy.",
            "Especially in situations where the environment can be stochastic, adversarial, or partially observable.",
            "One of the main points that I've already mentioned is the fact that since usually we don't know the state dynamics and the reward functions, then we need to learn them and at the same time as we need to plan so to find the best possible strategy in order to maximize our criterion.",
            "For example, some of the words and so this combination between learning and planning, which appears actually in any optimization problem where there are some.",
            "Unknown, I mean that you need to discover is also called the exploration exploitation tradeoff, and I will illustrate this exploration exploitation tradeoff more precisely when we reach the the part on multi arm bandits.",
            "And I've also mentioned the other problem.",
            "Which is the fact that we need to represent our functions in an approximate way, because usually the state space is so large that we cannot actually assign a value to each possible state of the state space.",
            "So we have illustrated in the case of, I mean, the video that we've seen.",
            "Actually, Martin Reimiller is using a neural network to approximate it's value function.",
            "And that's the type of function approximator that he used in his demo.",
            "So now I would like to.",
            "To start, by describing the tools that we have for the planning.",
            "So we're going to assume that everything else is known.",
            "The dynamics are known.",
            "The reward functions are known, everything is known and we just want to find the best possible plan.",
            "Best possible strategy."
        ],
        [
            "So this is a field called dynamic programming and I'm just going to describe a few tools that are developed in this field that are related to reinforcement learning.",
            "So usually in dynamic programming we assume that our process satisfies a Markov property and so we assume that we have a so called Markov decision process.",
            "Markov decision process is basically just a Markov chain with actions, so we have a state space.",
            "We have an action space.",
            "And we assume that the dynamics are Markovian, so this satisfies the properties that basically the transition to a new state XD plus one given all the past states an all past actions is just a function of.",
            "The current state and actions.",
            "So basically all the past doesn't give us additional information to predict the future than the current state in action.",
            "And so we can define transition probabilities.",
            "P of Y given XA, which represent the property to jump from state X to Y.",
            "Given that I choose action A.",
            "And we also have a reinforcement function or reward function.",
            "So for example, it can be a function of the state and action, so that will be the immediate reward that is obtained when my agent in State X and choose action.",
            "So the.",
            "Learning to write.",
            "Yes, so it depends on what you put in state space.",
            "But if you put in state space, all informations that are useful to predict the next state then it is so.",
            "For example, for the bicycle you will need to include in the state space the current position and also the velocities you know and so.",
            "So if you can reach a state space so that it contains enough information to predict the next state, then your dynamics are more caution.",
            "Yes.",
            "So it could be stochastic.",
            "Yeah, it could be stochastic.",
            "Also, it could be a stochastic function of the state action and the next day also.",
            "So here I just write it as a deterministic function, but it could also be stochastic.",
            "Equation is valid for.",
            "Young man no.",
            "The Bellman equation is good for Markovian only market initially.",
            "But if you assume that the reward.",
            "So yes, if you assume the rules depend on the future, then the bellman?",
            "So then you are not in then the Bellman equation will not hold.",
            "Yes.",
            "So, so that's the definition of an MDP Markov decision process, and so the goal of an MDP is too.",
            "To compute the best possible policy.",
            "So what is the policy?"
        ],
        [
            "A policy pie is a function that tells you at anytime step.",
            "What action you choose?",
            "For any possible state.",
            "So it's mapping from state to actions.",
            "At anytime step.",
            "And so if you fix a policy.",
            "Then basically you are Markov decision process.",
            "Is equivalent to a Markov chain where the transitions from state XT to the next state.",
            "Is defined as a polity to jump from exit to exit plus one, given that you choose the action corresponding to your policy by Tia fixed.",
            "Sydney.",
            "Sorry.",
            "That was my alarm Clock.",
            "And also we will see that in large class of problems actually the optimal policy is not.",
            "Is independent on the time.",
            "And so we call a policy that is independent of time, stationary policy or Markovian policy.",
            "So if a policy is just a function of state two actions, then we call this policy stationary.",
            "Action.",
            "When the policy is independent of time, then we call it stationary.",
            "Map selection impossible state.",
            "Oh yeah, map any possible state to an action.",
            "Yes, mapping from state to actions.",
            "And so our goal is to computer policy an the policy.",
            "Which is the best possible one in terms of which criterion?",
            "Well, that's why."
        ],
        [
            "We're going to define next the performance of a policy, and so this actually depends on the problem that you want to solve.",
            "So you can either consider.",
            "Finite or infinite, or reasons.",
            "So, for example, if you want to find a policy that maximizes, say, the expected sum of rewards.",
            "In a finite or is and then you can define the value of a policy.",
            "So V Pi the expectation of the sum of rewards given that you start at time T. From State X and you follow policy by.",
            "But you can also define.",
            "The value function V Pi for infinite reasons.",
            "So either in the discounted discounted setting.",
            "Or undiscounted or average setting.",
            "So in the rest of this lecture, I will focus on this first definition the discounted setting.",
            "So here basically you're going to define your value of a policy as the expectation of the sum of discounted rewards.",
            "Given that you start from the State X and you follow policy by and so gamet authority is your discount factor where gamma is number strictly less than one.",
            "So basically this says that you.",
            "Put more priority to immediate rewards, then later re what future rewards so the future rewards are discounted by gamma to the party.",
            "So I will say it more about that later, but I want to say that there are other criterion.",
            "For example the undiscounted setting where it's just the sum of rewards and so here in order to prevent us this series diverge, we have to make some assumptions, for example that there are some terminal states.",
            "Or absorbing states such that once you reach this state, then the process stops and there and no more rewards is obtained and then you're going to assume that you reach is type of states with property one for example, which will guarantee you that this sum is finite.",
            "Another criterion is the average one, so you take the limit, 20 goes to Infinity of the average of tee time step rewards where you take the.",
            "Expectation of this over all possible runs.",
            "So what is random here is XD.",
            "When I mentioned expectation of this quantity, so it's a reward because I assume that the reward is is deterministic function.",
            "But what is random here is is a state, because if I fix my policy Pi then I am in a Markov chain and so there are possible different runs and so my Steve is a random respect to this.",
            "The transition probabilities of my Markov chain.",
            "Oak."
        ],
        [
            "So I want to illustrate those definitions in the net at this summer school student problem.",
            "So it's an MDP where the states are represented by this ovals OK. And so the actions are in one.",
            "The sick arose.",
            "So if you are in this state, you can either think or sleep.",
            "If you think, then with property 1/2 represented by this light rose, you move to this next state.",
            "Otherwise you stay where you are.",
            "No, I've represented by.",
            "OK, bye in read the rewards you get.",
            "In each of those states, if you are there, you get the minus one reward.",
            "It's immediately one here, just assuming that there was just a function of the state.",
            "And so those states are terminal states.",
            "So when you reach those, you just stay there, you receive immediate reward and then you don't receive any more rewards and so your goal is to find the best.",
            "Both involve policy which means at each state which action we should choose in order to maximize the expectation of the sum of rewards until you reach terminal State.",
            "So let's try.",
            "If you are there, so your reward is this much minus 1000.",
            "Since its terminal state means that the value function there is also minus one side, so your value is this match.",
            "Sure, there you're pretty happy your value function is 100 if you're there, you're value function is minus 10.",
            "So now what happens if you are there?",
            "What should you do?",
            "Sleep yes, because if you have, she worked to merchants into merchant, you're going to be sick, so you need to think a little bit so that you understand this course, but after awhile you need to take a rest and enjoy just all your knowledge that you required and so you need to sleep at some points again.",
            "And so if you sleep then with probability 1/2 you want 90%.",
            "You're going to move to this state and have a good value an.",
            "But with probably 10% you're going to stay in that state.",
            "But that's always better than just thinking Moran getting to this bad state.",
            "No happens if you are.",
            "If you are there, for example, well, let's see what happens if you are there.",
            "You think you should think in order to reach this state or you think you should sleep in order to reach that or come back here?",
            "Think yes, it makes sense to think because as we say, the value function of this state is pretty high because from that state you know that you can reach this state, which is good.",
            "So the value there is good.",
            "Whereas the value there.",
            "So if you take a look at this one, if he goes there then the value is pretty bad.",
            "So probably the value of there is a good path here.",
            "So you might want to try to reach this state because this state is good.",
            "So by thinking then with half of the times you will reach this thing.",
            "And come back to your initial state.",
            "I love the other half of the times.",
            "So no, if you are there, what should you do?",
            "Yeah, you should think in order to reach this state, because probably this one is is also a good one.",
            "OK, rather than sleeping and having a bad bad reward because you slept so much that you didn't even listen to the speaker, so that's bad.",
            "So what happens if you are there?",
            "Should you sleep or should you?",
            "Think.",
            "So who is for thinking?",
            "So who is for sleeping?",
            "Not so many.",
            "Oh, you said sync already know you sleep already sleep.",
            "So you think the most actions are the same value?",
            "Well, actually you see that at this stage is not so obvious again because there are two States and we don't really know.",
            "So as we have seen the value here, what is the value function there?",
            "If we write the Bellman equation there, well many questions tells me that the value of this state is the immediate reward plus the maximum over 2 actions of the expectation of the optimal value at the resulting state.",
            "So it's a maximum resource to actions of 05 times the value there.",
            "05 times the value there, so the value there is already a function of this state, but also of this state and all this state because it takes a maximum over and the value there is a maximum of where these two actions of.",
            "So.",
            "Basically it's really kind of intricate problem because the value at this stage is a function of the value of any other all the other state and it's kind of a fixed point equation.",
            "And actually we will see that it's a value at all.",
            "The state is a fixed point equation of the bellman operator.",
            "We will define it in the next slide.",
            "So if we make the computations, we actually find that from here it's better to sleep.",
            "Why?",
            "Because you get this additional bonus here, plus one reward, and then once you're there, you try to reach state and then and get to this.",
            "Everyone's."
        ],
        [
            "And so if you write down, so that's the optimal solution.",
            "I represented the the optimal actions for each possible States and also the value function for each possible state.",
            "So we value function is minus 1000.",
            "Here's a value function is 88.9 because we can reach this.",
            "The state.",
            "And so on.",
            "And here we see that the value of two is slightly higher than the value of three, and because of that it makes sense to try to reach this state rather than directly reaching this one.",
            "And so we see that this type of equation that we have in order to solve this is this type of equations.",
            "That's exactly the Bellman equation as I mentioned before, so maximum of two quantities representing each one.",
            "What happens if you follow an action of the expected value of the next date?",
            "Do you have any questions?",
            "OK, so let's move on to more."
        ],
        [
            "Precise definitions, and so we're going to consider one specific setting, which is nice because it's simple, so setting I want to consider is infinite tourism, discounted rewards, so I'm just going to define my value function for state X and foreign policy by at least 8X as the expectation.",
            "The sum of discounted.",
            "Rewards, given that I start from X an I follow policy by.",
            "So I'm going to use this discount factor, which makes everything nice because because of that I'm guaranteed that all those quantities are finite.",
            "Then also makes sense from an economical point of view because this discount factor tells me.",
            "That, well, I have preference for immediate rewards rather than future ones.",
            "Right?",
            "If I tell you, I can give you $100 bill now or in one year, which one would you prefer?",
            "No, yes, because anyone you're not sure to see me again, but if I tell you I can give you with very strong guarantees you have the choice either I give you $200 now or 100 in next year.",
            "Which one do you prefer?",
            "Oh yeah, sorry.",
            "Yeah, I agree with you know if I wanna know are 200 next year.",
            "Then 100 no.",
            "Yeah, you really don't trust me.",
            "Anyway, I don't have any money, sorry, but OK if I guarantee that I can give you 500 next year, you may want to wait for next year because you know you will see me again and so and so if you multiply gamma by 500 and set it equal to 100 then you will see what is the value of gamma for this specific for your specific value.",
            "So Gamma Times 500 should be equal to 100 and so in that case this defines.",
            "The value of defined your preference for how much preference you assign to immediate rewards rather than future ones.",
            "OK, so let's assume that we want to define the performance or the value function of a policy Pi in this way, and so our goal is to find the policy Pi that maximizes for any possible state.",
            "Anne."
        ],
        [
            "So we're going to write down formally the Bellman equations that I've mentioned earlier for any policy Pi V. Pi satisfies this equation.",
            "So V Pi of X is just the immediate reward as said, X, given that I choose the action by of X plus the discounted value of the expectation of Veep, I add the next state.",
            "Given that I'm in X and I choose action via fax.",
            "OK, so that's just the expectation of the pie at the next state.",
            "The same overall possible next states.",
            "Why of the probability to go from X to Y if I choose action Pi of X times the value of Y?",
            "So V Pi is of X is immediate reward plus discounted expected value of the next state.",
            "So that's a property to remember that the pie is defined like that.",
            "Expectation of discount rewards in the future, and we have the properties at the Pi of X satisfies this equation called abandoning questions.",
            "In other words, the pie is actually the fixed point of the bellman operator.",
            "We write it Tip line which is defined as follows.",
            "The tip I applied to any function W returns a function and this function evaluated at a state X is.",
            "The immediate reward at X.",
            "Plus gamma times the expected value of W at the next state.",
            "So if I define T \u03c0.",
            "As follows with Tippi is an operator that takes a function that returns a function with tip I applied to W outer state, X is reward plus gamma times the expected value of W as an extractor.",
            "Then the Bellman equation just writes rewrites.",
            "V \u03c0 equals tip Ivy \u03c0.",
            "Because this is just.",
            "The bellman operator tip I applied to V \u03c0.",
            "And so here we see that the pie is a fixed point of the pie.",
            "So by defining this Bellman operator OTP?",
            "I have the properties at the pie is a fixed point of the pie so far.",
            "So for any policy \u03c0 V, \u03c0 is a value of my policy, so function, so the pie is the solution to a fixed point equation.",
            "V \u03c0 is a fixed point of my operator, Tippi, yes.",
            "And then immediately want.",
            "OK, so that's a good question.",
            "Actually, I'm not quite sure to know the details of the implementation, but you can think of immediate rewards.",
            "For example, for this inverted pendulum as the height of the.",
            "You know the extremes of the ball, so if you want to maximize it, it's maximized when you are in this upright position.",
            "So this could be like an immediate reward.",
            "OK, so it could be that you have like at each time step you have a reward immediate reward.",
            "It depends on the state, or it could be that you have only eternal reward that tells you only when you reach the call then you reward is this much.",
            "But in the setting that Martin did I think he has some immediate reward at each time step.",
            "So this gives some indication on which direction we should search, but of course for example there might be a lot maybe.",
            "Many local maximum, because maybe your pool is like that.",
            "An trying to pull it up doesn't work because you don't have enough torque.",
            "OK, so somehow you need to go this way in order to invert the pendulum.",
            "So you need to go through a local maximum in order to reach a global one.",
            "I can rewrite my Bellman operator using.",
            "Matrix notations or operator notations.",
            "So basically typy of W, so let's assume that the state space is finite.",
            "So if the state is finite then those functions are just vectors.",
            "OK, there is a finite number of values, so then you can see tip I as an operator that takes a vector an is defined as our \u03c0 plus gamma, \u03c0 W, where ARPI is a vector of rewards that assigns a reward to each possible state.",
            "When you follow policy \u03c0 and \u03c0 is a matrix.",
            "It's basically the transition matrix from state to next state.",
            "If you follow policy Pi.",
            "So in that case you can see that.",
            "Tippi is actually an affine operator that takes W and returns this constant plus gamma \u03c0 W. So the proof."
        ],
        [
            "This result is very simple, so in general I'm not going to go through all the proofs because it will take too much time, specially because the proof of this is not quite exciting.",
            "Basically, you just say that you start from the definition.",
            "An you isolate the first re one and say so.",
            "The value function at this stage is just the reward.",
            "The reward you get at the initial state plus the expectation of the rewards you get later and you rewrite this basically as this contractor times the expectation of the value function at the next date.",
            "And you obtain the Bellman equation.",
            "OK, so now let's move to the bed Monica."
        ],
        [
            "For Vista, So what is the star?",
            "It's defined as Superman for all possible policies of VPA and it's called the optimal value function.",
            "That's the best thing you can get studying from a state, an acting optimally.",
            "So the star of X is defined as a supermom overall policies of V Pi of X.",
            "And we also have a Bellman equation.",
            "So it's called Bellman equation or dynamic programming equation.",
            "On the star an it is written this way we start X is a maximum overall possible actions of the immediate action corresponding to a plus gamma times a discounted optimal value function at the next day.",
            "So basically the difference with what we've seen before is that there is a maximum of actions here.",
            "So what it tells you is that.",
            "The optimal value you can get from a state X is obtained by.",
            "Basically taking the best possible immediate action.",
            "And later being optimal at the resulting state.",
            "The value optimal value function is the Max or possible actions of the immediate reward plus gamma times expected value of the next optimal value of the next states.",
            "And in that case also we can define a dynamic programming operator going to write it T. Which is defined in this way.",
            "So T takes as input a function and returns function TW whose value of X is defined in that way.",
            "Max Overaction reward air of XA plus discounted average value of W at the next date.",
            "And so you see that this is just T applied to vystar, and so the bellman is a dynamic programming question, is just equivalent to saying that the star is a fixed point of."
        ],
        [
            "The proof of this result is also easy, so just not going to go through it.",
            "You can read the proof in the slides if you if you want it."
        ],
        [
            "So here are some properties of the bellman operators, so tipai, and also the dynamic programming operator T. So both operators are monotonic.",
            "And they are contract contraction in maximum though montinique means that if two functions satisfy this then tip IW is going to be basically tip.",
            "I preserve the order.",
            "In the function, W is less than double 225 W, one is less than TW.",
            "And so we also have the contraction property that says that tip I applied to W 1 -- T Pi applied to W2 is bounded by gamma times the distance between W1 and W2 in L Infinity norm.",
            "So this is very easy properties.",
            "You can see that for example, maybe the simplest way to see."
        ],
        [
            "This is when you look at this matrix notations.",
            "So basically Typi is monotonic because here we have a pie which is stochastic matrix.",
            "And so this preserves the order.",
            "So if W one is less than W2 in, then PW one is less than PP W2.",
            "And also the norm of TPW 1 -- T, P W2 basically is bounded by gamma times P \u03c0 W 1 -- W two and since people is stochastic matrix again then it's L Infinity norm is less than one.",
            "So we have that.",
            "And the norm of the PW 1 -- T P W2 is bounded by W 1 -- W two.",
            "Times gamma sorry.",
            "So there is this gamma factor and so."
        ],
        [
            "Because of this nice gamma, this contraction."
        ],
        [
            "This nice discount factor we have this construction property.",
            "And so that's interesting property, because, as we have said, the value function is a fixed point of some operator, and this operator happens to be a contraction in some norm, and so we have the additional properties at the fixed point is unique.",
            "And we will deduce algorithm in order to compute the fixed point.",
            "Basically, since we have a contraction by iterating this operator we will build a sequence of functions.",
            "So here is the result so."
        ],
        [
            "So the pie is the unique fixed point of Tippi, so we have already seen that it's it's a fixed point.",
            "The fact that it's unique, immediate consequence of the fact that Pie is a contraction.",
            "Vista is a unique fixed point of T. Again, this is a consequence of the fact that T is a contraction.",
            "We are the properties that using matrix notations V \u03c0 is.",
            "The inverse of the matrix identity minus gamma \u03c0.",
            "Times are by the reward vector, and we have also the very important property.",
            "That if we define the policy.",
            "Like this?",
            "Based on Vista, then this policy is optimal.",
            "So what it means here is that if I define the policy, call it by star of X, so at each state X I'm going to choose one action that reaches a maximum of this quantity, so.",
            "The action to the argument of this Max is an action that maximizes this quantity, which is immediate reward plus gamma.",
            "Discounted value of the optimal value function.",
            "At the next eight.",
            "So what is important about this property is that if you are able in some way to compute this star, then immediately.",
            "You have an optimal policy.",
            "That's a way to deduce an optimal policy based on the optimal value function.",
            "If you can assign the optimal value to any possible state, and basically the best thing you can do at any state is just being greedy.",
            "With respect.",
            "To this optimal value function, which means taking the action that maximizes the immediate reward plus the discounted expected value optimal value at the next states.",
            "OK, so let me prove those results.",
            "So we have already given arguments for points one and two 4.3.",
            "This is just."
        ],
        [
            "Is a consequence of the definition of Tippi, so 4.3 we have the pie is a fixed point of teepi by rewriting teepi using matrix notations we have.",
            "Tip I reply is defined as the reward.",
            "Our \u03c0 plus gamma Pi V Pi we can put the buy in design.",
            "So by factorizing the pie we have identity minus gamma Pi.",
            "Day \u03c0 equals or pie.",
            "And then we just need to prove that this matrix is invertible.",
            "And this matrix is invertible because we know that the eigenvalues of a stochastic matrix are bounded.",
            "In model is by 1.",
            "And so the eigenvalues of identity minus gamma Pi are equal to 1 minus gamma times the eigenvalues of the pie.",
            "And so because although again volumes there are bunch of models, lesson 1 gamma is less than one.",
            "It proves that basically the eigenvalues of of these metrics are bounded away from zero by 1 minus gamma.",
            "And so the metrics is invertible, and so we can rewrite V Pi as identity management by minus one or five.",
            "Which is what we claim."
        ],
        [
            "Here and then finally.",
            "We want to prove that the policy defines in this way is optimal.",
            "So what it means here it means that basically.",
            "T by star is equal to.",
            "T Vista.",
            "Because the bellman operator T is exactly applied to Vista is exactly this quantity.",
            "So T we star is exactly this.",
            "And so when you take the argument of this Max, it means that the bellman operator applied to P star.",
            "OK.",
            "The bellman operator T Pi star applied to Vista is equal to T Vista."
        ],
        [
            "So by defining Pi stars away I just defined before, we did use that T by star V star equals TV star.",
            "OK, the bellman operator applied to buy Star Benefit or four Pi star applied to Vista is equal to the dynamic programming operator, the one with the Max.",
            "I'm trying to restart OK, but we already know that the star is a fixed point of T, so TV star is equal to Vista.",
            "And So what we have is that T Pi star Vista is equal to Vista.",
            "But we already know that this.",
            "Operator as a unique solution, which is V \u03c0 star.",
            "And so the price R = V star.",
            "So V Pistori called Vista.",
            "What does it means that the value of the policy Pi star that we have defined earlier is equal to the optimal value function, which means that this policy is optimum by definition.",
            "So we have proven that the policy which is defined like that.",
            "Is optimal and."
        ],
        [
            "By the way, it's also stationary because here the policy doesn't depend on time, is just the function of the current state.",
            "So we have the property that in this setting an optimal policy can be obtained in the space of stationary policies.",
            "So we have obtained the fact that if we are able to compute the optimal value function V star, then we can derive the optimal policy Pi star.",
            "This policy is stationary and optimal."
        ],
        [
            "OK."
        ],
        [
            "And so now we can start to describe algorithms in order to compute.",
            "Well, we saw or going to compute the star because know that we know that from the start we can deduce the optimal policy or goal is to compute the star, and there are two type of algorithms.",
            "One is called value iteration.",
            "The other one is called policy iteration, so value iteration will perform iterations over the values policy iteration iteration over policies.",
            "So value function very simple.",
            "You start from any."
        ],
        [
            "Function V0 and for any policy Pi, if you define the sequence of functions VK define recursively like that.",
            "So where you iterate.",
            "The bellman Aparati Pi.",
            "Then the sequence VK tends to be by the value for the policy \u03c0. I know you can do the same by iterating the dynamic programming operator, so if you define the sequence VK such that VK plus one is called to TK, then your sequence VK converge to the star.",
            "And the proof of that is very simple, so it's just one line.",
            "If you look at the distance between VK plus 1 -- V star.",
            "So by definition, VK plus one is T VK.",
            "This star is the fixed point of T, so this is.",
            "TV star here.",
            "We use a contraction property of T to say that these distances less than gamma times the distance between Vicki and V star, and so we see that we have a contraction.",
            "So we K plus one movie star is less than gamma.",
            "We cannot Vista we can repeat this step K times to deduce that this is bounded by gamma to the power K 0 + K + 1.",
            "Times the norm of V 0 minus Vista, which is whatever it is, but it's fixed and so when K goes to Infinity this goes to 0.",
            "So we have an algorithm.",
            "That says OK. Start from any function V0 and then I iterate my Bellman operator.",
            "At each iteration and the sequence of function that I produce converges to the optimal value function."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Good morning everybody, thanks for the invitation to give a course on reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "To this summer school.",
                    "label": 0
                },
                {
                    "sent": "So so yes, so I am researcher at INRIA Lille in France on an I'm currently on leave Nunca at Microsoft Research in New England lab.",
                    "label": 1
                },
                {
                    "sent": "And so so I typed all this, this, this course reinforcement learning, introduction to reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "And I added on multi arm bandits.",
                    "label": 0
                },
                {
                    "sent": "So I would like also to give a brief introduction to the field of multi arm bandits.",
                    "label": 0
                },
                {
                    "sent": "Because I think it might be interesting and also it's very related to reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to give us 3 three lectures.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the course is going to follow this this outline, so there will be three parts.",
                    "label": 0
                },
                {
                    "sent": "About one is going to be a general introduction to reinforcement learning and dynamic programming will introduce the field of dynamic programming and.",
                    "label": 1
                },
                {
                    "sent": "And so introduce several algorithm value iteration policy iteration and also will introduce the algorithm called Q Learning first algorithm in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "And then so this part basically consists of the problem of.",
                    "label": 0
                },
                {
                    "sent": "So small problems where it says space is finite and where you can represent your function in with a very good accuracy, but in actual real problems the space is so large that you cannot represent your interesting functions perfectly everywhere.",
                    "label": 0
                },
                {
                    "sent": "So you have to use approximations and so in the second part we will see.",
                    "label": 0
                },
                {
                    "sent": "What are the impact of those approximations on the impact the impact on those of those approximations on the performance of the algorithms?",
                    "label": 0
                },
                {
                    "sent": "So we're going to look at.",
                    "label": 0
                },
                {
                    "sent": "So mentalities of dynamic programming and reinforcement learning when we consider approximations.",
                    "label": 0
                },
                {
                    "sent": "And at the end we will do some connections with the field of statistical learning and then the third part will be an introduction to the field of multi arm bandits and so in different settings either in stochastic bandits or adversarial bandits.",
                    "label": 0
                },
                {
                    "sent": "And then I will also mention some extensions such as computation of our approximation of Librium in games and also the field of Monte Carlo Tree Search.",
                    "label": 0
                },
                {
                    "sent": "So by the way, this is intended to be a interactive course, so please don't hesitate to interrupt me or ask me questions.",
                    "label": 0
                },
                {
                    "sent": "Whenever you want, because I guess the goal of this summer school is to, I mean to have interactions and so.",
                    "label": 0
                },
                {
                    "sent": "So please ask me any question you wish.",
                    "label": 0
                },
                {
                    "sent": "So I'd like to go as fast as possible during the first 2 lectures in order to arrive there and then spend 1 1/2 lecture.",
                    "label": 0
                },
                {
                    "sent": "For for actually the last part part three, I would like to spend some time there, so I'm going to go really fast, but so if you don't understand something, just let me know and I will try to adjust to the right speed.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So introduction to reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Just mention a few references.",
                    "label": 0
                },
                {
                    "sent": "Serve old books, so book by Bessica synthetically Neurodynamic programming the book by Sutton and Barto on introduction to Reinforcement learning on Markov decision problems.",
                    "label": 1
                },
                {
                    "sent": "There is a book by Martin Puterman and more recently there is a book by a championship is very on algorithm for reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So if you want to read more about the field.",
                    "label": 0
                },
                {
                    "sent": "I will.",
                    "label": 0
                },
                {
                    "sent": "I will recommend you to start with this one which is really introduction.",
                    "label": 0
                },
                {
                    "sent": "This one is more more material into it, specially into function approximation and in combination with dynamic programming and this one is really kind of an update of reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So it's specially interesting now.",
                    "label": 0
                },
                {
                    "sent": "So what is?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Interest money.",
                    "label": 0
                },
                {
                    "sent": "So the goal of interesting reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Is to learn to make good decisions in unknown environments.",
                    "label": 1
                },
                {
                    "sent": "So we want to learn.",
                    "label": 0
                },
                {
                    "sent": "To make good decisions.",
                    "label": 0
                },
                {
                    "sent": "So the goal is to make good decisions.",
                    "label": 0
                },
                {
                    "sent": "Good decisions means.",
                    "label": 0
                },
                {
                    "sent": "Make it take actions or follow some kind of policies or our goal is to follow a policy or strategy which tells us what we should do in each possible situation that we encounter and the way we're going to do that is by learning.",
                    "label": 0
                },
                {
                    "sent": "So we're going to learn to act and the type of learning we will consider is learning from experience.",
                    "label": 0
                },
                {
                    "sent": "So and more precisely, is kind of.",
                    "label": 1
                },
                {
                    "sent": "Learning from trials and errors.",
                    "label": 0
                },
                {
                    "sent": "So we're going to try something and then based on our things go, we will get some feedback on our previous actions.",
                    "label": 0
                },
                {
                    "sent": "This feedback is called reinforcement or rewards and based on this information, we're going to improve our strategy in order to do better in the later rounds.",
                    "label": 0
                },
                {
                    "sent": "So an example that illustrates this type of learning is, for example, a child that learns to ride a bicycle.",
                    "label": 1
                },
                {
                    "sent": "So the way the child learns to ride a bicycle.",
                    "label": 0
                },
                {
                    "sent": "Is not by writing down the stochastic differential equations of the system and then computing the optimal control by solving the Hamilton Jacobi Melbourne equations.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe it's doing that.",
                    "label": 0
                },
                {
                    "sent": "If it does, it is kind of a weird child anyway, so the way it does learn to ride a bicycle is just by trying and then depending on the what happens.",
                    "label": 0
                },
                {
                    "sent": "Well, you will learn so this is the type of learning that we want to model and so we want to design algorithms, study their convergence properties are the strategy going to converge to an optimal strategy in some sense, and how fast and so on.",
                    "label": 0
                },
                {
                    "sent": "There are many applications.",
                    "label": 0
                },
                {
                    "sent": "For example, in games we want to learn to play games, for example chess and well based on our previous games.",
                    "label": 0
                },
                {
                    "sent": "So we want to improve our strategy if we fail in previous games or if we succeeded as well, applications in robotics so mobile robotics for example, or also a control of whatever robot arm we want to control.",
                    "label": 0
                },
                {
                    "sent": "Many applications in operation research in well.",
                    "label": 0
                },
                {
                    "sent": "And other fields as well, and so here I mention a few application that has been.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Developed in the field.",
                    "label": 0
                },
                {
                    "sent": "Notably the first one.",
                    "label": 0
                },
                {
                    "sent": "That was really important, applications was.",
                    "label": 0
                },
                {
                    "sent": "The program that learn to play the game of backgammon.",
                    "label": 0
                },
                {
                    "sent": "By playing against itself in a repeated way for many several days actually of self playing, and after this these games.",
                    "label": 0
                },
                {
                    "sent": "So we achieve a level.",
                    "label": 0
                },
                {
                    "sent": "I was among the best backgammon player in the world.",
                    "label": 0
                },
                {
                    "sent": "There are many other applications in games also in robotics, packet routing, job shop scheduling, production, manufacturing, optimization.",
                    "label": 1
                },
                {
                    "sent": "Basically well, you can apply reinforcement learning in any problem that requires to make sequential decisions.",
                    "label": 0
                },
                {
                    "sent": "And where there are some unknown in the.",
                    "label": 0
                },
                {
                    "sent": "In the problem in your in your system, whether it's the dynamics or the reward function that you want to optimize.",
                    "label": 0
                },
                {
                    "sent": "Whenever there is something that you need to learn and at the same time need to optimize, then you might think of applying reinforcement.",
                    "label": 1
                },
                {
                    "sent": "Learning will also say a few words about those two applications, game of poker and the game of Go, but I will mention those one I will discuss about the setting of Milton bandits here you can find.",
                    "label": 1
                },
                {
                    "sent": "Website that gathers many recent applications in the field of reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "It's on the web page of JavaScript fatty.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "K here just a set of features that it is traded.",
                    "label": 0
                },
                {
                    "sent": "Some applications that used reinforcement learning or potentially could use reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "So maybe the best way to illustrate or this technique works is by showing you a demo by a friend of mine whose name is Martin Riedmiller.",
                    "label": 0
                },
                {
                    "sent": "And so the video shows an example of robotic application that tries to invert a pendulum.",
                    "label": 0
                },
                {
                    "sent": "And so here is a.",
                    "label": 0
                },
                {
                    "sent": "His video, so Martin is no treble University in Germany so here is a setting so you have a pendulum attached on the side to a cart and discard moves in a rail an so the computers can control.",
                    "label": 0
                },
                {
                    "sent": "The force applied to the court.",
                    "label": 0
                },
                {
                    "sent": "And the goal is to swing the pendulum to bring it to the inverted position.",
                    "label": 0
                },
                {
                    "sent": "So the way it works, I mean this setting is by several episodes added.",
                    "label": 0
                },
                {
                    "sent": "Each episode the computer tries strategy and applies it on the real system.",
                    "label": 0
                },
                {
                    "sent": "And see how it performs.",
                    "label": 0
                },
                {
                    "sent": "And based on this feedback at the next episode, it tries to improve its strategy.",
                    "label": 0
                },
                {
                    "sent": "Anan tried another video so that's trial #29.",
                    "label": 0
                },
                {
                    "sent": "So at the beginning, beginning seems to do nothing really interesting.",
                    "label": 0
                },
                {
                    "sent": "But actually exploring the state space or trying what happens if you try this action?",
                    "label": 0
                },
                {
                    "sent": "And so he learns to.",
                    "label": 0
                },
                {
                    "sent": "To see what are the consequences?",
                    "label": 0
                },
                {
                    "sent": "So it succeeded, the ones inverted.",
                    "label": 0
                },
                {
                    "sent": "But it's not very stable.",
                    "label": 0
                },
                {
                    "sent": "After this number of iterations, then he's doing it will be better.",
                    "label": 0
                },
                {
                    "sent": "Window.",
                    "label": 0
                },
                {
                    "sent": "Final controller.",
                    "label": 0
                },
                {
                    "sent": "This thing is the limit.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So what's interesting, yeah?",
                    "label": 0
                },
                {
                    "sent": "The number of possibilities infinite yes.",
                    "label": 0
                },
                {
                    "sent": "They selected previously number of sequences of actions and then the number of action is quite small.",
                    "label": 0
                },
                {
                    "sent": "It's just right and left, but the state space is huge because the spaces of is continuous and so it's a maybe 4 dimensional state space.",
                    "label": 0
                },
                {
                    "sent": "So you have the angle of the of the ball.",
                    "label": 0
                },
                {
                    "sent": "This velocity and also the position of the of the carton.",
                    "label": 0
                },
                {
                    "sent": "It's and its velocity.",
                    "label": 0
                },
                {
                    "sent": "So it's like this faces of dimension 4.",
                    "label": 0
                },
                {
                    "sent": "This continues and so.",
                    "label": 0
                },
                {
                    "sent": "And so, exploring space of that dimension with this number of states is already difficult, so that's why at the beginning what you see that it seems to do like completely stupid things, but actually doesn't know what else to do, and so he's trying basically randomly actions.",
                    "label": 0
                },
                {
                    "sent": "But based on that is learning some part of the dynamics, so not everywhere because he hasn't explore this part of the state space yet is just exploring this.",
                    "label": 0
                },
                {
                    "sent": "But once he managed to actually start balancing the ball, then is learning to explore other parts of the state space.",
                    "label": 0
                },
                {
                    "sent": "And so, so this is interesting example because you really see the process of learning and so there is this tradeoff in between learning and planning learning.",
                    "label": 0
                },
                {
                    "sent": "So we will discuss more later.",
                    "label": 0
                },
                {
                    "sent": "But learning means discovering useful information in your search space in your in your state space in terms of dynamics and reward functions and planning means based on your current information.",
                    "label": 0
                },
                {
                    "sent": "Then well, you want to maximize.",
                    "label": 0
                },
                {
                    "sent": "Find the best possible strategy.",
                    "label": 0
                },
                {
                    "sent": "And so at the beginning there is more running an towards the end there is more planning, so he is using what is acquiring in terms of knowledge of his system to.",
                    "label": 0
                },
                {
                    "sent": "I mean to do this is controller.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a general picture that we show in reinforcement learning to represent the problem.",
                    "label": 0
                },
                {
                    "sent": "So we have an agent that interacts with its environment.",
                    "label": 0
                },
                {
                    "sent": "So the agent perceives some state.",
                    "label": 0
                },
                {
                    "sent": "Of the environment and then as to make some decisions, make action, choose actions, and so there is a loop here.",
                    "label": 0
                },
                {
                    "sent": "Kind of a feedback loop, so we receive some state, then exactions.",
                    "label": 0
                },
                {
                    "sent": "Environment returns than you state an this continuum, and once in awhile he received some feedback, so the reward or reinforcement and his goal is to find the actions in order to maximize the rewards that we get.",
                    "label": 0
                },
                {
                    "sent": "So that's the general.",
                    "label": 0
                },
                {
                    "sent": "Diagram that we have in reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "So the problems of reinforcement learning is that the environment is usually unknown.",
                    "label": 0
                },
                {
                    "sent": "In addition to that is usually well.",
                    "label": 0
                },
                {
                    "sent": "It may be stochastic, maybe partially observable.",
                    "label": 1
                },
                {
                    "sent": "It may be adversarial also could be some opponents that are playing against him.",
                    "label": 0
                },
                {
                    "sent": "An the only information that he gets is a reinforcement which may even be delayed, which tells seems how well you performed in the past.",
                    "label": 0
                },
                {
                    "sent": "So this information is very poor if you compare it to supervised learning setting where supervised learning at each state you are given what action you should do or what.",
                    "label": 0
                },
                {
                    "sent": "What is your level or you value for this specific state.",
                    "label": 0
                },
                {
                    "sent": "Here is not told what action it should choose in any state.",
                    "label": 0
                },
                {
                    "sent": "We just let him do whatever he wants and then we just evaluate the quality of what he has done.",
                    "label": 0
                },
                {
                    "sent": "Based on this, feedback is to improve strategy, so learning is more difficult than in supervised learning.",
                    "label": 1
                },
                {
                    "sent": "And the goal is exceeds maximize the expected some future rewards.",
                    "label": 0
                },
                {
                    "sent": "So here we see that we not only want to maximize the immediate reward, we just don't want to just find the action that maximizes immediate reward.",
                    "label": 0
                },
                {
                    "sent": "But we want to maximize.",
                    "label": 0
                },
                {
                    "sent": "Possible rewards in the future as well.",
                    "label": 0
                },
                {
                    "sent": "So for example, in chess you may want to sacrifice some material you know to win the game in the end.",
                    "label": 0
                },
                {
                    "sent": "So that's the goal, and so one of the problem is really how to sacrifice short-term small reward in order to privilege larger rewards in the long term.",
                    "label": 1
                },
                {
                    "sent": "And the way reinforcement learning usually addressed this problem.",
                    "label": 0
                },
                {
                    "sent": "Is by learning function.",
                    "label": 0
                },
                {
                    "sent": "That represents the best.",
                    "label": 0
                },
                {
                    "sent": "The best quality of rewards you can get from any possible states.",
                    "label": 0
                },
                {
                    "sent": "So this is cool.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Old value function.",
                    "label": 0
                },
                {
                    "sent": "For optimal value function, which is defined as.",
                    "label": 0
                },
                {
                    "sent": "The value.",
                    "label": 0
                },
                {
                    "sent": "An action agent can obtain from any state if he plays optimally.",
                    "label": 1
                },
                {
                    "sent": "So for example, if his goal is to maximize the sum of rewards, then the optimal value function at a given state X is going to be defined as the supremum over all possible strategies of the expected sum of rewards you will get starting from this state and following this this policy.",
                    "label": 0
                },
                {
                    "sent": "So let me illustrate this in a non formal way on this example causing a consider this environment where at time T you are in that state XD.",
                    "label": 0
                },
                {
                    "sent": "And you have two possible actions.",
                    "label": 0
                },
                {
                    "sent": "If you choose this action, then you will move to one of those states with some transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "Then if you think about what it means.",
                    "label": 0
                },
                {
                    "sent": "To be optimal from that state, it means being optimal immediately at the current state and then on each of the resulting possible next state, continuing to be optimal.",
                    "label": 0
                },
                {
                    "sent": "So we see that we can have a link between the optimal value function at the current state and the optimal value function at the next state.",
                    "label": 0
                },
                {
                    "sent": "When that I reach when we follow the best immediate action.",
                    "label": 0
                },
                {
                    "sent": "And this is, this leads to a property of the optimal value function, which is called the Bellman equation that we will see more in details later but just want to mention this here.",
                    "label": 0
                },
                {
                    "sent": "The bellman equation here says exactly whether I've just said which is the optimal value function at this state, is a Max over all possible actions of the immediate reward obtained in that state.",
                    "label": 0
                },
                {
                    "sent": "If I choose this action plus the expected optimal value function at the next state?",
                    "label": 0
                },
                {
                    "sent": "If I choose this action A.",
                    "label": 0
                },
                {
                    "sent": "And then it takes the maximum over all possible actions.",
                    "label": 0
                },
                {
                    "sent": "So what it means is that if I am in the state XD, an act optimally, no.",
                    "label": 0
                },
                {
                    "sent": "Well, my optimal value function is going to be equal to the.",
                    "label": 0
                },
                {
                    "sent": "The immediate optimal action plus the expected optimal value of the next X.",
                    "label": 0
                },
                {
                    "sent": "And so in some way, if I learn this optimal.",
                    "label": 0
                },
                {
                    "sent": "Value function.",
                    "label": 0
                },
                {
                    "sent": "Then well, then I can act optimally, because if I know the optimal value function, those next states well, I will just choose the action that maximizes the.",
                    "label": 0
                },
                {
                    "sent": "You know the immediate expected value of the optimal value function at the next state.",
                    "label": 0
                },
                {
                    "sent": "So in some ways it's a way to solve the problem I mentioned just before, because if I'm able to learn this optimal value function on the whole state space, then by acting really with respect to this value function, then I'm actually choosing my action in an optimal way in order to maximize the sum of expected rewards.",
                    "label": 0
                },
                {
                    "sent": "So let me also mention the term to temporal difference, which is which means which is a difference in evaluations of two successive states.",
                    "label": 0
                },
                {
                    "sent": "Up to the reward function, so temporal difference when I'm in XD and I choose action 80 and move to next X + 1 is a different V star of X2 plus one plus the immediate reward minus V star of XT an.",
                    "label": 0
                },
                {
                    "sent": "Based on what I've just said before.",
                    "label": 0
                },
                {
                    "sent": "We have the property that if we in XT we choose the optimal action.",
                    "label": 0
                },
                {
                    "sent": "Then this Delta T in expectation is going to be equal to 0, while because we have that V star of XT is equal to the expectation of the reward plus we start the next state so.",
                    "label": 0
                },
                {
                    "sent": "Expectation over all possible next states.",
                    "label": 0
                },
                {
                    "sent": "If I over results probabilities here.",
                    "label": 1
                },
                {
                    "sent": "So I can rewrite this so if I choose the optimal action.",
                    "label": 0
                },
                {
                    "sent": "The action that reaches the maximum.",
                    "label": 0
                },
                {
                    "sent": "Here we see that the star of XT is equal to the immediate reward for this action plus this expectation of the store for the next date.",
                    "label": 0
                },
                {
                    "sent": "And so this means that if I choose the optimal action here in expectation, my temporal difference is equal to 0.",
                    "label": 0
                },
                {
                    "sent": "And that's actually the the temporal difference is going to be.",
                    "label": 0
                },
                {
                    "sent": "The signal that is going to allow me to learn the optimal value function cause.",
                    "label": 0
                },
                {
                    "sent": "If I have a current evaluation of state such that when I'm in this state and I had to take an action and I moved to another state where somehow there is a mismatch between these two consecutive evaluations.",
                    "label": 0
                },
                {
                    "sent": "For example, the next state is overestimated compared to the previous one.",
                    "label": 0
                },
                {
                    "sent": "Then it means that there is some incoherence in my evaluations of the States and so I can update.",
                    "label": 0
                },
                {
                    "sent": "My evaluation in order to make it more current, more current means that in expectation this should be equal to zero and one this is actually true.",
                    "label": 0
                },
                {
                    "sent": "So when I have learned function so that whenever I make the best possible actions I have the property that expectation, those Delta T are equal to zero.",
                    "label": 0
                },
                {
                    "sent": "That means that I have just learned the optimal value function.",
                    "label": 0
                },
                {
                    "sent": "So this is also called surprise signal.",
                    "label": 0
                },
                {
                    "sent": "Whenever there is a surprise inside sense that there is a new evaluation of the next state is not current with my previous expectation.",
                    "label": 0
                },
                {
                    "sent": "My previous evaluation, then it means that I'm I can learn from this surprise to modify my value function in order to make it more.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "2 compatible you know, between my current state and my next dates?",
                    "label": 0
                },
                {
                    "sent": "So whenever there is unlike this signal of surprise and then I can update my optimal value function and once I reach that state where in expectation there is no more signal of normal temporal difference, then it means that I've reached a good approximation of my optimal value function.",
                    "label": 1
                },
                {
                    "sent": "That's the intuition for whole.",
                    "label": 0
                },
                {
                    "sent": "We can learn this optimal value function and the fact that based on this optimal value function, then I will be able to make optimal decisions.",
                    "label": 0
                },
                {
                    "sent": "I will make all this more formal in later.",
                    "label": 0
                },
                {
                    "sent": "Plants.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But so far so I wanted to say that so the challenges of reinforcement learning as I said, is.",
                    "label": 0
                },
                {
                    "sent": "So how problem?",
                    "label": 0
                },
                {
                    "sent": "Because information that we are is very poor.",
                    "label": 0
                },
                {
                    "sent": "In order to learn something which is quite.",
                    "label": 0
                },
                {
                    "sent": "Quite difficult because we want to learn a policy.",
                    "label": 0
                },
                {
                    "sent": "Especially in situations where the environment can be stochastic, adversarial, or partially observable.",
                    "label": 1
                },
                {
                    "sent": "One of the main points that I've already mentioned is the fact that since usually we don't know the state dynamics and the reward functions, then we need to learn them and at the same time as we need to plan so to find the best possible strategy in order to maximize our criterion.",
                    "label": 1
                },
                {
                    "sent": "For example, some of the words and so this combination between learning and planning, which appears actually in any optimization problem where there are some.",
                    "label": 0
                },
                {
                    "sent": "Unknown, I mean that you need to discover is also called the exploration exploitation tradeoff, and I will illustrate this exploration exploitation tradeoff more precisely when we reach the the part on multi arm bandits.",
                    "label": 0
                },
                {
                    "sent": "And I've also mentioned the other problem.",
                    "label": 0
                },
                {
                    "sent": "Which is the fact that we need to represent our functions in an approximate way, because usually the state space is so large that we cannot actually assign a value to each possible state of the state space.",
                    "label": 0
                },
                {
                    "sent": "So we have illustrated in the case of, I mean, the video that we've seen.",
                    "label": 0
                },
                {
                    "sent": "Actually, Martin Reimiller is using a neural network to approximate it's value function.",
                    "label": 0
                },
                {
                    "sent": "And that's the type of function approximator that he used in his demo.",
                    "label": 0
                },
                {
                    "sent": "So now I would like to.",
                    "label": 0
                },
                {
                    "sent": "To start, by describing the tools that we have for the planning.",
                    "label": 0
                },
                {
                    "sent": "So we're going to assume that everything else is known.",
                    "label": 1
                },
                {
                    "sent": "The dynamics are known.",
                    "label": 0
                },
                {
                    "sent": "The reward functions are known, everything is known and we just want to find the best possible plan.",
                    "label": 0
                },
                {
                    "sent": "Best possible strategy.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is a field called dynamic programming and I'm just going to describe a few tools that are developed in this field that are related to reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So usually in dynamic programming we assume that our process satisfies a Markov property and so we assume that we have a so called Markov decision process.",
                    "label": 0
                },
                {
                    "sent": "Markov decision process is basically just a Markov chain with actions, so we have a state space.",
                    "label": 1
                },
                {
                    "sent": "We have an action space.",
                    "label": 0
                },
                {
                    "sent": "And we assume that the dynamics are Markovian, so this satisfies the properties that basically the transition to a new state XD plus one given all the past states an all past actions is just a function of.",
                    "label": 0
                },
                {
                    "sent": "The current state and actions.",
                    "label": 0
                },
                {
                    "sent": "So basically all the past doesn't give us additional information to predict the future than the current state in action.",
                    "label": 0
                },
                {
                    "sent": "And so we can define transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "P of Y given XA, which represent the property to jump from state X to Y.",
                    "label": 0
                },
                {
                    "sent": "Given that I choose action A.",
                    "label": 0
                },
                {
                    "sent": "And we also have a reinforcement function or reward function.",
                    "label": 0
                },
                {
                    "sent": "So for example, it can be a function of the state and action, so that will be the immediate reward that is obtained when my agent in State X and choose action.",
                    "label": 1
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "Learning to write.",
                    "label": 0
                },
                {
                    "sent": "Yes, so it depends on what you put in state space.",
                    "label": 0
                },
                {
                    "sent": "But if you put in state space, all informations that are useful to predict the next state then it is so.",
                    "label": 0
                },
                {
                    "sent": "For example, for the bicycle you will need to include in the state space the current position and also the velocities you know and so.",
                    "label": 0
                },
                {
                    "sent": "So if you can reach a state space so that it contains enough information to predict the next state, then your dynamics are more caution.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So it could be stochastic.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it could be stochastic.",
                    "label": 0
                },
                {
                    "sent": "Also, it could be a stochastic function of the state action and the next day also.",
                    "label": 0
                },
                {
                    "sent": "So here I just write it as a deterministic function, but it could also be stochastic.",
                    "label": 0
                },
                {
                    "sent": "Equation is valid for.",
                    "label": 0
                },
                {
                    "sent": "Young man no.",
                    "label": 0
                },
                {
                    "sent": "The Bellman equation is good for Markovian only market initially.",
                    "label": 0
                },
                {
                    "sent": "But if you assume that the reward.",
                    "label": 0
                },
                {
                    "sent": "So yes, if you assume the rules depend on the future, then the bellman?",
                    "label": 0
                },
                {
                    "sent": "So then you are not in then the Bellman equation will not hold.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So, so that's the definition of an MDP Markov decision process, and so the goal of an MDP is too.",
                    "label": 0
                },
                {
                    "sent": "To compute the best possible policy.",
                    "label": 0
                },
                {
                    "sent": "So what is the policy?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A policy pie is a function that tells you at anytime step.",
                    "label": 1
                },
                {
                    "sent": "What action you choose?",
                    "label": 0
                },
                {
                    "sent": "For any possible state.",
                    "label": 0
                },
                {
                    "sent": "So it's mapping from state to actions.",
                    "label": 0
                },
                {
                    "sent": "At anytime step.",
                    "label": 0
                },
                {
                    "sent": "And so if you fix a policy.",
                    "label": 0
                },
                {
                    "sent": "Then basically you are Markov decision process.",
                    "label": 0
                },
                {
                    "sent": "Is equivalent to a Markov chain where the transitions from state XT to the next state.",
                    "label": 0
                },
                {
                    "sent": "Is defined as a polity to jump from exit to exit plus one, given that you choose the action corresponding to your policy by Tia fixed.",
                    "label": 0
                },
                {
                    "sent": "Sydney.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "That was my alarm Clock.",
                    "label": 0
                },
                {
                    "sent": "And also we will see that in large class of problems actually the optimal policy is not.",
                    "label": 0
                },
                {
                    "sent": "Is independent on the time.",
                    "label": 0
                },
                {
                    "sent": "And so we call a policy that is independent of time, stationary policy or Markovian policy.",
                    "label": 0
                },
                {
                    "sent": "So if a policy is just a function of state two actions, then we call this policy stationary.",
                    "label": 0
                },
                {
                    "sent": "Action.",
                    "label": 0
                },
                {
                    "sent": "When the policy is independent of time, then we call it stationary.",
                    "label": 1
                },
                {
                    "sent": "Map selection impossible state.",
                    "label": 1
                },
                {
                    "sent": "Oh yeah, map any possible state to an action.",
                    "label": 0
                },
                {
                    "sent": "Yes, mapping from state to actions.",
                    "label": 0
                },
                {
                    "sent": "And so our goal is to computer policy an the policy.",
                    "label": 0
                },
                {
                    "sent": "Which is the best possible one in terms of which criterion?",
                    "label": 0
                },
                {
                    "sent": "Well, that's why.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're going to define next the performance of a policy, and so this actually depends on the problem that you want to solve.",
                    "label": 0
                },
                {
                    "sent": "So you can either consider.",
                    "label": 0
                },
                {
                    "sent": "Finite or infinite, or reasons.",
                    "label": 0
                },
                {
                    "sent": "So, for example, if you want to find a policy that maximizes, say, the expected sum of rewards.",
                    "label": 0
                },
                {
                    "sent": "In a finite or is and then you can define the value of a policy.",
                    "label": 1
                },
                {
                    "sent": "So V Pi the expectation of the sum of rewards given that you start at time T. From State X and you follow policy by.",
                    "label": 0
                },
                {
                    "sent": "But you can also define.",
                    "label": 1
                },
                {
                    "sent": "The value function V Pi for infinite reasons.",
                    "label": 0
                },
                {
                    "sent": "So either in the discounted discounted setting.",
                    "label": 0
                },
                {
                    "sent": "Or undiscounted or average setting.",
                    "label": 0
                },
                {
                    "sent": "So in the rest of this lecture, I will focus on this first definition the discounted setting.",
                    "label": 0
                },
                {
                    "sent": "So here basically you're going to define your value of a policy as the expectation of the sum of discounted rewards.",
                    "label": 0
                },
                {
                    "sent": "Given that you start from the State X and you follow policy by and so gamet authority is your discount factor where gamma is number strictly less than one.",
                    "label": 0
                },
                {
                    "sent": "So basically this says that you.",
                    "label": 0
                },
                {
                    "sent": "Put more priority to immediate rewards, then later re what future rewards so the future rewards are discounted by gamma to the party.",
                    "label": 0
                },
                {
                    "sent": "So I will say it more about that later, but I want to say that there are other criterion.",
                    "label": 0
                },
                {
                    "sent": "For example the undiscounted setting where it's just the sum of rewards and so here in order to prevent us this series diverge, we have to make some assumptions, for example that there are some terminal states.",
                    "label": 0
                },
                {
                    "sent": "Or absorbing states such that once you reach this state, then the process stops and there and no more rewards is obtained and then you're going to assume that you reach is type of states with property one for example, which will guarantee you that this sum is finite.",
                    "label": 0
                },
                {
                    "sent": "Another criterion is the average one, so you take the limit, 20 goes to Infinity of the average of tee time step rewards where you take the.",
                    "label": 0
                },
                {
                    "sent": "Expectation of this over all possible runs.",
                    "label": 0
                },
                {
                    "sent": "So what is random here is XD.",
                    "label": 0
                },
                {
                    "sent": "When I mentioned expectation of this quantity, so it's a reward because I assume that the reward is is deterministic function.",
                    "label": 0
                },
                {
                    "sent": "But what is random here is is a state, because if I fix my policy Pi then I am in a Markov chain and so there are possible different runs and so my Steve is a random respect to this.",
                    "label": 0
                },
                {
                    "sent": "The transition probabilities of my Markov chain.",
                    "label": 0
                },
                {
                    "sent": "Oak.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I want to illustrate those definitions in the net at this summer school student problem.",
                    "label": 0
                },
                {
                    "sent": "So it's an MDP where the states are represented by this ovals OK. And so the actions are in one.",
                    "label": 0
                },
                {
                    "sent": "The sick arose.",
                    "label": 0
                },
                {
                    "sent": "So if you are in this state, you can either think or sleep.",
                    "label": 0
                },
                {
                    "sent": "If you think, then with property 1/2 represented by this light rose, you move to this next state.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you stay where you are.",
                    "label": 0
                },
                {
                    "sent": "No, I've represented by.",
                    "label": 0
                },
                {
                    "sent": "OK, bye in read the rewards you get.",
                    "label": 0
                },
                {
                    "sent": "In each of those states, if you are there, you get the minus one reward.",
                    "label": 0
                },
                {
                    "sent": "It's immediately one here, just assuming that there was just a function of the state.",
                    "label": 0
                },
                {
                    "sent": "And so those states are terminal states.",
                    "label": 0
                },
                {
                    "sent": "So when you reach those, you just stay there, you receive immediate reward and then you don't receive any more rewards and so your goal is to find the best.",
                    "label": 0
                },
                {
                    "sent": "Both involve policy which means at each state which action we should choose in order to maximize the expectation of the sum of rewards until you reach terminal State.",
                    "label": 1
                },
                {
                    "sent": "So let's try.",
                    "label": 0
                },
                {
                    "sent": "If you are there, so your reward is this much minus 1000.",
                    "label": 0
                },
                {
                    "sent": "Since its terminal state means that the value function there is also minus one side, so your value is this match.",
                    "label": 0
                },
                {
                    "sent": "Sure, there you're pretty happy your value function is 100 if you're there, you're value function is minus 10.",
                    "label": 0
                },
                {
                    "sent": "So now what happens if you are there?",
                    "label": 0
                },
                {
                    "sent": "What should you do?",
                    "label": 0
                },
                {
                    "sent": "Sleep yes, because if you have, she worked to merchants into merchant, you're going to be sick, so you need to think a little bit so that you understand this course, but after awhile you need to take a rest and enjoy just all your knowledge that you required and so you need to sleep at some points again.",
                    "label": 0
                },
                {
                    "sent": "And so if you sleep then with probability 1/2 you want 90%.",
                    "label": 0
                },
                {
                    "sent": "You're going to move to this state and have a good value an.",
                    "label": 0
                },
                {
                    "sent": "But with probably 10% you're going to stay in that state.",
                    "label": 0
                },
                {
                    "sent": "But that's always better than just thinking Moran getting to this bad state.",
                    "label": 0
                },
                {
                    "sent": "No happens if you are.",
                    "label": 0
                },
                {
                    "sent": "If you are there, for example, well, let's see what happens if you are there.",
                    "label": 0
                },
                {
                    "sent": "You think you should think in order to reach this state or you think you should sleep in order to reach that or come back here?",
                    "label": 0
                },
                {
                    "sent": "Think yes, it makes sense to think because as we say, the value function of this state is pretty high because from that state you know that you can reach this state, which is good.",
                    "label": 0
                },
                {
                    "sent": "So the value there is good.",
                    "label": 0
                },
                {
                    "sent": "Whereas the value there.",
                    "label": 0
                },
                {
                    "sent": "So if you take a look at this one, if he goes there then the value is pretty bad.",
                    "label": 0
                },
                {
                    "sent": "So probably the value of there is a good path here.",
                    "label": 0
                },
                {
                    "sent": "So you might want to try to reach this state because this state is good.",
                    "label": 0
                },
                {
                    "sent": "So by thinking then with half of the times you will reach this thing.",
                    "label": 0
                },
                {
                    "sent": "And come back to your initial state.",
                    "label": 0
                },
                {
                    "sent": "I love the other half of the times.",
                    "label": 0
                },
                {
                    "sent": "So no, if you are there, what should you do?",
                    "label": 0
                },
                {
                    "sent": "Yeah, you should think in order to reach this state, because probably this one is is also a good one.",
                    "label": 0
                },
                {
                    "sent": "OK, rather than sleeping and having a bad bad reward because you slept so much that you didn't even listen to the speaker, so that's bad.",
                    "label": 0
                },
                {
                    "sent": "So what happens if you are there?",
                    "label": 0
                },
                {
                    "sent": "Should you sleep or should you?",
                    "label": 0
                },
                {
                    "sent": "Think.",
                    "label": 0
                },
                {
                    "sent": "So who is for thinking?",
                    "label": 0
                },
                {
                    "sent": "So who is for sleeping?",
                    "label": 0
                },
                {
                    "sent": "Not so many.",
                    "label": 0
                },
                {
                    "sent": "Oh, you said sync already know you sleep already sleep.",
                    "label": 0
                },
                {
                    "sent": "So you think the most actions are the same value?",
                    "label": 0
                },
                {
                    "sent": "Well, actually you see that at this stage is not so obvious again because there are two States and we don't really know.",
                    "label": 0
                },
                {
                    "sent": "So as we have seen the value here, what is the value function there?",
                    "label": 0
                },
                {
                    "sent": "If we write the Bellman equation there, well many questions tells me that the value of this state is the immediate reward plus the maximum over 2 actions of the expectation of the optimal value at the resulting state.",
                    "label": 0
                },
                {
                    "sent": "So it's a maximum resource to actions of 05 times the value there.",
                    "label": 0
                },
                {
                    "sent": "05 times the value there, so the value there is already a function of this state, but also of this state and all this state because it takes a maximum over and the value there is a maximum of where these two actions of.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Basically it's really kind of intricate problem because the value at this stage is a function of the value of any other all the other state and it's kind of a fixed point equation.",
                    "label": 0
                },
                {
                    "sent": "And actually we will see that it's a value at all.",
                    "label": 0
                },
                {
                    "sent": "The state is a fixed point equation of the bellman operator.",
                    "label": 0
                },
                {
                    "sent": "We will define it in the next slide.",
                    "label": 0
                },
                {
                    "sent": "So if we make the computations, we actually find that from here it's better to sleep.",
                    "label": 0
                },
                {
                    "sent": "Why?",
                    "label": 1
                },
                {
                    "sent": "Because you get this additional bonus here, plus one reward, and then once you're there, you try to reach state and then and get to this.",
                    "label": 0
                },
                {
                    "sent": "Everyone's.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so if you write down, so that's the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "I represented the the optimal actions for each possible States and also the value function for each possible state.",
                    "label": 0
                },
                {
                    "sent": "So we value function is minus 1000.",
                    "label": 0
                },
                {
                    "sent": "Here's a value function is 88.9 because we can reach this.",
                    "label": 0
                },
                {
                    "sent": "The state.",
                    "label": 0
                },
                {
                    "sent": "And so on.",
                    "label": 0
                },
                {
                    "sent": "And here we see that the value of two is slightly higher than the value of three, and because of that it makes sense to try to reach this state rather than directly reaching this one.",
                    "label": 0
                },
                {
                    "sent": "And so we see that this type of equation that we have in order to solve this is this type of equations.",
                    "label": 0
                },
                {
                    "sent": "That's exactly the Bellman equation as I mentioned before, so maximum of two quantities representing each one.",
                    "label": 0
                },
                {
                    "sent": "What happens if you follow an action of the expected value of the next date?",
                    "label": 0
                },
                {
                    "sent": "Do you have any questions?",
                    "label": 0
                },
                {
                    "sent": "OK, so let's move on to more.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Precise definitions, and so we're going to consider one specific setting, which is nice because it's simple, so setting I want to consider is infinite tourism, discounted rewards, so I'm just going to define my value function for state X and foreign policy by at least 8X as the expectation.",
                    "label": 0
                },
                {
                    "sent": "The sum of discounted.",
                    "label": 0
                },
                {
                    "sent": "Rewards, given that I start from X an I follow policy by.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to use this discount factor, which makes everything nice because because of that I'm guaranteed that all those quantities are finite.",
                    "label": 1
                },
                {
                    "sent": "Then also makes sense from an economical point of view because this discount factor tells me.",
                    "label": 0
                },
                {
                    "sent": "That, well, I have preference for immediate rewards rather than future ones.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "If I tell you, I can give you $100 bill now or in one year, which one would you prefer?",
                    "label": 0
                },
                {
                    "sent": "No, yes, because anyone you're not sure to see me again, but if I tell you I can give you with very strong guarantees you have the choice either I give you $200 now or 100 in next year.",
                    "label": 0
                },
                {
                    "sent": "Which one do you prefer?",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, sorry.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I agree with you know if I wanna know are 200 next year.",
                    "label": 0
                },
                {
                    "sent": "Then 100 no.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you really don't trust me.",
                    "label": 0
                },
                {
                    "sent": "Anyway, I don't have any money, sorry, but OK if I guarantee that I can give you 500 next year, you may want to wait for next year because you know you will see me again and so and so if you multiply gamma by 500 and set it equal to 100 then you will see what is the value of gamma for this specific for your specific value.",
                    "label": 0
                },
                {
                    "sent": "So Gamma Times 500 should be equal to 100 and so in that case this defines.",
                    "label": 0
                },
                {
                    "sent": "The value of defined your preference for how much preference you assign to immediate rewards rather than future ones.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's assume that we want to define the performance or the value function of a policy Pi in this way, and so our goal is to find the policy Pi that maximizes for any possible state.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're going to write down formally the Bellman equations that I've mentioned earlier for any policy Pi V. Pi satisfies this equation.",
                    "label": 0
                },
                {
                    "sent": "So V Pi of X is just the immediate reward as said, X, given that I choose the action by of X plus the discounted value of the expectation of Veep, I add the next state.",
                    "label": 0
                },
                {
                    "sent": "Given that I'm in X and I choose action via fax.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's just the expectation of the pie at the next state.",
                    "label": 0
                },
                {
                    "sent": "The same overall possible next states.",
                    "label": 0
                },
                {
                    "sent": "Why of the probability to go from X to Y if I choose action Pi of X times the value of Y?",
                    "label": 0
                },
                {
                    "sent": "So V Pi is of X is immediate reward plus discounted expected value of the next state.",
                    "label": 0
                },
                {
                    "sent": "So that's a property to remember that the pie is defined like that.",
                    "label": 0
                },
                {
                    "sent": "Expectation of discount rewards in the future, and we have the properties at the Pi of X satisfies this equation called abandoning questions.",
                    "label": 0
                },
                {
                    "sent": "In other words, the pie is actually the fixed point of the bellman operator.",
                    "label": 1
                },
                {
                    "sent": "We write it Tip line which is defined as follows.",
                    "label": 0
                },
                {
                    "sent": "The tip I applied to any function W returns a function and this function evaluated at a state X is.",
                    "label": 0
                },
                {
                    "sent": "The immediate reward at X.",
                    "label": 0
                },
                {
                    "sent": "Plus gamma times the expected value of W at the next state.",
                    "label": 0
                },
                {
                    "sent": "So if I define T \u03c0.",
                    "label": 0
                },
                {
                    "sent": "As follows with Tippi is an operator that takes a function that returns a function with tip I applied to W outer state, X is reward plus gamma times the expected value of W as an extractor.",
                    "label": 0
                },
                {
                    "sent": "Then the Bellman equation just writes rewrites.",
                    "label": 0
                },
                {
                    "sent": "V \u03c0 equals tip Ivy \u03c0.",
                    "label": 0
                },
                {
                    "sent": "Because this is just.",
                    "label": 0
                },
                {
                    "sent": "The bellman operator tip I applied to V \u03c0.",
                    "label": 0
                },
                {
                    "sent": "And so here we see that the pie is a fixed point of the pie.",
                    "label": 0
                },
                {
                    "sent": "So by defining this Bellman operator OTP?",
                    "label": 0
                },
                {
                    "sent": "I have the properties at the pie is a fixed point of the pie so far.",
                    "label": 0
                },
                {
                    "sent": "So for any policy \u03c0 V, \u03c0 is a value of my policy, so function, so the pie is the solution to a fixed point equation.",
                    "label": 1
                },
                {
                    "sent": "V \u03c0 is a fixed point of my operator, Tippi, yes.",
                    "label": 0
                },
                {
                    "sent": "And then immediately want.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's a good question.",
                    "label": 0
                },
                {
                    "sent": "Actually, I'm not quite sure to know the details of the implementation, but you can think of immediate rewards.",
                    "label": 0
                },
                {
                    "sent": "For example, for this inverted pendulum as the height of the.",
                    "label": 0
                },
                {
                    "sent": "You know the extremes of the ball, so if you want to maximize it, it's maximized when you are in this upright position.",
                    "label": 0
                },
                {
                    "sent": "So this could be like an immediate reward.",
                    "label": 0
                },
                {
                    "sent": "OK, so it could be that you have like at each time step you have a reward immediate reward.",
                    "label": 0
                },
                {
                    "sent": "It depends on the state, or it could be that you have only eternal reward that tells you only when you reach the call then you reward is this much.",
                    "label": 0
                },
                {
                    "sent": "But in the setting that Martin did I think he has some immediate reward at each time step.",
                    "label": 0
                },
                {
                    "sent": "So this gives some indication on which direction we should search, but of course for example there might be a lot maybe.",
                    "label": 0
                },
                {
                    "sent": "Many local maximum, because maybe your pool is like that.",
                    "label": 0
                },
                {
                    "sent": "An trying to pull it up doesn't work because you don't have enough torque.",
                    "label": 0
                },
                {
                    "sent": "OK, so somehow you need to go this way in order to invert the pendulum.",
                    "label": 1
                },
                {
                    "sent": "So you need to go through a local maximum in order to reach a global one.",
                    "label": 0
                },
                {
                    "sent": "I can rewrite my Bellman operator using.",
                    "label": 0
                },
                {
                    "sent": "Matrix notations or operator notations.",
                    "label": 0
                },
                {
                    "sent": "So basically typy of W, so let's assume that the state space is finite.",
                    "label": 0
                },
                {
                    "sent": "So if the state is finite then those functions are just vectors.",
                    "label": 0
                },
                {
                    "sent": "OK, there is a finite number of values, so then you can see tip I as an operator that takes a vector an is defined as our \u03c0 plus gamma, \u03c0 W, where ARPI is a vector of rewards that assigns a reward to each possible state.",
                    "label": 0
                },
                {
                    "sent": "When you follow policy \u03c0 and \u03c0 is a matrix.",
                    "label": 0
                },
                {
                    "sent": "It's basically the transition matrix from state to next state.",
                    "label": 0
                },
                {
                    "sent": "If you follow policy Pi.",
                    "label": 0
                },
                {
                    "sent": "So in that case you can see that.",
                    "label": 0
                },
                {
                    "sent": "Tippi is actually an affine operator that takes W and returns this constant plus gamma \u03c0 W. So the proof.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This result is very simple, so in general I'm not going to go through all the proofs because it will take too much time, specially because the proof of this is not quite exciting.",
                    "label": 0
                },
                {
                    "sent": "Basically, you just say that you start from the definition.",
                    "label": 0
                },
                {
                    "sent": "An you isolate the first re one and say so.",
                    "label": 0
                },
                {
                    "sent": "The value function at this stage is just the reward.",
                    "label": 0
                },
                {
                    "sent": "The reward you get at the initial state plus the expectation of the rewards you get later and you rewrite this basically as this contractor times the expectation of the value function at the next date.",
                    "label": 0
                },
                {
                    "sent": "And you obtain the Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "OK, so now let's move to the bed Monica.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For Vista, So what is the star?",
                    "label": 1
                },
                {
                    "sent": "It's defined as Superman for all possible policies of VPA and it's called the optimal value function.",
                    "label": 1
                },
                {
                    "sent": "That's the best thing you can get studying from a state, an acting optimally.",
                    "label": 1
                },
                {
                    "sent": "So the star of X is defined as a supermom overall policies of V Pi of X.",
                    "label": 0
                },
                {
                    "sent": "And we also have a Bellman equation.",
                    "label": 1
                },
                {
                    "sent": "So it's called Bellman equation or dynamic programming equation.",
                    "label": 0
                },
                {
                    "sent": "On the star an it is written this way we start X is a maximum overall possible actions of the immediate action corresponding to a plus gamma times a discounted optimal value function at the next day.",
                    "label": 0
                },
                {
                    "sent": "So basically the difference with what we've seen before is that there is a maximum of actions here.",
                    "label": 0
                },
                {
                    "sent": "So what it tells you is that.",
                    "label": 0
                },
                {
                    "sent": "The optimal value you can get from a state X is obtained by.",
                    "label": 0
                },
                {
                    "sent": "Basically taking the best possible immediate action.",
                    "label": 1
                },
                {
                    "sent": "And later being optimal at the resulting state.",
                    "label": 0
                },
                {
                    "sent": "The value optimal value function is the Max or possible actions of the immediate reward plus gamma times expected value of the next optimal value of the next states.",
                    "label": 0
                },
                {
                    "sent": "And in that case also we can define a dynamic programming operator going to write it T. Which is defined in this way.",
                    "label": 0
                },
                {
                    "sent": "So T takes as input a function and returns function TW whose value of X is defined in that way.",
                    "label": 0
                },
                {
                    "sent": "Max Overaction reward air of XA plus discounted average value of W at the next date.",
                    "label": 0
                },
                {
                    "sent": "And so you see that this is just T applied to vystar, and so the bellman is a dynamic programming question, is just equivalent to saying that the star is a fixed point of.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The proof of this result is also easy, so just not going to go through it.",
                    "label": 0
                },
                {
                    "sent": "You can read the proof in the slides if you if you want it.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are some properties of the bellman operators, so tipai, and also the dynamic programming operator T. So both operators are monotonic.",
                    "label": 1
                },
                {
                    "sent": "And they are contract contraction in maximum though montinique means that if two functions satisfy this then tip IW is going to be basically tip.",
                    "label": 0
                },
                {
                    "sent": "I preserve the order.",
                    "label": 0
                },
                {
                    "sent": "In the function, W is less than double 225 W, one is less than TW.",
                    "label": 1
                },
                {
                    "sent": "And so we also have the contraction property that says that tip I applied to W 1 -- T Pi applied to W2 is bounded by gamma times the distance between W1 and W2 in L Infinity norm.",
                    "label": 0
                },
                {
                    "sent": "So this is very easy properties.",
                    "label": 0
                },
                {
                    "sent": "You can see that for example, maybe the simplest way to see.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is when you look at this matrix notations.",
                    "label": 0
                },
                {
                    "sent": "So basically Typi is monotonic because here we have a pie which is stochastic matrix.",
                    "label": 0
                },
                {
                    "sent": "And so this preserves the order.",
                    "label": 0
                },
                {
                    "sent": "So if W one is less than W2 in, then PW one is less than PP W2.",
                    "label": 0
                },
                {
                    "sent": "And also the norm of TPW 1 -- T, P W2 basically is bounded by gamma times P \u03c0 W 1 -- W two and since people is stochastic matrix again then it's L Infinity norm is less than one.",
                    "label": 0
                },
                {
                    "sent": "So we have that.",
                    "label": 0
                },
                {
                    "sent": "And the norm of the PW 1 -- T P W2 is bounded by W 1 -- W two.",
                    "label": 0
                },
                {
                    "sent": "Times gamma sorry.",
                    "label": 0
                },
                {
                    "sent": "So there is this gamma factor and so.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because of this nice gamma, this contraction.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This nice discount factor we have this construction property.",
                    "label": 0
                },
                {
                    "sent": "And so that's interesting property, because, as we have said, the value function is a fixed point of some operator, and this operator happens to be a contraction in some norm, and so we have the additional properties at the fixed point is unique.",
                    "label": 0
                },
                {
                    "sent": "And we will deduce algorithm in order to compute the fixed point.",
                    "label": 0
                },
                {
                    "sent": "Basically, since we have a contraction by iterating this operator we will build a sequence of functions.",
                    "label": 0
                },
                {
                    "sent": "So here is the result so.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the pie is the unique fixed point of Tippi, so we have already seen that it's it's a fixed point.",
                    "label": 1
                },
                {
                    "sent": "The fact that it's unique, immediate consequence of the fact that Pie is a contraction.",
                    "label": 0
                },
                {
                    "sent": "Vista is a unique fixed point of T. Again, this is a consequence of the fact that T is a contraction.",
                    "label": 1
                },
                {
                    "sent": "We are the properties that using matrix notations V \u03c0 is.",
                    "label": 0
                },
                {
                    "sent": "The inverse of the matrix identity minus gamma \u03c0.",
                    "label": 0
                },
                {
                    "sent": "Times are by the reward vector, and we have also the very important property.",
                    "label": 0
                },
                {
                    "sent": "That if we define the policy.",
                    "label": 0
                },
                {
                    "sent": "Like this?",
                    "label": 0
                },
                {
                    "sent": "Based on Vista, then this policy is optimal.",
                    "label": 0
                },
                {
                    "sent": "So what it means here is that if I define the policy, call it by star of X, so at each state X I'm going to choose one action that reaches a maximum of this quantity, so.",
                    "label": 0
                },
                {
                    "sent": "The action to the argument of this Max is an action that maximizes this quantity, which is immediate reward plus gamma.",
                    "label": 0
                },
                {
                    "sent": "Discounted value of the optimal value function.",
                    "label": 0
                },
                {
                    "sent": "At the next eight.",
                    "label": 0
                },
                {
                    "sent": "So what is important about this property is that if you are able in some way to compute this star, then immediately.",
                    "label": 0
                },
                {
                    "sent": "You have an optimal policy.",
                    "label": 0
                },
                {
                    "sent": "That's a way to deduce an optimal policy based on the optimal value function.",
                    "label": 0
                },
                {
                    "sent": "If you can assign the optimal value to any possible state, and basically the best thing you can do at any state is just being greedy.",
                    "label": 0
                },
                {
                    "sent": "With respect.",
                    "label": 0
                },
                {
                    "sent": "To this optimal value function, which means taking the action that maximizes the immediate reward plus the discounted expected value optimal value at the next states.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me prove those results.",
                    "label": 0
                },
                {
                    "sent": "So we have already given arguments for points one and two 4.3.",
                    "label": 0
                },
                {
                    "sent": "This is just.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is a consequence of the definition of Tippi, so 4.3 we have the pie is a fixed point of teepi by rewriting teepi using matrix notations we have.",
                    "label": 1
                },
                {
                    "sent": "Tip I reply is defined as the reward.",
                    "label": 0
                },
                {
                    "sent": "Our \u03c0 plus gamma Pi V Pi we can put the buy in design.",
                    "label": 0
                },
                {
                    "sent": "So by factorizing the pie we have identity minus gamma Pi.",
                    "label": 0
                },
                {
                    "sent": "Day \u03c0 equals or pie.",
                    "label": 0
                },
                {
                    "sent": "And then we just need to prove that this matrix is invertible.",
                    "label": 1
                },
                {
                    "sent": "And this matrix is invertible because we know that the eigenvalues of a stochastic matrix are bounded.",
                    "label": 0
                },
                {
                    "sent": "In model is by 1.",
                    "label": 0
                },
                {
                    "sent": "And so the eigenvalues of identity minus gamma Pi are equal to 1 minus gamma times the eigenvalues of the pie.",
                    "label": 0
                },
                {
                    "sent": "And so because although again volumes there are bunch of models, lesson 1 gamma is less than one.",
                    "label": 0
                },
                {
                    "sent": "It proves that basically the eigenvalues of of these metrics are bounded away from zero by 1 minus gamma.",
                    "label": 0
                },
                {
                    "sent": "And so the metrics is invertible, and so we can rewrite V Pi as identity management by minus one or five.",
                    "label": 0
                },
                {
                    "sent": "Which is what we claim.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here and then finally.",
                    "label": 0
                },
                {
                    "sent": "We want to prove that the policy defines in this way is optimal.",
                    "label": 1
                },
                {
                    "sent": "So what it means here it means that basically.",
                    "label": 0
                },
                {
                    "sent": "T by star is equal to.",
                    "label": 0
                },
                {
                    "sent": "T Vista.",
                    "label": 0
                },
                {
                    "sent": "Because the bellman operator T is exactly applied to Vista is exactly this quantity.",
                    "label": 0
                },
                {
                    "sent": "So T we star is exactly this.",
                    "label": 1
                },
                {
                    "sent": "And so when you take the argument of this Max, it means that the bellman operator applied to P star.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The bellman operator T Pi star applied to Vista is equal to T Vista.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So by defining Pi stars away I just defined before, we did use that T by star V star equals TV star.",
                    "label": 0
                },
                {
                    "sent": "OK, the bellman operator applied to buy Star Benefit or four Pi star applied to Vista is equal to the dynamic programming operator, the one with the Max.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to restart OK, but we already know that the star is a fixed point of T, so TV star is equal to Vista.",
                    "label": 1
                },
                {
                    "sent": "And So what we have is that T Pi star Vista is equal to Vista.",
                    "label": 0
                },
                {
                    "sent": "But we already know that this.",
                    "label": 1
                },
                {
                    "sent": "Operator as a unique solution, which is V \u03c0 star.",
                    "label": 0
                },
                {
                    "sent": "And so the price R = V star.",
                    "label": 0
                },
                {
                    "sent": "So V Pistori called Vista.",
                    "label": 1
                },
                {
                    "sent": "What does it means that the value of the policy Pi star that we have defined earlier is equal to the optimal value function, which means that this policy is optimum by definition.",
                    "label": 0
                },
                {
                    "sent": "So we have proven that the policy which is defined like that.",
                    "label": 0
                },
                {
                    "sent": "Is optimal and.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "By the way, it's also stationary because here the policy doesn't depend on time, is just the function of the current state.",
                    "label": 1
                },
                {
                    "sent": "So we have the property that in this setting an optimal policy can be obtained in the space of stationary policies.",
                    "label": 0
                },
                {
                    "sent": "So we have obtained the fact that if we are able to compute the optimal value function V star, then we can derive the optimal policy Pi star.",
                    "label": 0
                },
                {
                    "sent": "This policy is stationary and optimal.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so now we can start to describe algorithms in order to compute.",
                    "label": 0
                },
                {
                    "sent": "Well, we saw or going to compute the star because know that we know that from the start we can deduce the optimal policy or goal is to compute the star, and there are two type of algorithms.",
                    "label": 0
                },
                {
                    "sent": "One is called value iteration.",
                    "label": 0
                },
                {
                    "sent": "The other one is called policy iteration, so value iteration will perform iterations over the values policy iteration iteration over policies.",
                    "label": 0
                },
                {
                    "sent": "So value function very simple.",
                    "label": 0
                },
                {
                    "sent": "You start from any.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Function V0 and for any policy Pi, if you define the sequence of functions VK define recursively like that.",
                    "label": 0
                },
                {
                    "sent": "So where you iterate.",
                    "label": 0
                },
                {
                    "sent": "The bellman Aparati Pi.",
                    "label": 0
                },
                {
                    "sent": "Then the sequence VK tends to be by the value for the policy \u03c0. I know you can do the same by iterating the dynamic programming operator, so if you define the sequence VK such that VK plus one is called to TK, then your sequence VK converge to the star.",
                    "label": 0
                },
                {
                    "sent": "And the proof of that is very simple, so it's just one line.",
                    "label": 0
                },
                {
                    "sent": "If you look at the distance between VK plus 1 -- V star.",
                    "label": 0
                },
                {
                    "sent": "So by definition, VK plus one is T VK.",
                    "label": 0
                },
                {
                    "sent": "This star is the fixed point of T, so this is.",
                    "label": 0
                },
                {
                    "sent": "TV star here.",
                    "label": 0
                },
                {
                    "sent": "We use a contraction property of T to say that these distances less than gamma times the distance between Vicki and V star, and so we see that we have a contraction.",
                    "label": 0
                },
                {
                    "sent": "So we K plus one movie star is less than gamma.",
                    "label": 0
                },
                {
                    "sent": "We cannot Vista we can repeat this step K times to deduce that this is bounded by gamma to the power K 0 + K + 1.",
                    "label": 0
                },
                {
                    "sent": "Times the norm of V 0 minus Vista, which is whatever it is, but it's fixed and so when K goes to Infinity this goes to 0.",
                    "label": 0
                },
                {
                    "sent": "So we have an algorithm.",
                    "label": 0
                },
                {
                    "sent": "That says OK. Start from any function V0 and then I iterate my Bellman operator.",
                    "label": 0
                },
                {
                    "sent": "At each iteration and the sequence of function that I produce converges to the optimal value function.",
                    "label": 0
                }
            ]
        }
    }
}