{
    "id": "oc7eejkyc7jg7hzjf3oyak5vviy2pgvh",
    "title": "Good Learners for Evil Teachers",
    "info": {
        "author": [
            "Ofer Dekel, Microsoft Research"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Supervised Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_dekel_glfet/",
    "segmentation": [
        [
            "Thank you yeah, so let's play this was this is joint work with watch Amir from the Heat."
        ],
        [
            "University we're actually going to consider a problem that's very, very similar to the problem of which was the topic of the last talk, but we have kind of a different approach to it.",
            "So here's our problem.",
            "Setting.",
            "We're going to talk about Bunyard classification, so we have some instance SpaceX, and we have a distribution over the set of instances in binary labels.",
            "So this is the standard supervised learning setting, but we again assume that the labels are provided by K teachers that were going to call them, and some of them are going to be malicious.",
            "So that's basically our setting, and here's how we assume the data is generated.",
            "So we assume this kind of three step process of getting the data set.",
            "So first we sample only the instances from the marginal distribution only over instances.",
            "So we don't.",
            "You know this is kind of.",
            "It doesn't take into account with the label that we just sample the unlabeled instances.",
            "Then we randomly split this set of instances among the teachers and we have to know how many examples each teacher gets ahead of time.",
            "Doesn't have to be the same number, but you know, for simplicity we can assume that each teacher gets the same number of examples and then each teacher labels the examples that he controls, so there's no overlap in the examples and they basically get distinct sets.",
            "So this is our setting.",
            "And the motive?"
        ],
        [
            "An example that I'd like you to kind of have in your head.",
            "This is what motivates us.",
            "You know you can choose between which one of these you like, so the Mechanical Turk example was given by the previous speaker speaker, so I'll just briefly mention that.",
            "So we, you know we use the Internet or any type of online.",
            "Basically we we set up a website where we ask the public to contribute labels either of their own free will.",
            "You know they do this and astrophysics in biology.",
            "This is really kind of a voluntary thing, or mechanical Turk, which is the example of doing it for money.",
            "So we put some labeling tests on the Internet and then we do micro payments.",
            "We pay people to give us labels and when there's money involved you know people are really highly motivated to trick us and they write these scripts that masquerade as humans and they basically give us bad labels 'cause they want to take her money.",
            "The second example is any type of logs that you get from a website such as a search engine.",
            "There you'd like to analyze these logs to improve the performance of your search engine, but there's you know many people misuse your search engine for their own agenda.",
            "They try to optimize it, they try to trick you.",
            "They try to click on stuff that they want you know promoted on your on your search engine, and you want to be able to."
        ],
        [
            "Filter those things out.",
            "So just kind of, you know, because we're already on the topic, I want to do some of the you Ristic techniques that are commonly used, and by commonly you know.",
            "I mean, I'm from Microsoft, so this is.",
            "This is what they do.",
            "Some of the techniques that we that we typically use, so repeated labeling is kind of the topic of the previous talk, so this is asking multiple teachers for to label the same examples, then doing something majority or something more sophisticated.",
            "The problem is that is that it's not always possible, for instance, in a search engine scenario, you never really ask for the labels.",
            "You kind of get what people click on.",
            "And it's also wasteful, so in the Mechanical Turk setting, you can imagine you like pay for each label.",
            "If I want this thing labeled 5 or 10 times after pay 5 or 10 times as much, you know if you can afford that, it's a great thing.",
            "We do it a lot, but the question I'm going to ask at the end is, you know, is it really necessary in this technique is not going to use any repeated labeling?",
            "Another thing people do is honey pots.",
            "These are just questions that you already know the answer too and you put them in.",
            "You don't say that their tests, but you know you kind of check that the certain teacher is giving you labels that are consistent with what you know is true.",
            "And this is kind of your way of evaluating the quality of the teacher.",
            "Again, not always possible and requires the truth set, which is sometimes very hard to obtain, and there are other techniques.",
            "Outlier detection is kind of, you know, the thing that people think of 1st, but it's really a bad one.",
            "So for instance in Mechanical Turk.",
            "You get to see how fast the guy labeled the things you know.",
            "If you see that you provided 1000 labels a second, then you know it's a script and you kick him out, but this is really a, you know, an arms race between us trying to detect these guys that are trying to trick us and the people who are trying to trick us.",
            "And they're always going to win.",
            "So this is very easy to pass.",
            "These outlier detection test we try to detect the bad guys just by seeing that they're not normal.",
            "So the question we're asking is, are these techniques really necessary?",
            "Can we get by without any of these?",
            "All we want is just the original data and can be some."
        ],
        [
            "Clint.",
            "And this is our model of the world, so we chose a very dramatic model.",
            "There are good teachers which we denote by the set G and there are evil teachers and we insist on the word evil, because these are really bad guys.",
            "You know they can do whatever they want their malicious and they want to trick us.",
            "So let's talk about the good teachers first.",
            "So what's a good teacher?",
            "Good teacher knows the distribution.",
            "So previously we said that the distributions is over instances and labels and up until now we've only used the part of the distribution on the on the instances.",
            "So the good teacher knows the distribution and just samples the label.",
            "From the distribution.",
            "So essentially the examples that I get from the good teachers are essentially sampled from the original distribution.",
            "That's what good teachers do.",
            "The evil teachers are just bad, so they nor algorithm they know the distribution they're trying to hurt us, and they'll do anything they can.",
            "You know, by playing with these labels to hurt us.",
            "And of course they can collude with each other.",
            "And the only assumption we make is that the good teachers don't cooperate with the bad guys, which we think is a reasonable assumption, or more precisely, the evil teachers don't get to see.",
            "Which examples are labeled by the good teacher?",
            "So the good teachers don't show their their examples or their labels to the bad guys.",
            "So this is the only assumption we're going to make.",
            "Our algorithm is going to be an extension of SVM, so I just want to be."
        ],
        [
            "We go over there, so this was mentioned a few times in the previous talk, so I'll do it quickly.",
            "So this is the SVM objective function, right?",
            "There's a regularization term which controls the complexity of our hypothesis, and this is a hinge loss term, which is our proxy to the training error.",
            "And you know, I want to write it as a function.",
            "So for any hypothesis you choose given some sample in some.",
            "So this Lambda is the trade off parameter that trades off the importance of regularization term and his last term.",
            "There's there's some value of the objective function, and SVM basically tries to find the minimum of this thing."
        ],
        [
            "So this is just straightforward SVM.",
            "Now this is, you know, what can we expect to do?",
            "Well, how do we define like the ideal situation?",
            "What can we aspire to do?",
            "So this is what I want to define?",
            "Is this SVM plus Oracle algorithms?",
            "So for this I need a little bit more notation, so define the set of good examples to remember S is my entire training set right?",
            "There's evil stuff and good stuff in there, so they find the good examples S sub G. So this is just the union of all the sets that are labeled by these good guys.",
            "So this is essentially a set that I can assume is sampled IID from the actual distribution that I care about.",
            "Right, so there's this decal graphic dies the distribution of the care, but I want to be evaluated with respect to that.",
            "I want to generalize with restricted distribution, and this is just a sample from there.",
            "Now you know, don't make the mistake.",
            "I don't know this set, so I don't know who's good and who's evil I can.",
            "I can define this notation, but my algorithm doesn't get to know which teachers are good or bad, otherwise the problem would be, you know, just trivial and then this is the set of bed complemented that is just a set of bad examples.",
            "So ideally what would happen?",
            "Some Oracle would tell me, you know, these are the good guys and these are the bad guys.",
            "We just get rid of the labels.",
            "Given by the bad guys and you know we just have the good ID.",
            "Clean data will just have to be clean.",
            "You know the distribution D could have some uncertainty in it, but we just have the standard thing that we're used to in supervised learning and then we train our favorite classifier on that.",
            "You know, clean data.",
            "And here by favorite item, you know SVM is just the one that we chose here.",
            "So if this is how we define SVM, the optimizer of the just the estimate gect objective on the original data, so this is W hat, W star is going to be the SVM plus this Oracle input.",
            "So basically it's the optimizer of the SVM objective only on that clean subset of the data.",
            "So I need an Oracle to be able to calculate this thing, and then there's a little technical detail here which is an artifact of our analysis more than anything, which is that we have to change this trade off parameter just a little bit.",
            "You can imagine that if you know most of the data comes from clean from good teachers, then this ratio is going to be close to one.",
            "It's not going to be a very big change, you know.",
            "Another intuitive explanation why we need this is, you know, we're showing the objective function gets less data, so somehow we have to re calibrate the tradeoff between the regularization term and the hinge loss to.",
            "So you know if you don't accept any of those intuitive explanations, then just accept it as an artifact of our overanalysis here.",
            "So W star is kind of what we aspire to.",
            "So our goal is to approximate the output to find something that's close to this W star.",
            "So we're trying to optimize this function.",
            "Basically, without knowing what G is, right?",
            "So why is this a good goal?",
            "Since SG is just ID from the distribution that I actually care about, then by solving this thing, I'm essentially, you know I get all of those nice generalization guarantees that SVM gives me.",
            "It's just SVM on some data which represents the real world.",
            "As I you know, it's like.",
            "Define it and this is a good there for a good answer.",
            "So by choosing to approximate the solution, how can I can now forget about generalization theory and all that stuff and just focus on doing you know as well as this SVN password, so that's going to be my goal.",
            "Understood and how?",
            "How do I approach?"
        ],
        [
            "Proposed to do this, so here's the main idea.",
            "This is the motivating idea we have, so you know a simple question is how many support vectors does each teacher contribute?",
            "So the SVM output hypothesis is defined by support vectors.",
            "Those are the examples that actually matter.",
            "They're the ones that actually influence the hypothesis.",
            "How many do I get from each teacher?",
            "And the interesting observation is that if all the teachers were good, then the expected number of support vectors that I would get from each teacher would be the same.",
            "You know why is that?",
            "Because if they're all good, they're basically all give me labels just by sampling them from the true distribution of labels, and therefore the label generation process is essentially equivalent to just sampling the training data.",
            "You know all at once with the labels from this distribution, training a standard SVM on it.",
            "At that point, I know exactly who's a support vector and who isn't, and then as a last stage as a technical lastage, splitting the data randomly between the teachers.",
            "So now each teacher has an equal probability of getting.",
            "Each one of these support vectors.",
            "So just because you know they're all doing the same thing in the randomization process of splitting the data among the teachers, has nothing to do with the way that they sample.",
            "Then we expect if they're all good for them, each to contribute contribute the same.",
            "So this is the observation.",
            "And then we thought you know what?",
            "If we enforce this, you know, if everyone is good, then they can shoot with the same.",
            "How about if I forced everyone to have the same influence on the output hypothesis with that?"
        ],
        [
            "And that's our album.",
            "So to do that, it's much more convenient to move to the dual form of the SVM problem, so I'm not going to talk about how to derive this from that.",
            "I'll assume that you know most of you know about this.",
            "The only thing that's important to note here is that these are two equivalent formulations of the problem.",
            "It's very easy to drive them directly from, you know, basic conflict concepts in convex analysis, and the nice thing about this dual formulation is that the problem is parameterized by these dual variables, and there's a variable per each example, so each example has its variable.",
            "Each teacher is associated with a subset of examples.",
            "So now I can say you know, these dual variables are associated with teacher number one.",
            "These dual variables belong to teacher number 2.",
            "So this is how the primal hypothesis in the dual hypothesis are related, and we say that a support vector, you know an example is support vector.",
            "If the value that corresponds to the dual variable that response to that example is positive.",
            "So now instead of actually counting the number of support vectors, we're going to have a more continuous kind of definition of the influence of each teacher.",
            "It's just going to be the sum of the dual variables that correspond to the examples that are controlled by the teacher."
        ],
        [
            "And now we can add this constraint.",
            "So basically the influence of teacher T on the hypothesis is to define to be this thing where you know when this is the optimal solution.",
            "So again you take the sum of the dual variables that are controlled by that teacher by teacher T. So I just want to play notations when I have, you know.",
            "Square brackets, it just denotes the set one through M or one through K. So this means for every teacher T in the set one through Ki want the sum of its dual variables normalized by the number of things that controls, not to exceed.",
            "Just the average overall doing dual variables plus something small.",
            "So this is basically a smooth way or a continuous way of constraining them to each have the same influence on the output hypothesis up to some small term.",
            "You know this thing kind of accounts for the for some reasonable variance that you would get because after all this is a randomized process, so this is kind of the variance that you would expect you don't want anymore than this.",
            "And then there's this parameter epsilon, so epsilon kind of controls how harshly I want to constrain these guys.",
            "So epsilon is just some parameter that I.",
            "They will choose and it'll appear in my analysis.",
            "So again, the intuition you know you.",
            "You know that if they're all good, they're going to contribute the same.",
            "I want to force them to roughly contribute influence the hypothesis the same.",
            "So the first theory that we prove.",
            "Is that if this epsilon parameter is big enough and let's talk in a minute about what big enough means.",
            "Then with some high probability you know, forget what this means.",
            "This is just a small."
        ],
        [
            "So with one minus small thing, this is probability over the random is the random process of assigning these examples to the different teachers, then that equal contribution constraint that I just added so I added it constraint for each one of the teachers is only going to be binding for the bad guys, so it's not going to be binding for any of the good things.",
            "So she ascentia Lee.",
            "I've added something which has no effect on the guys that actually contribute to signal the information that I want to learn from, and I'm only, you know, kind of punishing.",
            "People who deviate from from from that signal and then you know why do I have why it doesn't hold for every epsilon, so I didn't say anything about how many good guys or bad guys there are right?",
            "So somehow this has to come into play.",
            "If there's you know everyone is bad then what can I do?",
            "So you would expect the size of the bad set, the size of the good signal appealing to appear in all these things.",
            "So this just says you know, if you're not too strict.",
            "Of course, if epsilon zero everyone's going to be constrained.",
            "If epsilon is just a little bit bigger than what you would like, so imagine.",
            "The set of bad examples is pretty small and this is a pretty small number, so it's not such a harsh harsh restriction.",
            "So again, with high probability, you're not even touching the good guys."
        ],
        [
            "So this kind of gives us a positive indication.",
            "But this is this is a kind of the heart of our of our work.",
            "So just let's go over the definitions once again, so this is the SVM objective.",
            "So for any training set in any trade off parameter and for any hypothesis there's some objective value.",
            "SBM denoted by W hat is just the minimizer of this thing W star.",
            "Is this SVM plus Oracle that we aspire to be almost as good as, and this guy minimizes the STM objective only on the clean data.",
            "Our algorithm we're going to do it by W prime, so there's WFW started crying, but double primes isn't here yet.",
            "You'll see it in a minute, so the first thing that we found out is.",
            "SCM itself is pretty robust and resilient to this type of stuff, so we ask ourselves, you know the SVM objective.",
            "So this is the minimum of the actual SVM on the entire data set.",
            "How well does it do on the thing that I actually care about, which is just a few data.",
            "So the SVM SVM hypothesis only on the clean data is not much bigger than the SD plus Oracle hypothesis on that same clean on the same clean subset of the data.",
            "So this difference is upper bounded by something which is a constant times the ratio between bad examples including.",
            "So that's pretty good.",
            "It means that if most of the examples are good, then we're already in a pretty good.",
            "You know situation, but the question is, how does our algorithm do better than this?",
            "Because we want to do better than SVM.",
            "So this."
        ],
        [
            "The previous theorem, and this is this is a already theorem #3, so this again relates this quantity which says how good is our hope hypothesis only on the good subset of examples.",
            "So again we don't know what this subsidy is, but we can we can prove this theorem that says that you know we're not much worse than the best possible performance on this good subset by how much?",
            "So it's a very very similar term.",
            "It's again the ratio of the bad examples over the good examples times this constant times something else.",
            "So if there's something else is much smaller than one.",
            "It means that we've done a better job than S. You know, it means that we can guarantee that by adding that additional constraint per teacher, we're actually doing better on the clean data set that we don't even know what it is and what is this?",
            "The term that we hope will be smaller than one.",
            "So our analysis shows that this V term is essentially proportional to the fraction of good examples that are either very close to the separator, another kind of have a small margin or misclassified.",
            "So essentially you take all the good examples and you say all those whose margin is less than or equal to 1 plus gamma.",
            "So keep in mind that SVM just cares about the margin of 1, so that's been tries to achieve a margin of one on the training set, and then we say, well, if you take a little bit more, if you take gamma more and ask how many people found that, how many you know examples find that range.",
            "That's kind of the difficulty of the problem in sunsets.",
            "So this is just a clean data.",
            "So if the clean data is easy if most of it is well separated, you know it's kind of spread apart, doesn't have to be perfectly linearly separable, But if.",
            "Significant fraction of it can be separated not only with a margin of 1, which is what SPM tries to do, but actually a little bit more than that.",
            "Then you know very few examples will fall in this set.",
            "You divide by the total number of examples in fees going to be small.",
            "So here's the cartoon that shows this."
        ],
        [
            "So if our data looks like this and this is the clean data, this is before the bad guys.",
            "Little do their thing.",
            "So if the clean data has a distinctive you know set of negatives instead of positives and there's a big margin, not only one but one plus some gamma that we say specifically what is in the paper, then our algorithm has will do much better than this.",
            "If the original data you know just look at the distribution, it's a difficult distribution to separate in the sense that a lot of the data falls close to this margin.",
            "Then we're not going to do much better than a sphere.",
            "So this is basically a result.",
            "This setting basically says that the original problem was kind of easy and it only became difficult due to the fact that these malicious bad guys came and messed up the labels right?",
            "So if there's a lot of signal in the original problem will be able to identify those guys are messing us up if the original thing is already a mess, then it'll be hard to distinguish between you know malicious and just the regular noise in the in the problem."
        ],
        [
            "So those are our theorems.",
            "We have quite a few experiments, almost done.",
            "I'm just going to give one, you know.",
            "Most of the graphs look kind of the same, so we did various different experiments with Reuters corpus.",
            "So we look at all possible binary classification problems with the high level categories of Reuters.",
            "We took the data, we split it into 100 different teachers.",
            "We did this randomly and then we varied the number of bad guys and then we simulated malicious features just by having flip the label.",
            "So let's say you know how do you?",
            "How do you even being malicious teacher?",
            "That's kind of a difficult problem in itself, so we just flip the label of the bad guys and then this these graphs.",
            "Basically, we repeated this multiple times.",
            "That's why we have these foundation error bars and this is the ratio of the loss.",
            "The test loss that our algorithm gets versus SVN.",
            "So this is how much better we are than if then solo is good.",
            "This is, you know we're a fraction of the loss of SPM.",
            "Sometimes we see that the minimal effect and sometimes it's a bigger effect and it gets.",
            "You know our algorithm works better the bigger than."
        ],
        [
            "So just to conclude, the take home message that I want to stay here is that you know this is kind of a proof of concept.",
            "If you want to take anything back from this this talk that to clean data, you really don't need any of these hacks, even though there are great heck so you know, don't get me wrong, I use them all the time, but you don't really need them.",
            "You don't need repeated labels, you don't need any prior knowledge.",
            "You don't need a truth set.",
            "All you need to know is which subsets how to partition your data into subsets controlled by teachers.",
            "So you know how to treat these, you know treaties.",
            "As one quivalence class and this is a different equivalence class and then you can clean the data in the paper.",
            "We have a second algorithm which I encourage you to read about and we have experiments that actually deviate from our theoretical setting where the partitioning into features is done by subtopics.",
            "So rather than randomly partitioning the data, we say you know there's an expert on this topic and expert on this topic and with Reuters corpus, that's pretty convenient to do because it has, you know, multiple categories, so our algorithm empirically works in that setting as well.",
            "And the last thing I want to say.",
            "Is if you like this, you know topic of collecting labels from many, many teachers.",
            "Then I also encourage you to come in, see or talk on Sunday afternoon which is that cold which deals with a very similar problem.",
            "Only there we don't assume this good versus evil.",
            "Very dramatic split of the world, but actually each teacher has some inherent noise parameter and we don't know what it is, so there's no bad guys.",
            "They're just, you know, very incompetent teachers and very.",
            "That's it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you yeah, so let's play this was this is joint work with watch Amir from the Heat.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "University we're actually going to consider a problem that's very, very similar to the problem of which was the topic of the last talk, but we have kind of a different approach to it.",
                    "label": 0
                },
                {
                    "sent": "So here's our problem.",
                    "label": 0
                },
                {
                    "sent": "Setting.",
                    "label": 0
                },
                {
                    "sent": "We're going to talk about Bunyard classification, so we have some instance SpaceX, and we have a distribution over the set of instances in binary labels.",
                    "label": 0
                },
                {
                    "sent": "So this is the standard supervised learning setting, but we again assume that the labels are provided by K teachers that were going to call them, and some of them are going to be malicious.",
                    "label": 1
                },
                {
                    "sent": "So that's basically our setting, and here's how we assume the data is generated.",
                    "label": 0
                },
                {
                    "sent": "So we assume this kind of three step process of getting the data set.",
                    "label": 0
                },
                {
                    "sent": "So first we sample only the instances from the marginal distribution only over instances.",
                    "label": 0
                },
                {
                    "sent": "So we don't.",
                    "label": 0
                },
                {
                    "sent": "You know this is kind of.",
                    "label": 0
                },
                {
                    "sent": "It doesn't take into account with the label that we just sample the unlabeled instances.",
                    "label": 0
                },
                {
                    "sent": "Then we randomly split this set of instances among the teachers and we have to know how many examples each teacher gets ahead of time.",
                    "label": 0
                },
                {
                    "sent": "Doesn't have to be the same number, but you know, for simplicity we can assume that each teacher gets the same number of examples and then each teacher labels the examples that he controls, so there's no overlap in the examples and they basically get distinct sets.",
                    "label": 0
                },
                {
                    "sent": "So this is our setting.",
                    "label": 0
                },
                {
                    "sent": "And the motive?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An example that I'd like you to kind of have in your head.",
                    "label": 0
                },
                {
                    "sent": "This is what motivates us.",
                    "label": 0
                },
                {
                    "sent": "You know you can choose between which one of these you like, so the Mechanical Turk example was given by the previous speaker speaker, so I'll just briefly mention that.",
                    "label": 0
                },
                {
                    "sent": "So we, you know we use the Internet or any type of online.",
                    "label": 1
                },
                {
                    "sent": "Basically we we set up a website where we ask the public to contribute labels either of their own free will.",
                    "label": 0
                },
                {
                    "sent": "You know they do this and astrophysics in biology.",
                    "label": 0
                },
                {
                    "sent": "This is really kind of a voluntary thing, or mechanical Turk, which is the example of doing it for money.",
                    "label": 0
                },
                {
                    "sent": "So we put some labeling tests on the Internet and then we do micro payments.",
                    "label": 0
                },
                {
                    "sent": "We pay people to give us labels and when there's money involved you know people are really highly motivated to trick us and they write these scripts that masquerade as humans and they basically give us bad labels 'cause they want to take her money.",
                    "label": 0
                },
                {
                    "sent": "The second example is any type of logs that you get from a website such as a search engine.",
                    "label": 0
                },
                {
                    "sent": "There you'd like to analyze these logs to improve the performance of your search engine, but there's you know many people misuse your search engine for their own agenda.",
                    "label": 1
                },
                {
                    "sent": "They try to optimize it, they try to trick you.",
                    "label": 0
                },
                {
                    "sent": "They try to click on stuff that they want you know promoted on your on your search engine, and you want to be able to.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Filter those things out.",
                    "label": 0
                },
                {
                    "sent": "So just kind of, you know, because we're already on the topic, I want to do some of the you Ristic techniques that are commonly used, and by commonly you know.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm from Microsoft, so this is.",
                    "label": 0
                },
                {
                    "sent": "This is what they do.",
                    "label": 0
                },
                {
                    "sent": "Some of the techniques that we that we typically use, so repeated labeling is kind of the topic of the previous talk, so this is asking multiple teachers for to label the same examples, then doing something majority or something more sophisticated.",
                    "label": 0
                },
                {
                    "sent": "The problem is that is that it's not always possible, for instance, in a search engine scenario, you never really ask for the labels.",
                    "label": 1
                },
                {
                    "sent": "You kind of get what people click on.",
                    "label": 0
                },
                {
                    "sent": "And it's also wasteful, so in the Mechanical Turk setting, you can imagine you like pay for each label.",
                    "label": 0
                },
                {
                    "sent": "If I want this thing labeled 5 or 10 times after pay 5 or 10 times as much, you know if you can afford that, it's a great thing.",
                    "label": 0
                },
                {
                    "sent": "We do it a lot, but the question I'm going to ask at the end is, you know, is it really necessary in this technique is not going to use any repeated labeling?",
                    "label": 0
                },
                {
                    "sent": "Another thing people do is honey pots.",
                    "label": 0
                },
                {
                    "sent": "These are just questions that you already know the answer too and you put them in.",
                    "label": 0
                },
                {
                    "sent": "You don't say that their tests, but you know you kind of check that the certain teacher is giving you labels that are consistent with what you know is true.",
                    "label": 0
                },
                {
                    "sent": "And this is kind of your way of evaluating the quality of the teacher.",
                    "label": 0
                },
                {
                    "sent": "Again, not always possible and requires the truth set, which is sometimes very hard to obtain, and there are other techniques.",
                    "label": 1
                },
                {
                    "sent": "Outlier detection is kind of, you know, the thing that people think of 1st, but it's really a bad one.",
                    "label": 0
                },
                {
                    "sent": "So for instance in Mechanical Turk.",
                    "label": 0
                },
                {
                    "sent": "You get to see how fast the guy labeled the things you know.",
                    "label": 0
                },
                {
                    "sent": "If you see that you provided 1000 labels a second, then you know it's a script and you kick him out, but this is really a, you know, an arms race between us trying to detect these guys that are trying to trick us and the people who are trying to trick us.",
                    "label": 0
                },
                {
                    "sent": "And they're always going to win.",
                    "label": 1
                },
                {
                    "sent": "So this is very easy to pass.",
                    "label": 1
                },
                {
                    "sent": "These outlier detection test we try to detect the bad guys just by seeing that they're not normal.",
                    "label": 0
                },
                {
                    "sent": "So the question we're asking is, are these techniques really necessary?",
                    "label": 0
                },
                {
                    "sent": "Can we get by without any of these?",
                    "label": 0
                },
                {
                    "sent": "All we want is just the original data and can be some.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Clint.",
                    "label": 0
                },
                {
                    "sent": "And this is our model of the world, so we chose a very dramatic model.",
                    "label": 0
                },
                {
                    "sent": "There are good teachers which we denote by the set G and there are evil teachers and we insist on the word evil, because these are really bad guys.",
                    "label": 0
                },
                {
                    "sent": "You know they can do whatever they want their malicious and they want to trick us.",
                    "label": 0
                },
                {
                    "sent": "So let's talk about the good teachers first.",
                    "label": 0
                },
                {
                    "sent": "So what's a good teacher?",
                    "label": 0
                },
                {
                    "sent": "Good teacher knows the distribution.",
                    "label": 0
                },
                {
                    "sent": "So previously we said that the distributions is over instances and labels and up until now we've only used the part of the distribution on the on the instances.",
                    "label": 0
                },
                {
                    "sent": "So the good teacher knows the distribution and just samples the label.",
                    "label": 0
                },
                {
                    "sent": "From the distribution.",
                    "label": 0
                },
                {
                    "sent": "So essentially the examples that I get from the good teachers are essentially sampled from the original distribution.",
                    "label": 1
                },
                {
                    "sent": "That's what good teachers do.",
                    "label": 0
                },
                {
                    "sent": "The evil teachers are just bad, so they nor algorithm they know the distribution they're trying to hurt us, and they'll do anything they can.",
                    "label": 0
                },
                {
                    "sent": "You know, by playing with these labels to hurt us.",
                    "label": 0
                },
                {
                    "sent": "And of course they can collude with each other.",
                    "label": 0
                },
                {
                    "sent": "And the only assumption we make is that the good teachers don't cooperate with the bad guys, which we think is a reasonable assumption, or more precisely, the evil teachers don't get to see.",
                    "label": 1
                },
                {
                    "sent": "Which examples are labeled by the good teacher?",
                    "label": 1
                },
                {
                    "sent": "So the good teachers don't show their their examples or their labels to the bad guys.",
                    "label": 0
                },
                {
                    "sent": "So this is the only assumption we're going to make.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm is going to be an extension of SVM, so I just want to be.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We go over there, so this was mentioned a few times in the previous talk, so I'll do it quickly.",
                    "label": 0
                },
                {
                    "sent": "So this is the SVM objective function, right?",
                    "label": 1
                },
                {
                    "sent": "There's a regularization term which controls the complexity of our hypothesis, and this is a hinge loss term, which is our proxy to the training error.",
                    "label": 0
                },
                {
                    "sent": "And you know, I want to write it as a function.",
                    "label": 0
                },
                {
                    "sent": "So for any hypothesis you choose given some sample in some.",
                    "label": 0
                },
                {
                    "sent": "So this Lambda is the trade off parameter that trades off the importance of regularization term and his last term.",
                    "label": 0
                },
                {
                    "sent": "There's there's some value of the objective function, and SVM basically tries to find the minimum of this thing.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is just straightforward SVM.",
                    "label": 0
                },
                {
                    "sent": "Now this is, you know, what can we expect to do?",
                    "label": 0
                },
                {
                    "sent": "Well, how do we define like the ideal situation?",
                    "label": 0
                },
                {
                    "sent": "What can we aspire to do?",
                    "label": 0
                },
                {
                    "sent": "So this is what I want to define?",
                    "label": 0
                },
                {
                    "sent": "Is this SVM plus Oracle algorithms?",
                    "label": 0
                },
                {
                    "sent": "So for this I need a little bit more notation, so define the set of good examples to remember S is my entire training set right?",
                    "label": 1
                },
                {
                    "sent": "There's evil stuff and good stuff in there, so they find the good examples S sub G. So this is just the union of all the sets that are labeled by these good guys.",
                    "label": 0
                },
                {
                    "sent": "So this is essentially a set that I can assume is sampled IID from the actual distribution that I care about.",
                    "label": 0
                },
                {
                    "sent": "Right, so there's this decal graphic dies the distribution of the care, but I want to be evaluated with respect to that.",
                    "label": 0
                },
                {
                    "sent": "I want to generalize with restricted distribution, and this is just a sample from there.",
                    "label": 0
                },
                {
                    "sent": "Now you know, don't make the mistake.",
                    "label": 0
                },
                {
                    "sent": "I don't know this set, so I don't know who's good and who's evil I can.",
                    "label": 1
                },
                {
                    "sent": "I can define this notation, but my algorithm doesn't get to know which teachers are good or bad, otherwise the problem would be, you know, just trivial and then this is the set of bed complemented that is just a set of bad examples.",
                    "label": 0
                },
                {
                    "sent": "So ideally what would happen?",
                    "label": 0
                },
                {
                    "sent": "Some Oracle would tell me, you know, these are the good guys and these are the bad guys.",
                    "label": 0
                },
                {
                    "sent": "We just get rid of the labels.",
                    "label": 0
                },
                {
                    "sent": "Given by the bad guys and you know we just have the good ID.",
                    "label": 1
                },
                {
                    "sent": "Clean data will just have to be clean.",
                    "label": 0
                },
                {
                    "sent": "You know the distribution D could have some uncertainty in it, but we just have the standard thing that we're used to in supervised learning and then we train our favorite classifier on that.",
                    "label": 0
                },
                {
                    "sent": "You know, clean data.",
                    "label": 0
                },
                {
                    "sent": "And here by favorite item, you know SVM is just the one that we chose here.",
                    "label": 0
                },
                {
                    "sent": "So if this is how we define SVM, the optimizer of the just the estimate gect objective on the original data, so this is W hat, W star is going to be the SVM plus this Oracle input.",
                    "label": 0
                },
                {
                    "sent": "So basically it's the optimizer of the SVM objective only on that clean subset of the data.",
                    "label": 0
                },
                {
                    "sent": "So I need an Oracle to be able to calculate this thing, and then there's a little technical detail here which is an artifact of our analysis more than anything, which is that we have to change this trade off parameter just a little bit.",
                    "label": 0
                },
                {
                    "sent": "You can imagine that if you know most of the data comes from clean from good teachers, then this ratio is going to be close to one.",
                    "label": 0
                },
                {
                    "sent": "It's not going to be a very big change, you know.",
                    "label": 0
                },
                {
                    "sent": "Another intuitive explanation why we need this is, you know, we're showing the objective function gets less data, so somehow we have to re calibrate the tradeoff between the regularization term and the hinge loss to.",
                    "label": 0
                },
                {
                    "sent": "So you know if you don't accept any of those intuitive explanations, then just accept it as an artifact of our overanalysis here.",
                    "label": 1
                },
                {
                    "sent": "So W star is kind of what we aspire to.",
                    "label": 1
                },
                {
                    "sent": "So our goal is to approximate the output to find something that's close to this W star.",
                    "label": 1
                },
                {
                    "sent": "So we're trying to optimize this function.",
                    "label": 0
                },
                {
                    "sent": "Basically, without knowing what G is, right?",
                    "label": 0
                },
                {
                    "sent": "So why is this a good goal?",
                    "label": 0
                },
                {
                    "sent": "Since SG is just ID from the distribution that I actually care about, then by solving this thing, I'm essentially, you know I get all of those nice generalization guarantees that SVM gives me.",
                    "label": 0
                },
                {
                    "sent": "It's just SVM on some data which represents the real world.",
                    "label": 0
                },
                {
                    "sent": "As I you know, it's like.",
                    "label": 0
                },
                {
                    "sent": "Define it and this is a good there for a good answer.",
                    "label": 0
                },
                {
                    "sent": "So by choosing to approximate the solution, how can I can now forget about generalization theory and all that stuff and just focus on doing you know as well as this SVN password, so that's going to be my goal.",
                    "label": 0
                },
                {
                    "sent": "Understood and how?",
                    "label": 0
                },
                {
                    "sent": "How do I approach?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Proposed to do this, so here's the main idea.",
                    "label": 0
                },
                {
                    "sent": "This is the motivating idea we have, so you know a simple question is how many support vectors does each teacher contribute?",
                    "label": 1
                },
                {
                    "sent": "So the SVM output hypothesis is defined by support vectors.",
                    "label": 0
                },
                {
                    "sent": "Those are the examples that actually matter.",
                    "label": 0
                },
                {
                    "sent": "They're the ones that actually influence the hypothesis.",
                    "label": 0
                },
                {
                    "sent": "How many do I get from each teacher?",
                    "label": 0
                },
                {
                    "sent": "And the interesting observation is that if all the teachers were good, then the expected number of support vectors that I would get from each teacher would be the same.",
                    "label": 0
                },
                {
                    "sent": "You know why is that?",
                    "label": 0
                },
                {
                    "sent": "Because if they're all good, they're basically all give me labels just by sampling them from the true distribution of labels, and therefore the label generation process is essentially equivalent to just sampling the training data.",
                    "label": 0
                },
                {
                    "sent": "You know all at once with the labels from this distribution, training a standard SVM on it.",
                    "label": 0
                },
                {
                    "sent": "At that point, I know exactly who's a support vector and who isn't, and then as a last stage as a technical lastage, splitting the data randomly between the teachers.",
                    "label": 0
                },
                {
                    "sent": "So now each teacher has an equal probability of getting.",
                    "label": 0
                },
                {
                    "sent": "Each one of these support vectors.",
                    "label": 0
                },
                {
                    "sent": "So just because you know they're all doing the same thing in the randomization process of splitting the data among the teachers, has nothing to do with the way that they sample.",
                    "label": 0
                },
                {
                    "sent": "Then we expect if they're all good for them, each to contribute contribute the same.",
                    "label": 0
                },
                {
                    "sent": "So this is the observation.",
                    "label": 0
                },
                {
                    "sent": "And then we thought you know what?",
                    "label": 0
                },
                {
                    "sent": "If we enforce this, you know, if everyone is good, then they can shoot with the same.",
                    "label": 0
                },
                {
                    "sent": "How about if I forced everyone to have the same influence on the output hypothesis with that?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's our album.",
                    "label": 0
                },
                {
                    "sent": "So to do that, it's much more convenient to move to the dual form of the SVM problem, so I'm not going to talk about how to derive this from that.",
                    "label": 0
                },
                {
                    "sent": "I'll assume that you know most of you know about this.",
                    "label": 0
                },
                {
                    "sent": "The only thing that's important to note here is that these are two equivalent formulations of the problem.",
                    "label": 0
                },
                {
                    "sent": "It's very easy to drive them directly from, you know, basic conflict concepts in convex analysis, and the nice thing about this dual formulation is that the problem is parameterized by these dual variables, and there's a variable per each example, so each example has its variable.",
                    "label": 0
                },
                {
                    "sent": "Each teacher is associated with a subset of examples.",
                    "label": 0
                },
                {
                    "sent": "So now I can say you know, these dual variables are associated with teacher number one.",
                    "label": 0
                },
                {
                    "sent": "These dual variables belong to teacher number 2.",
                    "label": 0
                },
                {
                    "sent": "So this is how the primal hypothesis in the dual hypothesis are related, and we say that a support vector, you know an example is support vector.",
                    "label": 1
                },
                {
                    "sent": "If the value that corresponds to the dual variable that response to that example is positive.",
                    "label": 0
                },
                {
                    "sent": "So now instead of actually counting the number of support vectors, we're going to have a more continuous kind of definition of the influence of each teacher.",
                    "label": 0
                },
                {
                    "sent": "It's just going to be the sum of the dual variables that correspond to the examples that are controlled by the teacher.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now we can add this constraint.",
                    "label": 0
                },
                {
                    "sent": "So basically the influence of teacher T on the hypothesis is to define to be this thing where you know when this is the optimal solution.",
                    "label": 0
                },
                {
                    "sent": "So again you take the sum of the dual variables that are controlled by that teacher by teacher T. So I just want to play notations when I have, you know.",
                    "label": 0
                },
                {
                    "sent": "Square brackets, it just denotes the set one through M or one through K. So this means for every teacher T in the set one through Ki want the sum of its dual variables normalized by the number of things that controls, not to exceed.",
                    "label": 0
                },
                {
                    "sent": "Just the average overall doing dual variables plus something small.",
                    "label": 0
                },
                {
                    "sent": "So this is basically a smooth way or a continuous way of constraining them to each have the same influence on the output hypothesis up to some small term.",
                    "label": 0
                },
                {
                    "sent": "You know this thing kind of accounts for the for some reasonable variance that you would get because after all this is a randomized process, so this is kind of the variance that you would expect you don't want anymore than this.",
                    "label": 0
                },
                {
                    "sent": "And then there's this parameter epsilon, so epsilon kind of controls how harshly I want to constrain these guys.",
                    "label": 0
                },
                {
                    "sent": "So epsilon is just some parameter that I.",
                    "label": 0
                },
                {
                    "sent": "They will choose and it'll appear in my analysis.",
                    "label": 0
                },
                {
                    "sent": "So again, the intuition you know you.",
                    "label": 0
                },
                {
                    "sent": "You know that if they're all good, they're going to contribute the same.",
                    "label": 0
                },
                {
                    "sent": "I want to force them to roughly contribute influence the hypothesis the same.",
                    "label": 0
                },
                {
                    "sent": "So the first theory that we prove.",
                    "label": 0
                },
                {
                    "sent": "Is that if this epsilon parameter is big enough and let's talk in a minute about what big enough means.",
                    "label": 0
                },
                {
                    "sent": "Then with some high probability you know, forget what this means.",
                    "label": 0
                },
                {
                    "sent": "This is just a small.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So with one minus small thing, this is probability over the random is the random process of assigning these examples to the different teachers, then that equal contribution constraint that I just added so I added it constraint for each one of the teachers is only going to be binding for the bad guys, so it's not going to be binding for any of the good things.",
                    "label": 1
                },
                {
                    "sent": "So she ascentia Lee.",
                    "label": 0
                },
                {
                    "sent": "I've added something which has no effect on the guys that actually contribute to signal the information that I want to learn from, and I'm only, you know, kind of punishing.",
                    "label": 0
                },
                {
                    "sent": "People who deviate from from from that signal and then you know why do I have why it doesn't hold for every epsilon, so I didn't say anything about how many good guys or bad guys there are right?",
                    "label": 0
                },
                {
                    "sent": "So somehow this has to come into play.",
                    "label": 0
                },
                {
                    "sent": "If there's you know everyone is bad then what can I do?",
                    "label": 0
                },
                {
                    "sent": "So you would expect the size of the bad set, the size of the good signal appealing to appear in all these things.",
                    "label": 0
                },
                {
                    "sent": "So this just says you know, if you're not too strict.",
                    "label": 0
                },
                {
                    "sent": "Of course, if epsilon zero everyone's going to be constrained.",
                    "label": 0
                },
                {
                    "sent": "If epsilon is just a little bit bigger than what you would like, so imagine.",
                    "label": 0
                },
                {
                    "sent": "The set of bad examples is pretty small and this is a pretty small number, so it's not such a harsh harsh restriction.",
                    "label": 0
                },
                {
                    "sent": "So again, with high probability, you're not even touching the good guys.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this kind of gives us a positive indication.",
                    "label": 0
                },
                {
                    "sent": "But this is this is a kind of the heart of our of our work.",
                    "label": 0
                },
                {
                    "sent": "So just let's go over the definitions once again, so this is the SVM objective.",
                    "label": 0
                },
                {
                    "sent": "So for any training set in any trade off parameter and for any hypothesis there's some objective value.",
                    "label": 0
                },
                {
                    "sent": "SBM denoted by W hat is just the minimizer of this thing W star.",
                    "label": 0
                },
                {
                    "sent": "Is this SVM plus Oracle that we aspire to be almost as good as, and this guy minimizes the STM objective only on the clean data.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm we're going to do it by W prime, so there's WFW started crying, but double primes isn't here yet.",
                    "label": 0
                },
                {
                    "sent": "You'll see it in a minute, so the first thing that we found out is.",
                    "label": 0
                },
                {
                    "sent": "SCM itself is pretty robust and resilient to this type of stuff, so we ask ourselves, you know the SVM objective.",
                    "label": 0
                },
                {
                    "sent": "So this is the minimum of the actual SVM on the entire data set.",
                    "label": 0
                },
                {
                    "sent": "How well does it do on the thing that I actually care about, which is just a few data.",
                    "label": 0
                },
                {
                    "sent": "So the SVM SVM hypothesis only on the clean data is not much bigger than the SD plus Oracle hypothesis on that same clean on the same clean subset of the data.",
                    "label": 0
                },
                {
                    "sent": "So this difference is upper bounded by something which is a constant times the ratio between bad examples including.",
                    "label": 0
                },
                {
                    "sent": "So that's pretty good.",
                    "label": 0
                },
                {
                    "sent": "It means that if most of the examples are good, then we're already in a pretty good.",
                    "label": 0
                },
                {
                    "sent": "You know situation, but the question is, how does our algorithm do better than this?",
                    "label": 0
                },
                {
                    "sent": "Because we want to do better than SVM.",
                    "label": 0
                },
                {
                    "sent": "So this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The previous theorem, and this is this is a already theorem #3, so this again relates this quantity which says how good is our hope hypothesis only on the good subset of examples.",
                    "label": 0
                },
                {
                    "sent": "So again we don't know what this subsidy is, but we can we can prove this theorem that says that you know we're not much worse than the best possible performance on this good subset by how much?",
                    "label": 0
                },
                {
                    "sent": "So it's a very very similar term.",
                    "label": 0
                },
                {
                    "sent": "It's again the ratio of the bad examples over the good examples times this constant times something else.",
                    "label": 0
                },
                {
                    "sent": "So if there's something else is much smaller than one.",
                    "label": 0
                },
                {
                    "sent": "It means that we've done a better job than S. You know, it means that we can guarantee that by adding that additional constraint per teacher, we're actually doing better on the clean data set that we don't even know what it is and what is this?",
                    "label": 0
                },
                {
                    "sent": "The term that we hope will be smaller than one.",
                    "label": 0
                },
                {
                    "sent": "So our analysis shows that this V term is essentially proportional to the fraction of good examples that are either very close to the separator, another kind of have a small margin or misclassified.",
                    "label": 0
                },
                {
                    "sent": "So essentially you take all the good examples and you say all those whose margin is less than or equal to 1 plus gamma.",
                    "label": 0
                },
                {
                    "sent": "So keep in mind that SVM just cares about the margin of 1, so that's been tries to achieve a margin of one on the training set, and then we say, well, if you take a little bit more, if you take gamma more and ask how many people found that, how many you know examples find that range.",
                    "label": 0
                },
                {
                    "sent": "That's kind of the difficulty of the problem in sunsets.",
                    "label": 0
                },
                {
                    "sent": "So this is just a clean data.",
                    "label": 0
                },
                {
                    "sent": "So if the clean data is easy if most of it is well separated, you know it's kind of spread apart, doesn't have to be perfectly linearly separable, But if.",
                    "label": 0
                },
                {
                    "sent": "Significant fraction of it can be separated not only with a margin of 1, which is what SPM tries to do, but actually a little bit more than that.",
                    "label": 0
                },
                {
                    "sent": "Then you know very few examples will fall in this set.",
                    "label": 0
                },
                {
                    "sent": "You divide by the total number of examples in fees going to be small.",
                    "label": 0
                },
                {
                    "sent": "So here's the cartoon that shows this.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if our data looks like this and this is the clean data, this is before the bad guys.",
                    "label": 0
                },
                {
                    "sent": "Little do their thing.",
                    "label": 0
                },
                {
                    "sent": "So if the clean data has a distinctive you know set of negatives instead of positives and there's a big margin, not only one but one plus some gamma that we say specifically what is in the paper, then our algorithm has will do much better than this.",
                    "label": 0
                },
                {
                    "sent": "If the original data you know just look at the distribution, it's a difficult distribution to separate in the sense that a lot of the data falls close to this margin.",
                    "label": 0
                },
                {
                    "sent": "Then we're not going to do much better than a sphere.",
                    "label": 0
                },
                {
                    "sent": "So this is basically a result.",
                    "label": 0
                },
                {
                    "sent": "This setting basically says that the original problem was kind of easy and it only became difficult due to the fact that these malicious bad guys came and messed up the labels right?",
                    "label": 0
                },
                {
                    "sent": "So if there's a lot of signal in the original problem will be able to identify those guys are messing us up if the original thing is already a mess, then it'll be hard to distinguish between you know malicious and just the regular noise in the in the problem.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So those are our theorems.",
                    "label": 0
                },
                {
                    "sent": "We have quite a few experiments, almost done.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to give one, you know.",
                    "label": 0
                },
                {
                    "sent": "Most of the graphs look kind of the same, so we did various different experiments with Reuters corpus.",
                    "label": 0
                },
                {
                    "sent": "So we look at all possible binary classification problems with the high level categories of Reuters.",
                    "label": 0
                },
                {
                    "sent": "We took the data, we split it into 100 different teachers.",
                    "label": 0
                },
                {
                    "sent": "We did this randomly and then we varied the number of bad guys and then we simulated malicious features just by having flip the label.",
                    "label": 0
                },
                {
                    "sent": "So let's say you know how do you?",
                    "label": 0
                },
                {
                    "sent": "How do you even being malicious teacher?",
                    "label": 0
                },
                {
                    "sent": "That's kind of a difficult problem in itself, so we just flip the label of the bad guys and then this these graphs.",
                    "label": 0
                },
                {
                    "sent": "Basically, we repeated this multiple times.",
                    "label": 0
                },
                {
                    "sent": "That's why we have these foundation error bars and this is the ratio of the loss.",
                    "label": 0
                },
                {
                    "sent": "The test loss that our algorithm gets versus SVN.",
                    "label": 0
                },
                {
                    "sent": "So this is how much better we are than if then solo is good.",
                    "label": 0
                },
                {
                    "sent": "This is, you know we're a fraction of the loss of SPM.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we see that the minimal effect and sometimes it's a bigger effect and it gets.",
                    "label": 0
                },
                {
                    "sent": "You know our algorithm works better the bigger than.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to conclude, the take home message that I want to stay here is that you know this is kind of a proof of concept.",
                    "label": 0
                },
                {
                    "sent": "If you want to take anything back from this this talk that to clean data, you really don't need any of these hacks, even though there are great heck so you know, don't get me wrong, I use them all the time, but you don't really need them.",
                    "label": 0
                },
                {
                    "sent": "You don't need repeated labels, you don't need any prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "You don't need a truth set.",
                    "label": 0
                },
                {
                    "sent": "All you need to know is which subsets how to partition your data into subsets controlled by teachers.",
                    "label": 0
                },
                {
                    "sent": "So you know how to treat these, you know treaties.",
                    "label": 0
                },
                {
                    "sent": "As one quivalence class and this is a different equivalence class and then you can clean the data in the paper.",
                    "label": 0
                },
                {
                    "sent": "We have a second algorithm which I encourage you to read about and we have experiments that actually deviate from our theoretical setting where the partitioning into features is done by subtopics.",
                    "label": 0
                },
                {
                    "sent": "So rather than randomly partitioning the data, we say you know there's an expert on this topic and expert on this topic and with Reuters corpus, that's pretty convenient to do because it has, you know, multiple categories, so our algorithm empirically works in that setting as well.",
                    "label": 0
                },
                {
                    "sent": "And the last thing I want to say.",
                    "label": 0
                },
                {
                    "sent": "Is if you like this, you know topic of collecting labels from many, many teachers.",
                    "label": 0
                },
                {
                    "sent": "Then I also encourage you to come in, see or talk on Sunday afternoon which is that cold which deals with a very similar problem.",
                    "label": 1
                },
                {
                    "sent": "Only there we don't assume this good versus evil.",
                    "label": 0
                },
                {
                    "sent": "Very dramatic split of the world, but actually each teacher has some inherent noise parameter and we don't know what it is, so there's no bad guys.",
                    "label": 0
                },
                {
                    "sent": "They're just, you know, very incompetent teachers and very.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                }
            ]
        }
    }
}