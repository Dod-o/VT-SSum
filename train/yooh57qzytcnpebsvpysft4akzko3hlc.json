{
    "id": "yooh57qzytcnpebsvpysft4akzko3hlc",
    "title": "Nu-Support Vector Machine as Conditional Value-at-Risk Minimization",
    "info": {
        "author": [
            "Akiko Takeda, Dept. of Administration Engineering, Faculty of Science and Technology, Keio University"
        ],
        "published": "July 28, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/icml08_takeda_nsvm/",
    "segmentation": [
        [
            "OK, I in this talk I will talk about I show several nice properties for news about vector machines.",
            "This is the joint work with massive schema.",
            "So first I will overview several support vector machine."
        ],
        [
            "Models.",
            "So binary classification problem is to find a decision function using given training samples.",
            "In this talk will focus on this renewal function.",
            "So I know that the title of this session is kind of.",
            "But sorry I focus on this linear function in this case.",
            "This orange sample will have blue label, so this.",
            "The model I will show is extendable to nonlinear one using the kernel technique, but in this talk I focus on linear function and the paper in proceedings gives about.",
            "Kind of techniques extendable to our MoD."
        ],
        [
            "So this system matching formulation is called the SVC see support vector classification.",
            "This model has two conflicting.",
            "Goes like this, and the tradeoff between these goals is controlled by parameter C. But Caesar ladder unintuitive parameter, so a modification was proposed by them.",
            "Hey Fitch replaces C by intuitive parameter new."
        ],
        [
            "And they show that between these two support vector classification models there is equivalence exists like this and also they show that knew parameter new controls and number of margin errors and the number of support vectors.",
            "And so the parameter knew corresponds to fraction, so it should be able to be taken between zero and one, but admissible values of GNU OS."
        ],
        [
            "Dead.",
            "So they showed that I knew if when you is in this area and the optimal solutions of new SVC and see if we see the same.",
            "In this area, but if knew is in this area, new SBC has no optimal solution.",
            "And on the other hand, you when you is in this area, new SVC gives us.",
            "Trivial solution 00 vectors, so they showed two kinds of modifications.",
            "One is to extend the range up to one and the other one is to extend the range down to 0.",
            "This modification is very trivial, but this modification is far more complex and also this modification is very important.",
            "This slide modification is very important to attain a good generalization performance."
        ],
        [
            "So this is the numerical results of an database data and this access shows the value of knew Parimeter knew and this axis shows the error rate so smaller the better and the modification.",
            "To extend the range up to zero down to zero is done by in this paper and modified classification method is called in USA.",
            "See you find that the in you SVC good generalization performance here.",
            "So this modification is important to get a good."
        ],
        [
            "Diesel.",
            "And this is the formulation of extended GNU SVC.",
            "In short, in USB C. And note that this formulation has one non convex constraint here, and this constraint means that the norm of W should be one, so non convex constraint.",
            "And this formulation allows the margin to be negative for this range and then are not.",
            "I'm not trivial solution is obtained for this area.",
            "So they showed that intuitive Pyramid algorithm was proposed.",
            "They proposed an intuitive algorithm for computing local solution of this in USBC formulation.",
            "But the local optimality is not discussed well."
        ],
        [
            "All.",
            "These two issues are still not clear, so that goal of this talk is to give new theoretical insights into these two issues."
        ],
        [
            "So now.",
            "We we reviewed these three support vector classification models.",
            "So now you give knew interpretation for in USB C model in your space it could be interpreted as minimizing the conditional value at risk features risk measure used in finance and then we remove this topics next."
        ],
        [
            "So suppose that the number of the hyper hyper brain was given, then discovers for training samples are computed by this function.",
            "So negative signed distance function from the point to the hyperplane.",
            "And note that there correctly classified samples have negative risk scores.",
            "So in this example one samples 12345 have have negative risk scores, but fix N7 have negative risk scores.",
            "So by computing risk scores, we found two misclassified samples."
        ],
        [
            "Then next we consider how to find the normal vector of the hyperplane.",
            "We consider a histogram of risk scores for all training samples.",
            "And remind that the samples in this area have high risk of misclassification, so we regard the sample in this area as bad training samples.",
            "And we find a solution WNB which minimize the mean disk score over a set of bad training samples.",
            "So here I mean this score over bad training samples.",
            "So that to define that training samples, we use this threshold 100 beta passing type here.",
            "So the threshold part or not, is determined this passing time and determine this percentile.",
            "We need to choose a para meter value beta.",
            "OK. Then"
        ],
        [
            "No, we will we consider to minimize silver risk measure here.",
            "So to minimize.",
            "So they rocafella Anuria said show that these two problems are equivalent, so the lower program includes additional variable Alpha.",
            "The optimal value Alpha of this problem.",
            "This is corresponding to this percentile like this.",
            "So if we solve this problem, we can get a solution WNB of this problem.",
            "So we try to solve this civil minimization problem."
        ],
        [
            "And the problem is even minimization program results in this formulation.",
            "This formulation also includes this non convex constraint.",
            "The normal value is 1.",
            "So we show that in USB C is equivalent to silver minimization problem means even if we set parameters Dorian new like this.",
            "So in your space it can be interpreted as minimizing the mean disk score over a set of bad training samples.",
            "And the.",
            "This this bad training samples are corresponding to support vectors of India, SVC and also the margin of India.",
            "SVC is corresponding to this pattern type negative sign.",
            "Sold a negative margin.",
            "Negative margin means that the value of quantile is positive, so.",
            "If we this is the picture of the this negative margin.",
            "So it means that part of misclassified samples are used used as support vectors in this in USB C model."
        ],
        [
            "And.",
            "This is the formulation of in USB C. This formulation seems to be nonconvex problem, but as far as the optimal value is negative, this non convex program results in convex problem, which is equivalent to nufc formulation.",
            "But when the optimal value of India SVC is positive like this, then this problem is essentially non convex problem and we need to solve this problem using the nonconvex optimization technique.",
            "So."
        ],
        [
            "So now.",
            "We moved to this topics, minimizing the sea but improves generalization performance of classifiers."
        ],
        [
            "We show mu generalization error bounds for in SVC.",
            "They include the civil risk measure, so it meant that minimizing the Ciba Royster bound an.",
            "This result justify the use of any SVC, and also new SPC.",
            "So we analyzed generalization performance.",
            "Depending on the value of knew.",
            "And we consider these three cases where the zero position in this area, their position in this area and their opposition is in this area.",
            "3 cases we consider.",
            "And this case is corresponding to the convex case of in USB C. And remaining these two cases are corresponding to non convex case of in SEC."
        ],
        [
            "This is the generalization error bound for for the classifiers defining with feasible solution of GNU SVC.",
            "And you find that by minimizing this silver, this error bound is minimized.",
            "Soul Silver minimization gives some optimal solution which minimizes this bound, so this result supports that knew SVC is reasonable.",
            "And we get this elaborate and use combining these two theoretical results."
        ],
        [
            "So the these two 11 so for non convex cases of in USB C. So the difference between this iribarne and the previous one is just.",
            "Here.",
            "So this is a quantile of this distribution and civil risk measure does not appear in this bound, but civil risk measure is upper bound for quantile.",
            "So minimizing Silva, these two rolling quantile.",
            "So this bound is also lower.",
            "So in this case silver minimization is also important.",
            "An this third case on this case.",
            "If lower bound for generalization error lower bound, so it means that generalization error is great greater than or equal to this right hand side, so we're in silver is also important in this case."
        ],
        [
            "So now move this move to oh algorithm.",
            "I knew if we show and you efficient optimization procedure for in USB C."
        ],
        [
            "So the proposed algorithm consists of two steps.",
            "At first we solve knew SVC convex problem.",
            "And if the optimal value is negative terminate, it means that we can get optimal solution of in USBC nonconvex problem by solving convex problem USB C. But if the optimal value is equal to 0.",
            "This means that the in USB C is essentially non convex, so we try to find a global global optimum of in USB C using global optimization technique which consists of rock optimal search and cutting from method."
        ],
        [
            "And that's the case.",
            "I duration of workout.",
            "My algorithm.",
            "We solve this linear program which is constructed by linearizing this non convex constraint by linear long.",
            "And.",
            "Suppose that this part of sphere is corresponding to the feasible set up in USB C, and this is the initial point an this initial point, we linearize this quadratic surface like this.",
            "And solving this linear program whose feasible set is this running an we get down optimal solution here and project it into the surface an linear linearized.",
            "Linearisation line is linearized, line is updated.",
            "So we get these three points and their optimal value objective function value goes down and down.",
            "It ensures that the finite convergence.",
            "And also we can show rogue optimality of this algorithm from Katie optimality condition of Elf."
        ],
        [
            "He so we have a good properties like this for log optimization algorithm, But the algorithm we proposed is has little difference.",
            "With the new SVC algorithm of them.",
            "So difference is only here linear lies surface for LP is updated at this point or this point, but use if we use this update rule.",
            "Finite convergence may not be guaranteed."
        ],
        [
            "Anne.",
            "How so?",
            "After finding a local solution, we now allow the feasible set of Aeneas we see to remove local solution and its neighborhood by adding concavity cut or facial cut this cut it.",
            "To cut off a corner of the, for example, cut off, cut it off like this and facial cut removes face of the.",
            "So if we get this point then cut off.",
            "Like this, so remaining feasible set is this area.",
            "So we can show that global solution of this program is a corner of the.",
            "So already the~ domaining set has no global solution in this area, so.",
            "We add cats to in yes, we see until detailed includes no faces of the.",
            "So the number of faces of D is limited, so we can show finite convergence property for this algorithm."
        ],
        [
            "But the finite at all convergence can be shown, but the new fast.",
            "For example.",
            "All this rain shows the value of knew and also test error.",
            "This one is test error and this orange.",
            "Our area shows that the problem is essentially convex.",
            "An if we solve this in USBC problem with GNU equal to 0.7 zero point 0 zero point 6 five we can get global solutions.",
            "After adding such number of cuts.",
            "So we can we could find global global solution in this case.",
            "But when you become smaller and smaller, it's difficult because the number of necessary cuts increased rapidly.",
            "So not first, but we found that these issues there are only few local solutions and in SVC with small knew requires many cuts and the fast local solution is sufficiently good together.",
            "Estimation, good estimation."
        ],
        [
            "So that's all this is the conclusion.",
            "So thank you very much.",
            "I don't have time for questions.",
            "I have one that's sort of kind of related, which is that here you're talking about the support vector machine would be this conditional value at risk analysis.",
            "Would this also be helpful in this sort of the simpler case of logistic regression, which is kind of related to support vector?",
            "Yes, I also analyzed regression models and I can show similar properties for.",
            "News support vector regression model.",
            "OK, that's for this.",
            "Pulled back to like new support vector regression.",
            "But other models like say just simple logistic regression.",
            "So replacing the hinge loss with smooth version with those with this analysis be useful in those cases.",
            "I don't try yet, so I don't know there is a similar kind of technique is applicable to.",
            "The integration model.",
            "So I will try from now, thank you.",
            "Other questions.",
            "OK then let's thank the speaker again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, I in this talk I will talk about I show several nice properties for news about vector machines.",
                    "label": 0
                },
                {
                    "sent": "This is the joint work with massive schema.",
                    "label": 0
                },
                {
                    "sent": "So first I will overview several support vector machine.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Models.",
                    "label": 0
                },
                {
                    "sent": "So binary classification problem is to find a decision function using given training samples.",
                    "label": 1
                },
                {
                    "sent": "In this talk will focus on this renewal function.",
                    "label": 0
                },
                {
                    "sent": "So I know that the title of this session is kind of.",
                    "label": 0
                },
                {
                    "sent": "But sorry I focus on this linear function in this case.",
                    "label": 0
                },
                {
                    "sent": "This orange sample will have blue label, so this.",
                    "label": 0
                },
                {
                    "sent": "The model I will show is extendable to nonlinear one using the kernel technique, but in this talk I focus on linear function and the paper in proceedings gives about.",
                    "label": 1
                },
                {
                    "sent": "Kind of techniques extendable to our MoD.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this system matching formulation is called the SVC see support vector classification.",
                    "label": 0
                },
                {
                    "sent": "This model has two conflicting.",
                    "label": 1
                },
                {
                    "sent": "Goes like this, and the tradeoff between these goals is controlled by parameter C. But Caesar ladder unintuitive parameter, so a modification was proposed by them.",
                    "label": 1
                },
                {
                    "sent": "Hey Fitch replaces C by intuitive parameter new.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And they show that between these two support vector classification models there is equivalence exists like this and also they show that knew parameter new controls and number of margin errors and the number of support vectors.",
                    "label": 0
                },
                {
                    "sent": "And so the parameter knew corresponds to fraction, so it should be able to be taken between zero and one, but admissible values of GNU OS.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Dead.",
                    "label": 0
                },
                {
                    "sent": "So they showed that I knew if when you is in this area and the optimal solutions of new SVC and see if we see the same.",
                    "label": 1
                },
                {
                    "sent": "In this area, but if knew is in this area, new SBC has no optimal solution.",
                    "label": 0
                },
                {
                    "sent": "And on the other hand, you when you is in this area, new SVC gives us.",
                    "label": 0
                },
                {
                    "sent": "Trivial solution 00 vectors, so they showed two kinds of modifications.",
                    "label": 0
                },
                {
                    "sent": "One is to extend the range up to one and the other one is to extend the range down to 0.",
                    "label": 1
                },
                {
                    "sent": "This modification is very trivial, but this modification is far more complex and also this modification is very important.",
                    "label": 0
                },
                {
                    "sent": "This slide modification is very important to attain a good generalization performance.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the numerical results of an database data and this access shows the value of knew Parimeter knew and this axis shows the error rate so smaller the better and the modification.",
                    "label": 0
                },
                {
                    "sent": "To extend the range up to zero down to zero is done by in this paper and modified classification method is called in USA.",
                    "label": 0
                },
                {
                    "sent": "See you find that the in you SVC good generalization performance here.",
                    "label": 0
                },
                {
                    "sent": "So this modification is important to get a good.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Diesel.",
                    "label": 0
                },
                {
                    "sent": "And this is the formulation of extended GNU SVC.",
                    "label": 0
                },
                {
                    "sent": "In short, in USB C. And note that this formulation has one non convex constraint here, and this constraint means that the norm of W should be one, so non convex constraint.",
                    "label": 0
                },
                {
                    "sent": "And this formulation allows the margin to be negative for this range and then are not.",
                    "label": 1
                },
                {
                    "sent": "I'm not trivial solution is obtained for this area.",
                    "label": 1
                },
                {
                    "sent": "So they showed that intuitive Pyramid algorithm was proposed.",
                    "label": 0
                },
                {
                    "sent": "They proposed an intuitive algorithm for computing local solution of this in USBC formulation.",
                    "label": 0
                },
                {
                    "sent": "But the local optimality is not discussed well.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All.",
                    "label": 0
                },
                {
                    "sent": "These two issues are still not clear, so that goal of this talk is to give new theoretical insights into these two issues.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "We we reviewed these three support vector classification models.",
                    "label": 0
                },
                {
                    "sent": "So now you give knew interpretation for in USB C model in your space it could be interpreted as minimizing the conditional value at risk features risk measure used in finance and then we remove this topics next.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So suppose that the number of the hyper hyper brain was given, then discovers for training samples are computed by this function.",
                    "label": 1
                },
                {
                    "sent": "So negative signed distance function from the point to the hyperplane.",
                    "label": 1
                },
                {
                    "sent": "And note that there correctly classified samples have negative risk scores.",
                    "label": 0
                },
                {
                    "sent": "So in this example one samples 12345 have have negative risk scores, but fix N7 have negative risk scores.",
                    "label": 0
                },
                {
                    "sent": "So by computing risk scores, we found two misclassified samples.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then next we consider how to find the normal vector of the hyperplane.",
                    "label": 1
                },
                {
                    "sent": "We consider a histogram of risk scores for all training samples.",
                    "label": 1
                },
                {
                    "sent": "And remind that the samples in this area have high risk of misclassification, so we regard the sample in this area as bad training samples.",
                    "label": 0
                },
                {
                    "sent": "And we find a solution WNB which minimize the mean disk score over a set of bad training samples.",
                    "label": 1
                },
                {
                    "sent": "So here I mean this score over bad training samples.",
                    "label": 0
                },
                {
                    "sent": "So that to define that training samples, we use this threshold 100 beta passing type here.",
                    "label": 0
                },
                {
                    "sent": "So the threshold part or not, is determined this passing time and determine this percentile.",
                    "label": 0
                },
                {
                    "sent": "We need to choose a para meter value beta.",
                    "label": 0
                },
                {
                    "sent": "OK. Then",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, we will we consider to minimize silver risk measure here.",
                    "label": 1
                },
                {
                    "sent": "So to minimize.",
                    "label": 0
                },
                {
                    "sent": "So they rocafella Anuria said show that these two problems are equivalent, so the lower program includes additional variable Alpha.",
                    "label": 0
                },
                {
                    "sent": "The optimal value Alpha of this problem.",
                    "label": 0
                },
                {
                    "sent": "This is corresponding to this percentile like this.",
                    "label": 0
                },
                {
                    "sent": "So if we solve this problem, we can get a solution WNB of this problem.",
                    "label": 0
                },
                {
                    "sent": "So we try to solve this civil minimization problem.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the problem is even minimization program results in this formulation.",
                    "label": 0
                },
                {
                    "sent": "This formulation also includes this non convex constraint.",
                    "label": 0
                },
                {
                    "sent": "The normal value is 1.",
                    "label": 0
                },
                {
                    "sent": "So we show that in USB C is equivalent to silver minimization problem means even if we set parameters Dorian new like this.",
                    "label": 1
                },
                {
                    "sent": "So in your space it can be interpreted as minimizing the mean disk score over a set of bad training samples.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                },
                {
                    "sent": "This this bad training samples are corresponding to support vectors of India, SVC and also the margin of India.",
                    "label": 0
                },
                {
                    "sent": "SVC is corresponding to this pattern type negative sign.",
                    "label": 0
                },
                {
                    "sent": "Sold a negative margin.",
                    "label": 1
                },
                {
                    "sent": "Negative margin means that the value of quantile is positive, so.",
                    "label": 0
                },
                {
                    "sent": "If we this is the picture of the this negative margin.",
                    "label": 0
                },
                {
                    "sent": "So it means that part of misclassified samples are used used as support vectors in this in USB C model.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "This is the formulation of in USB C. This formulation seems to be nonconvex problem, but as far as the optimal value is negative, this non convex program results in convex problem, which is equivalent to nufc formulation.",
                    "label": 0
                },
                {
                    "sent": "But when the optimal value of India SVC is positive like this, then this problem is essentially non convex problem and we need to solve this problem using the nonconvex optimization technique.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now.",
                    "label": 0
                },
                {
                    "sent": "We moved to this topics, minimizing the sea but improves generalization performance of classifiers.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We show mu generalization error bounds for in SVC.",
                    "label": 1
                },
                {
                    "sent": "They include the civil risk measure, so it meant that minimizing the Ciba Royster bound an.",
                    "label": 1
                },
                {
                    "sent": "This result justify the use of any SVC, and also new SPC.",
                    "label": 0
                },
                {
                    "sent": "So we analyzed generalization performance.",
                    "label": 0
                },
                {
                    "sent": "Depending on the value of knew.",
                    "label": 0
                },
                {
                    "sent": "And we consider these three cases where the zero position in this area, their position in this area and their opposition is in this area.",
                    "label": 0
                },
                {
                    "sent": "3 cases we consider.",
                    "label": 0
                },
                {
                    "sent": "And this case is corresponding to the convex case of in USB C. And remaining these two cases are corresponding to non convex case of in SEC.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the generalization error bound for for the classifiers defining with feasible solution of GNU SVC.",
                    "label": 1
                },
                {
                    "sent": "And you find that by minimizing this silver, this error bound is minimized.",
                    "label": 0
                },
                {
                    "sent": "Soul Silver minimization gives some optimal solution which minimizes this bound, so this result supports that knew SVC is reasonable.",
                    "label": 1
                },
                {
                    "sent": "And we get this elaborate and use combining these two theoretical results.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the these two 11 so for non convex cases of in USB C. So the difference between this iribarne and the previous one is just.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "So this is a quantile of this distribution and civil risk measure does not appear in this bound, but civil risk measure is upper bound for quantile.",
                    "label": 0
                },
                {
                    "sent": "So minimizing Silva, these two rolling quantile.",
                    "label": 0
                },
                {
                    "sent": "So this bound is also lower.",
                    "label": 0
                },
                {
                    "sent": "So in this case silver minimization is also important.",
                    "label": 0
                },
                {
                    "sent": "An this third case on this case.",
                    "label": 0
                },
                {
                    "sent": "If lower bound for generalization error lower bound, so it means that generalization error is great greater than or equal to this right hand side, so we're in silver is also important in this case.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now move this move to oh algorithm.",
                    "label": 0
                },
                {
                    "sent": "I knew if we show and you efficient optimization procedure for in USB C.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the proposed algorithm consists of two steps.",
                    "label": 0
                },
                {
                    "sent": "At first we solve knew SVC convex problem.",
                    "label": 0
                },
                {
                    "sent": "And if the optimal value is negative terminate, it means that we can get optimal solution of in USBC nonconvex problem by solving convex problem USB C. But if the optimal value is equal to 0.",
                    "label": 0
                },
                {
                    "sent": "This means that the in USB C is essentially non convex, so we try to find a global global optimum of in USB C using global optimization technique which consists of rock optimal search and cutting from method.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's the case.",
                    "label": 0
                },
                {
                    "sent": "I duration of workout.",
                    "label": 0
                },
                {
                    "sent": "My algorithm.",
                    "label": 0
                },
                {
                    "sent": "We solve this linear program which is constructed by linearizing this non convex constraint by linear long.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Suppose that this part of sphere is corresponding to the feasible set up in USB C, and this is the initial point an this initial point, we linearize this quadratic surface like this.",
                    "label": 0
                },
                {
                    "sent": "And solving this linear program whose feasible set is this running an we get down optimal solution here and project it into the surface an linear linearized.",
                    "label": 0
                },
                {
                    "sent": "Linearisation line is linearized, line is updated.",
                    "label": 0
                },
                {
                    "sent": "So we get these three points and their optimal value objective function value goes down and down.",
                    "label": 0
                },
                {
                    "sent": "It ensures that the finite convergence.",
                    "label": 1
                },
                {
                    "sent": "And also we can show rogue optimality of this algorithm from Katie optimality condition of Elf.",
                    "label": 1
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "He so we have a good properties like this for log optimization algorithm, But the algorithm we proposed is has little difference.",
                    "label": 1
                },
                {
                    "sent": "With the new SVC algorithm of them.",
                    "label": 1
                },
                {
                    "sent": "So difference is only here linear lies surface for LP is updated at this point or this point, but use if we use this update rule.",
                    "label": 0
                },
                {
                    "sent": "Finite convergence may not be guaranteed.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "How so?",
                    "label": 0
                },
                {
                    "sent": "After finding a local solution, we now allow the feasible set of Aeneas we see to remove local solution and its neighborhood by adding concavity cut or facial cut this cut it.",
                    "label": 1
                },
                {
                    "sent": "To cut off a corner of the, for example, cut off, cut it off like this and facial cut removes face of the.",
                    "label": 0
                },
                {
                    "sent": "So if we get this point then cut off.",
                    "label": 0
                },
                {
                    "sent": "Like this, so remaining feasible set is this area.",
                    "label": 0
                },
                {
                    "sent": "So we can show that global solution of this program is a corner of the.",
                    "label": 0
                },
                {
                    "sent": "So already the~ domaining set has no global solution in this area, so.",
                    "label": 1
                },
                {
                    "sent": "We add cats to in yes, we see until detailed includes no faces of the.",
                    "label": 0
                },
                {
                    "sent": "So the number of faces of D is limited, so we can show finite convergence property for this algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the finite at all convergence can be shown, but the new fast.",
                    "label": 0
                },
                {
                    "sent": "For example.",
                    "label": 0
                },
                {
                    "sent": "All this rain shows the value of knew and also test error.",
                    "label": 0
                },
                {
                    "sent": "This one is test error and this orange.",
                    "label": 0
                },
                {
                    "sent": "Our area shows that the problem is essentially convex.",
                    "label": 0
                },
                {
                    "sent": "An if we solve this in USBC problem with GNU equal to 0.7 zero point 0 zero point 6 five we can get global solutions.",
                    "label": 0
                },
                {
                    "sent": "After adding such number of cuts.",
                    "label": 1
                },
                {
                    "sent": "So we can we could find global global solution in this case.",
                    "label": 0
                },
                {
                    "sent": "But when you become smaller and smaller, it's difficult because the number of necessary cuts increased rapidly.",
                    "label": 0
                },
                {
                    "sent": "So not first, but we found that these issues there are only few local solutions and in SVC with small knew requires many cuts and the fast local solution is sufficiently good together.",
                    "label": 1
                },
                {
                    "sent": "Estimation, good estimation.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's all this is the conclusion.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much.",
                    "label": 0
                },
                {
                    "sent": "I don't have time for questions.",
                    "label": 0
                },
                {
                    "sent": "I have one that's sort of kind of related, which is that here you're talking about the support vector machine would be this conditional value at risk analysis.",
                    "label": 0
                },
                {
                    "sent": "Would this also be helpful in this sort of the simpler case of logistic regression, which is kind of related to support vector?",
                    "label": 0
                },
                {
                    "sent": "Yes, I also analyzed regression models and I can show similar properties for.",
                    "label": 0
                },
                {
                    "sent": "News support vector regression model.",
                    "label": 0
                },
                {
                    "sent": "OK, that's for this.",
                    "label": 0
                },
                {
                    "sent": "Pulled back to like new support vector regression.",
                    "label": 0
                },
                {
                    "sent": "But other models like say just simple logistic regression.",
                    "label": 0
                },
                {
                    "sent": "So replacing the hinge loss with smooth version with those with this analysis be useful in those cases.",
                    "label": 0
                },
                {
                    "sent": "I don't try yet, so I don't know there is a similar kind of technique is applicable to.",
                    "label": 0
                },
                {
                    "sent": "The integration model.",
                    "label": 0
                },
                {
                    "sent": "So I will try from now, thank you.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "OK then let's thank the speaker again.",
                    "label": 0
                }
            ]
        }
    }
}