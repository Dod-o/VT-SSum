{
    "id": "5l4md5qctc3u7j53wme4vxjpjef7yxzk",
    "title": "Information Retrieval",
    "info": {
        "author": [
            "Thomas Hofmann, Google, Inc."
        ],
        "published": "Nov. 2, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Information Retrieval"
        ]
    },
    "url": "http://videolectures.net/mlss09uk_hofmann_ir/",
    "segmentation": [
        [
            "OK, good morning.",
            "Welcome to the second lecture of the day.",
            "My name is Thomas Hoffman.",
            "I work currently at Google in Switzerland.",
            "I think in the program it says Google Research, but that's not quite true.",
            "I'm actually director in an engineering center and while I think we have interesting research problems and also some I want to mention here, my work these days is really more about building systems and working with engineering teams to actually putting things in production.",
            "And this is what I've been doing the last three years.",
            "So some of the work I will present here sort of predates my Google time work I did on structured prediction and then I'll cover that and some recent work that other people have done in this area.",
            "As you will see, hopefully what I'll talk about towards the end when I'll discuss.",
            "So the first part of the lecture this morning will mainly be about the methodology and the formal setting, and then in the afternoon I will talk more about applications that has.",
            "Most of the applications will be in information retrieval, so you then you might imagine sort of some of the connections between working at Google and working on this specific problem.",
            "We might have in the end enough time to either, you know, finish a little bit earlier, or if you have questions about Google and things in general, use of machine learning in an industrial setting.",
            "I'm also happy to answer those.",
            "So let me begin.",
            "Feel free to interrupt me at anytime so."
        ],
        [
            "Of course we know."
        ],
        [
            "Just to wake up after the break.",
            "That supervised machine learning, you know is a is a wonderful thing to have where we can learn functional dependencies between inputs and outputs.",
            "And we can do this based on training data by using inductive inference and its most basic form.",
            "Of course we are facing classification problems and you know you've seen probably many examples here or you know many examples in your work classification problems such as optical character recognition, where we're given, you know, given an image of a character, map it to one of the possible digits or characters and things.",
            "For instance in the context of information retrieval, probably the most important application.",
            "Things like document correct, document categorization where given a particular document represented just by the text or some additional features or semi structured data metadata that we might have to map it to certain categories.",
            "So here's an example from Reuters where we might map it to things like a location code and to certain provide some topical annotations, for instance that this document is on money markets, foreign exchange markets and so on and so forth."
        ],
        [
            "Of course, in document correctores categorization.",
            "You know just to contrast it with other things you might other approaches that you might see the advantage of machine learning is that we can basically harness the knowledge that humans have, like an expert like this one here on labeling documents, given a particular classification system or particular taxonomy that people might be interested in and then use these training data to feed it into a mechanism, a learning machine of some sort.",
            "To then actually output using an inference mechanism, a complicated algorithm.",
            "Now that in practice has turned out to be more efficient than, let's say, the traditional approaches that you've seen in this area.",
            "For instance, that have been based on what's called knowledge engineering, where people have tried to design like handcrafted rules of how to classify things right so?",
            "It's interesting to note that, right, it's not clear our priority that for particular problem machine learning is always the best technique to use, but in text categorization actually has been quite successful and much more successful than other methods based on knowledge engineering.",
            "The advantage of machine learning is pretty clear, whereas knowledge engineering, right, the knowledge that resides within an expert to get that out and to get it formalized right, is a very difficult task, very hard to achieve, good coverage, very.",
            "Very hard to achieve good precision.",
            "The indirect approach through actually creating training examples has the advantage that really, you know we can just let the expert do what the expert is good at, namely categorizing things, using whatever knowledge he has, and we don't have to worry about the formalization, but rather than we can use an inference mechanism to come back to get really something that generalizes to new data.",
            "So I should also say sorry, I should also say that.",
            "Text categorization I'm not going to talk much about the plain text categorization example, but has probably been one of the most successful applications of machine learning in practice.",
            "And if you look at today's systems, they actually do reach human level accuracy in the categorization performance that you see over realistic taxonomies.",
            "Often what you notice that the precision of a human annotation might be higher in the sense that if a human says it belongs to that class, it's more reliable typically than ever machine makes the prediction, but what machines are much better at is the recall aspect of.",
            "If I take a document order all the annotations that the document should get.",
            "If you think about huge taxonomies with many many classes, humans are sometimes likely to basically omit certain categories because they don't think about it right, which is fairly natural.",
            "If you have a huge category systems.",
            "Machines are usually better at that part, which you could think of as a recall."
        ],
        [
            "Sorry.",
            "OK, but this is not what I would like to talk about here.",
            "Mainly I would like to talk about.",
            "Problem called structured classification or structured prediction.",
            "So by that, what I what I mean is really a setting where what we are trying to predict is not a class variable, binary variable or not even a multiclass problem, but it's something more interesting.",
            "So to give you an idea of what we will be talking about here and many more examples will follow, you can think about all kinds of structured objects such as sequences, strings, trees and so on and so forth.",
            "And what if you have machine learning problem, a prediction problem where this is really the output that you want to generate?",
            "How do you go about use?",
            "You know, standard machine learning technology to solve these problems, right?",
            "Most of the literature is probably on, you know, binary classification or regression or or perhaps multiclass classification.",
            "But a lot of problems out there you have more complicated prediction problems.",
            "The second class of problems is where the outputs are not structures in the strict sense of a tree, a graph, a sequence.",
            "But where you have multiple classification problems that are coupled together OK, so we will also see many examples of those where really there are multiple response variables and the response variables are all interdependent.",
            "OK, so you don't want to just predict individually, right?",
            "For each example, be the document or beaten image or what it is right?",
            "You don't want to independently predict a label, but really you have a large set of labels that are that are all coupled somehow.",
            "And so then the question is right, how can we extend machine learning techniques to deal with this collective classification problem in a way that we're better than just independently predicting individual respons variables?",
            "So here."
        ],
        [
            "Some motivating examples to start this so you get an idea of what types of problems you will be able to tackle with these methods, so we're not going to talk about really the full application here.",
            "That's more towards the end, but this is more of a motivation of the types of things I'm I'm talking about here.",
            "So if you do things like, for instance, part of speech tagging right, then what you want to do is you want to take a sentence like the sentence up there.",
            "Profits soared at Boeing blah blah blah, so this should be your input X.",
            "And what you would like to do is to annotate every word in that sentence here with a particular label from a set of part of speech tags.",
            "So N would be a noun, be would be a verb, P would be a preposition.",
            "I guess a possessive adjective.",
            "And so on and so forth.",
            "So you see, the output is the red part here, right?",
            "Is the augmented input by these tags.",
            "And why is this useful?",
            "It's often useful, for instance, as a first step in a syntactic analysis of a sentence when you know maybe at some point you want to parse the sentence.",
            "Often part of speech tagging is the first thing to get there.",
            "OK, so you can see why.",
            "Why is it not simply a multiclass or binary classification problem?",
            "It's because these things will be coupled right?",
            "So, for instance, whether you label you know certain things here, let's say as nouns or verbs might have an impact.",
            "On what neighboring labels should be right?",
            "Because maybe certain sequences of labels are more likely certain, you know and annotations on the whole sentence level are more likely than others, right?",
            "So you would like to take that into account, right?",
            "And how can you do that?"
        ],
        [
            "So the second problem is related, but is a bit different, so I mentioned it here independently.",
            "Is a problem known as information extraction, where this is really only.",
            "Like a broader title for for a whole set of different problems.",
            "So the idea here is that we are not so much interested in tags for individual words about what their part of speech role is, but what we're interested in is really to mock up a document with named entities that are mentioned in the text and also to classify these mentionings according to a particular set of types of entities that you might that you might want to define.",
            "So, for instance, an organization.",
            "Country a personal location.",
            "And various other entities that types that you might want to define.",
            "So ultimately what you would like to get it is if I have a paragraph like this, right?",
            "I want to know that the WTO, for instance, is an organization that the United States are a country that Richard Aboulafia is a person name.",
            "And so on and so forth, right?",
            "And what I want to do is really I want to add, let's say, one way of encoding this right.",
            "I want to add text that.",
            "Identify segments of consecutive tokens and give them a label that this is a person name or this is an organization name and so on and so forth, right?",
            "Like like seen here.",
            "And again, in doing that, right things will not be completely independent.",
            "In particular, if you if you look at neighboring things and you know the segmentation, since this is a segmentation problem, if you like for a sequence of words, you need to look at the overall segmentation as a whole in order to do this already.",
            "Sort of, you know, to identify a segment or variable length, right goes beyond like a simple independent classification problem.",
            "OK, so the challenge is here to predict the labeled."
        ],
        [
            "Temptation.",
            "OK, and then taking it step further in terms of complexity of structures and also the type of problems we might want to look at.",
            "Is here's an example from natural language parsing, where the input is again a sentence.",
            "What's in blue here?",
            "And what you want to predict is apostrophe, like this.",
            "So let's say you might be given a context free grammar of some sort, right?",
            "That tells you about you know what the types of constituents are that you can find here.",
            "So NP, for instance, that there are things like noun phrases and verb phrases, and you're given a little bit about you know how the set of production rules basically on how these nonterminals can be translated into, you know.",
            "Other sequences of non terminals and then ultimately into a sequence of terminals, which will be our sentence.",
            "But basically what you want to do is you want to parse the sentence and identify correct and among the.",
            "Usually you know large number of possible parse trees.",
            "Identify the most likely the most plausible one that gives the correct interpretation to this set of English.",
            "Let's say right so and the question here is of course traditionally if you look at this right in the.",
            "90s people have started using probabilistic models so they used PCF dogs.",
            "They lexicalized alot of the rules, and basically they're typically would use generative models and lowering the parameters of the generative model by using some maximum likelihood with the right level of smoothing.",
            "And more recently, what will be and this is also what we will be looking at here is how can you use more discriminative learning methods to actually infer parse trees so to do natural language parsing?",
            "OK, Ann."
        ],
        [
            "Then I won't really talk much about this, but here is a reference, like a paper that's now a couple of years old.",
            "Also, if you think about recognition of handwritten words, you have a situation where really you know you don't want to just segment the image and then predict individually what the characters are that you see in these pixel images, but rather you want to take dependencies into account.",
            "So probably on the word level at least.",
            "So that, for instance, what you see here, you know that you not mistakenly taken that for an E let's say, but rather for C, like in causes, right?",
            "Because this is more likely.",
            "So you have a lot of.",
            "Examples like that where there's a lot of local ambiguities, so if you will make a decision locali right, you might get the wrong result, but you can help.",
            "You can use the context of what you see to the right or to the left or what you think you see, right?",
            "So how you classify these characters as a guiding you know, as as additional help as it providing additional evidence for.",
            "For for recognizing these characters.",
            "OK, so."
        ],
        [
            "So.",
            "And then finally, I just like to mention mention this example here and like a typical group of people that has worked on this is inbuilt Freeman's Group which has to do with problems in computer vision where traditionally people have often looked at things like object recognition or image annotation and have really thought thought of that as like sort of an independent problem so I. I look for a particular object in the image, maybe a search over certain windows and regions in the image, and then I have some function that tells me whether there's enough evidence that we actually see a monitor somewhere in the image, let's say, and then the prediction would be OK.",
            "This is an image that contains screen, computer monitor, but what's interesting is also to actually understand that their dependencies between the different things that we see in an image.",
            "So if I you know this is a computer monitor, maybe it makes it more likely that there's a keyboard and a mouse nearby.",
            "Right, so I shouldn't make these decisions independently, but rather predictor whole scene graph if you like or recognize objects in the context, and then hopefully that will help with."
        ],
        [
            "The recognition accuracy.",
            "OK sorry I have one more example.",
            "Forgot about this one, so this is also relating to work.",
            "There's a reference here and then.",
            "Some work that trust in your hands has been doing over the last couple of years and this is a problem where the input from computational biology where the input is a representation of a protein.",
            "So here it's a primary sequence, so it will be just a string of letters.",
            "Every letter will correspond to an amino acid.",
            "And what we would like to do is in the case of secondary structure prediction, we want to identify segments that correspond to certain types of secondary protein structure like certain type of a Helix or sheet or things like that.",
            "So there's usually a very small alphabet that biologists are happy with and basically what we need to know is which segment on this string actually corresponds to a particular structure and then also what people are interested in.",
            "And that's if you want to take that a step further.",
            "Is to predict something like a binary contact matrix?",
            "OK, so think of this string.",
            "Each position here corresponds to a particular column and also row and what the binary context matrix would actually tell us is whether two amino acids are within a certain radius of each other after the folding process of the protein, right?",
            "So it would capture some of the 3D structure of the protein, and I'm not an expert in computational biology, but from what I know is if.",
            "If one could accurately predict the binary contact matrix, that would actually take take us a very long way and actually making a prediction of protein structure, right?",
            "So it's interesting to think about how can machine learning methods be used to start from, let's say, the primary sequence and then start to predict actually this contact matrix, right?",
            "Then clearly the binary entries in this contract matrix will not be independent, right?",
            "But will be there will be very strong dependencies between these things, right?",
            "Because?",
            "There's only a small number of contact matrices, for instance, they really make sense because you know not everything can be in contact with everything else at the same time.",
            "OK, so these are the types of problems that I would like to present a solution to here in a generic sense, I'm not going to solve all these problems.",
            "You know, as a serious application, but I want to show you how with a few ideas for machine learning.",
            "You can actually have a nice toolkit of things that you can that you can then use.",
            "For many of these things OK.",
            "So are there any questions so far?",
            "There wasn't much content, more motivation, but.",
            "Does that give you a sense of?",
            "Or problems.",
            "OK, so the first thing I'd like to do then is talk about multiclass classification."
        ],
        [
            "OK, because that's a natural way to get started towards more complicated things, right?",
            "Because on one hand right?",
            "If you think about like predicting large label sequences and things like that right, you can think of at least in a naive way as as a problem over a very large output space of different, you know, just a very large number of possible outputs, right?",
            "Namely all possible labeled sequences, let's say over a certain length.",
            "Now you can think of that as just being a gigantic multiclass problem, so obviously you need some more ideas to transform it back into something tractable in something that can be learned, but so it's good to start with."
        ],
        [
            "With multi class and introduce appear ideas OK?",
            "So let's see what we can do on the output side.",
            "If we have a case where we have K class.",
            "So I'll basically present one particular approach here of setting this up as a problem that then you can solve by a variant of a perceptron algorithm or a support vector machine.",
            "There are a number of different formulations, I'll just use one here because I'm not primarily interested in the multiclass classification aspect, but it's only two as a motivation too.",
            "To go beyond that, OK, so here's the general setting and also notation that I will use later, so I assume there's an input SpaceX usually that can be a subspace of, you know, a certain set of subsets of R2D, let's say finite dimensional representation, or it can be something more complicated, and we have an output space Y, and in this output space really consists of K labels, where this number K is just the number of classes or categories that we have.",
            "And the simplest model that we could look at if we look at linear classifiers is to take the binary case and generalize it in the way that we actually get now, a weight vector for each of the classes.",
            "OK, so if I pick a particular class Yi, associate a weight vector of the same dimension as our inputs with that specific class, and then I can define a discriminate function that assigns a score to a particular input that I can plug in here.",
            "And the particular class that function is indexed by that class label Y by just, you know the inner product between that weight vector and some representation that I might choose for my inputs, right?",
            "So this is just a generalization of of binary classification, where in binary classification usually you wouldn't even have two weight vectors, but rather you will look at the sign of this right and make decision based on that.",
            "So for convenience, I will also think of that function as a function of two arguments X&Y input and the output.",
            "Now, how would we then predict target class Y star?",
            "Well, we would just define why star forgiven X to be the argmax over all possible classes well, possible output of that function F of X, Y, and if we just plug it in, it just means we just get that right?",
            "So we just pick the score.",
            "Sorry, the class which gets the highest score and the score is defined by a weight vector that we have associated with this class, right?",
            "So it's relatively simple."
        ],
        [
            "If we look at this geometrically, what we do is we partition the input space with hyperplanes and if we look at the boundary between two classes right, we can ask ourselves well, ignoring all the other classes for a minute, right?",
            "When would be favor a class.",
            "Why or where class?",
            "Why prime?",
            "Well, we would do this if this inner product between WY with whatever input representation we have exceeds.",
            "The same inner product with a different weight vector, namely the 1 four Y prime and we can just subtract this, bring it to the left side, use the linearity of the inner product and then that's what we get, namely that we should look at the difference of the weight vector right and check the sign basically of the inner product of that and if that sign is positive then we favor Y if the sign is negative, we favor why prime?",
            "If it's zero, it's basically the boundary between the two.",
            "OK, now the region that is the region.",
            "Where we label inputs by a particular label Y right will just be the intersection of all of these half spaces, right where we prefer.",
            "Prefer why over some why prime, not just taking Y prime to be everything right?",
            "Except why itself?",
            "So this will then define some convex region can also be.",
            "Yeah, so you can visualize it like that and that will be basically the region where you label something with a label Y.",
            "So all the decision boundaries will be linear and you can also generalize this by introducing these bias terms you know be sub Y for each class right?",
            "So then you would also get like an offset that's class specific.",
            "OK, So what is then the?"
        ],
        [
            "Samples learning algorithm that you could think of.",
            "Just just using that linear representation.",
            "Of the these linear discriminant functions that I've just described, so the simplest approach would be something like a perceptron learning approach, where you cycle through your data.",
            "So now your data is our pairs XY, XY being the input, and why I being one of these K possible class labels.",
            "You use what I've shown you before to predict the optimal label as the one that gets the maximum score, so call that why I star and then you check whether that prediction is correct, whether it corresponds to your training label and as you do in the perceptron.",
            "If it is correct, then you do nothing, and if it is incorrect then you perform an update and the way we could define the updates.",
            "There are a number of ways of doing this, but one way you could do it with just kind of the simplest thing.",
            "Conceptually, the cleanest way.",
            "Is that you say?",
            "Well, you take what your prediction was, which was an incorrect prediction.",
            "An from that you subtract the vector representation corresponding to your example XI and then you take the.",
            "Weight vector for the correct class that you didn't predict and you add this weight vector 5 XI OK.",
            "So if you think about the perceptron rule, what it would usually do?",
            "It would update W to BW plus Y I * 5 XI.",
            "OK, so so there would be a + 1 -- 1 sign.",
            "So in the perceptual learning, if you make a mistake.",
            "On a positive example, you add it.",
            "If you make a.",
            "Mistake on a negative example, you basically subtract it from the weight vector.",
            "Here you see that you have like a contrastive rule.",
            "If you think of it for the two class case, it basically boils down to the same perceptron learning.",
            "OK, so you move the weight vector of the correct class towards the pattern and you move the weight vector of the incorrect class that you predicted away from the pattern OK. Now, after you do that, a number of things might happen.",
            "It might for instance happen that the next time you make a prediction, what you get is neither why nor why star, but something else right?",
            "And then you do an update on that right?",
            "Because ultimately what you need in order not to make a mistake.",
            "Is that the correct class Y is really or why I is really rated higher than every other class, right?",
            "And so this update rule would basically only look at two classes at a time and update them, right?",
            "And then there's variance of that where you could also update more than just the Y star.",
            "But this is the simplest way to do this.",
            "OK, and then you just cycle through your data and then you can actually show that assuming the data is separable in that representation.",
            "Given your discriminative functions that you will also find a correct separation and you will achieve 0 training error in the end.",
            "You have questions about this.",
            "Is that clear?",
            "Yeah, so I mean this is something right?",
            "You could just hack up in Matlab.",
            "In a few minutes, right?",
            "And it kind of works like the perception algorithm tends to work right.",
            "It gives you.",
            "First interesting contender to solve this learning problem, but it's probably not the last answer.",
            "OK. Then there's a number of ways to generalize this to something like support vector machine learning.",
            "If this is the type of things you're into.",
            "So one simple way, for instance, to move in that direction is to define a separate."
        ],
        [
            "And margin for an example OK.",
            "So how would you define a separation margin for an example, right?",
            "So usually the margin being positive means that you predict the correct output, right?",
            "But then you will say, oh, you know in general you know just interested in making a correct prediction, but also you want to achieve a certain separation margin between that correct prediction and the next best one.",
            "So the way that translates into a definition of a margin is as follows, right?",
            "So you define gamma I as the margin you obtain.",
            "Mean on a particular example xiy I to be the score that you give to XI using the weight vector corresponding to the correct label.",
            "So this will be this.",
            "So this is our F of XIY and then you will look at what score you give to all the other possibilities that are incorrect.",
            "Right and the margin will be determined to be the minimum of such difference.",
            "To, uh, why?",
            "That is an incorrect Y, and that means basically that if you know the minimum overall possible wise, it means that we should choose this part in a way that we use.",
            "Why had which is the arc Max over all possible incorrect wise in that fashion?",
            "Here, right?",
            "So basically I would show you also you know diagram to explain this you take.",
            "The Y hat that is the best, which is different from my eye, right?",
            "It could either be the best if it is even better than why I if Y is the best it is.",
            "It will be basically the second best solution that you have and you will define the margin to be the difference in score between the two.",
            "Note that if this is positive then it means that you know you'll score for why I will be higher than the score for whoever achieves the maximum score over the rest, so will also be higher than all the rest.",
            "Right, if you pick the maximum there."
        ],
        [
            "OK.",
            "So here is, here's a simple illustration for that.",
            "So the Y value here.",
            "The hate is kind of the score.",
            "The circles are the labels if you like, and the filled circle is the correct one.",
            "OK, so we look at different labels and their score, and there are three cases that are outlined here.",
            "So K is 1 would be 1 where the correct label gets the highest score, and there's a certain margin that.",
            "You also want to obtain, let's say maybe it's this right margin of.",
            "I don't know one point, something, and that's a case where or.",
            "Let's say you want a margin of 1 where you would indeed achieve a margin of one.",
            "In fact, a little bit more in the way you would measure the margin is the difference in score between this and the next best example.",
            "This second best, which would be this one here, right?",
            "If this one weren't here, then you would measure the margin to the circle here, right then you can have a case where.",
            "The correct label still comes out on top, but it doesn't have sufficient margin to the next best one.",
            "So this is here.",
            "The difference in score between this and this is less than, let's say, one here and so then you still classify correctly, But the margin is insufficient, right?",
            "And if you look at case here where actually something else comes out on top and the correct one is down here, then your margin actually turns out to be negative.",
            "And that's obviously bad, right?",
            "So this is so I don't know.",
            "Have you already seen this?",
            "But this is basically a way that you can introduce a margin in a multiclass type of decision setting without necessarily thinking about things being on the other side of the hyperplane.",
            "OK, there's no sort of other side of the hyperplane.",
            "This is not more simply about the distance between the top or the correct one, and the next best one."
        ],
        [
            "OK, so then you can.",
            "You can basically take put the two things together.",
            "So what I talked about on the perceptron learning and then the.",
            "Basically what I just said about the margin and come up with a large margin formulation for multiclass problems.",
            "So this is all well known.",
            "So basically you think of your weight vector as being a concatenation of all the weight vectors that you've defined for the different classes, so this would be transposition.",
            "OK, so you take all your column vectors and just stack them together.",
            "And then you do the usual thing.",
            "So if you do a soft margin SVM, you minimize a quadratic objective in W. Namely W just enters in the norm of W. So note that the norm of W Now basically is the sum of the norms of the individual weight vectors.",
            "If you want to think about it in these terms because it's all stacked together.",
            "Can you add a term for the slack variables?",
            "OK, there should be actually a C here.",
            "Like usual.",
            "You can wait this differently and the margin constraint the way you would define it now is he would just say OK, I want to minimize that with regard to W and size such that the margin that I obtained on a particular example is at least one or whatever.",
            "I've chosen this slack variable to be to lower their modern a bit, right?",
            "So in the in the.",
            "Separable case right where you wouldn't have a soft margin.",
            "This would go away here and then.",
            "This would go away.",
            "You would actually require that gamma.",
            "I has to be greater equal to 1 as one of the constraints and then in the soft margin case you add these slack variables that need to be non negative and you add a penalty term here right?",
            "So this is not a, this is not a quadratic program though and that becomes important not so much for multiclass problem but it will become much more important when we talk about structured classification structure prediction.",
            "Is that if we look at this now here right?",
            "This constraint is actually constrained that involves a maximum, right?",
            "Because we had defined the constraint in a way that we said, well, this is the score of the correct class.",
            "And what is the score of the second best?",
            "Sorry, the.",
            "The one that gets the highest score among the incorrect ones.",
            "Right, and so we have this maximum, but we can basically expand this, right?",
            "So this one constraint, because basically, as we've already seen, if we get the get a certain margin right separation margin between the correct one and the best incorrect one.",
            "Then clearly we get at least that margin.",
            "Also for the other incorrect ones, right?",
            "Because by definition, right?",
            "They're not the ones among the incorrect one that get the high score.",
            "So basically we can just expand it as follows that.",
            "That difference between that we simply require that the difference between the score we're getting for the correct class and what we're getting for any Y that is different from the correct clause.",
            "That that difference in score needs to be at least one or one minus something that we subtract using this library.",
            "Books I I OK so is this equivalence?",
            "Is that clear to anyone?",
            "So if we you know basically if we go to this picture here right?",
            "It just basically means that now we're saying, OK, we want the difference in score between these two to be greater equal 1 and these two and these two and these two write an if we have many more than we get many more equations.",
            "So The thing is that here.",
            "You know, we just have one equation that's nonlinear because it involves the Max.",
            "We can just explicitly unroll this maximum and then we get lots of linear constraints.",
            "Namely, we get as many constraints as we have classes, basically minus one, and then we get this constraint here.",
            "So the total number of constraints.",
            "That we would be getting in our in our power quadratic program in the end.",
            "Would be N the number of samples that we have times the cardinality of the output space.",
            "OK, so.",
            "If we have a relatively small number of classes, then we might be able to handle this if you know the cardinality of Y is several 10 thousands or so, as you know could be in a problem with a large taxonomy, then already this becomes a bit nasty to deal with explicitly."
        ],
        [
            "So now I'd like to move from multiclass classification to really the core of this talk, which is too strong to do structured prediction.",
            "And the reason why I talked about multiclass classification in the 1st place is, as I said before, that it can be.",
            "You can think of as an Eve way of thinking about structured prediction.",
            "Namely, you can think of, let's say a sentence like this.",
            "The dog chased the cat an then you can imagine you look at all possible parse trees that you could build on top of that you know the ones that are compatible with let's say the production rules that you find define your grammar and you.",
            "You can just think of all of them as being classes of their own right, and imagine that all our training examples would just be sentences of the same length for simplicity, right?",
            "Then you could build all kinds of trees on top of an English sentence of five words, save an each.",
            "Each tree would just have a class label, right?",
            "So then basically we could apply multiclass methods, but obviously it's not that easy.",
            "And here are the some of the challenges.",
            "The four main challenges that we actually need to overcome.",
            "To really come to a practical solution.",
            "So first of all, we need a more compact representation, right?",
            "So we cannot basically introduce right.",
            "If you have an exponential number or even super exponential number in the length of the sentence.",
            "Number of trees here number of classes right?",
            "You cannot introduce like a weight vector for every possible tree, right?",
            "That seems like it blows up the representation also in that representation you can learn anything right?",
            "Because you need to be able to generalize across different outputs, right?",
            "If you treat every output as just a class of its own, then basically you would just need to see each of these trees a couple of times in your training data to actually learn something about that specific tree, right?",
            "So that is not feasible.",
            "We need to actually look at features of these trees, right properties that these trees might have and an adequate representation.",
            "How we can capture that.",
            "The second is if you do this, you always have to keep in mind that you know in multiclass classification and we will see this also problem in structured prediction.",
            "That there's this dog, Max, right?",
            "So we've set up this discriminant function.",
            "It assigns scores for a given input to every possible class, but in order to make the optimal prediction, we need to find the best Y.",
            "That's actually that gets the high score, and that means if the number of classes grows and is very large, then search might become a problem because we no longer might no longer be able to do do it exhaustively, right?",
            "Certainly in parsing you cannot search exhaustively through your trees.",
            "The third is we also need some type of better error metrics, right?",
            "So usually in multiclass classification right?",
            "In binary classification you use the classification loss often right as a typical error metric that you use a nematic class classification right?",
            "You might also want to use something like a loss.",
            "Or maybe you have a matrix of the loss that you incur if your prediction is why when the correct label is some other Whitehead.",
            "But for structured classification structured prediction, we need we need much better error metric.",
            "So for instance.",
            "Right in this tree, if we were to predict a tree that is very similar to the correct one right?",
            "We want to say, well, you know, this is not the correct one, but at least it's a good guess, right?",
            "It comes close somehow to the correct one, and maybe a tree that is completely off, right?",
            "We want to say, well, this is really bad if that's your prediction, because it doesn't share any of the constituents with the original tree.",
            "Let's say right.",
            "So if we're predicting structures, we're facing inherently that problem, right?",
            "That is very unlikely that we get it completely right in most cases, right?",
            "So then we want to have a more fine grained notion of how wrong we are.",
            "So we need some error metric and it's important to have a method that can actually take that into account.",
            "And then also we would like the training time to be sub linear in the number of outputs.",
            "OK, so it means that yes, if we're talking about trees, and if that number is exponential in the sentence length.",
            "Then definitely we don't want the training time to be exponential in the sentence links.",
            "We want it to be a polynomial of the sentence length in some form, right?",
            "It needs to be still efficient, so these are the things we need to basically need to look at to move to straw."
        ],
        [
            "Check predictions OK.",
            "So here are the things that I will talk about in the rest of this methodological part of my presentation.",
            "So how do we get a compact representation?",
            "So the answer will be, and this will be the next thing.",
            "To use input output, feature functions and or, you can also encode these things using kernels that ensure generalization capabilities across inputs and outputs.",
            "OK, so.",
            "Think of it right.",
            "So in machine learning, if you do something like binary classification right, you would extract features that allow you to.",
            "Kind of define a good similarity measure in your input space, so to speak, right?",
            "So that you can actually learn so that right you capture certain attributes or properties of your input that are correlated with that input being in class plus one or minus one.",
            "Or if you have a multiclass problem, right?",
            "If we have an output now, that is also very complicated.",
            "What we basically need is we need to understand how certain aspects of the input.",
            "Relat to certain aspects of the output.",
            "OK, so a particular piece of the sentence, how it relates to let's say particular constituent in your past re and things like that.",
            "OK, so we need to decompose both or to represent both input and output over a suitable set of features.",
            "And these things need to be jointly tide together.",
            "OK, because what we were given is an input right?",
            "And we have to predict an output.",
            "So we always needs to be.",
            "A bridge between something in the input and something in the output that we look at, so this will be critical when you apply this method.",
            "Also, for practical problem, how do you design that feature function?",
            "Pretty much as it is.",
            "Also, I guess you know feature kernel engineering or feature engineering in more standard learning certain settings is also crucial.",
            "Then we need to watch out for this efficient search problem, right?",
            "As I said before and what what I will look at here is also something that is representation specific, but that can be often thought of as a black box mechanism that you can plug into your learning system.",
            "Anne, this is.",
            "A mechanism that actually picks you.",
            "That allows you to pick the output that receives the highest score dependent on what application we are looking at.",
            "This can be based on dynamic programming or some sort of a min flow type of algorithm.",
            "3 Max flow type of algorithm in the network and so on and so forth.",
            "OK, so it depends and we will see different examples of how you do this.",
            "The third is I just mentioned before is a refined error metric where you want to use application specific loss functions and build it right into the problem formulation, right?",
            "You don't want to.",
            "Learn something without knowing what the prediction loss is.",
            "You know and then just measuring it after the fact.",
            "You need to incorporate it into your learning algorithm from the start, and then as far as training algorithm is concerned, the one that I will really focus on here is an optimization method.",
            "That basically uses a cutting plane algorithm, or if you think of it in the dual variable selection algorithm, to incrementally solve an approximation of relaxation to the original optimization problem.",
            "OK, so these will be the things we need to talk about, yes?",
            "As for us like few lectures, we had a lot of.",
            "We saw a lot of ways to solve this kind of problem using plastic method, right?",
            "The last one, yeah, why are you not answering any probabilistic approach?",
            "You mean for the entire problem or just for the letter?",
            "About whatever you want.",
            "Right, well, but it's a question on a different level, right?",
            "So I mean for the optimization methods, we basically will see that in some cases they just correspond to things that you would also using probabilistic inference.",
            "Whether you do compute something in expectation or whether you want to compute a map, maximum configuration or sub configuration of your space, right?",
            "This can you can plug in here.",
            "The other question would be why are you not doing it in a fully Bayesian sense or what not right?",
            "Maybe we can then talk and talk in the end because I haven't been here for the other lectures right?",
            "How it would compare.",
            "To these other methods, but you know, I think this approach has some nice advantages, as we will see sparseness of the representation is 1 an.",
            "It's just, I just think it's a very effective way.",
            "It leads to very practical solutions.",
            "You know we would need to compare if Bayesian methods are really better then maybe that's something to consider here, but.",
            "So maybe we can discuss this in the end, because I haven't really talked about the salute."
        ],
        [
            "Alright, so let me at least before we have a little break in five or 10 minutes, introduce the basic setting here that I want to work in.",
            "So the way I want to set this up is as follows.",
            "As I said before, so this deals now mainly with this representation."
        ],
        [
            "Spectr I want to define and this will be in a problem specific application specific manner of function Phi that extract features from input output pairs an it will extract M of those so it'll be an M dimensional vector which encode.",
            "As I said, a certain combination of aspects of inputs and outputs, and then I will define a function F which also called compatibility function, pretty much as before as a function that's parameterized by some weight vector.",
            "And takes an input and output pair as arguments and then outputs basically a linear function of the inner product of this W with this feature representation.",
            "OK.",
            "Note that if you in the multiclass setting right, you could.",
            "Basically you can map this to the multiclass setting by taking the W vectors being a stacked vector, one vector for each class, and then encoding basically the class Y by copying X to the right position.",
            "So then when you compute the inner product you basically get something like.",
            "What we had before in the multiclass setting.",
            "OK, so you can do that as a little exercise if you want.",
            "And then prediction is done.",
            "You know, in a very straightforward way, assuming that you can actually do the arc Max.",
            "But if you could write then you would just choose forgiven X the Y that maximizes that function.",
            "And you can also see that.",
            "Binary classification I've already indicated that for multiclass classification, but a binary classification.",
            "For instance, if you define that function file to be some feature representation of, the X is multiplied by this plus 1 -- 1 label right?",
            "Then actually what you get here is just the usual sign rule that you use in linear classification OK?",
            "So let me.",
            "Let me give you a few few examples.",
            "One example in particular to really illustrate that man like to take some time to do that.",
            "So it's really clear what goes on here."
        ],
        [
            "So this is an example from information extraction, so I have a sentence in reality touch panel systems, capitalize Dad blah blah blah something before and after, so it's part of a sentence.",
            "And what I've shown here is different possible outputs.",
            "OK, so here is actually the correct output for this information extraction task of finding the named entities.",
            "So if Touchpanel systems is indeed a company name and nothing else is a name.",
            "The correct output would be the one that takes these three words together and says, you know this is a company name and all the rest is just blank, right?",
            "Is not a name OK?",
            "And what I've shown here now is also other possible outputs, right?",
            "Three additional ones among the exponential number of possibilities that you have.",
            "So here's for instance and output where we would actually believe that only touch panel is the name of the company and systems is not part of the name.",
            "Here's a hypothesis where we would think that reality is a location and touch panel systems capitalized is the company name.",
            "Here we have a hypothesis that things reality touches a location, and there's nothing else OK?",
            "So you know some of these are obviously for us, right?",
            "Less likely, and others are more likely, but clearly you know a machine learning algorithm first needs to have some representation that of appropriate features that we can even start thinking about a problem like this.",
            "So here are some features that you might want to consider OK.",
            "So future number one would be location segment following the word in right?",
            "So imagine there's no.",
            "You know bunch of linguists that sit down over lunch break and they think about you know what are the types of features that you might want to consider, right?",
            "So you know, you might also use just feature templates and then just include all of them.",
            "But let's just look at individual features here for concreteness, right?",
            "So basically location segment following the word in, you know, because maybe you believe that often right locations.",
            "Follow indeed proposition like in.",
            "So you would basically look at this an in this example here in the representation of Y.",
            "That feature would not occur, at least not in this part, right?",
            "So the feature would be zero 4X combined with Y prime.",
            "The future will be 0 right?",
            "And if I look at X, the pair XY double prime.",
            "If I plug this in here right into my feature representation, I will have it here.",
            "And I actually counted OK, so I'll have it once and in Y three prime.",
            "I also have it once.",
            "OK, so.",
            "This specific entry in this vector that corresponds to that feature right for YW prime and while triple prime it will be one for the others it will be 0.",
            "Then we can look at features, for instance, that look at whether certain words occur in a in a segment that is labeled with a particular label, for instance as a company name.",
            "So let's say you know word like systems might actually be pretty common in a company name, so.",
            "So you can see here.",
            "Then if we think of that as a binary feature, or then we actually counted over the string that that would be a feature that would be active for XX, Y or X, Y double prime.",
            "It wouldn't be active for the others.",
            "Right?",
            "And if we could also have a feature like the word systems occurring at the end of the company name, so this then would only qualify with this.",
            "Why output here the word capitalized following following a company name segment right?",
            "That would also be feature that would only be active here.",
            "And then you might also have features that are independent of the output, so we're effectively you know if you have this function, Phi X, Y, you can always choose to ignore X or Y for that matter, but if you ignore why, then it's not particularly useful because it doesn't help you to discriminate between different possible outputs, whereas you can always choose to ignore the input and just measure something that only measures kind of dependencies between different aspects of the output.",
            "So, for instance, you could say have a feature like a location segment being followed by a company segment.",
            "You know, maybe this is something you often see, so you want to capture it in a feature.",
            "So this is the case only here and then, or something like company name consisting of three words.",
            "So this would be.",
            "This would be a feature that fires sort of here and not for the others, right?",
            "So so these are all features that you could look at.",
            "And basically I hope now you get some understanding for this particular problem.",
            "You know what is behind these guys here.",
            "These fee of X, Y.",
            "What types of things you could engineer in there?",
            "OK then, let me just basically close this part.",
            "By just showing you again, you know then what you sort of finally get right, so we were considering these.",
            "I guess six different features.",
            "You know now we basically just go through it and we collect all the features.",
            "These count features that happen to be binary here because we don't have multiple events in the same sequence.",
            "So basically you know X comfy of X, Y will have you know feature 234 and six active.",
            "So we will basically get this binary vector to represent that.",
            "If these are our first 6 features.",
            "Why Prime has nothing active so it will just be the zero vector Y double prime you know has these three features active in the other three is not and the triple prime only has this first feature active."
        ],
        [
            "OK and and then basically just to give you a second example so you don't have only one example.",
            "Yes."
        ],
        [
            "This one.",
            "Yes.",
            "So to make some kind of a chain or.",
            "Take all the features, all the possible features.",
            "Then using the training set to find out whether these features are common or not, and then eliminate several of them and.",
            "They will consider like do some feature extraction from the crucifixion.",
            "Yeah, so yeah, definitely they are at heart, right?",
            "That's why I said right.",
            "Imagine like a few linguists over over lunch break, right?",
            "Coming up with these features.",
            "So I think we need.",
            "Usually we need to distinguish.",
            "I would say the features that just deal with output part right and correlations there.",
            "Usually you might have.",
            "You might have domain knowledge that really tells you right what type of dependencies you want to capture, what types of things to look for.",
            "If we look at the input side right and how that correlates with with the outputs, right?",
            "So for instance, which words are likely to occur within a particular segment, right?",
            "You could just take all possible words, right?",
            "That's why I said it could be like a template and then indeed you might need to use some feature induction or feature selection methods, right?",
            "Depending on how you actually train it right to basically prune back and limit it to a smaller number of features.",
            "But basically that yeah, so so here.",
            "This is not the way as I said, you're not actually going through.",
            "You know feature one by one.",
            "So for instance, you know exactly hand picking system and systems and so on and so forth, right?",
            "That's why I said this.",
            "Think of a template right template that does that.",
            "A template would be a word occurring in a name of a certain type, right?",
            "This could be a template and maybe you do this for all words, or maybe only four words that in your training data are frequent enough to actually show up there, right?",
            "So there's certainly lots of.",
            "Pruning and a word occurring at the end of a of a company name or at the end right.",
            "The template might be that indeed you might want to have features that not just tell you it's in the name, but then it's at the end or at the beginning, right?",
            "Because this can be important.",
            "So in that sense, the template, I think is designed manually, and then you use automatic techniques to prune it.",
            "Prune it down."
        ],
        [
            "Or in the example here.",
            "So this is this is for the parsing case right where?",
            "You can basically just take the context free grammar with the rules that are given to you.",
            "Right and just directly translate that into a feature representation and namely what you do here is just you basically look at right.",
            "So we need to.",
            "We are given an X and we hypothesize some Y and we want to know what is the corresponding feature representation.",
            "So what we could do is basically look at the number of times we've applied a certain production rule.",
            "So we've done S 2 N PvP ones, so this would get account of one in this tree.",
            "We would have a production rule.",
            "Let's say that says we would just have a verb phrase that didn't apply, so that zero.",
            "There's a production praise that where a noun phrase goes becomes a noun.",
            "OK, so we see this here and here.",
            "So we have that twice, and we have a verb phrase moving to a verb and a verb phrase.",
            "So we see this ones here and so on and so forth, right?",
            "So this is all the parts that have to do with the production part again.",
            "So these are things that are independent of the input, right?",
            "They model like which types of trees?",
            "Are likely which types of productions are likely and then when you come to the level of where you actually go to the terminals here, write the actual words.",
            "That's then where the representation becomes lexicalized right, and probably much more high dimensional, because now you actually need to know, right?",
            "Something like you know the probability to go from a noun and replace it by some actual word like Boeing, right or arrows, or Seattle and so on and so forth, right?",
            "So you also can encode that here, but you can hear you can imagine that this is a.",
            "A very high dimensional representation that you're getting there, right?",
            "Yes, I think in some cases, maybe from certain trees cannot correspond to the input string at cold.",
            "So do you some right quick processing because like some dank, I mean some sequence of strings cannot be generated by your grammar, right?",
            "Right?",
            "So we will see later how that how that comes in.",
            "So here.",
            "The feature representation in principle you could just do on anything, right?",
            "It doesn't matter whether.",
            "I mean whether it makes sense or not, right?",
            "I guess here just the branching.",
            "It just needs to workout such that you have a certain number of non terminals in the end that is really the length of the sentence that you're looking at, right?",
            "And and above, it's just you just need to apply the rules right?",
            "But you could you could you know, locate it, you might think is a noun and it was generated from a noun phrase or something.",
            "So maybe you know grammatically all of this of course is nonsensical, but that's what you want to learn, right?",
            "What is grammatical and what is not so?",
            "So basically an if you allow right that basically every.",
            "Every word can really be, you know if you have rules where actually on the right hand side here for the for the actual words, right you don't constrain it.",
            "Then basically you know if you have prior knowledge, of course that you know that located is always a verb.",
            "Let's say you know and you would never want to have a rule noun to locate it.",
            "Then of course you can just roll it out right there, right?",
            "You just just eliminate that rule, right?",
            "But the rest is just given by the grammar right?",
            "And whether you do something that doesn't make sense for the sentence that you're considering, as long as you build like a valid kind of tree, that's well formed, then that's OK. Say again sorry talking about.",
            "Well, I will talk about ways to.",
            "Well, it depends.",
            "There are ways that you actually don't have to complete it explicitly, so ways of avoiding explicitly constructing it.",
            "And the other problem is more about also yeah, so in that specific case here, for instance, right?",
            "This is not a representation that you will explicitly generate, right?",
            "That doesn't make sense.",
            "So here indeed I will basically.",
            "Basically this will happen through kernels, right?",
            "We will look at in a product of these five vectors and see you know.",
            "Yeah, sure, but but we don't have to go through this blow up in the representation, right?",
            "But will be more interesting from an algorithmic POV though, is the is actually this argmax problem right of finding?",
            "Finding not just computing for a given excellent, why the representation but really forgiven X finding the Y that maximizes this linear function right in five.",
            "But that's something we can talk about.",
            "OK, should we take a 5 minute break here?",
            "I was asked to have a break in between so we just continue at.",
            "I was only 20 hours, sorry.",
            "OK, well then we're sorry.",
            "Then we just move on.",
            "I was somehow almost looking at the Clock, but I thought I started at a different start time, OK?",
            "So."
        ],
        [
            "Here's here's also ways, yeah, that's why everyone so tired now, I guess.",
            "Here's also ways that you can actually construct these types of Maps in a way that is sometimes convenient.",
            "So for instance, one thing you can do is you can define if you have a good way of defining input features that just depend on X, so you have some vector representation, let's say of X, and you have some features you care about on the Y side.",
            "OK, you can just define the joint.",
            "Feature map to be sort of the tensor product of these two, which basically means you will now have think of it as a multi index IJ.",
            "So one feature here will be a product of the input feature and Jay thought that feature just multiplied together right?",
            "This will be my Feige feature here.",
            "And.",
            "And the interesting thing is right about about this is for instance, if we if we look at this here.",
            "If we want to compute an inner product of a function that has been designed by this tensor map.",
            "Trick is that basically it boils down to an inner product in that representation times an inner product in that representation.",
            "OK, so it nicely separates, so this is kind of 1 answer of right if of cases where you can build this function implicitly by just saying, well, you know it's all combinations of these guys, but you don't actually need to compare.",
            "Put all combinations.",
            "If in the end all you need is an inner product, right?",
            "So this is 1 trick.",
            "The other thing that."
        ],
        [
            "We will see for things like if you have a labeled sequence.",
            "Is that actually you have feature functions that extract something from position T?",
            "Right, this was already the features that I've shown you in detail for the information extraction task.",
            "And then basically you just count them up, sort of along the sequence.",
            "So if you have, you know if access the sequence X one to XT&Y is a sequence.",
            "Why want to Whitey?",
            "Then you extract at each position you extract some feature vector and you can just add it up.",
            "So think of sort of counts of features along along different positions if you have that obviously then things are also simple.",
            "So for instance in the primal if I have W and I want to multiply it with that right, I can just basically do the inner product in the sum.",
            "Right, and hopefully these feature these features.",
            "Here are maybe simpler, simpler to do so also when I compute an inner product between one of these vectors an for another input output pair.",
            "I can also then actually compute this as a double sum if need be, so there are some tricks and how."
        ],
        [
            "You can do this.",
            "In general, there's some right you can.",
            "You can take this further and you can actually move to a reproducing kernel Hilbert space, so you don't necessarily just limit yourself to like finite dimensional representations.",
            "And if you do that just to sketch that here, basically you could set it up in the following way that you have a Hilbert space of functions F, and these functions are defined basically as linear combinations over some sample set.",
            "Of Point set and these points are actually is a sample of input output pairs.",
            "OK, so a point here Zed is really an input output pair and then you have these coefficients.",
            "Alpha, zed, right?",
            "So this is a way that you can think of.",
            "You have a generating kernel and basically all the things that you do within these reproducing kernel Hilbert spaces.",
            "You can also do by just basically treating.",
            "The input output pair really as your input and sort of the kernel.",
            "Then just instead of just dependent on a single set here, it just depends on this cross product, so this is not."
        ],
        [
            "This is not too deep and of course you can think of these kernels as a special case that they are defined as we define it before, basically as in a product of some feature representation that you extract.",
            "OK. And and again, you know I said this.",
            "We already said this before.",
            "If you think in the kernel world again, right for the tensor product kernels, for instance, you basically just get a factorization over kernels that are just defined over input pairs and kernel functions that are just defined over output pairs.",
            "For something like this tensor product map.",
            "OK, so they have two purposes here really, so one is what you already know.",
            "You know in general, from kernel machines that you know they are flexible and efficient feature extraction.",
            "They often help you to avoid limit yourself too.",
            "You know finite dimensional feature representations.",
            "They might help in allowing you much faster learning by implicitly using the kernel functions instead of explicitly computing inner product in this context.",
            "Here you also have always the problem of combining things on the input and output side, which might be a problem in terms of the combinatorics of the combinations that you actually want to look at, like in this case here, right?",
            "So then, kernels by.",
            "If you have a corner, for instance, that factorizes nicely like this.",
            "Right then that's also an advantage on the computational side.",
            "That kernel approach would give you.",
            "OK, so then."
        ],
        [
            "Let me move to the sorry I'm running a little bit slower than I thought.",
            "Let me move to talk a little bit about statistical inference problems for structured prediction."
        ],
        [
            "So in general, if you look at it without committing to particular.",
            "Learning algorithm or a particular formulation, even off the learning problem, you can look at it in the most general setting as follows.",
            "You have a training sample South your training sample.",
            "These are now input output pairs, right?",
            "XYY, I and you have some.",
            "Cost function or functional that takes as input.",
            "Let's say function from the RKS.",
            "And it basically measures the goodness of fit of F, right?",
            "We will look at different possibilities for that.",
            "And then we have some regularizer and there typically we would use basically something that depends is a simple function of the Hilbert space norm of our function F. If we have a setting like that and we want to minimize such a regularised objective function with functional here that depends on the training sample and this additional.",
            "Stabalizer then we get the following represent a theorem and there's basically one thing that you need to understand here that's different from the general case.",
            "So basically you can show that the minimizer of that F hat.",
            "For a given sample.",
            "Is a function that will can be written as a sum over all your training example.",
            "OK, that's usual, but now you have a sum over all possible outputs Y of in your output space, and then you have a weight better iy.",
            "So this depends on the training index.",
            "Here I and on the actual some output Y.",
            "Note that this is not just why I, it's you need any all of the wise basically show up in this somewhere.",
            "You need to consider.",
            "And then it's a kernel function.",
            "Basically, this is the argument you're going to apply this to write an input output pair and the second argument is basically the pair XIY.",
            "OK, So what is if you just do this in the without output learning in the?",
            "In the case that you've probably seen this, it'll just be a sum over your training points.",
            "Then there will be a beta I and then it will be K blank, XI.",
            "Write something like that.",
            "So the interesting thing here is that the representative theorem.",
            "Also tells you that the function right that you're getting by minimizing over your Hilbert space of functions has a nice finite representation.",
            "Things are centered sort of on your training data, but not quite in the sense that for every input X are you also need to consider every possible Y. OK, not just.",
            "Xiy I write that's not enough and we will actually see in the algorithms.",
            "The main algorithm that I will talk about here.",
            "You know what that really means?",
            "How that comes about, right?",
            "So is that clear?",
            "So it means for instance, right so?",
            "The sparseness of this so in general you know the representative theorem has many advantages.",
            "One is certainly that you have a representation that you know cannot scale worse than the number of training points that you have, right?",
            "So if I have any training points, I know that I can represent my optimal function right using N coefficients.",
            "Like a linear combination of these N kernel function centered on my sample.",
            "The problem is here right?",
            "If the if that space is really large right even as N is small.",
            "Actually that representation, though finite right, might still be too large that we want to handle it, right?",
            "So for instance if it's all parse trees, right?",
            "So for every input sentence I need to consider all pass trees, then you know this is still, you know representation that is too large to handle, so it's interesting to look at further.",
            "Ways of sort of sparsifying that you know finding conditions or mechanisms to reduce the dimensionality of the representation even further.",
            "OK, so you can."
        ],
        [
            "Also, think of this that's also something that might give you some insights and dependent on what how exactly you set up your learning problem that can be useful.",
            "You can think of this whole thing in a probabilistic context.",
            "So namely, if you think of that function F of X, Y.",
            "As being for a given Y, sort of a sufficient statistics.",
            "Sorry, not really.",
            "They have themselves actually defies.",
            "Maybe have this down here.",
            "Sorry so if you think of this function F right you put it into an exponent where you say given ax, right?",
            "What is the probability of a particular output?",
            "So far we said that you know F is like a compatibility measure between inputs and outputs and we would like to maximize it.",
            "Forgiven X to find the optimal Y.",
            "Now this is kind of a softer version of that right where you would actually try to come up with a model conditional model of outputs given inputs.",
            "So you could put the F in the exponent here and you would have then a normalizing constant that makes sure this conditional distribution is well defined.",
            "And then you look at what you actually have up here.",
            "For instance, in this primal representation.",
            "Then you know this.",
            "In this function F, the weight vector will just correspond to Canonical parameters in this fight to sort of a sufficient statistics.",
            "If we think of this, X argument is being fixed here, because this is always what we condition on, right?",
            "So we can so we can also instead of justice talking about F, right?",
            "We can also associate in a Canonical way of probability distribution with.",
            "A compatibility function F that's defined like that, and then we can actually use that too.",
            "Set up other interesting, you know, specific loss functions that we might be interested in optimizing so."
        ],
        [
            "For instance, right?",
            "What you might want to do as a as a measure of fit for your function F given a sample S, you might use something like conditional log likelihood.",
            "OK, so you might say, well, the way I determine the quality of F is.",
            "I look at the negative log probability.",
            "If this is what I want to minimize, right?",
            "The sum of the log probabilities of outputs given the inputs on the training sample that I've seen.",
            "Right, and if I expand this basically using the definition from before, then you get a sum over all your training points.",
            "This compatibility function evaluated on your input output pair minus basically the log partition function from previously that normalizes it right?",
            "And So what you can see here is basically.",
            "That in some sense what you're doing is you're comparing the score on the correct input output pair with kind of an.",
            "Total score that you compute over over all possible outputs.",
            "Forgiven inputs right, and that score is exactly corresponding to this log partition function.",
            "Right, so then, if you minimize that with the negative sign here, you effectively you need to maximize the compatibility on training pairs.",
            "That makes sense, right?",
            "You try to choose your weight vectors so that on your training examples.",
            "You get a high F score, but at the same time you basically need to minimize the lock partition function.",
            "Right and so basically, if you have, you know you can visualize that, and so this would be a way of.",
            "Basically, thinking of this structured learning problem as a generalization of or you know a version of.",
            "If you like a conditional random field.",
            "Model where you know you would have.",
            "Basically you know.",
            "You would have outputs.",
            "Why there can be dependencies that are defined between your wise using a random field approach and it's conditional because basically you condition on a fixed set of input variables and you're only interested in really maximizing the predictive performance of your model.",
            "Sort of a discriminative model right of the outputs given the inputs OK."
        ],
        [
            "So the let me.",
            "So basically, if you do this in this CRF sense.",
            "Using using this particular conditional likelihood objective function.",
            "You look at what you get then.",
            "Basically, if I take the computer gradient with regard to my weight vector, let's say.",
            "I see that I get the typical optimality conditions that they usually get in this case, which is basically that the empirical statistics over these files, so just summing them up over all the training examples over xiy pairs that the optimality condition would be that that equals the expectation in average overall training examples of Phi XI XI is given, Y.",
            "The expectation is then over Y right over all possible outputs.",
            "So then what you need to be able to do right?",
            "For instance, if you want to do anything that is gradient based, is.",
            "And basically you need to be able to compute these expectations over the outputs, right?",
            "So I've told you before and I'll come back to that that you know, we have a setting where we.",
            "You know, in order to do prediction we would need to compute the AC Max over all possible outputs, and that because that will be our prediction right?",
            "In this setting here it would be the most probable output given an input.",
            "In order to do learning in this setting here using a gradient based method, you would need to be able to compute an expectation over outputs, right?",
            "So whether or not you can use that in a straightforward way, then will depend on whether how hard that inference problem is right.",
            "And in general you might.",
            "Want to use on let's say you have a label sequence.",
            "You might use something like a forward backward type of algorithms to compute it.",
            "Or you know if you can do exact inference using a junction tree algorithms if the clique size of the dependencies that you capture between your output variables are small enough, then you can do that.",
            "Or you can use approximate inference methods.",
            "And basically plug them in there.",
            "OK.",
            "So let me just."
        ],
        [
            "Basically.",
            "Make one more argument and then I think we should break for lunch and then I'm just going to resume there so.",
            "So this was this is 1 possibility of what you can do right?",
            "So you define this conditional probabilistic model and then you maximize a conditional likelihood and then as long as you can compute this expectations then that's fine.",
            "But there still is a problem.",
            "As I said before, you know this representation that we're talking about."
        ],
        [
            "Here, right, these representations can become really nasty, and if I want, you know I might have.",
            "You know a lot of features that I want to extract, so this becomes, you know, very high dimensional.",
            "And then even if I in principle can do the expectation right, it becomes just just everything becomes working in this primer representation just becomes very tedious and difficult, right?",
            "And so?"
        ],
        [
            "So we know from the representative theorem that.",
            "Yeah, you know, in principle also the solution to this problem here, because you know if we just plug in this cost function in."
        ],
        [
            "To the representative."
        ],
        [
            "It will allow a compact representation, you know an expansion over kernel function centered at these points.",
            "But but again, you know this sum is still huge.",
            "Now we look at the dual formulation, but the problem is still that now it scales the representation size with the size of the input right space, so that's not good.",
            "So basically.",
            "Well, I will talk about then when we resume in the afternoon is basically how to go beyond this and how to develop methods that actually exploit or get solutions that are more sparse in the sense that this some here actually becomes smaller so that we only need to look at a few possible outputs instead of at the whole output space.",
            "OK, so then we just leave it here.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, good morning.",
                    "label": 0
                },
                {
                    "sent": "Welcome to the second lecture of the day.",
                    "label": 0
                },
                {
                    "sent": "My name is Thomas Hoffman.",
                    "label": 0
                },
                {
                    "sent": "I work currently at Google in Switzerland.",
                    "label": 0
                },
                {
                    "sent": "I think in the program it says Google Research, but that's not quite true.",
                    "label": 0
                },
                {
                    "sent": "I'm actually director in an engineering center and while I think we have interesting research problems and also some I want to mention here, my work these days is really more about building systems and working with engineering teams to actually putting things in production.",
                    "label": 0
                },
                {
                    "sent": "And this is what I've been doing the last three years.",
                    "label": 0
                },
                {
                    "sent": "So some of the work I will present here sort of predates my Google time work I did on structured prediction and then I'll cover that and some recent work that other people have done in this area.",
                    "label": 0
                },
                {
                    "sent": "As you will see, hopefully what I'll talk about towards the end when I'll discuss.",
                    "label": 0
                },
                {
                    "sent": "So the first part of the lecture this morning will mainly be about the methodology and the formal setting, and then in the afternoon I will talk more about applications that has.",
                    "label": 0
                },
                {
                    "sent": "Most of the applications will be in information retrieval, so you then you might imagine sort of some of the connections between working at Google and working on this specific problem.",
                    "label": 1
                },
                {
                    "sent": "We might have in the end enough time to either, you know, finish a little bit earlier, or if you have questions about Google and things in general, use of machine learning in an industrial setting.",
                    "label": 0
                },
                {
                    "sent": "I'm also happy to answer those.",
                    "label": 0
                },
                {
                    "sent": "So let me begin.",
                    "label": 0
                },
                {
                    "sent": "Feel free to interrupt me at anytime so.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of course we know.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to wake up after the break.",
                    "label": 0
                },
                {
                    "sent": "That supervised machine learning, you know is a is a wonderful thing to have where we can learn functional dependencies between inputs and outputs.",
                    "label": 1
                },
                {
                    "sent": "And we can do this based on training data by using inductive inference and its most basic form.",
                    "label": 0
                },
                {
                    "sent": "Of course we are facing classification problems and you know you've seen probably many examples here or you know many examples in your work classification problems such as optical character recognition, where we're given, you know, given an image of a character, map it to one of the possible digits or characters and things.",
                    "label": 0
                },
                {
                    "sent": "For instance in the context of information retrieval, probably the most important application.",
                    "label": 0
                },
                {
                    "sent": "Things like document correct, document categorization where given a particular document represented just by the text or some additional features or semi structured data metadata that we might have to map it to certain categories.",
                    "label": 0
                },
                {
                    "sent": "So here's an example from Reuters where we might map it to things like a location code and to certain provide some topical annotations, for instance that this document is on money markets, foreign exchange markets and so on and so forth.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course, in document correctores categorization.",
                    "label": 0
                },
                {
                    "sent": "You know just to contrast it with other things you might other approaches that you might see the advantage of machine learning is that we can basically harness the knowledge that humans have, like an expert like this one here on labeling documents, given a particular classification system or particular taxonomy that people might be interested in and then use these training data to feed it into a mechanism, a learning machine of some sort.",
                    "label": 0
                },
                {
                    "sent": "To then actually output using an inference mechanism, a complicated algorithm.",
                    "label": 1
                },
                {
                    "sent": "Now that in practice has turned out to be more efficient than, let's say, the traditional approaches that you've seen in this area.",
                    "label": 0
                },
                {
                    "sent": "For instance, that have been based on what's called knowledge engineering, where people have tried to design like handcrafted rules of how to classify things right so?",
                    "label": 0
                },
                {
                    "sent": "It's interesting to note that, right, it's not clear our priority that for particular problem machine learning is always the best technique to use, but in text categorization actually has been quite successful and much more successful than other methods based on knowledge engineering.",
                    "label": 0
                },
                {
                    "sent": "The advantage of machine learning is pretty clear, whereas knowledge engineering, right, the knowledge that resides within an expert to get that out and to get it formalized right, is a very difficult task, very hard to achieve, good coverage, very.",
                    "label": 1
                },
                {
                    "sent": "Very hard to achieve good precision.",
                    "label": 0
                },
                {
                    "sent": "The indirect approach through actually creating training examples has the advantage that really, you know we can just let the expert do what the expert is good at, namely categorizing things, using whatever knowledge he has, and we don't have to worry about the formalization, but rather than we can use an inference mechanism to come back to get really something that generalizes to new data.",
                    "label": 0
                },
                {
                    "sent": "So I should also say sorry, I should also say that.",
                    "label": 0
                },
                {
                    "sent": "Text categorization I'm not going to talk much about the plain text categorization example, but has probably been one of the most successful applications of machine learning in practice.",
                    "label": 0
                },
                {
                    "sent": "And if you look at today's systems, they actually do reach human level accuracy in the categorization performance that you see over realistic taxonomies.",
                    "label": 0
                },
                {
                    "sent": "Often what you notice that the precision of a human annotation might be higher in the sense that if a human says it belongs to that class, it's more reliable typically than ever machine makes the prediction, but what machines are much better at is the recall aspect of.",
                    "label": 0
                },
                {
                    "sent": "If I take a document order all the annotations that the document should get.",
                    "label": 0
                },
                {
                    "sent": "If you think about huge taxonomies with many many classes, humans are sometimes likely to basically omit certain categories because they don't think about it right, which is fairly natural.",
                    "label": 0
                },
                {
                    "sent": "If you have a huge category systems.",
                    "label": 0
                },
                {
                    "sent": "Machines are usually better at that part, which you could think of as a recall.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "OK, but this is not what I would like to talk about here.",
                    "label": 0
                },
                {
                    "sent": "Mainly I would like to talk about.",
                    "label": 0
                },
                {
                    "sent": "Problem called structured classification or structured prediction.",
                    "label": 1
                },
                {
                    "sent": "So by that, what I what I mean is really a setting where what we are trying to predict is not a class variable, binary variable or not even a multiclass problem, but it's something more interesting.",
                    "label": 0
                },
                {
                    "sent": "So to give you an idea of what we will be talking about here and many more examples will follow, you can think about all kinds of structured objects such as sequences, strings, trees and so on and so forth.",
                    "label": 1
                },
                {
                    "sent": "And what if you have machine learning problem, a prediction problem where this is really the output that you want to generate?",
                    "label": 0
                },
                {
                    "sent": "How do you go about use?",
                    "label": 0
                },
                {
                    "sent": "You know, standard machine learning technology to solve these problems, right?",
                    "label": 0
                },
                {
                    "sent": "Most of the literature is probably on, you know, binary classification or regression or or perhaps multiclass classification.",
                    "label": 0
                },
                {
                    "sent": "But a lot of problems out there you have more complicated prediction problems.",
                    "label": 1
                },
                {
                    "sent": "The second class of problems is where the outputs are not structures in the strict sense of a tree, a graph, a sequence.",
                    "label": 0
                },
                {
                    "sent": "But where you have multiple classification problems that are coupled together OK, so we will also see many examples of those where really there are multiple response variables and the response variables are all interdependent.",
                    "label": 0
                },
                {
                    "sent": "OK, so you don't want to just predict individually, right?",
                    "label": 0
                },
                {
                    "sent": "For each example, be the document or beaten image or what it is right?",
                    "label": 0
                },
                {
                    "sent": "You don't want to independently predict a label, but really you have a large set of labels that are that are all coupled somehow.",
                    "label": 1
                },
                {
                    "sent": "And so then the question is right, how can we extend machine learning techniques to deal with this collective classification problem in a way that we're better than just independently predicting individual respons variables?",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some motivating examples to start this so you get an idea of what types of problems you will be able to tackle with these methods, so we're not going to talk about really the full application here.",
                    "label": 0
                },
                {
                    "sent": "That's more towards the end, but this is more of a motivation of the types of things I'm I'm talking about here.",
                    "label": 0
                },
                {
                    "sent": "So if you do things like, for instance, part of speech tagging right, then what you want to do is you want to take a sentence like the sentence up there.",
                    "label": 0
                },
                {
                    "sent": "Profits soared at Boeing blah blah blah, so this should be your input X.",
                    "label": 0
                },
                {
                    "sent": "And what you would like to do is to annotate every word in that sentence here with a particular label from a set of part of speech tags.",
                    "label": 0
                },
                {
                    "sent": "So N would be a noun, be would be a verb, P would be a preposition.",
                    "label": 0
                },
                {
                    "sent": "I guess a possessive adjective.",
                    "label": 0
                },
                {
                    "sent": "And so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "So you see, the output is the red part here, right?",
                    "label": 0
                },
                {
                    "sent": "Is the augmented input by these tags.",
                    "label": 0
                },
                {
                    "sent": "And why is this useful?",
                    "label": 0
                },
                {
                    "sent": "It's often useful, for instance, as a first step in a syntactic analysis of a sentence when you know maybe at some point you want to parse the sentence.",
                    "label": 0
                },
                {
                    "sent": "Often part of speech tagging is the first thing to get there.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can see why.",
                    "label": 0
                },
                {
                    "sent": "Why is it not simply a multiclass or binary classification problem?",
                    "label": 0
                },
                {
                    "sent": "It's because these things will be coupled right?",
                    "label": 0
                },
                {
                    "sent": "So, for instance, whether you label you know certain things here, let's say as nouns or verbs might have an impact.",
                    "label": 0
                },
                {
                    "sent": "On what neighboring labels should be right?",
                    "label": 0
                },
                {
                    "sent": "Because maybe certain sequences of labels are more likely certain, you know and annotations on the whole sentence level are more likely than others, right?",
                    "label": 0
                },
                {
                    "sent": "So you would like to take that into account, right?",
                    "label": 0
                },
                {
                    "sent": "And how can you do that?",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the second problem is related, but is a bit different, so I mentioned it here independently.",
                    "label": 0
                },
                {
                    "sent": "Is a problem known as information extraction, where this is really only.",
                    "label": 0
                },
                {
                    "sent": "Like a broader title for for a whole set of different problems.",
                    "label": 0
                },
                {
                    "sent": "So the idea here is that we are not so much interested in tags for individual words about what their part of speech role is, but what we're interested in is really to mock up a document with named entities that are mentioned in the text and also to classify these mentionings according to a particular set of types of entities that you might that you might want to define.",
                    "label": 0
                },
                {
                    "sent": "So, for instance, an organization.",
                    "label": 0
                },
                {
                    "sent": "Country a personal location.",
                    "label": 0
                },
                {
                    "sent": "And various other entities that types that you might want to define.",
                    "label": 0
                },
                {
                    "sent": "So ultimately what you would like to get it is if I have a paragraph like this, right?",
                    "label": 0
                },
                {
                    "sent": "I want to know that the WTO, for instance, is an organization that the United States are a country that Richard Aboulafia is a person name.",
                    "label": 0
                },
                {
                    "sent": "And so on and so forth, right?",
                    "label": 0
                },
                {
                    "sent": "And what I want to do is really I want to add, let's say, one way of encoding this right.",
                    "label": 0
                },
                {
                    "sent": "I want to add text that.",
                    "label": 0
                },
                {
                    "sent": "Identify segments of consecutive tokens and give them a label that this is a person name or this is an organization name and so on and so forth, right?",
                    "label": 0
                },
                {
                    "sent": "Like like seen here.",
                    "label": 0
                },
                {
                    "sent": "And again, in doing that, right things will not be completely independent.",
                    "label": 0
                },
                {
                    "sent": "In particular, if you if you look at neighboring things and you know the segmentation, since this is a segmentation problem, if you like for a sequence of words, you need to look at the overall segmentation as a whole in order to do this already.",
                    "label": 0
                },
                {
                    "sent": "Sort of, you know, to identify a segment or variable length, right goes beyond like a simple independent classification problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so the challenge is here to predict the labeled.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Temptation.",
                    "label": 0
                },
                {
                    "sent": "OK, and then taking it step further in terms of complexity of structures and also the type of problems we might want to look at.",
                    "label": 0
                },
                {
                    "sent": "Is here's an example from natural language parsing, where the input is again a sentence.",
                    "label": 1
                },
                {
                    "sent": "What's in blue here?",
                    "label": 0
                },
                {
                    "sent": "And what you want to predict is apostrophe, like this.",
                    "label": 0
                },
                {
                    "sent": "So let's say you might be given a context free grammar of some sort, right?",
                    "label": 0
                },
                {
                    "sent": "That tells you about you know what the types of constituents are that you can find here.",
                    "label": 0
                },
                {
                    "sent": "So NP, for instance, that there are things like noun phrases and verb phrases, and you're given a little bit about you know how the set of production rules basically on how these nonterminals can be translated into, you know.",
                    "label": 0
                },
                {
                    "sent": "Other sequences of non terminals and then ultimately into a sequence of terminals, which will be our sentence.",
                    "label": 0
                },
                {
                    "sent": "But basically what you want to do is you want to parse the sentence and identify correct and among the.",
                    "label": 0
                },
                {
                    "sent": "Usually you know large number of possible parse trees.",
                    "label": 0
                },
                {
                    "sent": "Identify the most likely the most plausible one that gives the correct interpretation to this set of English.",
                    "label": 0
                },
                {
                    "sent": "Let's say right so and the question here is of course traditionally if you look at this right in the.",
                    "label": 0
                },
                {
                    "sent": "90s people have started using probabilistic models so they used PCF dogs.",
                    "label": 0
                },
                {
                    "sent": "They lexicalized alot of the rules, and basically they're typically would use generative models and lowering the parameters of the generative model by using some maximum likelihood with the right level of smoothing.",
                    "label": 1
                },
                {
                    "sent": "And more recently, what will be and this is also what we will be looking at here is how can you use more discriminative learning methods to actually infer parse trees so to do natural language parsing?",
                    "label": 0
                },
                {
                    "sent": "OK, Ann.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then I won't really talk much about this, but here is a reference, like a paper that's now a couple of years old.",
                    "label": 0
                },
                {
                    "sent": "Also, if you think about recognition of handwritten words, you have a situation where really you know you don't want to just segment the image and then predict individually what the characters are that you see in these pixel images, but rather you want to take dependencies into account.",
                    "label": 0
                },
                {
                    "sent": "So probably on the word level at least.",
                    "label": 0
                },
                {
                    "sent": "So that, for instance, what you see here, you know that you not mistakenly taken that for an E let's say, but rather for C, like in causes, right?",
                    "label": 0
                },
                {
                    "sent": "Because this is more likely.",
                    "label": 0
                },
                {
                    "sent": "So you have a lot of.",
                    "label": 0
                },
                {
                    "sent": "Examples like that where there's a lot of local ambiguities, so if you will make a decision locali right, you might get the wrong result, but you can help.",
                    "label": 0
                },
                {
                    "sent": "You can use the context of what you see to the right or to the left or what you think you see, right?",
                    "label": 0
                },
                {
                    "sent": "So how you classify these characters as a guiding you know, as as additional help as it providing additional evidence for.",
                    "label": 0
                },
                {
                    "sent": "For for recognizing these characters.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And then finally, I just like to mention mention this example here and like a typical group of people that has worked on this is inbuilt Freeman's Group which has to do with problems in computer vision where traditionally people have often looked at things like object recognition or image annotation and have really thought thought of that as like sort of an independent problem so I. I look for a particular object in the image, maybe a search over certain windows and regions in the image, and then I have some function that tells me whether there's enough evidence that we actually see a monitor somewhere in the image, let's say, and then the prediction would be OK.",
                    "label": 0
                },
                {
                    "sent": "This is an image that contains screen, computer monitor, but what's interesting is also to actually understand that their dependencies between the different things that we see in an image.",
                    "label": 0
                },
                {
                    "sent": "So if I you know this is a computer monitor, maybe it makes it more likely that there's a keyboard and a mouse nearby.",
                    "label": 0
                },
                {
                    "sent": "Right, so I shouldn't make these decisions independently, but rather predictor whole scene graph if you like or recognize objects in the context, and then hopefully that will help with.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The recognition accuracy.",
                    "label": 0
                },
                {
                    "sent": "OK sorry I have one more example.",
                    "label": 0
                },
                {
                    "sent": "Forgot about this one, so this is also relating to work.",
                    "label": 0
                },
                {
                    "sent": "There's a reference here and then.",
                    "label": 0
                },
                {
                    "sent": "Some work that trust in your hands has been doing over the last couple of years and this is a problem where the input from computational biology where the input is a representation of a protein.",
                    "label": 0
                },
                {
                    "sent": "So here it's a primary sequence, so it will be just a string of letters.",
                    "label": 0
                },
                {
                    "sent": "Every letter will correspond to an amino acid.",
                    "label": 0
                },
                {
                    "sent": "And what we would like to do is in the case of secondary structure prediction, we want to identify segments that correspond to certain types of secondary protein structure like certain type of a Helix or sheet or things like that.",
                    "label": 0
                },
                {
                    "sent": "So there's usually a very small alphabet that biologists are happy with and basically what we need to know is which segment on this string actually corresponds to a particular structure and then also what people are interested in.",
                    "label": 0
                },
                {
                    "sent": "And that's if you want to take that a step further.",
                    "label": 0
                },
                {
                    "sent": "Is to predict something like a binary contact matrix?",
                    "label": 0
                },
                {
                    "sent": "OK, so think of this string.",
                    "label": 0
                },
                {
                    "sent": "Each position here corresponds to a particular column and also row and what the binary context matrix would actually tell us is whether two amino acids are within a certain radius of each other after the folding process of the protein, right?",
                    "label": 0
                },
                {
                    "sent": "So it would capture some of the 3D structure of the protein, and I'm not an expert in computational biology, but from what I know is if.",
                    "label": 0
                },
                {
                    "sent": "If one could accurately predict the binary contact matrix, that would actually take take us a very long way and actually making a prediction of protein structure, right?",
                    "label": 1
                },
                {
                    "sent": "So it's interesting to think about how can machine learning methods be used to start from, let's say, the primary sequence and then start to predict actually this contact matrix, right?",
                    "label": 0
                },
                {
                    "sent": "Then clearly the binary entries in this contract matrix will not be independent, right?",
                    "label": 0
                },
                {
                    "sent": "But will be there will be very strong dependencies between these things, right?",
                    "label": 0
                },
                {
                    "sent": "Because?",
                    "label": 0
                },
                {
                    "sent": "There's only a small number of contact matrices, for instance, they really make sense because you know not everything can be in contact with everything else at the same time.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are the types of problems that I would like to present a solution to here in a generic sense, I'm not going to solve all these problems.",
                    "label": 1
                },
                {
                    "sent": "You know, as a serious application, but I want to show you how with a few ideas for machine learning.",
                    "label": 1
                },
                {
                    "sent": "You can actually have a nice toolkit of things that you can that you can then use.",
                    "label": 0
                },
                {
                    "sent": "For many of these things OK.",
                    "label": 0
                },
                {
                    "sent": "So are there any questions so far?",
                    "label": 0
                },
                {
                    "sent": "There wasn't much content, more motivation, but.",
                    "label": 0
                },
                {
                    "sent": "Does that give you a sense of?",
                    "label": 0
                },
                {
                    "sent": "Or problems.",
                    "label": 0
                },
                {
                    "sent": "OK, so the first thing I'd like to do then is talk about multiclass classification.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, because that's a natural way to get started towards more complicated things, right?",
                    "label": 0
                },
                {
                    "sent": "Because on one hand right?",
                    "label": 0
                },
                {
                    "sent": "If you think about like predicting large label sequences and things like that right, you can think of at least in a naive way as as a problem over a very large output space of different, you know, just a very large number of possible outputs, right?",
                    "label": 0
                },
                {
                    "sent": "Namely all possible labeled sequences, let's say over a certain length.",
                    "label": 0
                },
                {
                    "sent": "Now you can think of that as just being a gigantic multiclass problem, so obviously you need some more ideas to transform it back into something tractable in something that can be learned, but so it's good to start with.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With multi class and introduce appear ideas OK?",
                    "label": 0
                },
                {
                    "sent": "So let's see what we can do on the output side.",
                    "label": 0
                },
                {
                    "sent": "If we have a case where we have K class.",
                    "label": 0
                },
                {
                    "sent": "So I'll basically present one particular approach here of setting this up as a problem that then you can solve by a variant of a perceptron algorithm or a support vector machine.",
                    "label": 1
                },
                {
                    "sent": "There are a number of different formulations, I'll just use one here because I'm not primarily interested in the multiclass classification aspect, but it's only two as a motivation too.",
                    "label": 0
                },
                {
                    "sent": "To go beyond that, OK, so here's the general setting and also notation that I will use later, so I assume there's an input SpaceX usually that can be a subspace of, you know, a certain set of subsets of R2D, let's say finite dimensional representation, or it can be something more complicated, and we have an output space Y, and in this output space really consists of K labels, where this number K is just the number of classes or categories that we have.",
                    "label": 0
                },
                {
                    "sent": "And the simplest model that we could look at if we look at linear classifiers is to take the binary case and generalize it in the way that we actually get now, a weight vector for each of the classes.",
                    "label": 0
                },
                {
                    "sent": "OK, so if I pick a particular class Yi, associate a weight vector of the same dimension as our inputs with that specific class, and then I can define a discriminate function that assigns a score to a particular input that I can plug in here.",
                    "label": 0
                },
                {
                    "sent": "And the particular class that function is indexed by that class label Y by just, you know the inner product between that weight vector and some representation that I might choose for my inputs, right?",
                    "label": 0
                },
                {
                    "sent": "So this is just a generalization of of binary classification, where in binary classification usually you wouldn't even have two weight vectors, but rather you will look at the sign of this right and make decision based on that.",
                    "label": 0
                },
                {
                    "sent": "So for convenience, I will also think of that function as a function of two arguments X&Y input and the output.",
                    "label": 0
                },
                {
                    "sent": "Now, how would we then predict target class Y star?",
                    "label": 0
                },
                {
                    "sent": "Well, we would just define why star forgiven X to be the argmax over all possible classes well, possible output of that function F of X, Y, and if we just plug it in, it just means we just get that right?",
                    "label": 0
                },
                {
                    "sent": "So we just pick the score.",
                    "label": 0
                },
                {
                    "sent": "Sorry, the class which gets the highest score and the score is defined by a weight vector that we have associated with this class, right?",
                    "label": 0
                },
                {
                    "sent": "So it's relatively simple.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we look at this geometrically, what we do is we partition the input space with hyperplanes and if we look at the boundary between two classes right, we can ask ourselves well, ignoring all the other classes for a minute, right?",
                    "label": 0
                },
                {
                    "sent": "When would be favor a class.",
                    "label": 0
                },
                {
                    "sent": "Why or where class?",
                    "label": 0
                },
                {
                    "sent": "Why prime?",
                    "label": 0
                },
                {
                    "sent": "Well, we would do this if this inner product between WY with whatever input representation we have exceeds.",
                    "label": 0
                },
                {
                    "sent": "The same inner product with a different weight vector, namely the 1 four Y prime and we can just subtract this, bring it to the left side, use the linearity of the inner product and then that's what we get, namely that we should look at the difference of the weight vector right and check the sign basically of the inner product of that and if that sign is positive then we favor Y if the sign is negative, we favor why prime?",
                    "label": 0
                },
                {
                    "sent": "If it's zero, it's basically the boundary between the two.",
                    "label": 0
                },
                {
                    "sent": "OK, now the region that is the region.",
                    "label": 0
                },
                {
                    "sent": "Where we label inputs by a particular label Y right will just be the intersection of all of these half spaces, right where we prefer.",
                    "label": 0
                },
                {
                    "sent": "Prefer why over some why prime, not just taking Y prime to be everything right?",
                    "label": 0
                },
                {
                    "sent": "Except why itself?",
                    "label": 0
                },
                {
                    "sent": "So this will then define some convex region can also be.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you can visualize it like that and that will be basically the region where you label something with a label Y.",
                    "label": 0
                },
                {
                    "sent": "So all the decision boundaries will be linear and you can also generalize this by introducing these bias terms you know be sub Y for each class right?",
                    "label": 0
                },
                {
                    "sent": "So then you would also get like an offset that's class specific.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is then the?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Samples learning algorithm that you could think of.",
                    "label": 0
                },
                {
                    "sent": "Just just using that linear representation.",
                    "label": 0
                },
                {
                    "sent": "Of the these linear discriminant functions that I've just described, so the simplest approach would be something like a perceptron learning approach, where you cycle through your data.",
                    "label": 0
                },
                {
                    "sent": "So now your data is our pairs XY, XY being the input, and why I being one of these K possible class labels.",
                    "label": 0
                },
                {
                    "sent": "You use what I've shown you before to predict the optimal label as the one that gets the maximum score, so call that why I star and then you check whether that prediction is correct, whether it corresponds to your training label and as you do in the perceptron.",
                    "label": 0
                },
                {
                    "sent": "If it is correct, then you do nothing, and if it is incorrect then you perform an update and the way we could define the updates.",
                    "label": 0
                },
                {
                    "sent": "There are a number of ways of doing this, but one way you could do it with just kind of the simplest thing.",
                    "label": 0
                },
                {
                    "sent": "Conceptually, the cleanest way.",
                    "label": 0
                },
                {
                    "sent": "Is that you say?",
                    "label": 0
                },
                {
                    "sent": "Well, you take what your prediction was, which was an incorrect prediction.",
                    "label": 0
                },
                {
                    "sent": "An from that you subtract the vector representation corresponding to your example XI and then you take the.",
                    "label": 0
                },
                {
                    "sent": "Weight vector for the correct class that you didn't predict and you add this weight vector 5 XI OK.",
                    "label": 0
                },
                {
                    "sent": "So if you think about the perceptron rule, what it would usually do?",
                    "label": 0
                },
                {
                    "sent": "It would update W to BW plus Y I * 5 XI.",
                    "label": 0
                },
                {
                    "sent": "OK, so so there would be a + 1 -- 1 sign.",
                    "label": 0
                },
                {
                    "sent": "So in the perceptual learning, if you make a mistake.",
                    "label": 0
                },
                {
                    "sent": "On a positive example, you add it.",
                    "label": 0
                },
                {
                    "sent": "If you make a.",
                    "label": 0
                },
                {
                    "sent": "Mistake on a negative example, you basically subtract it from the weight vector.",
                    "label": 0
                },
                {
                    "sent": "Here you see that you have like a contrastive rule.",
                    "label": 0
                },
                {
                    "sent": "If you think of it for the two class case, it basically boils down to the same perceptron learning.",
                    "label": 0
                },
                {
                    "sent": "OK, so you move the weight vector of the correct class towards the pattern and you move the weight vector of the incorrect class that you predicted away from the pattern OK. Now, after you do that, a number of things might happen.",
                    "label": 0
                },
                {
                    "sent": "It might for instance happen that the next time you make a prediction, what you get is neither why nor why star, but something else right?",
                    "label": 0
                },
                {
                    "sent": "And then you do an update on that right?",
                    "label": 0
                },
                {
                    "sent": "Because ultimately what you need in order not to make a mistake.",
                    "label": 0
                },
                {
                    "sent": "Is that the correct class Y is really or why I is really rated higher than every other class, right?",
                    "label": 0
                },
                {
                    "sent": "And so this update rule would basically only look at two classes at a time and update them, right?",
                    "label": 0
                },
                {
                    "sent": "And then there's variance of that where you could also update more than just the Y star.",
                    "label": 0
                },
                {
                    "sent": "But this is the simplest way to do this.",
                    "label": 0
                },
                {
                    "sent": "OK, and then you just cycle through your data and then you can actually show that assuming the data is separable in that representation.",
                    "label": 0
                },
                {
                    "sent": "Given your discriminative functions that you will also find a correct separation and you will achieve 0 training error in the end.",
                    "label": 0
                },
                {
                    "sent": "You have questions about this.",
                    "label": 0
                },
                {
                    "sent": "Is that clear?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I mean this is something right?",
                    "label": 0
                },
                {
                    "sent": "You could just hack up in Matlab.",
                    "label": 0
                },
                {
                    "sent": "In a few minutes, right?",
                    "label": 0
                },
                {
                    "sent": "And it kind of works like the perception algorithm tends to work right.",
                    "label": 0
                },
                {
                    "sent": "It gives you.",
                    "label": 0
                },
                {
                    "sent": "First interesting contender to solve this learning problem, but it's probably not the last answer.",
                    "label": 0
                },
                {
                    "sent": "OK. Then there's a number of ways to generalize this to something like support vector machine learning.",
                    "label": 1
                },
                {
                    "sent": "If this is the type of things you're into.",
                    "label": 0
                },
                {
                    "sent": "So one simple way, for instance, to move in that direction is to define a separate.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And margin for an example OK.",
                    "label": 0
                },
                {
                    "sent": "So how would you define a separation margin for an example, right?",
                    "label": 1
                },
                {
                    "sent": "So usually the margin being positive means that you predict the correct output, right?",
                    "label": 0
                },
                {
                    "sent": "But then you will say, oh, you know in general you know just interested in making a correct prediction, but also you want to achieve a certain separation margin between that correct prediction and the next best one.",
                    "label": 0
                },
                {
                    "sent": "So the way that translates into a definition of a margin is as follows, right?",
                    "label": 0
                },
                {
                    "sent": "So you define gamma I as the margin you obtain.",
                    "label": 0
                },
                {
                    "sent": "Mean on a particular example xiy I to be the score that you give to XI using the weight vector corresponding to the correct label.",
                    "label": 0
                },
                {
                    "sent": "So this will be this.",
                    "label": 0
                },
                {
                    "sent": "So this is our F of XIY and then you will look at what score you give to all the other possibilities that are incorrect.",
                    "label": 0
                },
                {
                    "sent": "Right and the margin will be determined to be the minimum of such difference.",
                    "label": 0
                },
                {
                    "sent": "To, uh, why?",
                    "label": 0
                },
                {
                    "sent": "That is an incorrect Y, and that means basically that if you know the minimum overall possible wise, it means that we should choose this part in a way that we use.",
                    "label": 0
                },
                {
                    "sent": "Why had which is the arc Max over all possible incorrect wise in that fashion?",
                    "label": 0
                },
                {
                    "sent": "Here, right?",
                    "label": 0
                },
                {
                    "sent": "So basically I would show you also you know diagram to explain this you take.",
                    "label": 0
                },
                {
                    "sent": "The Y hat that is the best, which is different from my eye, right?",
                    "label": 0
                },
                {
                    "sent": "It could either be the best if it is even better than why I if Y is the best it is.",
                    "label": 0
                },
                {
                    "sent": "It will be basically the second best solution that you have and you will define the margin to be the difference in score between the two.",
                    "label": 0
                },
                {
                    "sent": "Note that if this is positive then it means that you know you'll score for why I will be higher than the score for whoever achieves the maximum score over the rest, so will also be higher than all the rest.",
                    "label": 0
                },
                {
                    "sent": "Right, if you pick the maximum there.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So here is, here's a simple illustration for that.",
                    "label": 0
                },
                {
                    "sent": "So the Y value here.",
                    "label": 0
                },
                {
                    "sent": "The hate is kind of the score.",
                    "label": 0
                },
                {
                    "sent": "The circles are the labels if you like, and the filled circle is the correct one.",
                    "label": 0
                },
                {
                    "sent": "OK, so we look at different labels and their score, and there are three cases that are outlined here.",
                    "label": 0
                },
                {
                    "sent": "So K is 1 would be 1 where the correct label gets the highest score, and there's a certain margin that.",
                    "label": 0
                },
                {
                    "sent": "You also want to obtain, let's say maybe it's this right margin of.",
                    "label": 0
                },
                {
                    "sent": "I don't know one point, something, and that's a case where or.",
                    "label": 0
                },
                {
                    "sent": "Let's say you want a margin of 1 where you would indeed achieve a margin of one.",
                    "label": 0
                },
                {
                    "sent": "In fact, a little bit more in the way you would measure the margin is the difference in score between this and the next best example.",
                    "label": 0
                },
                {
                    "sent": "This second best, which would be this one here, right?",
                    "label": 0
                },
                {
                    "sent": "If this one weren't here, then you would measure the margin to the circle here, right then you can have a case where.",
                    "label": 0
                },
                {
                    "sent": "The correct label still comes out on top, but it doesn't have sufficient margin to the next best one.",
                    "label": 0
                },
                {
                    "sent": "So this is here.",
                    "label": 0
                },
                {
                    "sent": "The difference in score between this and this is less than, let's say, one here and so then you still classify correctly, But the margin is insufficient, right?",
                    "label": 0
                },
                {
                    "sent": "And if you look at case here where actually something else comes out on top and the correct one is down here, then your margin actually turns out to be negative.",
                    "label": 0
                },
                {
                    "sent": "And that's obviously bad, right?",
                    "label": 0
                },
                {
                    "sent": "So this is so I don't know.",
                    "label": 0
                },
                {
                    "sent": "Have you already seen this?",
                    "label": 0
                },
                {
                    "sent": "But this is basically a way that you can introduce a margin in a multiclass type of decision setting without necessarily thinking about things being on the other side of the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "OK, there's no sort of other side of the hyperplane.",
                    "label": 0
                },
                {
                    "sent": "This is not more simply about the distance between the top or the correct one, and the next best one.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so then you can.",
                    "label": 0
                },
                {
                    "sent": "You can basically take put the two things together.",
                    "label": 0
                },
                {
                    "sent": "So what I talked about on the perceptron learning and then the.",
                    "label": 0
                },
                {
                    "sent": "Basically what I just said about the margin and come up with a large margin formulation for multiclass problems.",
                    "label": 0
                },
                {
                    "sent": "So this is all well known.",
                    "label": 0
                },
                {
                    "sent": "So basically you think of your weight vector as being a concatenation of all the weight vectors that you've defined for the different classes, so this would be transposition.",
                    "label": 0
                },
                {
                    "sent": "OK, so you take all your column vectors and just stack them together.",
                    "label": 0
                },
                {
                    "sent": "And then you do the usual thing.",
                    "label": 0
                },
                {
                    "sent": "So if you do a soft margin SVM, you minimize a quadratic objective in W. Namely W just enters in the norm of W. So note that the norm of W Now basically is the sum of the norms of the individual weight vectors.",
                    "label": 0
                },
                {
                    "sent": "If you want to think about it in these terms because it's all stacked together.",
                    "label": 0
                },
                {
                    "sent": "Can you add a term for the slack variables?",
                    "label": 0
                },
                {
                    "sent": "OK, there should be actually a C here.",
                    "label": 0
                },
                {
                    "sent": "Like usual.",
                    "label": 0
                },
                {
                    "sent": "You can wait this differently and the margin constraint the way you would define it now is he would just say OK, I want to minimize that with regard to W and size such that the margin that I obtained on a particular example is at least one or whatever.",
                    "label": 0
                },
                {
                    "sent": "I've chosen this slack variable to be to lower their modern a bit, right?",
                    "label": 0
                },
                {
                    "sent": "So in the in the.",
                    "label": 0
                },
                {
                    "sent": "Separable case right where you wouldn't have a soft margin.",
                    "label": 0
                },
                {
                    "sent": "This would go away here and then.",
                    "label": 0
                },
                {
                    "sent": "This would go away.",
                    "label": 0
                },
                {
                    "sent": "You would actually require that gamma.",
                    "label": 0
                },
                {
                    "sent": "I has to be greater equal to 1 as one of the constraints and then in the soft margin case you add these slack variables that need to be non negative and you add a penalty term here right?",
                    "label": 0
                },
                {
                    "sent": "So this is not a, this is not a quadratic program though and that becomes important not so much for multiclass problem but it will become much more important when we talk about structured classification structure prediction.",
                    "label": 0
                },
                {
                    "sent": "Is that if we look at this now here right?",
                    "label": 0
                },
                {
                    "sent": "This constraint is actually constrained that involves a maximum, right?",
                    "label": 0
                },
                {
                    "sent": "Because we had defined the constraint in a way that we said, well, this is the score of the correct class.",
                    "label": 0
                },
                {
                    "sent": "And what is the score of the second best?",
                    "label": 0
                },
                {
                    "sent": "Sorry, the.",
                    "label": 0
                },
                {
                    "sent": "The one that gets the highest score among the incorrect ones.",
                    "label": 0
                },
                {
                    "sent": "Right, and so we have this maximum, but we can basically expand this, right?",
                    "label": 0
                },
                {
                    "sent": "So this one constraint, because basically, as we've already seen, if we get the get a certain margin right separation margin between the correct one and the best incorrect one.",
                    "label": 0
                },
                {
                    "sent": "Then clearly we get at least that margin.",
                    "label": 0
                },
                {
                    "sent": "Also for the other incorrect ones, right?",
                    "label": 0
                },
                {
                    "sent": "Because by definition, right?",
                    "label": 0
                },
                {
                    "sent": "They're not the ones among the incorrect one that get the high score.",
                    "label": 0
                },
                {
                    "sent": "So basically we can just expand it as follows that.",
                    "label": 0
                },
                {
                    "sent": "That difference between that we simply require that the difference between the score we're getting for the correct class and what we're getting for any Y that is different from the correct clause.",
                    "label": 0
                },
                {
                    "sent": "That that difference in score needs to be at least one or one minus something that we subtract using this library.",
                    "label": 0
                },
                {
                    "sent": "Books I I OK so is this equivalence?",
                    "label": 0
                },
                {
                    "sent": "Is that clear to anyone?",
                    "label": 0
                },
                {
                    "sent": "So if we you know basically if we go to this picture here right?",
                    "label": 0
                },
                {
                    "sent": "It just basically means that now we're saying, OK, we want the difference in score between these two to be greater equal 1 and these two and these two and these two write an if we have many more than we get many more equations.",
                    "label": 0
                },
                {
                    "sent": "So The thing is that here.",
                    "label": 0
                },
                {
                    "sent": "You know, we just have one equation that's nonlinear because it involves the Max.",
                    "label": 0
                },
                {
                    "sent": "We can just explicitly unroll this maximum and then we get lots of linear constraints.",
                    "label": 0
                },
                {
                    "sent": "Namely, we get as many constraints as we have classes, basically minus one, and then we get this constraint here.",
                    "label": 0
                },
                {
                    "sent": "So the total number of constraints.",
                    "label": 0
                },
                {
                    "sent": "That we would be getting in our in our power quadratic program in the end.",
                    "label": 0
                },
                {
                    "sent": "Would be N the number of samples that we have times the cardinality of the output space.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "If we have a relatively small number of classes, then we might be able to handle this if you know the cardinality of Y is several 10 thousands or so, as you know could be in a problem with a large taxonomy, then already this becomes a bit nasty to deal with explicitly.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I'd like to move from multiclass classification to really the core of this talk, which is too strong to do structured prediction.",
                    "label": 0
                },
                {
                    "sent": "And the reason why I talked about multiclass classification in the 1st place is, as I said before, that it can be.",
                    "label": 0
                },
                {
                    "sent": "You can think of as an Eve way of thinking about structured prediction.",
                    "label": 0
                },
                {
                    "sent": "Namely, you can think of, let's say a sentence like this.",
                    "label": 0
                },
                {
                    "sent": "The dog chased the cat an then you can imagine you look at all possible parse trees that you could build on top of that you know the ones that are compatible with let's say the production rules that you find define your grammar and you.",
                    "label": 0
                },
                {
                    "sent": "You can just think of all of them as being classes of their own right, and imagine that all our training examples would just be sentences of the same length for simplicity, right?",
                    "label": 0
                },
                {
                    "sent": "Then you could build all kinds of trees on top of an English sentence of five words, save an each.",
                    "label": 0
                },
                {
                    "sent": "Each tree would just have a class label, right?",
                    "label": 0
                },
                {
                    "sent": "So then basically we could apply multiclass methods, but obviously it's not that easy.",
                    "label": 0
                },
                {
                    "sent": "And here are the some of the challenges.",
                    "label": 1
                },
                {
                    "sent": "The four main challenges that we actually need to overcome.",
                    "label": 0
                },
                {
                    "sent": "To really come to a practical solution.",
                    "label": 0
                },
                {
                    "sent": "So first of all, we need a more compact representation, right?",
                    "label": 0
                },
                {
                    "sent": "So we cannot basically introduce right.",
                    "label": 0
                },
                {
                    "sent": "If you have an exponential number or even super exponential number in the length of the sentence.",
                    "label": 0
                },
                {
                    "sent": "Number of trees here number of classes right?",
                    "label": 0
                },
                {
                    "sent": "You cannot introduce like a weight vector for every possible tree, right?",
                    "label": 0
                },
                {
                    "sent": "That seems like it blows up the representation also in that representation you can learn anything right?",
                    "label": 0
                },
                {
                    "sent": "Because you need to be able to generalize across different outputs, right?",
                    "label": 0
                },
                {
                    "sent": "If you treat every output as just a class of its own, then basically you would just need to see each of these trees a couple of times in your training data to actually learn something about that specific tree, right?",
                    "label": 0
                },
                {
                    "sent": "So that is not feasible.",
                    "label": 0
                },
                {
                    "sent": "We need to actually look at features of these trees, right properties that these trees might have and an adequate representation.",
                    "label": 0
                },
                {
                    "sent": "How we can capture that.",
                    "label": 0
                },
                {
                    "sent": "The second is if you do this, you always have to keep in mind that you know in multiclass classification and we will see this also problem in structured prediction.",
                    "label": 0
                },
                {
                    "sent": "That there's this dog, Max, right?",
                    "label": 0
                },
                {
                    "sent": "So we've set up this discriminant function.",
                    "label": 0
                },
                {
                    "sent": "It assigns scores for a given input to every possible class, but in order to make the optimal prediction, we need to find the best Y.",
                    "label": 0
                },
                {
                    "sent": "That's actually that gets the high score, and that means if the number of classes grows and is very large, then search might become a problem because we no longer might no longer be able to do do it exhaustively, right?",
                    "label": 0
                },
                {
                    "sent": "Certainly in parsing you cannot search exhaustively through your trees.",
                    "label": 0
                },
                {
                    "sent": "The third is we also need some type of better error metrics, right?",
                    "label": 0
                },
                {
                    "sent": "So usually in multiclass classification right?",
                    "label": 0
                },
                {
                    "sent": "In binary classification you use the classification loss often right as a typical error metric that you use a nematic class classification right?",
                    "label": 0
                },
                {
                    "sent": "You might also want to use something like a loss.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you have a matrix of the loss that you incur if your prediction is why when the correct label is some other Whitehead.",
                    "label": 0
                },
                {
                    "sent": "But for structured classification structured prediction, we need we need much better error metric.",
                    "label": 1
                },
                {
                    "sent": "So for instance.",
                    "label": 0
                },
                {
                    "sent": "Right in this tree, if we were to predict a tree that is very similar to the correct one right?",
                    "label": 0
                },
                {
                    "sent": "We want to say, well, you know, this is not the correct one, but at least it's a good guess, right?",
                    "label": 0
                },
                {
                    "sent": "It comes close somehow to the correct one, and maybe a tree that is completely off, right?",
                    "label": 0
                },
                {
                    "sent": "We want to say, well, this is really bad if that's your prediction, because it doesn't share any of the constituents with the original tree.",
                    "label": 0
                },
                {
                    "sent": "Let's say right.",
                    "label": 0
                },
                {
                    "sent": "So if we're predicting structures, we're facing inherently that problem, right?",
                    "label": 0
                },
                {
                    "sent": "That is very unlikely that we get it completely right in most cases, right?",
                    "label": 0
                },
                {
                    "sent": "So then we want to have a more fine grained notion of how wrong we are.",
                    "label": 0
                },
                {
                    "sent": "So we need some error metric and it's important to have a method that can actually take that into account.",
                    "label": 0
                },
                {
                    "sent": "And then also we would like the training time to be sub linear in the number of outputs.",
                    "label": 0
                },
                {
                    "sent": "OK, so it means that yes, if we're talking about trees, and if that number is exponential in the sentence length.",
                    "label": 0
                },
                {
                    "sent": "Then definitely we don't want the training time to be exponential in the sentence links.",
                    "label": 0
                },
                {
                    "sent": "We want it to be a polynomial of the sentence length in some form, right?",
                    "label": 0
                },
                {
                    "sent": "It needs to be still efficient, so these are the things we need to basically need to look at to move to straw.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Check predictions OK.",
                    "label": 0
                },
                {
                    "sent": "So here are the things that I will talk about in the rest of this methodological part of my presentation.",
                    "label": 0
                },
                {
                    "sent": "So how do we get a compact representation?",
                    "label": 1
                },
                {
                    "sent": "So the answer will be, and this will be the next thing.",
                    "label": 0
                },
                {
                    "sent": "To use input output, feature functions and or, you can also encode these things using kernels that ensure generalization capabilities across inputs and outputs.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Think of it right.",
                    "label": 0
                },
                {
                    "sent": "So in machine learning, if you do something like binary classification right, you would extract features that allow you to.",
                    "label": 0
                },
                {
                    "sent": "Kind of define a good similarity measure in your input space, so to speak, right?",
                    "label": 0
                },
                {
                    "sent": "So that you can actually learn so that right you capture certain attributes or properties of your input that are correlated with that input being in class plus one or minus one.",
                    "label": 0
                },
                {
                    "sent": "Or if you have a multiclass problem, right?",
                    "label": 0
                },
                {
                    "sent": "If we have an output now, that is also very complicated.",
                    "label": 0
                },
                {
                    "sent": "What we basically need is we need to understand how certain aspects of the input.",
                    "label": 0
                },
                {
                    "sent": "Relat to certain aspects of the output.",
                    "label": 0
                },
                {
                    "sent": "OK, so a particular piece of the sentence, how it relates to let's say particular constituent in your past re and things like that.",
                    "label": 0
                },
                {
                    "sent": "OK, so we need to decompose both or to represent both input and output over a suitable set of features.",
                    "label": 0
                },
                {
                    "sent": "And these things need to be jointly tide together.",
                    "label": 0
                },
                {
                    "sent": "OK, because what we were given is an input right?",
                    "label": 0
                },
                {
                    "sent": "And we have to predict an output.",
                    "label": 0
                },
                {
                    "sent": "So we always needs to be.",
                    "label": 0
                },
                {
                    "sent": "A bridge between something in the input and something in the output that we look at, so this will be critical when you apply this method.",
                    "label": 0
                },
                {
                    "sent": "Also, for practical problem, how do you design that feature function?",
                    "label": 0
                },
                {
                    "sent": "Pretty much as it is.",
                    "label": 0
                },
                {
                    "sent": "Also, I guess you know feature kernel engineering or feature engineering in more standard learning certain settings is also crucial.",
                    "label": 0
                },
                {
                    "sent": "Then we need to watch out for this efficient search problem, right?",
                    "label": 0
                },
                {
                    "sent": "As I said before and what what I will look at here is also something that is representation specific, but that can be often thought of as a black box mechanism that you can plug into your learning system.",
                    "label": 0
                },
                {
                    "sent": "Anne, this is.",
                    "label": 0
                },
                {
                    "sent": "A mechanism that actually picks you.",
                    "label": 0
                },
                {
                    "sent": "That allows you to pick the output that receives the highest score dependent on what application we are looking at.",
                    "label": 1
                },
                {
                    "sent": "This can be based on dynamic programming or some sort of a min flow type of algorithm.",
                    "label": 0
                },
                {
                    "sent": "3 Max flow type of algorithm in the network and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "OK, so it depends and we will see different examples of how you do this.",
                    "label": 0
                },
                {
                    "sent": "The third is I just mentioned before is a refined error metric where you want to use application specific loss functions and build it right into the problem formulation, right?",
                    "label": 0
                },
                {
                    "sent": "You don't want to.",
                    "label": 0
                },
                {
                    "sent": "Learn something without knowing what the prediction loss is.",
                    "label": 0
                },
                {
                    "sent": "You know and then just measuring it after the fact.",
                    "label": 0
                },
                {
                    "sent": "You need to incorporate it into your learning algorithm from the start, and then as far as training algorithm is concerned, the one that I will really focus on here is an optimization method.",
                    "label": 0
                },
                {
                    "sent": "That basically uses a cutting plane algorithm, or if you think of it in the dual variable selection algorithm, to incrementally solve an approximation of relaxation to the original optimization problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so these will be the things we need to talk about, yes?",
                    "label": 0
                },
                {
                    "sent": "As for us like few lectures, we had a lot of.",
                    "label": 0
                },
                {
                    "sent": "We saw a lot of ways to solve this kind of problem using plastic method, right?",
                    "label": 0
                },
                {
                    "sent": "The last one, yeah, why are you not answering any probabilistic approach?",
                    "label": 0
                },
                {
                    "sent": "You mean for the entire problem or just for the letter?",
                    "label": 0
                },
                {
                    "sent": "About whatever you want.",
                    "label": 0
                },
                {
                    "sent": "Right, well, but it's a question on a different level, right?",
                    "label": 0
                },
                {
                    "sent": "So I mean for the optimization methods, we basically will see that in some cases they just correspond to things that you would also using probabilistic inference.",
                    "label": 0
                },
                {
                    "sent": "Whether you do compute something in expectation or whether you want to compute a map, maximum configuration or sub configuration of your space, right?",
                    "label": 0
                },
                {
                    "sent": "This can you can plug in here.",
                    "label": 0
                },
                {
                    "sent": "The other question would be why are you not doing it in a fully Bayesian sense or what not right?",
                    "label": 0
                },
                {
                    "sent": "Maybe we can then talk and talk in the end because I haven't been here for the other lectures right?",
                    "label": 0
                },
                {
                    "sent": "How it would compare.",
                    "label": 0
                },
                {
                    "sent": "To these other methods, but you know, I think this approach has some nice advantages, as we will see sparseness of the representation is 1 an.",
                    "label": 0
                },
                {
                    "sent": "It's just, I just think it's a very effective way.",
                    "label": 0
                },
                {
                    "sent": "It leads to very practical solutions.",
                    "label": 0
                },
                {
                    "sent": "You know we would need to compare if Bayesian methods are really better then maybe that's something to consider here, but.",
                    "label": 0
                },
                {
                    "sent": "So maybe we can discuss this in the end, because I haven't really talked about the salute.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alright, so let me at least before we have a little break in five or 10 minutes, introduce the basic setting here that I want to work in.",
                    "label": 0
                },
                {
                    "sent": "So the way I want to set this up is as follows.",
                    "label": 0
                },
                {
                    "sent": "As I said before, so this deals now mainly with this representation.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Spectr I want to define and this will be in a problem specific application specific manner of function Phi that extract features from input output pairs an it will extract M of those so it'll be an M dimensional vector which encode.",
                    "label": 0
                },
                {
                    "sent": "As I said, a certain combination of aspects of inputs and outputs, and then I will define a function F which also called compatibility function, pretty much as before as a function that's parameterized by some weight vector.",
                    "label": 1
                },
                {
                    "sent": "And takes an input and output pair as arguments and then outputs basically a linear function of the inner product of this W with this feature representation.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Note that if you in the multiclass setting right, you could.",
                    "label": 0
                },
                {
                    "sent": "Basically you can map this to the multiclass setting by taking the W vectors being a stacked vector, one vector for each class, and then encoding basically the class Y by copying X to the right position.",
                    "label": 0
                },
                {
                    "sent": "So then when you compute the inner product you basically get something like.",
                    "label": 0
                },
                {
                    "sent": "What we had before in the multiclass setting.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can do that as a little exercise if you want.",
                    "label": 0
                },
                {
                    "sent": "And then prediction is done.",
                    "label": 0
                },
                {
                    "sent": "You know, in a very straightforward way, assuming that you can actually do the arc Max.",
                    "label": 0
                },
                {
                    "sent": "But if you could write then you would just choose forgiven X the Y that maximizes that function.",
                    "label": 0
                },
                {
                    "sent": "And you can also see that.",
                    "label": 1
                },
                {
                    "sent": "Binary classification I've already indicated that for multiclass classification, but a binary classification.",
                    "label": 0
                },
                {
                    "sent": "For instance, if you define that function file to be some feature representation of, the X is multiplied by this plus 1 -- 1 label right?",
                    "label": 1
                },
                {
                    "sent": "Then actually what you get here is just the usual sign rule that you use in linear classification OK?",
                    "label": 0
                },
                {
                    "sent": "So let me.",
                    "label": 0
                },
                {
                    "sent": "Let me give you a few few examples.",
                    "label": 0
                },
                {
                    "sent": "One example in particular to really illustrate that man like to take some time to do that.",
                    "label": 0
                },
                {
                    "sent": "So it's really clear what goes on here.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is an example from information extraction, so I have a sentence in reality touch panel systems, capitalize Dad blah blah blah something before and after, so it's part of a sentence.",
                    "label": 0
                },
                {
                    "sent": "And what I've shown here is different possible outputs.",
                    "label": 0
                },
                {
                    "sent": "OK, so here is actually the correct output for this information extraction task of finding the named entities.",
                    "label": 0
                },
                {
                    "sent": "So if Touchpanel systems is indeed a company name and nothing else is a name.",
                    "label": 0
                },
                {
                    "sent": "The correct output would be the one that takes these three words together and says, you know this is a company name and all the rest is just blank, right?",
                    "label": 0
                },
                {
                    "sent": "Is not a name OK?",
                    "label": 0
                },
                {
                    "sent": "And what I've shown here now is also other possible outputs, right?",
                    "label": 0
                },
                {
                    "sent": "Three additional ones among the exponential number of possibilities that you have.",
                    "label": 0
                },
                {
                    "sent": "So here's for instance and output where we would actually believe that only touch panel is the name of the company and systems is not part of the name.",
                    "label": 0
                },
                {
                    "sent": "Here's a hypothesis where we would think that reality is a location and touch panel systems capitalized is the company name.",
                    "label": 1
                },
                {
                    "sent": "Here we have a hypothesis that things reality touches a location, and there's nothing else OK?",
                    "label": 0
                },
                {
                    "sent": "So you know some of these are obviously for us, right?",
                    "label": 0
                },
                {
                    "sent": "Less likely, and others are more likely, but clearly you know a machine learning algorithm first needs to have some representation that of appropriate features that we can even start thinking about a problem like this.",
                    "label": 0
                },
                {
                    "sent": "So here are some features that you might want to consider OK.",
                    "label": 0
                },
                {
                    "sent": "So future number one would be location segment following the word in right?",
                    "label": 1
                },
                {
                    "sent": "So imagine there's no.",
                    "label": 0
                },
                {
                    "sent": "You know bunch of linguists that sit down over lunch break and they think about you know what are the types of features that you might want to consider, right?",
                    "label": 0
                },
                {
                    "sent": "So you know, you might also use just feature templates and then just include all of them.",
                    "label": 0
                },
                {
                    "sent": "But let's just look at individual features here for concreteness, right?",
                    "label": 0
                },
                {
                    "sent": "So basically location segment following the word in, you know, because maybe you believe that often right locations.",
                    "label": 0
                },
                {
                    "sent": "Follow indeed proposition like in.",
                    "label": 0
                },
                {
                    "sent": "So you would basically look at this an in this example here in the representation of Y.",
                    "label": 0
                },
                {
                    "sent": "That feature would not occur, at least not in this part, right?",
                    "label": 0
                },
                {
                    "sent": "So the feature would be zero 4X combined with Y prime.",
                    "label": 0
                },
                {
                    "sent": "The future will be 0 right?",
                    "label": 0
                },
                {
                    "sent": "And if I look at X, the pair XY double prime.",
                    "label": 0
                },
                {
                    "sent": "If I plug this in here right into my feature representation, I will have it here.",
                    "label": 0
                },
                {
                    "sent": "And I actually counted OK, so I'll have it once and in Y three prime.",
                    "label": 0
                },
                {
                    "sent": "I also have it once.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "This specific entry in this vector that corresponds to that feature right for YW prime and while triple prime it will be one for the others it will be 0.",
                    "label": 1
                },
                {
                    "sent": "Then we can look at features, for instance, that look at whether certain words occur in a in a segment that is labeled with a particular label, for instance as a company name.",
                    "label": 0
                },
                {
                    "sent": "So let's say you know word like systems might actually be pretty common in a company name, so.",
                    "label": 0
                },
                {
                    "sent": "So you can see here.",
                    "label": 0
                },
                {
                    "sent": "Then if we think of that as a binary feature, or then we actually counted over the string that that would be a feature that would be active for XX, Y or X, Y double prime.",
                    "label": 0
                },
                {
                    "sent": "It wouldn't be active for the others.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And if we could also have a feature like the word systems occurring at the end of the company name, so this then would only qualify with this.",
                    "label": 1
                },
                {
                    "sent": "Why output here the word capitalized following following a company name segment right?",
                    "label": 1
                },
                {
                    "sent": "That would also be feature that would only be active here.",
                    "label": 0
                },
                {
                    "sent": "And then you might also have features that are independent of the output, so we're effectively you know if you have this function, Phi X, Y, you can always choose to ignore X or Y for that matter, but if you ignore why, then it's not particularly useful because it doesn't help you to discriminate between different possible outputs, whereas you can always choose to ignore the input and just measure something that only measures kind of dependencies between different aspects of the output.",
                    "label": 0
                },
                {
                    "sent": "So, for instance, you could say have a feature like a location segment being followed by a company segment.",
                    "label": 1
                },
                {
                    "sent": "You know, maybe this is something you often see, so you want to capture it in a feature.",
                    "label": 0
                },
                {
                    "sent": "So this is the case only here and then, or something like company name consisting of three words.",
                    "label": 0
                },
                {
                    "sent": "So this would be.",
                    "label": 0
                },
                {
                    "sent": "This would be a feature that fires sort of here and not for the others, right?",
                    "label": 0
                },
                {
                    "sent": "So so these are all features that you could look at.",
                    "label": 0
                },
                {
                    "sent": "And basically I hope now you get some understanding for this particular problem.",
                    "label": 0
                },
                {
                    "sent": "You know what is behind these guys here.",
                    "label": 0
                },
                {
                    "sent": "These fee of X, Y.",
                    "label": 0
                },
                {
                    "sent": "What types of things you could engineer in there?",
                    "label": 0
                },
                {
                    "sent": "OK then, let me just basically close this part.",
                    "label": 0
                },
                {
                    "sent": "By just showing you again, you know then what you sort of finally get right, so we were considering these.",
                    "label": 0
                },
                {
                    "sent": "I guess six different features.",
                    "label": 0
                },
                {
                    "sent": "You know now we basically just go through it and we collect all the features.",
                    "label": 0
                },
                {
                    "sent": "These count features that happen to be binary here because we don't have multiple events in the same sequence.",
                    "label": 0
                },
                {
                    "sent": "So basically you know X comfy of X, Y will have you know feature 234 and six active.",
                    "label": 0
                },
                {
                    "sent": "So we will basically get this binary vector to represent that.",
                    "label": 0
                },
                {
                    "sent": "If these are our first 6 features.",
                    "label": 0
                },
                {
                    "sent": "Why Prime has nothing active so it will just be the zero vector Y double prime you know has these three features active in the other three is not and the triple prime only has this first feature active.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK and and then basically just to give you a second example so you don't have only one example.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This one.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "So to make some kind of a chain or.",
                    "label": 0
                },
                {
                    "sent": "Take all the features, all the possible features.",
                    "label": 0
                },
                {
                    "sent": "Then using the training set to find out whether these features are common or not, and then eliminate several of them and.",
                    "label": 0
                },
                {
                    "sent": "They will consider like do some feature extraction from the crucifixion.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so yeah, definitely they are at heart, right?",
                    "label": 0
                },
                {
                    "sent": "That's why I said right.",
                    "label": 0
                },
                {
                    "sent": "Imagine like a few linguists over over lunch break, right?",
                    "label": 0
                },
                {
                    "sent": "Coming up with these features.",
                    "label": 0
                },
                {
                    "sent": "So I think we need.",
                    "label": 0
                },
                {
                    "sent": "Usually we need to distinguish.",
                    "label": 0
                },
                {
                    "sent": "I would say the features that just deal with output part right and correlations there.",
                    "label": 0
                },
                {
                    "sent": "Usually you might have.",
                    "label": 0
                },
                {
                    "sent": "You might have domain knowledge that really tells you right what type of dependencies you want to capture, what types of things to look for.",
                    "label": 0
                },
                {
                    "sent": "If we look at the input side right and how that correlates with with the outputs, right?",
                    "label": 0
                },
                {
                    "sent": "So for instance, which words are likely to occur within a particular segment, right?",
                    "label": 0
                },
                {
                    "sent": "You could just take all possible words, right?",
                    "label": 0
                },
                {
                    "sent": "That's why I said it could be like a template and then indeed you might need to use some feature induction or feature selection methods, right?",
                    "label": 0
                },
                {
                    "sent": "Depending on how you actually train it right to basically prune back and limit it to a smaller number of features.",
                    "label": 0
                },
                {
                    "sent": "But basically that yeah, so so here.",
                    "label": 0
                },
                {
                    "sent": "This is not the way as I said, you're not actually going through.",
                    "label": 0
                },
                {
                    "sent": "You know feature one by one.",
                    "label": 0
                },
                {
                    "sent": "So for instance, you know exactly hand picking system and systems and so on and so forth, right?",
                    "label": 0
                },
                {
                    "sent": "That's why I said this.",
                    "label": 0
                },
                {
                    "sent": "Think of a template right template that does that.",
                    "label": 0
                },
                {
                    "sent": "A template would be a word occurring in a name of a certain type, right?",
                    "label": 1
                },
                {
                    "sent": "This could be a template and maybe you do this for all words, or maybe only four words that in your training data are frequent enough to actually show up there, right?",
                    "label": 0
                },
                {
                    "sent": "So there's certainly lots of.",
                    "label": 0
                },
                {
                    "sent": "Pruning and a word occurring at the end of a of a company name or at the end right.",
                    "label": 1
                },
                {
                    "sent": "The template might be that indeed you might want to have features that not just tell you it's in the name, but then it's at the end or at the beginning, right?",
                    "label": 0
                },
                {
                    "sent": "Because this can be important.",
                    "label": 0
                },
                {
                    "sent": "So in that sense, the template, I think is designed manually, and then you use automatic techniques to prune it.",
                    "label": 0
                },
                {
                    "sent": "Prune it down.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or in the example here.",
                    "label": 0
                },
                {
                    "sent": "So this is this is for the parsing case right where?",
                    "label": 0
                },
                {
                    "sent": "You can basically just take the context free grammar with the rules that are given to you.",
                    "label": 0
                },
                {
                    "sent": "Right and just directly translate that into a feature representation and namely what you do here is just you basically look at right.",
                    "label": 0
                },
                {
                    "sent": "So we need to.",
                    "label": 0
                },
                {
                    "sent": "We are given an X and we hypothesize some Y and we want to know what is the corresponding feature representation.",
                    "label": 0
                },
                {
                    "sent": "So what we could do is basically look at the number of times we've applied a certain production rule.",
                    "label": 0
                },
                {
                    "sent": "So we've done S 2 N PvP ones, so this would get account of one in this tree.",
                    "label": 0
                },
                {
                    "sent": "We would have a production rule.",
                    "label": 0
                },
                {
                    "sent": "Let's say that says we would just have a verb phrase that didn't apply, so that zero.",
                    "label": 0
                },
                {
                    "sent": "There's a production praise that where a noun phrase goes becomes a noun.",
                    "label": 0
                },
                {
                    "sent": "OK, so we see this here and here.",
                    "label": 0
                },
                {
                    "sent": "So we have that twice, and we have a verb phrase moving to a verb and a verb phrase.",
                    "label": 0
                },
                {
                    "sent": "So we see this ones here and so on and so forth, right?",
                    "label": 0
                },
                {
                    "sent": "So this is all the parts that have to do with the production part again.",
                    "label": 0
                },
                {
                    "sent": "So these are things that are independent of the input, right?",
                    "label": 0
                },
                {
                    "sent": "They model like which types of trees?",
                    "label": 0
                },
                {
                    "sent": "Are likely which types of productions are likely and then when you come to the level of where you actually go to the terminals here, write the actual words.",
                    "label": 0
                },
                {
                    "sent": "That's then where the representation becomes lexicalized right, and probably much more high dimensional, because now you actually need to know, right?",
                    "label": 0
                },
                {
                    "sent": "Something like you know the probability to go from a noun and replace it by some actual word like Boeing, right or arrows, or Seattle and so on and so forth, right?",
                    "label": 0
                },
                {
                    "sent": "So you also can encode that here, but you can hear you can imagine that this is a.",
                    "label": 0
                },
                {
                    "sent": "A very high dimensional representation that you're getting there, right?",
                    "label": 0
                },
                {
                    "sent": "Yes, I think in some cases, maybe from certain trees cannot correspond to the input string at cold.",
                    "label": 0
                },
                {
                    "sent": "So do you some right quick processing because like some dank, I mean some sequence of strings cannot be generated by your grammar, right?",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So we will see later how that how that comes in.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "The feature representation in principle you could just do on anything, right?",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter whether.",
                    "label": 0
                },
                {
                    "sent": "I mean whether it makes sense or not, right?",
                    "label": 0
                },
                {
                    "sent": "I guess here just the branching.",
                    "label": 0
                },
                {
                    "sent": "It just needs to workout such that you have a certain number of non terminals in the end that is really the length of the sentence that you're looking at, right?",
                    "label": 0
                },
                {
                    "sent": "And and above, it's just you just need to apply the rules right?",
                    "label": 0
                },
                {
                    "sent": "But you could you could you know, locate it, you might think is a noun and it was generated from a noun phrase or something.",
                    "label": 0
                },
                {
                    "sent": "So maybe you know grammatically all of this of course is nonsensical, but that's what you want to learn, right?",
                    "label": 0
                },
                {
                    "sent": "What is grammatical and what is not so?",
                    "label": 0
                },
                {
                    "sent": "So basically an if you allow right that basically every.",
                    "label": 0
                },
                {
                    "sent": "Every word can really be, you know if you have rules where actually on the right hand side here for the for the actual words, right you don't constrain it.",
                    "label": 0
                },
                {
                    "sent": "Then basically you know if you have prior knowledge, of course that you know that located is always a verb.",
                    "label": 0
                },
                {
                    "sent": "Let's say you know and you would never want to have a rule noun to locate it.",
                    "label": 0
                },
                {
                    "sent": "Then of course you can just roll it out right there, right?",
                    "label": 0
                },
                {
                    "sent": "You just just eliminate that rule, right?",
                    "label": 0
                },
                {
                    "sent": "But the rest is just given by the grammar right?",
                    "label": 0
                },
                {
                    "sent": "And whether you do something that doesn't make sense for the sentence that you're considering, as long as you build like a valid kind of tree, that's well formed, then that's OK. Say again sorry talking about.",
                    "label": 0
                },
                {
                    "sent": "Well, I will talk about ways to.",
                    "label": 0
                },
                {
                    "sent": "Well, it depends.",
                    "label": 0
                },
                {
                    "sent": "There are ways that you actually don't have to complete it explicitly, so ways of avoiding explicitly constructing it.",
                    "label": 0
                },
                {
                    "sent": "And the other problem is more about also yeah, so in that specific case here, for instance, right?",
                    "label": 0
                },
                {
                    "sent": "This is not a representation that you will explicitly generate, right?",
                    "label": 0
                },
                {
                    "sent": "That doesn't make sense.",
                    "label": 0
                },
                {
                    "sent": "So here indeed I will basically.",
                    "label": 0
                },
                {
                    "sent": "Basically this will happen through kernels, right?",
                    "label": 0
                },
                {
                    "sent": "We will look at in a product of these five vectors and see you know.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sure, but but we don't have to go through this blow up in the representation, right?",
                    "label": 0
                },
                {
                    "sent": "But will be more interesting from an algorithmic POV though, is the is actually this argmax problem right of finding?",
                    "label": 0
                },
                {
                    "sent": "Finding not just computing for a given excellent, why the representation but really forgiven X finding the Y that maximizes this linear function right in five.",
                    "label": 0
                },
                {
                    "sent": "But that's something we can talk about.",
                    "label": 0
                },
                {
                    "sent": "OK, should we take a 5 minute break here?",
                    "label": 0
                },
                {
                    "sent": "I was asked to have a break in between so we just continue at.",
                    "label": 0
                },
                {
                    "sent": "I was only 20 hours, sorry.",
                    "label": 0
                },
                {
                    "sent": "OK, well then we're sorry.",
                    "label": 0
                },
                {
                    "sent": "Then we just move on.",
                    "label": 0
                },
                {
                    "sent": "I was somehow almost looking at the Clock, but I thought I started at a different start time, OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's here's also ways, yeah, that's why everyone so tired now, I guess.",
                    "label": 0
                },
                {
                    "sent": "Here's also ways that you can actually construct these types of Maps in a way that is sometimes convenient.",
                    "label": 0
                },
                {
                    "sent": "So for instance, one thing you can do is you can define if you have a good way of defining input features that just depend on X, so you have some vector representation, let's say of X, and you have some features you care about on the Y side.",
                    "label": 0
                },
                {
                    "sent": "OK, you can just define the joint.",
                    "label": 0
                },
                {
                    "sent": "Feature map to be sort of the tensor product of these two, which basically means you will now have think of it as a multi index IJ.",
                    "label": 0
                },
                {
                    "sent": "So one feature here will be a product of the input feature and Jay thought that feature just multiplied together right?",
                    "label": 0
                },
                {
                    "sent": "This will be my Feige feature here.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing is right about about this is for instance, if we if we look at this here.",
                    "label": 0
                },
                {
                    "sent": "If we want to compute an inner product of a function that has been designed by this tensor map.",
                    "label": 0
                },
                {
                    "sent": "Trick is that basically it boils down to an inner product in that representation times an inner product in that representation.",
                    "label": 0
                },
                {
                    "sent": "OK, so it nicely separates, so this is kind of 1 answer of right if of cases where you can build this function implicitly by just saying, well, you know it's all combinations of these guys, but you don't actually need to compare.",
                    "label": 0
                },
                {
                    "sent": "Put all combinations.",
                    "label": 0
                },
                {
                    "sent": "If in the end all you need is an inner product, right?",
                    "label": 0
                },
                {
                    "sent": "So this is 1 trick.",
                    "label": 0
                },
                {
                    "sent": "The other thing that.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We will see for things like if you have a labeled sequence.",
                    "label": 0
                },
                {
                    "sent": "Is that actually you have feature functions that extract something from position T?",
                    "label": 0
                },
                {
                    "sent": "Right, this was already the features that I've shown you in detail for the information extraction task.",
                    "label": 0
                },
                {
                    "sent": "And then basically you just count them up, sort of along the sequence.",
                    "label": 0
                },
                {
                    "sent": "So if you have, you know if access the sequence X one to XT&Y is a sequence.",
                    "label": 0
                },
                {
                    "sent": "Why want to Whitey?",
                    "label": 0
                },
                {
                    "sent": "Then you extract at each position you extract some feature vector and you can just add it up.",
                    "label": 0
                },
                {
                    "sent": "So think of sort of counts of features along along different positions if you have that obviously then things are also simple.",
                    "label": 0
                },
                {
                    "sent": "So for instance in the primal if I have W and I want to multiply it with that right, I can just basically do the inner product in the sum.",
                    "label": 0
                },
                {
                    "sent": "Right, and hopefully these feature these features.",
                    "label": 0
                },
                {
                    "sent": "Here are maybe simpler, simpler to do so also when I compute an inner product between one of these vectors an for another input output pair.",
                    "label": 0
                },
                {
                    "sent": "I can also then actually compute this as a double sum if need be, so there are some tricks and how.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can do this.",
                    "label": 0
                },
                {
                    "sent": "In general, there's some right you can.",
                    "label": 0
                },
                {
                    "sent": "You can take this further and you can actually move to a reproducing kernel Hilbert space, so you don't necessarily just limit yourself to like finite dimensional representations.",
                    "label": 0
                },
                {
                    "sent": "And if you do that just to sketch that here, basically you could set it up in the following way that you have a Hilbert space of functions F, and these functions are defined basically as linear combinations over some sample set.",
                    "label": 0
                },
                {
                    "sent": "Of Point set and these points are actually is a sample of input output pairs.",
                    "label": 0
                },
                {
                    "sent": "OK, so a point here Zed is really an input output pair and then you have these coefficients.",
                    "label": 0
                },
                {
                    "sent": "Alpha, zed, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a way that you can think of.",
                    "label": 0
                },
                {
                    "sent": "You have a generating kernel and basically all the things that you do within these reproducing kernel Hilbert spaces.",
                    "label": 1
                },
                {
                    "sent": "You can also do by just basically treating.",
                    "label": 0
                },
                {
                    "sent": "The input output pair really as your input and sort of the kernel.",
                    "label": 0
                },
                {
                    "sent": "Then just instead of just dependent on a single set here, it just depends on this cross product, so this is not.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is not too deep and of course you can think of these kernels as a special case that they are defined as we define it before, basically as in a product of some feature representation that you extract.",
                    "label": 0
                },
                {
                    "sent": "OK. And and again, you know I said this.",
                    "label": 0
                },
                {
                    "sent": "We already said this before.",
                    "label": 0
                },
                {
                    "sent": "If you think in the kernel world again, right for the tensor product kernels, for instance, you basically just get a factorization over kernels that are just defined over input pairs and kernel functions that are just defined over output pairs.",
                    "label": 0
                },
                {
                    "sent": "For something like this tensor product map.",
                    "label": 1
                },
                {
                    "sent": "OK, so they have two purposes here really, so one is what you already know.",
                    "label": 0
                },
                {
                    "sent": "You know in general, from kernel machines that you know they are flexible and efficient feature extraction.",
                    "label": 1
                },
                {
                    "sent": "They often help you to avoid limit yourself too.",
                    "label": 1
                },
                {
                    "sent": "You know finite dimensional feature representations.",
                    "label": 0
                },
                {
                    "sent": "They might help in allowing you much faster learning by implicitly using the kernel functions instead of explicitly computing inner product in this context.",
                    "label": 0
                },
                {
                    "sent": "Here you also have always the problem of combining things on the input and output side, which might be a problem in terms of the combinatorics of the combinations that you actually want to look at, like in this case here, right?",
                    "label": 0
                },
                {
                    "sent": "So then, kernels by.",
                    "label": 0
                },
                {
                    "sent": "If you have a corner, for instance, that factorizes nicely like this.",
                    "label": 0
                },
                {
                    "sent": "Right then that's also an advantage on the computational side.",
                    "label": 0
                },
                {
                    "sent": "That kernel approach would give you.",
                    "label": 0
                },
                {
                    "sent": "OK, so then.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me move to the sorry I'm running a little bit slower than I thought.",
                    "label": 0
                },
                {
                    "sent": "Let me move to talk a little bit about statistical inference problems for structured prediction.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in general, if you look at it without committing to particular.",
                    "label": 0
                },
                {
                    "sent": "Learning algorithm or a particular formulation, even off the learning problem, you can look at it in the most general setting as follows.",
                    "label": 0
                },
                {
                    "sent": "You have a training sample South your training sample.",
                    "label": 0
                },
                {
                    "sent": "These are now input output pairs, right?",
                    "label": 0
                },
                {
                    "sent": "XYY, I and you have some.",
                    "label": 0
                },
                {
                    "sent": "Cost function or functional that takes as input.",
                    "label": 0
                },
                {
                    "sent": "Let's say function from the RKS.",
                    "label": 0
                },
                {
                    "sent": "And it basically measures the goodness of fit of F, right?",
                    "label": 0
                },
                {
                    "sent": "We will look at different possibilities for that.",
                    "label": 0
                },
                {
                    "sent": "And then we have some regularizer and there typically we would use basically something that depends is a simple function of the Hilbert space norm of our function F. If we have a setting like that and we want to minimize such a regularised objective function with functional here that depends on the training sample and this additional.",
                    "label": 0
                },
                {
                    "sent": "Stabalizer then we get the following represent a theorem and there's basically one thing that you need to understand here that's different from the general case.",
                    "label": 0
                },
                {
                    "sent": "So basically you can show that the minimizer of that F hat.",
                    "label": 0
                },
                {
                    "sent": "For a given sample.",
                    "label": 0
                },
                {
                    "sent": "Is a function that will can be written as a sum over all your training example.",
                    "label": 1
                },
                {
                    "sent": "OK, that's usual, but now you have a sum over all possible outputs Y of in your output space, and then you have a weight better iy.",
                    "label": 0
                },
                {
                    "sent": "So this depends on the training index.",
                    "label": 0
                },
                {
                    "sent": "Here I and on the actual some output Y.",
                    "label": 0
                },
                {
                    "sent": "Note that this is not just why I, it's you need any all of the wise basically show up in this somewhere.",
                    "label": 0
                },
                {
                    "sent": "You need to consider.",
                    "label": 0
                },
                {
                    "sent": "And then it's a kernel function.",
                    "label": 0
                },
                {
                    "sent": "Basically, this is the argument you're going to apply this to write an input output pair and the second argument is basically the pair XIY.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is if you just do this in the without output learning in the?",
                    "label": 0
                },
                {
                    "sent": "In the case that you've probably seen this, it'll just be a sum over your training points.",
                    "label": 0
                },
                {
                    "sent": "Then there will be a beta I and then it will be K blank, XI.",
                    "label": 0
                },
                {
                    "sent": "Write something like that.",
                    "label": 0
                },
                {
                    "sent": "So the interesting thing here is that the representative theorem.",
                    "label": 0
                },
                {
                    "sent": "Also tells you that the function right that you're getting by minimizing over your Hilbert space of functions has a nice finite representation.",
                    "label": 0
                },
                {
                    "sent": "Things are centered sort of on your training data, but not quite in the sense that for every input X are you also need to consider every possible Y. OK, not just.",
                    "label": 0
                },
                {
                    "sent": "Xiy I write that's not enough and we will actually see in the algorithms.",
                    "label": 0
                },
                {
                    "sent": "The main algorithm that I will talk about here.",
                    "label": 0
                },
                {
                    "sent": "You know what that really means?",
                    "label": 0
                },
                {
                    "sent": "How that comes about, right?",
                    "label": 0
                },
                {
                    "sent": "So is that clear?",
                    "label": 0
                },
                {
                    "sent": "So it means for instance, right so?",
                    "label": 0
                },
                {
                    "sent": "The sparseness of this so in general you know the representative theorem has many advantages.",
                    "label": 0
                },
                {
                    "sent": "One is certainly that you have a representation that you know cannot scale worse than the number of training points that you have, right?",
                    "label": 0
                },
                {
                    "sent": "So if I have any training points, I know that I can represent my optimal function right using N coefficients.",
                    "label": 0
                },
                {
                    "sent": "Like a linear combination of these N kernel function centered on my sample.",
                    "label": 0
                },
                {
                    "sent": "The problem is here right?",
                    "label": 0
                },
                {
                    "sent": "If the if that space is really large right even as N is small.",
                    "label": 1
                },
                {
                    "sent": "Actually that representation, though finite right, might still be too large that we want to handle it, right?",
                    "label": 0
                },
                {
                    "sent": "So for instance if it's all parse trees, right?",
                    "label": 0
                },
                {
                    "sent": "So for every input sentence I need to consider all pass trees, then you know this is still, you know representation that is too large to handle, so it's interesting to look at further.",
                    "label": 0
                },
                {
                    "sent": "Ways of sort of sparsifying that you know finding conditions or mechanisms to reduce the dimensionality of the representation even further.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also, think of this that's also something that might give you some insights and dependent on what how exactly you set up your learning problem that can be useful.",
                    "label": 0
                },
                {
                    "sent": "You can think of this whole thing in a probabilistic context.",
                    "label": 0
                },
                {
                    "sent": "So namely, if you think of that function F of X, Y.",
                    "label": 0
                },
                {
                    "sent": "As being for a given Y, sort of a sufficient statistics.",
                    "label": 0
                },
                {
                    "sent": "Sorry, not really.",
                    "label": 0
                },
                {
                    "sent": "They have themselves actually defies.",
                    "label": 0
                },
                {
                    "sent": "Maybe have this down here.",
                    "label": 0
                },
                {
                    "sent": "Sorry so if you think of this function F right you put it into an exponent where you say given ax, right?",
                    "label": 0
                },
                {
                    "sent": "What is the probability of a particular output?",
                    "label": 0
                },
                {
                    "sent": "So far we said that you know F is like a compatibility measure between inputs and outputs and we would like to maximize it.",
                    "label": 0
                },
                {
                    "sent": "Forgiven X to find the optimal Y.",
                    "label": 0
                },
                {
                    "sent": "Now this is kind of a softer version of that right where you would actually try to come up with a model conditional model of outputs given inputs.",
                    "label": 0
                },
                {
                    "sent": "So you could put the F in the exponent here and you would have then a normalizing constant that makes sure this conditional distribution is well defined.",
                    "label": 0
                },
                {
                    "sent": "And then you look at what you actually have up here.",
                    "label": 0
                },
                {
                    "sent": "For instance, in this primal representation.",
                    "label": 0
                },
                {
                    "sent": "Then you know this.",
                    "label": 0
                },
                {
                    "sent": "In this function F, the weight vector will just correspond to Canonical parameters in this fight to sort of a sufficient statistics.",
                    "label": 1
                },
                {
                    "sent": "If we think of this, X argument is being fixed here, because this is always what we condition on, right?",
                    "label": 0
                },
                {
                    "sent": "So we can so we can also instead of justice talking about F, right?",
                    "label": 0
                },
                {
                    "sent": "We can also associate in a Canonical way of probability distribution with.",
                    "label": 0
                },
                {
                    "sent": "A compatibility function F that's defined like that, and then we can actually use that too.",
                    "label": 0
                },
                {
                    "sent": "Set up other interesting, you know, specific loss functions that we might be interested in optimizing so.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For instance, right?",
                    "label": 0
                },
                {
                    "sent": "What you might want to do as a as a measure of fit for your function F given a sample S, you might use something like conditional log likelihood.",
                    "label": 0
                },
                {
                    "sent": "OK, so you might say, well, the way I determine the quality of F is.",
                    "label": 0
                },
                {
                    "sent": "I look at the negative log probability.",
                    "label": 0
                },
                {
                    "sent": "If this is what I want to minimize, right?",
                    "label": 0
                },
                {
                    "sent": "The sum of the log probabilities of outputs given the inputs on the training sample that I've seen.",
                    "label": 0
                },
                {
                    "sent": "Right, and if I expand this basically using the definition from before, then you get a sum over all your training points.",
                    "label": 0
                },
                {
                    "sent": "This compatibility function evaluated on your input output pair minus basically the log partition function from previously that normalizes it right?",
                    "label": 0
                },
                {
                    "sent": "And So what you can see here is basically.",
                    "label": 0
                },
                {
                    "sent": "That in some sense what you're doing is you're comparing the score on the correct input output pair with kind of an.",
                    "label": 0
                },
                {
                    "sent": "Total score that you compute over over all possible outputs.",
                    "label": 0
                },
                {
                    "sent": "Forgiven inputs right, and that score is exactly corresponding to this log partition function.",
                    "label": 1
                },
                {
                    "sent": "Right, so then, if you minimize that with the negative sign here, you effectively you need to maximize the compatibility on training pairs.",
                    "label": 1
                },
                {
                    "sent": "That makes sense, right?",
                    "label": 0
                },
                {
                    "sent": "You try to choose your weight vectors so that on your training examples.",
                    "label": 0
                },
                {
                    "sent": "You get a high F score, but at the same time you basically need to minimize the lock partition function.",
                    "label": 0
                },
                {
                    "sent": "Right and so basically, if you have, you know you can visualize that, and so this would be a way of.",
                    "label": 0
                },
                {
                    "sent": "Basically, thinking of this structured learning problem as a generalization of or you know a version of.",
                    "label": 1
                },
                {
                    "sent": "If you like a conditional random field.",
                    "label": 0
                },
                {
                    "sent": "Model where you know you would have.",
                    "label": 0
                },
                {
                    "sent": "Basically you know.",
                    "label": 0
                },
                {
                    "sent": "You would have outputs.",
                    "label": 0
                },
                {
                    "sent": "Why there can be dependencies that are defined between your wise using a random field approach and it's conditional because basically you condition on a fixed set of input variables and you're only interested in really maximizing the predictive performance of your model.",
                    "label": 0
                },
                {
                    "sent": "Sort of a discriminative model right of the outputs given the inputs OK.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the let me.",
                    "label": 0
                },
                {
                    "sent": "So basically, if you do this in this CRF sense.",
                    "label": 0
                },
                {
                    "sent": "Using using this particular conditional likelihood objective function.",
                    "label": 0
                },
                {
                    "sent": "You look at what you get then.",
                    "label": 0
                },
                {
                    "sent": "Basically, if I take the computer gradient with regard to my weight vector, let's say.",
                    "label": 0
                },
                {
                    "sent": "I see that I get the typical optimality conditions that they usually get in this case, which is basically that the empirical statistics over these files, so just summing them up over all the training examples over xiy pairs that the optimality condition would be that that equals the expectation in average overall training examples of Phi XI XI is given, Y.",
                    "label": 0
                },
                {
                    "sent": "The expectation is then over Y right over all possible outputs.",
                    "label": 0
                },
                {
                    "sent": "So then what you need to be able to do right?",
                    "label": 0
                },
                {
                    "sent": "For instance, if you want to do anything that is gradient based, is.",
                    "label": 0
                },
                {
                    "sent": "And basically you need to be able to compute these expectations over the outputs, right?",
                    "label": 0
                },
                {
                    "sent": "So I've told you before and I'll come back to that that you know, we have a setting where we.",
                    "label": 0
                },
                {
                    "sent": "You know, in order to do prediction we would need to compute the AC Max over all possible outputs, and that because that will be our prediction right?",
                    "label": 0
                },
                {
                    "sent": "In this setting here it would be the most probable output given an input.",
                    "label": 0
                },
                {
                    "sent": "In order to do learning in this setting here using a gradient based method, you would need to be able to compute an expectation over outputs, right?",
                    "label": 1
                },
                {
                    "sent": "So whether or not you can use that in a straightforward way, then will depend on whether how hard that inference problem is right.",
                    "label": 0
                },
                {
                    "sent": "And in general you might.",
                    "label": 0
                },
                {
                    "sent": "Want to use on let's say you have a label sequence.",
                    "label": 0
                },
                {
                    "sent": "You might use something like a forward backward type of algorithms to compute it.",
                    "label": 1
                },
                {
                    "sent": "Or you know if you can do exact inference using a junction tree algorithms if the clique size of the dependencies that you capture between your output variables are small enough, then you can do that.",
                    "label": 0
                },
                {
                    "sent": "Or you can use approximate inference methods.",
                    "label": 0
                },
                {
                    "sent": "And basically plug them in there.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let me just.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "Make one more argument and then I think we should break for lunch and then I'm just going to resume there so.",
                    "label": 0
                },
                {
                    "sent": "So this was this is 1 possibility of what you can do right?",
                    "label": 0
                },
                {
                    "sent": "So you define this conditional probabilistic model and then you maximize a conditional likelihood and then as long as you can compute this expectations then that's fine.",
                    "label": 0
                },
                {
                    "sent": "But there still is a problem.",
                    "label": 0
                },
                {
                    "sent": "As I said before, you know this representation that we're talking about.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here, right, these representations can become really nasty, and if I want, you know I might have.",
                    "label": 0
                },
                {
                    "sent": "You know a lot of features that I want to extract, so this becomes, you know, very high dimensional.",
                    "label": 0
                },
                {
                    "sent": "And then even if I in principle can do the expectation right, it becomes just just everything becomes working in this primer representation just becomes very tedious and difficult, right?",
                    "label": 0
                },
                {
                    "sent": "And so?",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we know from the representative theorem that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you know, in principle also the solution to this problem here, because you know if we just plug in this cost function in.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To the representative.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It will allow a compact representation, you know an expansion over kernel function centered at these points.",
                    "label": 0
                },
                {
                    "sent": "But but again, you know this sum is still huge.",
                    "label": 0
                },
                {
                    "sent": "Now we look at the dual formulation, but the problem is still that now it scales the representation size with the size of the input right space, so that's not good.",
                    "label": 0
                },
                {
                    "sent": "So basically.",
                    "label": 0
                },
                {
                    "sent": "Well, I will talk about then when we resume in the afternoon is basically how to go beyond this and how to develop methods that actually exploit or get solutions that are more sparse in the sense that this some here actually becomes smaller so that we only need to look at a few possible outputs instead of at the whole output space.",
                    "label": 0
                },
                {
                    "sent": "OK, so then we just leave it here.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}