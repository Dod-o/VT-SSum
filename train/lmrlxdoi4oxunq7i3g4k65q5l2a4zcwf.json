{
    "id": "lmrlxdoi4oxunq7i3g4k65q5l2a4zcwf",
    "title": "Speeding Up Stochastic Gradient Descent",
    "info": {
        "author": [
            "Yoshua Bengio, University of Montreal"
        ],
        "published": "Dec. 29, 2007",
        "recorded": "December 2007",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/eml07_bengio_ssg/",
    "segmentation": [
        [
            "Stochastic gradient descent.",
            "Is a very useful tool for a lot of machine learning algorithms.",
            "And I'm interested in running it faster because I want to learn big big models for large large datasets."
        ],
        [
            "I'm interested in.",
            "Seeing how machine learning can scale up to AI problems.",
            "One of the first things I will tell you about which is a motivation for some other things I'm doing is that in order to scale machine learning to AI problems, the kinds of functions that we should try to learn must have some structure, which I called death.",
            "Explain what that means.",
            "And why it's important for generalization?",
            "So what's the connection with speed?",
            "Well, unfortunately this comes with more difficult optimization problems.",
            "And I'll tell you why stochastic gradient and maybe it's order online, 2nd order methods or.",
            "Methods of choice to get started on this path, but then later.",
            "I'll come back to this issue of non convexity and local minima.",
            "I guess it's plugged in living on power.",
            "Alright, so let me let me let me go."
        ],
        [
            "Run.",
            "1st for the motivations concerning deep architectures.",
            "Think about the vision problem.",
            "The way.",
            "Engineer handles the vision problem or the way the brain handles vision.",
            "Problem is through a series of operations and transformations and changes in representation of the raw image.",
            "And.",
            "We should also.",
            "Design learning algorithms that can learn these sequences of representations.",
            "Of course, one option is to design everything by hand, but maybe it's harder.",
            "To scale up to more difficult problems in those we've already tackled.",
            "So if we could find it learning algorithms that could find that could discover these hierarchies of representations starting from low level representations to more abstract ones, it would be really great.",
            "So that's one kind of intuitive motivation."
        ],
        [
            "Well, another word."
        ],
        [
            "Nation actually."
        ],
        [
            "This one is nice.",
            "When we solve, say, vision problems today in machine learning, we usually.",
            "Consider very narrow tasks.",
            "And if you really want to scale the AI.",
            "It means the vision engine is not just going to recognize handwritten digits and faces, but a lot of different thousands 10s of thousands of categories.",
            "And complicated scenes.",
            "And it's likely that the representations that are useful for some tasks are useful for some other tasks.",
            "An having this kind of hierarchy of representations will help us there, because once we've seen number of tasks or a number of types of images, it will be more natural to generalize through these other tasks using the lower level and intermediate representations that we found."
        ],
        [
            "OK notion of death.",
            "Functions can be represented by a circuit whose elements could be plus times sinus and or neurons.",
            "Whatever you want.",
            "Once you define a set of element.",
            "And you represent the function with sets such a graph such as circuit.",
            "The notion of depth, simply the longest path from input to."
        ],
        [
            "Output.",
            "Turns out if you look at most machine learning algorithms we use now depth is.",
            "12 sometimes 3."
        ],
        [
            "OK.",
            "Unfortunately, it looks from vertical results like.",
            "Many functions if you try to represent them with insufficient depth, you're going to run into trouble and you will need a huge circuit with which will be very fat like the left hand one.",
            "So for example, it's not a very good example actually, but the parity function if you represent it with a two level Boolean circuit, you need an exponential number of gates.",
            "If you were presented with.",
            "Login levels Boolean circuit.",
            "You need log in order of and gates."
        ],
        [
            "I'm going."
        ],
        [
            "This.",
            "Basically, there are a bunch of results that go in this direction.",
            "OK, so we'd like to have deep architectures.",
            "So what kind of deep architectures are known in machine learning?",
            "The only ones.",
            "I could find where you could increase the depth.",
            "Are multi layer neural Nets.",
            "There are exceptions, for example stacking and boosting.",
            "Both are ways to add 1 level of depth.",
            "I'm.",
            "Now multiply in your notes.",
            "People have tried to train deep Nets and until recently didn't succeed.",
            "Basically they got results worse than if you had a shallow neural net that is one or two hidden layers.",
            "And of course, we know that these are non convex optimization problems.",
            "But Furthermore it looks like it gets worse as we have more layers.",
            "So the traditional approach to train your Nets is with random initial parameters and then to tune the parameters with stochastic gradient descent.",
            "So what happens?",
            "We don't really know.",
            "Training doesn't progress.",
            "What does it mean?",
            "Well, the gradient is small.",
            "Means you are either in a local minimum or you are in a plateau.",
            "There are some units that seem to be trainable, even if there are deep in there called convolutional Nets, and I'm not going to say much more about that.",
            "But again, we don't really understand why these are an exception, but these are spec."
        ],
        [
            "Applies to vision problems.",
            "Now 2006 something happened.",
            "Hinton Nose and Arrow and Teh published paper called the Fast Learning Algorithm for deep belief Nets.",
            "And there's an idea in there which turns out to be useful for other types of deep networks.",
            "The idea is to train our deep network.",
            "We're going to use a kind of greedy approach.",
            "First, we're going to try to train an unsupervised network which only sees the input images.",
            "And we can train it layer by layer.",
            "Adding one layer on top of each other.",
            "Each layer is going to be a restricted Boltzmann machine, which is something I'm not going to explain, but it's it's a kind of nonlinear PCA or ICA or whatever, which learns a representation of its input as part of the learning process.",
            "And unlike PCA and ICA, the representation can have more components than the actual input.",
            "There's no constraint whether it doesn't have to be bigger or smaller.",
            "So once you've trained in RBM.",
            "You get a both model of the input and a way to transform the input in the new representation.",
            "Well, actually we don't really have a clear idea of why this representation should be better than the original one, but when we use this representation instead of the original one as input features for, say, an SVM or nearest neighbor or Gaussian process, we find that usually we get better classification.",
            "So what you do is you add layer by layer like this and then you put on a supervised learning algorithm.",
            "Your preferred classifier or whatever you want to do regression.",
            "And this is already pretty good, but then you can get even better by taking this big net and.",
            "Training the whole thing by gradient descent on the supervised cost, so you can fine tune this big network.",
            "And what happens is basically we've used the improvised layer wise greedy thing as a very sophisticated initialization for our big net.",
            "Oh, why is this working?",
            "I'll see more about that, but at least one thing that's clear is that when we do this initialization with, the optimization is done.",
            "Locali within each layer.",
            "So we train all these weights and they don't have to look at what's happening above an essentially below there are no interactions.",
            "We know that training a smaller model is presumably easier than a bigger one, and also it looks like back to Gatien computation of gradients when it goes through many layers introduces something that makes that gradient less useful.",
            "So in when you try any one layer at a time, maybe it's easier."
        ],
        [
            "OK, so now I'm going to say something about online learning.",
            "Can you give me time?",
            "OK. 34 minutes.",
            "25 OK, good so my friend Leon Bottou.",
            "Did nice pieces of work about online learning and among those.",
            "Something that is important for me because I'm interested in learning.",
            "Very large scale problems.",
            "And there's a paper with young McCann Nips 2003.",
            "Brings an argument which, on the face of it is obvious when you training with abundant training data and they synthetically made with the amount of data goes to Infinity.",
            "Online learning.",
            "Asymptotically outperforms batch learning.",
            "Simply because.",
            "For batch learning we have to iterate multiple times through the data.",
            "And while you're iterating a second and third and 10th time, the online learning could have looked at more data.",
            "And as N goes larger and larger, that advantage in terms of generalization becomes bigger.",
            "Now it's not totally obvious because one converges faster to the optimum of training error and the other and so.",
            "There are some tradeoffs that they studied, and there this year's paper continues in the same track.",
            "Looks at generalization error, which is usually studied in terms of the tradeoff between approximation error because we have a class of function that's too restricted to smooth an estimation error, the variance because their class is too large.",
            "We don't have enough data.",
            "But now introduces a third term, which is the effect due to the optimization algorithm.",
            "Because our optimization algorithm is never perfect.",
            "Usually we stop optimizing when we find that we're close enough to a local minimum.",
            "But what is the effect of this?",
            "And basically this study introduces a very nice idea.",
            "Which is we have two constraints to consider when we train learning algorithm.",
            "One is how much time we have and the other is how many examples we can afford.",
            "And so, depending on how many examples you actually have.",
            "It turns out that if you want to minimize generalization error, well, you could use less examples and then spend more time optimizing.",
            "Or you could.",
            "Use more examples, maybe all the examples and spend less time optimizing.",
            "It turns out that depending on the number of examples, there's a number of examples which is going to give us.",
            "One of these two constraints is going to be active, so either we're going to not care about optimization an use a small number of examples.",
            "Which would be all the examples.",
            "Or actually we have so many examples that the limiting factor is optimization time.",
            "Andy Compare 1st Order and 2nd order batch and online methods with.",
            "Which which factors are important in the symbolic error rates?",
            "And it turns out actually that between 1st order and 2nd order online methods which beat the batch ones essentially, or at least in the early phases of convergence, the things that matter are condition number of the Hessian, an input dimension."
        ],
        [
            "Which aren't like huge things but could be.",
            "OK, So what I what the message I got out of this is.",
            "We've been focusing in the last 15 years of NIPS.",
            "Especially in theory on the issue of underfitting and how do we avoid on underfitting?",
            "But if you're Google or maybe Yahoo.",
            "You'd like to train with.",
            "Tons of data, and so you're going to be on the active constraint is going to be the time, not the amount of data.",
            "So my friend Ronald Colbert is currently training some models with 17 billion examples.",
            "So I can tell you that he deals with the optimization problem.",
            "And what happens when you have that many examples is the types of methods we've been using that are being very popular based on sensually local nonparametric parametric models where you have to store all the examples and do something with them.",
            "Things like SVM, nearest neighbor type of things they have problems both on the statistical side that I mentioned a little bit because of the depth of the architecture.",
            "And computational it becomes also difficult.",
            "And I'm not going to go into these details, so if.",
            "We don't use these local and parametric models where we have an easy optimization task easy in the sense that we don't have.",
            "To deal with local minima, then basically as far as I know we have to deal with these deep architectures that involve a nonconvex optimization problem and that means.",
            "We're mostly faced with the underfitting problem and not the overfitting problem.",
            "Now it's always easy to overfit anyway if you take for example a deep neural net.",
            "Then you put a nearest neighbor on top.",
            "You can always overfit.",
            "Obviously you can or you put enough capacity on the top layer.",
            "You can overfit, so it's not like overfitting is not there, but it's an issue we know how to resolve.",
            "And the real challenge now is how to deal with Underfitting.",
            "That is, how to a better optimize."
        ],
        [
            "So now let me tell you about some work that we've done recently.",
            "On trying to make natural gradient faster, So what is natural gradient?",
            "Let's say you have a true loss function, which is the interval of a loss L Theta X Theta is a parameter XSS examples.",
            "We have a distribution to the empirical distribution.",
            "No, actually P till there's a true distribution of our data.",
            "Of course we don't know it and we want to minimize the expected value of our loss.",
            "So we've got a gradient which is expected value of the individual gradients.",
            "And then we define the natural gradient as C -- 1 G. Where Jesus gradient and sees the covariance matrix of the gradients.",
            "So.",
            "Mary has some nice motivations for it, and one nice feature as far as I'm concerned is that it's invariant to affine repair metrization, so if you change data to a time state of a matrix, you go in the same."
        ],
        [
            "Places.",
            "OK, so now there's a nice result we've found to justify natural gradient in different ways.",
            "Um?",
            "Consider so the G we had before was a G. Till then was the true gradient, the true mean with respect the true expected error.",
            "But actually we don't have that.",
            "Of course we only have samples, so we've gotta mean gradient G. And we would like to go down in the direction of the true gradient, because that's the direction that will make the generalization error go down fastest locally.",
            "But we don't have that.",
            "We have the mean grade, so usually just people.",
            "People just go down the mean gradient and hope that it's OK. Now central Limit Theorem tells us that there's a relation between the true gradient and the mean gradient.",
            "Involving the covariance matrix of the gradients.",
            "So with a little bit of math and assuming something very mild, I think.",
            "On the true gradient.",
            "So now it will be.",
            "I'm going to put my Bayesian hat, which is very rare.",
            "And assume that.",
            "Well, we don't know what the true gradient is, but let's assume some very flat Gaussian prior on what the true gradient is centered at 0.",
            "With a Sigma squared as a variance.",
            "Then you can show that the direction.",
            "That minimizes that is most in line with the true gradient.",
            "In other words, minimizing the expected value of the dot product.",
            "So you mean you transpose G?",
            "Where Jesus true gradient.",
            "That direction.",
            "Is given by this formula.",
            "So what is this formula?",
            "N is the number of examples.",
            "Sigmas are variants and we have C. The covariance of the gradients.",
            "So it says essentially something like the natural gradient, but the premultiplying matrix should be the covariance plus something times the identity.",
            "So essentially it's a regularised.",
            "Natural gradient, so if you take natural gradient an you just regularize the covariance matrix.",
            "You're going in the right direction.",
            "Now, of course, there's some things I like.",
            "You should be using the true covariance, but of course you don't have it, so you're going to have to meet it.",
            "There are other arguments you can make, like instead of thinking of the direction that goes down the fastest you can think, what is the direction where the probability of not going up is largest and you get the natural gradient without regularization so the."
        ],
        [
            "These are related.",
            "Um, yeah, what does it mean to go down the natural gradient?",
            "It means you want to go.",
            "More in the direction where there is less variance and less in the direction where there is more variance.",
            "Basically you don't trust the direction the directions where the gradient has a lot of variability in the data, so this is trying to illustrate this.",
            "So the blue arrows are.",
            "Samples of the gradient.",
            "The Red one is the mean, which was which would be what you would follow if you just do stochastic gradient and the green one is natural gradient.",
            "And then the black ones are the eigenvector eigenvalue representation of the covariance matrix of.",
            "The gradient so.",
            "Let's see, this one is easier.",
            "You could see that in this direction.",
            "Think about these cloud points.",
            "You see that there's a kind of covariance here, right?",
            "So you see, there is not much variance in this direction, but there's a lot of variance in that direction.",
            "So that's why we have these eigenvectors value values.",
            "And what what we get is a message.",
            "Is that because the variance is large in this direction, we should have, we should reduce our change in that direction.",
            "And you can see that the natural gradient is almost orthogonal to this one, so it has very little component in that highly variable direction.",
            "And you can.",
            "You can tell a similar story, so maybe this one here is easier to understand.",
            "You can see that there's a lot of variance in this direction and very little variance in that direction, and so natural gradient tells us well, we shouldn't move too much in that direction.",
            "So in fact we take a step in that direction.",
            "Where is the mean gradient?",
            "Maybe because there are a few more of these here.",
            "The main gradient tells you to go in this direction.",
            "So that's what it means.",
            "OK, now there's a problem.",
            "Problem is my mission is."
        ],
        [
            "Responding OK. Is that this matrix is not obvious?",
            "It's not obvious how to get it.",
            "If you have.",
            "If you want to estimate it with an examples and we are in P dimensional space, it's MP squared and then you have to invert it.",
            "That's P cube.",
            "And you need P squared memory to store it is it's on reasonably small neural net four M list with 300 units.",
            "That's 200 gig, forget it.",
            "Suck.",
            "And then there's the issue of estimating seed, ideally.",
            "We should re estimate C for each new value of the parameters, which means we would have to look at a lot of examples for each parameter setting, so that's that's impractical.",
            "But the thing we're guessing and makes a lot of sense is that as we move in parameter space, covariance matrix moves moves smoothly, so we can estimate it by some."
        ],
        [
            "Moving average process.",
            "So there's an algorithm that's described in our current NIPS paper.",
            "Call Tonga for top moment.",
            "Natural gradient online natural gradient algorithm.",
            "But I'm not going to go through this, but you can see that it's a few lines of Matlab.",
            "And nice thing about this algorithm, so it's an estimator of the natural gradient.",
            "It's based on the low rank approximation of the matrix.",
            "It exploits the fact that you have when you have a low rank approximation of the matrix, you can do the inversion in the transform space.",
            "Essentially the gram matrix where you instead of having a you have a P bikie approximation low rank approximation.",
            "So instead of doing a P by P inversion you can do K by K inversion and you work in that space.",
            "That's one trick and the other important trick is we're going to.",
            "I.",
            "Do it online updating of our estimate of the covariance matrix and only redo an actual inversion inversion every B steps so we expensive inversion step.",
            "We don't have to do it all the time and with this we actually get an algorithm which is basically.",
            "If we make B&P equal and I'm sorry, no K and B = K is a dimension of the lower, the rank of the matrix and B is how many times we update the inverse, then it's only K times slower than the regular stochastic gradient.",
            "And actually that K will."
        ],
        [
            "See that can be made pretty small.",
            "To make it small, what we do is.",
            "Instead of considering the whole covariance matrix is 1, big thing we're going to make a block diagonal approximation.",
            "And it turns out that for things like mixture models, annual Nets, their natural blocks in the neural net is just all the weights coming into a neuron.",
            "We could also do weights coming out, but actually works better with where it's coming into your own.",
            "And if you look at the actual covariance matrix of these beasts, you do find blocks, so those weights are much more correlated with each other.",
            "So instead of approximating this one big matrix, we have a bunch of blocks and we do a block down approximation.",
            "And if you do that now, the nice thing is.",
            "The rank of the over approximation is going to be very large, so let's say each block is 100 by 100.",
            "We can approximate it with K = 3, four, 5, or something very small and do a good job.",
            "And let's say we have 1000 blocks, then the overall rank is going to be say we used K for each block is going to be 1000 * K, So we have a huge rank which actually works."
        ],
        [
            "Well.",
            "I bet the computation now only the cost is going to be OK, so it's going to be 3 * 5 times slower than regular stochastic gradient.",
            "And we can recover that loss by having much faster convergence.",
            "So these are experiments on.",
            "Amnist using regular 1 hidden layer neural net.",
            "How am I doing on time?",
            "More minutes, OK?",
            "So I'm not going to go through the details of this, but just see the bottom one is our method and.",
            "The others are different stochastic, so the top one is stochastic gradient and then the others are mini batch gradients.",
            "So actually the reason matches going faster here is because we are doing matrix matrix multiplies which are much faster if you have mini batch.",
            "Yeah.",
            "After a certain time while the cursor error, yeah.",
            "Because the log likelihood is not the same thing as the classification error, and what happens is that when the model becomes very confident about some output weights becomes large become large.",
            "It becomes very confident about some classes wrongly when you're doing classification error, you only pay one when you are doing negative log likelihood, you pay arbitrarily large, so these few cases where it's confidently wrong, you pay a lot, and so overfitting is much more apparent here than there optimizing the wrong quantity.",
            "Depends what you want to do, right?",
            "I do."
        ],
        [
            "OK, so we have other results with other datasets with similar kind of nice speedup curves.",
            "And the important thing to know is this axis is not number of updates, it's CPU time.",
            "OK, and all of these comparisons is extremely important that we use CPU time as the metric and not what other authors have done which are moving that name and account number of examples seen or something like that.",
            "So if you want to see the details, there's NIPS paper and I'm going to use my time for talk."
        ],
        [
            "About another subject, which is other things.",
            "To speed up these kinds of networks.",
            "So first of all.",
            "You want to do matrix matrix multiply using glass and there are many different blasts."
        ],
        [
            "Packages around and we did a bunch of comparisons.",
            "We also used the NVIDIA graphic card to see if we get some interesting speedup.",
            "An if you do.",
            "Reasonably chosen matrix matrix multiply.",
            "You can actually get nice speedups, so this is running the same code in C. Handwritten matrix multiply and blast that these are different implementations of glass.",
            "We like to go to library which is free and works really well and this is NVIDIA, so actually.",
            "You got about a three fold speedup with the NVIDIA card.",
            "The one that we used anyway.",
            "Which is not quite worth it given the extra effort of programming this thing.",
            "So we're sticking to to blast on multicore machines.",
            "Ano, yeah, yeah.",
            "And the other thing is this is 1 CPU if you just have a multicore machine that you can see right with a four core machine, it's a bit more expensive than adding a card, but it's so much easier to program that you know we go for form."
        ],
        [
            "Sing so first of all, use glass."
        ],
        [
            "I'm going to skip that and then we can tell you about parallelization on these multicore machines.",
            "So let's say you have 4 Core, 8 Core 16 core machine.",
            "Can you exploit that?",
            "I'm not talking about the cluster here, so the communication costs you think are very small.",
            "Actually, there are important, so you have to be careful how you organize your code to exploit parallelism on these machines.",
            "We can use the matrix matrix multiply.",
            "That could be.",
            "They can be paralyzed, but actually.",
            "The gain is not always what we want, so we are looking at alternative methods.",
            "And the thing the important message that we found experimenting.",
            "Is that on on different architectures that we've tried, it looks like very obvious thing that multiple multiple cores try to read from the same area at the same time.",
            "There is not much overhead, but if they try to write in the same place or the same area at the same time.",
            "It's hell.",
            "OK, so how can we organize our parallelization so that we don't have these conflicts?"
        ],
        [
            "So we're going to split the data.",
            "We're going to data parallel training of these stochastic gradient algorithms going to split the data in chunks.",
            "So each core is going to see one chunk of the data, and we can do mini batch gradient descent on each of the cores, and then the question is so each core sees a little bit of data, computes a gradient based on that, and it would like to know what the other guys have computers gradient so that they can all take advantage of each others computation.",
            "So the trivial thing to do is that the run in parallel the old computer gradients and then we have some kind of scheme for exchanging the information about the sum of all those gradients and those things we've tried didn't work because the phenomenon that they were waiting for each other locking the memory for writing.",
            "So simple solution.",
            "You allow only one core to update the parameters at a time, but you don't want the others to be waiting during this time, so you let them compute some gradients while one is writing to memory.",
            "So that's going to be the idea.",
            "We're going to have a flag and index which is going to say which core is allowed to write.",
            "This score is going to be writing its results to a shared memory while the other is continually collecting more gradients and then it passes a flag to another poor and continues and goes back to collecting more gradients."
        ],
        [
            "So that's more or less the kind of communication.",
            "Architecture we have all the weights, all the parameters are in shared memory.",
            "Each core is going to have is going to use a local memory for its gradients, which actually is the same size as this.",
            "And it's going to have its version of the data, or is subset of the data and there's going to be in shared memory an index that says which CPU is allowed to go to memory for writing."
        ],
        [
            "OK, so we've done some pretty experiments on the UCI letter data data set.",
            "Anime."
        ],
        [
            "Gonna go through quickly.",
            "So we compare with the straumann which.",
            "Where all the cores tried to write at the same time?"
        ],
        [
            "And using our method, if we just look at time spent for example so we don't look at convergence time.",
            "The speedup is pretty good.",
            "So on a 16 core machine we got about 80% of linear speedup.",
            "But now the really important question is, is there a loss due to slower convergence of the stochastic gradient?",
            "Because we are actually now having to use bigger mini batches now remember because these mini batches are now if you end cores at least end times the mini batch size seen by each core.",
            "So these are."
        ],
        [
            "Big mini batches.",
            "OK, so now how are we going to measure performance when we consider?",
            "The time for training.",
            "We're going to measure performance by.",
            "Looking at how much CPU time you need to reach a certain error level.",
            "This is a fair comparison.",
            "Unfortunately it leads to high variance in the measurement of time because it's a highly nonlinear thing because you know there is noise due to initialization and the optimization goes in different path depending on where you started.",
            "So you have to repeat the experiments many times to get reliable estimates of the speedup."
        ],
        [
            "And so.",
            "Forget about these, this is high variance points the.",
            "Blue here is so this is was done with five repetitions.",
            "Which is not enough.",
            "The blue here is what you get with the naive algorithm where you don't constrain the course to write at different times.",
            "And the red here is with the algorithm.",
            "An you can see essentially a linear speedup, so.",
            "Yeah.",
            "The last few slides.",
            "He said he's out.",
            "I had variable will even buy that.",
            "Yeah, that's what I was saying.",
            "What I mean by that that?",
            "If you, if you look at the variances on these are pretty high and the reason is I said that we are measuring the time CPU time to reach a particular error level.",
            "So you can imagine.",
            "That if if if there is some variation due to the initial.",
            "Initial weights the seed that used to start the thing.",
            "Then that can be amplified.",
            "You know that noise can be amplified by this when we decide to cut at certain time.",
            "Anyways, there's there's variance when you try to measure those speedups.",
            "There is quite a bit of variance.",
            "Uh.",
            "Can you get back to your house?",
            "Odanga yes."
        ],
        [
            "So these last experiments are not using tongue guys, just regular stochastic gradient.",
            "Can you explain to me why you can use a low rank approach, low rank adjustment formulas for computing the inverse?",
            "So you have a movie, yes?",
            "Still low rank are adjusted right?",
            "Um?",
            "Putting it in version takes a cubic time.",
            "Because we don't want to do it.",
            "Let's say we want to do it in this in the space of the.",
            "Of the eigenvectors, the low rank vectors we don't want to hold this big matrix.",
            "In memory actually.",
            "In Windows 8, yeah, yeah.",
            "Good point, yeah.",
            "Little rancor low ranking right?",
            "I don't have a good answer for you.",
            "Nicola Lheureux probably has.",
            "Because he's so he's there.",
            "Not getting out is just forgetting factor.",
            "So there multiplied by gamma every time we added to the ranks of eating disorders.",
            "The steps we have to do an identical position to reduce their entries originally, so we let the rank increase versus tabs, and then we increase it once using a negative because it's a moving average we're taking.",
            "Let's say you have a low rank estimation of the matrix.",
            "Now comes a new point so that new point is bringing a new GG transpose component that's going to add rank at one more term to the rank right?",
            "So because we don't want to recompute the low rank estimation every example we have to keep these higher rank estimation as we go along, and that's why we need this this trick.",
            "No, Tonga only helps us get faster to local minimum, so the stuff that I didn't have time to talk about.",
            "Is presumably to help us deal with local minimum."
        ],
        [
            "Flashing fast right?",
            "But basically it's it's trying to exploit the idea of continuation methods, which are ways to deal with global minima.",
            "An without guarantees, but starting with smoother version maybe convex version of the costs and gradually making it closer to the cause.",
            "We're really interested in and tracking the local minimum."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Is a very useful tool for a lot of machine learning algorithms.",
                    "label": 1
                },
                {
                    "sent": "And I'm interested in running it faster because I want to learn big big models for large large datasets.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm interested in.",
                    "label": 0
                },
                {
                    "sent": "Seeing how machine learning can scale up to AI problems.",
                    "label": 0
                },
                {
                    "sent": "One of the first things I will tell you about which is a motivation for some other things I'm doing is that in order to scale machine learning to AI problems, the kinds of functions that we should try to learn must have some structure, which I called death.",
                    "label": 0
                },
                {
                    "sent": "Explain what that means.",
                    "label": 0
                },
                {
                    "sent": "And why it's important for generalization?",
                    "label": 0
                },
                {
                    "sent": "So what's the connection with speed?",
                    "label": 0
                },
                {
                    "sent": "Well, unfortunately this comes with more difficult optimization problems.",
                    "label": 0
                },
                {
                    "sent": "And I'll tell you why stochastic gradient and maybe it's order online, 2nd order methods or.",
                    "label": 1
                },
                {
                    "sent": "Methods of choice to get started on this path, but then later.",
                    "label": 0
                },
                {
                    "sent": "I'll come back to this issue of non convexity and local minima.",
                    "label": 0
                },
                {
                    "sent": "I guess it's plugged in living on power.",
                    "label": 0
                },
                {
                    "sent": "Alright, so let me let me let me go.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Run.",
                    "label": 0
                },
                {
                    "sent": "1st for the motivations concerning deep architectures.",
                    "label": 0
                },
                {
                    "sent": "Think about the vision problem.",
                    "label": 0
                },
                {
                    "sent": "The way.",
                    "label": 0
                },
                {
                    "sent": "Engineer handles the vision problem or the way the brain handles vision.",
                    "label": 0
                },
                {
                    "sent": "Problem is through a series of operations and transformations and changes in representation of the raw image.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We should also.",
                    "label": 0
                },
                {
                    "sent": "Design learning algorithms that can learn these sequences of representations.",
                    "label": 0
                },
                {
                    "sent": "Of course, one option is to design everything by hand, but maybe it's harder.",
                    "label": 0
                },
                {
                    "sent": "To scale up to more difficult problems in those we've already tackled.",
                    "label": 0
                },
                {
                    "sent": "So if we could find it learning algorithms that could find that could discover these hierarchies of representations starting from low level representations to more abstract ones, it would be really great.",
                    "label": 0
                },
                {
                    "sent": "So that's one kind of intuitive motivation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, another word.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nation actually.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This one is nice.",
                    "label": 0
                },
                {
                    "sent": "When we solve, say, vision problems today in machine learning, we usually.",
                    "label": 0
                },
                {
                    "sent": "Consider very narrow tasks.",
                    "label": 0
                },
                {
                    "sent": "And if you really want to scale the AI.",
                    "label": 0
                },
                {
                    "sent": "It means the vision engine is not just going to recognize handwritten digits and faces, but a lot of different thousands 10s of thousands of categories.",
                    "label": 0
                },
                {
                    "sent": "And complicated scenes.",
                    "label": 0
                },
                {
                    "sent": "And it's likely that the representations that are useful for some tasks are useful for some other tasks.",
                    "label": 0
                },
                {
                    "sent": "An having this kind of hierarchy of representations will help us there, because once we've seen number of tasks or a number of types of images, it will be more natural to generalize through these other tasks using the lower level and intermediate representations that we found.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK notion of death.",
                    "label": 0
                },
                {
                    "sent": "Functions can be represented by a circuit whose elements could be plus times sinus and or neurons.",
                    "label": 0
                },
                {
                    "sent": "Whatever you want.",
                    "label": 0
                },
                {
                    "sent": "Once you define a set of element.",
                    "label": 0
                },
                {
                    "sent": "And you represent the function with sets such a graph such as circuit.",
                    "label": 0
                },
                {
                    "sent": "The notion of depth, simply the longest path from input to.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Output.",
                    "label": 0
                },
                {
                    "sent": "Turns out if you look at most machine learning algorithms we use now depth is.",
                    "label": 0
                },
                {
                    "sent": "12 sometimes 3.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, it looks from vertical results like.",
                    "label": 0
                },
                {
                    "sent": "Many functions if you try to represent them with insufficient depth, you're going to run into trouble and you will need a huge circuit with which will be very fat like the left hand one.",
                    "label": 0
                },
                {
                    "sent": "So for example, it's not a very good example actually, but the parity function if you represent it with a two level Boolean circuit, you need an exponential number of gates.",
                    "label": 0
                },
                {
                    "sent": "If you were presented with.",
                    "label": 0
                },
                {
                    "sent": "Login levels Boolean circuit.",
                    "label": 0
                },
                {
                    "sent": "You need log in order of and gates.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Basically, there are a bunch of results that go in this direction.",
                    "label": 0
                },
                {
                    "sent": "OK, so we'd like to have deep architectures.",
                    "label": 0
                },
                {
                    "sent": "So what kind of deep architectures are known in machine learning?",
                    "label": 0
                },
                {
                    "sent": "The only ones.",
                    "label": 0
                },
                {
                    "sent": "I could find where you could increase the depth.",
                    "label": 0
                },
                {
                    "sent": "Are multi layer neural Nets.",
                    "label": 1
                },
                {
                    "sent": "There are exceptions, for example stacking and boosting.",
                    "label": 0
                },
                {
                    "sent": "Both are ways to add 1 level of depth.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "Now multiply in your notes.",
                    "label": 0
                },
                {
                    "sent": "People have tried to train deep Nets and until recently didn't succeed.",
                    "label": 0
                },
                {
                    "sent": "Basically they got results worse than if you had a shallow neural net that is one or two hidden layers.",
                    "label": 1
                },
                {
                    "sent": "And of course, we know that these are non convex optimization problems.",
                    "label": 0
                },
                {
                    "sent": "But Furthermore it looks like it gets worse as we have more layers.",
                    "label": 0
                },
                {
                    "sent": "So the traditional approach to train your Nets is with random initial parameters and then to tune the parameters with stochastic gradient descent.",
                    "label": 1
                },
                {
                    "sent": "So what happens?",
                    "label": 0
                },
                {
                    "sent": "We don't really know.",
                    "label": 0
                },
                {
                    "sent": "Training doesn't progress.",
                    "label": 0
                },
                {
                    "sent": "What does it mean?",
                    "label": 0
                },
                {
                    "sent": "Well, the gradient is small.",
                    "label": 0
                },
                {
                    "sent": "Means you are either in a local minimum or you are in a plateau.",
                    "label": 0
                },
                {
                    "sent": "There are some units that seem to be trainable, even if there are deep in there called convolutional Nets, and I'm not going to say much more about that.",
                    "label": 0
                },
                {
                    "sent": "But again, we don't really understand why these are an exception, but these are spec.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Applies to vision problems.",
                    "label": 0
                },
                {
                    "sent": "Now 2006 something happened.",
                    "label": 0
                },
                {
                    "sent": "Hinton Nose and Arrow and Teh published paper called the Fast Learning Algorithm for deep belief Nets.",
                    "label": 1
                },
                {
                    "sent": "And there's an idea in there which turns out to be useful for other types of deep networks.",
                    "label": 0
                },
                {
                    "sent": "The idea is to train our deep network.",
                    "label": 0
                },
                {
                    "sent": "We're going to use a kind of greedy approach.",
                    "label": 0
                },
                {
                    "sent": "First, we're going to try to train an unsupervised network which only sees the input images.",
                    "label": 0
                },
                {
                    "sent": "And we can train it layer by layer.",
                    "label": 0
                },
                {
                    "sent": "Adding one layer on top of each other.",
                    "label": 0
                },
                {
                    "sent": "Each layer is going to be a restricted Boltzmann machine, which is something I'm not going to explain, but it's it's a kind of nonlinear PCA or ICA or whatever, which learns a representation of its input as part of the learning process.",
                    "label": 0
                },
                {
                    "sent": "And unlike PCA and ICA, the representation can have more components than the actual input.",
                    "label": 0
                },
                {
                    "sent": "There's no constraint whether it doesn't have to be bigger or smaller.",
                    "label": 0
                },
                {
                    "sent": "So once you've trained in RBM.",
                    "label": 0
                },
                {
                    "sent": "You get a both model of the input and a way to transform the input in the new representation.",
                    "label": 0
                },
                {
                    "sent": "Well, actually we don't really have a clear idea of why this representation should be better than the original one, but when we use this representation instead of the original one as input features for, say, an SVM or nearest neighbor or Gaussian process, we find that usually we get better classification.",
                    "label": 0
                },
                {
                    "sent": "So what you do is you add layer by layer like this and then you put on a supervised learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Your preferred classifier or whatever you want to do regression.",
                    "label": 0
                },
                {
                    "sent": "And this is already pretty good, but then you can get even better by taking this big net and.",
                    "label": 0
                },
                {
                    "sent": "Training the whole thing by gradient descent on the supervised cost, so you can fine tune this big network.",
                    "label": 0
                },
                {
                    "sent": "And what happens is basically we've used the improvised layer wise greedy thing as a very sophisticated initialization for our big net.",
                    "label": 0
                },
                {
                    "sent": "Oh, why is this working?",
                    "label": 0
                },
                {
                    "sent": "I'll see more about that, but at least one thing that's clear is that when we do this initialization with, the optimization is done.",
                    "label": 0
                },
                {
                    "sent": "Locali within each layer.",
                    "label": 0
                },
                {
                    "sent": "So we train all these weights and they don't have to look at what's happening above an essentially below there are no interactions.",
                    "label": 0
                },
                {
                    "sent": "We know that training a smaller model is presumably easier than a bigger one, and also it looks like back to Gatien computation of gradients when it goes through many layers introduces something that makes that gradient less useful.",
                    "label": 0
                },
                {
                    "sent": "So in when you try any one layer at a time, maybe it's easier.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now I'm going to say something about online learning.",
                    "label": 0
                },
                {
                    "sent": "Can you give me time?",
                    "label": 0
                },
                {
                    "sent": "OK. 34 minutes.",
                    "label": 0
                },
                {
                    "sent": "25 OK, good so my friend Leon Bottou.",
                    "label": 0
                },
                {
                    "sent": "Did nice pieces of work about online learning and among those.",
                    "label": 0
                },
                {
                    "sent": "Something that is important for me because I'm interested in learning.",
                    "label": 0
                },
                {
                    "sent": "Very large scale problems.",
                    "label": 0
                },
                {
                    "sent": "And there's a paper with young McCann Nips 2003.",
                    "label": 0
                },
                {
                    "sent": "Brings an argument which, on the face of it is obvious when you training with abundant training data and they synthetically made with the amount of data goes to Infinity.",
                    "label": 1
                },
                {
                    "sent": "Online learning.",
                    "label": 0
                },
                {
                    "sent": "Asymptotically outperforms batch learning.",
                    "label": 0
                },
                {
                    "sent": "Simply because.",
                    "label": 0
                },
                {
                    "sent": "For batch learning we have to iterate multiple times through the data.",
                    "label": 0
                },
                {
                    "sent": "And while you're iterating a second and third and 10th time, the online learning could have looked at more data.",
                    "label": 0
                },
                {
                    "sent": "And as N goes larger and larger, that advantage in terms of generalization becomes bigger.",
                    "label": 0
                },
                {
                    "sent": "Now it's not totally obvious because one converges faster to the optimum of training error and the other and so.",
                    "label": 0
                },
                {
                    "sent": "There are some tradeoffs that they studied, and there this year's paper continues in the same track.",
                    "label": 0
                },
                {
                    "sent": "Looks at generalization error, which is usually studied in terms of the tradeoff between approximation error because we have a class of function that's too restricted to smooth an estimation error, the variance because their class is too large.",
                    "label": 1
                },
                {
                    "sent": "We don't have enough data.",
                    "label": 0
                },
                {
                    "sent": "But now introduces a third term, which is the effect due to the optimization algorithm.",
                    "label": 0
                },
                {
                    "sent": "Because our optimization algorithm is never perfect.",
                    "label": 0
                },
                {
                    "sent": "Usually we stop optimizing when we find that we're close enough to a local minimum.",
                    "label": 0
                },
                {
                    "sent": "But what is the effect of this?",
                    "label": 0
                },
                {
                    "sent": "And basically this study introduces a very nice idea.",
                    "label": 0
                },
                {
                    "sent": "Which is we have two constraints to consider when we train learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "One is how much time we have and the other is how many examples we can afford.",
                    "label": 0
                },
                {
                    "sent": "And so, depending on how many examples you actually have.",
                    "label": 0
                },
                {
                    "sent": "It turns out that if you want to minimize generalization error, well, you could use less examples and then spend more time optimizing.",
                    "label": 0
                },
                {
                    "sent": "Or you could.",
                    "label": 0
                },
                {
                    "sent": "Use more examples, maybe all the examples and spend less time optimizing.",
                    "label": 0
                },
                {
                    "sent": "It turns out that depending on the number of examples, there's a number of examples which is going to give us.",
                    "label": 0
                },
                {
                    "sent": "One of these two constraints is going to be active, so either we're going to not care about optimization an use a small number of examples.",
                    "label": 0
                },
                {
                    "sent": "Which would be all the examples.",
                    "label": 0
                },
                {
                    "sent": "Or actually we have so many examples that the limiting factor is optimization time.",
                    "label": 1
                },
                {
                    "sent": "Andy Compare 1st Order and 2nd order batch and online methods with.",
                    "label": 1
                },
                {
                    "sent": "Which which factors are important in the symbolic error rates?",
                    "label": 0
                },
                {
                    "sent": "And it turns out actually that between 1st order and 2nd order online methods which beat the batch ones essentially, or at least in the early phases of convergence, the things that matter are condition number of the Hessian, an input dimension.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which aren't like huge things but could be.",
                    "label": 0
                },
                {
                    "sent": "OK, So what I what the message I got out of this is.",
                    "label": 0
                },
                {
                    "sent": "We've been focusing in the last 15 years of NIPS.",
                    "label": 1
                },
                {
                    "sent": "Especially in theory on the issue of underfitting and how do we avoid on underfitting?",
                    "label": 0
                },
                {
                    "sent": "But if you're Google or maybe Yahoo.",
                    "label": 0
                },
                {
                    "sent": "You'd like to train with.",
                    "label": 0
                },
                {
                    "sent": "Tons of data, and so you're going to be on the active constraint is going to be the time, not the amount of data.",
                    "label": 1
                },
                {
                    "sent": "So my friend Ronald Colbert is currently training some models with 17 billion examples.",
                    "label": 0
                },
                {
                    "sent": "So I can tell you that he deals with the optimization problem.",
                    "label": 0
                },
                {
                    "sent": "And what happens when you have that many examples is the types of methods we've been using that are being very popular based on sensually local nonparametric parametric models where you have to store all the examples and do something with them.",
                    "label": 1
                },
                {
                    "sent": "Things like SVM, nearest neighbor type of things they have problems both on the statistical side that I mentioned a little bit because of the depth of the architecture.",
                    "label": 0
                },
                {
                    "sent": "And computational it becomes also difficult.",
                    "label": 0
                },
                {
                    "sent": "And I'm not going to go into these details, so if.",
                    "label": 0
                },
                {
                    "sent": "We don't use these local and parametric models where we have an easy optimization task easy in the sense that we don't have.",
                    "label": 0
                },
                {
                    "sent": "To deal with local minima, then basically as far as I know we have to deal with these deep architectures that involve a nonconvex optimization problem and that means.",
                    "label": 0
                },
                {
                    "sent": "We're mostly faced with the underfitting problem and not the overfitting problem.",
                    "label": 0
                },
                {
                    "sent": "Now it's always easy to overfit anyway if you take for example a deep neural net.",
                    "label": 0
                },
                {
                    "sent": "Then you put a nearest neighbor on top.",
                    "label": 0
                },
                {
                    "sent": "You can always overfit.",
                    "label": 0
                },
                {
                    "sent": "Obviously you can or you put enough capacity on the top layer.",
                    "label": 0
                },
                {
                    "sent": "You can overfit, so it's not like overfitting is not there, but it's an issue we know how to resolve.",
                    "label": 0
                },
                {
                    "sent": "And the real challenge now is how to deal with Underfitting.",
                    "label": 0
                },
                {
                    "sent": "That is, how to a better optimize.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let me tell you about some work that we've done recently.",
                    "label": 0
                },
                {
                    "sent": "On trying to make natural gradient faster, So what is natural gradient?",
                    "label": 1
                },
                {
                    "sent": "Let's say you have a true loss function, which is the interval of a loss L Theta X Theta is a parameter XSS examples.",
                    "label": 0
                },
                {
                    "sent": "We have a distribution to the empirical distribution.",
                    "label": 1
                },
                {
                    "sent": "No, actually P till there's a true distribution of our data.",
                    "label": 1
                },
                {
                    "sent": "Of course we don't know it and we want to minimize the expected value of our loss.",
                    "label": 0
                },
                {
                    "sent": "So we've got a gradient which is expected value of the individual gradients.",
                    "label": 0
                },
                {
                    "sent": "And then we define the natural gradient as C -- 1 G. Where Jesus gradient and sees the covariance matrix of the gradients.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Mary has some nice motivations for it, and one nice feature as far as I'm concerned is that it's invariant to affine repair metrization, so if you change data to a time state of a matrix, you go in the same.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Places.",
                    "label": 0
                },
                {
                    "sent": "OK, so now there's a nice result we've found to justify natural gradient in different ways.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Consider so the G we had before was a G. Till then was the true gradient, the true mean with respect the true expected error.",
                    "label": 0
                },
                {
                    "sent": "But actually we don't have that.",
                    "label": 0
                },
                {
                    "sent": "Of course we only have samples, so we've gotta mean gradient G. And we would like to go down in the direction of the true gradient, because that's the direction that will make the generalization error go down fastest locally.",
                    "label": 0
                },
                {
                    "sent": "But we don't have that.",
                    "label": 0
                },
                {
                    "sent": "We have the mean grade, so usually just people.",
                    "label": 0
                },
                {
                    "sent": "People just go down the mean gradient and hope that it's OK. Now central Limit Theorem tells us that there's a relation between the true gradient and the mean gradient.",
                    "label": 0
                },
                {
                    "sent": "Involving the covariance matrix of the gradients.",
                    "label": 0
                },
                {
                    "sent": "So with a little bit of math and assuming something very mild, I think.",
                    "label": 0
                },
                {
                    "sent": "On the true gradient.",
                    "label": 0
                },
                {
                    "sent": "So now it will be.",
                    "label": 0
                },
                {
                    "sent": "I'm going to put my Bayesian hat, which is very rare.",
                    "label": 0
                },
                {
                    "sent": "And assume that.",
                    "label": 0
                },
                {
                    "sent": "Well, we don't know what the true gradient is, but let's assume some very flat Gaussian prior on what the true gradient is centered at 0.",
                    "label": 0
                },
                {
                    "sent": "With a Sigma squared as a variance.",
                    "label": 0
                },
                {
                    "sent": "Then you can show that the direction.",
                    "label": 0
                },
                {
                    "sent": "That minimizes that is most in line with the true gradient.",
                    "label": 0
                },
                {
                    "sent": "In other words, minimizing the expected value of the dot product.",
                    "label": 0
                },
                {
                    "sent": "So you mean you transpose G?",
                    "label": 0
                },
                {
                    "sent": "Where Jesus true gradient.",
                    "label": 0
                },
                {
                    "sent": "That direction.",
                    "label": 0
                },
                {
                    "sent": "Is given by this formula.",
                    "label": 0
                },
                {
                    "sent": "So what is this formula?",
                    "label": 0
                },
                {
                    "sent": "N is the number of examples.",
                    "label": 0
                },
                {
                    "sent": "Sigmas are variants and we have C. The covariance of the gradients.",
                    "label": 0
                },
                {
                    "sent": "So it says essentially something like the natural gradient, but the premultiplying matrix should be the covariance plus something times the identity.",
                    "label": 0
                },
                {
                    "sent": "So essentially it's a regularised.",
                    "label": 0
                },
                {
                    "sent": "Natural gradient, so if you take natural gradient an you just regularize the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "You're going in the right direction.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, there's some things I like.",
                    "label": 0
                },
                {
                    "sent": "You should be using the true covariance, but of course you don't have it, so you're going to have to meet it.",
                    "label": 0
                },
                {
                    "sent": "There are other arguments you can make, like instead of thinking of the direction that goes down the fastest you can think, what is the direction where the probability of not going up is largest and you get the natural gradient without regularization so the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These are related.",
                    "label": 0
                },
                {
                    "sent": "Um, yeah, what does it mean to go down the natural gradient?",
                    "label": 1
                },
                {
                    "sent": "It means you want to go.",
                    "label": 0
                },
                {
                    "sent": "More in the direction where there is less variance and less in the direction where there is more variance.",
                    "label": 0
                },
                {
                    "sent": "Basically you don't trust the direction the directions where the gradient has a lot of variability in the data, so this is trying to illustrate this.",
                    "label": 0
                },
                {
                    "sent": "So the blue arrows are.",
                    "label": 0
                },
                {
                    "sent": "Samples of the gradient.",
                    "label": 0
                },
                {
                    "sent": "The Red one is the mean, which was which would be what you would follow if you just do stochastic gradient and the green one is natural gradient.",
                    "label": 0
                },
                {
                    "sent": "And then the black ones are the eigenvector eigenvalue representation of the covariance matrix of.",
                    "label": 0
                },
                {
                    "sent": "The gradient so.",
                    "label": 0
                },
                {
                    "sent": "Let's see, this one is easier.",
                    "label": 0
                },
                {
                    "sent": "You could see that in this direction.",
                    "label": 0
                },
                {
                    "sent": "Think about these cloud points.",
                    "label": 0
                },
                {
                    "sent": "You see that there's a kind of covariance here, right?",
                    "label": 0
                },
                {
                    "sent": "So you see, there is not much variance in this direction, but there's a lot of variance in that direction.",
                    "label": 0
                },
                {
                    "sent": "So that's why we have these eigenvectors value values.",
                    "label": 0
                },
                {
                    "sent": "And what what we get is a message.",
                    "label": 0
                },
                {
                    "sent": "Is that because the variance is large in this direction, we should have, we should reduce our change in that direction.",
                    "label": 0
                },
                {
                    "sent": "And you can see that the natural gradient is almost orthogonal to this one, so it has very little component in that highly variable direction.",
                    "label": 0
                },
                {
                    "sent": "And you can.",
                    "label": 0
                },
                {
                    "sent": "You can tell a similar story, so maybe this one here is easier to understand.",
                    "label": 0
                },
                {
                    "sent": "You can see that there's a lot of variance in this direction and very little variance in that direction, and so natural gradient tells us well, we shouldn't move too much in that direction.",
                    "label": 0
                },
                {
                    "sent": "So in fact we take a step in that direction.",
                    "label": 0
                },
                {
                    "sent": "Where is the mean gradient?",
                    "label": 1
                },
                {
                    "sent": "Maybe because there are a few more of these here.",
                    "label": 0
                },
                {
                    "sent": "The main gradient tells you to go in this direction.",
                    "label": 0
                },
                {
                    "sent": "So that's what it means.",
                    "label": 0
                },
                {
                    "sent": "OK, now there's a problem.",
                    "label": 0
                },
                {
                    "sent": "Problem is my mission is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Responding OK. Is that this matrix is not obvious?",
                    "label": 0
                },
                {
                    "sent": "It's not obvious how to get it.",
                    "label": 0
                },
                {
                    "sent": "If you have.",
                    "label": 0
                },
                {
                    "sent": "If you want to estimate it with an examples and we are in P dimensional space, it's MP squared and then you have to invert it.",
                    "label": 0
                },
                {
                    "sent": "That's P cube.",
                    "label": 0
                },
                {
                    "sent": "And you need P squared memory to store it is it's on reasonably small neural net four M list with 300 units.",
                    "label": 0
                },
                {
                    "sent": "That's 200 gig, forget it.",
                    "label": 0
                },
                {
                    "sent": "Suck.",
                    "label": 0
                },
                {
                    "sent": "And then there's the issue of estimating seed, ideally.",
                    "label": 0
                },
                {
                    "sent": "We should re estimate C for each new value of the parameters, which means we would have to look at a lot of examples for each parameter setting, so that's that's impractical.",
                    "label": 0
                },
                {
                    "sent": "But the thing we're guessing and makes a lot of sense is that as we move in parameter space, covariance matrix moves moves smoothly, so we can estimate it by some.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Moving average process.",
                    "label": 0
                },
                {
                    "sent": "So there's an algorithm that's described in our current NIPS paper.",
                    "label": 0
                },
                {
                    "sent": "Call Tonga for top moment.",
                    "label": 0
                },
                {
                    "sent": "Natural gradient online natural gradient algorithm.",
                    "label": 0
                },
                {
                    "sent": "But I'm not going to go through this, but you can see that it's a few lines of Matlab.",
                    "label": 0
                },
                {
                    "sent": "And nice thing about this algorithm, so it's an estimator of the natural gradient.",
                    "label": 0
                },
                {
                    "sent": "It's based on the low rank approximation of the matrix.",
                    "label": 0
                },
                {
                    "sent": "It exploits the fact that you have when you have a low rank approximation of the matrix, you can do the inversion in the transform space.",
                    "label": 0
                },
                {
                    "sent": "Essentially the gram matrix where you instead of having a you have a P bikie approximation low rank approximation.",
                    "label": 1
                },
                {
                    "sent": "So instead of doing a P by P inversion you can do K by K inversion and you work in that space.",
                    "label": 0
                },
                {
                    "sent": "That's one trick and the other important trick is we're going to.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 1
                },
                {
                    "sent": "Do it online updating of our estimate of the covariance matrix and only redo an actual inversion inversion every B steps so we expensive inversion step.",
                    "label": 0
                },
                {
                    "sent": "We don't have to do it all the time and with this we actually get an algorithm which is basically.",
                    "label": 0
                },
                {
                    "sent": "If we make B&P equal and I'm sorry, no K and B = K is a dimension of the lower, the rank of the matrix and B is how many times we update the inverse, then it's only K times slower than the regular stochastic gradient.",
                    "label": 0
                },
                {
                    "sent": "And actually that K will.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "See that can be made pretty small.",
                    "label": 0
                },
                {
                    "sent": "To make it small, what we do is.",
                    "label": 0
                },
                {
                    "sent": "Instead of considering the whole covariance matrix is 1, big thing we're going to make a block diagonal approximation.",
                    "label": 1
                },
                {
                    "sent": "And it turns out that for things like mixture models, annual Nets, their natural blocks in the neural net is just all the weights coming into a neuron.",
                    "label": 0
                },
                {
                    "sent": "We could also do weights coming out, but actually works better with where it's coming into your own.",
                    "label": 0
                },
                {
                    "sent": "And if you look at the actual covariance matrix of these beasts, you do find blocks, so those weights are much more correlated with each other.",
                    "label": 0
                },
                {
                    "sent": "So instead of approximating this one big matrix, we have a bunch of blocks and we do a block down approximation.",
                    "label": 0
                },
                {
                    "sent": "And if you do that now, the nice thing is.",
                    "label": 0
                },
                {
                    "sent": "The rank of the over approximation is going to be very large, so let's say each block is 100 by 100.",
                    "label": 0
                },
                {
                    "sent": "We can approximate it with K = 3, four, 5, or something very small and do a good job.",
                    "label": 0
                },
                {
                    "sent": "And let's say we have 1000 blocks, then the overall rank is going to be say we used K for each block is going to be 1000 * K, So we have a huge rank which actually works.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "I bet the computation now only the cost is going to be OK, so it's going to be 3 * 5 times slower than regular stochastic gradient.",
                    "label": 0
                },
                {
                    "sent": "And we can recover that loss by having much faster convergence.",
                    "label": 0
                },
                {
                    "sent": "So these are experiments on.",
                    "label": 0
                },
                {
                    "sent": "Amnist using regular 1 hidden layer neural net.",
                    "label": 0
                },
                {
                    "sent": "How am I doing on time?",
                    "label": 0
                },
                {
                    "sent": "More minutes, OK?",
                    "label": 0
                },
                {
                    "sent": "So I'm not going to go through the details of this, but just see the bottom one is our method and.",
                    "label": 0
                },
                {
                    "sent": "The others are different stochastic, so the top one is stochastic gradient and then the others are mini batch gradients.",
                    "label": 0
                },
                {
                    "sent": "So actually the reason matches going faster here is because we are doing matrix matrix multiplies which are much faster if you have mini batch.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "After a certain time while the cursor error, yeah.",
                    "label": 0
                },
                {
                    "sent": "Because the log likelihood is not the same thing as the classification error, and what happens is that when the model becomes very confident about some output weights becomes large become large.",
                    "label": 0
                },
                {
                    "sent": "It becomes very confident about some classes wrongly when you're doing classification error, you only pay one when you are doing negative log likelihood, you pay arbitrarily large, so these few cases where it's confidently wrong, you pay a lot, and so overfitting is much more apparent here than there optimizing the wrong quantity.",
                    "label": 0
                },
                {
                    "sent": "Depends what you want to do, right?",
                    "label": 0
                },
                {
                    "sent": "I do.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we have other results with other datasets with similar kind of nice speedup curves.",
                    "label": 0
                },
                {
                    "sent": "And the important thing to know is this axis is not number of updates, it's CPU time.",
                    "label": 0
                },
                {
                    "sent": "OK, and all of these comparisons is extremely important that we use CPU time as the metric and not what other authors have done which are moving that name and account number of examples seen or something like that.",
                    "label": 0
                },
                {
                    "sent": "So if you want to see the details, there's NIPS paper and I'm going to use my time for talk.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About another subject, which is other things.",
                    "label": 0
                },
                {
                    "sent": "To speed up these kinds of networks.",
                    "label": 0
                },
                {
                    "sent": "So first of all.",
                    "label": 0
                },
                {
                    "sent": "You want to do matrix matrix multiply using glass and there are many different blasts.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Packages around and we did a bunch of comparisons.",
                    "label": 0
                },
                {
                    "sent": "We also used the NVIDIA graphic card to see if we get some interesting speedup.",
                    "label": 0
                },
                {
                    "sent": "An if you do.",
                    "label": 0
                },
                {
                    "sent": "Reasonably chosen matrix matrix multiply.",
                    "label": 0
                },
                {
                    "sent": "You can actually get nice speedups, so this is running the same code in C. Handwritten matrix multiply and blast that these are different implementations of glass.",
                    "label": 0
                },
                {
                    "sent": "We like to go to library which is free and works really well and this is NVIDIA, so actually.",
                    "label": 0
                },
                {
                    "sent": "You got about a three fold speedup with the NVIDIA card.",
                    "label": 0
                },
                {
                    "sent": "The one that we used anyway.",
                    "label": 0
                },
                {
                    "sent": "Which is not quite worth it given the extra effort of programming this thing.",
                    "label": 0
                },
                {
                    "sent": "So we're sticking to to blast on multicore machines.",
                    "label": 0
                },
                {
                    "sent": "Ano, yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "And the other thing is this is 1 CPU if you just have a multicore machine that you can see right with a four core machine, it's a bit more expensive than adding a card, but it's so much easier to program that you know we go for form.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sing so first of all, use glass.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to skip that and then we can tell you about parallelization on these multicore machines.",
                    "label": 0
                },
                {
                    "sent": "So let's say you have 4 Core, 8 Core 16 core machine.",
                    "label": 0
                },
                {
                    "sent": "Can you exploit that?",
                    "label": 0
                },
                {
                    "sent": "I'm not talking about the cluster here, so the communication costs you think are very small.",
                    "label": 0
                },
                {
                    "sent": "Actually, there are important, so you have to be careful how you organize your code to exploit parallelism on these machines.",
                    "label": 0
                },
                {
                    "sent": "We can use the matrix matrix multiply.",
                    "label": 0
                },
                {
                    "sent": "That could be.",
                    "label": 0
                },
                {
                    "sent": "They can be paralyzed, but actually.",
                    "label": 0
                },
                {
                    "sent": "The gain is not always what we want, so we are looking at alternative methods.",
                    "label": 0
                },
                {
                    "sent": "And the thing the important message that we found experimenting.",
                    "label": 0
                },
                {
                    "sent": "Is that on on different architectures that we've tried, it looks like very obvious thing that multiple multiple cores try to read from the same area at the same time.",
                    "label": 0
                },
                {
                    "sent": "There is not much overhead, but if they try to write in the same place or the same area at the same time.",
                    "label": 0
                },
                {
                    "sent": "It's hell.",
                    "label": 0
                },
                {
                    "sent": "OK, so how can we organize our parallelization so that we don't have these conflicts?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we're going to split the data.",
                    "label": 0
                },
                {
                    "sent": "We're going to data parallel training of these stochastic gradient algorithms going to split the data in chunks.",
                    "label": 0
                },
                {
                    "sent": "So each core is going to see one chunk of the data, and we can do mini batch gradient descent on each of the cores, and then the question is so each core sees a little bit of data, computes a gradient based on that, and it would like to know what the other guys have computers gradient so that they can all take advantage of each others computation.",
                    "label": 1
                },
                {
                    "sent": "So the trivial thing to do is that the run in parallel the old computer gradients and then we have some kind of scheme for exchanging the information about the sum of all those gradients and those things we've tried didn't work because the phenomenon that they were waiting for each other locking the memory for writing.",
                    "label": 0
                },
                {
                    "sent": "So simple solution.",
                    "label": 1
                },
                {
                    "sent": "You allow only one core to update the parameters at a time, but you don't want the others to be waiting during this time, so you let them compute some gradients while one is writing to memory.",
                    "label": 1
                },
                {
                    "sent": "So that's going to be the idea.",
                    "label": 0
                },
                {
                    "sent": "We're going to have a flag and index which is going to say which core is allowed to write.",
                    "label": 0
                },
                {
                    "sent": "This score is going to be writing its results to a shared memory while the other is continually collecting more gradients and then it passes a flag to another poor and continues and goes back to collecting more gradients.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's more or less the kind of communication.",
                    "label": 0
                },
                {
                    "sent": "Architecture we have all the weights, all the parameters are in shared memory.",
                    "label": 0
                },
                {
                    "sent": "Each core is going to have is going to use a local memory for its gradients, which actually is the same size as this.",
                    "label": 0
                },
                {
                    "sent": "And it's going to have its version of the data, or is subset of the data and there's going to be in shared memory an index that says which CPU is allowed to go to memory for writing.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we've done some pretty experiments on the UCI letter data data set.",
                    "label": 0
                },
                {
                    "sent": "Anime.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gonna go through quickly.",
                    "label": 0
                },
                {
                    "sent": "So we compare with the straumann which.",
                    "label": 0
                },
                {
                    "sent": "Where all the cores tried to write at the same time?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And using our method, if we just look at time spent for example so we don't look at convergence time.",
                    "label": 0
                },
                {
                    "sent": "The speedup is pretty good.",
                    "label": 0
                },
                {
                    "sent": "So on a 16 core machine we got about 80% of linear speedup.",
                    "label": 1
                },
                {
                    "sent": "But now the really important question is, is there a loss due to slower convergence of the stochastic gradient?",
                    "label": 0
                },
                {
                    "sent": "Because we are actually now having to use bigger mini batches now remember because these mini batches are now if you end cores at least end times the mini batch size seen by each core.",
                    "label": 0
                },
                {
                    "sent": "So these are.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Big mini batches.",
                    "label": 0
                },
                {
                    "sent": "OK, so now how are we going to measure performance when we consider?",
                    "label": 0
                },
                {
                    "sent": "The time for training.",
                    "label": 0
                },
                {
                    "sent": "We're going to measure performance by.",
                    "label": 0
                },
                {
                    "sent": "Looking at how much CPU time you need to reach a certain error level.",
                    "label": 1
                },
                {
                    "sent": "This is a fair comparison.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately it leads to high variance in the measurement of time because it's a highly nonlinear thing because you know there is noise due to initialization and the optimization goes in different path depending on where you started.",
                    "label": 0
                },
                {
                    "sent": "So you have to repeat the experiments many times to get reliable estimates of the speedup.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "Forget about these, this is high variance points the.",
                    "label": 0
                },
                {
                    "sent": "Blue here is so this is was done with five repetitions.",
                    "label": 0
                },
                {
                    "sent": "Which is not enough.",
                    "label": 0
                },
                {
                    "sent": "The blue here is what you get with the naive algorithm where you don't constrain the course to write at different times.",
                    "label": 0
                },
                {
                    "sent": "And the red here is with the algorithm.",
                    "label": 0
                },
                {
                    "sent": "An you can see essentially a linear speedup, so.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "The last few slides.",
                    "label": 0
                },
                {
                    "sent": "He said he's out.",
                    "label": 0
                },
                {
                    "sent": "I had variable will even buy that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's what I was saying.",
                    "label": 0
                },
                {
                    "sent": "What I mean by that that?",
                    "label": 0
                },
                {
                    "sent": "If you, if you look at the variances on these are pretty high and the reason is I said that we are measuring the time CPU time to reach a particular error level.",
                    "label": 0
                },
                {
                    "sent": "So you can imagine.",
                    "label": 0
                },
                {
                    "sent": "That if if if there is some variation due to the initial.",
                    "label": 0
                },
                {
                    "sent": "Initial weights the seed that used to start the thing.",
                    "label": 0
                },
                {
                    "sent": "Then that can be amplified.",
                    "label": 0
                },
                {
                    "sent": "You know that noise can be amplified by this when we decide to cut at certain time.",
                    "label": 0
                },
                {
                    "sent": "Anyways, there's there's variance when you try to measure those speedups.",
                    "label": 0
                },
                {
                    "sent": "There is quite a bit of variance.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Can you get back to your house?",
                    "label": 0
                },
                {
                    "sent": "Odanga yes.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So these last experiments are not using tongue guys, just regular stochastic gradient.",
                    "label": 0
                },
                {
                    "sent": "Can you explain to me why you can use a low rank approach, low rank adjustment formulas for computing the inverse?",
                    "label": 0
                },
                {
                    "sent": "So you have a movie, yes?",
                    "label": 0
                },
                {
                    "sent": "Still low rank are adjusted right?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Putting it in version takes a cubic time.",
                    "label": 0
                },
                {
                    "sent": "Because we don't want to do it.",
                    "label": 0
                },
                {
                    "sent": "Let's say we want to do it in this in the space of the.",
                    "label": 0
                },
                {
                    "sent": "Of the eigenvectors, the low rank vectors we don't want to hold this big matrix.",
                    "label": 0
                },
                {
                    "sent": "In memory actually.",
                    "label": 0
                },
                {
                    "sent": "In Windows 8, yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Good point, yeah.",
                    "label": 0
                },
                {
                    "sent": "Little rancor low ranking right?",
                    "label": 0
                },
                {
                    "sent": "I don't have a good answer for you.",
                    "label": 0
                },
                {
                    "sent": "Nicola Lheureux probably has.",
                    "label": 0
                },
                {
                    "sent": "Because he's so he's there.",
                    "label": 0
                },
                {
                    "sent": "Not getting out is just forgetting factor.",
                    "label": 0
                },
                {
                    "sent": "So there multiplied by gamma every time we added to the ranks of eating disorders.",
                    "label": 0
                },
                {
                    "sent": "The steps we have to do an identical position to reduce their entries originally, so we let the rank increase versus tabs, and then we increase it once using a negative because it's a moving average we're taking.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have a low rank estimation of the matrix.",
                    "label": 0
                },
                {
                    "sent": "Now comes a new point so that new point is bringing a new GG transpose component that's going to add rank at one more term to the rank right?",
                    "label": 0
                },
                {
                    "sent": "So because we don't want to recompute the low rank estimation every example we have to keep these higher rank estimation as we go along, and that's why we need this this trick.",
                    "label": 0
                },
                {
                    "sent": "No, Tonga only helps us get faster to local minimum, so the stuff that I didn't have time to talk about.",
                    "label": 0
                },
                {
                    "sent": "Is presumably to help us deal with local minimum.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Flashing fast right?",
                    "label": 0
                },
                {
                    "sent": "But basically it's it's trying to exploit the idea of continuation methods, which are ways to deal with global minima.",
                    "label": 0
                },
                {
                    "sent": "An without guarantees, but starting with smoother version maybe convex version of the costs and gradually making it closer to the cause.",
                    "label": 0
                },
                {
                    "sent": "We're really interested in and tracking the local minimum.",
                    "label": 0
                }
            ]
        }
    }
}