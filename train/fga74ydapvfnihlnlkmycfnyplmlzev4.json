{
    "id": "fga74ydapvfnihlnlkmycfnyplmlzev4",
    "title": "Learning language from its perceptual context",
    "info": {
        "author": [
            "Raymond J. Mooney, Department of Computer Science, University of Texas at Austin"
        ],
        "published": "Oct. 10, 2008",
        "recorded": "September 2008",
        "category": [
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/ecmlpkdd08_mooney_llfi/",
    "segmentation": [
        [
            "For such a nice introduction, I appreciate the invitation from the rest of the program Committee as well to talk to you today about some of the recent work that I've been doing with my graduate students on trying to connect language and perception and learning language from perceptual context.",
            "So this is joint work with several of my current and recent graduate students, David Chen, Rohit Cotte, and Yaqub or John Wong."
        ],
        [
            "So the as Walter Walter alluded to, there's been a sort of revolution and natural language processing from manually developed systems to machine learning, develop systems, and most current state of the art.",
            "NLP systems are constructed by training on large supervised corpora, various types.",
            "So if you're doing syntactic analysis, you have resources like the Penn treebank to learn from labeled trees on sentence is how to do parsing.",
            "If you're interested in an disambiguating ambiguous words, there's various corpora like the sense of balance Emma Val corpora.",
            "Interested in what's called semantic role labeling.",
            "Assigning semantic roles like agent and patient.",
            "Two sentence is there's a prop bank.",
            "If you're interested machine translation, there are lots of water, called parallel corpora of the same text in different languages, like the Hansford Corpus, which is the Canadian Parliament that by law appears in both French and English, but there's a lot of effort that goes into constructing these supervised corpora and a lot of time and expense, and an is gone is dedicated to building all these.",
            "Labeled corpora for being able to develop machine to use in order to use machine learning algorithms to then train on these corporate developed natural language processing systems."
        ],
        [
            "So the particular task that I've worked on a lot over the last decade or more is a little bit less studied area that I call learning for semantic parsing, where semantic parser by my definition Maps and natural language sentence to a complete, detailed formal representation of its meaning.",
            "Sometimes linguists will refer to this as logical form.",
            "Here I'll use a somewhat less committal term, meaning representation or Mr, so it doesn't necessarily have to be strictly like a first order logic representation.",
            "And particularly, the task that I've looked at, we've tried to focus on meaning representations that are immediately useful for some other program to do some task where the output of the semantic parser is not just what you know.",
            "Some philosopher claims is the meaning of this sentence, but it's actually a formal computer language that then can be used to perform some tasks.",
            "So over the last few years, we've looked at a couple of applications on this.",
            "One is database front end, so I can query databases in natural language and not have to type in arcane query languages like SQL.",
            "The other one my colleague Peter Stone at the University of Texas is deeply involved in one of the founders of the Robo Cup competition.",
            "I assume mostly familiar with, which is, you know, robots playing soccer and we've developed something that learns to understand coaching instructions in English using that pass.",
            "So just to briefly illustrate."
        ],
        [
            "These sorts of tasks that we've done semantic parsing on.",
            "Here's an example of a database application that we built a query about questions in US geography dating back to the mid mid 90s.",
            "So you might get a question in like how many states does the Mississippi run through an I have to translate that via semantic parsing into some formal representation.",
            "This is a prologue representation of a meaning of that sentence.",
            "One of the issues that just one of the subproblems that comes up in this process is word sense disambiguation, right?",
            "I have to infer whether the Mississippi in this case refers to the River Mississippi or the state of which there is.",
            "Both in the US and then once I have that mapped into a formal prolog query, in our case, or you could put that into SQL or some more traditional database that goes into some database system and then comes out with the answer.",
            "It turns out the answer is 10.",
            "Another application."
        ],
        [
            "That we've looked at in detail, is this Robo Cup coaching instructions, so there's a competition that the Robo Cup competitions hold as a lot of sub competitions within it, and one of the ones that's not one of the more popular ones is called the coach competition, where you actually compete to instruct your players as a coach on how to play the game and are to do that.",
            "There's a formal language that the Robo Cup community is developed called Silang, just for coaching language, and you might have an instruction in English like this if the ball is in our penalty area then all players except player for should stay in our half.",
            "In to get the RoboCop system to understand that you have to translate that into this formal language.",
            "It looks a little bit like lispy for those of us who remember list, but it's hard to set of rule representation about what to do under certain circumstances, so we've developed systems that have learned to automatically translate from English into this formal language, which then can go into the RoboCop simulator.",
            "I'll talk more about the World Cup simulator, so there are lots of leagues with real robots, but in some sense the simplest Leggett RoboCop is simulated Lee, where it's like a little video game and you control the individual.",
            "Agents and if you give that formal language to the system, your players will actually obey that that that instruction.",
            "So it's a way of having a semantic representation that's immediately useful for some other task.",
            "So we've looked at both of these."
        ],
        [
            "Now as as I said, it's very hard to manually develop these systems.",
            "You know, throughout the 60s and 70s and into the 80s, people try to manually write programs to do these sorts of tasks, like mapping sentences to logical farm, and it's very complicated, and it's hard to build a system that's robust and accurate across a wide range of input.",
            "So our approach has been how can we use, you know, the latest machine learning technology to automate this task.",
            "So if I just give you examples of sentence is paired with a formal representation.",
            "And then I give that to some learning system.",
            "It can automatically then generate a semantic parser that then if I give it a new natural language sentence, it's not seen during training.",
            "It can accurately map it to some formal meaning representation, which can then go off to the database system or to the RoboCop simulator.",
            "So to actually do something for me."
        ],
        [
            "So over the last more than a decade, my graduate since I've worked on various approaches to automatically learning semantic parsers, I'm not going to bore you with a lot of the details of the earlier ones on this slide.",
            "In this talk, I'll mainly focus on two of our most recent systems, wast developed by John Wong and myself, which there's a wide body of work these days in statistical methods for learning to translate 1 natural language into another, and we use some of the technology developed there, particularly it's called syntax based statistical machine translation.",
            "And I'll talk a little bit more about that later.",
            "Our other most recent system is a system called Chris developed by my student Roky Cotte, and it uses a series of SVM classifiers that use string kernels.",
            "If you're familiar with string kernels to iteratively build up semantic representations for sentences, I'll talk a little bit about Chris, but I'll mainly focus on the wasp system today."
        ],
        [
            "Are OK, so while says I said uses machine translation approach to semantic parsing and it uses the latest techniques in statistical empty.",
            "So within the the statistical machine learning machine translation community there have been development and use of these formalisms called synchronous context free grammars lately, which I'll abbreviate, CFG and they've been widely used by Wu in Malamed and David Chang and other people to do.",
            "To learn to build systems to translate from one natural language to another and another one of the underlying technologies.",
            "Here she goes back to the early 90s by the group at IBM that started the work in statistical machine machine translation on word alignments, where the basic idea is just from parallel corpora of English text paired with French sex taste.",
            "Identify the word correspondences automatically and learn that word in English translates to what word in Spanish and that's typically called word alignment.",
            "So by using both of these technologies, we developed a system that learns semantic parsing and we call it what it says.",
            "Acronym for Word Alignment based semantic parsing."
        ],
        [
            "So here's sort of a larger picture of the framework that we're working in, so with in machine translation, these technologies have been used to map from one natural language in."
        ],
        [
            "Oh, another, but we're interested in the task of there's some formal computer language, like this coaching language or a query language for a database, and we're interested initially were primarily interested in the semantic parsing task is how can we translate a natural language into a formal language.",
            "The computer system can actually utilized we."
        ],
        [
            "Also, more recently been interested in the reverse task, which in in natural language processing is typically called tactical generation, will see why they called it's called tactical compared to strategic, which is an issue we'll talk about later where we take an expression in a formal language and translated back into English so we can communicate that more effectively to a human being."
        ],
        [
            "Well, all of this.",
            "All of these tasks can be done in the context of synchronous parsing using these synchronous context free grammars."
        ],
        [
            "And it actually originally synchronous.",
            "Context free grammars were developed in the compiler community by a hoe annulment way back in 1972 to translate from one formal language to another, mainly from a high level programming language into a low level machine language, and so all of these different types of translation within and between formal and natural languages can be handled with this general nice framework of synchronous."
        ],
        [
            "Grammars so again there originally developed by 1/2 an omen way back in 72 as part of a compiler theory to be able to combine syntax analysis and code generation in a single step, and the idea is a normal context, free grammar or any grammar generates one string in one language.",
            "Synchronous grammar.",
            "Eighters synchronous grammars generate two strings in two different languages."
        ],
        [
            "For a pair of string, so here's how it works.",
            "Isn't it a simple example?",
            "In the case of translating database queries into a formal query language, we might have a top level non terminal like query and then it has two right hand sides, one in English like what is City for some other non terminal city and then that has a representation in the formal side of answer.",
            "Open Paren, City, close paren.",
            "So the grammar consists of generating two strings, one in each of the language.",
            "If we're doing this machine translation, we might have one side B English and the other be French.",
            "On"
        ],
        [
            "And then what happens is you have synchronous derivation that derive two strings, one in each of the languages.",
            "So we start with our top level sentence nonterminal like query and we have these things called synchronous context free grammar rules that are illustrated on the bottom like the one I showed you on the last slide and then we apply each of those recursively.",
            "Whoa, whoa, what happened?",
            "Sorry bout that.",
            "Wow, I lost all my my gosh something went crazy.",
            "Sorry bout this.",
            "Where were we?",
            "That's the problem with this technology.",
            "Sometimes it screws up on you.",
            "OK, so now we we re derive strings in both.",
            "Sentence is at each point applying one of these synchronous rules to rewrite a non terminal that shared between the two until eventually we get a complete parse tree on both sides and then that gives us a translation between the one language and the other.",
            "In this case between a natural language query and a formal prolog query that we can then send off to Prolog to actually answer this question like what is the capital?",
            "Ohio, which is actually Columbus, which is where a bunch of computational linguists were this summer, because that's where the computation linguistics conference was.",
            "OK, so these synchronous grammars provide a nice formalism for doing translation because they provide a nice grammatical formalism for generating two strings."
        ],
        [
            "So one of the problems with language, of course, is it's ambiguous and even a simple word like Ohio in this domain of US geography is ambiguous because I don't necessarily know whether I'm talking about the state which is this synchronous."
        ],
        [
            "Terra Bashan, or if I'm talking about the River.",
            "There's also an Ohio River in the US, but of course capital rivers usually don't have capitals.",
            "And cities do, and so the system needs to realize that derivation one here is much more likely than derivation two, so additional on top of this synchronous grammar."
        ],
        [
            "We have weights attached to each one of the synchronous grammar rules, which is a very analogous for those of, you know about probabilistic context free grammars.",
            "Except here the numbers don't need to be the actually probabilities.",
            "Their arbitrary weights that get put into an exponential model to do the disambiguation.",
            "So where we have the two derivations here of a capital of a city versus the capital of a River, and we have numbers assigned, each one of those productions rules and you can see here in this case that the production rule for.",
            "Cities of rivers is much lower than this production rule that subsidies of States and these numbers.",
            "These Lambda parameters are going to be learned from training data eventually.",
            "And now we can by using a simple exponential or log linear model.",
            "We can assign a probability to each one of these derivations.",
            "The Z.",
            "Here is a normalizing constant to make sure that we actually get a legitimate probability distribution.",
            "So we eventually assign a probability to each one of these derivations based on the scores of the individual synchronous context.",
            "Free grammar rules involved, and that eventually assigns a horse a higher probability to the more correct or city of capital capital of a city interpretation.",
            "So this is this weighted.",
            "Synchronous context free grammar formalism that's again become quite popular within the statistical machine translation community, and we've been adapting it to use for this task of mapping sentences to formal meaning representation."
        ],
        [
            "OK, so the overall wise system works something like this.",
            "It assumes you start out with an unambiguous context free grammar for the meaning representation language.",
            "That's what the MRL stands for.",
            "So this is the formal language that the computer knows quite well.",
            "It has a well defined grammar for it and then I give it training.",
            "Sets of sentence is in the two languages that I'm interested in translating between the traditionally in the SMT literature these are called ENF.",
            "It originally stood for English and French, 'cause that was original pair.",
            "For us it means English informal.",
            "Because we're translating from English to formal language rather than from English to French, that's given first to a system we call lexical acquisition, which constructs a lexecon, which is the basic translation of terms in the one language to terms in the other, and these are represented as these synchronous context free grammar rules.",
            "And then there's a separate phase of then learning those Lambda parameters on each one of those production, so that we get a well defined probability distribution across the synchronous derivations.",
            "So that's a separate step that we have to estimate all these Lambda parameters for each one of the synchronous context free grammar rules we construct in the first step.",
            "After we've done that, we have a complete synchronous parsing model that can do translation, so we give it a new sentence in the language.",
            "English, say in this case, and by using this parameterized or weighted synchronous context free grammar, it will automatically translate it into an select.",
            "The best, most likely interpretation in the F language, or the formal language for our case now lost is it has fairly complex algorithms for doing both lexical acquisition and parameter estimation that I don't have time to talk into.",
            "You talk to you here today about I recommend.",
            "Looking at the papers on this or John has his complete thesis up on the web."
        ],
        [
            "That gives all the details ohmygosh.",
            "Here we go again.",
            "Why is it doing this?",
            "OK, so.",
            "Another task though then semantic parsing is tactical generation, so here I have an example from the Robo Cup domain.",
            "We first start and so we want to map.",
            "The goalie should always stay in our half to the formal coaching language, but we'd also like to do the reverse.",
            "We'd like to be able to translate the formal language back into English.",
            "The nice one of the great things about synchronous context free grammars is there can."
        ],
        [
            "Lately symmetric, they have two strings on the right hand side, but there's no reason why I need to use one as the input and one is the output.",
            "I can easily just switch the two of those and immediately I have a system that can do generation as opposed to parsing.",
            "It can take formal expressions in and find the most likely synchronous derivation that generates that formal expression and corresponding to that on the other side will be the most likely interpretation in English for that formal query.",
            "So we've also developed a version of last.",
            "Sometimes we call this wasp inverse that can do generation by using the.",
            "Cemetery of account.",
            "Synchronous context.",
            "Free grammar to generate as well as to parse."
        ],
        [
            "OK, so I encourage Assessor whirlwind tour of this system called Lost that we developed and there's a lot of literature.",
            "If you want to learn more of the technical details so that sort of older work.",
            "But now I want to start talking really about where we've been moving with this work recently, which is, I've never liked this idea that we've had to Ann, and believe me, my graduate students don't like it either, 'cause they're the ones that have to sit down and build these corpora of annotating all of these natural language expressions with formal representation.",
            "And it's a very difficult and painful process, and an also of course, children do not learn language from annotated corpora.",
            "We don't feed children.",
            "The Penn Treebank in the crib, in order for them to learn English and neither do they learn language from just reading the newspaper, surfing the web or listening to the radio.",
            "There's a lot of certain amount of work.",
            "Most successful work in natural language processing in supervised learning.",
            "But there's a growing body of unsupervised.",
            "It just tries to use raw text to learn language.",
            "There's also a new project funded by the DARPA agency in the US called Learning by Reading where they're trying to develop systems.",
            "This is just in the early stages that actually will just look on the web and try to learn language by just looking at tons and tons and tons of text.",
            "On the web, but I really don't think this is the right way to approach language.",
            "Children don't learn language just from language alone.",
            "I mean, I'm sure that you government of the US Government will not allow you to do this experiment, but try raising your child with all you do is have them surf the web or listen to the radio and see if they learn language.",
            "You know you're not going to get promove up.",
            "Hopefully you're not going to get any permission from your government to fund those studies because we know it's not going to work.",
            "Children need to be exposed to language in the context in which that language is relevant, and use in art and learn languages.",
            "And I think if we're ever going to get.",
            "Our computer systems to really, truly deeply understand language, we need to do the same thing.",
            "So the natural way to learn languages to perceive language in the context of its use in the physical and social world, and that requires inferring the meaning of utterances from the perceptual context and not having poor graduate students label large corpora with formal expressions, representations of meaning.",
            "So how can we get around?"
        ],
        [
            "That so another issue that comes up here is what is sometimes called language grounding, where the the meanings of many words are grounded in the perception of the physical world.",
            "So I know what the meaning of red is or ball is, or Cup or run or hit or fall, because I understand the events in the world that that that that word refers to and how it connects to my perception of those events in the world.",
            "Steve Tarnet had this very influential paper back in 1990 called symbol grounding, where that a fundamental problem in AI is connecting our abstract.",
            "Symbols in our.",
            "In our formal systems to actual the perception and interpretation of those events in the world itself.",
            "And you might say, well, a lot of languages about abstract things.",
            "It's not about observable things in the physical world, but actually most of our abstract words are actually grounded as sort of metaphors.",
            "Are abstractions grounded in the physical world itself?",
            "Just simple words like up and down and over, and we use those in a metaphorical sense, but their original meanings are do come from the physical world, so a very famous book called Metaphors We Live by by Lakoff and Johnson, tried to make this point quite clearly.",
            "Here's just one example I have is.",
            "It's difficult to put my word into ideas.",
            "It's as if I have containers called ideas.",
            "I'm somehow shoving these words into them, and we use that sort of language to talk about abstract ideas and think of ever going to really understand these sorts of senses.",
            "We also have to understand the physical metaphor that lies behind them.",
            "So, but contrary to this, most work in semantics and natural language process tries to represent meanings with by some circular sort of definitions of these words being.",
            "These other words, are these words mean these formal expressions, and I think if we're ever going to really get to understanding language, we have to not have these circular definitions defining means of words in other words, but we need to have the grounding of our language of the having our eventual primitive terms connected to our perception of those objects and events.",
            "In the world.",
            "Um?"
        ],
        [
            "OK, so here's a very picture version of what we're trying to do.",
            "It sort of trying to emulate the sort of problem that's confronted with language learning as a child is so the child in some environment there's a lot of things going on in his perceptual environment.",
            "The the psychologist William James called this the buzzing, blooming confusion, right?",
            "The child comes into the world.",
            "There's all this stuff going on in their perceptual system, and, you know, say the child here.",
            "Some sentence is like Mary is on the phone, and you know he's looking at a lot of things in the world, and he doesn't necessarily know which one of those that refers to.",
            "Maybe it's not referring to anything currently.",
            "In his perceptual environment, but hopefully a certain fraction of the time it is.",
            "I.",
            "So what we would like to be able to do is learn language from this sort of."
        ],
        [
            "Big Yuus sort of supervision where we have a bunch of things happening in the world and we hear some sentence that's probably refers to something happening out there, but we don't know exactly what we'd like to build a computer system that Simon today exposed to perceptual context and natural language utterances and be able to learn the underlying semantics from that.",
            "So we have this sort of ambiguous sort of training data, where each sentence is associated with a multitude of possible things that could mean, based on what's currently happening in the world at that point.",
            "So Jeff does siskin back in the 90s, did some interesting work on Lexicon learning, just learning the meaning of words.",
            "Using this sort of what he called referentially uncertain training data where each sentence was paired with a number of possible meanings, and from that he tried to learn meanings of individual words, but he didn't go on to try to learn meanings of complete sentences and build a compositionally semantic system that can construct meanings of sentences in generate, and so we're trying to do is take this ambiguous sort of data and be able to learn semantic parsers and language generators from it.",
            "An extract of course, the problem of extracting meaning representations from the perceptual environment is very hard, right?",
            "People in computer vision have done a lot of work of trying to do this over many decades, and so we're currently sort of sidestepping this problem.",
            "I'd like to eventually get back to it.",
            "I'll talk about briefly about that at the end, but for right now we're assuming that that step is somehow automated, that we're able to abstract the actual perceptual world into a bunch of symbolic representation of what's happening."
        ],
        [
            "So instead of having this picture like this, we first replace."
        ],
        [
            "The baby, of course.",
            "With the computer we want to learn language and since we don't want to solve the computer vision problem, we assume."
        ],
        [
            "And that instead of these actual perceptual images, we get some symbol."
        ],
        [
            "Like representation of these meanings now I've used English words here to just give you some vague idea of what I'm talking about, but these could be arbitrary conceptual symbols that have no connection to the."
        ],
        [
            "Natural language, I just know that this natural language string probably refers to one of these conceptual representations that are out there in the world at that time."
        ],
        [
            "I'm.",
            "So they then say you know the context changes and somebody utters another sentence so that the idea of learning this context is my perceptual system is."
        ],
        [
            "Giving me this constant stream of changing, you know descriptions of objects and events that are out there in the world, and then I hear language uttered every so often, but it could refer to a lot of different things.",
            "Can I actually learn language from this sort of ambiguous sort of supervision?",
            "So."
        ],
        [
            "This is an interesting problem that we've been working on for a couple of years now and we tried.",
            "We built a model that tries to learn to take this sort of ambiguous supervision that corresponds to this sort of idea of a stream of perceptual events with occasional language commentary and try to learn language parsers and generators from that sort of data.",
            "Now we're making currently a very simplifying assumption, which is that each sentence has exactly 1 meaning in the perceptual context, so any sentence I uttered, it refers to one of these things that's happening in the world, and that's clearly a vast oversimplification.",
            "I think it's a reasonable place to start.",
            "We've currently made some changes so that some sentences can just not refer to anything that's happening in the world at this point, and we've also we have a slightly different algorithm from the one out of the straight here that tries to handle that case, but by and large we make the assumption that each sentence means something that's happening in the world at that point.",
            "And at most one sentence.",
            "So each meaning is associated with it."
        ],
        [
            "So what you end up with?",
            "Sorry if you can't see the green, it's always the colors.",
            "Always project a slightly differently than they look on the screen, but the idea is on the one side of this bipartite graph, so this turns into a bipartite graph for the training data on the left hand side we have, we have a bunch of sentences in natural language here in English and on the right hand side we have a bunch of conceptual representations.",
            "Again, I've used English words there, but that could be all in gibberish.",
            "It's just a matter of the English words helps you, and I understand it, and I have these dotted line would say, well, this sends could mean that.",
            "And I get a bunch of these sequence of sentences that could mean any of these various conceptual representations that are coming in through my perceptual system."
        ],
        [
            "So we originally took our system, crisp.",
            "I didn't talk much.",
            "I talk just briefly about Chris before it uses string kernels in SVM's to do this semantic parsing pass and a triple AI.",
            "Last year we had a paper we presented the system called crisper, that is an extension of that system.",
            "To learn with this ambiguous sort of training.",
            "Initially assumed I have sentences paired with unique meaning representations, we developed a new version we called Crisper, which stands for Crisp with them, like retraining.",
            "Where we do this sort of EM tight loop of self training.",
            "It's not strictly em because we don't have a proof of convergence to a maximum likelihood estimate, but it's following the same sort.",
            "Framework where it changes the representation matching to each of the sentences gradually overtime until it converges on one that's hopefully correct."
        ],
        [
            "So I have a nice animation here that tries to illustrate this, so this is just some artificial data that my student Rohe Cotte originally generated to do this task.",
            "So it's you might seem a little weird if you actually read it.",
            "'cause we just sort of had a bunch of objects in the world in a set of events or descriptions and we sort of randomly generated pairs of sentences with sets of potential meaning.",
            "So initially you start with this bipartite graph, where you each sentence is ambiguously linked to several different events."
        ],
        [
            "Well, the first thing we do is just say, well, let's train a system.",
            "Assuming all these representations are correct, so I'll give it a gold standard training example.",
            "Each sentence linked to every meaning representation it could mean, and I give all of this supervision.",
            "And now in both lines to a normal supervised system.",
            "Now most of the supervision it's getting is garbage, right?",
            "It's noise, but maybe it can work out if it does some good learning and it avoids overfitting.",
            "It can separate some of the wheat from the chaff and figure out what's going."
        ],
        [
            "Now to make this a little bit simpler, we know a little bit more about how ambiguous each of these sentences is, right?",
            "'cause some sentence is, there's only one event happening in the world when that's that was uttered.",
            "Other things, there might be 100 things happening, so we weight each of these training examples by a fraction of how ambiguous it is.",
            "So if it has two meanings, each link gets 1/2.",
            "If it has five meanings there each way it gets 150, so it gives the learner a little bit more idea of how likely each of these is to be a correct example.",
            "Now we give these weighted training examples to our normal supervised learning system.",
            "And it learns an initial semantic parser in this case.",
            "But of course there's potentially a lot of air."
        ],
        [
            "In that data.",
            "Now we take the initial parser we learned in iteration one, and we go off and we try to parse each one of these sentences and assign a probability.",
            "So this is a probabilistic system that will reduce the probability that a given sentence Maps to a particular meaning representation.",
            "We assign these weighted links based on the probability that the initially learn parser believes that that's the correct representation for that sentence.",
            "OK, now we get this weighted bipartite graph.",
            "The next thing we do."
        ],
        [
            "Do is we use this old algorithm from the 50s to solve what's called the maximum weighted matching problem where we now take?",
            "We have since we're assuming each sentence actually corresponds to exactly 1, meaning we do a maximum matching where we try to pick one link for each English sentence that maximizes the total weighted sum of all of the matches that we get, and that will eventually give us this."
        ],
        [
            "Green bold matching there.",
            "Now notice every sentence doesn't get its maximal probability because.",
            "Let's see, we have this here.",
            "This sentence with a 95% probability means this, but with 89 it means this.",
            "So this sentence grab this sentence.",
            "So this can't have it, so it had to go to its second best choice of .85.",
            "So this maximum matching algorithm solves the highest weighted match given the constraint that each sentence has to correspond to one representation in each representation can represent the meaning of at most one sentence."
        ],
        [
            "OK, then we just get rid of all the other noisy supervision.",
            "We trained it on this gold supervision that we got from the maximum maximum weighted matching and we trained the system.",
            "And now we have a new semantic parser, and we iterate this just like when you go through iterations in VM Ware.",
            "Each step it converges on a better and better and better solution until this converges to some fixed point.",
            "OK, so that's how the basic resolving of damned."
        ],
        [
            "Duity works so Rohit originally tested this in some artificial data that we constructed that I briefly referred to, and we.",
            "Compared the red line here is training on gold standard data where I give it direct supervision.",
            "There's no ambiguity and we see as we go from blue to green to this diamond curve.",
            "Here we're adding more and more and more ambiguity to the data.",
            "So the number of dotted lines starts increasing and increasing, so I have less idea directly what things mean.",
            "But given enough training examples here on the X axis, the system starts doing better and better.",
            "It starts quite poor with just 225 ambiguous examples.",
            "But as we get up to 900, it's starting to get close to as if I didn't have any ambiguity in the training data at all.",
            "So given enough of this ambiguous data, it can start to approach the accuracy of learning with supervised gold standard data.",
            "Ah, OK."
        ],
        [
            "Some more recently we've been trying to attack do what I think is an interesting challenge problem, which is what I call the learning to be a sportscaster task, which is ideas to learn from realistic date of natural language used in a representative context, while avoiding the difficult issues in computer vision.",
            "So I know this problem is hard enough.",
            "I don't try to solve the speech problem.",
            "I use textual input and I don't try to solve the computer vision problem.",
            "We use symbolic representations for what the various conceptual things that were pairing the language come from.",
            "So we we chose this task of learning to sportscaster Robo Cup game, given textually annotated traces of activity in some in the simulated environment where I don't have to solve the computer vision problem.",
            "So we're given traces of games in the Robo Cup simulator paired with textual sportscaster commentary.",
            "So it looks like this.",
            "You have a simulated game, some user sits there and watches the game.",
            "I'll show you a video of this in a minute and we have a simple sort of simulated perception that takes the low level details of the simulator and extracts these high level sort of perceived facts that I was showing you on the right hand side about various events.",
            "I'll show you.",
            "Example in a minute.",
            "Meanwhile, the sportscaster is giving me some linguistic input in text.",
            "Since we don't deal with speech and then both of that perceptual output and the linguistic input goes into what I'm calling the grounded language learner here and we for our wash system.",
            "This produces a synchronous context free grammar that can support both tactical generation, an semantic parsing.",
            "It can either take English in and generate meaning representations or generate taken meaning representations and generate language.",
            "So then if I watch a new game.",
            "Perception can go through and generate events that are happening in the game.",
            "In this formal language, and then I use my natural language generator that I've learned to automatically generate text describing that game.",
            "So the idea is then to build a system that just watch as a human sportscaster game, and then I give it a new game.",
            "And I say, OK, you tell me what's happening and it has to learn to be able to do that."
        ],
        [
            "So here's an example of what the input to the system looks like.",
            "I have English comments here on the left hand side, and then I have all these events that are being extracted from the pre simulated perception and notice."
        ],
        [
            "Is that the fact that and then we use a 5 second window, so anything that this sentence, any perceptual event that's happened within the five seconds preceding that comment, get attached as a possible meaning for that sentence, and notice that I use English terms here in the meaning representation, but that's totally."
        ],
        [
            "And we first establish the human goes in, and at this is not the learners not going to see this data, but for evaluation purposes we had a human judge go in and decide what was the correct corresponding representation for each sentence and notice some of them don't have one.",
            "So there's a comment here.",
            "Purple Team is very sloppy today.",
            "Our perceptual system is not sophisticated enough to extract abstract descriptions of the world like the sloppiness.",
            "So this sentence doesn't actually correspond to anything, so it violates our one to one assumption.",
            "And down here we have.",
            "Pink 11 looks around for a teammate.",
            "It doesn't know how to match that.",
            "So all of this is sort of noisy.",
            "Every sentence doesn't necessarily correspond to a meaning representation, but most many of them do, and many of them actually do have a correct thing that they correspond to the green line.",
            "But the system doesn't see the green line.",
            "That's for value."
        ],
        [
            "Nation only and I also want to note that I used English words here for these meaning representations, but you could translate this into arbitrary tokens.",
            "That's all that's assumed.",
            "Here is your meaning.",
            "Your perceptual system generates some symbolic representation that's represented in mentales, right?",
            "That has some internal language.",
            "It has nothing to do with natural language.",
            "We're currently in the process.",
            "I have a Korean student that's doing Korean annotation on these games to show that the system is language independent.",
            "It can learn to generate descriptions of games in Korean as well as in English, but right now we just have English result."
        ],
        [
            "So here's what we did.",
            "We collected human textual commentary for four Robo Cup championship games.",
            "The champion team, the final game in the in the Robo Cup Simulator League from 2001 to 2004.",
            "The perceptual system extracts about an average of 2600 events that happen in each one of those games, and the human talks about gives about 500 sentences in English, describing things that are happening during that game.",
            "Now each sentence is matched to all events within 5 seconds, which means on average each sentence refers to about 2 1/2 events, which actually is pretty low ambiguity.",
            "I think for the real world, but I thought it think its original place, reasonable place to start, but there's quite a variance.",
            "Some of the sentence is only have one thing they could possibly refer to.",
            "Some could be referring to is up is as many as 12 different things.",
            "Now we manually annotated each of the sentences with the correct meaning representation.",
            "If one existed in the perceptual output, and this is again not given to the machine learning system, that's just so we, as experimenters, can evaluate how well it's establishing the matching between the sentence and the Rep."
        ],
        [
            "Station.",
            "OK, so we built several systems to be able to learn from this sort of ambiguous data and learn this task of sportscasting.",
            "So I mentioned crisper that can learn from this ambiguous data, but crisper only does semantic parsing.",
            "It doesn't do generation, and we want to be able to learn to be a sportscaster to generate language.",
            "So we developed a version of our wash system which remember can do both semantic parsing and generation by using the symmetry of a synchronous context free grammar.",
            "And we just added this sort of M like retraining loop to wasp as well as two crisp, and we built this system we call losper."
        ],
        [
            "Um?",
            "Now we also built a system that we called crisper was because this am at this initial training with ambiguous data, creates a lot of noise.",
            "I showed you that there's 2 1/2 meanings for each sentence, which means that more than half of the the supervision we give to the system is noise.",
            "It's actually incorrect, and that our system Chris we've shown empirically, is a little better at handling large amounts of noisy data because it uses support vector machines to use the Max margin criterion to try to prevent overfitting.",
            "It also has the string kernels in that gives it a little bit more flexibility.",
            "So by and large, Chris does better at handling noisy supervision than what does, which is a little bit more of a symbolic system that learns these symbolic S CFG rules.",
            "It has this.",
            "This Maxent log linear model on top of it, but the rules itself are sort of hard matching rules.",
            "But Chris doesn't support language generation, so we built a system that first trains crisp to determine the matching.",
            "So we run crisper to get those dark green lines that established the right matching between the sentence in the meaning representation.",
            "Then we give that data as gold standard training data to wasp.",
            "So Wasp can learn a generator from that.",
            "So it's somewhat of a hybrid between this Chris system that uses SVM and string kernels together with loss, which uses synchronous context free grammars."
        ],
        [
            "OK, we finally build a system that was more focused on the generation pass, so even Wasp are is focused on the task of learning to semantically parse natural language into meaning representations.",
            "We built a system called Wasp Arjen that sort of inverts the role of those, and so we generated quite wasp Arjen and it tries to purposely focus on the task of generation.",
            "Determining the best matching based on generation.",
            "So remember those scores that I attached to each of the dotted links based on the probability that this is the correct parts for this sentence.",
            "We now assign numbers based on going the other way.",
            "If we take the input representation, what score does it give to the generated English?",
            "So it focuses the system more on the generation task rather than the parsing task.",
            "And the details of why we did.",
            "How we did this.",
            "I recommend you look at the papers for those who might have some familiarity with statistical machine translation.",
            "We computed a standard machine translation score called Nest, which is slightly different from the blue score.",
            "If you're familiar with that between the generated sentence in the potential matching sentence.",
            "So we had the system generate its best language and then we compared that language to the human language and scored how well the human language matched the system language and use that to score the links in the maximum matching rather than the parsing scores.",
            "But basically it just creates a system that tries to resolve matches in a way that best maximizes the performance of generation as opposed to parsing."
        ],
        [
            "OK, so now a problem that I haven't talked much about that I'd like to briefly talk about is strategic generation, which is knowing.",
            "What to say rather than just how so strategics as I tell you in formal language what to say and you tell me, just translate it into natural language.",
            "But how do I pick what to say so in the."
        ],
        [
            "Switching task and the sportscasting task, the perceptual system is extracting all of these events.",
            "The human doesn't talk about everything that's happening on the field, right?",
            "They chew."
        ],
        [
            "Use a particular set once.",
            "Here I put here in red to actually talk about, so the system has to learn how to identify the red event.",
            "The interesting events to talk about out of all the events that my perception is giving me."
        ],
        [
            "Well, I don't have time to go into the details on this, but we learned a very simple model for just putting a probability on each type of event, like kicking versus passing versus the ball stopping versus, you know interceptions and things of that sort.",
            "But that requires us to know the correct matching between the sentence in the representation, but we don't know that match, but we can estimate it from the matching that comes from wasp are or wasp ergener crisper.",
            "We also developed another version that establish is the best matching purposely for the.",
            "The reason of determine the probability that events are commented on, and again."
        ],
        [
            "I don't have time to talk about this.",
            "We developed this algorithm, we called iterative generation strategy, learning that again uses an M type iteration to converge on a hyper good probability estimate of how likely each event is talked about by by the human being.",
            "So again, I will point you to the papers for more evidence."
        ],
        [
            "It's on that.",
            "OK, so I want to just briefly give the demo of how this system has learned to talk about Robo Cup games, so this is a test game.",
            "It never was trained on this game.",
            "We trained it on other games and now I'm asking it to comment on this new Robo Cup game, and here's what it does, and there's a text to speech system here that translates the output in text to speech.",
            "And it's had to learn the names of all the players in order to do this.",
            "Well that gives you at least a little idea of what the final system is capable of doing.",
            "So it actually learns to sportscaster these Robo Cup games reasonably affectively.",
            "So that's sort of the cute."
        ],
        [
            "Demo, but what about quantitative evaluation?",
            "I'm all about quantitative evaluation, so we generated learning curve by training on all combinations of one to three games and testing on the games not used for training.",
            "We compared to a couple of baselines, which is.",
            "I went through this complicated algorithm tries to establish how each English sentence corresponds to some event in the world.",
            "What if I just pick an event at random that I just take one of those dotted lines at random and make it solid to represent the meaning of that sentence?",
            "How well does that work?",
            "That's like a lower bound and upper bound is what if I actually use the matching between the sentence is in than the representations that the human expert provided we call that gold matching and then for each of our systems we also compared on standard metrics of precision, the percentage of the system annotations that are correct.",
            "Recall the percentage of the gold standard annotations that were correctly produced by the system, and then the standard F measure, which is the harmonic mean of these two."
        ],
        [
            "Vision and recall.",
            "So what we've evaluated several things.",
            "One is how well out of this ambiguous training data does it actually establish the right representation for each one of these sentences?",
            "So we measured how accurately various methods assign the correct meaning representations to each sentence in the ambiguous comparing to the gold standard human of.",
            "Matching as the."
        ],
        [
            "Uation and here we see the results on this on F measure, you know.",
            "So the random baseline actually doesn't do too horribly bad here, because it's slightly less than 50%, right?",
            "'cause I showed you that on average there's 2 1/2 representations percent, and so if you guess it right, you're going to get 50%.",
            "Now.",
            "The systems we currently have gone through a lot better, crisper does the best, because it's the best at handling this sort of noisy supervision.",
            "Gets maybe close to 70%.",
            "Wasp Arjen does pretty well at about 60 three or something.",
            "It looks like there so.",
            "Or matching him up much, you know, signaling better than random chance, but still, there's definitely room for improvement on this.",
            "I think if we improved our matching, we'd get better results overall."
        ],
        [
            "We also evaluated how well does it translate natural language into these meaning representations, and we have to exactly match the gold standard representation to count is correct.",
            "There's no partial credit for a sentence.",
            "You either get the meaning representation exactly right or you get it totally wrong and he."
        ],
        [
            "Some learning curves just to show you how it does on the semantic parsing task.",
            "If the red line and here is an upper baseline, it's cheating, right?",
            "I'm giving it the correct matching in the training data, so that's sort of an upper baseline.",
            "We see Chris Pier is the green line here, so the funny it doesn't do well with just one training game, but by three training games.",
            "It's actually doing the best, so it seems to learn better eventually, but it learns a little bit more slowly, and you know crisper wasp here doesn't do too bad either.",
            "In the blue and then.",
            "Of course, the bottom baseline here is the random matching, which doesn't do nearly as well."
        ],
        [
            "What about generating natural language from English?",
            "Well, we did this evaluation of technical generation where we had the system generate English for those representations that we knew had in English corresponding expression, then compare that using this empty evaluation score called Miss, which just measures how close is the English that the system generated to the English that the human being generated for that event."
        ],
        [
            "And really quickly here on the results, you know we get the upper baseline here is in red and then our best performing system for generation.",
            "Not surprisingly is wasp Arjen because during the Valley Valley the resolving of the ambiguities, it focuses on resolving ambiguities in a way that maximizes generation performance.",
            "So not surprisingly, when we evaluate the system on generation, that system does the best.",
            "So that's why when we're actually producing these actual sportscasts, we use Wasp Arjen, because it's been trained specifically to do good at the generation task to do well at the graduation."
        ],
        [
            "Patient asked.",
            "We also evaluated how well does it pick out the right events to talk about?",
            "How well does it match the choosing of the events to talk about that the human chose.",
            "So we compare the set of events that the system chose to talk about with the set of events that the human."
        ],
        [
            "Chose to talk about an here's the F measure score between the matching of what the system talked about versus what the human talked about, and notice if we have a gold standard matching.",
            "It gets about what 75% of the time it agrees with to talk about the same thing that the human does, and our best performing system.",
            "This new algorithm that I didn't have much time to talk about I GSL, comes arbitrarily close to doing as well as it could do if it actually knew the correct matching between the natural language and the meaning representation.",
            "So we're pretty happy.",
            "With how well it actually decides what to talk about, it pretty quickly learns that you talk about kicking an interception and scoring, rather than some of these more a boring events that actually come about in the game."
        ],
        [
            "OK, the last final valuation I wanted to talk quickly about is what sometimes I call the quasi Turing test.",
            "So we ask for fluent English speakers to evaluate overall quality of sportscasts.",
            "We randomly picked a 2 minute segment you just saw part of one of the two minute segments that we showed to the human judges.",
            "Unfortunately we did this with the human judges.",
            "We didn't have the text to speech working, so they had to actually read like they had to read subtitles and watch the game, which is trickier.",
            "It's much easier to evaluate these things if you can listen to the speech, so we'd like to redo this evaluation now that we have this speech synthesis working.",
            "And each human judge evaluated 8 commented game clips.",
            "Each of the four segments commented once by a human and once by a machine, and we jumbled them up, right?",
            "So the human doesn't know whether their evaluating a human generated sportscast are machine generated one and they scored this."
        ],
        [
            "Based on several metrics of evaluation, how fluent is the English?",
            "How you know, does it sound like good English?",
            "How much does it correspond to the actual events that are happening on the field?",
            "Is it actually saying true things that are happening in the game and then we just gave him this vague thing?",
            "You know how good a sportscaster is this guy?",
            "Our computer, they didn't know which one was."
        ],
        [
            "So here's the eventual result.",
            "Now we weren't expecting to pass the Turing test.",
            "We weren't expected this system to be as good as a human being, but we were hoping that it wasn't too much worse.",
            "So on a 5 point scale, we can see that the system does about a half a point up to two point 7% points out of a 5 point scale less than the human.",
            "And I told my student that if we got within one point on a 5 point scale, I thought that we were demonstrating that the system was more or less working so.",
            "Of course, we still have a little bit of room here to get to human level performance, but I think this is a clear demonstration that these are that's generating sportscasts that people can understand.",
            "the English isn't bad, and they're fairly accurate and close to human performance, but not matching it yet."
        ],
        [
            "OK, so real quickly on some future directions on this work.",
            "There's some immediate things we'd like to do in this sportscasting domain.",
            "One thing is, the correct matching is not established by any information about the strategic generation, so it doesn't use the fact that certain events are more likely for a human to talk about to try to establish the right matching.",
            "And we think we could do some things to make that matching score better.",
            "I showed you were not doing all that well on matching still lost.",
            "We'd like to be able to make it more resilient to noisy training data.",
            "We think there are various, you know, modern techniques in machine learning to prevent overfitting to do that.",
            "And also the current simulated perception sort of pours out puts out a fairly low level set of boring events happening in the game.",
            "If we if the perceptual system could recognize more interesting things, then I think the language could be able to attach to more interesting events that are happening in the world.",
            "Um?"
        ],
        [
            "On a longer term direction I I started thinking about this problem in a more general machine learning context an I think there's been certainly working machine learning work on learning from sort of what you might call weak supervision.",
            "Particular, there's this type of learning.",
            "Presumably most of you are familiar with called multiple instance learning, where you assume a weak supervision where you don't label examples positive and negative.",
            "I give you so called bags of sets and all I tell you is for the positive bags.",
            "One of the examples in this set is positive.",
            "Not all of them necessarily.",
            "There's also this task of structured output learning, where I try to map from string to string or tree to tree or string to tree.",
            "There's been a certain amount of work in what's called structured output learning or learning with structured data which connects to the whole area of statistical relational learning.",
            "Also, this is a recent.",
            "This references to a recent book from MIT Press with a bunch of stuff unstructured learning My My problem of learning these semantic parsers actually combines these two problems, weak structured.",
            "Data, so I think there needs to be work on this new topic, which I've called structured multiple instance learning, where an input string is paired with a set of possible output.",
            "So I say, here's an input structure.",
            "One of these is the correct output structure and I give you a bunch of sets like that.",
            "How can I learn to produce structured output in the face of ambiguous training data?",
            "And that's exactly the sort of problem here I'm confronted with.",
            "We've concocted sort of our own method specific to this language problem, but I think structured multiple instance learning is a general problem.",
            "That might occur in other applications also, and I think it's an interesting problem of combining structured output with multiple instance learning.",
            "I haven't seen any work on this.",
            "If anyone has time, I'd be curious to learn about it."
        ],
        [
            "A little bit on the longer term sort of directions.",
            "To avoid the computer vision problem, we worked in this simulated domain.",
            "I think a really potentially interesting area is working on learning language in videogame environment.",
            "So Deb Roy and his some of his students like Gorniak it at the MIT Media Lab.",
            "That done some interesting thing where you could teach agents in an AI videogame environment to learn language about their world and you don't have to solve the computer vision problem because it's all in this simulated environment.",
            "But there are a lot of people who play these games and wouldn't it.",
            "Wouldn't they enjoy it if they could actually teach there?",
            "AI in the game to talk to them in natural language, and I think using some of this technology we could develop more interesting computer games where you could actually teach the agents in the game to talk to you.",
            "Of course, the next obvious step is to move beyond this, given the perceptual input and move to really connecting language to actual video and pictures, so it would be nice to do this sort of work where we actually have real vision computer vision to extract objects and relations and events from real perceptual data.",
            "Another one of Debroy stood at the MIT Media Lab.",
            "Mike Fleischmann has done some interesting work using Closed caption Text on video and learning from the correspondence between the caption video and the actual video information, and."
        ],
        [
            "There's a blatant plug for one of my students who will be giving a talk on Thursday at 11:40 in R002 in the semi supervised learning section.",
            "I've started my first work on really doing computer vision with a computer vision colleague of mine, Kristen Grauman at the University of Texas together with a couple of new graduate students.",
            "So now Gupta, who will give the talk on Thursday about called Watch, Listen and Learn Co training on captioned images and videos where we're trying to learn to interpret images and video by corresponding what's happening in the video and in the image.",
            "With the language description of that event, and I so I think it's one of our current steps towards trying to really do this and with real computer vision of connecting language and perception."
        ],
        [
            "OK, so to quickly wrap up, I've tried to argue that current language learning work uses expensive, unrealistic training data.",
            "We've started to develop language learning systems that can learn from sentence is paired with an ambiguous perceptual environment, and we've evaluated on the task of learning to sportscast simulated Robo Cup games where it learns to comment games not quite but in the same ballpark as the ability of humans to do this task an I think this general broader problem of learning to connect language and perception is an important exciting research area.",
            "Unfortunately I our various research communities have sort of become Balkanized where the computer vision people go to their conferences and talk about their stuff.",
            "They use a lot of machine learning these days.",
            "Obviously, the natural language people go to their conferences and very few people are looking at what I think is a very critical, important problem which is the connection between language and perception, and I think there's lots of interesting work to be done in that area.",
            "OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For such a nice introduction, I appreciate the invitation from the rest of the program Committee as well to talk to you today about some of the recent work that I've been doing with my graduate students on trying to connect language and perception and learning language from perceptual context.",
                    "label": 0
                },
                {
                    "sent": "So this is joint work with several of my current and recent graduate students, David Chen, Rohit Cotte, and Yaqub or John Wong.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the as Walter Walter alluded to, there's been a sort of revolution and natural language processing from manually developed systems to machine learning, develop systems, and most current state of the art.",
                    "label": 0
                },
                {
                    "sent": "NLP systems are constructed by training on large supervised corpora, various types.",
                    "label": 1
                },
                {
                    "sent": "So if you're doing syntactic analysis, you have resources like the Penn treebank to learn from labeled trees on sentence is how to do parsing.",
                    "label": 0
                },
                {
                    "sent": "If you're interested in an disambiguating ambiguous words, there's various corpora like the sense of balance Emma Val corpora.",
                    "label": 0
                },
                {
                    "sent": "Interested in what's called semantic role labeling.",
                    "label": 0
                },
                {
                    "sent": "Assigning semantic roles like agent and patient.",
                    "label": 0
                },
                {
                    "sent": "Two sentence is there's a prop bank.",
                    "label": 0
                },
                {
                    "sent": "If you're interested machine translation, there are lots of water, called parallel corpora of the same text in different languages, like the Hansford Corpus, which is the Canadian Parliament that by law appears in both French and English, but there's a lot of effort that goes into constructing these supervised corpora and a lot of time and expense, and an is gone is dedicated to building all these.",
                    "label": 0
                },
                {
                    "sent": "Labeled corpora for being able to develop machine to use in order to use machine learning algorithms to then train on these corporate developed natural language processing systems.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the particular task that I've worked on a lot over the last decade or more is a little bit less studied area that I call learning for semantic parsing, where semantic parser by my definition Maps and natural language sentence to a complete, detailed formal representation of its meaning.",
                    "label": 1
                },
                {
                    "sent": "Sometimes linguists will refer to this as logical form.",
                    "label": 0
                },
                {
                    "sent": "Here I'll use a somewhat less committal term, meaning representation or Mr, so it doesn't necessarily have to be strictly like a first order logic representation.",
                    "label": 0
                },
                {
                    "sent": "And particularly, the task that I've looked at, we've tried to focus on meaning representations that are immediately useful for some other program to do some task where the output of the semantic parser is not just what you know.",
                    "label": 0
                },
                {
                    "sent": "Some philosopher claims is the meaning of this sentence, but it's actually a formal computer language that then can be used to perform some tasks.",
                    "label": 0
                },
                {
                    "sent": "So over the last few years, we've looked at a couple of applications on this.",
                    "label": 0
                },
                {
                    "sent": "One is database front end, so I can query databases in natural language and not have to type in arcane query languages like SQL.",
                    "label": 0
                },
                {
                    "sent": "The other one my colleague Peter Stone at the University of Texas is deeply involved in one of the founders of the Robo Cup competition.",
                    "label": 0
                },
                {
                    "sent": "I assume mostly familiar with, which is, you know, robots playing soccer and we've developed something that learns to understand coaching instructions in English using that pass.",
                    "label": 0
                },
                {
                    "sent": "So just to briefly illustrate.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These sorts of tasks that we've done semantic parsing on.",
                    "label": 0
                },
                {
                    "sent": "Here's an example of a database application that we built a query about questions in US geography dating back to the mid mid 90s.",
                    "label": 0
                },
                {
                    "sent": "So you might get a question in like how many states does the Mississippi run through an I have to translate that via semantic parsing into some formal representation.",
                    "label": 1
                },
                {
                    "sent": "This is a prologue representation of a meaning of that sentence.",
                    "label": 0
                },
                {
                    "sent": "One of the issues that just one of the subproblems that comes up in this process is word sense disambiguation, right?",
                    "label": 0
                },
                {
                    "sent": "I have to infer whether the Mississippi in this case refers to the River Mississippi or the state of which there is.",
                    "label": 0
                },
                {
                    "sent": "Both in the US and then once I have that mapped into a formal prolog query, in our case, or you could put that into SQL or some more traditional database that goes into some database system and then comes out with the answer.",
                    "label": 0
                },
                {
                    "sent": "It turns out the answer is 10.",
                    "label": 0
                },
                {
                    "sent": "Another application.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That we've looked at in detail, is this Robo Cup coaching instructions, so there's a competition that the Robo Cup competitions hold as a lot of sub competitions within it, and one of the ones that's not one of the more popular ones is called the coach competition, where you actually compete to instruct your players as a coach on how to play the game and are to do that.",
                    "label": 0
                },
                {
                    "sent": "There's a formal language that the Robo Cup community is developed called Silang, just for coaching language, and you might have an instruction in English like this if the ball is in our penalty area then all players except player for should stay in our half.",
                    "label": 1
                },
                {
                    "sent": "In to get the RoboCop system to understand that you have to translate that into this formal language.",
                    "label": 0
                },
                {
                    "sent": "It looks a little bit like lispy for those of us who remember list, but it's hard to set of rule representation about what to do under certain circumstances, so we've developed systems that have learned to automatically translate from English into this formal language, which then can go into the RoboCop simulator.",
                    "label": 0
                },
                {
                    "sent": "I'll talk more about the World Cup simulator, so there are lots of leagues with real robots, but in some sense the simplest Leggett RoboCop is simulated Lee, where it's like a little video game and you control the individual.",
                    "label": 0
                },
                {
                    "sent": "Agents and if you give that formal language to the system, your players will actually obey that that that instruction.",
                    "label": 0
                },
                {
                    "sent": "So it's a way of having a semantic representation that's immediately useful for some other task.",
                    "label": 0
                },
                {
                    "sent": "So we've looked at both of these.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now as as I said, it's very hard to manually develop these systems.",
                    "label": 0
                },
                {
                    "sent": "You know, throughout the 60s and 70s and into the 80s, people try to manually write programs to do these sorts of tasks, like mapping sentences to logical farm, and it's very complicated, and it's hard to build a system that's robust and accurate across a wide range of input.",
                    "label": 0
                },
                {
                    "sent": "So our approach has been how can we use, you know, the latest machine learning technology to automate this task.",
                    "label": 0
                },
                {
                    "sent": "So if I just give you examples of sentence is paired with a formal representation.",
                    "label": 1
                },
                {
                    "sent": "And then I give that to some learning system.",
                    "label": 1
                },
                {
                    "sent": "It can automatically then generate a semantic parser that then if I give it a new natural language sentence, it's not seen during training.",
                    "label": 1
                },
                {
                    "sent": "It can accurately map it to some formal meaning representation, which can then go off to the database system or to the RoboCop simulator.",
                    "label": 0
                },
                {
                    "sent": "So to actually do something for me.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So over the last more than a decade, my graduate since I've worked on various approaches to automatically learning semantic parsers, I'm not going to bore you with a lot of the details of the earlier ones on this slide.",
                    "label": 0
                },
                {
                    "sent": "In this talk, I'll mainly focus on two of our most recent systems, wast developed by John Wong and myself, which there's a wide body of work these days in statistical methods for learning to translate 1 natural language into another, and we use some of the technology developed there, particularly it's called syntax based statistical machine translation.",
                    "label": 0
                },
                {
                    "sent": "And I'll talk a little bit more about that later.",
                    "label": 0
                },
                {
                    "sent": "Our other most recent system is a system called Chris developed by my student Roky Cotte, and it uses a series of SVM classifiers that use string kernels.",
                    "label": 1
                },
                {
                    "sent": "If you're familiar with string kernels to iteratively build up semantic representations for sentences, I'll talk a little bit about Chris, but I'll mainly focus on the wasp system today.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are OK, so while says I said uses machine translation approach to semantic parsing and it uses the latest techniques in statistical empty.",
                    "label": 1
                },
                {
                    "sent": "So within the the statistical machine learning machine translation community there have been development and use of these formalisms called synchronous context free grammars lately, which I'll abbreviate, CFG and they've been widely used by Wu in Malamed and David Chang and other people to do.",
                    "label": 0
                },
                {
                    "sent": "To learn to build systems to translate from one natural language to another and another one of the underlying technologies.",
                    "label": 0
                },
                {
                    "sent": "Here she goes back to the early 90s by the group at IBM that started the work in statistical machine machine translation on word alignments, where the basic idea is just from parallel corpora of English text paired with French sex taste.",
                    "label": 0
                },
                {
                    "sent": "Identify the word correspondences automatically and learn that word in English translates to what word in Spanish and that's typically called word alignment.",
                    "label": 0
                },
                {
                    "sent": "So by using both of these technologies, we developed a system that learns semantic parsing and we call it what it says.",
                    "label": 1
                },
                {
                    "sent": "Acronym for Word Alignment based semantic parsing.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's sort of a larger picture of the framework that we're working in, so with in machine translation, these technologies have been used to map from one natural language in.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh, another, but we're interested in the task of there's some formal computer language, like this coaching language or a query language for a database, and we're interested initially were primarily interested in the semantic parsing task is how can we translate a natural language into a formal language.",
                    "label": 0
                },
                {
                    "sent": "The computer system can actually utilized we.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, more recently been interested in the reverse task, which in in natural language processing is typically called tactical generation, will see why they called it's called tactical compared to strategic, which is an issue we'll talk about later where we take an expression in a formal language and translated back into English so we can communicate that more effectively to a human being.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, all of this.",
                    "label": 0
                },
                {
                    "sent": "All of these tasks can be done in the context of synchronous parsing using these synchronous context free grammars.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it actually originally synchronous.",
                    "label": 0
                },
                {
                    "sent": "Context free grammars were developed in the compiler community by a hoe annulment way back in 1972 to translate from one formal language to another, mainly from a high level programming language into a low level machine language, and so all of these different types of translation within and between formal and natural languages can be handled with this general nice framework of synchronous.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Grammars so again there originally developed by 1/2 an omen way back in 72 as part of a compiler theory to be able to combine syntax analysis and code generation in a single step, and the idea is a normal context, free grammar or any grammar generates one string in one language.",
                    "label": 1
                },
                {
                    "sent": "Synchronous grammar.",
                    "label": 1
                },
                {
                    "sent": "Eighters synchronous grammars generate two strings in two different languages.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For a pair of string, so here's how it works.",
                    "label": 0
                },
                {
                    "sent": "Isn't it a simple example?",
                    "label": 0
                },
                {
                    "sent": "In the case of translating database queries into a formal query language, we might have a top level non terminal like query and then it has two right hand sides, one in English like what is City for some other non terminal city and then that has a representation in the formal side of answer.",
                    "label": 1
                },
                {
                    "sent": "Open Paren, City, close paren.",
                    "label": 0
                },
                {
                    "sent": "So the grammar consists of generating two strings, one in each of the language.",
                    "label": 0
                },
                {
                    "sent": "If we're doing this machine translation, we might have one side B English and the other be French.",
                    "label": 0
                },
                {
                    "sent": "On",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then what happens is you have synchronous derivation that derive two strings, one in each of the languages.",
                    "label": 0
                },
                {
                    "sent": "So we start with our top level sentence nonterminal like query and we have these things called synchronous context free grammar rules that are illustrated on the bottom like the one I showed you on the last slide and then we apply each of those recursively.",
                    "label": 0
                },
                {
                    "sent": "Whoa, whoa, what happened?",
                    "label": 0
                },
                {
                    "sent": "Sorry bout that.",
                    "label": 0
                },
                {
                    "sent": "Wow, I lost all my my gosh something went crazy.",
                    "label": 0
                },
                {
                    "sent": "Sorry bout this.",
                    "label": 0
                },
                {
                    "sent": "Where were we?",
                    "label": 0
                },
                {
                    "sent": "That's the problem with this technology.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it screws up on you.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we we re derive strings in both.",
                    "label": 0
                },
                {
                    "sent": "Sentence is at each point applying one of these synchronous rules to rewrite a non terminal that shared between the two until eventually we get a complete parse tree on both sides and then that gives us a translation between the one language and the other.",
                    "label": 0
                },
                {
                    "sent": "In this case between a natural language query and a formal prolog query that we can then send off to Prolog to actually answer this question like what is the capital?",
                    "label": 1
                },
                {
                    "sent": "Ohio, which is actually Columbus, which is where a bunch of computational linguists were this summer, because that's where the computation linguistics conference was.",
                    "label": 0
                },
                {
                    "sent": "OK, so these synchronous grammars provide a nice formalism for doing translation because they provide a nice grammatical formalism for generating two strings.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one of the problems with language, of course, is it's ambiguous and even a simple word like Ohio in this domain of US geography is ambiguous because I don't necessarily know whether I'm talking about the state which is this synchronous.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Terra Bashan, or if I'm talking about the River.",
                    "label": 0
                },
                {
                    "sent": "There's also an Ohio River in the US, but of course capital rivers usually don't have capitals.",
                    "label": 0
                },
                {
                    "sent": "And cities do, and so the system needs to realize that derivation one here is much more likely than derivation two, so additional on top of this synchronous grammar.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have weights attached to each one of the synchronous grammar rules, which is a very analogous for those of, you know about probabilistic context free grammars.",
                    "label": 0
                },
                {
                    "sent": "Except here the numbers don't need to be the actually probabilities.",
                    "label": 0
                },
                {
                    "sent": "Their arbitrary weights that get put into an exponential model to do the disambiguation.",
                    "label": 0
                },
                {
                    "sent": "So where we have the two derivations here of a capital of a city versus the capital of a River, and we have numbers assigned, each one of those productions rules and you can see here in this case that the production rule for.",
                    "label": 0
                },
                {
                    "sent": "Cities of rivers is much lower than this production rule that subsidies of States and these numbers.",
                    "label": 0
                },
                {
                    "sent": "These Lambda parameters are going to be learned from training data eventually.",
                    "label": 0
                },
                {
                    "sent": "And now we can by using a simple exponential or log linear model.",
                    "label": 0
                },
                {
                    "sent": "We can assign a probability to each one of these derivations.",
                    "label": 0
                },
                {
                    "sent": "The Z.",
                    "label": 0
                },
                {
                    "sent": "Here is a normalizing constant to make sure that we actually get a legitimate probability distribution.",
                    "label": 0
                },
                {
                    "sent": "So we eventually assign a probability to each one of these derivations based on the scores of the individual synchronous context.",
                    "label": 0
                },
                {
                    "sent": "Free grammar rules involved, and that eventually assigns a horse a higher probability to the more correct or city of capital capital of a city interpretation.",
                    "label": 0
                },
                {
                    "sent": "So this is this weighted.",
                    "label": 0
                },
                {
                    "sent": "Synchronous context free grammar formalism that's again become quite popular within the statistical machine translation community, and we've been adapting it to use for this task of mapping sentences to formal meaning representation.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the overall wise system works something like this.",
                    "label": 0
                },
                {
                    "sent": "It assumes you start out with an unambiguous context free grammar for the meaning representation language.",
                    "label": 0
                },
                {
                    "sent": "That's what the MRL stands for.",
                    "label": 0
                },
                {
                    "sent": "So this is the formal language that the computer knows quite well.",
                    "label": 0
                },
                {
                    "sent": "It has a well defined grammar for it and then I give it training.",
                    "label": 0
                },
                {
                    "sent": "Sets of sentence is in the two languages that I'm interested in translating between the traditionally in the SMT literature these are called ENF.",
                    "label": 0
                },
                {
                    "sent": "It originally stood for English and French, 'cause that was original pair.",
                    "label": 0
                },
                {
                    "sent": "For us it means English informal.",
                    "label": 0
                },
                {
                    "sent": "Because we're translating from English to formal language rather than from English to French, that's given first to a system we call lexical acquisition, which constructs a lexecon, which is the basic translation of terms in the one language to terms in the other, and these are represented as these synchronous context free grammar rules.",
                    "label": 0
                },
                {
                    "sent": "And then there's a separate phase of then learning those Lambda parameters on each one of those production, so that we get a well defined probability distribution across the synchronous derivations.",
                    "label": 0
                },
                {
                    "sent": "So that's a separate step that we have to estimate all these Lambda parameters for each one of the synchronous context free grammar rules we construct in the first step.",
                    "label": 0
                },
                {
                    "sent": "After we've done that, we have a complete synchronous parsing model that can do translation, so we give it a new sentence in the language.",
                    "label": 0
                },
                {
                    "sent": "English, say in this case, and by using this parameterized or weighted synchronous context free grammar, it will automatically translate it into an select.",
                    "label": 0
                },
                {
                    "sent": "The best, most likely interpretation in the F language, or the formal language for our case now lost is it has fairly complex algorithms for doing both lexical acquisition and parameter estimation that I don't have time to talk into.",
                    "label": 1
                },
                {
                    "sent": "You talk to you here today about I recommend.",
                    "label": 0
                },
                {
                    "sent": "Looking at the papers on this or John has his complete thesis up on the web.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That gives all the details ohmygosh.",
                    "label": 0
                },
                {
                    "sent": "Here we go again.",
                    "label": 0
                },
                {
                    "sent": "Why is it doing this?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Another task though then semantic parsing is tactical generation, so here I have an example from the Robo Cup domain.",
                    "label": 0
                },
                {
                    "sent": "We first start and so we want to map.",
                    "label": 0
                },
                {
                    "sent": "The goalie should always stay in our half to the formal coaching language, but we'd also like to do the reverse.",
                    "label": 1
                },
                {
                    "sent": "We'd like to be able to translate the formal language back into English.",
                    "label": 0
                },
                {
                    "sent": "The nice one of the great things about synchronous context free grammars is there can.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lately symmetric, they have two strings on the right hand side, but there's no reason why I need to use one as the input and one is the output.",
                    "label": 0
                },
                {
                    "sent": "I can easily just switch the two of those and immediately I have a system that can do generation as opposed to parsing.",
                    "label": 0
                },
                {
                    "sent": "It can take formal expressions in and find the most likely synchronous derivation that generates that formal expression and corresponding to that on the other side will be the most likely interpretation in English for that formal query.",
                    "label": 0
                },
                {
                    "sent": "So we've also developed a version of last.",
                    "label": 0
                },
                {
                    "sent": "Sometimes we call this wasp inverse that can do generation by using the.",
                    "label": 0
                },
                {
                    "sent": "Cemetery of account.",
                    "label": 0
                },
                {
                    "sent": "Synchronous context.",
                    "label": 0
                },
                {
                    "sent": "Free grammar to generate as well as to parse.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I encourage Assessor whirlwind tour of this system called Lost that we developed and there's a lot of literature.",
                    "label": 0
                },
                {
                    "sent": "If you want to learn more of the technical details so that sort of older work.",
                    "label": 0
                },
                {
                    "sent": "But now I want to start talking really about where we've been moving with this work recently, which is, I've never liked this idea that we've had to Ann, and believe me, my graduate students don't like it either, 'cause they're the ones that have to sit down and build these corpora of annotating all of these natural language expressions with formal representation.",
                    "label": 0
                },
                {
                    "sent": "And it's a very difficult and painful process, and an also of course, children do not learn language from annotated corpora.",
                    "label": 1
                },
                {
                    "sent": "We don't feed children.",
                    "label": 0
                },
                {
                    "sent": "The Penn Treebank in the crib, in order for them to learn English and neither do they learn language from just reading the newspaper, surfing the web or listening to the radio.",
                    "label": 1
                },
                {
                    "sent": "There's a lot of certain amount of work.",
                    "label": 0
                },
                {
                    "sent": "Most successful work in natural language processing in supervised learning.",
                    "label": 0
                },
                {
                    "sent": "But there's a growing body of unsupervised.",
                    "label": 0
                },
                {
                    "sent": "It just tries to use raw text to learn language.",
                    "label": 0
                },
                {
                    "sent": "There's also a new project funded by the DARPA agency in the US called Learning by Reading where they're trying to develop systems.",
                    "label": 0
                },
                {
                    "sent": "This is just in the early stages that actually will just look on the web and try to learn language by just looking at tons and tons and tons of text.",
                    "label": 0
                },
                {
                    "sent": "On the web, but I really don't think this is the right way to approach language.",
                    "label": 0
                },
                {
                    "sent": "Children don't learn language just from language alone.",
                    "label": 0
                },
                {
                    "sent": "I mean, I'm sure that you government of the US Government will not allow you to do this experiment, but try raising your child with all you do is have them surf the web or listen to the radio and see if they learn language.",
                    "label": 0
                },
                {
                    "sent": "You know you're not going to get promove up.",
                    "label": 0
                },
                {
                    "sent": "Hopefully you're not going to get any permission from your government to fund those studies because we know it's not going to work.",
                    "label": 0
                },
                {
                    "sent": "Children need to be exposed to language in the context in which that language is relevant, and use in art and learn languages.",
                    "label": 0
                },
                {
                    "sent": "And I think if we're ever going to get.",
                    "label": 0
                },
                {
                    "sent": "Our computer systems to really, truly deeply understand language, we need to do the same thing.",
                    "label": 0
                },
                {
                    "sent": "So the natural way to learn languages to perceive language in the context of its use in the physical and social world, and that requires inferring the meaning of utterances from the perceptual context and not having poor graduate students label large corpora with formal expressions, representations of meaning.",
                    "label": 1
                },
                {
                    "sent": "So how can we get around?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That so another issue that comes up here is what is sometimes called language grounding, where the the meanings of many words are grounded in the perception of the physical world.",
                    "label": 1
                },
                {
                    "sent": "So I know what the meaning of red is or ball is, or Cup or run or hit or fall, because I understand the events in the world that that that that word refers to and how it connects to my perception of those events in the world.",
                    "label": 0
                },
                {
                    "sent": "Steve Tarnet had this very influential paper back in 1990 called symbol grounding, where that a fundamental problem in AI is connecting our abstract.",
                    "label": 0
                },
                {
                    "sent": "Symbols in our.",
                    "label": 0
                },
                {
                    "sent": "In our formal systems to actual the perception and interpretation of those events in the world itself.",
                    "label": 0
                },
                {
                    "sent": "And you might say, well, a lot of languages about abstract things.",
                    "label": 0
                },
                {
                    "sent": "It's not about observable things in the physical world, but actually most of our abstract words are actually grounded as sort of metaphors.",
                    "label": 1
                },
                {
                    "sent": "Are abstractions grounded in the physical world itself?",
                    "label": 0
                },
                {
                    "sent": "Just simple words like up and down and over, and we use those in a metaphorical sense, but their original meanings are do come from the physical world, so a very famous book called Metaphors We Live by by Lakoff and Johnson, tried to make this point quite clearly.",
                    "label": 0
                },
                {
                    "sent": "Here's just one example I have is.",
                    "label": 1
                },
                {
                    "sent": "It's difficult to put my word into ideas.",
                    "label": 0
                },
                {
                    "sent": "It's as if I have containers called ideas.",
                    "label": 0
                },
                {
                    "sent": "I'm somehow shoving these words into them, and we use that sort of language to talk about abstract ideas and think of ever going to really understand these sorts of senses.",
                    "label": 0
                },
                {
                    "sent": "We also have to understand the physical metaphor that lies behind them.",
                    "label": 0
                },
                {
                    "sent": "So, but contrary to this, most work in semantics and natural language process tries to represent meanings with by some circular sort of definitions of these words being.",
                    "label": 0
                },
                {
                    "sent": "These other words, are these words mean these formal expressions, and I think if we're ever going to really get to understanding language, we have to not have these circular definitions defining means of words in other words, but we need to have the grounding of our language of the having our eventual primitive terms connected to our perception of those objects and events.",
                    "label": 0
                },
                {
                    "sent": "In the world.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's a very picture version of what we're trying to do.",
                    "label": 0
                },
                {
                    "sent": "It sort of trying to emulate the sort of problem that's confronted with language learning as a child is so the child in some environment there's a lot of things going on in his perceptual environment.",
                    "label": 0
                },
                {
                    "sent": "The the psychologist William James called this the buzzing, blooming confusion, right?",
                    "label": 0
                },
                {
                    "sent": "The child comes into the world.",
                    "label": 0
                },
                {
                    "sent": "There's all this stuff going on in their perceptual system, and, you know, say the child here.",
                    "label": 0
                },
                {
                    "sent": "Some sentence is like Mary is on the phone, and you know he's looking at a lot of things in the world, and he doesn't necessarily know which one of those that refers to.",
                    "label": 1
                },
                {
                    "sent": "Maybe it's not referring to anything currently.",
                    "label": 0
                },
                {
                    "sent": "In his perceptual environment, but hopefully a certain fraction of the time it is.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "So what we would like to be able to do is learn language from this sort of.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Big Yuus sort of supervision where we have a bunch of things happening in the world and we hear some sentence that's probably refers to something happening out there, but we don't know exactly what we'd like to build a computer system that Simon today exposed to perceptual context and natural language utterances and be able to learn the underlying semantics from that.",
                    "label": 1
                },
                {
                    "sent": "So we have this sort of ambiguous sort of training data, where each sentence is associated with a multitude of possible things that could mean, based on what's currently happening in the world at that point.",
                    "label": 0
                },
                {
                    "sent": "So Jeff does siskin back in the 90s, did some interesting work on Lexicon learning, just learning the meaning of words.",
                    "label": 0
                },
                {
                    "sent": "Using this sort of what he called referentially uncertain training data where each sentence was paired with a number of possible meanings, and from that he tried to learn meanings of individual words, but he didn't go on to try to learn meanings of complete sentences and build a compositionally semantic system that can construct meanings of sentences in generate, and so we're trying to do is take this ambiguous sort of data and be able to learn semantic parsers and language generators from it.",
                    "label": 1
                },
                {
                    "sent": "An extract of course, the problem of extracting meaning representations from the perceptual environment is very hard, right?",
                    "label": 0
                },
                {
                    "sent": "People in computer vision have done a lot of work of trying to do this over many decades, and so we're currently sort of sidestepping this problem.",
                    "label": 0
                },
                {
                    "sent": "I'd like to eventually get back to it.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about briefly about that at the end, but for right now we're assuming that that step is somehow automated, that we're able to abstract the actual perceptual world into a bunch of symbolic representation of what's happening.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So instead of having this picture like this, we first replace.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The baby, of course.",
                    "label": 0
                },
                {
                    "sent": "With the computer we want to learn language and since we don't want to solve the computer vision problem, we assume.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that instead of these actual perceptual images, we get some symbol.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like representation of these meanings now I've used English words here to just give you some vague idea of what I'm talking about, but these could be arbitrary conceptual symbols that have no connection to the.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Natural language, I just know that this natural language string probably refers to one of these conceptual representations that are out there in the world at that time.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So they then say you know the context changes and somebody utters another sentence so that the idea of learning this context is my perceptual system is.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Giving me this constant stream of changing, you know descriptions of objects and events that are out there in the world, and then I hear language uttered every so often, but it could refer to a lot of different things.",
                    "label": 0
                },
                {
                    "sent": "Can I actually learn language from this sort of ambiguous sort of supervision?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is an interesting problem that we've been working on for a couple of years now and we tried.",
                    "label": 0
                },
                {
                    "sent": "We built a model that tries to learn to take this sort of ambiguous supervision that corresponds to this sort of idea of a stream of perceptual events with occasional language commentary and try to learn language parsers and generators from that sort of data.",
                    "label": 1
                },
                {
                    "sent": "Now we're making currently a very simplifying assumption, which is that each sentence has exactly 1 meaning in the perceptual context, so any sentence I uttered, it refers to one of these things that's happening in the world, and that's clearly a vast oversimplification.",
                    "label": 0
                },
                {
                    "sent": "I think it's a reasonable place to start.",
                    "label": 0
                },
                {
                    "sent": "We've currently made some changes so that some sentences can just not refer to anything that's happening in the world at this point, and we've also we have a slightly different algorithm from the one out of the straight here that tries to handle that case, but by and large we make the assumption that each sentence means something that's happening in the world at that point.",
                    "label": 1
                },
                {
                    "sent": "And at most one sentence.",
                    "label": 1
                },
                {
                    "sent": "So each meaning is associated with it.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what you end up with?",
                    "label": 0
                },
                {
                    "sent": "Sorry if you can't see the green, it's always the colors.",
                    "label": 0
                },
                {
                    "sent": "Always project a slightly differently than they look on the screen, but the idea is on the one side of this bipartite graph, so this turns into a bipartite graph for the training data on the left hand side we have, we have a bunch of sentences in natural language here in English and on the right hand side we have a bunch of conceptual representations.",
                    "label": 0
                },
                {
                    "sent": "Again, I've used English words there, but that could be all in gibberish.",
                    "label": 0
                },
                {
                    "sent": "It's just a matter of the English words helps you, and I understand it, and I have these dotted line would say, well, this sends could mean that.",
                    "label": 0
                },
                {
                    "sent": "And I get a bunch of these sequence of sentences that could mean any of these various conceptual representations that are coming in through my perceptual system.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we originally took our system, crisp.",
                    "label": 0
                },
                {
                    "sent": "I didn't talk much.",
                    "label": 0
                },
                {
                    "sent": "I talk just briefly about Chris before it uses string kernels in SVM's to do this semantic parsing pass and a triple AI.",
                    "label": 0
                },
                {
                    "sent": "Last year we had a paper we presented the system called crisper, that is an extension of that system.",
                    "label": 0
                },
                {
                    "sent": "To learn with this ambiguous sort of training.",
                    "label": 0
                },
                {
                    "sent": "Initially assumed I have sentences paired with unique meaning representations, we developed a new version we called Crisper, which stands for Crisp with them, like retraining.",
                    "label": 0
                },
                {
                    "sent": "Where we do this sort of EM tight loop of self training.",
                    "label": 0
                },
                {
                    "sent": "It's not strictly em because we don't have a proof of convergence to a maximum likelihood estimate, but it's following the same sort.",
                    "label": 0
                },
                {
                    "sent": "Framework where it changes the representation matching to each of the sentences gradually overtime until it converges on one that's hopefully correct.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I have a nice animation here that tries to illustrate this, so this is just some artificial data that my student Rohe Cotte originally generated to do this task.",
                    "label": 0
                },
                {
                    "sent": "So it's you might seem a little weird if you actually read it.",
                    "label": 0
                },
                {
                    "sent": "'cause we just sort of had a bunch of objects in the world in a set of events or descriptions and we sort of randomly generated pairs of sentences with sets of potential meaning.",
                    "label": 0
                },
                {
                    "sent": "So initially you start with this bipartite graph, where you each sentence is ambiguously linked to several different events.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, the first thing we do is just say, well, let's train a system.",
                    "label": 0
                },
                {
                    "sent": "Assuming all these representations are correct, so I'll give it a gold standard training example.",
                    "label": 0
                },
                {
                    "sent": "Each sentence linked to every meaning representation it could mean, and I give all of this supervision.",
                    "label": 0
                },
                {
                    "sent": "And now in both lines to a normal supervised system.",
                    "label": 0
                },
                {
                    "sent": "Now most of the supervision it's getting is garbage, right?",
                    "label": 0
                },
                {
                    "sent": "It's noise, but maybe it can work out if it does some good learning and it avoids overfitting.",
                    "label": 0
                },
                {
                    "sent": "It can separate some of the wheat from the chaff and figure out what's going.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now to make this a little bit simpler, we know a little bit more about how ambiguous each of these sentences is, right?",
                    "label": 0
                },
                {
                    "sent": "'cause some sentence is, there's only one event happening in the world when that's that was uttered.",
                    "label": 0
                },
                {
                    "sent": "Other things, there might be 100 things happening, so we weight each of these training examples by a fraction of how ambiguous it is.",
                    "label": 0
                },
                {
                    "sent": "So if it has two meanings, each link gets 1/2.",
                    "label": 0
                },
                {
                    "sent": "If it has five meanings there each way it gets 150, so it gives the learner a little bit more idea of how likely each of these is to be a correct example.",
                    "label": 0
                },
                {
                    "sent": "Now we give these weighted training examples to our normal supervised learning system.",
                    "label": 0
                },
                {
                    "sent": "And it learns an initial semantic parser in this case.",
                    "label": 0
                },
                {
                    "sent": "But of course there's potentially a lot of air.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In that data.",
                    "label": 0
                },
                {
                    "sent": "Now we take the initial parser we learned in iteration one, and we go off and we try to parse each one of these sentences and assign a probability.",
                    "label": 0
                },
                {
                    "sent": "So this is a probabilistic system that will reduce the probability that a given sentence Maps to a particular meaning representation.",
                    "label": 0
                },
                {
                    "sent": "We assign these weighted links based on the probability that the initially learn parser believes that that's the correct representation for that sentence.",
                    "label": 0
                },
                {
                    "sent": "OK, now we get this weighted bipartite graph.",
                    "label": 0
                },
                {
                    "sent": "The next thing we do.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do is we use this old algorithm from the 50s to solve what's called the maximum weighted matching problem where we now take?",
                    "label": 0
                },
                {
                    "sent": "We have since we're assuming each sentence actually corresponds to exactly 1, meaning we do a maximum matching where we try to pick one link for each English sentence that maximizes the total weighted sum of all of the matches that we get, and that will eventually give us this.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Green bold matching there.",
                    "label": 0
                },
                {
                    "sent": "Now notice every sentence doesn't get its maximal probability because.",
                    "label": 0
                },
                {
                    "sent": "Let's see, we have this here.",
                    "label": 0
                },
                {
                    "sent": "This sentence with a 95% probability means this, but with 89 it means this.",
                    "label": 0
                },
                {
                    "sent": "So this sentence grab this sentence.",
                    "label": 0
                },
                {
                    "sent": "So this can't have it, so it had to go to its second best choice of .85.",
                    "label": 0
                },
                {
                    "sent": "So this maximum matching algorithm solves the highest weighted match given the constraint that each sentence has to correspond to one representation in each representation can represent the meaning of at most one sentence.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, then we just get rid of all the other noisy supervision.",
                    "label": 0
                },
                {
                    "sent": "We trained it on this gold supervision that we got from the maximum maximum weighted matching and we trained the system.",
                    "label": 0
                },
                {
                    "sent": "And now we have a new semantic parser, and we iterate this just like when you go through iterations in VM Ware.",
                    "label": 0
                },
                {
                    "sent": "Each step it converges on a better and better and better solution until this converges to some fixed point.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's how the basic resolving of damned.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Duity works so Rohit originally tested this in some artificial data that we constructed that I briefly referred to, and we.",
                    "label": 0
                },
                {
                    "sent": "Compared the red line here is training on gold standard data where I give it direct supervision.",
                    "label": 0
                },
                {
                    "sent": "There's no ambiguity and we see as we go from blue to green to this diamond curve.",
                    "label": 1
                },
                {
                    "sent": "Here we're adding more and more and more ambiguity to the data.",
                    "label": 1
                },
                {
                    "sent": "So the number of dotted lines starts increasing and increasing, so I have less idea directly what things mean.",
                    "label": 1
                },
                {
                    "sent": "But given enough training examples here on the X axis, the system starts doing better and better.",
                    "label": 0
                },
                {
                    "sent": "It starts quite poor with just 225 ambiguous examples.",
                    "label": 0
                },
                {
                    "sent": "But as we get up to 900, it's starting to get close to as if I didn't have any ambiguity in the training data at all.",
                    "label": 0
                },
                {
                    "sent": "So given enough of this ambiguous data, it can start to approach the accuracy of learning with supervised gold standard data.",
                    "label": 0
                },
                {
                    "sent": "Ah, OK.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some more recently we've been trying to attack do what I think is an interesting challenge problem, which is what I call the learning to be a sportscaster task, which is ideas to learn from realistic date of natural language used in a representative context, while avoiding the difficult issues in computer vision.",
                    "label": 1
                },
                {
                    "sent": "So I know this problem is hard enough.",
                    "label": 0
                },
                {
                    "sent": "I don't try to solve the speech problem.",
                    "label": 0
                },
                {
                    "sent": "I use textual input and I don't try to solve the computer vision problem.",
                    "label": 0
                },
                {
                    "sent": "We use symbolic representations for what the various conceptual things that were pairing the language come from.",
                    "label": 0
                },
                {
                    "sent": "So we we chose this task of learning to sportscaster Robo Cup game, given textually annotated traces of activity in some in the simulated environment where I don't have to solve the computer vision problem.",
                    "label": 1
                },
                {
                    "sent": "So we're given traces of games in the Robo Cup simulator paired with textual sportscaster commentary.",
                    "label": 0
                },
                {
                    "sent": "So it looks like this.",
                    "label": 0
                },
                {
                    "sent": "You have a simulated game, some user sits there and watches the game.",
                    "label": 0
                },
                {
                    "sent": "I'll show you a video of this in a minute and we have a simple sort of simulated perception that takes the low level details of the simulator and extracts these high level sort of perceived facts that I was showing you on the right hand side about various events.",
                    "label": 0
                },
                {
                    "sent": "I'll show you.",
                    "label": 0
                },
                {
                    "sent": "Example in a minute.",
                    "label": 0
                },
                {
                    "sent": "Meanwhile, the sportscaster is giving me some linguistic input in text.",
                    "label": 0
                },
                {
                    "sent": "Since we don't deal with speech and then both of that perceptual output and the linguistic input goes into what I'm calling the grounded language learner here and we for our wash system.",
                    "label": 0
                },
                {
                    "sent": "This produces a synchronous context free grammar that can support both tactical generation, an semantic parsing.",
                    "label": 0
                },
                {
                    "sent": "It can either take English in and generate meaning representations or generate taken meaning representations and generate language.",
                    "label": 0
                },
                {
                    "sent": "So then if I watch a new game.",
                    "label": 0
                },
                {
                    "sent": "Perception can go through and generate events that are happening in the game.",
                    "label": 0
                },
                {
                    "sent": "In this formal language, and then I use my natural language generator that I've learned to automatically generate text describing that game.",
                    "label": 0
                },
                {
                    "sent": "So the idea is then to build a system that just watch as a human sportscaster game, and then I give it a new game.",
                    "label": 0
                },
                {
                    "sent": "And I say, OK, you tell me what's happening and it has to learn to be able to do that.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example of what the input to the system looks like.",
                    "label": 0
                },
                {
                    "sent": "I have English comments here on the left hand side, and then I have all these events that are being extracted from the pre simulated perception and notice.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that the fact that and then we use a 5 second window, so anything that this sentence, any perceptual event that's happened within the five seconds preceding that comment, get attached as a possible meaning for that sentence, and notice that I use English terms here in the meaning representation, but that's totally.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we first establish the human goes in, and at this is not the learners not going to see this data, but for evaluation purposes we had a human judge go in and decide what was the correct corresponding representation for each sentence and notice some of them don't have one.",
                    "label": 0
                },
                {
                    "sent": "So there's a comment here.",
                    "label": 0
                },
                {
                    "sent": "Purple Team is very sloppy today.",
                    "label": 1
                },
                {
                    "sent": "Our perceptual system is not sophisticated enough to extract abstract descriptions of the world like the sloppiness.",
                    "label": 0
                },
                {
                    "sent": "So this sentence doesn't actually correspond to anything, so it violates our one to one assumption.",
                    "label": 0
                },
                {
                    "sent": "And down here we have.",
                    "label": 1
                },
                {
                    "sent": "Pink 11 looks around for a teammate.",
                    "label": 0
                },
                {
                    "sent": "It doesn't know how to match that.",
                    "label": 0
                },
                {
                    "sent": "So all of this is sort of noisy.",
                    "label": 0
                },
                {
                    "sent": "Every sentence doesn't necessarily correspond to a meaning representation, but most many of them do, and many of them actually do have a correct thing that they correspond to the green line.",
                    "label": 0
                },
                {
                    "sent": "But the system doesn't see the green line.",
                    "label": 0
                },
                {
                    "sent": "That's for value.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nation only and I also want to note that I used English words here for these meaning representations, but you could translate this into arbitrary tokens.",
                    "label": 0
                },
                {
                    "sent": "That's all that's assumed.",
                    "label": 0
                },
                {
                    "sent": "Here is your meaning.",
                    "label": 0
                },
                {
                    "sent": "Your perceptual system generates some symbolic representation that's represented in mentales, right?",
                    "label": 0
                },
                {
                    "sent": "That has some internal language.",
                    "label": 0
                },
                {
                    "sent": "It has nothing to do with natural language.",
                    "label": 0
                },
                {
                    "sent": "We're currently in the process.",
                    "label": 0
                },
                {
                    "sent": "I have a Korean student that's doing Korean annotation on these games to show that the system is language independent.",
                    "label": 0
                },
                {
                    "sent": "It can learn to generate descriptions of games in Korean as well as in English, but right now we just have English result.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's what we did.",
                    "label": 0
                },
                {
                    "sent": "We collected human textual commentary for four Robo Cup championship games.",
                    "label": 1
                },
                {
                    "sent": "The champion team, the final game in the in the Robo Cup Simulator League from 2001 to 2004.",
                    "label": 0
                },
                {
                    "sent": "The perceptual system extracts about an average of 2600 events that happen in each one of those games, and the human talks about gives about 500 sentences in English, describing things that are happening during that game.",
                    "label": 1
                },
                {
                    "sent": "Now each sentence is matched to all events within 5 seconds, which means on average each sentence refers to about 2 1/2 events, which actually is pretty low ambiguity.",
                    "label": 0
                },
                {
                    "sent": "I think for the real world, but I thought it think its original place, reasonable place to start, but there's quite a variance.",
                    "label": 0
                },
                {
                    "sent": "Some of the sentence is only have one thing they could possibly refer to.",
                    "label": 0
                },
                {
                    "sent": "Some could be referring to is up is as many as 12 different things.",
                    "label": 0
                },
                {
                    "sent": "Now we manually annotated each of the sentences with the correct meaning representation.",
                    "label": 0
                },
                {
                    "sent": "If one existed in the perceptual output, and this is again not given to the machine learning system, that's just so we, as experimenters, can evaluate how well it's establishing the matching between the sentence and the Rep.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Station.",
                    "label": 0
                },
                {
                    "sent": "OK, so we built several systems to be able to learn from this sort of ambiguous data and learn this task of sportscasting.",
                    "label": 0
                },
                {
                    "sent": "So I mentioned crisper that can learn from this ambiguous data, but crisper only does semantic parsing.",
                    "label": 0
                },
                {
                    "sent": "It doesn't do generation, and we want to be able to learn to be a sportscaster to generate language.",
                    "label": 0
                },
                {
                    "sent": "So we developed a version of our wash system which remember can do both semantic parsing and generation by using the symmetry of a synchronous context free grammar.",
                    "label": 0
                },
                {
                    "sent": "And we just added this sort of M like retraining loop to wasp as well as two crisp, and we built this system we call losper.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Now we also built a system that we called crisper was because this am at this initial training with ambiguous data, creates a lot of noise.",
                    "label": 0
                },
                {
                    "sent": "I showed you that there's 2 1/2 meanings for each sentence, which means that more than half of the the supervision we give to the system is noise.",
                    "label": 0
                },
                {
                    "sent": "It's actually incorrect, and that our system Chris we've shown empirically, is a little better at handling large amounts of noisy data because it uses support vector machines to use the Max margin criterion to try to prevent overfitting.",
                    "label": 0
                },
                {
                    "sent": "It also has the string kernels in that gives it a little bit more flexibility.",
                    "label": 0
                },
                {
                    "sent": "So by and large, Chris does better at handling noisy supervision than what does, which is a little bit more of a symbolic system that learns these symbolic S CFG rules.",
                    "label": 1
                },
                {
                    "sent": "It has this.",
                    "label": 0
                },
                {
                    "sent": "This Maxent log linear model on top of it, but the rules itself are sort of hard matching rules.",
                    "label": 0
                },
                {
                    "sent": "But Chris doesn't support language generation, so we built a system that first trains crisp to determine the matching.",
                    "label": 1
                },
                {
                    "sent": "So we run crisper to get those dark green lines that established the right matching between the sentence in the meaning representation.",
                    "label": 1
                },
                {
                    "sent": "Then we give that data as gold standard training data to wasp.",
                    "label": 0
                },
                {
                    "sent": "So Wasp can learn a generator from that.",
                    "label": 0
                },
                {
                    "sent": "So it's somewhat of a hybrid between this Chris system that uses SVM and string kernels together with loss, which uses synchronous context free grammars.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, we finally build a system that was more focused on the generation pass, so even Wasp are is focused on the task of learning to semantically parse natural language into meaning representations.",
                    "label": 0
                },
                {
                    "sent": "We built a system called Wasp Arjen that sort of inverts the role of those, and so we generated quite wasp Arjen and it tries to purposely focus on the task of generation.",
                    "label": 0
                },
                {
                    "sent": "Determining the best matching based on generation.",
                    "label": 1
                },
                {
                    "sent": "So remember those scores that I attached to each of the dotted links based on the probability that this is the correct parts for this sentence.",
                    "label": 0
                },
                {
                    "sent": "We now assign numbers based on going the other way.",
                    "label": 0
                },
                {
                    "sent": "If we take the input representation, what score does it give to the generated English?",
                    "label": 0
                },
                {
                    "sent": "So it focuses the system more on the generation task rather than the parsing task.",
                    "label": 1
                },
                {
                    "sent": "And the details of why we did.",
                    "label": 0
                },
                {
                    "sent": "How we did this.",
                    "label": 0
                },
                {
                    "sent": "I recommend you look at the papers for those who might have some familiarity with statistical machine translation.",
                    "label": 0
                },
                {
                    "sent": "We computed a standard machine translation score called Nest, which is slightly different from the blue score.",
                    "label": 0
                },
                {
                    "sent": "If you're familiar with that between the generated sentence in the potential matching sentence.",
                    "label": 1
                },
                {
                    "sent": "So we had the system generate its best language and then we compared that language to the human language and scored how well the human language matched the system language and use that to score the links in the maximum matching rather than the parsing scores.",
                    "label": 0
                },
                {
                    "sent": "But basically it just creates a system that tries to resolve matches in a way that best maximizes the performance of generation as opposed to parsing.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now a problem that I haven't talked much about that I'd like to briefly talk about is strategic generation, which is knowing.",
                    "label": 0
                },
                {
                    "sent": "What to say rather than just how so strategics as I tell you in formal language what to say and you tell me, just translate it into natural language.",
                    "label": 1
                },
                {
                    "sent": "But how do I pick what to say so in the.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Switching task and the sportscasting task, the perceptual system is extracting all of these events.",
                    "label": 0
                },
                {
                    "sent": "The human doesn't talk about everything that's happening on the field, right?",
                    "label": 0
                },
                {
                    "sent": "They chew.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use a particular set once.",
                    "label": 0
                },
                {
                    "sent": "Here I put here in red to actually talk about, so the system has to learn how to identify the red event.",
                    "label": 0
                },
                {
                    "sent": "The interesting events to talk about out of all the events that my perception is giving me.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, I don't have time to go into the details on this, but we learned a very simple model for just putting a probability on each type of event, like kicking versus passing versus the ball stopping versus, you know interceptions and things of that sort.",
                    "label": 0
                },
                {
                    "sent": "But that requires us to know the correct matching between the sentence in the representation, but we don't know that match, but we can estimate it from the matching that comes from wasp are or wasp ergener crisper.",
                    "label": 1
                },
                {
                    "sent": "We also developed another version that establish is the best matching purposely for the.",
                    "label": 0
                },
                {
                    "sent": "The reason of determine the probability that events are commented on, and again.",
                    "label": 1
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I don't have time to talk about this.",
                    "label": 0
                },
                {
                    "sent": "We developed this algorithm, we called iterative generation strategy, learning that again uses an M type iteration to converge on a hyper good probability estimate of how likely each event is talked about by by the human being.",
                    "label": 1
                },
                {
                    "sent": "So again, I will point you to the papers for more evidence.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's on that.",
                    "label": 0
                },
                {
                    "sent": "OK, so I want to just briefly give the demo of how this system has learned to talk about Robo Cup games, so this is a test game.",
                    "label": 0
                },
                {
                    "sent": "It never was trained on this game.",
                    "label": 0
                },
                {
                    "sent": "We trained it on other games and now I'm asking it to comment on this new Robo Cup game, and here's what it does, and there's a text to speech system here that translates the output in text to speech.",
                    "label": 0
                },
                {
                    "sent": "And it's had to learn the names of all the players in order to do this.",
                    "label": 0
                },
                {
                    "sent": "Well that gives you at least a little idea of what the final system is capable of doing.",
                    "label": 0
                },
                {
                    "sent": "So it actually learns to sportscaster these Robo Cup games reasonably affectively.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of the cute.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Demo, but what about quantitative evaluation?",
                    "label": 0
                },
                {
                    "sent": "I'm all about quantitative evaluation, so we generated learning curve by training on all combinations of one to three games and testing on the games not used for training.",
                    "label": 1
                },
                {
                    "sent": "We compared to a couple of baselines, which is.",
                    "label": 0
                },
                {
                    "sent": "I went through this complicated algorithm tries to establish how each English sentence corresponds to some event in the world.",
                    "label": 0
                },
                {
                    "sent": "What if I just pick an event at random that I just take one of those dotted lines at random and make it solid to represent the meaning of that sentence?",
                    "label": 0
                },
                {
                    "sent": "How well does that work?",
                    "label": 0
                },
                {
                    "sent": "That's like a lower bound and upper bound is what if I actually use the matching between the sentence is in than the representations that the human expert provided we call that gold matching and then for each of our systems we also compared on standard metrics of precision, the percentage of the system annotations that are correct.",
                    "label": 1
                },
                {
                    "sent": "Recall the percentage of the gold standard annotations that were correctly produced by the system, and then the standard F measure, which is the harmonic mean of these two.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Vision and recall.",
                    "label": 0
                },
                {
                    "sent": "So what we've evaluated several things.",
                    "label": 0
                },
                {
                    "sent": "One is how well out of this ambiguous training data does it actually establish the right representation for each one of these sentences?",
                    "label": 0
                },
                {
                    "sent": "So we measured how accurately various methods assign the correct meaning representations to each sentence in the ambiguous comparing to the gold standard human of.",
                    "label": 1
                },
                {
                    "sent": "Matching as the.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Uation and here we see the results on this on F measure, you know.",
                    "label": 1
                },
                {
                    "sent": "So the random baseline actually doesn't do too horribly bad here, because it's slightly less than 50%, right?",
                    "label": 0
                },
                {
                    "sent": "'cause I showed you that on average there's 2 1/2 representations percent, and so if you guess it right, you're going to get 50%.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The systems we currently have gone through a lot better, crisper does the best, because it's the best at handling this sort of noisy supervision.",
                    "label": 0
                },
                {
                    "sent": "Gets maybe close to 70%.",
                    "label": 0
                },
                {
                    "sent": "Wasp Arjen does pretty well at about 60 three or something.",
                    "label": 0
                },
                {
                    "sent": "It looks like there so.",
                    "label": 0
                },
                {
                    "sent": "Or matching him up much, you know, signaling better than random chance, but still, there's definitely room for improvement on this.",
                    "label": 0
                },
                {
                    "sent": "I think if we improved our matching, we'd get better results overall.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also evaluated how well does it translate natural language into these meaning representations, and we have to exactly match the gold standard representation to count is correct.",
                    "label": 1
                },
                {
                    "sent": "There's no partial credit for a sentence.",
                    "label": 0
                },
                {
                    "sent": "You either get the meaning representation exactly right or you get it totally wrong and he.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some learning curves just to show you how it does on the semantic parsing task.",
                    "label": 0
                },
                {
                    "sent": "If the red line and here is an upper baseline, it's cheating, right?",
                    "label": 0
                },
                {
                    "sent": "I'm giving it the correct matching in the training data, so that's sort of an upper baseline.",
                    "label": 0
                },
                {
                    "sent": "We see Chris Pier is the green line here, so the funny it doesn't do well with just one training game, but by three training games.",
                    "label": 0
                },
                {
                    "sent": "It's actually doing the best, so it seems to learn better eventually, but it learns a little bit more slowly, and you know crisper wasp here doesn't do too bad either.",
                    "label": 0
                },
                {
                    "sent": "In the blue and then.",
                    "label": 0
                },
                {
                    "sent": "Of course, the bottom baseline here is the random matching, which doesn't do nearly as well.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What about generating natural language from English?",
                    "label": 0
                },
                {
                    "sent": "Well, we did this evaluation of technical generation where we had the system generate English for those representations that we knew had in English corresponding expression, then compare that using this empty evaluation score called Miss, which just measures how close is the English that the system generated to the English that the human being generated for that event.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And really quickly here on the results, you know we get the upper baseline here is in red and then our best performing system for generation.",
                    "label": 0
                },
                {
                    "sent": "Not surprisingly is wasp Arjen because during the Valley Valley the resolving of the ambiguities, it focuses on resolving ambiguities in a way that maximizes generation performance.",
                    "label": 0
                },
                {
                    "sent": "So not surprisingly, when we evaluate the system on generation, that system does the best.",
                    "label": 0
                },
                {
                    "sent": "So that's why when we're actually producing these actual sportscasts, we use Wasp Arjen, because it's been trained specifically to do good at the generation task to do well at the graduation.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Patient asked.",
                    "label": 0
                },
                {
                    "sent": "We also evaluated how well does it pick out the right events to talk about?",
                    "label": 1
                },
                {
                    "sent": "How well does it match the choosing of the events to talk about that the human chose.",
                    "label": 0
                },
                {
                    "sent": "So we compare the set of events that the system chose to talk about with the set of events that the human.",
                    "label": 1
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Chose to talk about an here's the F measure score between the matching of what the system talked about versus what the human talked about, and notice if we have a gold standard matching.",
                    "label": 0
                },
                {
                    "sent": "It gets about what 75% of the time it agrees with to talk about the same thing that the human does, and our best performing system.",
                    "label": 0
                },
                {
                    "sent": "This new algorithm that I didn't have much time to talk about I GSL, comes arbitrarily close to doing as well as it could do if it actually knew the correct matching between the natural language and the meaning representation.",
                    "label": 0
                },
                {
                    "sent": "So we're pretty happy.",
                    "label": 0
                },
                {
                    "sent": "With how well it actually decides what to talk about, it pretty quickly learns that you talk about kicking an interception and scoring, rather than some of these more a boring events that actually come about in the game.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, the last final valuation I wanted to talk quickly about is what sometimes I call the quasi Turing test.",
                    "label": 0
                },
                {
                    "sent": "So we ask for fluent English speakers to evaluate overall quality of sportscasts.",
                    "label": 1
                },
                {
                    "sent": "We randomly picked a 2 minute segment you just saw part of one of the two minute segments that we showed to the human judges.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately we did this with the human judges.",
                    "label": 0
                },
                {
                    "sent": "We didn't have the text to speech working, so they had to actually read like they had to read subtitles and watch the game, which is trickier.",
                    "label": 0
                },
                {
                    "sent": "It's much easier to evaluate these things if you can listen to the speech, so we'd like to redo this evaluation now that we have this speech synthesis working.",
                    "label": 1
                },
                {
                    "sent": "And each human judge evaluated 8 commented game clips.",
                    "label": 0
                },
                {
                    "sent": "Each of the four segments commented once by a human and once by a machine, and we jumbled them up, right?",
                    "label": 1
                },
                {
                    "sent": "So the human doesn't know whether their evaluating a human generated sportscast are machine generated one and they scored this.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Based on several metrics of evaluation, how fluent is the English?",
                    "label": 0
                },
                {
                    "sent": "How you know, does it sound like good English?",
                    "label": 0
                },
                {
                    "sent": "How much does it correspond to the actual events that are happening on the field?",
                    "label": 0
                },
                {
                    "sent": "Is it actually saying true things that are happening in the game and then we just gave him this vague thing?",
                    "label": 0
                },
                {
                    "sent": "You know how good a sportscaster is this guy?",
                    "label": 0
                },
                {
                    "sent": "Our computer, they didn't know which one was.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the eventual result.",
                    "label": 0
                },
                {
                    "sent": "Now we weren't expecting to pass the Turing test.",
                    "label": 0
                },
                {
                    "sent": "We weren't expected this system to be as good as a human being, but we were hoping that it wasn't too much worse.",
                    "label": 0
                },
                {
                    "sent": "So on a 5 point scale, we can see that the system does about a half a point up to two point 7% points out of a 5 point scale less than the human.",
                    "label": 0
                },
                {
                    "sent": "And I told my student that if we got within one point on a 5 point scale, I thought that we were demonstrating that the system was more or less working so.",
                    "label": 0
                },
                {
                    "sent": "Of course, we still have a little bit of room here to get to human level performance, but I think this is a clear demonstration that these are that's generating sportscasts that people can understand.",
                    "label": 0
                },
                {
                    "sent": "the English isn't bad, and they're fairly accurate and close to human performance, but not matching it yet.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so real quickly on some future directions on this work.",
                    "label": 1
                },
                {
                    "sent": "There's some immediate things we'd like to do in this sportscasting domain.",
                    "label": 0
                },
                {
                    "sent": "One thing is, the correct matching is not established by any information about the strategic generation, so it doesn't use the fact that certain events are more likely for a human to talk about to try to establish the right matching.",
                    "label": 0
                },
                {
                    "sent": "And we think we could do some things to make that matching score better.",
                    "label": 0
                },
                {
                    "sent": "I showed you were not doing all that well on matching still lost.",
                    "label": 0
                },
                {
                    "sent": "We'd like to be able to make it more resilient to noisy training data.",
                    "label": 1
                },
                {
                    "sent": "We think there are various, you know, modern techniques in machine learning to prevent overfitting to do that.",
                    "label": 0
                },
                {
                    "sent": "And also the current simulated perception sort of pours out puts out a fairly low level set of boring events happening in the game.",
                    "label": 0
                },
                {
                    "sent": "If we if the perceptual system could recognize more interesting things, then I think the language could be able to attach to more interesting events that are happening in the world.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On a longer term direction I I started thinking about this problem in a more general machine learning context an I think there's been certainly working machine learning work on learning from sort of what you might call weak supervision.",
                    "label": 0
                },
                {
                    "sent": "Particular, there's this type of learning.",
                    "label": 0
                },
                {
                    "sent": "Presumably most of you are familiar with called multiple instance learning, where you assume a weak supervision where you don't label examples positive and negative.",
                    "label": 0
                },
                {
                    "sent": "I give you so called bags of sets and all I tell you is for the positive bags.",
                    "label": 0
                },
                {
                    "sent": "One of the examples in this set is positive.",
                    "label": 0
                },
                {
                    "sent": "Not all of them necessarily.",
                    "label": 0
                },
                {
                    "sent": "There's also this task of structured output learning, where I try to map from string to string or tree to tree or string to tree.",
                    "label": 0
                },
                {
                    "sent": "There's been a certain amount of work in what's called structured output learning or learning with structured data which connects to the whole area of statistical relational learning.",
                    "label": 0
                },
                {
                    "sent": "Also, this is a recent.",
                    "label": 0
                },
                {
                    "sent": "This references to a recent book from MIT Press with a bunch of stuff unstructured learning My My problem of learning these semantic parsers actually combines these two problems, weak structured.",
                    "label": 0
                },
                {
                    "sent": "Data, so I think there needs to be work on this new topic, which I've called structured multiple instance learning, where an input string is paired with a set of possible output.",
                    "label": 1
                },
                {
                    "sent": "So I say, here's an input structure.",
                    "label": 0
                },
                {
                    "sent": "One of these is the correct output structure and I give you a bunch of sets like that.",
                    "label": 0
                },
                {
                    "sent": "How can I learn to produce structured output in the face of ambiguous training data?",
                    "label": 0
                },
                {
                    "sent": "And that's exactly the sort of problem here I'm confronted with.",
                    "label": 0
                },
                {
                    "sent": "We've concocted sort of our own method specific to this language problem, but I think structured multiple instance learning is a general problem.",
                    "label": 0
                },
                {
                    "sent": "That might occur in other applications also, and I think it's an interesting problem of combining structured output with multiple instance learning.",
                    "label": 0
                },
                {
                    "sent": "I haven't seen any work on this.",
                    "label": 0
                },
                {
                    "sent": "If anyone has time, I'd be curious to learn about it.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A little bit on the longer term sort of directions.",
                    "label": 0
                },
                {
                    "sent": "To avoid the computer vision problem, we worked in this simulated domain.",
                    "label": 0
                },
                {
                    "sent": "I think a really potentially interesting area is working on learning language in videogame environment.",
                    "label": 1
                },
                {
                    "sent": "So Deb Roy and his some of his students like Gorniak it at the MIT Media Lab.",
                    "label": 0
                },
                {
                    "sent": "That done some interesting thing where you could teach agents in an AI videogame environment to learn language about their world and you don't have to solve the computer vision problem because it's all in this simulated environment.",
                    "label": 0
                },
                {
                    "sent": "But there are a lot of people who play these games and wouldn't it.",
                    "label": 0
                },
                {
                    "sent": "Wouldn't they enjoy it if they could actually teach there?",
                    "label": 0
                },
                {
                    "sent": "AI in the game to talk to them in natural language, and I think using some of this technology we could develop more interesting computer games where you could actually teach the agents in the game to talk to you.",
                    "label": 0
                },
                {
                    "sent": "Of course, the next obvious step is to move beyond this, given the perceptual input and move to really connecting language to actual video and pictures, so it would be nice to do this sort of work where we actually have real vision computer vision to extract objects and relations and events from real perceptual data.",
                    "label": 1
                },
                {
                    "sent": "Another one of Debroy stood at the MIT Media Lab.",
                    "label": 0
                },
                {
                    "sent": "Mike Fleischmann has done some interesting work using Closed caption Text on video and learning from the correspondence between the caption video and the actual video information, and.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a blatant plug for one of my students who will be giving a talk on Thursday at 11:40 in R002 in the semi supervised learning section.",
                    "label": 0
                },
                {
                    "sent": "I've started my first work on really doing computer vision with a computer vision colleague of mine, Kristen Grauman at the University of Texas together with a couple of new graduate students.",
                    "label": 0
                },
                {
                    "sent": "So now Gupta, who will give the talk on Thursday about called Watch, Listen and Learn Co training on captioned images and videos where we're trying to learn to interpret images and video by corresponding what's happening in the video and in the image.",
                    "label": 1
                },
                {
                    "sent": "With the language description of that event, and I so I think it's one of our current steps towards trying to really do this and with real computer vision of connecting language and perception.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so to quickly wrap up, I've tried to argue that current language learning work uses expensive, unrealistic training data.",
                    "label": 1
                },
                {
                    "sent": "We've started to develop language learning systems that can learn from sentence is paired with an ambiguous perceptual environment, and we've evaluated on the task of learning to sportscast simulated Robo Cup games where it learns to comment games not quite but in the same ballpark as the ability of humans to do this task an I think this general broader problem of learning to connect language and perception is an important exciting research area.",
                    "label": 1
                },
                {
                    "sent": "Unfortunately I our various research communities have sort of become Balkanized where the computer vision people go to their conferences and talk about their stuff.",
                    "label": 0
                },
                {
                    "sent": "They use a lot of machine learning these days.",
                    "label": 0
                },
                {
                    "sent": "Obviously, the natural language people go to their conferences and very few people are looking at what I think is a very critical, important problem which is the connection between language and perception, and I think there's lots of interesting work to be done in that area.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}