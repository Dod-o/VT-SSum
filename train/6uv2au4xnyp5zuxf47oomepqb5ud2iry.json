{
    "id": "6uv2au4xnyp5zuxf47oomepqb5ud2iry",
    "title": "Model-Based Reinforcement Learning",
    "info": {
        "author": [
            "Michael Littman, Department of Computer Science, Rutgers, The State University of New Jersey"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/nips09_littman_mbrl/",
    "segmentation": [
        [
            "He's done huge contributions to our field, especially to planning and reinforcement learning.",
            "He's also done incredibly well in staging, where he regularly wins awards, so we're in for a good treat.",
            "It certainly very funny when he teaches, quite appreciate that.",
            "He's also done other things that have also led to several paper awards.",
            "In particular, just learn that he's actually an expert in crosswords, and he attends the meetings of the national Whatever Crossword Association.",
            "So maybe you can tell us more about it.",
            "Anyway, it's a great pleasure to have Michael Littman.",
            "Thanks very much, just as just as a quick check, let's see.",
            "So people who are here, how many of you already know reinforcement learning and are working in the field we have?",
            "I see and why am I giving you a tutorial?",
            "This is like just a nice chance for us to all you know, hang out together and right so of those who are left.",
            "How many were here?",
            "Because there's really just nothing else to do in Vancouver right now.",
            "I got three hands, big smiles, and then the rest.",
            "How many people here are actually interested in learning about the background here, but aren't already aware of it in many ways.",
            "I could swear that some of the same hands are going up, but alright, but that's OK, alright, I'm going to do my best iPod for people who are already doing this stuff.",
            "I'm going to apologize in advance.",
            "I'm lately teaching wise.",
            "I've been teaching introduction introductory computer science for people who aren't going into computer science, so I think I've kind of adopted a little bit of.",
            "Low level kind of presentation style and I apologize in advance if you're an expert already and I don't wanna hear that, but that's what I'm going to do and beyond that, what else do I want to tell you that if you're working this area and I didn't cite you?",
            "I'm sorry, but ask a loaded question at the end where you say you know.",
            "How does this relate to the work oven and I'll talk about how cool your work is.",
            "Excellent, alright, so I will be talking about."
        ],
        [
            "Based reinforcement learning.",
            "Basic, the basic flow here is to talk a little bit background wise about Markov decision processes, an reinforcement learning general because model based reinforcement learning is a special kind of reinforcement learning.",
            "So let's talk.",
            "We need to make sure that we're on the same page about what reinforcement learning is.",
            "The the bulk of what I'm going to talk about is, is the problem of efficient exploration.",
            "How is it that we can create algorithms that are going to explore their environments and be able to perform nearly optimal but with bounds on how much experience it takes to do that?",
            "So that's an area that's that.",
            "I've been focused on quite a bit in my own research with my students in my lab, and so that will be mostly what I talk about.",
            "Lately I've become very intrigued by Bayesian reinforcement learning and its connection to efficient exploration, and I'll tell you about that, and then the bottleneck here is.",
            "Learning algorithms that are very good at acquiring their own models, but then they actually have to plan with them and so there is a bottleneck there and I'll talk a little bit about what some of the current approaches are for for planning with these learned models.",
            "And I'm also fascinated by the connection to human cognition, so I'll just I'll mention a little bit about that.",
            "I would say more except I don't know that much more, but I'm fascinated by it.",
            "So if you have information about it, I'd be happy to hear more about it.",
            "I know there are some workshops going on at Whistler at the end of Nips that get it not particularly model based reinforcement learning, but various issues about how people learn and what that tells us about machine learning and vice versa.",
            "So I definitely encourage interested people to get involved in that.",
            "Alright, so I forgot to set."
        ],
        [
            "But I think I think we could start off with the game.",
            "How many people already know this games?",
            "In which case that would be kind of fun fun, alright?",
            "It's not that many.",
            "Not that many.",
            "Alright, here we go.",
            "Alright, so this is a little video game actually.",
            "On Saturday a student and I went to Quaker Bridge Mall in New Jersey.",
            "We have malls in New Jersey and.",
            "And had people actually do this, and we recorded their the results and you guys are going to do much, much better than the people in the mall did.",
            "And the reason I say that is because I've given this in talks many times and I had a good guess as to how well it was going to go in the mall.",
            "It didn't go as well as I was hoping it would go.",
            "So here's the game.",
            "This is what you see.",
            "This is your input state information and your goal is to win the game and at each step you get to make a series of decisions.",
            "Each step you get to choose up, down, left, right A or B and if you've seen this before, please don't say anything because you'll ruin it for the rest of us who are trying to have fun.",
            "So OK, so now you have to tell me what do you want to do up, down, left, right A or B?",
            "Alright, I heard today, but I'm going to do up here that was louder.",
            "You may think for a moment and now you have to tell me the next thing to do, 'cause you haven't won yet, right?",
            "I had right again, I hit right again.",
            "App.",
            "I heard up and right I'll do them both up, right.",
            "Nice confident a.",
            "B.",
            "Be again.",
            "Left a a wait.",
            "Hang on.",
            "There was too many things.",
            "It was no I did I did a I heard be here down.",
            "That's the time when you make a noise that was very good.",
            "The people in the mall didn't make that noise.",
            "I was.",
            "I was very disappointed.",
            "Down left left left.",
            "OP is fine, I'm getting a probability distribution of actions back, which is very nice.",
            "Left up B.",
            "Hang on a second, wait, wait, wait.",
            "I think I hit B, but it didn't seem that I hit be there.",
            "I did hippie, you won congratulations alright so.",
            "Why won't you shopping in the mall on Saturday?",
            "I needed at one point I actually stopped by let the experiment go on an I ran upstairs to the video Game Store and I ask the people in the video game store if they would come down and help us and be a ringer for our experiment.",
            "So what people non nips people were doing, they bumble around a little bit.",
            "They eventually figured out that they were controlling the Orange Square.",
            "Some of them actually managed to go and visit the little circle thing and maybe even picked it up.",
            "Maybe.",
            "Maybe even walked around with it.",
            "But they never quite figured out where it was supposed to go.",
            "Where was it supposed to go?",
            "Right here red.",
            "The same color, right?",
            "So I don't know if the example that I did had the passenger being red or not, but in general the passenger is whatever color of the destination it wants to go too, so the yellow circle wants to go to the yellow square like that, and if you did this, if you didn't figure it out, probably if you did it twice you get it, but it turns out you're not normal.",
            "In fact, it's really interesting to see what normal is it.",
            "This kind of very clever exploration is not is not the norm.",
            "I think people and definitely not in the program, so I'm going to talk about either.",
            "So even the people in the mall did significantly better in some ways than the learning algorithms I'm going to tell you about so.",
            "So they did get stuck more.",
            "Are learning algorithms, at least the ones I'm going to tell you about are very effective at actually figuring out what they haven't tried yet, and they try new things people they got into this mode where they would.",
            "They would decide what it is that they wanted.",
            "They decided how it was going to work and they would just stick to that.",
            "Didn't matter how long it was going to take.",
            "Alright, so good.",
            "So what is?",
            "Why do we play this game other than it was sort of a fun way to break the ice.",
            "Then I would say that this is an example of a reinforcement learn."
        ],
        [
            "Environment, so basically you had a task.",
            "There was some kind of reward signal that you were trying to get which was finishing the game.",
            "You had actions that you could choose that left down up, right AB, and possibly a sequence of actions was necessary to get high reward.",
            "In this case you had to get all through that sequence of steps to win an you have some kind of state representation of some kind of information about what the current state is on which to base your decisions.",
            "So there's other problems that fit into this setting as well.",
            "This is an example that we played with in our lab a couple years ago where we've gotten AIBO robot and what it's trying to do is decide it's just two choices.",
            "It's got A&B, which in this case correspond to turning a little bit left and turning a little bit right, and its goal in this case, instead of dropping off the passenger, was making visual contact with the pink ball, so in some ways a very simple task, but in other ways it's real right?",
            "It's actually moving around in physical space and implementing these things.",
            "It's not some kind of a.",
            "You know theoretical data structure.",
            "Well, there's theoretical data structures in the robot, but it's actually taking actions in the real world, and that's the data that is using for its decision-making, so again, this is this is another example that shows the different elements of the reinforcement learning problem.",
            "Maybe not a commercially significant problem.",
            "The turning around to see the pink ball problem I think, is well solved by other technology at the moment, but it has all the pieces in it, and it was.",
            "Good for us to experiment with using noisy data as opposed to the standard simulated.",
            "Very clean data.",
            "Alright, so when one is trying trying to create a reinforcement."
        ],
        [
            "System that's going to solve one of these tasks.",
            "I would say that there's three major pieces, at least in the model based perspective.",
            "There's these three major pieces that have to be addressed.",
            "The first one is using the knowledge that's gained from experience and applying it to new situations, generalizing it so that it actually applies to situations that the decision maker hasn't necessarily seen before.",
            "And that's really the learning problem.",
            "That's a core problem in our community.",
            "We have lots of different technology that we can use to solve problems like this.",
            "On the other side of things is, well, it's not just about learning about the environment.",
            "It's about acting well in the environment, and to do that, the agent has to carry out sequential decision making.",
            "It actually has to take steps in the world so when we were playing our little taxi game it's called.",
            "That's called the taxi problem.",
            "By the way, is introduced by Tom Dietrich number of years ago.",
            "There's like 300 some odd papers.",
            "I was able to find that talk about that problem.",
            "'cause alright anyway, so most of the actions that you took didn't give you any kind of immediate reward.",
            "You didn't win the game because you chose left at a certain point, it was only.",
            "Knitting together all those actions that got you to the final goal, and so in this particular case, dealing with this delayed gratification is it's a planning problem.",
            "It's sequencing actions to achieve some kind of goal, or maximizing reward or minimizing costs.",
            "And that's actually fairly well studied problem as well, so we've got these two nice, fairly well studied problems.",
            "The piece that kind of glue these together and gives reinforcement learning its own little problem to focus on is how do you put those two together?",
            "How do you do?",
            "Action choices and planning so that you get the right information out of the learning so that you can apply that and do well in the planning.",
            "So solving this exploration exploitation balance is one of the problems that makes reinforcement learning.",
            "I think a lot of fun to think about and a lot of fun to work on, and it's really significant.",
            "A lot of these problems are difficult to solve if the agents don't figure out how to explore and gain the data that they need.",
            "Alright, so the."
        ],
        [
            "A formal model that underpins a lot of work in reinforcement learning is the Markov decision process here.",
            "This was introduced a number of years ago, actually at Anand over lunch said people like me.",
            "We've devoted our lives to Bellman and I don't think that's quite true, but the sense in which it is true is that Bellman introduced these models and a lot of what goes into solving reinforcement learning problems is about solving what's known as the Bellman equation.",
            "Is this Q equation here at the bottom.",
            "So just to set it up for you, we imagine that the environment consists of some number of states.",
            "Call it in.",
            "In this case, some set of actions that the agent gets to choose among some discount factor which is controlling how important is it to get reward now versus reward in the future.",
            "The discount factor is very close to one, then rewards matter all throughout time.",
            "If the discount factor is closer to 0, then rewards now really significant, but you know who cares about later later is later on each step time step T, the agents informed of what its current state is.",
            "What's the state of the environment and it gets to choose an action from that it receives a payoff are some tea.",
            "And the expected value of which is determined by what's called the reward function, the reinforcement function.",
            "And then there's a transition to a next date in that transition is governed by this this T function.",
            "The transition function, which says that the probability that if you're in some state St and you choose some action 80, the probability will end up next in state S prime.",
            "Is that expression there alright?",
            "So schematically we've got an agent environment and they're having this conversation back and forth.",
            "The environment says the state the agent gives the action.",
            "The environment gives back the reward.",
            "State and the game continues.",
            "Now how do you solve a problem like this?",
            "He decide what actions take.",
            "Well, if you know the transition and reward function then this Bellman equation.",
            "Here if I had it.",
            "So I have a pointer.",
            "Cool this equation.",
            "Now I should probably use the pointer right?",
            "So this equation right here gives us a way of determining what to actually do.",
            "So what this says here is we're going to determine this.",
            "This thing called the Q function.",
            "the Q value of being in some state S and taking some action A is semantically what it is is, how much reward do you get as an agent get if it starts off in state S, choose action A and from then on does whatever the optimal thing is to do whatever maximizes expected reward, and the way we're going to we're going to compute that.",
            "Is by saying, well, we can get that by saying, well, it's going to get some immediate reward determined by the reward function, and then it's going to make a transition to a new state and that transition the probability that that state is prime is TSS prime once it gets to that new state, it's going to choose the optimal action, which in this case, since we're pretending that we know what this Q function is is, whichever action in that state has the highest value, so.",
            "Now we look over all possible next dates.",
            "This is weighted by their probability.",
            "We sum them up to take the expected value and we discounted by gamma to take into consideration.",
            "That's one step in the future, all those rewards are one step delayed, so it turns out if we can solve this equation, we know how to behave, in particular when we're in some state SST, we just choose whichever action a maximizes QSTA.",
            "So this is almost a linear system of equations.",
            "It has a little nonlinearity in it, which we've named Max and nonetheless there's.",
            "There's lots of techniques for actually solving these sorts of things, which is a kind of planning problem given the definition of the environment, what actions do you want to choose to maximize reward right now in the reinforcement learning setting, but basically defines the reinforcement learning setting.",
            "Is that we don't know.",
            "The agent doesn't know in advance what the reward and the transitions are.",
            "OK, maybe it is a guess.",
            "Maybe has a prior, maybe it has to set of three or four possibilities.",
            "Maybe it knows absolutely nothing.",
            "Regardless if it knew everything and it was certain that it was correct, this isn't the reinforcement learning problem.",
            "It's only when these are unknown to some degree that some amount of experimentation is needed to learn them.",
            "Apply them and then actually take actions in the world.",
            "So that's all of the reinforcement learning problem in a nutshell.",
            "We can think of."
        ],
        [
            "The problems is fitting into this mode, so the robot turning around to see the pink ball problem, we can view it as a Markov decision process consisting of a set of states actions are these arrows that transport cause transitions between the states.",
            "This particular state has a reward of pink, which I guess is good in this case, plus one and otherwise it's getting 0, so it's moving around in the environment trying to maximize reward just basically rotating left or right around this circle.",
            "Alright, so that's the problem."
        ],
        [
            "We actually developed solutions to solve the problem.",
            "What kind of algorithms can we use to decide what actions to take to maximize reward given some uncertainty about the way that the environment works?",
            "So one class of algorithms that I'll say a little bit about our policy, search algorithms, policy search is trying to directly learn this mapping from state to what you should do.",
            "So Pi.",
            "Here is the policy.",
            "It says it takes as input what's going on now takes his output.",
            "What it should do, and that's that would be a good thing to learn.",
            "If you could learn this, that's the whole problem.",
            "Learning the policy, deciding what to do.",
            "You're all set.",
            "The problem is that you don't get any concrete examples of well when you're in this state.",
            "You should do this action.",
            "We don't have a training set of that form, so in some sense direct policy search is tricky.",
            "One way to do it is to actually do basically hillclimbing guess apolosi, run it in the environment for awhile, see how much reward it gets, and see if any local changes to it might make things better.",
            "It's a very general algorithm.",
            "It can be applied in lots of different situations, but it can be very, very slow.",
            "It's not using a lot of information from the environment to kind of slowly Hill climb up in a space of policies, so."
        ],
        [
            "Other class of algorithms called value function based algorithms that take a different approach there.",
            "Instead of trying to learn the policy directly, they tried to learn this Q function first as I showed on the Bellman equation slide.",
            "So here we want to function that given the state in action reports what the expected value of future reward is going to be V. OK, and if it if we learn this, if the agent is able to learn this, it can act by just taking whatever action maximizes the value given the current state.",
            "So knowing this is sufficient for knowing how to behave.",
            "Furthermore, this slightly more direct data about given a state in action, what the future value is going to be because the agent is acting in the world, is in some state it tries some action and then it can observe what kind of reward it gets into the future.",
            "It's not very direct because in the meantime, while it's collecting up summing up the rewards that it's getting from one step to the next.",
            "Maybe it's you know, taking other actions in other States and so it's not really clear necessarily that they connect directly, but there's a whole bunch of algorithms including Q learning algorithm for learning the Q function that can be applied to do this, and in fact I think the majority.",
            "I haven't actually computed this.",
            "I'm not sure how to exactly compute this.",
            "If anybody works on that stuff, let me know, but I think the majority of work in the literature in reinforcement learning is about value function based approaches, so they are the most popular."
        ],
        [
            "What I'm talking about in this tutorial or model based approaches model based approaches are one yet one more step backed away from the direct application of the policy.",
            "So what model based approaches do is they try to learn this mapping.",
            "They try to learn what what's the transition function that takes state and action returns next date or probability distribution over next date an what's the reward function that takes state in action an returns immediate reward value that can be directly observed, not the long term value which it has to wait to see.",
            "The implications of his actions first, so this is actually something that can be directly learned.",
            "This is a supervised learning problem.",
            "It gets examples of hey, if I wanted an example of what happens if I choose some action, some state, I just have to get to that state, try that action and see what happens.",
            "So then to use.",
            "This is a little bit more indirect.",
            "So to go from the learned transition reward functions to the Q function and therefore to the policy you have to solve the Bellman equations.",
            "So this can actually be extraordinarily expensive depending on the size of the state space and other constraints that you've got.",
            "So really, there's a kind of a tradeoff happening here where we've got on this side.",
            "We're learning something that's very directly usable, doesn't require a lot of computation to use what's being learned here, but learning it is actually very indirect, and you don't get direct examples of you should choose this action in this state.",
            "On this side we have a quantity that is very direct to learn, but then it's computationally challenging to actually use that to make decisions in the environment, but nonetheless I think because for me anyway, the experiments we've been doing in our lab.",
            "Data was really expensive when we run these little robot examples they don't have tons and tons of data.",
            "They can't run a million trials like you can do in a simulator.",
            "So it's really important to us to make the most use out of the data that we got.",
            "These are often more data efficient as well.",
            "They each data sample that they get can directly be used to estimate something.",
            "So so that was why our lab has been focused on these model based approaches and that's what I was sort of equipped to tell you about.",
            "So that's what the tutorials about.",
            "Alright, any I guess any?"
        ],
        [
            "Up to this point, we good so far.",
            "We didn't want to see this animation again, I apologize.",
            "And OK alright, one more animation moving on so so so how do you do model based learning?",
            "So here's kind of a schematic approach to understanding what the different pieces are and how they relate back to the way that I described.",
            "The main problems in reinforcement learning earlier.",
            "So this is the same figure from a couple times ago, but I expanded this environment block and definitely agent block and we're going to peer inside the agent and see what's going on here.",
            "So if you were thinking about model based reinforcement learning algorithm really, what's going on is there's a learning piece that's actually learning the transition and reward functions.",
            "It's getting training examples by acting in the world.",
            "It gets state and reward samples.",
            "It gets next date reward samples just by waiting a single step and its actions are actually coming from solving this model using this model to decide what the best action is in the in its guess about the environment.",
            "It produces these actions which go out to the world, but they also come around, and they used as training examples as well.",
            "So here's our learning piece.",
            "Here's our planning piece and deciding what to do.",
            "And then there's this other little exploration piece that may modify, right?",
            "So when you're learning this model, if you just have a guess, your learning algorithm just gives you its best guess that may need to be modified, someone you don't want to just act according to your best guess.",
            "Because if you're wrong, you might not be able to discover high value rewards someplace else in the environment so often.",
            "People are finding practice that is really necessary to add some kind of exploration component to what they're doing.",
            "Some kind of thing that will cause the system to learn about actions that it might not otherwise have taken, given its best guess about the transitions and rewards."
        ],
        [
            "Alright, so this is now the beginning of that next block.",
            "Now we're going to talk about some of the formal and experimental work we've done on trying to.",
            "Show that we can learn in some of these environments depending on various properties of the transitions.",
            "We can learn efficiently.",
            "We can learn with the polynomial, polynomial number of samples, so to do that we have to define pretty closely what we think the problem is.",
            "So we're taking at the moment the reinforcement learning problem to be as follows.",
            "We're going we're learning in a kind of a pack setting, probably approximately correct, extended to reinforcement learning by a bunch of researchers that says as follows our algorithm.",
            "Is given epsilon and Delta as inputs.",
            "It knows in the world that there are K actions and States and it knows the discount factor, but it doesn't know the transition and reward functions yet and we're going to say it now begins acting in the world begins taking actions, learning about state transitions and gathering rewards, and we're going to say that each time in the environment, each time step T where the Q value, it doesn't know the real Q function, but according to the real Q function, if the action that it took in that state.",
            "Is more than epsilon away from the best.",
            "the Q value of the best action in that state we're going to call that a mistake now.",
            "Nothing in the environment is telling the agent.",
            "Oh, you know, that was a mistake.",
            "It doesn't really know that, but we as the ones evaluating how the algorithm is performing, we could imagine that we're we're applying that rule to it.",
            "OK, so so an algorithm is going out there.",
            "It's going to act, however it's going to act and we can keep track of the number of mistakes that it makes, and we're going to say that an algorithm is a good algorithm.",
            "It's an efficient algorithm if there's some M, that's a bound on the number of mistakes that it makes.",
            "That bound holds with some probability, 1 minus Delta, and that bound itself is relatively small polynomial in the number of States and actions.",
            "How close you want to do it this epsilon parameter?",
            "How sure you want to be this Delta parameter and these sort of the effective horizon time?",
            "How much does future reward count?",
            "As measured by 1 /, 1 minus gamma, so we're going to say that an algorithm is a good algorithm if it's the number of mistakes it makes his small an infinite run.",
            "OK, but you know we're letting it cheat a little bit by by having the epsilon.",
            "Delta doesn't have to always work and by work it doesn't have to be perfect, it just has to be close to perfect.",
            "Alright, the only way to actually solve this problem is by balancing exploration and exploitation to some extent.",
            "An algorithm can't just go around taking the action that it thinks is best, because it could be that there's some other high scoring action out there that it needs to know more about that would be pure exploitation.",
            "Pure exploration doesn't work either.",
            "That means just kind of running around trying things all the time.",
            "And that's going to lead to lots and lots of mistakes, so it has to make it has to explore just enough.",
            "And it has to exploit the rest of the time, right?",
            "And so we're going to show what I'm going to talk about is that there is a number of algorithms that actually can achieve these kinds of bounds, and even do it in the face of various kinds of generalization.",
            "If you know something about the function class from which transition probability matrices are constructed.",
            "So to give you a sense of how this work."
        ],
        [
            "How, how might this work?",
            "So let's imagine that we've got an agent that's in this really simple chain environment, sometimes called the combination lock environment, but starts off over here, and it has two choices.",
            "It can take the black actions which move it gradually along the chain, and eventually to this state.",
            "That has really high reward.",
            "It can sit in here and gather 20s forever, or each step along the way.",
            "It has a choice it can take.",
            "This blue dashed action, which has an immediate reward, not a very large one, but it's bigger than zero, which resets it back to the beginning of this chain.",
            "So OK, so if we know this problem, we know that the well depending on the discount factor, probably the optimal thing to do is to March down the chain and then gather 20s until the end of time.",
            "But a learning algorithm as it's exploring this domain.",
            "It might be in a situation where you know it's explored these States and it knows what they do, but it's never tried the black action in this last statement.",
            "Camp so well now what?",
            "So this is this is the model that's been learned so far.",
            "It knows that the black action moves it to the right, and except here it doesn't know, and the dashed action moves it back in the model based setting and we're going to use this model to plan and actually decide what to do in the environment, which actions to take.",
            "But the only way we can plan in this environment is if we make some sort of assumptions about what happens in parts of the model that we don't know about.",
            "So maybe we just assume this black action goes to some other random state that we've already seen.",
            "Maybe we assume that it immediately gives some high reward or some low reward.",
            "There's lots of different things that an algorithm can assume.",
            "Maybe we'll just leave it up to the learning algorithm.",
            "It's going to make its best guess, you know.",
            "Maybe the learning algorithm is going to generalize.",
            "It says black actions always take me to something new, so maybe there's something new there.",
            "Regardless, whatever the learning algorithm does, it's going to have to make some kind of guess here.",
            "Now what's going to?",
            "What's going to happen in terms of this pack?",
            "MDP assumption if the truth is that this?",
            "Going to the right here can lead to only low reward.",
            "Bad reward and the algorithm assumes that out here is just bad reward.",
            "Well, that's good, we could just ignore what's going on in the question Mark zone and take a little policy that just goes like this.",
            "Gather your point ones over and over and over again, and that would actually be optimal.",
            "If the reward out here was low.",
            "Also, if the Furthermore, if the if the reward out there is low, but we assume that it's high, what will happen in that case?",
            "Well, in that case the agent is imagining, well, there's something really great out here, and if it starts out over here at the end of the chain, it might say, wow, there's something really great out there.",
            "I'm just going to go and get that really great.",
            "Well, it may be great.",
            "It may not be great, but the point is it's going to visit and find out it's going to actually explore by checking out this part of the environment.",
            "On the other hand, if the truth is that there's very high reward, either these are fine.",
            "By the way, if the truth is that there's really high reward out there, but the algorithm assumes that it's low.",
            "It's not going to go there.",
            "It's not going to find out all the wondrous rich is that it missed out on an.",
            "It's actually going to perform suboptimally, possibly worse than epsilon.",
            "Suboptimally, an algorithm that does that can't be pack MVP.",
            "By this assumption, it hasn't explored enough.",
            "It hasn't checked out these high reward areas, and so that's a failure.",
            "That red box here?",
            "That's a failure.",
            "On the other hand, if it's out, there is really high reward.",
            "It's going to visit.",
            "It's going to find out its optimal all is good.",
            "So these two cases are fine.",
            "This case is fine eventually there's exploration, but this is bad if we want to achieve pack MVP, we have to avoid this case.",
            "We can't ever assume.",
            "That something is low and it really is high."
        ],
        [
            "Alright, which leads to a very natural idea.",
            "So the idea of model driven exploration is to say this.",
            "We're going to be learning the reward and transition function, so we have some cartoon, some approximated version of the reward and transition functions and what we're going to do.",
            "We're going to learn those, and then we're going to augment the model with a bonus for unknown transitions.",
            "Anything out there that we don't understand yet?",
            "The algorithm doesn't know well.",
            "It's going to assume is really great.",
            "Then it's going to use that augmented model to plan to decide what to actually do, and then it's going to repeat by actually following.",
            "This planet may learn new things.",
            "It may get to states it hasn't been to before.",
            "Try actions that hasn't seen before and that will cause it to learn new things which will change the model which may change the behavior.",
            "But what's interesting is that what I'll say is that this basic scheme can actually lead to pack MDP.",
            "Algorithms algorithms that with just a polynomial number of mistakes get epsilon optimal reward and very simple idea.",
            "The key, though, is that the learner actually has to know what it knows.",
            "It has to be able to distinguish between transitions that it's modeling correctly, and ones that it just isn't sure about enough yet and maybe need some more data.",
            "So we formalize this learning problem.",
            "This knows what it learns, knows what it knows.",
            "Learning problem, we call it quick learning, and that's kind of the useful learning setting for plugging into these autonomous reinforcement learning algorithms when they're learning about the environment.",
            "They if they want to explore effectively, they're going to distinguish from the known the unknown from the known transitions.",
            "Actually, I don't know that they need to do that, but this is one way that we know works.",
            "It's sufficient to plug in a quick learner.",
            "It may not be necessary, so just to give you a sense of how this works."
        ],
        [
            "There's three models of of.",
            "Of the supervised learning problem, right?",
            "So now we're taking the learning of the transition function or the transition rewards.",
            "Let me focus on the transition function as a supervised problem.",
            "It's going outta learners going out of the world.",
            "It's trying things out.",
            "It's gathering data about when I'm in this state.",
            "I try this action.",
            "Here's what I see next, and it's just collect these as examples in a training set.",
            "And now what should it do well?",
            "There's there's two well known settings that pack setting, and the mistake bound setting.",
            "And here's the quick settings so I can distinguish them.",
            "I can point out what the differences are between them, so the pack setting, kind of which is more or less the classical setting, which people often do.",
            "Machine learning says this.",
            "What we're going to do is imagine that all our training examples, all the things we're trying to learn.",
            "They're all drawn from some distribution IID, so independent samples from some distribution we're going to take a bunch of labeled examples.",
            "Which are these blue bars here and after we have enough of them, then we run our learning algorithm and.",
            "Error, you know, struggles away and it figures out how things work and then after that for something to be successful in the pack model, it has to make all correct predictions.",
            "After that any new sample that comes in it needs to.",
            "Predicted correctly, OK, most of them with high probability, that sort of thing.",
            "So we've got a bunch of samples that are labeled and then we get a bunch of samples in which we have to be correct.",
            "That's fine, but it turns out that this is very problematic to apply this kind of learning in a reinforcement learning setting, and the reason for that is the samples.",
            "Each of these training examples is coming from the agent interacting with the environment, and to say that's IID is like saying that well, during the training the agent has to act a particular way so that it's getting samples from the distribution of the states that it's visiting and after it's learned it needs to continue to act that way so that it can get testing examples from that same distribution.",
            "So imagine I don't know when I'm feeling whimsical.",
            "I imagine that, you know, here's a learner and it's out in the world exploring and it's learning all about the environment.",
            "And it finally gets it in and understand how the environment works and now it spends the rest of its life kind of trapped in its own body, being forced to act the way it did when it was still learning, but knowing the right thing to do and not being able to do it.",
            "So this isn't, it's just not a good fit.",
            "The fact that you really depend on this IID assumption really makes it problematic to apply learning algorithms that are great in the pack setting.",
            "In this reinforcement learning setting with exploration.",
            "So, but that's OK. People have come up with other learning models.",
            "The mistake bound model is nice in that it assumes inputs are presented adversarially, right?",
            "That's some something out there is can produce any training example at anytime.",
            "Now the model is a little bit different.",
            "Now.",
            "The model says that each time an example comes in, the learner has to provide the label and it could be wrong.",
            "And then I'll cover those bars red so it gets it gets an instance.",
            "It tries to predict the label.",
            "It gets it wrong.",
            "OK, it moves on.",
            "It gets another instance, maybe correct, it gets it correct.",
            "That's a green bar.",
            "Another correct one.",
            "Then it gets another one.",
            "It's wrong.",
            "It can continue to make mistakes at any arbitrary time, but the total number of mistakes has to be bounded.",
            "That's the mistake bound part.",
            "OK, so here we've got.",
            "This is a learner, that is, it can be given any kind of examples from any kind of distribution, but it can only make a finite number of mistakes, unbounded number of mistakes for it to be a successful mistake bound learning algorithm.",
            "Now you can only really well actually this is true pack as well.",
            "You can only really successfully make learning algorithms under restriction restricted assumptions about what the hypothesis classes, what is the function that's actually labeling these instances, but nonetheless there's.",
            "There's a wide variety of algorithms that are sorry function classes that are learnable in the mistake bound model so.",
            "This is getting us much closer to something that we can use in a reinforcement learning setting.",
            "The problem is from the previous slide, if it's making mistakes and it doesn't know when it's making mistakes, it doesn't know when something's a guess.",
            "It's just guessing, and if it's wrong, it counts against it.",
            "It won't necessarily explore efficiently, so in particular, if it's imagining, oh, you know what's behind the screen, I'm going to assume that there's nothing valuable behind the screen that might be a mistake, and the mistake model says that's OK, it's just a mistake.",
            "But in the MDP setting in the reinforcement learning setting.",
            "That mistake could actually cause it to never get near optimal reward, because maybe the optimal thing is to just go behind the screen and probably nothing there.",
            "To one second, yeah, surely this is really nothing there as far as you know.",
            "So.",
            "So this mistake bound setting where where it doesn't really know when it could be making mistakes and not it can actually achieve this pack.",
            "MDP kind of kind of bound.",
            "Alright, so.",
            "So what do we need instead?",
            "Well, this the quick setting says we're going to do things a little bit differently now.",
            "The learning algorithm, if it's given an instance that it can't predict the label, it knows that it can't accurately predict the label 'cause there's multiple possibilities that could work.",
            "It has to say I don't know.",
            "Shrug its shoulders and so we can start adversarially.",
            "Selected input is coming in.",
            "It can say I don't know, and then maybe another instance comes in.",
            "It's like oh wait, this one I get and it gives.",
            "It has to give the right answer.",
            "I don't know right answer right answer, right answer.",
            "I don't know like the mistake Bell model.",
            "It could continue saying I don't know to any arbitrary time in the future.",
            "But the total number of times that it's going to say I don't know has to be bounded by something small.",
            "If we're going to say that it's a successful quick learning algorithm, this can be used in a reinforcement learning setting and give us those those exploration pack MDP kinds of bounds, because what it can do is assume any transition that it can't predict.",
            "Maybe do something good.",
            "Anything that can predict it's predicting accurately, and therefore it won't be making mistakes on those.",
            "So the combination of some kind of successful quick learner and this model based setting gets you reinforcement learning algorithms with these nice guarantees.",
            "So that's that's the basic story there."
        ],
        [
            "Sure.",
            "There's there's a nice proof that this works in general when it's combined with an algorithm that's often referred to as our Max because it assumes that anything that's unknown is that has maximum reward reward Max.",
            "It's not the most brilliant name, but anyway it does work and this this this class of algorithms has been used for learning in these flat MPs where there's M States and each state could be completely different from every other state, but it's still going to learn about them.",
            "It's going to decide what to do in those States and there's a number of key ideas.",
            "We know that the optimal action for an approximate model is near optimal in the real model, which means if we know that the model we've learned in the environment might not be perfect, but it's epsilon close in all its transitions, then taking the.",
            "Optimal actions with respect to that guess about the environment.",
            "This close guess is going to lead to near optimal rewards in the environment.",
            "It's not like there's these weird discontinuity's where if you just get one little thing a little bit wrong, you're arbitrarily far away from optimal, so that's the simulation lemma says we can simulate the environment with an approximate model and the Explore exploit lemma says basically this is a cute idea.",
            "If there's parts of the state space that we can't reach 'cause it's very low probability to be able to get to one of these states, then even if they have high reward, we don't have to go there.",
            "We can get near optimal roared.",
            "Just staying among the states that we can get to with high probability and that means that when we take actions within the known states, if we can't reach any unknown state, well either it's OK.",
            "This is a better way to say it, either it's the case that our algorithm can explore it can with high probability get to some state that it hasn't seen yet, and learn about it.",
            "Or if it can't do that, it must be able to exploit.",
            "Which means among the states that had already understands, well, it can achieve high reward for the real model.",
            "OK, so these two pieces together give us these nice efficient learning algorithms.",
            "In the in the flat case, there's a number of papers that talked about this, but some people have looked at.",
            "I didn't know what to call it.",
            "UN flat in cases where there's generalizations that can be made between states.",
            "Like for example, if the dynamics are determined by a dynamic Bayes net, some kind of factored model, or it's there's a metric space, it's continuous, there's a continuous state space and there's some relationship between nearby States and the other states that are nearby or in what we showed in.",
            "Generally any function class that can be quick learned we get these same kind of nice ideas that come into play and we can get near optimal reward.",
            "And in particular, what we showed with this is Leon Lee, who's my grandson who is a postdoc now and looking for a good job.",
            "Very bright young man.",
            "And So what he was able to show in his thesis is that as you know, any quick learner that we plug into this framework gives us the pack MD bounds, the time to learn the environment depends on the time to learn the transition function.",
            "So if it's the kind of transition function that you can learn with very few examples very effectively, then learning to explore in the state space is actually going to be efficient as well.",
            "So so you might ask."
        ],
        [
            "Hope if you're paying attention you would ask OK, but are there things that we can quick learn so all we showed so far is if you can quickly under function class actually show it well, I've stated to so far.",
            "If you can quick learn a function class then you can behave well.",
            "You can behave efficiently in reinforcement learning sense in environment where the transitions have that function, but what are some things we can quick learn well?",
            "The easiest, simplest an actually the basis of almost all the interesting stochastic algorithms is to learn a probability value.",
            "So basically coin flip.",
            "You have a weighted coin.",
            "You don't know initially the probability that that's going to come up heads or tails.",
            "That's the thing you have to learn.",
            "But given M trials and we've observed X successes X times, the coin came up heads.",
            "We can estimate the probability is X / M and there's very nice bound very standard bounds, tufting, bounds, turnoff bound type things that say how many times you have to flip a coin before this estimate.",
            "This empirical estimate is really close with high probability to the true value, so this is this you can think of this as a quick learning algorithm in that somebody gives you a coin and you have to predict what's the probability it comes up heads.",
            "And you have to be right?",
            "Or say I don't know, but only say I don't know a small number of times we can quickly learn this probability by just saying I don't know until we've flipped it enough times that this bound kicks in and we can be epsilon accurate with high probability.",
            "So quick learner learning a coin is no big deal.",
            "People know how to do this.",
            "This is a standard building block.",
            "But there's other things in addition to the coin probability that."
        ],
        [
            "Making quick learn so we can anything that we can quick learn.",
            "We can learn.",
            "The hypothesis class that says that the output is the output of some quick learner, right?",
            "So let's.",
            "Well, in this particular case, you could think of it as being the dice learning problem.",
            "So imagine that we're going to try to estimate got instead of a weighted coin.",
            "We have a way to die.",
            "Then we're going to roll it, and it comes up 12345 or six with unknown probabilities.",
            "Well, we can treat that as being 6 separate coins and what we're doing is on each trial we're predicting the output of each of those coins simultaneously, so we can quick learning to the coin probabilities and what this is saying is that if we have a vector of something that we can quick learn, then we can quick learn that whole vector so we can do dice learning.",
            "There's actually better ways of doing it than thinking of it as a bunch of coins, but that's one concrete example here.",
            "Another one is if we're learning a mapping from some input space to some output space, and the input is partitioned into regions where each of those is quick learnable, then we can quickly, in the whole space, and that's a really easy result as well.",
            "It just says that we run a separate quick learner for each of the partitions of the input, and then when an input comes in, we just ask the corresponding quick learner for the answer.",
            "So that's this seems sort of.",
            "Two simple to be worth mentioning, but combine this and that and the coin probability and you can learn a standard transition function in an MDP, 'cause what is it as transition function it's mapping the state, the choice of state and the choice of action that bust up the input space into into end times.",
            "K separate problems to learn in each of those you have to learn a vector of probabilities, which are the state transition probabilities to each possible next state and each of those values itself is just a Bernoulli trial.",
            "So by combining these at three levels.",
            "We can actually quick learn an entire transition function and therefore.",
            "Quick we can we can solve reinforcement learning problems efficiently in this in this flat setting where each state action pair is considered to be a separate problem.",
            "So there's a handicap.",
            "Yeah, go ahead.",
            "Or other animals be back in the case where I flat state space, but I don't know it's cardinality by discovering personality effectively if it works right now when you say cardinality, that's like the partitions of the input space that was calling partitions.",
            "Here we don't know the number of states.",
            "So I know maybe definition when I'm asking, OK?",
            "There is.",
            "I see I see and we want to bound that grows with that unknown number of states say, yeah, so these these algorithms do that as well.",
            "They tend not to actually need to know necessarily the number of states.",
            "But they behave well with respect to what the number of states actually is.",
            "The other question is, well here.",
            "I'm talking about a known input partition.",
            "What if it's unknown?",
            "What if a bunch of States Act act like each other, but we don't know which ones and we have to discover which ones connect with friends will get to that, but this is kind of the lowest level.",
            "Basic is just a regular MVP.",
            "The way Bellman talked about them, we can now efficiently learn in that setting how to behave with unknown, initially unknown transition reward functions.",
            "Alright, so yes.",
            "I was wanted by it so interesting to have this Pattern Bowl.",
            "Property because it is polynomial in the state space.",
            "So it seems like the most naive things which is visible states.",
            "So if it were.",
            "OK, so hang on.",
            "So if if we have a deterministic system where you can actually choose, you know when you take an action it always goes to the same next state exploring that graph and getting these same kinds of bounds is easy.",
            "You just just basically like depth first search.",
            "Almost you just go to any transition you haven't visited yet.",
            "You try it, which the reason that this isn't just completely trivial is in the MVP setting.",
            "There's probabilities involved.",
            "It may be hard to get yourself to a state to learn about it, so you can't really just reason about it saying OK, well, just I can quickly visit all the States and learn about all those states.",
            "Because you can't force yourself, the agent can't put itself in a state necessarily.",
            "So you need you need a more flexible notion of what optimality means.",
            "That can account for the case where maybe there's a state that you can't get to.",
            "Or maybe you can only get to it with really low probability.",
            "You don't count that against the learner.",
            "That answer the question.",
            "So motional.",
            "Every city comes in where it actually visit visibility.",
            "Visit visibility to any states, becomes a property, and then we could have systems where we actually becomes an exponentially small probability to visitor parking space.",
            "Or how is that alright?",
            "So the question is what happens if, say, there are parts of the state space that are very very difficult to learn?",
            "It's not a sort of rapidly mixing Markov chain kind of situation, and that's OK.",
            "These results will still apply.",
            "They tell us that for the stuff that we do get too often enough we're getting.",
            "Getting high reward if things are very hard to get to, there may be very high reward there, but it can't have a big impact on our expected value.",
            "'cause it's very hard to get there.",
            "So the the those are the sorts of issues that make the analysis tricky, but the But the top level story things are actually sort of nice and clean as long as we can quick learn the transition function, no matter how horrible the state space is, we get high reward with high probability with only a polynomial number of mistakes.",
            "OK, so there are other studies.",
            "I particularly mentioned.",
            "These quick, learnable classes because it gets us up to transition probabilities.",
            "There are other sorts of things the union of two quick, learnable classes, quick, learnable.",
            "I'll mention a couple others as we go."
        ],
        [
            "Right, so just in case there were too many epsilons and deltas for a minute, here's here's a video of a robot.",
            "Again, this is an AIBO robot in our lab.",
            "It's trapped in a 4 sided box, but there's a door on one side.",
            "It has to escape through the door.",
            "The door is over.",
            "Here you can see it.",
            "It's got there's the door.",
            "It's got forward, backward, left, right slide left and slide right.",
            "You know, like step without turning to left and right, which turn out to be important to not get stuck with your shoulder on the door.",
            "It's if we describe discretize the state space into 4050 states.",
            "Number of positions and orientations, and then give one of these model based learners.",
            "This are Max where it's quick learning the transition probabilities.",
            "To the system to learn it can actually learn this fairly effectively in the dog, can escape from the box.",
            "All is well.",
            "In this case the dog is being tracked.",
            "You can see it's got a little green little Raspberry Beret here.",
            "Anna Green, Fanny Pack and those are being tracked from above mounted camera, so that's how it knows where it is.",
            "It can, it can work."
        ],
        [
            "But I think a natural thing to say at this point is, well, if it's growing with the size of the state space.",
            "If you have to learn about all these states in the environment, most problems that we're thinking about and that we care bout have a very large number of states.",
            "So if we really view each of the states is completely independent.",
            "This transition knowledge doesn't transfer.",
            "Learning is going to be really slow.",
            "These algorithms can actually even the really smart ones can do really stupid things under this assumption where they actually walk around in the environment and like touch everything right there, like they bump themselves into everything because they don't know.",
            "Maybe that's the magic way out of the boxes to do this crazy thing so.",
            "So we need to take advantage of the fact that the world has some regularity.",
            "Therefore the transition functions can have some regularity.",
            "So if the quick learners have some regularity that can exploit that, the whole thing can go a lot faster.",
            "So here's, I'll just I'll show you one concrete example of this idea.",
            "Alright, so we've got a.",
            "Same kind of environment as before.",
            "There's a robot that's in an environment.",
            "It's got an X&Y coordinate.",
            "It also has an orientation, but we're going to add one more thing to it, which is there's a bumble ball in the environment and obstacle, moving obstacle, stationary obstacles are whatever they are, but moving obstacles are interesting because they become part of the state space, so the right thing to do at a given point in time may depend not only where the robot is, but where the obstacle is in relation to the robot.",
            "So now we've got.",
            "Well, potentially very high dimensional, even if we discretize the X&Y the ball, the X&Y, the robot and the state of the robot.",
            "We're talking about a lot of States and the robots going to learn really, really slowly.",
            "It's very often going to encounter states it's never seen before it's going to be exploring for a very long time, so here's what we're going to do.",
            "We're going to do our own domain knowledge to say, hey, you know what?",
            "There's some structure in this environment we're going to build a little graphical model DBN that says.",
            "As the when we when the agent, the robot chooses the forward action, the change in the balls X&Y coordinates is independent.",
            "It doesn't depend on anything about where the ball or the robot are, just moves around randomly.",
            "Amazingly, this ball really does that.",
            "I'll show you the video in a second.",
            "I thought it would have all kinds of history.",
            "It's like no, it's it's pretty much like IID movement like Brownian motion.",
            "So the robots X&Y under the forward action, the change in the X&Y doesn't change, is only affected by the angle that the robots facing, so right, so if it's facing this way and it goes forward, then X&Y changes certain way according to trigonometry, right?",
            "If the robots facing this way then why change a different way when it moves forward, but the robots angle actually is, again, independent of everything else it's changing angle.",
            "So that's for the forward action.",
            "We just write down this model, but we don't have to fill in the numbers here.",
            "We just say what depends on what moving when.",
            "It takes a right hand turn, actually.",
            "They're almost all independent.",
            "I think that's the model that we use that that the X&Y don't depend on the previous X&Y, 'cause it's really not moving in X&Y, just turning the robot X&Y.",
            "Oh sorry, the ball on the robot and the angle actually does change, but it doesn't matter what the current angle was, it's going to change relative to whichever way it's facing, so now we have a model where there's lots of things that are independent.",
            "There's a lot less to learn.",
            "There's way fewer parameters.",
            "And what's nice is that we can.",
            "Well, OK, I'll show you the video and then I'll talk about quick learning."
        ],
        [
            "So that's sorry that's not a robot, but that is the Bumble.",
            "Here's a bumble ball, just brutal Mumbles around like that and this is a robot again wearing patterns on the on its back so we can detect it from the above camera.",
            "We're also detecting the ball which is a distinctive color and is trying to get to this goal location at the end.",
            "The reward function is it gets high reward for getting to the goal it gets.",
            "Big negative reward.",
            "Every time it bumps into the ball and it gets small negative rewards just for not being at the goal yet.",
            "So just kind of encourage it to get on with his life.",
            "Alright, so.",
            "We tried very sort of function approximation with with with Q learning that didn't didn't go very well.",
            "So remember trying to get down here and this is after after 45 episodes.",
            "Now this is now 13 episodes using the model based algorithm which is actually learning those transition probabilities with the conditional independences from quick learning them from experience and so you can see it's already doing a very good job of pointing in the right direction, the ball the ball is out of the way at this particular case.",
            "It's heading fairly directly towards this corner.",
            "And if the ball is close to it, you can see it.",
            "It's a little bit more careful trying to get by the ball.",
            "Sometimes it looks scared, but I don't think it's feeling frightened so its value function does drop when the ball gets close.",
            "So it depends on how you define frightened, right?",
            "So now here's the robot is trying to get down here to the corner.",
            "Here's our bumble ball.",
            "It's unfortunately placed in its way, but that's OK.",
            "The robot kind of waits it out since it's moving randomly.",
            "It's just wait long.",
            "And Oh no, that's not helping.",
            "Sometimes it looks less random than others.",
            "To the ball is kind of attacking.",
            "Oh good, we got an opening.",
            "We got an opening right?",
            "We're going to head right to the goal now here we go just just gotta kind of facing the right direction and start trucking.",
            "And unfortunately the ball has other plans.",
            "Alright so.",
            "Yeah, I don't know what computational models of frustration look like.",
            "I guess if we ever saw the robot actually go up to the ball and just that would count as basically giving up, but it did.",
            "It's patiently waiting for the ball to be in a position where it can actually get to where it needs to go.",
            "Just get over there alright?",
            "Alright so it made it good so good.",
            "So in this example the what's going on here is because we know the conditional independence is what really it's."
        ],
        [
            "And instead of learning the whole transition probability matrix that says from this combination of ball position, robot position robot angle, here's what the next one looks like.",
            "Instead, it's just learning little ones that say when you do the forward action, what's the probability distribution over the change in the balls position, robots position and robots data all independently and so lot fewer things to learn.",
            "These are all independently quick learnable.",
            "This this idea I think I had it on the previous slide, but this this observation that if you know the conditional independence structure, you can learn the transition matrices really efficiently, even though there's an exponential number of combinations of values of the state variables, we don't need anywhere near that.",
            "We need polynomial in the size of these little mini transition state diagrams, so that observation was made by Kerns in color and so they showed how you could do this, and so I'm not saying anything new beyond what they said in this particular case.",
            "So we did discover that for the robot examples it was nice to have it be the change in variable instead of predicting from the old value of the variable the new value of the variable.",
            "So sometimes sometimes that's the thing that you want to be predicting.",
            "They would just kind of obvious."
        ],
        [
            "So that's that's a different kind of representation of the transition function that is efficiently learnable and applies really well in lots of different circumstances.",
            "Here's another one.",
            "This is one that we found to be useful in several settings, and we're continuing to work on it where we represent transitions instead of from states to states or from features to features like in the DBN case.",
            "We're going to represent them in terms of object attributes to object attributes, so talking about the world as consisting of a set of objects.",
            "So this is this, is a figure from a paper, which is the taxi problem that I showed you in the beginning.",
            "It's very convenient, and in fact I think if you ask yourself what you were doing when you were exploring in this game to talk about the state in terms of a set of objects, there's a taxi.",
            "It's in someplace there's a passenger.",
            "It's in someplace, there's walls there in some place there's a destination.",
            "It's also in some place, and whether two things are in the same place is very relevant to figuring out what's going to happen when you choose an action.",
            "So so now, instead of yeah, So what happens when objects interact?",
            "That's what the learner is going to try to figure out, and when we run these sorts of things, we get much more human like exploration instead of.",
            "The algorithm you know bumping up against all the walls and all combinations.",
            "Once it figures out how navigation works, it zips over to like the Green Square, right?",
            "It doesn't try all actions everywhere on the way there.",
            "It actually goes to wear something new might happen."
        ],
        [
            "And when we actually haven't learned in the taxi problem, the sorts of elements of the transition function that it picks up are very readable.",
            "Natural kinds of things.",
            "It learns that if the current action is North and it's not the case that the taxi is touching to the North wall, then the Y coordinate of the taxi is incremented.",
            "OK, this is in fact what the code looks like, but this is what the learner figures out as well.",
            "If you do the drop-off action and the passengers in the taxi and the taxi is touching the destination, then the passenger will no longer be in the taxi, it gets dropped off.",
            "OK, so these are the sort of various little little bits and pieces that it picks up and it's able to solve this problem much more efficiently.",
            "So in fact if you look at and we have a quick bound for how long it takes to do this.",
            "It's polynomial in the number of types in the world exponential in the number of conditions.",
            "Like drop this and this and this all have to be true simultaneously.",
            "It doesn't search through that conjunction space very efficiently.",
            "It's exponential in the length of the conditions, but it's polynomial in the number different types in the world and independent of, say, the number of states in the world because most of the states are just combinations of objects in different positions.",
            "Alright, so when we run this kind of a learning algorithm on the taxi problem, we see.",
            "Well, here's compared to a bunch of other algorithms Q.",
            "Learning this just kind of classical model free value function based reinforcement learner.",
            "Where exploration is done epsilon greedy, it tries random actions every once in awhile and it's you know it's good for Q learning.",
            "It takes fewer than a million steps for like 47 thousand said.",
            "Can you imagine how long the devil would have been if it took you guys that long to solve the problem?",
            "That would not have been fun for any of us like what kind of stupid tutorial was that?",
            "Anyway?",
            "Compare that to same same basic model but learning.",
            "Sorry the same basic representation of the state space, but it's learning the model.",
            "It's learning with a flat version of our Max, so it assumes that the environment consists of a.",
            "Like a Markov chain, MDP sort of thing and it just learns about all those states independently, but the exploration is driven by what states are known and which are unknown, and now it takes, you know it takes order of magnitude less.",
            "Steps for it to find out how to behave optimally.",
            "This environment only like 4000, so it's pretty good.",
            "And in fact, if you watch it go, it's getting.",
            "It's getting systematically moving from states that it knows about the states it doesn't know about until it finds a reward, and then it can exploit that.",
            "We also ran if if we have a factored representation of the transition probabilities then we can quick learn on that and using a factor representation of transitions leads to a much faster learning again.",
            "So now like half what it was when the state of state was flat, and here if the state space is made much much larger we make a much bigger taxi problem.",
            "With eight colored locations instead of four color locations, then this number does not really go up very much.",
            "This one goes up a lot more.",
            "'cause this is really learning about all the states independently.",
            "This is learning about the features and how they how they work.",
            "On the other hand, learning from the objects.",
            "What I just described here is much much smaller.",
            "So now we're down to just one 50 steps.",
            "You can actually watch this, learn an you won't pull your hair out.",
            "These guys.",
            "It really is very frustrating 'cause the learners just keep bashing themselves against the wall over and over and over again.",
            "But here you can see it actually.",
            "Once he figures out how to move, it goes and tries other objects that it wants to interact with in the world.",
            "That being said, we think people are like, really sorry.",
            "Scientists are about 50 steps to solve this problem.",
            "We now know that people, whatever that means is probably a much larger number than this will, let's say 143, and then our algorithm is optimal, right?",
            "I guess that's not quite true, but the fact the matter is certainly experienced video game players, one guy claimed to have just looked at the initial state, unknown.",
            "The optimal trajectory from there, 'cause you know it uses the picture, uses the vocabulary that gamers kind of have come to expect, and so this could be 0, but 50 seems about right for people who.",
            "Know how to do this sort of thing so there still is a little bit of headroom, but you know by making these kinds of representational assumptions, we can.",
            "We can learn a whole lot."
        ],
        [
            "Here's us now using that same algorithm to solve a video game.",
            "We're up to the 80s now.",
            "This is from 1984.",
            "We're hoping to get to the 90s next year.",
            "Here in this is a game called Pitfall which some of us may be familiar with.",
            "I'm extremely familiar with it.",
            "I played a lot of pitfall where this guy his name is pitfall Harry by the way and he's trying to get off the screen to the right at least at this point.",
            "And there's various kinds of objects can interact with.",
            "There's a wall, there's a lot.",
            "There's a ladder with the pit, there's the ground, there's another ground.",
            "These trees are there, but they're not really there.",
            "You can't do anything with them, but the learner doesn't know this, so the learner can see is actually taking the screen as input.",
            "So let me freeze this for a second.",
            "Yeah, that's not feeding.",
            "Oh, good, now it's come back to being So what we do is we actually take the video output that the Atari emulator gives us and parse this into colored regions.",
            "So there's the tree top canopy.",
            "There's this tree trunk, this tree trunk and we give the learner just these rectangles and we say, you know, good luck with that.",
            "Figure it out, figure out what to do to win this game, and so you can see what it's doing is.",
            "It's moving around and it's purposely trying to interact with all the different objects and when it's interacting with them, it's trying all the different actions.",
            "What happens when you go left and you're next to the wall?",
            "What happens when you jump when you're next to the wall?",
            "What happens when you go up on the ladder?",
            "What happens when you jump to the right where you're at the top of the ladder?",
            "It tries all those things out, but only basically once per object.",
            "Hey, there's a tree that you're supposed to just treat truck.",
            "You're supposed to.",
            "You lose points when you touch it, but it doesn't know that it thinks anything good could happen.",
            "So it starts rubbing up against it in various ways.",
            "But after after it's finished, whatever it's doing, it solves the task, goes back to the beginning, and now it knows how the environment works.",
            "Just like you guys.",
            "When you played the little taxi problem didn't hit A&B in every square, right?",
            "There was a couple of calls I didn't listen to.",
            "The couple calls today and be sooner.",
            "But not very many, and as soon as you reach that first colored square, it's like what do they do now?",
            "I want to know what he does right and that's what this learner was doing as well.",
            "It was particularly trying to get to where the log is and say, wow jumping what is jumping do here?",
            "Let me let me find that out so it looks more people like an.",
            "In fact it can learn.",
            "It learns this task which if you just thought about it in terms of a flat state space, would be hideously enormous.",
            "Now this was 1980 stuff, right?",
            "So that actually there it is innumerable.",
            "The number of pixel positions on the screen.",
            "But that being said, it's still very large.",
            "Alright, one thing I don't know is how to keep track of time.",
            "15 or 5 zero OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "He's done huge contributions to our field, especially to planning and reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "He's also done incredibly well in staging, where he regularly wins awards, so we're in for a good treat.",
                    "label": 0
                },
                {
                    "sent": "It certainly very funny when he teaches, quite appreciate that.",
                    "label": 0
                },
                {
                    "sent": "He's also done other things that have also led to several paper awards.",
                    "label": 0
                },
                {
                    "sent": "In particular, just learn that he's actually an expert in crosswords, and he attends the meetings of the national Whatever Crossword Association.",
                    "label": 0
                },
                {
                    "sent": "So maybe you can tell us more about it.",
                    "label": 0
                },
                {
                    "sent": "Anyway, it's a great pleasure to have Michael Littman.",
                    "label": 0
                },
                {
                    "sent": "Thanks very much, just as just as a quick check, let's see.",
                    "label": 1
                },
                {
                    "sent": "So people who are here, how many of you already know reinforcement learning and are working in the field we have?",
                    "label": 0
                },
                {
                    "sent": "I see and why am I giving you a tutorial?",
                    "label": 0
                },
                {
                    "sent": "This is like just a nice chance for us to all you know, hang out together and right so of those who are left.",
                    "label": 0
                },
                {
                    "sent": "How many were here?",
                    "label": 0
                },
                {
                    "sent": "Because there's really just nothing else to do in Vancouver right now.",
                    "label": 0
                },
                {
                    "sent": "I got three hands, big smiles, and then the rest.",
                    "label": 0
                },
                {
                    "sent": "How many people here are actually interested in learning about the background here, but aren't already aware of it in many ways.",
                    "label": 0
                },
                {
                    "sent": "I could swear that some of the same hands are going up, but alright, but that's OK, alright, I'm going to do my best iPod for people who are already doing this stuff.",
                    "label": 0
                },
                {
                    "sent": "I'm going to apologize in advance.",
                    "label": 0
                },
                {
                    "sent": "I'm lately teaching wise.",
                    "label": 0
                },
                {
                    "sent": "I've been teaching introduction introductory computer science for people who aren't going into computer science, so I think I've kind of adopted a little bit of.",
                    "label": 0
                },
                {
                    "sent": "Low level kind of presentation style and I apologize in advance if you're an expert already and I don't wanna hear that, but that's what I'm going to do and beyond that, what else do I want to tell you that if you're working this area and I didn't cite you?",
                    "label": 0
                },
                {
                    "sent": "I'm sorry, but ask a loaded question at the end where you say you know.",
                    "label": 0
                },
                {
                    "sent": "How does this relate to the work oven and I'll talk about how cool your work is.",
                    "label": 0
                },
                {
                    "sent": "Excellent, alright, so I will be talking about.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Based reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Basic, the basic flow here is to talk a little bit background wise about Markov decision processes, an reinforcement learning general because model based reinforcement learning is a special kind of reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "So let's talk.",
                    "label": 0
                },
                {
                    "sent": "We need to make sure that we're on the same page about what reinforcement learning is.",
                    "label": 0
                },
                {
                    "sent": "The the bulk of what I'm going to talk about is, is the problem of efficient exploration.",
                    "label": 0
                },
                {
                    "sent": "How is it that we can create algorithms that are going to explore their environments and be able to perform nearly optimal but with bounds on how much experience it takes to do that?",
                    "label": 0
                },
                {
                    "sent": "So that's an area that's that.",
                    "label": 0
                },
                {
                    "sent": "I've been focused on quite a bit in my own research with my students in my lab, and so that will be mostly what I talk about.",
                    "label": 0
                },
                {
                    "sent": "Lately I've become very intrigued by Bayesian reinforcement learning and its connection to efficient exploration, and I'll tell you about that, and then the bottleneck here is.",
                    "label": 1
                },
                {
                    "sent": "Learning algorithms that are very good at acquiring their own models, but then they actually have to plan with them and so there is a bottleneck there and I'll talk a little bit about what some of the current approaches are for for planning with these learned models.",
                    "label": 0
                },
                {
                    "sent": "And I'm also fascinated by the connection to human cognition, so I'll just I'll mention a little bit about that.",
                    "label": 0
                },
                {
                    "sent": "I would say more except I don't know that much more, but I'm fascinated by it.",
                    "label": 0
                },
                {
                    "sent": "So if you have information about it, I'd be happy to hear more about it.",
                    "label": 0
                },
                {
                    "sent": "I know there are some workshops going on at Whistler at the end of Nips that get it not particularly model based reinforcement learning, but various issues about how people learn and what that tells us about machine learning and vice versa.",
                    "label": 0
                },
                {
                    "sent": "So I definitely encourage interested people to get involved in that.",
                    "label": 0
                },
                {
                    "sent": "Alright, so I forgot to set.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But I think I think we could start off with the game.",
                    "label": 0
                },
                {
                    "sent": "How many people already know this games?",
                    "label": 0
                },
                {
                    "sent": "In which case that would be kind of fun fun, alright?",
                    "label": 0
                },
                {
                    "sent": "It's not that many.",
                    "label": 0
                },
                {
                    "sent": "Not that many.",
                    "label": 0
                },
                {
                    "sent": "Alright, here we go.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is a little video game actually.",
                    "label": 0
                },
                {
                    "sent": "On Saturday a student and I went to Quaker Bridge Mall in New Jersey.",
                    "label": 0
                },
                {
                    "sent": "We have malls in New Jersey and.",
                    "label": 0
                },
                {
                    "sent": "And had people actually do this, and we recorded their the results and you guys are going to do much, much better than the people in the mall did.",
                    "label": 0
                },
                {
                    "sent": "And the reason I say that is because I've given this in talks many times and I had a good guess as to how well it was going to go in the mall.",
                    "label": 0
                },
                {
                    "sent": "It didn't go as well as I was hoping it would go.",
                    "label": 0
                },
                {
                    "sent": "So here's the game.",
                    "label": 0
                },
                {
                    "sent": "This is what you see.",
                    "label": 0
                },
                {
                    "sent": "This is your input state information and your goal is to win the game and at each step you get to make a series of decisions.",
                    "label": 0
                },
                {
                    "sent": "Each step you get to choose up, down, left, right A or B and if you've seen this before, please don't say anything because you'll ruin it for the rest of us who are trying to have fun.",
                    "label": 0
                },
                {
                    "sent": "So OK, so now you have to tell me what do you want to do up, down, left, right A or B?",
                    "label": 1
                },
                {
                    "sent": "Alright, I heard today, but I'm going to do up here that was louder.",
                    "label": 0
                },
                {
                    "sent": "You may think for a moment and now you have to tell me the next thing to do, 'cause you haven't won yet, right?",
                    "label": 0
                },
                {
                    "sent": "I had right again, I hit right again.",
                    "label": 0
                },
                {
                    "sent": "App.",
                    "label": 0
                },
                {
                    "sent": "I heard up and right I'll do them both up, right.",
                    "label": 0
                },
                {
                    "sent": "Nice confident a.",
                    "label": 0
                },
                {
                    "sent": "B.",
                    "label": 0
                },
                {
                    "sent": "Be again.",
                    "label": 0
                },
                {
                    "sent": "Left a a wait.",
                    "label": 0
                },
                {
                    "sent": "Hang on.",
                    "label": 0
                },
                {
                    "sent": "There was too many things.",
                    "label": 0
                },
                {
                    "sent": "It was no I did I did a I heard be here down.",
                    "label": 0
                },
                {
                    "sent": "That's the time when you make a noise that was very good.",
                    "label": 0
                },
                {
                    "sent": "The people in the mall didn't make that noise.",
                    "label": 0
                },
                {
                    "sent": "I was.",
                    "label": 0
                },
                {
                    "sent": "I was very disappointed.",
                    "label": 0
                },
                {
                    "sent": "Down left left left.",
                    "label": 0
                },
                {
                    "sent": "OP is fine, I'm getting a probability distribution of actions back, which is very nice.",
                    "label": 0
                },
                {
                    "sent": "Left up B.",
                    "label": 0
                },
                {
                    "sent": "Hang on a second, wait, wait, wait.",
                    "label": 0
                },
                {
                    "sent": "I think I hit B, but it didn't seem that I hit be there.",
                    "label": 0
                },
                {
                    "sent": "I did hippie, you won congratulations alright so.",
                    "label": 0
                },
                {
                    "sent": "Why won't you shopping in the mall on Saturday?",
                    "label": 0
                },
                {
                    "sent": "I needed at one point I actually stopped by let the experiment go on an I ran upstairs to the video Game Store and I ask the people in the video game store if they would come down and help us and be a ringer for our experiment.",
                    "label": 0
                },
                {
                    "sent": "So what people non nips people were doing, they bumble around a little bit.",
                    "label": 0
                },
                {
                    "sent": "They eventually figured out that they were controlling the Orange Square.",
                    "label": 0
                },
                {
                    "sent": "Some of them actually managed to go and visit the little circle thing and maybe even picked it up.",
                    "label": 0
                },
                {
                    "sent": "Maybe.",
                    "label": 0
                },
                {
                    "sent": "Maybe even walked around with it.",
                    "label": 0
                },
                {
                    "sent": "But they never quite figured out where it was supposed to go.",
                    "label": 0
                },
                {
                    "sent": "Where was it supposed to go?",
                    "label": 0
                },
                {
                    "sent": "Right here red.",
                    "label": 0
                },
                {
                    "sent": "The same color, right?",
                    "label": 0
                },
                {
                    "sent": "So I don't know if the example that I did had the passenger being red or not, but in general the passenger is whatever color of the destination it wants to go too, so the yellow circle wants to go to the yellow square like that, and if you did this, if you didn't figure it out, probably if you did it twice you get it, but it turns out you're not normal.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's really interesting to see what normal is it.",
                    "label": 0
                },
                {
                    "sent": "This kind of very clever exploration is not is not the norm.",
                    "label": 0
                },
                {
                    "sent": "I think people and definitely not in the program, so I'm going to talk about either.",
                    "label": 0
                },
                {
                    "sent": "So even the people in the mall did significantly better in some ways than the learning algorithms I'm going to tell you about so.",
                    "label": 0
                },
                {
                    "sent": "So they did get stuck more.",
                    "label": 0
                },
                {
                    "sent": "Are learning algorithms, at least the ones I'm going to tell you about are very effective at actually figuring out what they haven't tried yet, and they try new things people they got into this mode where they would.",
                    "label": 0
                },
                {
                    "sent": "They would decide what it is that they wanted.",
                    "label": 0
                },
                {
                    "sent": "They decided how it was going to work and they would just stick to that.",
                    "label": 0
                },
                {
                    "sent": "Didn't matter how long it was going to take.",
                    "label": 0
                },
                {
                    "sent": "Alright, so good.",
                    "label": 0
                },
                {
                    "sent": "So what is?",
                    "label": 0
                },
                {
                    "sent": "Why do we play this game other than it was sort of a fun way to break the ice.",
                    "label": 0
                },
                {
                    "sent": "Then I would say that this is an example of a reinforcement learn.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Environment, so basically you had a task.",
                    "label": 0
                },
                {
                    "sent": "There was some kind of reward signal that you were trying to get which was finishing the game.",
                    "label": 0
                },
                {
                    "sent": "You had actions that you could choose that left down up, right AB, and possibly a sequence of actions was necessary to get high reward.",
                    "label": 0
                },
                {
                    "sent": "In this case you had to get all through that sequence of steps to win an you have some kind of state representation of some kind of information about what the current state is on which to base your decisions.",
                    "label": 0
                },
                {
                    "sent": "So there's other problems that fit into this setting as well.",
                    "label": 0
                },
                {
                    "sent": "This is an example that we played with in our lab a couple years ago where we've gotten AIBO robot and what it's trying to do is decide it's just two choices.",
                    "label": 0
                },
                {
                    "sent": "It's got A&B, which in this case correspond to turning a little bit left and turning a little bit right, and its goal in this case, instead of dropping off the passenger, was making visual contact with the pink ball, so in some ways a very simple task, but in other ways it's real right?",
                    "label": 0
                },
                {
                    "sent": "It's actually moving around in physical space and implementing these things.",
                    "label": 0
                },
                {
                    "sent": "It's not some kind of a.",
                    "label": 0
                },
                {
                    "sent": "You know theoretical data structure.",
                    "label": 0
                },
                {
                    "sent": "Well, there's theoretical data structures in the robot, but it's actually taking actions in the real world, and that's the data that is using for its decision-making, so again, this is this is another example that shows the different elements of the reinforcement learning problem.",
                    "label": 0
                },
                {
                    "sent": "Maybe not a commercially significant problem.",
                    "label": 0
                },
                {
                    "sent": "The turning around to see the pink ball problem I think, is well solved by other technology at the moment, but it has all the pieces in it, and it was.",
                    "label": 0
                },
                {
                    "sent": "Good for us to experiment with using noisy data as opposed to the standard simulated.",
                    "label": 0
                },
                {
                    "sent": "Very clean data.",
                    "label": 0
                },
                {
                    "sent": "Alright, so when one is trying trying to create a reinforcement.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "System that's going to solve one of these tasks.",
                    "label": 1
                },
                {
                    "sent": "I would say that there's three major pieces, at least in the model based perspective.",
                    "label": 0
                },
                {
                    "sent": "There's these three major pieces that have to be addressed.",
                    "label": 0
                },
                {
                    "sent": "The first one is using the knowledge that's gained from experience and applying it to new situations, generalizing it so that it actually applies to situations that the decision maker hasn't necessarily seen before.",
                    "label": 0
                },
                {
                    "sent": "And that's really the learning problem.",
                    "label": 0
                },
                {
                    "sent": "That's a core problem in our community.",
                    "label": 0
                },
                {
                    "sent": "We have lots of different technology that we can use to solve problems like this.",
                    "label": 0
                },
                {
                    "sent": "On the other side of things is, well, it's not just about learning about the environment.",
                    "label": 0
                },
                {
                    "sent": "It's about acting well in the environment, and to do that, the agent has to carry out sequential decision making.",
                    "label": 0
                },
                {
                    "sent": "It actually has to take steps in the world so when we were playing our little taxi game it's called.",
                    "label": 0
                },
                {
                    "sent": "That's called the taxi problem.",
                    "label": 0
                },
                {
                    "sent": "By the way, is introduced by Tom Dietrich number of years ago.",
                    "label": 0
                },
                {
                    "sent": "There's like 300 some odd papers.",
                    "label": 0
                },
                {
                    "sent": "I was able to find that talk about that problem.",
                    "label": 0
                },
                {
                    "sent": "'cause alright anyway, so most of the actions that you took didn't give you any kind of immediate reward.",
                    "label": 1
                },
                {
                    "sent": "You didn't win the game because you chose left at a certain point, it was only.",
                    "label": 0
                },
                {
                    "sent": "Knitting together all those actions that got you to the final goal, and so in this particular case, dealing with this delayed gratification is it's a planning problem.",
                    "label": 0
                },
                {
                    "sent": "It's sequencing actions to achieve some kind of goal, or maximizing reward or minimizing costs.",
                    "label": 1
                },
                {
                    "sent": "And that's actually fairly well studied problem as well, so we've got these two nice, fairly well studied problems.",
                    "label": 0
                },
                {
                    "sent": "The piece that kind of glue these together and gives reinforcement learning its own little problem to focus on is how do you put those two together?",
                    "label": 0
                },
                {
                    "sent": "How do you do?",
                    "label": 0
                },
                {
                    "sent": "Action choices and planning so that you get the right information out of the learning so that you can apply that and do well in the planning.",
                    "label": 0
                },
                {
                    "sent": "So solving this exploration exploitation balance is one of the problems that makes reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "I think a lot of fun to think about and a lot of fun to work on, and it's really significant.",
                    "label": 0
                },
                {
                    "sent": "A lot of these problems are difficult to solve if the agents don't figure out how to explore and gain the data that they need.",
                    "label": 0
                },
                {
                    "sent": "Alright, so the.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A formal model that underpins a lot of work in reinforcement learning is the Markov decision process here.",
                    "label": 0
                },
                {
                    "sent": "This was introduced a number of years ago, actually at Anand over lunch said people like me.",
                    "label": 0
                },
                {
                    "sent": "We've devoted our lives to Bellman and I don't think that's quite true, but the sense in which it is true is that Bellman introduced these models and a lot of what goes into solving reinforcement learning problems is about solving what's known as the Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "Is this Q equation here at the bottom.",
                    "label": 0
                },
                {
                    "sent": "So just to set it up for you, we imagine that the environment consists of some number of states.",
                    "label": 0
                },
                {
                    "sent": "Call it in.",
                    "label": 0
                },
                {
                    "sent": "In this case, some set of actions that the agent gets to choose among some discount factor which is controlling how important is it to get reward now versus reward in the future.",
                    "label": 0
                },
                {
                    "sent": "The discount factor is very close to one, then rewards matter all throughout time.",
                    "label": 0
                },
                {
                    "sent": "If the discount factor is closer to 0, then rewards now really significant, but you know who cares about later later is later on each step time step T, the agents informed of what its current state is.",
                    "label": 0
                },
                {
                    "sent": "What's the state of the environment and it gets to choose an action from that it receives a payoff are some tea.",
                    "label": 0
                },
                {
                    "sent": "And the expected value of which is determined by what's called the reward function, the reinforcement function.",
                    "label": 0
                },
                {
                    "sent": "And then there's a transition to a next date in that transition is governed by this this T function.",
                    "label": 0
                },
                {
                    "sent": "The transition function, which says that the probability that if you're in some state St and you choose some action 80, the probability will end up next in state S prime.",
                    "label": 0
                },
                {
                    "sent": "Is that expression there alright?",
                    "label": 0
                },
                {
                    "sent": "So schematically we've got an agent environment and they're having this conversation back and forth.",
                    "label": 0
                },
                {
                    "sent": "The environment says the state the agent gives the action.",
                    "label": 0
                },
                {
                    "sent": "The environment gives back the reward.",
                    "label": 0
                },
                {
                    "sent": "State and the game continues.",
                    "label": 0
                },
                {
                    "sent": "Now how do you solve a problem like this?",
                    "label": 0
                },
                {
                    "sent": "He decide what actions take.",
                    "label": 0
                },
                {
                    "sent": "Well, if you know the transition and reward function then this Bellman equation.",
                    "label": 0
                },
                {
                    "sent": "Here if I had it.",
                    "label": 0
                },
                {
                    "sent": "So I have a pointer.",
                    "label": 0
                },
                {
                    "sent": "Cool this equation.",
                    "label": 0
                },
                {
                    "sent": "Now I should probably use the pointer right?",
                    "label": 0
                },
                {
                    "sent": "So this equation right here gives us a way of determining what to actually do.",
                    "label": 0
                },
                {
                    "sent": "So what this says here is we're going to determine this.",
                    "label": 0
                },
                {
                    "sent": "This thing called the Q function.",
                    "label": 0
                },
                {
                    "sent": "the Q value of being in some state S and taking some action A is semantically what it is is, how much reward do you get as an agent get if it starts off in state S, choose action A and from then on does whatever the optimal thing is to do whatever maximizes expected reward, and the way we're going to we're going to compute that.",
                    "label": 0
                },
                {
                    "sent": "Is by saying, well, we can get that by saying, well, it's going to get some immediate reward determined by the reward function, and then it's going to make a transition to a new state and that transition the probability that that state is prime is TSS prime once it gets to that new state, it's going to choose the optimal action, which in this case, since we're pretending that we know what this Q function is is, whichever action in that state has the highest value, so.",
                    "label": 0
                },
                {
                    "sent": "Now we look over all possible next dates.",
                    "label": 0
                },
                {
                    "sent": "This is weighted by their probability.",
                    "label": 0
                },
                {
                    "sent": "We sum them up to take the expected value and we discounted by gamma to take into consideration.",
                    "label": 0
                },
                {
                    "sent": "That's one step in the future, all those rewards are one step delayed, so it turns out if we can solve this equation, we know how to behave, in particular when we're in some state SST, we just choose whichever action a maximizes QSTA.",
                    "label": 0
                },
                {
                    "sent": "So this is almost a linear system of equations.",
                    "label": 0
                },
                {
                    "sent": "It has a little nonlinearity in it, which we've named Max and nonetheless there's.",
                    "label": 0
                },
                {
                    "sent": "There's lots of techniques for actually solving these sorts of things, which is a kind of planning problem given the definition of the environment, what actions do you want to choose to maximize reward right now in the reinforcement learning setting, but basically defines the reinforcement learning setting.",
                    "label": 0
                },
                {
                    "sent": "Is that we don't know.",
                    "label": 0
                },
                {
                    "sent": "The agent doesn't know in advance what the reward and the transitions are.",
                    "label": 0
                },
                {
                    "sent": "OK, maybe it is a guess.",
                    "label": 0
                },
                {
                    "sent": "Maybe has a prior, maybe it has to set of three or four possibilities.",
                    "label": 0
                },
                {
                    "sent": "Maybe it knows absolutely nothing.",
                    "label": 0
                },
                {
                    "sent": "Regardless if it knew everything and it was certain that it was correct, this isn't the reinforcement learning problem.",
                    "label": 0
                },
                {
                    "sent": "It's only when these are unknown to some degree that some amount of experimentation is needed to learn them.",
                    "label": 0
                },
                {
                    "sent": "Apply them and then actually take actions in the world.",
                    "label": 0
                },
                {
                    "sent": "So that's all of the reinforcement learning problem in a nutshell.",
                    "label": 0
                },
                {
                    "sent": "We can think of.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problems is fitting into this mode, so the robot turning around to see the pink ball problem, we can view it as a Markov decision process consisting of a set of states actions are these arrows that transport cause transitions between the states.",
                    "label": 0
                },
                {
                    "sent": "This particular state has a reward of pink, which I guess is good in this case, plus one and otherwise it's getting 0, so it's moving around in the environment trying to maximize reward just basically rotating left or right around this circle.",
                    "label": 0
                },
                {
                    "sent": "Alright, so that's the problem.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We actually developed solutions to solve the problem.",
                    "label": 0
                },
                {
                    "sent": "What kind of algorithms can we use to decide what actions to take to maximize reward given some uncertainty about the way that the environment works?",
                    "label": 0
                },
                {
                    "sent": "So one class of algorithms that I'll say a little bit about our policy, search algorithms, policy search is trying to directly learn this mapping from state to what you should do.",
                    "label": 0
                },
                {
                    "sent": "So Pi.",
                    "label": 0
                },
                {
                    "sent": "Here is the policy.",
                    "label": 0
                },
                {
                    "sent": "It says it takes as input what's going on now takes his output.",
                    "label": 0
                },
                {
                    "sent": "What it should do, and that's that would be a good thing to learn.",
                    "label": 0
                },
                {
                    "sent": "If you could learn this, that's the whole problem.",
                    "label": 0
                },
                {
                    "sent": "Learning the policy, deciding what to do.",
                    "label": 0
                },
                {
                    "sent": "You're all set.",
                    "label": 0
                },
                {
                    "sent": "The problem is that you don't get any concrete examples of well when you're in this state.",
                    "label": 0
                },
                {
                    "sent": "You should do this action.",
                    "label": 0
                },
                {
                    "sent": "We don't have a training set of that form, so in some sense direct policy search is tricky.",
                    "label": 0
                },
                {
                    "sent": "One way to do it is to actually do basically hillclimbing guess apolosi, run it in the environment for awhile, see how much reward it gets, and see if any local changes to it might make things better.",
                    "label": 0
                },
                {
                    "sent": "It's a very general algorithm.",
                    "label": 0
                },
                {
                    "sent": "It can be applied in lots of different situations, but it can be very, very slow.",
                    "label": 0
                },
                {
                    "sent": "It's not using a lot of information from the environment to kind of slowly Hill climb up in a space of policies, so.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other class of algorithms called value function based algorithms that take a different approach there.",
                    "label": 0
                },
                {
                    "sent": "Instead of trying to learn the policy directly, they tried to learn this Q function first as I showed on the Bellman equation slide.",
                    "label": 0
                },
                {
                    "sent": "So here we want to function that given the state in action reports what the expected value of future reward is going to be V. OK, and if it if we learn this, if the agent is able to learn this, it can act by just taking whatever action maximizes the value given the current state.",
                    "label": 0
                },
                {
                    "sent": "So knowing this is sufficient for knowing how to behave.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, this slightly more direct data about given a state in action, what the future value is going to be because the agent is acting in the world, is in some state it tries some action and then it can observe what kind of reward it gets into the future.",
                    "label": 0
                },
                {
                    "sent": "It's not very direct because in the meantime, while it's collecting up summing up the rewards that it's getting from one step to the next.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's you know, taking other actions in other States and so it's not really clear necessarily that they connect directly, but there's a whole bunch of algorithms including Q learning algorithm for learning the Q function that can be applied to do this, and in fact I think the majority.",
                    "label": 0
                },
                {
                    "sent": "I haven't actually computed this.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure how to exactly compute this.",
                    "label": 0
                },
                {
                    "sent": "If anybody works on that stuff, let me know, but I think the majority of work in the literature in reinforcement learning is about value function based approaches, so they are the most popular.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What I'm talking about in this tutorial or model based approaches model based approaches are one yet one more step backed away from the direct application of the policy.",
                    "label": 0
                },
                {
                    "sent": "So what model based approaches do is they try to learn this mapping.",
                    "label": 0
                },
                {
                    "sent": "They try to learn what what's the transition function that takes state and action returns next date or probability distribution over next date an what's the reward function that takes state in action an returns immediate reward value that can be directly observed, not the long term value which it has to wait to see.",
                    "label": 0
                },
                {
                    "sent": "The implications of his actions first, so this is actually something that can be directly learned.",
                    "label": 0
                },
                {
                    "sent": "This is a supervised learning problem.",
                    "label": 0
                },
                {
                    "sent": "It gets examples of hey, if I wanted an example of what happens if I choose some action, some state, I just have to get to that state, try that action and see what happens.",
                    "label": 0
                },
                {
                    "sent": "So then to use.",
                    "label": 0
                },
                {
                    "sent": "This is a little bit more indirect.",
                    "label": 0
                },
                {
                    "sent": "So to go from the learned transition reward functions to the Q function and therefore to the policy you have to solve the Bellman equations.",
                    "label": 0
                },
                {
                    "sent": "So this can actually be extraordinarily expensive depending on the size of the state space and other constraints that you've got.",
                    "label": 0
                },
                {
                    "sent": "So really, there's a kind of a tradeoff happening here where we've got on this side.",
                    "label": 0
                },
                {
                    "sent": "We're learning something that's very directly usable, doesn't require a lot of computation to use what's being learned here, but learning it is actually very indirect, and you don't get direct examples of you should choose this action in this state.",
                    "label": 0
                },
                {
                    "sent": "On this side we have a quantity that is very direct to learn, but then it's computationally challenging to actually use that to make decisions in the environment, but nonetheless I think because for me anyway, the experiments we've been doing in our lab.",
                    "label": 0
                },
                {
                    "sent": "Data was really expensive when we run these little robot examples they don't have tons and tons of data.",
                    "label": 0
                },
                {
                    "sent": "They can't run a million trials like you can do in a simulator.",
                    "label": 0
                },
                {
                    "sent": "So it's really important to us to make the most use out of the data that we got.",
                    "label": 0
                },
                {
                    "sent": "These are often more data efficient as well.",
                    "label": 0
                },
                {
                    "sent": "They each data sample that they get can directly be used to estimate something.",
                    "label": 0
                },
                {
                    "sent": "So so that was why our lab has been focused on these model based approaches and that's what I was sort of equipped to tell you about.",
                    "label": 0
                },
                {
                    "sent": "So that's what the tutorials about.",
                    "label": 0
                },
                {
                    "sent": "Alright, any I guess any?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Up to this point, we good so far.",
                    "label": 0
                },
                {
                    "sent": "We didn't want to see this animation again, I apologize.",
                    "label": 0
                },
                {
                    "sent": "And OK alright, one more animation moving on so so so how do you do model based learning?",
                    "label": 0
                },
                {
                    "sent": "So here's kind of a schematic approach to understanding what the different pieces are and how they relate back to the way that I described.",
                    "label": 0
                },
                {
                    "sent": "The main problems in reinforcement learning earlier.",
                    "label": 0
                },
                {
                    "sent": "So this is the same figure from a couple times ago, but I expanded this environment block and definitely agent block and we're going to peer inside the agent and see what's going on here.",
                    "label": 0
                },
                {
                    "sent": "So if you were thinking about model based reinforcement learning algorithm really, what's going on is there's a learning piece that's actually learning the transition and reward functions.",
                    "label": 0
                },
                {
                    "sent": "It's getting training examples by acting in the world.",
                    "label": 1
                },
                {
                    "sent": "It gets state and reward samples.",
                    "label": 0
                },
                {
                    "sent": "It gets next date reward samples just by waiting a single step and its actions are actually coming from solving this model using this model to decide what the best action is in the in its guess about the environment.",
                    "label": 0
                },
                {
                    "sent": "It produces these actions which go out to the world, but they also come around, and they used as training examples as well.",
                    "label": 0
                },
                {
                    "sent": "So here's our learning piece.",
                    "label": 0
                },
                {
                    "sent": "Here's our planning piece and deciding what to do.",
                    "label": 0
                },
                {
                    "sent": "And then there's this other little exploration piece that may modify, right?",
                    "label": 0
                },
                {
                    "sent": "So when you're learning this model, if you just have a guess, your learning algorithm just gives you its best guess that may need to be modified, someone you don't want to just act according to your best guess.",
                    "label": 0
                },
                {
                    "sent": "Because if you're wrong, you might not be able to discover high value rewards someplace else in the environment so often.",
                    "label": 0
                },
                {
                    "sent": "People are finding practice that is really necessary to add some kind of exploration component to what they're doing.",
                    "label": 0
                },
                {
                    "sent": "Some kind of thing that will cause the system to learn about actions that it might not otherwise have taken, given its best guess about the transitions and rewards.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, so this is now the beginning of that next block.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to talk about some of the formal and experimental work we've done on trying to.",
                    "label": 0
                },
                {
                    "sent": "Show that we can learn in some of these environments depending on various properties of the transitions.",
                    "label": 0
                },
                {
                    "sent": "We can learn efficiently.",
                    "label": 0
                },
                {
                    "sent": "We can learn with the polynomial, polynomial number of samples, so to do that we have to define pretty closely what we think the problem is.",
                    "label": 0
                },
                {
                    "sent": "So we're taking at the moment the reinforcement learning problem to be as follows.",
                    "label": 0
                },
                {
                    "sent": "We're going we're learning in a kind of a pack setting, probably approximately correct, extended to reinforcement learning by a bunch of researchers that says as follows our algorithm.",
                    "label": 1
                },
                {
                    "sent": "Is given epsilon and Delta as inputs.",
                    "label": 0
                },
                {
                    "sent": "It knows in the world that there are K actions and States and it knows the discount factor, but it doesn't know the transition and reward functions yet and we're going to say it now begins acting in the world begins taking actions, learning about state transitions and gathering rewards, and we're going to say that each time in the environment, each time step T where the Q value, it doesn't know the real Q function, but according to the real Q function, if the action that it took in that state.",
                    "label": 0
                },
                {
                    "sent": "Is more than epsilon away from the best.",
                    "label": 0
                },
                {
                    "sent": "the Q value of the best action in that state we're going to call that a mistake now.",
                    "label": 0
                },
                {
                    "sent": "Nothing in the environment is telling the agent.",
                    "label": 1
                },
                {
                    "sent": "Oh, you know, that was a mistake.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really know that, but we as the ones evaluating how the algorithm is performing, we could imagine that we're we're applying that rule to it.",
                    "label": 0
                },
                {
                    "sent": "OK, so so an algorithm is going out there.",
                    "label": 0
                },
                {
                    "sent": "It's going to act, however it's going to act and we can keep track of the number of mistakes that it makes, and we're going to say that an algorithm is a good algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's an efficient algorithm if there's some M, that's a bound on the number of mistakes that it makes.",
                    "label": 1
                },
                {
                    "sent": "That bound holds with some probability, 1 minus Delta, and that bound itself is relatively small polynomial in the number of States and actions.",
                    "label": 0
                },
                {
                    "sent": "How close you want to do it this epsilon parameter?",
                    "label": 0
                },
                {
                    "sent": "How sure you want to be this Delta parameter and these sort of the effective horizon time?",
                    "label": 0
                },
                {
                    "sent": "How much does future reward count?",
                    "label": 0
                },
                {
                    "sent": "As measured by 1 /, 1 minus gamma, so we're going to say that an algorithm is a good algorithm if it's the number of mistakes it makes his small an infinite run.",
                    "label": 1
                },
                {
                    "sent": "OK, but you know we're letting it cheat a little bit by by having the epsilon.",
                    "label": 0
                },
                {
                    "sent": "Delta doesn't have to always work and by work it doesn't have to be perfect, it just has to be close to perfect.",
                    "label": 0
                },
                {
                    "sent": "Alright, the only way to actually solve this problem is by balancing exploration and exploitation to some extent.",
                    "label": 0
                },
                {
                    "sent": "An algorithm can't just go around taking the action that it thinks is best, because it could be that there's some other high scoring action out there that it needs to know more about that would be pure exploitation.",
                    "label": 0
                },
                {
                    "sent": "Pure exploration doesn't work either.",
                    "label": 0
                },
                {
                    "sent": "That means just kind of running around trying things all the time.",
                    "label": 0
                },
                {
                    "sent": "And that's going to lead to lots and lots of mistakes, so it has to make it has to explore just enough.",
                    "label": 0
                },
                {
                    "sent": "And it has to exploit the rest of the time, right?",
                    "label": 0
                },
                {
                    "sent": "And so we're going to show what I'm going to talk about is that there is a number of algorithms that actually can achieve these kinds of bounds, and even do it in the face of various kinds of generalization.",
                    "label": 0
                },
                {
                    "sent": "If you know something about the function class from which transition probability matrices are constructed.",
                    "label": 0
                },
                {
                    "sent": "So to give you a sense of how this work.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How, how might this work?",
                    "label": 0
                },
                {
                    "sent": "So let's imagine that we've got an agent that's in this really simple chain environment, sometimes called the combination lock environment, but starts off over here, and it has two choices.",
                    "label": 0
                },
                {
                    "sent": "It can take the black actions which move it gradually along the chain, and eventually to this state.",
                    "label": 0
                },
                {
                    "sent": "That has really high reward.",
                    "label": 0
                },
                {
                    "sent": "It can sit in here and gather 20s forever, or each step along the way.",
                    "label": 0
                },
                {
                    "sent": "It has a choice it can take.",
                    "label": 0
                },
                {
                    "sent": "This blue dashed action, which has an immediate reward, not a very large one, but it's bigger than zero, which resets it back to the beginning of this chain.",
                    "label": 0
                },
                {
                    "sent": "So OK, so if we know this problem, we know that the well depending on the discount factor, probably the optimal thing to do is to March down the chain and then gather 20s until the end of time.",
                    "label": 0
                },
                {
                    "sent": "But a learning algorithm as it's exploring this domain.",
                    "label": 0
                },
                {
                    "sent": "It might be in a situation where you know it's explored these States and it knows what they do, but it's never tried the black action in this last statement.",
                    "label": 0
                },
                {
                    "sent": "Camp so well now what?",
                    "label": 0
                },
                {
                    "sent": "So this is this is the model that's been learned so far.",
                    "label": 0
                },
                {
                    "sent": "It knows that the black action moves it to the right, and except here it doesn't know, and the dashed action moves it back in the model based setting and we're going to use this model to plan and actually decide what to do in the environment, which actions to take.",
                    "label": 0
                },
                {
                    "sent": "But the only way we can plan in this environment is if we make some sort of assumptions about what happens in parts of the model that we don't know about.",
                    "label": 0
                },
                {
                    "sent": "So maybe we just assume this black action goes to some other random state that we've already seen.",
                    "label": 0
                },
                {
                    "sent": "Maybe we assume that it immediately gives some high reward or some low reward.",
                    "label": 0
                },
                {
                    "sent": "There's lots of different things that an algorithm can assume.",
                    "label": 0
                },
                {
                    "sent": "Maybe we'll just leave it up to the learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's going to make its best guess, you know.",
                    "label": 0
                },
                {
                    "sent": "Maybe the learning algorithm is going to generalize.",
                    "label": 0
                },
                {
                    "sent": "It says black actions always take me to something new, so maybe there's something new there.",
                    "label": 0
                },
                {
                    "sent": "Regardless, whatever the learning algorithm does, it's going to have to make some kind of guess here.",
                    "label": 0
                },
                {
                    "sent": "Now what's going to?",
                    "label": 0
                },
                {
                    "sent": "What's going to happen in terms of this pack?",
                    "label": 0
                },
                {
                    "sent": "MDP assumption if the truth is that this?",
                    "label": 0
                },
                {
                    "sent": "Going to the right here can lead to only low reward.",
                    "label": 0
                },
                {
                    "sent": "Bad reward and the algorithm assumes that out here is just bad reward.",
                    "label": 0
                },
                {
                    "sent": "Well, that's good, we could just ignore what's going on in the question Mark zone and take a little policy that just goes like this.",
                    "label": 0
                },
                {
                    "sent": "Gather your point ones over and over and over again, and that would actually be optimal.",
                    "label": 0
                },
                {
                    "sent": "If the reward out here was low.",
                    "label": 0
                },
                {
                    "sent": "Also, if the Furthermore, if the if the reward out there is low, but we assume that it's high, what will happen in that case?",
                    "label": 0
                },
                {
                    "sent": "Well, in that case the agent is imagining, well, there's something really great out here, and if it starts out over here at the end of the chain, it might say, wow, there's something really great out there.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to go and get that really great.",
                    "label": 0
                },
                {
                    "sent": "Well, it may be great.",
                    "label": 0
                },
                {
                    "sent": "It may not be great, but the point is it's going to visit and find out it's going to actually explore by checking out this part of the environment.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if the truth is that there's very high reward, either these are fine.",
                    "label": 0
                },
                {
                    "sent": "By the way, if the truth is that there's really high reward out there, but the algorithm assumes that it's low.",
                    "label": 0
                },
                {
                    "sent": "It's not going to go there.",
                    "label": 0
                },
                {
                    "sent": "It's not going to find out all the wondrous rich is that it missed out on an.",
                    "label": 0
                },
                {
                    "sent": "It's actually going to perform suboptimally, possibly worse than epsilon.",
                    "label": 0
                },
                {
                    "sent": "Suboptimally, an algorithm that does that can't be pack MVP.",
                    "label": 0
                },
                {
                    "sent": "By this assumption, it hasn't explored enough.",
                    "label": 0
                },
                {
                    "sent": "It hasn't checked out these high reward areas, and so that's a failure.",
                    "label": 0
                },
                {
                    "sent": "That red box here?",
                    "label": 0
                },
                {
                    "sent": "That's a failure.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, if it's out, there is really high reward.",
                    "label": 0
                },
                {
                    "sent": "It's going to visit.",
                    "label": 0
                },
                {
                    "sent": "It's going to find out its optimal all is good.",
                    "label": 0
                },
                {
                    "sent": "So these two cases are fine.",
                    "label": 0
                },
                {
                    "sent": "This case is fine eventually there's exploration, but this is bad if we want to achieve pack MVP, we have to avoid this case.",
                    "label": 0
                },
                {
                    "sent": "We can't ever assume.",
                    "label": 0
                },
                {
                    "sent": "That something is low and it really is high.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, which leads to a very natural idea.",
                    "label": 0
                },
                {
                    "sent": "So the idea of model driven exploration is to say this.",
                    "label": 0
                },
                {
                    "sent": "We're going to be learning the reward and transition function, so we have some cartoon, some approximated version of the reward and transition functions and what we're going to do.",
                    "label": 0
                },
                {
                    "sent": "We're going to learn those, and then we're going to augment the model with a bonus for unknown transitions.",
                    "label": 1
                },
                {
                    "sent": "Anything out there that we don't understand yet?",
                    "label": 0
                },
                {
                    "sent": "The algorithm doesn't know well.",
                    "label": 0
                },
                {
                    "sent": "It's going to assume is really great.",
                    "label": 0
                },
                {
                    "sent": "Then it's going to use that augmented model to plan to decide what to actually do, and then it's going to repeat by actually following.",
                    "label": 0
                },
                {
                    "sent": "This planet may learn new things.",
                    "label": 0
                },
                {
                    "sent": "It may get to states it hasn't been to before.",
                    "label": 0
                },
                {
                    "sent": "Try actions that hasn't seen before and that will cause it to learn new things which will change the model which may change the behavior.",
                    "label": 0
                },
                {
                    "sent": "But what's interesting is that what I'll say is that this basic scheme can actually lead to pack MDP.",
                    "label": 0
                },
                {
                    "sent": "Algorithms algorithms that with just a polynomial number of mistakes get epsilon optimal reward and very simple idea.",
                    "label": 0
                },
                {
                    "sent": "The key, though, is that the learner actually has to know what it knows.",
                    "label": 0
                },
                {
                    "sent": "It has to be able to distinguish between transitions that it's modeling correctly, and ones that it just isn't sure about enough yet and maybe need some more data.",
                    "label": 0
                },
                {
                    "sent": "So we formalize this learning problem.",
                    "label": 1
                },
                {
                    "sent": "This knows what it learns, knows what it knows.",
                    "label": 1
                },
                {
                    "sent": "Learning problem, we call it quick learning, and that's kind of the useful learning setting for plugging into these autonomous reinforcement learning algorithms when they're learning about the environment.",
                    "label": 0
                },
                {
                    "sent": "They if they want to explore effectively, they're going to distinguish from the known the unknown from the known transitions.",
                    "label": 0
                },
                {
                    "sent": "Actually, I don't know that they need to do that, but this is one way that we know works.",
                    "label": 0
                },
                {
                    "sent": "It's sufficient to plug in a quick learner.",
                    "label": 0
                },
                {
                    "sent": "It may not be necessary, so just to give you a sense of how this works.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's three models of of.",
                    "label": 0
                },
                {
                    "sent": "Of the supervised learning problem, right?",
                    "label": 0
                },
                {
                    "sent": "So now we're taking the learning of the transition function or the transition rewards.",
                    "label": 0
                },
                {
                    "sent": "Let me focus on the transition function as a supervised problem.",
                    "label": 0
                },
                {
                    "sent": "It's going outta learners going out of the world.",
                    "label": 0
                },
                {
                    "sent": "It's trying things out.",
                    "label": 0
                },
                {
                    "sent": "It's gathering data about when I'm in this state.",
                    "label": 0
                },
                {
                    "sent": "I try this action.",
                    "label": 0
                },
                {
                    "sent": "Here's what I see next, and it's just collect these as examples in a training set.",
                    "label": 0
                },
                {
                    "sent": "And now what should it do well?",
                    "label": 0
                },
                {
                    "sent": "There's there's two well known settings that pack setting, and the mistake bound setting.",
                    "label": 0
                },
                {
                    "sent": "And here's the quick settings so I can distinguish them.",
                    "label": 0
                },
                {
                    "sent": "I can point out what the differences are between them, so the pack setting, kind of which is more or less the classical setting, which people often do.",
                    "label": 0
                },
                {
                    "sent": "Machine learning says this.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do is imagine that all our training examples, all the things we're trying to learn.",
                    "label": 0
                },
                {
                    "sent": "They're all drawn from some distribution IID, so independent samples from some distribution we're going to take a bunch of labeled examples.",
                    "label": 0
                },
                {
                    "sent": "Which are these blue bars here and after we have enough of them, then we run our learning algorithm and.",
                    "label": 0
                },
                {
                    "sent": "Error, you know, struggles away and it figures out how things work and then after that for something to be successful in the pack model, it has to make all correct predictions.",
                    "label": 0
                },
                {
                    "sent": "After that any new sample that comes in it needs to.",
                    "label": 0
                },
                {
                    "sent": "Predicted correctly, OK, most of them with high probability, that sort of thing.",
                    "label": 0
                },
                {
                    "sent": "So we've got a bunch of samples that are labeled and then we get a bunch of samples in which we have to be correct.",
                    "label": 0
                },
                {
                    "sent": "That's fine, but it turns out that this is very problematic to apply this kind of learning in a reinforcement learning setting, and the reason for that is the samples.",
                    "label": 0
                },
                {
                    "sent": "Each of these training examples is coming from the agent interacting with the environment, and to say that's IID is like saying that well, during the training the agent has to act a particular way so that it's getting samples from the distribution of the states that it's visiting and after it's learned it needs to continue to act that way so that it can get testing examples from that same distribution.",
                    "label": 0
                },
                {
                    "sent": "So imagine I don't know when I'm feeling whimsical.",
                    "label": 0
                },
                {
                    "sent": "I imagine that, you know, here's a learner and it's out in the world exploring and it's learning all about the environment.",
                    "label": 0
                },
                {
                    "sent": "And it finally gets it in and understand how the environment works and now it spends the rest of its life kind of trapped in its own body, being forced to act the way it did when it was still learning, but knowing the right thing to do and not being able to do it.",
                    "label": 0
                },
                {
                    "sent": "So this isn't, it's just not a good fit.",
                    "label": 0
                },
                {
                    "sent": "The fact that you really depend on this IID assumption really makes it problematic to apply learning algorithms that are great in the pack setting.",
                    "label": 0
                },
                {
                    "sent": "In this reinforcement learning setting with exploration.",
                    "label": 0
                },
                {
                    "sent": "So, but that's OK. People have come up with other learning models.",
                    "label": 0
                },
                {
                    "sent": "The mistake bound model is nice in that it assumes inputs are presented adversarially, right?",
                    "label": 0
                },
                {
                    "sent": "That's some something out there is can produce any training example at anytime.",
                    "label": 0
                },
                {
                    "sent": "Now the model is a little bit different.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "The model says that each time an example comes in, the learner has to provide the label and it could be wrong.",
                    "label": 0
                },
                {
                    "sent": "And then I'll cover those bars red so it gets it gets an instance.",
                    "label": 0
                },
                {
                    "sent": "It tries to predict the label.",
                    "label": 0
                },
                {
                    "sent": "It gets it wrong.",
                    "label": 0
                },
                {
                    "sent": "OK, it moves on.",
                    "label": 0
                },
                {
                    "sent": "It gets another instance, maybe correct, it gets it correct.",
                    "label": 0
                },
                {
                    "sent": "That's a green bar.",
                    "label": 0
                },
                {
                    "sent": "Another correct one.",
                    "label": 0
                },
                {
                    "sent": "Then it gets another one.",
                    "label": 0
                },
                {
                    "sent": "It's wrong.",
                    "label": 0
                },
                {
                    "sent": "It can continue to make mistakes at any arbitrary time, but the total number of mistakes has to be bounded.",
                    "label": 0
                },
                {
                    "sent": "That's the mistake bound part.",
                    "label": 0
                },
                {
                    "sent": "OK, so here we've got.",
                    "label": 0
                },
                {
                    "sent": "This is a learner, that is, it can be given any kind of examples from any kind of distribution, but it can only make a finite number of mistakes, unbounded number of mistakes for it to be a successful mistake bound learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now you can only really well actually this is true pack as well.",
                    "label": 0
                },
                {
                    "sent": "You can only really successfully make learning algorithms under restriction restricted assumptions about what the hypothesis classes, what is the function that's actually labeling these instances, but nonetheless there's.",
                    "label": 0
                },
                {
                    "sent": "There's a wide variety of algorithms that are sorry function classes that are learnable in the mistake bound model so.",
                    "label": 0
                },
                {
                    "sent": "This is getting us much closer to something that we can use in a reinforcement learning setting.",
                    "label": 0
                },
                {
                    "sent": "The problem is from the previous slide, if it's making mistakes and it doesn't know when it's making mistakes, it doesn't know when something's a guess.",
                    "label": 0
                },
                {
                    "sent": "It's just guessing, and if it's wrong, it counts against it.",
                    "label": 0
                },
                {
                    "sent": "It won't necessarily explore efficiently, so in particular, if it's imagining, oh, you know what's behind the screen, I'm going to assume that there's nothing valuable behind the screen that might be a mistake, and the mistake model says that's OK, it's just a mistake.",
                    "label": 0
                },
                {
                    "sent": "But in the MDP setting in the reinforcement learning setting.",
                    "label": 0
                },
                {
                    "sent": "That mistake could actually cause it to never get near optimal reward, because maybe the optimal thing is to just go behind the screen and probably nothing there.",
                    "label": 0
                },
                {
                    "sent": "To one second, yeah, surely this is really nothing there as far as you know.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this mistake bound setting where where it doesn't really know when it could be making mistakes and not it can actually achieve this pack.",
                    "label": 0
                },
                {
                    "sent": "MDP kind of kind of bound.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "So what do we need instead?",
                    "label": 0
                },
                {
                    "sent": "Well, this the quick setting says we're going to do things a little bit differently now.",
                    "label": 0
                },
                {
                    "sent": "The learning algorithm, if it's given an instance that it can't predict the label, it knows that it can't accurately predict the label 'cause there's multiple possibilities that could work.",
                    "label": 0
                },
                {
                    "sent": "It has to say I don't know.",
                    "label": 1
                },
                {
                    "sent": "Shrug its shoulders and so we can start adversarially.",
                    "label": 0
                },
                {
                    "sent": "Selected input is coming in.",
                    "label": 0
                },
                {
                    "sent": "It can say I don't know, and then maybe another instance comes in.",
                    "label": 1
                },
                {
                    "sent": "It's like oh wait, this one I get and it gives.",
                    "label": 0
                },
                {
                    "sent": "It has to give the right answer.",
                    "label": 0
                },
                {
                    "sent": "I don't know right answer right answer, right answer.",
                    "label": 0
                },
                {
                    "sent": "I don't know like the mistake Bell model.",
                    "label": 0
                },
                {
                    "sent": "It could continue saying I don't know to any arbitrary time in the future.",
                    "label": 0
                },
                {
                    "sent": "But the total number of times that it's going to say I don't know has to be bounded by something small.",
                    "label": 0
                },
                {
                    "sent": "If we're going to say that it's a successful quick learning algorithm, this can be used in a reinforcement learning setting and give us those those exploration pack MDP kinds of bounds, because what it can do is assume any transition that it can't predict.",
                    "label": 0
                },
                {
                    "sent": "Maybe do something good.",
                    "label": 0
                },
                {
                    "sent": "Anything that can predict it's predicting accurately, and therefore it won't be making mistakes on those.",
                    "label": 0
                },
                {
                    "sent": "So the combination of some kind of successful quick learner and this model based setting gets you reinforcement learning algorithms with these nice guarantees.",
                    "label": 0
                },
                {
                    "sent": "So that's that's the basic story there.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "There's there's a nice proof that this works in general when it's combined with an algorithm that's often referred to as our Max because it assumes that anything that's unknown is that has maximum reward reward Max.",
                    "label": 0
                },
                {
                    "sent": "It's not the most brilliant name, but anyway it does work and this this this class of algorithms has been used for learning in these flat MPs where there's M States and each state could be completely different from every other state, but it's still going to learn about them.",
                    "label": 0
                },
                {
                    "sent": "It's going to decide what to do in those States and there's a number of key ideas.",
                    "label": 0
                },
                {
                    "sent": "We know that the optimal action for an approximate model is near optimal in the real model, which means if we know that the model we've learned in the environment might not be perfect, but it's epsilon close in all its transitions, then taking the.",
                    "label": 0
                },
                {
                    "sent": "Optimal actions with respect to that guess about the environment.",
                    "label": 1
                },
                {
                    "sent": "This close guess is going to lead to near optimal rewards in the environment.",
                    "label": 0
                },
                {
                    "sent": "It's not like there's these weird discontinuity's where if you just get one little thing a little bit wrong, you're arbitrarily far away from optimal, so that's the simulation lemma says we can simulate the environment with an approximate model and the Explore exploit lemma says basically this is a cute idea.",
                    "label": 1
                },
                {
                    "sent": "If there's parts of the state space that we can't reach 'cause it's very low probability to be able to get to one of these states, then even if they have high reward, we don't have to go there.",
                    "label": 0
                },
                {
                    "sent": "We can get near optimal roared.",
                    "label": 0
                },
                {
                    "sent": "Just staying among the states that we can get to with high probability and that means that when we take actions within the known states, if we can't reach any unknown state, well either it's OK.",
                    "label": 0
                },
                {
                    "sent": "This is a better way to say it, either it's the case that our algorithm can explore it can with high probability get to some state that it hasn't seen yet, and learn about it.",
                    "label": 0
                },
                {
                    "sent": "Or if it can't do that, it must be able to exploit.",
                    "label": 0
                },
                {
                    "sent": "Which means among the states that had already understands, well, it can achieve high reward for the real model.",
                    "label": 1
                },
                {
                    "sent": "OK, so these two pieces together give us these nice efficient learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "In the in the flat case, there's a number of papers that talked about this, but some people have looked at.",
                    "label": 0
                },
                {
                    "sent": "I didn't know what to call it.",
                    "label": 0
                },
                {
                    "sent": "UN flat in cases where there's generalizations that can be made between states.",
                    "label": 0
                },
                {
                    "sent": "Like for example, if the dynamics are determined by a dynamic Bayes net, some kind of factored model, or it's there's a metric space, it's continuous, there's a continuous state space and there's some relationship between nearby States and the other states that are nearby or in what we showed in.",
                    "label": 0
                },
                {
                    "sent": "Generally any function class that can be quick learned we get these same kind of nice ideas that come into play and we can get near optimal reward.",
                    "label": 0
                },
                {
                    "sent": "And in particular, what we showed with this is Leon Lee, who's my grandson who is a postdoc now and looking for a good job.",
                    "label": 0
                },
                {
                    "sent": "Very bright young man.",
                    "label": 0
                },
                {
                    "sent": "And So what he was able to show in his thesis is that as you know, any quick learner that we plug into this framework gives us the pack MD bounds, the time to learn the environment depends on the time to learn the transition function.",
                    "label": 1
                },
                {
                    "sent": "So if it's the kind of transition function that you can learn with very few examples very effectively, then learning to explore in the state space is actually going to be efficient as well.",
                    "label": 0
                },
                {
                    "sent": "So so you might ask.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hope if you're paying attention you would ask OK, but are there things that we can quick learn so all we showed so far is if you can quickly under function class actually show it well, I've stated to so far.",
                    "label": 0
                },
                {
                    "sent": "If you can quick learn a function class then you can behave well.",
                    "label": 0
                },
                {
                    "sent": "You can behave efficiently in reinforcement learning sense in environment where the transitions have that function, but what are some things we can quick learn well?",
                    "label": 0
                },
                {
                    "sent": "The easiest, simplest an actually the basis of almost all the interesting stochastic algorithms is to learn a probability value.",
                    "label": 0
                },
                {
                    "sent": "So basically coin flip.",
                    "label": 0
                },
                {
                    "sent": "You have a weighted coin.",
                    "label": 0
                },
                {
                    "sent": "You don't know initially the probability that that's going to come up heads or tails.",
                    "label": 0
                },
                {
                    "sent": "That's the thing you have to learn.",
                    "label": 0
                },
                {
                    "sent": "But given M trials and we've observed X successes X times, the coin came up heads.",
                    "label": 1
                },
                {
                    "sent": "We can estimate the probability is X / M and there's very nice bound very standard bounds, tufting, bounds, turnoff bound type things that say how many times you have to flip a coin before this estimate.",
                    "label": 0
                },
                {
                    "sent": "This empirical estimate is really close with high probability to the true value, so this is this you can think of this as a quick learning algorithm in that somebody gives you a coin and you have to predict what's the probability it comes up heads.",
                    "label": 0
                },
                {
                    "sent": "And you have to be right?",
                    "label": 0
                },
                {
                    "sent": "Or say I don't know, but only say I don't know a small number of times we can quickly learn this probability by just saying I don't know until we've flipped it enough times that this bound kicks in and we can be epsilon accurate with high probability.",
                    "label": 0
                },
                {
                    "sent": "So quick learner learning a coin is no big deal.",
                    "label": 0
                },
                {
                    "sent": "People know how to do this.",
                    "label": 0
                },
                {
                    "sent": "This is a standard building block.",
                    "label": 0
                },
                {
                    "sent": "But there's other things in addition to the coin probability that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Making quick learn so we can anything that we can quick learn.",
                    "label": 0
                },
                {
                    "sent": "We can learn.",
                    "label": 0
                },
                {
                    "sent": "The hypothesis class that says that the output is the output of some quick learner, right?",
                    "label": 0
                },
                {
                    "sent": "So let's.",
                    "label": 0
                },
                {
                    "sent": "Well, in this particular case, you could think of it as being the dice learning problem.",
                    "label": 0
                },
                {
                    "sent": "So imagine that we're going to try to estimate got instead of a weighted coin.",
                    "label": 0
                },
                {
                    "sent": "We have a way to die.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to roll it, and it comes up 12345 or six with unknown probabilities.",
                    "label": 0
                },
                {
                    "sent": "Well, we can treat that as being 6 separate coins and what we're doing is on each trial we're predicting the output of each of those coins simultaneously, so we can quick learning to the coin probabilities and what this is saying is that if we have a vector of something that we can quick learn, then we can quick learn that whole vector so we can do dice learning.",
                    "label": 0
                },
                {
                    "sent": "There's actually better ways of doing it than thinking of it as a bunch of coins, but that's one concrete example here.",
                    "label": 0
                },
                {
                    "sent": "Another one is if we're learning a mapping from some input space to some output space, and the input is partitioned into regions where each of those is quick learnable, then we can quickly, in the whole space, and that's a really easy result as well.",
                    "label": 0
                },
                {
                    "sent": "It just says that we run a separate quick learner for each of the partitions of the input, and then when an input comes in, we just ask the corresponding quick learner for the answer.",
                    "label": 0
                },
                {
                    "sent": "So that's this seems sort of.",
                    "label": 0
                },
                {
                    "sent": "Two simple to be worth mentioning, but combine this and that and the coin probability and you can learn a standard transition function in an MDP, 'cause what is it as transition function it's mapping the state, the choice of state and the choice of action that bust up the input space into into end times.",
                    "label": 1
                },
                {
                    "sent": "K separate problems to learn in each of those you have to learn a vector of probabilities, which are the state transition probabilities to each possible next state and each of those values itself is just a Bernoulli trial.",
                    "label": 0
                },
                {
                    "sent": "So by combining these at three levels.",
                    "label": 0
                },
                {
                    "sent": "We can actually quick learn an entire transition function and therefore.",
                    "label": 0
                },
                {
                    "sent": "Quick we can we can solve reinforcement learning problems efficiently in this in this flat setting where each state action pair is considered to be a separate problem.",
                    "label": 0
                },
                {
                    "sent": "So there's a handicap.",
                    "label": 0
                },
                {
                    "sent": "Yeah, go ahead.",
                    "label": 0
                },
                {
                    "sent": "Or other animals be back in the case where I flat state space, but I don't know it's cardinality by discovering personality effectively if it works right now when you say cardinality, that's like the partitions of the input space that was calling partitions.",
                    "label": 0
                },
                {
                    "sent": "Here we don't know the number of states.",
                    "label": 0
                },
                {
                    "sent": "So I know maybe definition when I'm asking, OK?",
                    "label": 0
                },
                {
                    "sent": "There is.",
                    "label": 0
                },
                {
                    "sent": "I see I see and we want to bound that grows with that unknown number of states say, yeah, so these these algorithms do that as well.",
                    "label": 0
                },
                {
                    "sent": "They tend not to actually need to know necessarily the number of states.",
                    "label": 0
                },
                {
                    "sent": "But they behave well with respect to what the number of states actually is.",
                    "label": 0
                },
                {
                    "sent": "The other question is, well here.",
                    "label": 1
                },
                {
                    "sent": "I'm talking about a known input partition.",
                    "label": 0
                },
                {
                    "sent": "What if it's unknown?",
                    "label": 0
                },
                {
                    "sent": "What if a bunch of States Act act like each other, but we don't know which ones and we have to discover which ones connect with friends will get to that, but this is kind of the lowest level.",
                    "label": 0
                },
                {
                    "sent": "Basic is just a regular MVP.",
                    "label": 0
                },
                {
                    "sent": "The way Bellman talked about them, we can now efficiently learn in that setting how to behave with unknown, initially unknown transition reward functions.",
                    "label": 0
                },
                {
                    "sent": "Alright, so yes.",
                    "label": 0
                },
                {
                    "sent": "I was wanted by it so interesting to have this Pattern Bowl.",
                    "label": 0
                },
                {
                    "sent": "Property because it is polynomial in the state space.",
                    "label": 0
                },
                {
                    "sent": "So it seems like the most naive things which is visible states.",
                    "label": 0
                },
                {
                    "sent": "So if it were.",
                    "label": 0
                },
                {
                    "sent": "OK, so hang on.",
                    "label": 0
                },
                {
                    "sent": "So if if we have a deterministic system where you can actually choose, you know when you take an action it always goes to the same next state exploring that graph and getting these same kinds of bounds is easy.",
                    "label": 0
                },
                {
                    "sent": "You just just basically like depth first search.",
                    "label": 0
                },
                {
                    "sent": "Almost you just go to any transition you haven't visited yet.",
                    "label": 0
                },
                {
                    "sent": "You try it, which the reason that this isn't just completely trivial is in the MVP setting.",
                    "label": 0
                },
                {
                    "sent": "There's probabilities involved.",
                    "label": 0
                },
                {
                    "sent": "It may be hard to get yourself to a state to learn about it, so you can't really just reason about it saying OK, well, just I can quickly visit all the States and learn about all those states.",
                    "label": 0
                },
                {
                    "sent": "Because you can't force yourself, the agent can't put itself in a state necessarily.",
                    "label": 0
                },
                {
                    "sent": "So you need you need a more flexible notion of what optimality means.",
                    "label": 0
                },
                {
                    "sent": "That can account for the case where maybe there's a state that you can't get to.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you can only get to it with really low probability.",
                    "label": 0
                },
                {
                    "sent": "You don't count that against the learner.",
                    "label": 0
                },
                {
                    "sent": "That answer the question.",
                    "label": 0
                },
                {
                    "sent": "So motional.",
                    "label": 0
                },
                {
                    "sent": "Every city comes in where it actually visit visibility.",
                    "label": 0
                },
                {
                    "sent": "Visit visibility to any states, becomes a property, and then we could have systems where we actually becomes an exponentially small probability to visitor parking space.",
                    "label": 0
                },
                {
                    "sent": "Or how is that alright?",
                    "label": 0
                },
                {
                    "sent": "So the question is what happens if, say, there are parts of the state space that are very very difficult to learn?",
                    "label": 0
                },
                {
                    "sent": "It's not a sort of rapidly mixing Markov chain kind of situation, and that's OK.",
                    "label": 0
                },
                {
                    "sent": "These results will still apply.",
                    "label": 0
                },
                {
                    "sent": "They tell us that for the stuff that we do get too often enough we're getting.",
                    "label": 0
                },
                {
                    "sent": "Getting high reward if things are very hard to get to, there may be very high reward there, but it can't have a big impact on our expected value.",
                    "label": 0
                },
                {
                    "sent": "'cause it's very hard to get there.",
                    "label": 0
                },
                {
                    "sent": "So the the those are the sorts of issues that make the analysis tricky, but the But the top level story things are actually sort of nice and clean as long as we can quick learn the transition function, no matter how horrible the state space is, we get high reward with high probability with only a polynomial number of mistakes.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are other studies.",
                    "label": 0
                },
                {
                    "sent": "I particularly mentioned.",
                    "label": 0
                },
                {
                    "sent": "These quick, learnable classes because it gets us up to transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "There are other sorts of things the union of two quick, learnable classes, quick, learnable.",
                    "label": 1
                },
                {
                    "sent": "I'll mention a couple others as we go.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so just in case there were too many epsilons and deltas for a minute, here's here's a video of a robot.",
                    "label": 0
                },
                {
                    "sent": "Again, this is an AIBO robot in our lab.",
                    "label": 0
                },
                {
                    "sent": "It's trapped in a 4 sided box, but there's a door on one side.",
                    "label": 0
                },
                {
                    "sent": "It has to escape through the door.",
                    "label": 0
                },
                {
                    "sent": "The door is over.",
                    "label": 0
                },
                {
                    "sent": "Here you can see it.",
                    "label": 0
                },
                {
                    "sent": "It's got there's the door.",
                    "label": 0
                },
                {
                    "sent": "It's got forward, backward, left, right slide left and slide right.",
                    "label": 1
                },
                {
                    "sent": "You know, like step without turning to left and right, which turn out to be important to not get stuck with your shoulder on the door.",
                    "label": 1
                },
                {
                    "sent": "It's if we describe discretize the state space into 4050 states.",
                    "label": 0
                },
                {
                    "sent": "Number of positions and orientations, and then give one of these model based learners.",
                    "label": 0
                },
                {
                    "sent": "This are Max where it's quick learning the transition probabilities.",
                    "label": 0
                },
                {
                    "sent": "To the system to learn it can actually learn this fairly effectively in the dog, can escape from the box.",
                    "label": 0
                },
                {
                    "sent": "All is well.",
                    "label": 0
                },
                {
                    "sent": "In this case the dog is being tracked.",
                    "label": 0
                },
                {
                    "sent": "You can see it's got a little green little Raspberry Beret here.",
                    "label": 0
                },
                {
                    "sent": "Anna Green, Fanny Pack and those are being tracked from above mounted camera, so that's how it knows where it is.",
                    "label": 0
                },
                {
                    "sent": "It can, it can work.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But I think a natural thing to say at this point is, well, if it's growing with the size of the state space.",
                    "label": 0
                },
                {
                    "sent": "If you have to learn about all these states in the environment, most problems that we're thinking about and that we care bout have a very large number of states.",
                    "label": 0
                },
                {
                    "sent": "So if we really view each of the states is completely independent.",
                    "label": 0
                },
                {
                    "sent": "This transition knowledge doesn't transfer.",
                    "label": 1
                },
                {
                    "sent": "Learning is going to be really slow.",
                    "label": 0
                },
                {
                    "sent": "These algorithms can actually even the really smart ones can do really stupid things under this assumption where they actually walk around in the environment and like touch everything right there, like they bump themselves into everything because they don't know.",
                    "label": 0
                },
                {
                    "sent": "Maybe that's the magic way out of the boxes to do this crazy thing so.",
                    "label": 0
                },
                {
                    "sent": "So we need to take advantage of the fact that the world has some regularity.",
                    "label": 0
                },
                {
                    "sent": "Therefore the transition functions can have some regularity.",
                    "label": 0
                },
                {
                    "sent": "So if the quick learners have some regularity that can exploit that, the whole thing can go a lot faster.",
                    "label": 0
                },
                {
                    "sent": "So here's, I'll just I'll show you one concrete example of this idea.",
                    "label": 0
                },
                {
                    "sent": "Alright, so we've got a.",
                    "label": 0
                },
                {
                    "sent": "Same kind of environment as before.",
                    "label": 0
                },
                {
                    "sent": "There's a robot that's in an environment.",
                    "label": 0
                },
                {
                    "sent": "It's got an X&Y coordinate.",
                    "label": 0
                },
                {
                    "sent": "It also has an orientation, but we're going to add one more thing to it, which is there's a bumble ball in the environment and obstacle, moving obstacle, stationary obstacles are whatever they are, but moving obstacles are interesting because they become part of the state space, so the right thing to do at a given point in time may depend not only where the robot is, but where the obstacle is in relation to the robot.",
                    "label": 0
                },
                {
                    "sent": "So now we've got.",
                    "label": 0
                },
                {
                    "sent": "Well, potentially very high dimensional, even if we discretize the X&Y the ball, the X&Y, the robot and the state of the robot.",
                    "label": 0
                },
                {
                    "sent": "We're talking about a lot of States and the robots going to learn really, really slowly.",
                    "label": 0
                },
                {
                    "sent": "It's very often going to encounter states it's never seen before it's going to be exploring for a very long time, so here's what we're going to do.",
                    "label": 0
                },
                {
                    "sent": "We're going to do our own domain knowledge to say, hey, you know what?",
                    "label": 0
                },
                {
                    "sent": "There's some structure in this environment we're going to build a little graphical model DBN that says.",
                    "label": 0
                },
                {
                    "sent": "As the when we when the agent, the robot chooses the forward action, the change in the balls X&Y coordinates is independent.",
                    "label": 0
                },
                {
                    "sent": "It doesn't depend on anything about where the ball or the robot are, just moves around randomly.",
                    "label": 0
                },
                {
                    "sent": "Amazingly, this ball really does that.",
                    "label": 0
                },
                {
                    "sent": "I'll show you the video in a second.",
                    "label": 0
                },
                {
                    "sent": "I thought it would have all kinds of history.",
                    "label": 0
                },
                {
                    "sent": "It's like no, it's it's pretty much like IID movement like Brownian motion.",
                    "label": 0
                },
                {
                    "sent": "So the robots X&Y under the forward action, the change in the X&Y doesn't change, is only affected by the angle that the robots facing, so right, so if it's facing this way and it goes forward, then X&Y changes certain way according to trigonometry, right?",
                    "label": 0
                },
                {
                    "sent": "If the robots facing this way then why change a different way when it moves forward, but the robots angle actually is, again, independent of everything else it's changing angle.",
                    "label": 0
                },
                {
                    "sent": "So that's for the forward action.",
                    "label": 0
                },
                {
                    "sent": "We just write down this model, but we don't have to fill in the numbers here.",
                    "label": 0
                },
                {
                    "sent": "We just say what depends on what moving when.",
                    "label": 0
                },
                {
                    "sent": "It takes a right hand turn, actually.",
                    "label": 0
                },
                {
                    "sent": "They're almost all independent.",
                    "label": 0
                },
                {
                    "sent": "I think that's the model that we use that that the X&Y don't depend on the previous X&Y, 'cause it's really not moving in X&Y, just turning the robot X&Y.",
                    "label": 0
                },
                {
                    "sent": "Oh sorry, the ball on the robot and the angle actually does change, but it doesn't matter what the current angle was, it's going to change relative to whichever way it's facing, so now we have a model where there's lots of things that are independent.",
                    "label": 1
                },
                {
                    "sent": "There's a lot less to learn.",
                    "label": 0
                },
                {
                    "sent": "There's way fewer parameters.",
                    "label": 0
                },
                {
                    "sent": "And what's nice is that we can.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, I'll show you the video and then I'll talk about quick learning.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's sorry that's not a robot, but that is the Bumble.",
                    "label": 0
                },
                {
                    "sent": "Here's a bumble ball, just brutal Mumbles around like that and this is a robot again wearing patterns on the on its back so we can detect it from the above camera.",
                    "label": 0
                },
                {
                    "sent": "We're also detecting the ball which is a distinctive color and is trying to get to this goal location at the end.",
                    "label": 0
                },
                {
                    "sent": "The reward function is it gets high reward for getting to the goal it gets.",
                    "label": 0
                },
                {
                    "sent": "Big negative reward.",
                    "label": 0
                },
                {
                    "sent": "Every time it bumps into the ball and it gets small negative rewards just for not being at the goal yet.",
                    "label": 0
                },
                {
                    "sent": "So just kind of encourage it to get on with his life.",
                    "label": 0
                },
                {
                    "sent": "Alright, so.",
                    "label": 0
                },
                {
                    "sent": "We tried very sort of function approximation with with with Q learning that didn't didn't go very well.",
                    "label": 0
                },
                {
                    "sent": "So remember trying to get down here and this is after after 45 episodes.",
                    "label": 0
                },
                {
                    "sent": "Now this is now 13 episodes using the model based algorithm which is actually learning those transition probabilities with the conditional independences from quick learning them from experience and so you can see it's already doing a very good job of pointing in the right direction, the ball the ball is out of the way at this particular case.",
                    "label": 0
                },
                {
                    "sent": "It's heading fairly directly towards this corner.",
                    "label": 0
                },
                {
                    "sent": "And if the ball is close to it, you can see it.",
                    "label": 0
                },
                {
                    "sent": "It's a little bit more careful trying to get by the ball.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it looks scared, but I don't think it's feeling frightened so its value function does drop when the ball gets close.",
                    "label": 0
                },
                {
                    "sent": "So it depends on how you define frightened, right?",
                    "label": 0
                },
                {
                    "sent": "So now here's the robot is trying to get down here to the corner.",
                    "label": 0
                },
                {
                    "sent": "Here's our bumble ball.",
                    "label": 0
                },
                {
                    "sent": "It's unfortunately placed in its way, but that's OK.",
                    "label": 0
                },
                {
                    "sent": "The robot kind of waits it out since it's moving randomly.",
                    "label": 0
                },
                {
                    "sent": "It's just wait long.",
                    "label": 0
                },
                {
                    "sent": "And Oh no, that's not helping.",
                    "label": 0
                },
                {
                    "sent": "Sometimes it looks less random than others.",
                    "label": 0
                },
                {
                    "sent": "To the ball is kind of attacking.",
                    "label": 0
                },
                {
                    "sent": "Oh good, we got an opening.",
                    "label": 0
                },
                {
                    "sent": "We got an opening right?",
                    "label": 0
                },
                {
                    "sent": "We're going to head right to the goal now here we go just just gotta kind of facing the right direction and start trucking.",
                    "label": 0
                },
                {
                    "sent": "And unfortunately the ball has other plans.",
                    "label": 0
                },
                {
                    "sent": "Alright so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't know what computational models of frustration look like.",
                    "label": 0
                },
                {
                    "sent": "I guess if we ever saw the robot actually go up to the ball and just that would count as basically giving up, but it did.",
                    "label": 0
                },
                {
                    "sent": "It's patiently waiting for the ball to be in a position where it can actually get to where it needs to go.",
                    "label": 0
                },
                {
                    "sent": "Just get over there alright?",
                    "label": 0
                },
                {
                    "sent": "Alright so it made it good so good.",
                    "label": 0
                },
                {
                    "sent": "So in this example the what's going on here is because we know the conditional independence is what really it's.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And instead of learning the whole transition probability matrix that says from this combination of ball position, robot position robot angle, here's what the next one looks like.",
                    "label": 0
                },
                {
                    "sent": "Instead, it's just learning little ones that say when you do the forward action, what's the probability distribution over the change in the balls position, robots position and robots data all independently and so lot fewer things to learn.",
                    "label": 0
                },
                {
                    "sent": "These are all independently quick learnable.",
                    "label": 0
                },
                {
                    "sent": "This this idea I think I had it on the previous slide, but this this observation that if you know the conditional independence structure, you can learn the transition matrices really efficiently, even though there's an exponential number of combinations of values of the state variables, we don't need anywhere near that.",
                    "label": 0
                },
                {
                    "sent": "We need polynomial in the size of these little mini transition state diagrams, so that observation was made by Kerns in color and so they showed how you could do this, and so I'm not saying anything new beyond what they said in this particular case.",
                    "label": 0
                },
                {
                    "sent": "So we did discover that for the robot examples it was nice to have it be the change in variable instead of predicting from the old value of the variable the new value of the variable.",
                    "label": 0
                },
                {
                    "sent": "So sometimes sometimes that's the thing that you want to be predicting.",
                    "label": 0
                },
                {
                    "sent": "They would just kind of obvious.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's that's a different kind of representation of the transition function that is efficiently learnable and applies really well in lots of different circumstances.",
                    "label": 0
                },
                {
                    "sent": "Here's another one.",
                    "label": 0
                },
                {
                    "sent": "This is one that we found to be useful in several settings, and we're continuing to work on it where we represent transitions instead of from states to states or from features to features like in the DBN case.",
                    "label": 0
                },
                {
                    "sent": "We're going to represent them in terms of object attributes to object attributes, so talking about the world as consisting of a set of objects.",
                    "label": 0
                },
                {
                    "sent": "So this is this, is a figure from a paper, which is the taxi problem that I showed you in the beginning.",
                    "label": 0
                },
                {
                    "sent": "It's very convenient, and in fact I think if you ask yourself what you were doing when you were exploring in this game to talk about the state in terms of a set of objects, there's a taxi.",
                    "label": 0
                },
                {
                    "sent": "It's in someplace there's a passenger.",
                    "label": 0
                },
                {
                    "sent": "It's in someplace, there's walls there in some place there's a destination.",
                    "label": 0
                },
                {
                    "sent": "It's also in some place, and whether two things are in the same place is very relevant to figuring out what's going to happen when you choose an action.",
                    "label": 0
                },
                {
                    "sent": "So so now, instead of yeah, So what happens when objects interact?",
                    "label": 1
                },
                {
                    "sent": "That's what the learner is going to try to figure out, and when we run these sorts of things, we get much more human like exploration instead of.",
                    "label": 0
                },
                {
                    "sent": "The algorithm you know bumping up against all the walls and all combinations.",
                    "label": 0
                },
                {
                    "sent": "Once it figures out how navigation works, it zips over to like the Green Square, right?",
                    "label": 0
                },
                {
                    "sent": "It doesn't try all actions everywhere on the way there.",
                    "label": 0
                },
                {
                    "sent": "It actually goes to wear something new might happen.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And when we actually haven't learned in the taxi problem, the sorts of elements of the transition function that it picks up are very readable.",
                    "label": 0
                },
                {
                    "sent": "Natural kinds of things.",
                    "label": 0
                },
                {
                    "sent": "It learns that if the current action is North and it's not the case that the taxi is touching to the North wall, then the Y coordinate of the taxi is incremented.",
                    "label": 0
                },
                {
                    "sent": "OK, this is in fact what the code looks like, but this is what the learner figures out as well.",
                    "label": 0
                },
                {
                    "sent": "If you do the drop-off action and the passengers in the taxi and the taxi is touching the destination, then the passenger will no longer be in the taxi, it gets dropped off.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are the sort of various little little bits and pieces that it picks up and it's able to solve this problem much more efficiently.",
                    "label": 0
                },
                {
                    "sent": "So in fact if you look at and we have a quick bound for how long it takes to do this.",
                    "label": 0
                },
                {
                    "sent": "It's polynomial in the number of types in the world exponential in the number of conditions.",
                    "label": 0
                },
                {
                    "sent": "Like drop this and this and this all have to be true simultaneously.",
                    "label": 0
                },
                {
                    "sent": "It doesn't search through that conjunction space very efficiently.",
                    "label": 0
                },
                {
                    "sent": "It's exponential in the length of the conditions, but it's polynomial in the number different types in the world and independent of, say, the number of states in the world because most of the states are just combinations of objects in different positions.",
                    "label": 0
                },
                {
                    "sent": "Alright, so when we run this kind of a learning algorithm on the taxi problem, we see.",
                    "label": 0
                },
                {
                    "sent": "Well, here's compared to a bunch of other algorithms Q.",
                    "label": 0
                },
                {
                    "sent": "Learning this just kind of classical model free value function based reinforcement learner.",
                    "label": 0
                },
                {
                    "sent": "Where exploration is done epsilon greedy, it tries random actions every once in awhile and it's you know it's good for Q learning.",
                    "label": 0
                },
                {
                    "sent": "It takes fewer than a million steps for like 47 thousand said.",
                    "label": 0
                },
                {
                    "sent": "Can you imagine how long the devil would have been if it took you guys that long to solve the problem?",
                    "label": 0
                },
                {
                    "sent": "That would not have been fun for any of us like what kind of stupid tutorial was that?",
                    "label": 0
                },
                {
                    "sent": "Anyway?",
                    "label": 0
                },
                {
                    "sent": "Compare that to same same basic model but learning.",
                    "label": 0
                },
                {
                    "sent": "Sorry the same basic representation of the state space, but it's learning the model.",
                    "label": 0
                },
                {
                    "sent": "It's learning with a flat version of our Max, so it assumes that the environment consists of a.",
                    "label": 0
                },
                {
                    "sent": "Like a Markov chain, MDP sort of thing and it just learns about all those states independently, but the exploration is driven by what states are known and which are unknown, and now it takes, you know it takes order of magnitude less.",
                    "label": 0
                },
                {
                    "sent": "Steps for it to find out how to behave optimally.",
                    "label": 0
                },
                {
                    "sent": "This environment only like 4000, so it's pretty good.",
                    "label": 0
                },
                {
                    "sent": "And in fact, if you watch it go, it's getting.",
                    "label": 0
                },
                {
                    "sent": "It's getting systematically moving from states that it knows about the states it doesn't know about until it finds a reward, and then it can exploit that.",
                    "label": 0
                },
                {
                    "sent": "We also ran if if we have a factored representation of the transition probabilities then we can quick learn on that and using a factor representation of transitions leads to a much faster learning again.",
                    "label": 0
                },
                {
                    "sent": "So now like half what it was when the state of state was flat, and here if the state space is made much much larger we make a much bigger taxi problem.",
                    "label": 0
                },
                {
                    "sent": "With eight colored locations instead of four color locations, then this number does not really go up very much.",
                    "label": 0
                },
                {
                    "sent": "This one goes up a lot more.",
                    "label": 0
                },
                {
                    "sent": "'cause this is really learning about all the states independently.",
                    "label": 0
                },
                {
                    "sent": "This is learning about the features and how they how they work.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, learning from the objects.",
                    "label": 0
                },
                {
                    "sent": "What I just described here is much much smaller.",
                    "label": 0
                },
                {
                    "sent": "So now we're down to just one 50 steps.",
                    "label": 0
                },
                {
                    "sent": "You can actually watch this, learn an you won't pull your hair out.",
                    "label": 0
                },
                {
                    "sent": "These guys.",
                    "label": 0
                },
                {
                    "sent": "It really is very frustrating 'cause the learners just keep bashing themselves against the wall over and over and over again.",
                    "label": 0
                },
                {
                    "sent": "But here you can see it actually.",
                    "label": 0
                },
                {
                    "sent": "Once he figures out how to move, it goes and tries other objects that it wants to interact with in the world.",
                    "label": 0
                },
                {
                    "sent": "That being said, we think people are like, really sorry.",
                    "label": 0
                },
                {
                    "sent": "Scientists are about 50 steps to solve this problem.",
                    "label": 0
                },
                {
                    "sent": "We now know that people, whatever that means is probably a much larger number than this will, let's say 143, and then our algorithm is optimal, right?",
                    "label": 0
                },
                {
                    "sent": "I guess that's not quite true, but the fact the matter is certainly experienced video game players, one guy claimed to have just looked at the initial state, unknown.",
                    "label": 0
                },
                {
                    "sent": "The optimal trajectory from there, 'cause you know it uses the picture, uses the vocabulary that gamers kind of have come to expect, and so this could be 0, but 50 seems about right for people who.",
                    "label": 0
                },
                {
                    "sent": "Know how to do this sort of thing so there still is a little bit of headroom, but you know by making these kinds of representational assumptions, we can.",
                    "label": 0
                },
                {
                    "sent": "We can learn a whole lot.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's us now using that same algorithm to solve a video game.",
                    "label": 0
                },
                {
                    "sent": "We're up to the 80s now.",
                    "label": 0
                },
                {
                    "sent": "This is from 1984.",
                    "label": 0
                },
                {
                    "sent": "We're hoping to get to the 90s next year.",
                    "label": 0
                },
                {
                    "sent": "Here in this is a game called Pitfall which some of us may be familiar with.",
                    "label": 0
                },
                {
                    "sent": "I'm extremely familiar with it.",
                    "label": 0
                },
                {
                    "sent": "I played a lot of pitfall where this guy his name is pitfall Harry by the way and he's trying to get off the screen to the right at least at this point.",
                    "label": 0
                },
                {
                    "sent": "And there's various kinds of objects can interact with.",
                    "label": 0
                },
                {
                    "sent": "There's a wall, there's a lot.",
                    "label": 0
                },
                {
                    "sent": "There's a ladder with the pit, there's the ground, there's another ground.",
                    "label": 0
                },
                {
                    "sent": "These trees are there, but they're not really there.",
                    "label": 0
                },
                {
                    "sent": "You can't do anything with them, but the learner doesn't know this, so the learner can see is actually taking the screen as input.",
                    "label": 0
                },
                {
                    "sent": "So let me freeze this for a second.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's not feeding.",
                    "label": 0
                },
                {
                    "sent": "Oh, good, now it's come back to being So what we do is we actually take the video output that the Atari emulator gives us and parse this into colored regions.",
                    "label": 0
                },
                {
                    "sent": "So there's the tree top canopy.",
                    "label": 0
                },
                {
                    "sent": "There's this tree trunk, this tree trunk and we give the learner just these rectangles and we say, you know, good luck with that.",
                    "label": 0
                },
                {
                    "sent": "Figure it out, figure out what to do to win this game, and so you can see what it's doing is.",
                    "label": 0
                },
                {
                    "sent": "It's moving around and it's purposely trying to interact with all the different objects and when it's interacting with them, it's trying all the different actions.",
                    "label": 0
                },
                {
                    "sent": "What happens when you go left and you're next to the wall?",
                    "label": 0
                },
                {
                    "sent": "What happens when you jump when you're next to the wall?",
                    "label": 0
                },
                {
                    "sent": "What happens when you go up on the ladder?",
                    "label": 0
                },
                {
                    "sent": "What happens when you jump to the right where you're at the top of the ladder?",
                    "label": 0
                },
                {
                    "sent": "It tries all those things out, but only basically once per object.",
                    "label": 0
                },
                {
                    "sent": "Hey, there's a tree that you're supposed to just treat truck.",
                    "label": 0
                },
                {
                    "sent": "You're supposed to.",
                    "label": 0
                },
                {
                    "sent": "You lose points when you touch it, but it doesn't know that it thinks anything good could happen.",
                    "label": 0
                },
                {
                    "sent": "So it starts rubbing up against it in various ways.",
                    "label": 0
                },
                {
                    "sent": "But after after it's finished, whatever it's doing, it solves the task, goes back to the beginning, and now it knows how the environment works.",
                    "label": 0
                },
                {
                    "sent": "Just like you guys.",
                    "label": 0
                },
                {
                    "sent": "When you played the little taxi problem didn't hit A&B in every square, right?",
                    "label": 0
                },
                {
                    "sent": "There was a couple of calls I didn't listen to.",
                    "label": 0
                },
                {
                    "sent": "The couple calls today and be sooner.",
                    "label": 0
                },
                {
                    "sent": "But not very many, and as soon as you reach that first colored square, it's like what do they do now?",
                    "label": 0
                },
                {
                    "sent": "I want to know what he does right and that's what this learner was doing as well.",
                    "label": 0
                },
                {
                    "sent": "It was particularly trying to get to where the log is and say, wow jumping what is jumping do here?",
                    "label": 0
                },
                {
                    "sent": "Let me let me find that out so it looks more people like an.",
                    "label": 0
                },
                {
                    "sent": "In fact it can learn.",
                    "label": 0
                },
                {
                    "sent": "It learns this task which if you just thought about it in terms of a flat state space, would be hideously enormous.",
                    "label": 0
                },
                {
                    "sent": "Now this was 1980 stuff, right?",
                    "label": 0
                },
                {
                    "sent": "So that actually there it is innumerable.",
                    "label": 0
                },
                {
                    "sent": "The number of pixel positions on the screen.",
                    "label": 0
                },
                {
                    "sent": "But that being said, it's still very large.",
                    "label": 0
                },
                {
                    "sent": "Alright, one thing I don't know is how to keep track of time.",
                    "label": 0
                },
                {
                    "sent": "15 or 5 zero OK.",
                    "label": 0
                }
            ]
        }
    }
}