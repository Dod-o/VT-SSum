{
    "id": "tpu7hqy4lglar67aoclmsy4vaepanpnw",
    "title": "Kernel Topic Models",
    "info": {
        "author": [
            "Thore Graepel, Microsoft Research, Cambridge, Microsoft Research"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Graphical Models",
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops2012_graepel_models/",
    "segmentation": [
        [
            "Thank you very much for the invitation to talk here.",
            "I would like to emphasize that what I'm presenting is really mostly the work of Phillip Penick and who's organizing another workshop, and so he asked me to jump in and present this, but he did most of the work.",
            "So for everything that you like in this presentation, please think of Phillip.",
            "And if you have criticism then please."
        ],
        [
            "At view me as responsible.",
            "This is based on a paper called Kernel Topic models that was published in AI starts a couple of years ago, so I'll give a quick introduction motivation to the problem and then describe what the kernel topic model is in terms of graphical model terms, and then I'll focus on a particular problem in inference that arises in this type of model, namely bridging from an unconstrained space.",
            "In this case from a Gaussian process.",
            "Into a probability space for directly distributions for which we've developed a message passing solution that connects these two.",
            "And that's really the central theme, and then I have some I think might be entertaining experiment."
        ],
        [
            "Results here"
        ],
        [
            "So I'm sure you're all familiar with topic models.",
            "I'll just go through this for the sake of notation and so that you can.",
            "You can just revisit this briefly, so this is a sketch of a standard LDA type topic model.",
            "A document is modeled here as a bag of words.",
            "For each document D, we draw probability vector Pi D over topics from idyllically distribution for each topic.",
            "Here we draw probability vector theater K over words from from a directly, again with parameters Peter K and then inside.",
            "Here we have for each word that there is an indicator variable that we draw from from the distribution Pi here, which indicates.",
            "Which topic that word comes from and then once we know that we draw from the appropriate topic distribution, the word that's the basic standard topic model and that will be one."
        ],
        [
            "Art of of our model.",
            "So these models are useful to organize text corpora to get an overview of what's in there.",
            "Also to create features, but the classical latent directly allocation disregards context information of text, right?",
            "For example, what we typically know about the text is that there's an author.",
            "There's a time and place when the document was created.",
            "This might be part of the book Journal Proceedings page numbers, all of that information people to which the document might have been addressed.",
            "Or web links and sensations, all of which create this structure around the document, which surely show."
        ],
        [
            "Be useful for the task of topic modeling.",
            "So just a few examples here.",
            "Here's an email I received a few months ago about this very talk and you can see here.",
            "Yes, there's information from whom it is too humid is who was see seed.",
            "What is this subject?",
            "We know when it was sent, and so on.",
            "All of this information is available, and that's really the rule rather than the exception, right?",
            "I mean, there's hardly ever any piece of document where we don't know anything about it.",
            "Another example, of course, is just citation data.",
            "Individual citation data already contains a lot of information, but then of course, if you view the network of citations or links between documents or social network among authors and so on, you see that there's all of this structure that we would like to use."
        ],
        [
            "For topic models, now just to talk a little bit about related work, there's of course the classical latent directly allocation paper by Blei ET al.",
            "And some people have started addressing this problem that there is actually this context information available.",
            "For example, there are dynamic topic models which take into account the structure in time that documents may have.",
            "There's a correlated topic model which allows topics to be correlated and that.",
            "Idea we will have some ideas in the model here from this.",
            "There's topic models conditioned on arbitrary features, which is probably coming closest to the type of work we're looking at here by MEM.",
            "Know Ann McCallum, and there's also relational topic models, so there this space has been looked upon, and."
        ],
        [
            "We hope to contribute to this general."
        ],
        [
            "Area, so let's look at topic models from the matrix factorization point of view.",
            "These are rather informal sketches just to get the idea what the structure of the problem is.",
            "So what we want to model is the document word matrix.",
            "Because we were assuming that the we have a bag of words representation of the data and now you can roughly view a topic model as matrix factorization where we factor this matrix of documents and words into a matrix of documents.",
            "And topics times a matrix of topics, times, words right.",
            "So this is basically the topic distribution and this is the distribution for each topic over words.",
            "And so just to illustrate."
        ],
        [
            "In which direction this is going here is the conditional topic model where you can condition on features.",
            "So in addition, here you assume that you have a feature vector 5 for each document, and then you can think of of the factorization of the document word matrix as first having a document feature matrix times a feature times topic matrix.",
            "And here we are in this linear world.",
            "So we need some kind of softmax operator.",
            "To get into the world of probabilities and then we have a probability distribution of a topics and then over words so we can view this as some kind of generalized."
        ],
        [
            "Matrix factorization and just to show you how this new model fits in.",
            "It's basically very similar to the previous one, but then we'll have a nonlinear component here driven by a Gaussian process.",
            "This function H will be drawn from a Gaussian process and so you can also view it as some form of matrix factorization for the document word matrix."
        ],
        [
            "So let's get back to the graphical models view.",
            "We had discussed this standard topic model representation and now let me show you how this changes because this part of the model will basically be retained and."
        ],
        [
            "Will tack on a Gaussian process regression model to the left.",
            "So if you just forget about this left part, that's the standard topic model that we had.",
            "But now we're basically saying that there's this vector Y which just gets squashed by a softmax into this into this vector of topic proportions, and we do a regression on the document features.",
            "But we do this in a non parametric way.",
            "Now where we draw this function H. Or it's like a vector of functions, one for each topic from a Gaussian process prior.",
            "And it's basically you can view this as a Gaussian process which has as its likelihood a topic model that's kind of the kind of compositionality that we're looking at here."
        ],
        [
            "So the.",
            "What will this?",
            "I briefly mentioned this, what will this Gaussian process be over?",
            "Well, it will be over.",
            "Did we just?",
            "This is looking slightly yellowish.",
            "I don't know, it's not on this end.",
            "Well, maybe if I don't know what the system is trying to tell us here.",
            "Yeah, can you sure?",
            "I don't know.",
            "I think we're OK, right?",
            "So the kernel over documents could be based on this information that I talked about.",
            "The matter information Author, Time, location and publication, but of course that could also be handled by normal features.",
            "Maybe more interesting Lee, we can model information like web link structure, citation structure or social network structure, which would lend itself to a kernel based approach because of the graph structures that are involved here.",
            "And those would then drive the covariance."
        ],
        [
            "Matrix of the Gaussian process.",
            "So where does the difficulty arise?",
            "And this is where the Laplace Bridge comes in, which is this kind of element that I would like to do."
        ],
        [
            "Scribe here in a little more detail, so the problem really is how do we get from the world of unconstrained Y variables into the world of probabilities?",
            "Pi D. Here in a message passing framework so that the messages generated by the data here in the topic model can pass into our regression model essentially as observations to drive the inference here.",
            "And of course, how the prior the Gaussian process prior here can be sent in terms of messages.",
            "Over to the directly distribution for the topic model and so this is actually a slightly more general problem that that might occur in a number of other."
        ],
        [
            "As well.",
            "So we call this Laplace Bridge and of course as the name suggests, it's based on the Laplace approximation.",
            "I don't know if many of you are familiar with this.",
            "This goes back to earlier work by Mackay on neural networks in the 90s.",
            "The ideas of the Laplace approximation is that you want to approximate the posterior distribution in your in your Bayesian inference, and the idea is to find the mode of the posterior distribution, then fit the Gaussian distribution based on that mode.",
            "And the curvature of the posterior at that point.",
            "And in the context of Mckay's work that was then used to evaluate approximate posterior integrals, because once you have that convenient Gaussian form of the posterior, which of course is unimodal and neglects or the multi modality that might have been there in the posterior, then you can do all of these integrals, expectations and so on.",
            "Now Mackay in 1998.",
            "Discuss is that the Laplace approximation is really basis dependent, so for example, traditionally if you had it directly distribution, you would probably do this approximation in probability space right?",
            "Which is a mismatch because you know Gaussians extend over the real line, whereas probabilities are bounded.",
            "That's actually not a very good idea, and so he suggests changing the basis to a softmax basis, and the idea of our work here is that we use the Laplace approximation in the softmax basis.",
            "To transform the probability messages into those unconstrained Gaussian messages and back and thereby enable message passing to work on for inference."
        ],
        [
            "In these models.",
            "So let's look at this in a formal way.",
            "This just the declared distribution without normalization.",
            "I tried to save that so it's a distribution over probability vectors as you know, parameterized by these pseudocounts Alpha.",
            "And it basically takes this functional form of the multinomial distribution because it's the conjugate to the multinomial distribution and but of course the parameters here the distribution is over the pies, and then of course we know it really is defined only on the probability simplex, and hence we have this Delta function that restricts it to this lower dimensional N -- 1 dimensional subspace now.",
            "What Mackay suggests is transformed to an you softmax basis.",
            "So basically represent the probabilities here in terms of the softmax function of these new variables Y and that then if you put that in leads to new parameterized form of the directly distribution that you see here.",
            "And the nice thing, for example, is the Alpha.",
            "I now don't has this annoying minus one anymore, so it's actually a more benign formulation of the directly distribution and we also have to choose a measure G here, which we will later let converge to the Delta peak, which again enforces the fact that if we have a probability distribution over CHI over K possible outcomes, then we really do the normalization of the measure.",
            "I only have a K -- 1 dimensional subspace, so this is basically what make I came up with."
        ],
        [
            "And then the question that we're asking is how can we now basically create a correspondence between the declared distribution and a multivariate Gaussian?",
            "We're looking for a one to one correspondence so that we can send the messages forth and back in both directions, and if we carry through with the Laplace approximation in this new basis, where do we find is that we can infect express the mean in terms of the pseudo counts of the.",
            "Of the declared distribution.",
            "And we can also express the diagonal elements of the covariance matrix in terms of these pseudo counts of the directly distribution.",
            "So we have a one to one correspondence here, and in fact we can invert these equations and express this huge accounts of the directly distribution in terms of the parameters.",
            "Here the covariance and mean of that Gaussian.",
            "And just to check for plausibility, if you like, so clearly the mu K is.",
            "It's monotonically related to the Alpha K here, so that makes sense.",
            "So if the Gaussian mean is greater than the probability or the Gaussian mean indirection K is greater than the probability of the case component will be higher.",
            "And here we have the inverse relationship with the covariance, so that's the probability."
        ],
        [
            "Like if you like, let's take a look at what this looks like in practice.",
            "If we do this in this case in the special case of the beta distribution, which is just a very clear distribution with just two possible outcomes, so the starting point is our three declared distributions here these are the full lines here for example this one.",
            "What we can do now is we can use our equation to go back into the Gaussian space and we then find find a.",
            "You know a unimodal distribution.",
            "In this transformed space here, and the idea is now that we approximate this unimodal distribution by a Gaussian here.",
            "And we can then transform that back and we see what that approximation looks like in this space, and you can see for these different types of directly distributions.",
            "That's actually a fairly good match, so it's a very benign kind of approximation that we do, and that allows us to go from one side to the other here."
        ],
        [
            "So just to take a look at little smaller part of the model.",
            "The interesting part here you can think of this as follows.",
            "The generative process is we have the multivariate Gaussian variable X.",
            "Here that we sample from then we transform that sample into probability vector and then we sample from that probability vector some observations that would be kind of this up part of the model that's of interest here and then how do we do inference?",
            "We use the Laplace Bridge to obtain declare beliefs on pie.",
            "Then when we do the update here, we treat this update just as it directly update, which is really simple because of the conjugacy property of the directly.",
            "It's basically just counting increasing pseudocounts depending on the observations, and then we can go back and transform the directly belief back into a belief on the Gaussian variable."
        ],
        [
            "So we also tried this out.",
            "Of course, people are suspicious.",
            "We are suspicious of our own results, so we compare this to MCMC sampling in the same situation using slice sampling and what this plot basically shows is.",
            "Where when you take a multivariate Gaussian X and you in 10 dimensions and you go through the kind of generative process I described on the previous slide and then you want to reconstruct back what that X was, that in fact the MCMC estimate which is this blue line here is very behaves very similarly to the Laplace estimate here, and we know that asymptotically the MCMC estimate.",
            "Converges to the right answer, so that gives some nice confirmation that this is a B9 approximation that we're making."
        ],
        [
            "Here.",
            "So let me show you some experimental results.",
            "How are we doing with time?"
        ],
        [
            "Oh excellent, yeah, very good.",
            "So the data set that we're looking at here is the state of the Union data set.",
            "I don't know if you've played with this, but it is very entertaining.",
            "So every year the President of the United States gives this state of the Union address and that has been collected for many years since 1790.",
            "And you can imagine that that's a really interesting corpus to look at because all the historical trends through the years will be represented in that corpus.",
            "Now we can ask what are the features that we have for this kind of data and what we went for is.",
            "Here we use discrete author features, namely, who was the president who gave the actual address and takes 44 different values in this time and then.",
            "We of course also take the year feature and we don't consider years to be independent, but we want to have some coupling because obviously there would be some continuity through time that we can take advantage of here in the modeling.",
            "Now we will only compare here to the directly multinomial regression work by Memo ET al, which is the closest one.",
            "Remember that's the one that can take into account features you know, condition on features of the document, which is already a very, very powerful and useful framework, I think.",
            "So now somehow we need to encode these features into into what these algorithms can take as input.",
            "So for the directly multinomial regression for the time features we use 100 radial basis functions with a width of five years that we kind of space in an overlapping way over the over the period of time, and for author features, we just take 44 mutually exclusive binary features.",
            "You know you could do some more in terms of feature engineering, for example.",
            "There could be party affiliation, right?",
            "And you might suspect that could have an impact on people say so.",
            "There's definitely scope for a little more detailed modeling on this.",
            "What do we do for the kernel topic model?",
            "We here choose a rational quadratic kernel with a width of five years.",
            "Now the rational quadratic kernel is a kernel that allows us to consider different length scales at the same time, because it's essentially an integral.",
            "Over a Gaussian, if you like for different timescales, and that's of course very powerful to be able to do some kind of multi scale modeling here, which would be trickier in this model, although you could somehow try to implement it using using feature engineering I guess Now the other thing we do for the kernel topic model is that we use an additional constant term or punishment if you like.",
            "If the authors are not the same, so that will just you know.",
            "And give the model the chance to do a little bit more variation.",
            "If between two years the author changed, so to speak.",
            "So those are the two models that we look at here, and maybe one general remark.",
            "It's not yet clear how to evaluate topic models at all, right?",
            "This is an ongoing debate.",
            "There's some interesting work on that people have been using crowdsourcing.",
            "You can look at the perplexity just in terms of prediction accuracy and so on.",
            "So what we'll go here with is perplexity, and we look at the actual topics and I'll leave it to your judgment if if you like.",
            "The results, but certainly this is to a degree more of an art."
        ],
        [
            "And it is a science fun to look at though.",
            "So what we see here are the topics and topic proportions for these speeches from 1790 up to 2011, and.",
            "And this is the result that the kernel topic model came up with and you can see that there's a lot of variability in the topics, so there's really seems to be stuff going on, and I'm not an expert in the American history, but maybe we can look at a few things that make sense.",
            "For example, apparently there was a war with Spain around this time here, and that was part of part of those.",
            "Of those speeches, quite a bit here in this particular year.",
            "Then what I found interesting is let me just step back.",
            "Yeah, we have this this whole trend of of the American people, which is, you know, maybe the most over used phrase in these in these situations.",
            "And let me see the.",
            "You know here you you know, maybe a little bit after the oil crisis, you have things like energy, oil and so on coming up as interesting topics.",
            "Of course, here in the 1940s you have war as a topic and then you know then you have peace and you know the free world.",
            "Having won in some sense or not.",
            "Sorry, where is God?",
            "Yeah yeah, it's a good question.",
            "I think since God appears in all of these so frequently, it's just not picked by any particular translation.",
            "There is good, right and God is good.",
            "Yeah, that's right and so."
        ],
        [
            "Now, if you compare this to the linear model that you know just uses the features, you see that somehow that doesn't seem to have the kind of the dynamics in it that we saw in the previous model.",
            "It basically finds relatively constant things overtime.",
            "You know, sometimes there are a little bigger blocks here.",
            "But overall, a lot less has been captured at the different length scales, right?",
            "Because that model is basically smoothing at a particular length scale, and so we couldn't get much better than that out of the model when we played with different different settings of the width parameter."
        ],
        [
            "So on.",
            "So you can also look at perplexity.",
            "This is actually training perplexity, so it's not a particularly great measure.",
            "I would say of, you know it doesn't measure the generalization, but you can see here if we compare the kernel model to the linear model, and this is on the same data set, the state of the Union data, set that.",
            "In fact the perplexity of the kernel model is quite a bit lower, so it models at a higher Fidelity if you like.",
            "Of course it could.",
            "Also, it's a more powerful model, so it could also overfit potentially and it just to understand the perplexity.",
            "The vocabulary here is 5000, so the perplexity, if nothing has been learned would be 5000, and then if something has been learned then that measure goes down.",
            "Basically, you're surprised when you see a new word goes down once you have a decent model."
        ],
        [
            "We also looked at a couple of other datasets.",
            "This is a Wikipedia data set.",
            "The list of all probability related topics and here we used a squared exponential kernel on the link distance so we can look at the link graph between these topics and if there's a direct link then the distance is 1 and if there's only an indirect link, the distances to we take the shortest distance in that graph squared exponential kernel and then.",
            "Compare that to the constant model, which is basically just the latent directly allocation, and you can see again that the kernel model achieves better perplexity.",
            "What you see here is this in this little spike is that we also do some hyperparameter selection based on evidence, which we can do because it's a Gaussian process model and that for a short period seems to lead to this thing becoming worse, But then that.",
            "Overall model is then better and it goes on to have a have a better perplexity.",
            "You can see similar things here for the NIPS data set by Global Snitow.",
            "That's basically NIPS papers that are being modeled, and you can see here that the model, after making a couple of good decisions in terms of the hyperparameters, then then achieves a much better perplexity than the linear model does.",
            "So I'm not saying that you couldn't kind of tune the linear model with appropriate feature selection, but the kernel approach seems to lend itself.",
            "Do this kind of graph distance."
        ],
        [
            "That you can use.",
            "So let me conclude.",
            "I think it's pretty clear that in topic modeling we do want to use the context information.",
            "There's always context with the documents.",
            "I haven't found a single document where there wasn't any context about space, time, author.",
            "Citation data Social Network, citation network, and so on, and it would be stupid not to use that so that that seems clear and what we're suggesting here is basically to use a kernel or Gaussian process framework and integrate that into the latent directly allocation graphical model to enable us to use that kind of metadata.",
            "And as a byproduct.",
            "If you like, we introduced this Lapras Bridge, which I think could just be a standard component in.",
            "Situations whenever you want to bring the nonparametric kernel world and the graphical models world in particular with discrete probability distributions together, 'cause it enables us to seamlessly send messages forth and back between.",
            "Kernels and probabilities and hence author it is an element of the confluence of kernel methods and graphical models, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you very much for the invitation to talk here.",
                    "label": 0
                },
                {
                    "sent": "I would like to emphasize that what I'm presenting is really mostly the work of Phillip Penick and who's organizing another workshop, and so he asked me to jump in and present this, but he did most of the work.",
                    "label": 0
                },
                {
                    "sent": "So for everything that you like in this presentation, please think of Phillip.",
                    "label": 0
                },
                {
                    "sent": "And if you have criticism then please.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At view me as responsible.",
                    "label": 0
                },
                {
                    "sent": "This is based on a paper called Kernel Topic models that was published in AI starts a couple of years ago, so I'll give a quick introduction motivation to the problem and then describe what the kernel topic model is in terms of graphical model terms, and then I'll focus on a particular problem in inference that arises in this type of model, namely bridging from an unconstrained space.",
                    "label": 1
                },
                {
                    "sent": "In this case from a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "Into a probability space for directly distributions for which we've developed a message passing solution that connects these two.",
                    "label": 0
                },
                {
                    "sent": "And that's really the central theme, and then I have some I think might be entertaining experiment.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results here",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm sure you're all familiar with topic models.",
                    "label": 0
                },
                {
                    "sent": "I'll just go through this for the sake of notation and so that you can.",
                    "label": 0
                },
                {
                    "sent": "You can just revisit this briefly, so this is a sketch of a standard LDA type topic model.",
                    "label": 0
                },
                {
                    "sent": "A document is modeled here as a bag of words.",
                    "label": 0
                },
                {
                    "sent": "For each document D, we draw probability vector Pi D over topics from idyllically distribution for each topic.",
                    "label": 0
                },
                {
                    "sent": "Here we draw probability vector theater K over words from from a directly, again with parameters Peter K and then inside.",
                    "label": 0
                },
                {
                    "sent": "Here we have for each word that there is an indicator variable that we draw from from the distribution Pi here, which indicates.",
                    "label": 0
                },
                {
                    "sent": "Which topic that word comes from and then once we know that we draw from the appropriate topic distribution, the word that's the basic standard topic model and that will be one.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Art of of our model.",
                    "label": 0
                },
                {
                    "sent": "So these models are useful to organize text corpora to get an overview of what's in there.",
                    "label": 1
                },
                {
                    "sent": "Also to create features, but the classical latent directly allocation disregards context information of text, right?",
                    "label": 0
                },
                {
                    "sent": "For example, what we typically know about the text is that there's an author.",
                    "label": 1
                },
                {
                    "sent": "There's a time and place when the document was created.",
                    "label": 0
                },
                {
                    "sent": "This might be part of the book Journal Proceedings page numbers, all of that information people to which the document might have been addressed.",
                    "label": 0
                },
                {
                    "sent": "Or web links and sensations, all of which create this structure around the document, which surely show.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be useful for the task of topic modeling.",
                    "label": 0
                },
                {
                    "sent": "So just a few examples here.",
                    "label": 0
                },
                {
                    "sent": "Here's an email I received a few months ago about this very talk and you can see here.",
                    "label": 0
                },
                {
                    "sent": "Yes, there's information from whom it is too humid is who was see seed.",
                    "label": 0
                },
                {
                    "sent": "What is this subject?",
                    "label": 0
                },
                {
                    "sent": "We know when it was sent, and so on.",
                    "label": 0
                },
                {
                    "sent": "All of this information is available, and that's really the rule rather than the exception, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, there's hardly ever any piece of document where we don't know anything about it.",
                    "label": 0
                },
                {
                    "sent": "Another example, of course, is just citation data.",
                    "label": 0
                },
                {
                    "sent": "Individual citation data already contains a lot of information, but then of course, if you view the network of citations or links between documents or social network among authors and so on, you see that there's all of this structure that we would like to use.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For topic models, now just to talk a little bit about related work, there's of course the classical latent directly allocation paper by Blei ET al.",
                    "label": 0
                },
                {
                    "sent": "And some people have started addressing this problem that there is actually this context information available.",
                    "label": 0
                },
                {
                    "sent": "For example, there are dynamic topic models which take into account the structure in time that documents may have.",
                    "label": 1
                },
                {
                    "sent": "There's a correlated topic model which allows topics to be correlated and that.",
                    "label": 1
                },
                {
                    "sent": "Idea we will have some ideas in the model here from this.",
                    "label": 1
                },
                {
                    "sent": "There's topic models conditioned on arbitrary features, which is probably coming closest to the type of work we're looking at here by MEM.",
                    "label": 1
                },
                {
                    "sent": "Know Ann McCallum, and there's also relational topic models, so there this space has been looked upon, and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We hope to contribute to this general.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Area, so let's look at topic models from the matrix factorization point of view.",
                    "label": 0
                },
                {
                    "sent": "These are rather informal sketches just to get the idea what the structure of the problem is.",
                    "label": 0
                },
                {
                    "sent": "So what we want to model is the document word matrix.",
                    "label": 0
                },
                {
                    "sent": "Because we were assuming that the we have a bag of words representation of the data and now you can roughly view a topic model as matrix factorization where we factor this matrix of documents and words into a matrix of documents.",
                    "label": 1
                },
                {
                    "sent": "And topics times a matrix of topics, times, words right.",
                    "label": 0
                },
                {
                    "sent": "So this is basically the topic distribution and this is the distribution for each topic over words.",
                    "label": 0
                },
                {
                    "sent": "And so just to illustrate.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In which direction this is going here is the conditional topic model where you can condition on features.",
                    "label": 0
                },
                {
                    "sent": "So in addition, here you assume that you have a feature vector 5 for each document, and then you can think of of the factorization of the document word matrix as first having a document feature matrix times a feature times topic matrix.",
                    "label": 0
                },
                {
                    "sent": "And here we are in this linear world.",
                    "label": 0
                },
                {
                    "sent": "So we need some kind of softmax operator.",
                    "label": 0
                },
                {
                    "sent": "To get into the world of probabilities and then we have a probability distribution of a topics and then over words so we can view this as some kind of generalized.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matrix factorization and just to show you how this new model fits in.",
                    "label": 0
                },
                {
                    "sent": "It's basically very similar to the previous one, but then we'll have a nonlinear component here driven by a Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "This function H will be drawn from a Gaussian process and so you can also view it as some form of matrix factorization for the document word matrix.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's get back to the graphical models view.",
                    "label": 0
                },
                {
                    "sent": "We had discussed this standard topic model representation and now let me show you how this changes because this part of the model will basically be retained and.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will tack on a Gaussian process regression model to the left.",
                    "label": 0
                },
                {
                    "sent": "So if you just forget about this left part, that's the standard topic model that we had.",
                    "label": 0
                },
                {
                    "sent": "But now we're basically saying that there's this vector Y which just gets squashed by a softmax into this into this vector of topic proportions, and we do a regression on the document features.",
                    "label": 0
                },
                {
                    "sent": "But we do this in a non parametric way.",
                    "label": 0
                },
                {
                    "sent": "Now where we draw this function H. Or it's like a vector of functions, one for each topic from a Gaussian process prior.",
                    "label": 0
                },
                {
                    "sent": "And it's basically you can view this as a Gaussian process which has as its likelihood a topic model that's kind of the kind of compositionality that we're looking at here.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "What will this?",
                    "label": 0
                },
                {
                    "sent": "I briefly mentioned this, what will this Gaussian process be over?",
                    "label": 0
                },
                {
                    "sent": "Well, it will be over.",
                    "label": 0
                },
                {
                    "sent": "Did we just?",
                    "label": 0
                },
                {
                    "sent": "This is looking slightly yellowish.",
                    "label": 0
                },
                {
                    "sent": "I don't know, it's not on this end.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe if I don't know what the system is trying to tell us here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, can you sure?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I think we're OK, right?",
                    "label": 0
                },
                {
                    "sent": "So the kernel over documents could be based on this information that I talked about.",
                    "label": 1
                },
                {
                    "sent": "The matter information Author, Time, location and publication, but of course that could also be handled by normal features.",
                    "label": 0
                },
                {
                    "sent": "Maybe more interesting Lee, we can model information like web link structure, citation structure or social network structure, which would lend itself to a kernel based approach because of the graph structures that are involved here.",
                    "label": 1
                },
                {
                    "sent": "And those would then drive the covariance.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Matrix of the Gaussian process.",
                    "label": 0
                },
                {
                    "sent": "So where does the difficulty arise?",
                    "label": 0
                },
                {
                    "sent": "And this is where the Laplace Bridge comes in, which is this kind of element that I would like to do.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Scribe here in a little more detail, so the problem really is how do we get from the world of unconstrained Y variables into the world of probabilities?",
                    "label": 0
                },
                {
                    "sent": "Pi D. Here in a message passing framework so that the messages generated by the data here in the topic model can pass into our regression model essentially as observations to drive the inference here.",
                    "label": 0
                },
                {
                    "sent": "And of course, how the prior the Gaussian process prior here can be sent in terms of messages.",
                    "label": 0
                },
                {
                    "sent": "Over to the directly distribution for the topic model and so this is actually a slightly more general problem that that might occur in a number of other.",
                    "label": 1
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As well.",
                    "label": 0
                },
                {
                    "sent": "So we call this Laplace Bridge and of course as the name suggests, it's based on the Laplace approximation.",
                    "label": 0
                },
                {
                    "sent": "I don't know if many of you are familiar with this.",
                    "label": 0
                },
                {
                    "sent": "This goes back to earlier work by Mackay on neural networks in the 90s.",
                    "label": 0
                },
                {
                    "sent": "The ideas of the Laplace approximation is that you want to approximate the posterior distribution in your in your Bayesian inference, and the idea is to find the mode of the posterior distribution, then fit the Gaussian distribution based on that mode.",
                    "label": 0
                },
                {
                    "sent": "And the curvature of the posterior at that point.",
                    "label": 0
                },
                {
                    "sent": "And in the context of Mckay's work that was then used to evaluate approximate posterior integrals, because once you have that convenient Gaussian form of the posterior, which of course is unimodal and neglects or the multi modality that might have been there in the posterior, then you can do all of these integrals, expectations and so on.",
                    "label": 0
                },
                {
                    "sent": "Now Mackay in 1998.",
                    "label": 0
                },
                {
                    "sent": "Discuss is that the Laplace approximation is really basis dependent, so for example, traditionally if you had it directly distribution, you would probably do this approximation in probability space right?",
                    "label": 0
                },
                {
                    "sent": "Which is a mismatch because you know Gaussians extend over the real line, whereas probabilities are bounded.",
                    "label": 0
                },
                {
                    "sent": "That's actually not a very good idea, and so he suggests changing the basis to a softmax basis, and the idea of our work here is that we use the Laplace approximation in the softmax basis.",
                    "label": 1
                },
                {
                    "sent": "To transform the probability messages into those unconstrained Gaussian messages and back and thereby enable message passing to work on for inference.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In these models.",
                    "label": 0
                },
                {
                    "sent": "So let's look at this in a formal way.",
                    "label": 0
                },
                {
                    "sent": "This just the declared distribution without normalization.",
                    "label": 0
                },
                {
                    "sent": "I tried to save that so it's a distribution over probability vectors as you know, parameterized by these pseudocounts Alpha.",
                    "label": 0
                },
                {
                    "sent": "And it basically takes this functional form of the multinomial distribution because it's the conjugate to the multinomial distribution and but of course the parameters here the distribution is over the pies, and then of course we know it really is defined only on the probability simplex, and hence we have this Delta function that restricts it to this lower dimensional N -- 1 dimensional subspace now.",
                    "label": 0
                },
                {
                    "sent": "What Mackay suggests is transformed to an you softmax basis.",
                    "label": 0
                },
                {
                    "sent": "So basically represent the probabilities here in terms of the softmax function of these new variables Y and that then if you put that in leads to new parameterized form of the directly distribution that you see here.",
                    "label": 0
                },
                {
                    "sent": "And the nice thing, for example, is the Alpha.",
                    "label": 0
                },
                {
                    "sent": "I now don't has this annoying minus one anymore, so it's actually a more benign formulation of the directly distribution and we also have to choose a measure G here, which we will later let converge to the Delta peak, which again enforces the fact that if we have a probability distribution over CHI over K possible outcomes, then we really do the normalization of the measure.",
                    "label": 0
                },
                {
                    "sent": "I only have a K -- 1 dimensional subspace, so this is basically what make I came up with.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the question that we're asking is how can we now basically create a correspondence between the declared distribution and a multivariate Gaussian?",
                    "label": 0
                },
                {
                    "sent": "We're looking for a one to one correspondence so that we can send the messages forth and back in both directions, and if we carry through with the Laplace approximation in this new basis, where do we find is that we can infect express the mean in terms of the pseudo counts of the.",
                    "label": 0
                },
                {
                    "sent": "Of the declared distribution.",
                    "label": 0
                },
                {
                    "sent": "And we can also express the diagonal elements of the covariance matrix in terms of these pseudo counts of the directly distribution.",
                    "label": 0
                },
                {
                    "sent": "So we have a one to one correspondence here, and in fact we can invert these equations and express this huge accounts of the directly distribution in terms of the parameters.",
                    "label": 0
                },
                {
                    "sent": "Here the covariance and mean of that Gaussian.",
                    "label": 0
                },
                {
                    "sent": "And just to check for plausibility, if you like, so clearly the mu K is.",
                    "label": 0
                },
                {
                    "sent": "It's monotonically related to the Alpha K here, so that makes sense.",
                    "label": 0
                },
                {
                    "sent": "So if the Gaussian mean is greater than the probability or the Gaussian mean indirection K is greater than the probability of the case component will be higher.",
                    "label": 0
                },
                {
                    "sent": "And here we have the inverse relationship with the covariance, so that's the probability.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like if you like, let's take a look at what this looks like in practice.",
                    "label": 0
                },
                {
                    "sent": "If we do this in this case in the special case of the beta distribution, which is just a very clear distribution with just two possible outcomes, so the starting point is our three declared distributions here these are the full lines here for example this one.",
                    "label": 0
                },
                {
                    "sent": "What we can do now is we can use our equation to go back into the Gaussian space and we then find find a.",
                    "label": 0
                },
                {
                    "sent": "You know a unimodal distribution.",
                    "label": 0
                },
                {
                    "sent": "In this transformed space here, and the idea is now that we approximate this unimodal distribution by a Gaussian here.",
                    "label": 0
                },
                {
                    "sent": "And we can then transform that back and we see what that approximation looks like in this space, and you can see for these different types of directly distributions.",
                    "label": 0
                },
                {
                    "sent": "That's actually a fairly good match, so it's a very benign kind of approximation that we do, and that allows us to go from one side to the other here.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just to take a look at little smaller part of the model.",
                    "label": 0
                },
                {
                    "sent": "The interesting part here you can think of this as follows.",
                    "label": 0
                },
                {
                    "sent": "The generative process is we have the multivariate Gaussian variable X.",
                    "label": 0
                },
                {
                    "sent": "Here that we sample from then we transform that sample into probability vector and then we sample from that probability vector some observations that would be kind of this up part of the model that's of interest here and then how do we do inference?",
                    "label": 0
                },
                {
                    "sent": "We use the Laplace Bridge to obtain declare beliefs on pie.",
                    "label": 0
                },
                {
                    "sent": "Then when we do the update here, we treat this update just as it directly update, which is really simple because of the conjugacy property of the directly.",
                    "label": 0
                },
                {
                    "sent": "It's basically just counting increasing pseudocounts depending on the observations, and then we can go back and transform the directly belief back into a belief on the Gaussian variable.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we also tried this out.",
                    "label": 0
                },
                {
                    "sent": "Of course, people are suspicious.",
                    "label": 0
                },
                {
                    "sent": "We are suspicious of our own results, so we compare this to MCMC sampling in the same situation using slice sampling and what this plot basically shows is.",
                    "label": 0
                },
                {
                    "sent": "Where when you take a multivariate Gaussian X and you in 10 dimensions and you go through the kind of generative process I described on the previous slide and then you want to reconstruct back what that X was, that in fact the MCMC estimate which is this blue line here is very behaves very similarly to the Laplace estimate here, and we know that asymptotically the MCMC estimate.",
                    "label": 0
                },
                {
                    "sent": "Converges to the right answer, so that gives some nice confirmation that this is a B9 approximation that we're making.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "So let me show you some experimental results.",
                    "label": 1
                },
                {
                    "sent": "How are we doing with time?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh excellent, yeah, very good.",
                    "label": 0
                },
                {
                    "sent": "So the data set that we're looking at here is the state of the Union data set.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you've played with this, but it is very entertaining.",
                    "label": 0
                },
                {
                    "sent": "So every year the President of the United States gives this state of the Union address and that has been collected for many years since 1790.",
                    "label": 0
                },
                {
                    "sent": "And you can imagine that that's a really interesting corpus to look at because all the historical trends through the years will be represented in that corpus.",
                    "label": 0
                },
                {
                    "sent": "Now we can ask what are the features that we have for this kind of data and what we went for is.",
                    "label": 0
                },
                {
                    "sent": "Here we use discrete author features, namely, who was the president who gave the actual address and takes 44 different values in this time and then.",
                    "label": 0
                },
                {
                    "sent": "We of course also take the year feature and we don't consider years to be independent, but we want to have some coupling because obviously there would be some continuity through time that we can take advantage of here in the modeling.",
                    "label": 0
                },
                {
                    "sent": "Now we will only compare here to the directly multinomial regression work by Memo ET al, which is the closest one.",
                    "label": 0
                },
                {
                    "sent": "Remember that's the one that can take into account features you know, condition on features of the document, which is already a very, very powerful and useful framework, I think.",
                    "label": 0
                },
                {
                    "sent": "So now somehow we need to encode these features into into what these algorithms can take as input.",
                    "label": 0
                },
                {
                    "sent": "So for the directly multinomial regression for the time features we use 100 radial basis functions with a width of five years that we kind of space in an overlapping way over the over the period of time, and for author features, we just take 44 mutually exclusive binary features.",
                    "label": 0
                },
                {
                    "sent": "You know you could do some more in terms of feature engineering, for example.",
                    "label": 0
                },
                {
                    "sent": "There could be party affiliation, right?",
                    "label": 0
                },
                {
                    "sent": "And you might suspect that could have an impact on people say so.",
                    "label": 0
                },
                {
                    "sent": "There's definitely scope for a little more detailed modeling on this.",
                    "label": 0
                },
                {
                    "sent": "What do we do for the kernel topic model?",
                    "label": 0
                },
                {
                    "sent": "We here choose a rational quadratic kernel with a width of five years.",
                    "label": 0
                },
                {
                    "sent": "Now the rational quadratic kernel is a kernel that allows us to consider different length scales at the same time, because it's essentially an integral.",
                    "label": 0
                },
                {
                    "sent": "Over a Gaussian, if you like for different timescales, and that's of course very powerful to be able to do some kind of multi scale modeling here, which would be trickier in this model, although you could somehow try to implement it using using feature engineering I guess Now the other thing we do for the kernel topic model is that we use an additional constant term or punishment if you like.",
                    "label": 0
                },
                {
                    "sent": "If the authors are not the same, so that will just you know.",
                    "label": 0
                },
                {
                    "sent": "And give the model the chance to do a little bit more variation.",
                    "label": 0
                },
                {
                    "sent": "If between two years the author changed, so to speak.",
                    "label": 0
                },
                {
                    "sent": "So those are the two models that we look at here, and maybe one general remark.",
                    "label": 0
                },
                {
                    "sent": "It's not yet clear how to evaluate topic models at all, right?",
                    "label": 0
                },
                {
                    "sent": "This is an ongoing debate.",
                    "label": 0
                },
                {
                    "sent": "There's some interesting work on that people have been using crowdsourcing.",
                    "label": 0
                },
                {
                    "sent": "You can look at the perplexity just in terms of prediction accuracy and so on.",
                    "label": 0
                },
                {
                    "sent": "So what we'll go here with is perplexity, and we look at the actual topics and I'll leave it to your judgment if if you like.",
                    "label": 0
                },
                {
                    "sent": "The results, but certainly this is to a degree more of an art.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And it is a science fun to look at though.",
                    "label": 0
                },
                {
                    "sent": "So what we see here are the topics and topic proportions for these speeches from 1790 up to 2011, and.",
                    "label": 0
                },
                {
                    "sent": "And this is the result that the kernel topic model came up with and you can see that there's a lot of variability in the topics, so there's really seems to be stuff going on, and I'm not an expert in the American history, but maybe we can look at a few things that make sense.",
                    "label": 1
                },
                {
                    "sent": "For example, apparently there was a war with Spain around this time here, and that was part of part of those.",
                    "label": 0
                },
                {
                    "sent": "Of those speeches, quite a bit here in this particular year.",
                    "label": 0
                },
                {
                    "sent": "Then what I found interesting is let me just step back.",
                    "label": 1
                },
                {
                    "sent": "Yeah, we have this this whole trend of of the American people, which is, you know, maybe the most over used phrase in these in these situations.",
                    "label": 0
                },
                {
                    "sent": "And let me see the.",
                    "label": 0
                },
                {
                    "sent": "You know here you you know, maybe a little bit after the oil crisis, you have things like energy, oil and so on coming up as interesting topics.",
                    "label": 0
                },
                {
                    "sent": "Of course, here in the 1940s you have war as a topic and then you know then you have peace and you know the free world.",
                    "label": 0
                },
                {
                    "sent": "Having won in some sense or not.",
                    "label": 0
                },
                {
                    "sent": "Sorry, where is God?",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, it's a good question.",
                    "label": 0
                },
                {
                    "sent": "I think since God appears in all of these so frequently, it's just not picked by any particular translation.",
                    "label": 0
                },
                {
                    "sent": "There is good, right and God is good.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right and so.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, if you compare this to the linear model that you know just uses the features, you see that somehow that doesn't seem to have the kind of the dynamics in it that we saw in the previous model.",
                    "label": 1
                },
                {
                    "sent": "It basically finds relatively constant things overtime.",
                    "label": 0
                },
                {
                    "sent": "You know, sometimes there are a little bigger blocks here.",
                    "label": 0
                },
                {
                    "sent": "But overall, a lot less has been captured at the different length scales, right?",
                    "label": 0
                },
                {
                    "sent": "Because that model is basically smoothing at a particular length scale, and so we couldn't get much better than that out of the model when we played with different different settings of the width parameter.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So on.",
                    "label": 0
                },
                {
                    "sent": "So you can also look at perplexity.",
                    "label": 0
                },
                {
                    "sent": "This is actually training perplexity, so it's not a particularly great measure.",
                    "label": 0
                },
                {
                    "sent": "I would say of, you know it doesn't measure the generalization, but you can see here if we compare the kernel model to the linear model, and this is on the same data set, the state of the Union data, set that.",
                    "label": 1
                },
                {
                    "sent": "In fact the perplexity of the kernel model is quite a bit lower, so it models at a higher Fidelity if you like.",
                    "label": 0
                },
                {
                    "sent": "Of course it could.",
                    "label": 0
                },
                {
                    "sent": "Also, it's a more powerful model, so it could also overfit potentially and it just to understand the perplexity.",
                    "label": 0
                },
                {
                    "sent": "The vocabulary here is 5000, so the perplexity, if nothing has been learned would be 5000, and then if something has been learned then that measure goes down.",
                    "label": 0
                },
                {
                    "sent": "Basically, you're surprised when you see a new word goes down once you have a decent model.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also looked at a couple of other datasets.",
                    "label": 0
                },
                {
                    "sent": "This is a Wikipedia data set.",
                    "label": 0
                },
                {
                    "sent": "The list of all probability related topics and here we used a squared exponential kernel on the link distance so we can look at the link graph between these topics and if there's a direct link then the distance is 1 and if there's only an indirect link, the distances to we take the shortest distance in that graph squared exponential kernel and then.",
                    "label": 0
                },
                {
                    "sent": "Compare that to the constant model, which is basically just the latent directly allocation, and you can see again that the kernel model achieves better perplexity.",
                    "label": 0
                },
                {
                    "sent": "What you see here is this in this little spike is that we also do some hyperparameter selection based on evidence, which we can do because it's a Gaussian process model and that for a short period seems to lead to this thing becoming worse, But then that.",
                    "label": 0
                },
                {
                    "sent": "Overall model is then better and it goes on to have a have a better perplexity.",
                    "label": 0
                },
                {
                    "sent": "You can see similar things here for the NIPS data set by Global Snitow.",
                    "label": 1
                },
                {
                    "sent": "That's basically NIPS papers that are being modeled, and you can see here that the model, after making a couple of good decisions in terms of the hyperparameters, then then achieves a much better perplexity than the linear model does.",
                    "label": 0
                },
                {
                    "sent": "So I'm not saying that you couldn't kind of tune the linear model with appropriate feature selection, but the kernel approach seems to lend itself.",
                    "label": 0
                },
                {
                    "sent": "Do this kind of graph distance.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That you can use.",
                    "label": 0
                },
                {
                    "sent": "So let me conclude.",
                    "label": 0
                },
                {
                    "sent": "I think it's pretty clear that in topic modeling we do want to use the context information.",
                    "label": 0
                },
                {
                    "sent": "There's always context with the documents.",
                    "label": 0
                },
                {
                    "sent": "I haven't found a single document where there wasn't any context about space, time, author.",
                    "label": 0
                },
                {
                    "sent": "Citation data Social Network, citation network, and so on, and it would be stupid not to use that so that that seems clear and what we're suggesting here is basically to use a kernel or Gaussian process framework and integrate that into the latent directly allocation graphical model to enable us to use that kind of metadata.",
                    "label": 0
                },
                {
                    "sent": "And as a byproduct.",
                    "label": 0
                },
                {
                    "sent": "If you like, we introduced this Lapras Bridge, which I think could just be a standard component in.",
                    "label": 0
                },
                {
                    "sent": "Situations whenever you want to bring the nonparametric kernel world and the graphical models world in particular with discrete probability distributions together, 'cause it enables us to seamlessly send messages forth and back between.",
                    "label": 0
                },
                {
                    "sent": "Kernels and probabilities and hence author it is an element of the confluence of kernel methods and graphical models, thanks.",
                    "label": 0
                }
            ]
        }
    }
}