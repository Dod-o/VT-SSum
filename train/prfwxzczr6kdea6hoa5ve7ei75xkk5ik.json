{
    "id": "prfwxzczr6kdea6hoa5ve7ei75xkk5ik",
    "title": "When to Reach for the Cloud: Using Parallel Hardware for Link Discovery",
    "info": {
        "introducer": [
            "Axel Polleres, Institute for Information Business, Vienna University of Economics and Business"
        ],
        "author": [
            "Axel-Cyrille Ngonga Ngomo, University of Leipzig"
        ],
        "published": "July 8, 2013",
        "recorded": "May 2013",
        "category": [
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Big Data"
        ]
    },
    "url": "http://videolectures.net/eswc2013_ngonga_ngomo_cloud/",
    "segmentation": [
        [
            "We have three talks in this session and the first one will be on went to reach for the cloud using parallel hardware for link discovery and it's going to be presented by Axel, so my name is Axel.",
            "I'm from the University of Leipzig in Germany and I'm going to talk about when to reach for the cloud and this is John work with last call Michelle hard tongue on it, hard RAM from the database Department at the University and with Norman Hainan, CERN outward from the semantic Web.",
            "Group."
        ],
        [
            "So first of all."
        ],
        [
            "A bit of motivation why link discovery?",
            "Why do we need that?",
            "Well, basically links are the backbone for the architecture that we've chosen for the link data weapon that's central to actually use the link data weapon itself.",
            "All sorts of applications rely on the fact that we have links between different knowledge bases.",
            "For example, cross Ontology question answering when we ask for things like give me the side effects of medication related to a certain gene, say for example Fox B2.",
            "We need we need links across different knowledge bases to be able to answer that question, we need links for data integration.",
            "If we take two data sources and we put them together, we want to use the links to ensure that we don't have resources in our fuse knowledge base that refer to the same world entity but have different you arise.",
            "We need links for large scale reasoning for Federated queries and so on and so forth, But if you look at the current topology of the linked open data cloud, we basically see that we have.",
            "Quite a few triples dead 31 plus billion, but only approximately 0.5 billion links.",
            "That is less than 2% and in most cases we have our same as links.",
            "So."
        ],
        [
            "So.",
            "The question is now, why is then why is it so difficult to create links and actually we have two problems.",
            "There refers problem is time complexity.",
            "Most link discovery frameworks assume that they are given two sets as input is set of source resources.",
            "A set of target resources and a certain relation R, and the idea is that they're trying to find a set M that is such that it is a subset of the Cartesian product of S&T and all the elements of the set actually bound by the relation art.",
            "So basically RFC holds.",
            "For all the elements of M. Now, in most cases it is quite tricky to compute that, So what you usually do is we try to approximate this September set in prime, where we say we have a complex similarity function and we say the elements of M prime are such that the similarity between S&T is larger or equal to a certain set threshold data.",
            "Or the other way around.",
            "We use a distance function.",
            "Then we see the distance between S&T is less or equal to a certain threshold data and this is obviously quadratic.",
            "If we use it would actually.",
            "Problem if we use native approach.",
            "We've actually computed that.",
            "So if you assume that we want to link cities from DB pedia with cities from geonames and one comparison costs one millisecond, we're already need 69 days just for the cities.",
            "And imagine you want to do that for the whole linked open data cloud, or even more for all sorts of link data sources.",
            "You obviously need ages for that, so the time complexity problem is definitely one that we need to deal with.",
            "The second part."
        ],
        [
            "Is actually the complexity of specifications, so is not sufficient to have specifications.",
            "We want to have good specifications, good specifications obviously being specifications that betray high precision and recall respect to the links that we generate, and this little one here.",
            "Actually you can use to link directors for LinkedIn DB DB Pedia.",
            "But I'm not going to talk about that today.",
            "So if you want to hear things about how to deal with the complexity of ring specifications, come to talk about koala bears and I'll tell you what koalas and link specifications.",
            "Have to do with each other, but today we're going to focus on the time complexity issue."
        ],
        [
            "Now, how do we deal with that?",
            "What are solutions to deal with the time complexity issue?",
            "First idea is obviously let us develop better algorithms.",
            "Algorithms are faster algorithms that do not need so much time to carry out the link.",
            "The link Discovery, now quite a show algorithms have been developed for that purpose.",
            "We have different blocking approaches that try to portion of the space in which the link specification is an actually just to compare elements that are within the blocks.",
            "We have specific algorithms that have been developed for particular metrics.",
            "For example the people join algorithm for three grams overlaps and so on, and the edge join algorithm for Levenshtein distances.",
            "And we've actually been able to develop algorithms that are near optimal respect to their reduction ratio.",
            "Basically means that they don't even carry out more.",
            "They don't carry out too many computations with respect to the minimal number of computations that they have to carry out to actually be able to compute links.",
            "But still, even having such algorithms, we still need a lot of time when we have really large.",
            "Link discovery tasks, which basically means that we need to go a step further.",
            "The step further here is to use parallel hardware where the basic idea is we try to distribute the task over a lot of machines and basically to have the whole thing running parallel so that we can carry out the link specification or actually the link discovery in a reduced amount of time."
        ],
        [
            "Now the interesting thing here is that over the years we've developed quite a few different architectures or different approaches to deal with Link discovery over parallel hardware.",
            "So basically we have the simple threat pools.",
            "We have GPU's that we can use.",
            "We have the cloud and these hardware parallel hardware approaches have very different architectures, so we have hardware that rely on shared memory.",
            "For example, in the case with Red Bulls.",
            "But have great memories where we have a combination of shared memory and distributed memory, and we have obviously distributed memory.",
            "In the case of the cloud, we also have hardware that have different requirements on respected execution paths.",
            "So for example, here each thread can do basically whatever it wants while it's on GPU's.",
            "We do have restrictions there.",
            "Certain threads after basically follow the same execution paths and also the location of the hardware is different.",
            "We have remote hardware in the case of the cloud.",
            "And local hardware.",
            "When you do, the thread pools or when you use GPS, although they also obviously GPU farms on the cloud.",
            "They're not going to look at them in this talk, so the question that we had was then when should we use which hardware?",
            "So we know by using parallelism we can actually improve the runtime, but when is it actually interesting to go to the cloud?",
            "Or once we use GPS and so on?"
        ],
        [
            "So."
        ],
        [
            "The premises for work was if we have an algorithm that run all three hardwares we can actually compare the run times of the algorithm on.",
            "In all these cases and find break even points where we say OK from this point on, please do use this particular type of artwork.",
            "The main problem that we had to solve for the first friend I've had to solve actually is that there was no such implementation of algorithms that run on all throughout West for Link Discovery.",
            "So basically you had to build 1."
        ],
        [
            "We pick H R3 because it is near reduction ratio optimal, and we implemented each or three on both thread pools, GPU's and also had a MapReduce version for the cloud and then we were able actually 2.",
            "Achieve our goal, which is to compare the runtime on all three parkitecture and such as break even points to basically say when to use what.",
            "And what I'm going to talk about now.",
            "It's basically present the algorithm shortly, talk about a different implementations, and then present the evaluation results that we got."
        ],
        [
            "OK, So what is each of three H?",
            "R3 assumes that the link specifications is is in a space that has a Minkowski metric in cost metric is defined this way.",
            "It's basically a distant function that is of the form square root.",
            "The PTH root of the sum I from one to N of S I -- T I to the power of P. And what chapter actually uses the fact that these components are independent, which basically means that we can actually greyed out the space that would.",
            "Working in and try to approximate circles with cubes."
        ],
        [
            "Let's look at an example here.",
            "So we assume that we have here 2 dimensions, longitude and latitude, and the first thing that it's at three does is create a grid of with Delta and Delta is nothing else than terror divided by Alpha.",
            "In this case Alpha is 4 and we call our father granularity parameter.",
            "Formally, what we can show is that H R3, the reduction ratio of HR3 is optimal, where Alpha goes towards Infinity.",
            "So once we've got that, what once we've got degreed.",
            "We can actually use the fact that the link specification as I defined it before, which says Delta St is less or equal to Terra, actually defines a hyperbowl in this particular space.",
            "Now, now that we know that we have your hyperbowl, we can approximate the hyperbowl by using a hypercube, and hypercubes are obviously way easier to compute than than hyperballs.",
            "We define the hypercube that is such that it contains all the elements of the hyper sphere and we basically say we're going to carry out the link.",
            "Discovery within the hyper cube.",
            "So basically for all the elements that are within this little square in here we know that all the other points that have touched that Delta St or Delta BT is less or equal to terror will be found within this hypercube.",
            "Now what H R3 does is instead of just using a hypercube.",
            "Actually also has an indexing function that allows to get rid of some portions of the cube that will actually never contain elements.",
            "That solves that satisfy the condition that we have so.",
            "We use the indexing function.",
            "We discard portions of direction.",
            "In these cases.",
            "These corners and based on this discarding of functions, we can actually show that if Alpha goes to Infinity, we can approximate the hypersphere perfectly and will never compare things that do not need to be compared.",
            "The basic idea is shot through there.",
            "If you want all the technical details, there's a paper from last year that describes the full algorithm and proves that is optimal.",
            "Now the question is how do you implement this algorithm on parallel hardware?",
            "Now 10 minutes left.",
            "That's incredible so.",
            "Yeah.",
            "OK, how do you implement this on prior hardware?",
            "Obviously what you want to do is you take each of the cubes.",
            "Here you give it to a thread and say number one.",
            "Compute this box to find other points.",
            "I need to compare the elements of this cube with a #2.",
            "Reasons and give me the mapping for this particular cube.",
            "So basically you go through all the cubes and you have your results.",
            "That's basically how you implement it when you want to run for threads now."
        ],
        [
            "If you try it, you to implement this exactly that way.",
            "On GPU's you'll run into massive time problems and that is simply due to the architecture of GPU's.",
            "Basically inside deep use what you find our so called work items that are those are simple compute units that actually follow a certain path is that in program path.",
            "Now the word atoms are joined in workgroups.",
            "And this work groups share a bit of a certain amount of memory, and Jaden has several work groups in the GPU.",
            "The GPU then communicates with the rest of the computer via the PCI Express bus, and that is actually the main problem that's the main bottleneck in the GPU architecture.",
            "You basically have the problem that the PCI Express bus is really slow, so if you want to do large scale link discovery, your bottleneck is actually getting the data into the GPU.",
            "So instead of implementing the algorithm the way suggested before 4 threads, what we did is we run the discretization.",
            "So basically the grid computation on the CPU.",
            "We ran the computation of the indexes on the GPU because we just.",
            "Had the boxes then and we then run the comparisons back on the CPU, which basically means that we didn't have to transfer much data from this from the CPU to the GPU, just the box is basically under indexes."
        ],
        [
            "How do you implement the same approach on the cloud?",
            "The naive approach would basically consist of taking the input entities, running the indexing in the map jobs, and then running the distance computation to reduce jobs.",
            "But why you basically figure out is that some of the map jobs or some of the rituals jobs actually take a lot of time or take the whole runtime becausw they have boxes that contain a lot of points.",
            "So basically you have a skew, especially when you have skewed distribution of data.",
            "So what we did instead.",
            "Oh, in addition, actually to be precise, is we also developed a load balancing approach for link discovery on the cloud.",
            "Basic idea is that we have two jobs that are consequently out.",
            "Basically that happened one after the other.",
            "First job computes is so called Q population matrix.",
            "The basic idea here is that this box tells us or the Q population metric tells us how many points you have and how many cubes and how large the indexes are.",
            "For giving tubes we can then use that information.",
            "2.",
            "Carry out a load balancing in a second job.",
            "So basically to decide which reduces going to get how much information to do the distance computations and regard we actually run.",
            "Faster as I will show in the results of the experiments."
        ],
        [
            "So what have you got so far?",
            "We've got three different implementations of this same algorithm.",
            "Now we can actually fulfill our goal, which was to compare the runtimes on the different architectures, and to say when actually to use what type of hardware."
        ],
        [
            "So which hardware did we use for experiments for the CPU experiments?",
            "We used the 22 core server with AMD AMD Opteron, struck that two Giga Hertz for the GPU.",
            "Experiments obviously had a remote server that didn't have a GPU, so we needed an extra server with GPU's for the GPU experiments.",
            "We used an AMD Radeon 7870.",
            "We tried to compute units and 64 power threads and the host program run on an Intel Core I-7377-O with Vikram and Linux 12:10 and to ensure that we could actually compare the results that we got on the 1st and the 2nd, we run exactly this in Java Code X on this machine as well to have a scaling factor for the results and in the third.",
            "Case, which is basically the cloud we use 10C1 medium nodes for the small experiments.",
            "That is for the three datasets I'm going to 2nd and 30 C one large nodes.",
            "For the large experiment that is for the scalability experiments."
        ],
        [
            "As I pointed out before, we needed to scale the results because we use different hardware.",
            "So we run the same code on different machines and we also notice that it's quite a difference whether you implement the same code in C++ or in Java.",
            "Also get quite a bit of runtime difference there, so we did that and we evaluated the scalability of the implementations across different hardware, especially with a large data set that contained 6 million points.",
            "And I said before we had four of those.",
            "The first data set was from DB pedia.",
            "It was basically all the entities that hard in minimal, medium and maximum innovation.",
            "We looked also at two points in the period of latitude and longitude and we also took link to data there.",
            "We took 500,000 points that have lasted longer than 6 million for the Sky Bridge experiments."
        ],
        [
            "So what do our results say now?",
            "The results on the three dimensional data set actually showed that yeah, you have to remember that we do have to reschedule the results basically, But what the results suggest is that for these small amount of data really, there's no point going for any kind of GPU implementation.",
            "It's sufficient to just do some thread pooling and you're fine."
        ],
        [
            "Their results on the second data set actually already show that the GPU, though, does scale better when you have two dimensions.",
            "As you can see if you just look at the way the run times for the for the Java.",
            "So what I forgot to point out is that Java 1 means Java running on one core and two cores on focus on 8 cores.",
            "There we have GPU map reduce and map reduce with load balancing.",
            "As we can see here.",
            "Actually simply does make sense to use the GPU even from here.",
            "On and we found a breaking point.",
            "Our own 10 to 8 results.",
            "So basically when you have when you assume that you're going to have tend to tend to the results or more, please use a GPU.",
            "Implementation seems to be fast."
        ],
        [
            "And basically, these results were corroborated by the experiments that we run on the third data set."
        ],
        [
            "Now respect to the scalability experiment.",
            "What we were actually able to show is that for the cloud setting that we got, which is 30 computer that is 30 nodes with load balancing.",
            "We can actually use this architecture when we go from 10 to 10 results upwards.",
            "That's basically kind of the cut off where we say then we actually should use a cloud implementation, even if you're dealing with a lot of data."
        ],
        [
            "OK, let me just tie up the loose ends here."
        ],
        [
            "So our question was when should we use which architecture for link discovery we implemented H R3 on different hardware and we provided by this means the first implementation of linked discovery on GPU's and we also device their load balancing approach for linking and our results basically suggests that for small tasks and those are actually most of the tasks that will carry out because usually link respect to classes.",
            "Is sufficient to use local resources.",
            "It is.",
            "It only makes sense to use the cloud when we go way beyond the 10 to the 9:10 to the 10 results."
        ],
        [
            "The insights that we get is that load balancing is central for actually using the cloud.",
            "If you remember this picture."
        ],
        [
            "You're basically see that the load the curve with load balancing and occur without load balancing.",
            "That's quite a difference there, so it's important to do work in that direction."
        ],
        [
            "Forest and.",
            "What is also interesting is that I'll be telling you a completely different story.",
            "If we had had another boss for GPU, so that's actually something that we might want to think about.",
            "Whether we can influence the vendors to actually give US buses that go maybe at fireware speed, then we could actually run even experiments that have 10 to 10 results.",
            "Look on local resources, and we need to actually access the cloud, so our major insights, what the Act was that the accurate use of local resources is sufficient for most of the current applications that.",
            "We have respecta link discovery."
        ],
        [
            "But the vision behind the whole thing is actually that we want to at the end have self adaptive link Discovery 6 systems that can check for free hardware and use all the free resources that they can get to be as time efficient as possible.",
            "So that's basically the vision behind the whole work that we've been carrying out.",
            "What we still need to do in this direction is develop orchestration approaches and are through."
        ],
        [
            "I'm almost done through in a picture of a user interface that we've actually developed to show that we also think about end users."
        ],
        [
            "And that was it for my part.",
            "Thank you very much.",
            "So can you go back to your graph with the?",
            "The final datasets on the large graph.",
            "That one yeah.",
            "So you said that it's better to do cloud or essentially the MapReduce version than the GPU version, but your GPU line is under the cloud version, right?",
            "So I don't understand why you would suggest to go to the cloud there, yes?"
        ],
        [
            "Sure, these are the results with 20 nodes.",
            "Actually there's the the results with the line with 30 nodes of missing.",
            "You're absolutely right, this is the old picture.",
            "That's my mistake.",
            "We run the experiments with different configurations.",
            "So basically 10 N 20 nodes and 30 nodes, and we figured out that we 30 nodes actually you get under the GPU curve, But that's basically set up that I gave before.",
            "So yeah, definitely there's a book in the in the picture, but we study notes graph.",
            "Yes, what I'm saying is this is the graph for 20 notes.",
            "And actually when you run with 30 note to get under the curve around 9:50 results after it right?",
            "Thank you.",
            "Some benchmarks will take costs into account, so I'm wondering, have you evaluated evaluated your experience with regarding costs, or how much would an approach of using this map reduce framework increased the cost?",
            "That is a very good question.",
            "We had long discussions on whether to have a cost based model in here as well.",
            "We did not consider that for the moment simply be cause.",
            "Actually it is usually cheaper to run things locali and our experiments suggest that.",
            "It is also better to run them locally.",
            "So basically the cost base model would have brought much more results with respect to what we figured out, so that's the reason why we didn't consider that so.",
            "This seems you've tackled here.",
            "Very specific problem which were basically this link discovery where you have these pairwise comparisons between resources.",
            "So first would be whether you consider also some maybe more complex measures for link discovery where you have.",
            "Maybe consider the environment in a graph of the so which is not just pairwise comparisons over or or any other problems related to semantic web, which you could basically benefit from these experiments.",
            "So the basically he do generalize, I think about generalizing.",
            "These are applying this to other problems and we have.",
            "We've actually done some work on checking first number one on other measures 'cause basically H R3 specific to certain types of spaces.",
            "And we basically want to see what happens when you use triggers, similarities and so on and so forth.",
            "The results are not yet completely still running the experiments, but so far it seems as if you have similar results there.",
            "Is she as I showed in the beginning, you usually have very complex in specifications.",
            "So the question is basically what happens when you run really large link specifications.",
            "But what you figure out is if you look at the link specification, grammar usually have atomic parts that are combined by different operators.",
            "So basically if you have results on the atoms, we can actually generalize them on the results for the holding specifications.",
            "OK, thanks.",
            "Thank you very much.",
            "Thanks again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have three talks in this session and the first one will be on went to reach for the cloud using parallel hardware for link discovery and it's going to be presented by Axel, so my name is Axel.",
                    "label": 1
                },
                {
                    "sent": "I'm from the University of Leipzig in Germany and I'm going to talk about when to reach for the cloud and this is John work with last call Michelle hard tongue on it, hard RAM from the database Department at the University and with Norman Hainan, CERN outward from the semantic Web.",
                    "label": 0
                },
                {
                    "sent": "Group.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first of all.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A bit of motivation why link discovery?",
                    "label": 1
                },
                {
                    "sent": "Why do we need that?",
                    "label": 1
                },
                {
                    "sent": "Well, basically links are the backbone for the architecture that we've chosen for the link data weapon that's central to actually use the link data weapon itself.",
                    "label": 0
                },
                {
                    "sent": "All sorts of applications rely on the fact that we have links between different knowledge bases.",
                    "label": 0
                },
                {
                    "sent": "For example, cross Ontology question answering when we ask for things like give me the side effects of medication related to a certain gene, say for example Fox B2.",
                    "label": 0
                },
                {
                    "sent": "We need we need links across different knowledge bases to be able to answer that question, we need links for data integration.",
                    "label": 0
                },
                {
                    "sent": "If we take two data sources and we put them together, we want to use the links to ensure that we don't have resources in our fuse knowledge base that refer to the same world entity but have different you arise.",
                    "label": 1
                },
                {
                    "sent": "We need links for large scale reasoning for Federated queries and so on and so forth, But if you look at the current topology of the linked open data cloud, we basically see that we have.",
                    "label": 1
                },
                {
                    "sent": "Quite a few triples dead 31 plus billion, but only approximately 0.5 billion links.",
                    "label": 0
                },
                {
                    "sent": "That is less than 2% and in most cases we have our same as links.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The question is now, why is then why is it so difficult to create links and actually we have two problems.",
                    "label": 1
                },
                {
                    "sent": "There refers problem is time complexity.",
                    "label": 0
                },
                {
                    "sent": "Most link discovery frameworks assume that they are given two sets as input is set of source resources.",
                    "label": 0
                },
                {
                    "sent": "A set of target resources and a certain relation R, and the idea is that they're trying to find a set M that is such that it is a subset of the Cartesian product of S&T and all the elements of the set actually bound by the relation art.",
                    "label": 0
                },
                {
                    "sent": "So basically RFC holds.",
                    "label": 0
                },
                {
                    "sent": "For all the elements of M. Now, in most cases it is quite tricky to compute that, So what you usually do is we try to approximate this September set in prime, where we say we have a complex similarity function and we say the elements of M prime are such that the similarity between S&T is larger or equal to a certain set threshold data.",
                    "label": 0
                },
                {
                    "sent": "Or the other way around.",
                    "label": 0
                },
                {
                    "sent": "We use a distance function.",
                    "label": 0
                },
                {
                    "sent": "Then we see the distance between S&T is less or equal to a certain threshold data and this is obviously quadratic.",
                    "label": 0
                },
                {
                    "sent": "If we use it would actually.",
                    "label": 0
                },
                {
                    "sent": "Problem if we use native approach.",
                    "label": 0
                },
                {
                    "sent": "We've actually computed that.",
                    "label": 0
                },
                {
                    "sent": "So if you assume that we want to link cities from DB pedia with cities from geonames and one comparison costs one millisecond, we're already need 69 days just for the cities.",
                    "label": 1
                },
                {
                    "sent": "And imagine you want to do that for the whole linked open data cloud, or even more for all sorts of link data sources.",
                    "label": 0
                },
                {
                    "sent": "You obviously need ages for that, so the time complexity problem is definitely one that we need to deal with.",
                    "label": 0
                },
                {
                    "sent": "The second part.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is actually the complexity of specifications, so is not sufficient to have specifications.",
                    "label": 1
                },
                {
                    "sent": "We want to have good specifications, good specifications obviously being specifications that betray high precision and recall respect to the links that we generate, and this little one here.",
                    "label": 0
                },
                {
                    "sent": "Actually you can use to link directors for LinkedIn DB DB Pedia.",
                    "label": 0
                },
                {
                    "sent": "But I'm not going to talk about that today.",
                    "label": 0
                },
                {
                    "sent": "So if you want to hear things about how to deal with the complexity of ring specifications, come to talk about koala bears and I'll tell you what koalas and link specifications.",
                    "label": 0
                },
                {
                    "sent": "Have to do with each other, but today we're going to focus on the time complexity issue.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, how do we deal with that?",
                    "label": 0
                },
                {
                    "sent": "What are solutions to deal with the time complexity issue?",
                    "label": 1
                },
                {
                    "sent": "First idea is obviously let us develop better algorithms.",
                    "label": 1
                },
                {
                    "sent": "Algorithms are faster algorithms that do not need so much time to carry out the link.",
                    "label": 0
                },
                {
                    "sent": "The link Discovery, now quite a show algorithms have been developed for that purpose.",
                    "label": 0
                },
                {
                    "sent": "We have different blocking approaches that try to portion of the space in which the link specification is an actually just to compare elements that are within the blocks.",
                    "label": 0
                },
                {
                    "sent": "We have specific algorithms that have been developed for particular metrics.",
                    "label": 0
                },
                {
                    "sent": "For example the people join algorithm for three grams overlaps and so on, and the edge join algorithm for Levenshtein distances.",
                    "label": 0
                },
                {
                    "sent": "And we've actually been able to develop algorithms that are near optimal respect to their reduction ratio.",
                    "label": 0
                },
                {
                    "sent": "Basically means that they don't even carry out more.",
                    "label": 0
                },
                {
                    "sent": "They don't carry out too many computations with respect to the minimal number of computations that they have to carry out to actually be able to compute links.",
                    "label": 0
                },
                {
                    "sent": "But still, even having such algorithms, we still need a lot of time when we have really large.",
                    "label": 0
                },
                {
                    "sent": "Link discovery tasks, which basically means that we need to go a step further.",
                    "label": 0
                },
                {
                    "sent": "The step further here is to use parallel hardware where the basic idea is we try to distribute the task over a lot of machines and basically to have the whole thing running parallel so that we can carry out the link specification or actually the link discovery in a reduced amount of time.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the interesting thing here is that over the years we've developed quite a few different architectures or different approaches to deal with Link discovery over parallel hardware.",
                    "label": 0
                },
                {
                    "sent": "So basically we have the simple threat pools.",
                    "label": 0
                },
                {
                    "sent": "We have GPU's that we can use.",
                    "label": 0
                },
                {
                    "sent": "We have the cloud and these hardware parallel hardware approaches have very different architectures, so we have hardware that rely on shared memory.",
                    "label": 0
                },
                {
                    "sent": "For example, in the case with Red Bulls.",
                    "label": 0
                },
                {
                    "sent": "But have great memories where we have a combination of shared memory and distributed memory, and we have obviously distributed memory.",
                    "label": 0
                },
                {
                    "sent": "In the case of the cloud, we also have hardware that have different requirements on respected execution paths.",
                    "label": 1
                },
                {
                    "sent": "So for example, here each thread can do basically whatever it wants while it's on GPU's.",
                    "label": 0
                },
                {
                    "sent": "We do have restrictions there.",
                    "label": 0
                },
                {
                    "sent": "Certain threads after basically follow the same execution paths and also the location of the hardware is different.",
                    "label": 0
                },
                {
                    "sent": "We have remote hardware in the case of the cloud.",
                    "label": 0
                },
                {
                    "sent": "And local hardware.",
                    "label": 0
                },
                {
                    "sent": "When you do, the thread pools or when you use GPS, although they also obviously GPU farms on the cloud.",
                    "label": 0
                },
                {
                    "sent": "They're not going to look at them in this talk, so the question that we had was then when should we use which hardware?",
                    "label": 1
                },
                {
                    "sent": "So we know by using parallelism we can actually improve the runtime, but when is it actually interesting to go to the cloud?",
                    "label": 0
                },
                {
                    "sent": "Or once we use GPS and so on?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The premises for work was if we have an algorithm that run all three hardwares we can actually compare the run times of the algorithm on.",
                    "label": 1
                },
                {
                    "sent": "In all these cases and find break even points where we say OK from this point on, please do use this particular type of artwork.",
                    "label": 1
                },
                {
                    "sent": "The main problem that we had to solve for the first friend I've had to solve actually is that there was no such implementation of algorithms that run on all throughout West for Link Discovery.",
                    "label": 0
                },
                {
                    "sent": "So basically you had to build 1.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We pick H R3 because it is near reduction ratio optimal, and we implemented each or three on both thread pools, GPU's and also had a MapReduce version for the cloud and then we were able actually 2.",
                    "label": 1
                },
                {
                    "sent": "Achieve our goal, which is to compare the runtime on all three parkitecture and such as break even points to basically say when to use what.",
                    "label": 1
                },
                {
                    "sent": "And what I'm going to talk about now.",
                    "label": 0
                },
                {
                    "sent": "It's basically present the algorithm shortly, talk about a different implementations, and then present the evaluation results that we got.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what is each of three H?",
                    "label": 0
                },
                {
                    "sent": "R3 assumes that the link specifications is is in a space that has a Minkowski metric in cost metric is defined this way.",
                    "label": 0
                },
                {
                    "sent": "It's basically a distant function that is of the form square root.",
                    "label": 0
                },
                {
                    "sent": "The PTH root of the sum I from one to N of S I -- T I to the power of P. And what chapter actually uses the fact that these components are independent, which basically means that we can actually greyed out the space that would.",
                    "label": 0
                },
                {
                    "sent": "Working in and try to approximate circles with cubes.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's look at an example here.",
                    "label": 0
                },
                {
                    "sent": "So we assume that we have here 2 dimensions, longitude and latitude, and the first thing that it's at three does is create a grid of with Delta and Delta is nothing else than terror divided by Alpha.",
                    "label": 0
                },
                {
                    "sent": "In this case Alpha is 4 and we call our father granularity parameter.",
                    "label": 0
                },
                {
                    "sent": "Formally, what we can show is that H R3, the reduction ratio of HR3 is optimal, where Alpha goes towards Infinity.",
                    "label": 0
                },
                {
                    "sent": "So once we've got that, what once we've got degreed.",
                    "label": 0
                },
                {
                    "sent": "We can actually use the fact that the link specification as I defined it before, which says Delta St is less or equal to Terra, actually defines a hyperbowl in this particular space.",
                    "label": 0
                },
                {
                    "sent": "Now, now that we know that we have your hyperbowl, we can approximate the hyperbowl by using a hypercube, and hypercubes are obviously way easier to compute than than hyperballs.",
                    "label": 0
                },
                {
                    "sent": "We define the hypercube that is such that it contains all the elements of the hyper sphere and we basically say we're going to carry out the link.",
                    "label": 0
                },
                {
                    "sent": "Discovery within the hyper cube.",
                    "label": 0
                },
                {
                    "sent": "So basically for all the elements that are within this little square in here we know that all the other points that have touched that Delta St or Delta BT is less or equal to terror will be found within this hypercube.",
                    "label": 0
                },
                {
                    "sent": "Now what H R3 does is instead of just using a hypercube.",
                    "label": 0
                },
                {
                    "sent": "Actually also has an indexing function that allows to get rid of some portions of the cube that will actually never contain elements.",
                    "label": 0
                },
                {
                    "sent": "That solves that satisfy the condition that we have so.",
                    "label": 0
                },
                {
                    "sent": "We use the indexing function.",
                    "label": 0
                },
                {
                    "sent": "We discard portions of direction.",
                    "label": 1
                },
                {
                    "sent": "In these cases.",
                    "label": 0
                },
                {
                    "sent": "These corners and based on this discarding of functions, we can actually show that if Alpha goes to Infinity, we can approximate the hypersphere perfectly and will never compare things that do not need to be compared.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is shot through there.",
                    "label": 0
                },
                {
                    "sent": "If you want all the technical details, there's a paper from last year that describes the full algorithm and proves that is optimal.",
                    "label": 0
                },
                {
                    "sent": "Now the question is how do you implement this algorithm on parallel hardware?",
                    "label": 0
                },
                {
                    "sent": "Now 10 minutes left.",
                    "label": 0
                },
                {
                    "sent": "That's incredible so.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, how do you implement this on prior hardware?",
                    "label": 0
                },
                {
                    "sent": "Obviously what you want to do is you take each of the cubes.",
                    "label": 0
                },
                {
                    "sent": "Here you give it to a thread and say number one.",
                    "label": 0
                },
                {
                    "sent": "Compute this box to find other points.",
                    "label": 0
                },
                {
                    "sent": "I need to compare the elements of this cube with a #2.",
                    "label": 0
                },
                {
                    "sent": "Reasons and give me the mapping for this particular cube.",
                    "label": 0
                },
                {
                    "sent": "So basically you go through all the cubes and you have your results.",
                    "label": 0
                },
                {
                    "sent": "That's basically how you implement it when you want to run for threads now.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you try it, you to implement this exactly that way.",
                    "label": 0
                },
                {
                    "sent": "On GPU's you'll run into massive time problems and that is simply due to the architecture of GPU's.",
                    "label": 0
                },
                {
                    "sent": "Basically inside deep use what you find our so called work items that are those are simple compute units that actually follow a certain path is that in program path.",
                    "label": 0
                },
                {
                    "sent": "Now the word atoms are joined in workgroups.",
                    "label": 0
                },
                {
                    "sent": "And this work groups share a bit of a certain amount of memory, and Jaden has several work groups in the GPU.",
                    "label": 0
                },
                {
                    "sent": "The GPU then communicates with the rest of the computer via the PCI Express bus, and that is actually the main problem that's the main bottleneck in the GPU architecture.",
                    "label": 0
                },
                {
                    "sent": "You basically have the problem that the PCI Express bus is really slow, so if you want to do large scale link discovery, your bottleneck is actually getting the data into the GPU.",
                    "label": 0
                },
                {
                    "sent": "So instead of implementing the algorithm the way suggested before 4 threads, what we did is we run the discretization.",
                    "label": 0
                },
                {
                    "sent": "So basically the grid computation on the CPU.",
                    "label": 0
                },
                {
                    "sent": "We ran the computation of the indexes on the GPU because we just.",
                    "label": 0
                },
                {
                    "sent": "Had the boxes then and we then run the comparisons back on the CPU, which basically means that we didn't have to transfer much data from this from the CPU to the GPU, just the box is basically under indexes.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How do you implement the same approach on the cloud?",
                    "label": 1
                },
                {
                    "sent": "The naive approach would basically consist of taking the input entities, running the indexing in the map jobs, and then running the distance computation to reduce jobs.",
                    "label": 0
                },
                {
                    "sent": "But why you basically figure out is that some of the map jobs or some of the rituals jobs actually take a lot of time or take the whole runtime becausw they have boxes that contain a lot of points.",
                    "label": 0
                },
                {
                    "sent": "So basically you have a skew, especially when you have skewed distribution of data.",
                    "label": 0
                },
                {
                    "sent": "So what we did instead.",
                    "label": 1
                },
                {
                    "sent": "Oh, in addition, actually to be precise, is we also developed a load balancing approach for link discovery on the cloud.",
                    "label": 1
                },
                {
                    "sent": "Basic idea is that we have two jobs that are consequently out.",
                    "label": 0
                },
                {
                    "sent": "Basically that happened one after the other.",
                    "label": 1
                },
                {
                    "sent": "First job computes is so called Q population matrix.",
                    "label": 0
                },
                {
                    "sent": "The basic idea here is that this box tells us or the Q population metric tells us how many points you have and how many cubes and how large the indexes are.",
                    "label": 0
                },
                {
                    "sent": "For giving tubes we can then use that information.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "Carry out a load balancing in a second job.",
                    "label": 0
                },
                {
                    "sent": "So basically to decide which reduces going to get how much information to do the distance computations and regard we actually run.",
                    "label": 0
                },
                {
                    "sent": "Faster as I will show in the results of the experiments.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what have you got so far?",
                    "label": 0
                },
                {
                    "sent": "We've got three different implementations of this same algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now we can actually fulfill our goal, which was to compare the runtimes on the different architectures, and to say when actually to use what type of hardware.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So which hardware did we use for experiments for the CPU experiments?",
                    "label": 0
                },
                {
                    "sent": "We used the 22 core server with AMD AMD Opteron, struck that two Giga Hertz for the GPU.",
                    "label": 0
                },
                {
                    "sent": "Experiments obviously had a remote server that didn't have a GPU, so we needed an extra server with GPU's for the GPU experiments.",
                    "label": 0
                },
                {
                    "sent": "We used an AMD Radeon 7870.",
                    "label": 0
                },
                {
                    "sent": "We tried to compute units and 64 power threads and the host program run on an Intel Core I-7377-O with Vikram and Linux 12:10 and to ensure that we could actually compare the results that we got on the 1st and the 2nd, we run exactly this in Java Code X on this machine as well to have a scaling factor for the results and in the third.",
                    "label": 1
                },
                {
                    "sent": "Case, which is basically the cloud we use 10C1 medium nodes for the small experiments.",
                    "label": 0
                },
                {
                    "sent": "That is for the three datasets I'm going to 2nd and 30 C one large nodes.",
                    "label": 0
                },
                {
                    "sent": "For the large experiment that is for the scalability experiments.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I pointed out before, we needed to scale the results because we use different hardware.",
                    "label": 1
                },
                {
                    "sent": "So we run the same code on different machines and we also notice that it's quite a difference whether you implement the same code in C++ or in Java.",
                    "label": 0
                },
                {
                    "sent": "Also get quite a bit of runtime difference there, so we did that and we evaluated the scalability of the implementations across different hardware, especially with a large data set that contained 6 million points.",
                    "label": 0
                },
                {
                    "sent": "And I said before we had four of those.",
                    "label": 0
                },
                {
                    "sent": "The first data set was from DB pedia.",
                    "label": 0
                },
                {
                    "sent": "It was basically all the entities that hard in minimal, medium and maximum innovation.",
                    "label": 0
                },
                {
                    "sent": "We looked also at two points in the period of latitude and longitude and we also took link to data there.",
                    "label": 1
                },
                {
                    "sent": "We took 500,000 points that have lasted longer than 6 million for the Sky Bridge experiments.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what do our results say now?",
                    "label": 0
                },
                {
                    "sent": "The results on the three dimensional data set actually showed that yeah, you have to remember that we do have to reschedule the results basically, But what the results suggest is that for these small amount of data really, there's no point going for any kind of GPU implementation.",
                    "label": 0
                },
                {
                    "sent": "It's sufficient to just do some thread pooling and you're fine.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Their results on the second data set actually already show that the GPU, though, does scale better when you have two dimensions.",
                    "label": 1
                },
                {
                    "sent": "As you can see if you just look at the way the run times for the for the Java.",
                    "label": 1
                },
                {
                    "sent": "So what I forgot to point out is that Java 1 means Java running on one core and two cores on focus on 8 cores.",
                    "label": 0
                },
                {
                    "sent": "There we have GPU map reduce and map reduce with load balancing.",
                    "label": 0
                },
                {
                    "sent": "As we can see here.",
                    "label": 0
                },
                {
                    "sent": "Actually simply does make sense to use the GPU even from here.",
                    "label": 0
                },
                {
                    "sent": "On and we found a breaking point.",
                    "label": 0
                },
                {
                    "sent": "Our own 10 to 8 results.",
                    "label": 0
                },
                {
                    "sent": "So basically when you have when you assume that you're going to have tend to tend to the results or more, please use a GPU.",
                    "label": 0
                },
                {
                    "sent": "Implementation seems to be fast.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And basically, these results were corroborated by the experiments that we run on the third data set.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now respect to the scalability experiment.",
                    "label": 0
                },
                {
                    "sent": "What we were actually able to show is that for the cloud setting that we got, which is 30 computer that is 30 nodes with load balancing.",
                    "label": 1
                },
                {
                    "sent": "We can actually use this architecture when we go from 10 to 10 results upwards.",
                    "label": 0
                },
                {
                    "sent": "That's basically kind of the cut off where we say then we actually should use a cloud implementation, even if you're dealing with a lot of data.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let me just tie up the loose ends here.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our question was when should we use which architecture for link discovery we implemented H R3 on different hardware and we provided by this means the first implementation of linked discovery on GPU's and we also device their load balancing approach for linking and our results basically suggests that for small tasks and those are actually most of the tasks that will carry out because usually link respect to classes.",
                    "label": 1
                },
                {
                    "sent": "Is sufficient to use local resources.",
                    "label": 0
                },
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "It only makes sense to use the cloud when we go way beyond the 10 to the 9:10 to the 10 results.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The insights that we get is that load balancing is central for actually using the cloud.",
                    "label": 0
                },
                {
                    "sent": "If you remember this picture.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You're basically see that the load the curve with load balancing and occur without load balancing.",
                    "label": 0
                },
                {
                    "sent": "That's quite a difference there, so it's important to do work in that direction.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Forest and.",
                    "label": 0
                },
                {
                    "sent": "What is also interesting is that I'll be telling you a completely different story.",
                    "label": 0
                },
                {
                    "sent": "If we had had another boss for GPU, so that's actually something that we might want to think about.",
                    "label": 0
                },
                {
                    "sent": "Whether we can influence the vendors to actually give US buses that go maybe at fireware speed, then we could actually run even experiments that have 10 to 10 results.",
                    "label": 0
                },
                {
                    "sent": "Look on local resources, and we need to actually access the cloud, so our major insights, what the Act was that the accurate use of local resources is sufficient for most of the current applications that.",
                    "label": 1
                },
                {
                    "sent": "We have respecta link discovery.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But the vision behind the whole thing is actually that we want to at the end have self adaptive link Discovery 6 systems that can check for free hardware and use all the free resources that they can get to be as time efficient as possible.",
                    "label": 1
                },
                {
                    "sent": "So that's basically the vision behind the whole work that we've been carrying out.",
                    "label": 1
                },
                {
                    "sent": "What we still need to do in this direction is develop orchestration approaches and are through.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm almost done through in a picture of a user interface that we've actually developed to show that we also think about end users.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that was it for my part.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "So can you go back to your graph with the?",
                    "label": 0
                },
                {
                    "sent": "The final datasets on the large graph.",
                    "label": 0
                },
                {
                    "sent": "That one yeah.",
                    "label": 0
                },
                {
                    "sent": "So you said that it's better to do cloud or essentially the MapReduce version than the GPU version, but your GPU line is under the cloud version, right?",
                    "label": 0
                },
                {
                    "sent": "So I don't understand why you would suggest to go to the cloud there, yes?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sure, these are the results with 20 nodes.",
                    "label": 0
                },
                {
                    "sent": "Actually there's the the results with the line with 30 nodes of missing.",
                    "label": 0
                },
                {
                    "sent": "You're absolutely right, this is the old picture.",
                    "label": 0
                },
                {
                    "sent": "That's my mistake.",
                    "label": 0
                },
                {
                    "sent": "We run the experiments with different configurations.",
                    "label": 0
                },
                {
                    "sent": "So basically 10 N 20 nodes and 30 nodes, and we figured out that we 30 nodes actually you get under the GPU curve, But that's basically set up that I gave before.",
                    "label": 0
                },
                {
                    "sent": "So yeah, definitely there's a book in the in the picture, but we study notes graph.",
                    "label": 0
                },
                {
                    "sent": "Yes, what I'm saying is this is the graph for 20 notes.",
                    "label": 0
                },
                {
                    "sent": "And actually when you run with 30 note to get under the curve around 9:50 results after it right?",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Some benchmarks will take costs into account, so I'm wondering, have you evaluated evaluated your experience with regarding costs, or how much would an approach of using this map reduce framework increased the cost?",
                    "label": 0
                },
                {
                    "sent": "That is a very good question.",
                    "label": 0
                },
                {
                    "sent": "We had long discussions on whether to have a cost based model in here as well.",
                    "label": 0
                },
                {
                    "sent": "We did not consider that for the moment simply be cause.",
                    "label": 0
                },
                {
                    "sent": "Actually it is usually cheaper to run things locali and our experiments suggest that.",
                    "label": 0
                },
                {
                    "sent": "It is also better to run them locally.",
                    "label": 0
                },
                {
                    "sent": "So basically the cost base model would have brought much more results with respect to what we figured out, so that's the reason why we didn't consider that so.",
                    "label": 0
                },
                {
                    "sent": "This seems you've tackled here.",
                    "label": 0
                },
                {
                    "sent": "Very specific problem which were basically this link discovery where you have these pairwise comparisons between resources.",
                    "label": 0
                },
                {
                    "sent": "So first would be whether you consider also some maybe more complex measures for link discovery where you have.",
                    "label": 0
                },
                {
                    "sent": "Maybe consider the environment in a graph of the so which is not just pairwise comparisons over or or any other problems related to semantic web, which you could basically benefit from these experiments.",
                    "label": 0
                },
                {
                    "sent": "So the basically he do generalize, I think about generalizing.",
                    "label": 0
                },
                {
                    "sent": "These are applying this to other problems and we have.",
                    "label": 0
                },
                {
                    "sent": "We've actually done some work on checking first number one on other measures 'cause basically H R3 specific to certain types of spaces.",
                    "label": 0
                },
                {
                    "sent": "And we basically want to see what happens when you use triggers, similarities and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "The results are not yet completely still running the experiments, but so far it seems as if you have similar results there.",
                    "label": 0
                },
                {
                    "sent": "Is she as I showed in the beginning, you usually have very complex in specifications.",
                    "label": 0
                },
                {
                    "sent": "So the question is basically what happens when you run really large link specifications.",
                    "label": 0
                },
                {
                    "sent": "But what you figure out is if you look at the link specification, grammar usually have atomic parts that are combined by different operators.",
                    "label": 0
                },
                {
                    "sent": "So basically if you have results on the atoms, we can actually generalize them on the results for the holding specifications.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Thanks again.",
                    "label": 0
                }
            ]
        }
    }
}