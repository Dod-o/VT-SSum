{
    "id": "6nknu6a24tv3v4bpuvivkivhhz7ktngq",
    "title": "Strider: A Hybrid Adaptive Distributed RDF Stream Processing Engine",
    "info": {
        "author": [
            "Xiangnan Ren, ATOS"
        ],
        "published": "Nov. 28, 2017",
        "recorded": "October 2017",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2017_ren_processing_engine/",
    "segmentation": [
        [
            "OK, thanks for the introduction and young man from Paris, France and I'm with PhD student in University of Paris East and I'm also a research engineering adults.",
            "Today I can present my work Strider hybrid adapt hybrid distributed RDF stream processing engine, which is cooperated with my PhD supervisor Olivier.",
            "And."
        ],
        [
            "This presentation consists of six parts and I will start by giving, giving some general context information and the other details will be also given in the first slide slide step by step.",
            "So the 1st."
        ],
        [
            "Question we want to ask is why when you stream processing because in nowadays the string is everywhere.",
            "Many applications requires the real time aspect.",
            "For example the social network and the financial transaction and education system.",
            "And of course for the Internet of Things ecosystems so."
        ],
        [
            "So an so for us.",
            "This work is sponsored by a French national environment project waves if U I-17 and idea of this project is to build industry is to build a distributed distributed stream processing platform for industrial water resource management.",
            "So we collect the sensor data from various locations in Paris and we process them in real time and try to do some advanced data analytics.",
            "For example the automatic detection or the water risk.",
            "Water consumption prediction.",
            "So the next question will be why we need to pray?",
            "Present the streaming data in RDF formats.",
            "I believe there are two main reasons.",
            "The first one, it allows us to facilitate the data integration from Heat Regina Resource and the second one will be.",
            "It allows us to do some reasoning tasks over the real time data data stream so."
        ],
        [
            "Here I want to show you some knowledge about the RDF stream processing ecosystem.",
            "So when we talk about the ISP engines, there are two main categories.",
            "The first one is a centralized system like very new and Sparkle sequel.",
            "Sparkle string, or it'll list and the second category would be a distributed system like the sequels cloud or cuts.",
            "The reason why we do not choose the existing RSP engines for our project is because we believe that none of them cover the scalability and reasoning and adaptivity.",
            "And here adaptivity I mean, it refers to three.",
            "It refers to the capacity of engine which can adjust the query execution plan to run in real time based on the character characteristic of the data stream.",
            "So."
        ],
        [
            "So you know, for strings for streaming services, normally it will run 24 by 7 and in ideal situation we can assume that the data structure will never change.",
            "However, in practical use case it's not possible be cause for example, the sensor can emit the different type of messages, different type or a different number of messages in different period of time.",
            "So the change list query execution plan potentially lead sharp.",
            "Performance degration now I want to show."
        ],
        [
            "To use architecture view about the system design and."
        ],
        [
            "Trader is viewed on top of is built on 12 with Spark and Kafka which are two very famous distributed computing framework and there are two men.",
            "Modular insider.",
            "The first part is the data flow management.",
            "So we collect the sensor data anways summarized data into RDF more formats and listen to the Kafka message brokers.",
            "After that we continuously send the RDF data stream to spark streaming and for the computing Core Insider it has several layers.",
            "The first layer is the request layer which allows user to.",
            "Register the continuous sparkle queries an once we have.",
            "We received register query.",
            "We will pass the query and obtain its query algebra tree.",
            "After that we will push in the push the query algebra tree into the query optimizer and in our query optimizer we have two main types of query optimization strategy which is the static static optimization and dynamic dynamic query optimizations.",
            "And for the dynamic, which is also the adaptive query optimization, it has two subtypes of query optimization strategy.",
            "One the first one is forward and the second one is backward and for more details I will give you.",
            "I will give you the more information in the next part of this presentation.",
            "So once the query algebra tree has been optimized, we continuously push the algebra into spark streaming and we process the incoming data string.",
            "And finally we think.",
            "Result of each queries into Hadoop distributed file System or Kafka?"
        ],
        [
            "So here I want to give you a quick view about the syntax were using insider.",
            "First you need to specify or streaming clause and it's just I mean just to you.",
            "It is just used to initialize the spark streaming context and it also concerns the basic operator to configure your windowing operator.",
            "After that you need to write a register clause and it concerns the basic configuration about Europe."
        ],
        [
            "There is for example the query ID and the reasoning options.",
            "So here I want to discuss the query processing model.",
            "We're using strider."
        ],
        [
            "As what I have mentioned, the straighter user a set of hybrid strategy to optimize the query algebra tree at runtime.",
            "So there are two categories.",
            "The first one is static optimization and which essentially is just a heuristic rule based query optimization strategy and for this presentation I want to focus on the second type we apply in our query optimize query optimizer which is a dynamic query optimization strategy.",
            "Ann is it is a cost based query optimization strategy we use and there are two subtypes for."
        ],
        [
            "The dynamic optimization.",
            "The first one is backward and 2nd wines forward.",
            "So how does this works?",
            "So let's look at one example.",
            "Once you got a query, the first thing you need to do is construct it's undirected connected graph and someone call it as a join graph so that."
        ],
        [
            "Step if we look at in the streaming context.",
            "For example here we got two Windows WNW N + 1 two consecutive Windows and each window consists of three RDS and the first step we need to trigger the query execution an we always evaluate query in a bottom up manner.",
            "So the first thing."
        ],
        [
            "Need to compute is all the triple pattern, so once once the triple pattern has have been computed, computed and we need to collect the statistique of of each triple patterns.",
            "And here it refers to the cardinality of each one."
        ],
        [
            "Next we need to refer the cardinality of the joint patent by its two triple patterns and due to the time limitation I cannot show you the cost model where used in Strider, but if you are interested in you can we can discuss this later after the presentation.",
            "So and after that we applied Floyd Warshall algorithm to find the optimal path covering usage graph and then."
        ],
        [
            "We need to have have to specify the access points and this axis points.",
            "It refers to the root note in the query algebra tree.",
            "So we construct the query Algebra algebra tree in the top down manner so."
        ],
        [
            "Finally, we can obtain our query execution plan.",
            "As you can see we from WN 2 W N + 1 the the position of this or just to join those have been switched so."
        ],
        [
            "So here I want to discuss why we need to distinguish forward and backward forward.",
            "Basically it means we use the static static information collected in current window to estimate and to generate the query execution plan for current window.",
            "For example, we gotta window WI.",
            "So we collect the status information from WI and we generate is query execution plan for Wii so."
        ],
        [
            "Backward it means way use statistique information which is collected in previous window and we try to estimate an generate the query execution plan for the next window.",
            "So for example here you can see we have two consecutive Windows wiw plus one and we collect the statistique from WI and we generate the query execution plan for Wii plus one."
        ],
        [
            "Here I briefly summarize the characteristics for these two approach.",
            "So as we can see, for backward approach the advantage is there is no extra overhead to generate the query execution plan and.",
            "We don't need to collect.",
            "We don't need to spend the time to collect the statistique.",
            "However, the drawback for backward approach is there.",
            "The statistique you collect may not be so precise becausw it based on the previous window on the other side for the forward approach we can see that the advantage of forward is the static is is is pretty pretty precise becauses just come from the most recent window and the drawback forward is that 'cause we have to cut down the query execution pipeline and we need to catch a lot of.",
            "Intermediate result, so in a distributed environment and such overhead is not negligible and sometimes it will cause abnormal abnormal garbage collector behavior.",
            "So and we can see that this to approach quite complementary, so we try to use a decision maker to switch one to another and he."
        ],
        [
            "There is an example.",
            "We have four consecutive Windows and we and the decision maker.",
            "It takes a parimeter.",
            "It's basically three shoot from zero to 1.",
            "So for each query execution we we compute a fraction number, the fraction number equals to the fraction of the total query execution time and the Windows sliding size.",
            "If this fraction number is smaller than the threshold ways.",
            "Keep the backward if if it is larger than this threshold, we switch to the forwards so the idea is always.",
            "The idea is always to adjust the query execution plan as soon as possible.",
            "So so here we can see the four consecutive windows they apply different strategies to.",
            "Support."
        ],
        [
            "Query evaluation, so let's look at some evaluations in accordance with Yahoo benchmark for distributed stream processing engine, we consider that the system throughput and query latency as the two most important factor in our micro benchmark.",
            "And here we take a data which we design like queries and we take the data set from it's our bench and another data set from data set from our project.",
            "We evaluated our system in Amazon Web service.",
            "With a cluster of nine Ozanne, 3 News for the data flow management at 6 nodes for Spark cluster and."
        ],
        [
            "Here we can see the Generalised Rider can achieve a million level throughput an we have also evaluated Strider in local mode and even on a single machine.",
            "Strategy is still out performs and state of the state of the R engine like sparkling sequels because this is because we are still benefits from the multicore multithreaded architectures."
        ],
        [
            "And for the latency result generally is rather can achieve a second level delays, which is also quite enough for use case.",
            "And here is another example for the adaptive query."
        ],
        [
            "Optimization and there are two curves, the blue one and the red one, the blue curve.",
            "It supplies the adaptive query optimization that we can see that it is much stable than the red curve, so so so.",
            "The idea is to avoid the sharp performance degration so we can see that in the blue curve the rate curve cannot adapt the.",
            "The query execution plan based on the input data stream structures."
        ],
        [
            "So just a quick introduction about the supported Spark operator in our system and if you are interested you can find this on our GitHub page and here is another feature."
        ],
        [
            "Sean, which is not introduced in our paper with support the reasoning on RDF sore and the OSM S and idea is inspired by our previous work, which is published in Big Data Conference 2015 and we still got them paper this year in Vail DB.",
            "So for quick conclusions first we."
        ],
        [
            "The the code source is available on GitHub, so for any country, any contribution I think is where is welcome and our future work includes.",
            "I mean how to reduce the premium latency and I think the multiple query optimization is is also a very interesting topic an I think the reactive system design maybe also a possible direction for research so."
        ],
        [
            "Q 4 rotation If you have any questions, I'm listening.",
            "Wondering about how you execute Sparkle on top of Spark?",
            "Yeah yeah.",
            "And on the architecture you presented that you have several query layers and so forth.",
            "So what is your mechanism of executing sparkle on top of Spark?",
            "Because as far as I know that right now that's a bit of a problem for Spark, I kako just repeal a little.",
            "I mean how do you execute sparkle queries on top of?",
            "Spark infrastructure.",
            "OK, I'll just show you the architecture we.",
            "So first step, we receive a sparkle query, so it basically is just a I mean a string, right?",
            "And we use Gina to parse this query and we will obtain its algebra tree and we really can't reconstruct the algebra tree and for each operator we bind to sparkle relation, Spark, Spark SQL relational operators.",
            "So that's the basic idea.",
            "So you should have done some mappings mechanism or some.",
            "Play somewhere where you store metadata for such mappings so.",
            "No, I don't think So what?",
            "What?",
            "No, I don't think so.",
            "Yeah, you store RDF data in Spark in memory, right?",
            "You don't store it physically.",
            "Yeah, then yeah.",
            "Then what is the mechanism of rewriting sparkle to RDD?",
            "And there's nothing.",
            "RDD, we what we use is better friend.",
            "So it I mean you just translate your sparkle query into the corresponding SQL query.",
            "That's all OK, OK, thank you.",
            "And this question, I think.",
            "Can you go to the syntax one moment, please?",
            "The Centex right?",
            "Yeah?",
            "So as far as I understand you do not do not have just one window per query.",
            "Yeah, so you put all the typos together, right?",
            "Yeah, so I think the problem is graph partitioning here.",
            "I mean the question is it seems you put everything together.",
            "How do you split the data across data frame in a way which is consistent to the intended semantics of sparkle?",
            "Like because you might have relation across data frames.",
            "You might lose this in your.",
            "Pretty answering, right?",
            "Yeah, I mean, I don't know.",
            "Maybe you solve this.",
            "Yeah, that's a problem we have seen in Spark Streaming.",
            "Be cause for a single Spark streaming application.",
            "You can only have one windowing operators, so the there's no way you can do this.",
            "You know, maybe you can initialize several consumers and each one consume the different topics, but multiple query multiple windowing operator is not supported.",
            "That's I mean, that's a problem, yeah, but for our use case, we do not involve such kind of things.",
            "OK, so my my follow up is that you should check as future work.",
            "Yeah, how far are you in terms of you know answers from playing sparkle on the same data set when you store all the data in a single single place?",
            "Assuming that you can of course you can.",
            "So my opinion is OK to approximate like this.",
            "OK, but you know show I think this was.",
            "What was a problem?",
            "I guess.",
            "OK, OK, thank you.",
            "And regarding the query rewriter to SQL, yeah you have to use the general API, right?",
            "Yeah, January for getting the translation between the algebra and SQL syntax.",
            "Yeah yeah.",
            "And since you use Windows here, did you consider that between two sequence Windows there could be latency?",
            "And if the data which in which comes and the throughput of the Internet is.",
            "Overhead, do you consider that the decision maker it's true or false?",
            "Yeah, I mean that yeah, we'll see.",
            "Yes, maybe a problem, I mean, but this could be that you miss some data which are coming on the latency when they are between 2 Windows.",
            "Yeah, I yeah it's possible here.",
            "Yeah, and another question.",
            "How big was the datasets which you have to hold?",
            "How big?",
            "How large was the datasets?",
            "So for the evaluation ways.",
            "This stream rate we have generated is around 100 and 200,000, two 120,000 three plus per second.",
            "And we have another paper and submit it in Big Data which is refused.",
            "So and it's we used an I mean more stressful test, so it's around 200,000 people per second and we can easily handle this.",
            "So thanks no problem.",
            "You compared against some systems but I couldn't read which systems they were.",
            "So I saw sequel S and others are sequels as sparkles, right?",
            "OK, so here these two.",
            "So you so?",
            "How were these competitors being set up?",
            "So we're just a machine or a cluster.",
            "So here as you can see we, I mean, for example the straight or DS, the D means distributed mode and LS means I mean local mode and ask means the static optimization.",
            "So we did the evaluation on a single machine.",
            "So here you can see here the result of throughput.",
            "The third line is the.",
            "Performance on a single machine.",
            "So you use 6 machine, so six time computation power.",
            "Notice what 1 version the local mode is just one more ship.",
            "Yes yes.",
            "When you compare this distributed against the local.",
            "Yeah, you're comparing 6 machines versus 1.",
            "Yeah, yeah, and there I see you are 50% faster 60%.",
            "Yeah, some kind of thing like that.",
            "OK, yeah, and then you said that for your use case is a second of latency is enough?",
            "Yes, she couldn't you achieve ever achieve that also with other competitors?",
            "I mean the latency.",
            "The reason I mean I have explained the reason why we do not record the latency forces broken sequels because basically it's not compareable.",
            "It's always very difficult to compare Huawei because the Seas Barco is.",
            "I mean if you generate a very high input stream rates and you will have some abnormal behavior and we just simply crash the system for example 100,000 rubles per second is very high and I'm sure you cannot do this.",
            "I mean you see Sparkle annefors equals it's eager execution, which is.",
            "I mean, it's a data driven, so I mean it's just not compatible.",
            "Yeah, thank you.",
            "I have a question on your data frame sort column design, right?",
            "OK, you mentioned that you started graph data in your data frames.",
            "So what are the columns?",
            "What are the types?",
            "Just roughly, what are the what are the column definitions?",
            "The column definition is SPO basic, I mean RDF representation subject, predicate, object.",
            "So you handled all those your eyes and literals.",
            "Yeah, single column.",
            "Yeah we also.",
            "I mean for example you got.",
            "A data type.",
            "You are I for double or I mean or float and we handle it manually.",
            "We do a runtime reflection to detect to detect the type of the data.",
            "OK, got it."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, thanks for the introduction and young man from Paris, France and I'm with PhD student in University of Paris East and I'm also a research engineering adults.",
                    "label": 0
                },
                {
                    "sent": "Today I can present my work Strider hybrid adapt hybrid distributed RDF stream processing engine, which is cooperated with my PhD supervisor Olivier.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This presentation consists of six parts and I will start by giving, giving some general context information and the other details will be also given in the first slide slide step by step.",
                    "label": 0
                },
                {
                    "sent": "So the 1st.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Question we want to ask is why when you stream processing because in nowadays the string is everywhere.",
                    "label": 1
                },
                {
                    "sent": "Many applications requires the real time aspect.",
                    "label": 0
                },
                {
                    "sent": "For example the social network and the financial transaction and education system.",
                    "label": 0
                },
                {
                    "sent": "And of course for the Internet of Things ecosystems so.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So an so for us.",
                    "label": 0
                },
                {
                    "sent": "This work is sponsored by a French national environment project waves if U I-17 and idea of this project is to build industry is to build a distributed distributed stream processing platform for industrial water resource management.",
                    "label": 1
                },
                {
                    "sent": "So we collect the sensor data from various locations in Paris and we process them in real time and try to do some advanced data analytics.",
                    "label": 0
                },
                {
                    "sent": "For example the automatic detection or the water risk.",
                    "label": 0
                },
                {
                    "sent": "Water consumption prediction.",
                    "label": 0
                },
                {
                    "sent": "So the next question will be why we need to pray?",
                    "label": 1
                },
                {
                    "sent": "Present the streaming data in RDF formats.",
                    "label": 0
                },
                {
                    "sent": "I believe there are two main reasons.",
                    "label": 0
                },
                {
                    "sent": "The first one, it allows us to facilitate the data integration from Heat Regina Resource and the second one will be.",
                    "label": 0
                },
                {
                    "sent": "It allows us to do some reasoning tasks over the real time data data stream so.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here I want to show you some knowledge about the RDF stream processing ecosystem.",
                    "label": 1
                },
                {
                    "sent": "So when we talk about the ISP engines, there are two main categories.",
                    "label": 0
                },
                {
                    "sent": "The first one is a centralized system like very new and Sparkle sequel.",
                    "label": 0
                },
                {
                    "sent": "Sparkle string, or it'll list and the second category would be a distributed system like the sequels cloud or cuts.",
                    "label": 0
                },
                {
                    "sent": "The reason why we do not choose the existing RSP engines for our project is because we believe that none of them cover the scalability and reasoning and adaptivity.",
                    "label": 1
                },
                {
                    "sent": "And here adaptivity I mean, it refers to three.",
                    "label": 0
                },
                {
                    "sent": "It refers to the capacity of engine which can adjust the query execution plan to run in real time based on the character characteristic of the data stream.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you know, for strings for streaming services, normally it will run 24 by 7 and in ideal situation we can assume that the data structure will never change.",
                    "label": 1
                },
                {
                    "sent": "However, in practical use case it's not possible be cause for example, the sensor can emit the different type of messages, different type or a different number of messages in different period of time.",
                    "label": 0
                },
                {
                    "sent": "So the change list query execution plan potentially lead sharp.",
                    "label": 0
                },
                {
                    "sent": "Performance degration now I want to show.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To use architecture view about the system design and.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trader is viewed on top of is built on 12 with Spark and Kafka which are two very famous distributed computing framework and there are two men.",
                    "label": 0
                },
                {
                    "sent": "Modular insider.",
                    "label": 0
                },
                {
                    "sent": "The first part is the data flow management.",
                    "label": 0
                },
                {
                    "sent": "So we collect the sensor data anways summarized data into RDF more formats and listen to the Kafka message brokers.",
                    "label": 0
                },
                {
                    "sent": "After that we continuously send the RDF data stream to spark streaming and for the computing Core Insider it has several layers.",
                    "label": 0
                },
                {
                    "sent": "The first layer is the request layer which allows user to.",
                    "label": 0
                },
                {
                    "sent": "Register the continuous sparkle queries an once we have.",
                    "label": 0
                },
                {
                    "sent": "We received register query.",
                    "label": 0
                },
                {
                    "sent": "We will pass the query and obtain its query algebra tree.",
                    "label": 0
                },
                {
                    "sent": "After that we will push in the push the query algebra tree into the query optimizer and in our query optimizer we have two main types of query optimization strategy which is the static static optimization and dynamic dynamic query optimizations.",
                    "label": 0
                },
                {
                    "sent": "And for the dynamic, which is also the adaptive query optimization, it has two subtypes of query optimization strategy.",
                    "label": 0
                },
                {
                    "sent": "One the first one is forward and the second one is backward and for more details I will give you.",
                    "label": 0
                },
                {
                    "sent": "I will give you the more information in the next part of this presentation.",
                    "label": 0
                },
                {
                    "sent": "So once the query algebra tree has been optimized, we continuously push the algebra into spark streaming and we process the incoming data string.",
                    "label": 0
                },
                {
                    "sent": "And finally we think.",
                    "label": 0
                },
                {
                    "sent": "Result of each queries into Hadoop distributed file System or Kafka?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here I want to give you a quick view about the syntax were using insider.",
                    "label": 0
                },
                {
                    "sent": "First you need to specify or streaming clause and it's just I mean just to you.",
                    "label": 0
                },
                {
                    "sent": "It is just used to initialize the spark streaming context and it also concerns the basic operator to configure your windowing operator.",
                    "label": 0
                },
                {
                    "sent": "After that you need to write a register clause and it concerns the basic configuration about Europe.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is for example the query ID and the reasoning options.",
                    "label": 0
                },
                {
                    "sent": "So here I want to discuss the query processing model.",
                    "label": 1
                },
                {
                    "sent": "We're using strider.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As what I have mentioned, the straighter user a set of hybrid strategy to optimize the query algebra tree at runtime.",
                    "label": 1
                },
                {
                    "sent": "So there are two categories.",
                    "label": 0
                },
                {
                    "sent": "The first one is static optimization and which essentially is just a heuristic rule based query optimization strategy and for this presentation I want to focus on the second type we apply in our query optimize query optimizer which is a dynamic query optimization strategy.",
                    "label": 0
                },
                {
                    "sent": "Ann is it is a cost based query optimization strategy we use and there are two subtypes for.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The dynamic optimization.",
                    "label": 0
                },
                {
                    "sent": "The first one is backward and 2nd wines forward.",
                    "label": 0
                },
                {
                    "sent": "So how does this works?",
                    "label": 0
                },
                {
                    "sent": "So let's look at one example.",
                    "label": 0
                },
                {
                    "sent": "Once you got a query, the first thing you need to do is construct it's undirected connected graph and someone call it as a join graph so that.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Step if we look at in the streaming context.",
                    "label": 0
                },
                {
                    "sent": "For example here we got two Windows WNW N + 1 two consecutive Windows and each window consists of three RDS and the first step we need to trigger the query execution an we always evaluate query in a bottom up manner.",
                    "label": 0
                },
                {
                    "sent": "So the first thing.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Need to compute is all the triple pattern, so once once the triple pattern has have been computed, computed and we need to collect the statistique of of each triple patterns.",
                    "label": 0
                },
                {
                    "sent": "And here it refers to the cardinality of each one.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next we need to refer the cardinality of the joint patent by its two triple patterns and due to the time limitation I cannot show you the cost model where used in Strider, but if you are interested in you can we can discuss this later after the presentation.",
                    "label": 0
                },
                {
                    "sent": "So and after that we applied Floyd Warshall algorithm to find the optimal path covering usage graph and then.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We need to have have to specify the access points and this axis points.",
                    "label": 0
                },
                {
                    "sent": "It refers to the root note in the query algebra tree.",
                    "label": 0
                },
                {
                    "sent": "So we construct the query Algebra algebra tree in the top down manner so.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, we can obtain our query execution plan.",
                    "label": 0
                },
                {
                    "sent": "As you can see we from WN 2 W N + 1 the the position of this or just to join those have been switched so.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here I want to discuss why we need to distinguish forward and backward forward.",
                    "label": 0
                },
                {
                    "sent": "Basically it means we use the static static information collected in current window to estimate and to generate the query execution plan for current window.",
                    "label": 0
                },
                {
                    "sent": "For example, we gotta window WI.",
                    "label": 0
                },
                {
                    "sent": "So we collect the status information from WI and we generate is query execution plan for Wii so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Backward it means way use statistique information which is collected in previous window and we try to estimate an generate the query execution plan for the next window.",
                    "label": 0
                },
                {
                    "sent": "So for example here you can see we have two consecutive Windows wiw plus one and we collect the statistique from WI and we generate the query execution plan for Wii plus one.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here I briefly summarize the characteristics for these two approach.",
                    "label": 0
                },
                {
                    "sent": "So as we can see, for backward approach the advantage is there is no extra overhead to generate the query execution plan and.",
                    "label": 0
                },
                {
                    "sent": "We don't need to collect.",
                    "label": 0
                },
                {
                    "sent": "We don't need to spend the time to collect the statistique.",
                    "label": 0
                },
                {
                    "sent": "However, the drawback for backward approach is there.",
                    "label": 0
                },
                {
                    "sent": "The statistique you collect may not be so precise becausw it based on the previous window on the other side for the forward approach we can see that the advantage of forward is the static is is is pretty pretty precise becauses just come from the most recent window and the drawback forward is that 'cause we have to cut down the query execution pipeline and we need to catch a lot of.",
                    "label": 0
                },
                {
                    "sent": "Intermediate result, so in a distributed environment and such overhead is not negligible and sometimes it will cause abnormal abnormal garbage collector behavior.",
                    "label": 0
                },
                {
                    "sent": "So and we can see that this to approach quite complementary, so we try to use a decision maker to switch one to another and he.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is an example.",
                    "label": 0
                },
                {
                    "sent": "We have four consecutive Windows and we and the decision maker.",
                    "label": 0
                },
                {
                    "sent": "It takes a parimeter.",
                    "label": 0
                },
                {
                    "sent": "It's basically three shoot from zero to 1.",
                    "label": 0
                },
                {
                    "sent": "So for each query execution we we compute a fraction number, the fraction number equals to the fraction of the total query execution time and the Windows sliding size.",
                    "label": 0
                },
                {
                    "sent": "If this fraction number is smaller than the threshold ways.",
                    "label": 0
                },
                {
                    "sent": "Keep the backward if if it is larger than this threshold, we switch to the forwards so the idea is always.",
                    "label": 0
                },
                {
                    "sent": "The idea is always to adjust the query execution plan as soon as possible.",
                    "label": 0
                },
                {
                    "sent": "So so here we can see the four consecutive windows they apply different strategies to.",
                    "label": 0
                },
                {
                    "sent": "Support.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Query evaluation, so let's look at some evaluations in accordance with Yahoo benchmark for distributed stream processing engine, we consider that the system throughput and query latency as the two most important factor in our micro benchmark.",
                    "label": 0
                },
                {
                    "sent": "And here we take a data which we design like queries and we take the data set from it's our bench and another data set from data set from our project.",
                    "label": 0
                },
                {
                    "sent": "We evaluated our system in Amazon Web service.",
                    "label": 0
                },
                {
                    "sent": "With a cluster of nine Ozanne, 3 News for the data flow management at 6 nodes for Spark cluster and.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we can see the Generalised Rider can achieve a million level throughput an we have also evaluated Strider in local mode and even on a single machine.",
                    "label": 0
                },
                {
                    "sent": "Strategy is still out performs and state of the state of the R engine like sparkling sequels because this is because we are still benefits from the multicore multithreaded architectures.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the latency result generally is rather can achieve a second level delays, which is also quite enough for use case.",
                    "label": 0
                },
                {
                    "sent": "And here is another example for the adaptive query.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Optimization and there are two curves, the blue one and the red one, the blue curve.",
                    "label": 0
                },
                {
                    "sent": "It supplies the adaptive query optimization that we can see that it is much stable than the red curve, so so so.",
                    "label": 0
                },
                {
                    "sent": "The idea is to avoid the sharp performance degration so we can see that in the blue curve the rate curve cannot adapt the.",
                    "label": 0
                },
                {
                    "sent": "The query execution plan based on the input data stream structures.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So just a quick introduction about the supported Spark operator in our system and if you are interested you can find this on our GitHub page and here is another feature.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sean, which is not introduced in our paper with support the reasoning on RDF sore and the OSM S and idea is inspired by our previous work, which is published in Big Data Conference 2015 and we still got them paper this year in Vail DB.",
                    "label": 0
                },
                {
                    "sent": "So for quick conclusions first we.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The the code source is available on GitHub, so for any country, any contribution I think is where is welcome and our future work includes.",
                    "label": 0
                },
                {
                    "sent": "I mean how to reduce the premium latency and I think the multiple query optimization is is also a very interesting topic an I think the reactive system design maybe also a possible direction for research so.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Q 4 rotation If you have any questions, I'm listening.",
                    "label": 0
                },
                {
                    "sent": "Wondering about how you execute Sparkle on top of Spark?",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "And on the architecture you presented that you have several query layers and so forth.",
                    "label": 0
                },
                {
                    "sent": "So what is your mechanism of executing sparkle on top of Spark?",
                    "label": 0
                },
                {
                    "sent": "Because as far as I know that right now that's a bit of a problem for Spark, I kako just repeal a little.",
                    "label": 0
                },
                {
                    "sent": "I mean how do you execute sparkle queries on top of?",
                    "label": 0
                },
                {
                    "sent": "Spark infrastructure.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll just show you the architecture we.",
                    "label": 0
                },
                {
                    "sent": "So first step, we receive a sparkle query, so it basically is just a I mean a string, right?",
                    "label": 0
                },
                {
                    "sent": "And we use Gina to parse this query and we will obtain its algebra tree and we really can't reconstruct the algebra tree and for each operator we bind to sparkle relation, Spark, Spark SQL relational operators.",
                    "label": 0
                },
                {
                    "sent": "So that's the basic idea.",
                    "label": 0
                },
                {
                    "sent": "So you should have done some mappings mechanism or some.",
                    "label": 0
                },
                {
                    "sent": "Play somewhere where you store metadata for such mappings so.",
                    "label": 0
                },
                {
                    "sent": "No, I don't think So what?",
                    "label": 0
                },
                {
                    "sent": "What?",
                    "label": 0
                },
                {
                    "sent": "No, I don't think so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you store RDF data in Spark in memory, right?",
                    "label": 0
                },
                {
                    "sent": "You don't store it physically.",
                    "label": 0
                },
                {
                    "sent": "Yeah, then yeah.",
                    "label": 0
                },
                {
                    "sent": "Then what is the mechanism of rewriting sparkle to RDD?",
                    "label": 0
                },
                {
                    "sent": "And there's nothing.",
                    "label": 0
                },
                {
                    "sent": "RDD, we what we use is better friend.",
                    "label": 0
                },
                {
                    "sent": "So it I mean you just translate your sparkle query into the corresponding SQL query.",
                    "label": 0
                },
                {
                    "sent": "That's all OK, OK, thank you.",
                    "label": 1
                },
                {
                    "sent": "And this question, I think.",
                    "label": 0
                },
                {
                    "sent": "Can you go to the syntax one moment, please?",
                    "label": 0
                },
                {
                    "sent": "The Centex right?",
                    "label": 0
                },
                {
                    "sent": "Yeah?",
                    "label": 0
                },
                {
                    "sent": "So as far as I understand you do not do not have just one window per query.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you put all the typos together, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I think the problem is graph partitioning here.",
                    "label": 0
                },
                {
                    "sent": "I mean the question is it seems you put everything together.",
                    "label": 0
                },
                {
                    "sent": "How do you split the data across data frame in a way which is consistent to the intended semantics of sparkle?",
                    "label": 0
                },
                {
                    "sent": "Like because you might have relation across data frames.",
                    "label": 0
                },
                {
                    "sent": "You might lose this in your.",
                    "label": 0
                },
                {
                    "sent": "Pretty answering, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean, I don't know.",
                    "label": 0
                },
                {
                    "sent": "Maybe you solve this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a problem we have seen in Spark Streaming.",
                    "label": 0
                },
                {
                    "sent": "Be cause for a single Spark streaming application.",
                    "label": 0
                },
                {
                    "sent": "You can only have one windowing operators, so the there's no way you can do this.",
                    "label": 0
                },
                {
                    "sent": "You know, maybe you can initialize several consumers and each one consume the different topics, but multiple query multiple windowing operator is not supported.",
                    "label": 0
                },
                {
                    "sent": "That's I mean, that's a problem, yeah, but for our use case, we do not involve such kind of things.",
                    "label": 0
                },
                {
                    "sent": "OK, so my my follow up is that you should check as future work.",
                    "label": 0
                },
                {
                    "sent": "Yeah, how far are you in terms of you know answers from playing sparkle on the same data set when you store all the data in a single single place?",
                    "label": 0
                },
                {
                    "sent": "Assuming that you can of course you can.",
                    "label": 0
                },
                {
                    "sent": "So my opinion is OK to approximate like this.",
                    "label": 0
                },
                {
                    "sent": "OK, but you know show I think this was.",
                    "label": 0
                },
                {
                    "sent": "What was a problem?",
                    "label": 0
                },
                {
                    "sent": "I guess.",
                    "label": 0
                },
                {
                    "sent": "OK, OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "And regarding the query rewriter to SQL, yeah you have to use the general API, right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, January for getting the translation between the algebra and SQL syntax.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "And since you use Windows here, did you consider that between two sequence Windows there could be latency?",
                    "label": 0
                },
                {
                    "sent": "And if the data which in which comes and the throughput of the Internet is.",
                    "label": 0
                },
                {
                    "sent": "Overhead, do you consider that the decision maker it's true or false?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean that yeah, we'll see.",
                    "label": 0
                },
                {
                    "sent": "Yes, maybe a problem, I mean, but this could be that you miss some data which are coming on the latency when they are between 2 Windows.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I yeah it's possible here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and another question.",
                    "label": 0
                },
                {
                    "sent": "How big was the datasets which you have to hold?",
                    "label": 0
                },
                {
                    "sent": "How big?",
                    "label": 0
                },
                {
                    "sent": "How large was the datasets?",
                    "label": 0
                },
                {
                    "sent": "So for the evaluation ways.",
                    "label": 0
                },
                {
                    "sent": "This stream rate we have generated is around 100 and 200,000, two 120,000 three plus per second.",
                    "label": 0
                },
                {
                    "sent": "And we have another paper and submit it in Big Data which is refused.",
                    "label": 0
                },
                {
                    "sent": "So and it's we used an I mean more stressful test, so it's around 200,000 people per second and we can easily handle this.",
                    "label": 0
                },
                {
                    "sent": "So thanks no problem.",
                    "label": 0
                },
                {
                    "sent": "You compared against some systems but I couldn't read which systems they were.",
                    "label": 0
                },
                {
                    "sent": "So I saw sequel S and others are sequels as sparkles, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so here these two.",
                    "label": 0
                },
                {
                    "sent": "So you so?",
                    "label": 0
                },
                {
                    "sent": "How were these competitors being set up?",
                    "label": 0
                },
                {
                    "sent": "So we're just a machine or a cluster.",
                    "label": 0
                },
                {
                    "sent": "So here as you can see we, I mean, for example the straight or DS, the D means distributed mode and LS means I mean local mode and ask means the static optimization.",
                    "label": 0
                },
                {
                    "sent": "So we did the evaluation on a single machine.",
                    "label": 0
                },
                {
                    "sent": "So here you can see here the result of throughput.",
                    "label": 0
                },
                {
                    "sent": "The third line is the.",
                    "label": 0
                },
                {
                    "sent": "Performance on a single machine.",
                    "label": 0
                },
                {
                    "sent": "So you use 6 machine, so six time computation power.",
                    "label": 0
                },
                {
                    "sent": "Notice what 1 version the local mode is just one more ship.",
                    "label": 0
                },
                {
                    "sent": "Yes yes.",
                    "label": 0
                },
                {
                    "sent": "When you compare this distributed against the local.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you're comparing 6 machines versus 1.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, and there I see you are 50% faster 60%.",
                    "label": 0
                },
                {
                    "sent": "Yeah, some kind of thing like that.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah, and then you said that for your use case is a second of latency is enough?",
                    "label": 0
                },
                {
                    "sent": "Yes, she couldn't you achieve ever achieve that also with other competitors?",
                    "label": 0
                },
                {
                    "sent": "I mean the latency.",
                    "label": 0
                },
                {
                    "sent": "The reason I mean I have explained the reason why we do not record the latency forces broken sequels because basically it's not compareable.",
                    "label": 0
                },
                {
                    "sent": "It's always very difficult to compare Huawei because the Seas Barco is.",
                    "label": 0
                },
                {
                    "sent": "I mean if you generate a very high input stream rates and you will have some abnormal behavior and we just simply crash the system for example 100,000 rubles per second is very high and I'm sure you cannot do this.",
                    "label": 0
                },
                {
                    "sent": "I mean you see Sparkle annefors equals it's eager execution, which is.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's a data driven, so I mean it's just not compatible.",
                    "label": 0
                },
                {
                    "sent": "Yeah, thank you.",
                    "label": 0
                },
                {
                    "sent": "I have a question on your data frame sort column design, right?",
                    "label": 0
                },
                {
                    "sent": "OK, you mentioned that you started graph data in your data frames.",
                    "label": 0
                },
                {
                    "sent": "So what are the columns?",
                    "label": 0
                },
                {
                    "sent": "What are the types?",
                    "label": 0
                },
                {
                    "sent": "Just roughly, what are the what are the column definitions?",
                    "label": 0
                },
                {
                    "sent": "The column definition is SPO basic, I mean RDF representation subject, predicate, object.",
                    "label": 0
                },
                {
                    "sent": "So you handled all those your eyes and literals.",
                    "label": 0
                },
                {
                    "sent": "Yeah, single column.",
                    "label": 0
                },
                {
                    "sent": "Yeah we also.",
                    "label": 0
                },
                {
                    "sent": "I mean for example you got.",
                    "label": 0
                },
                {
                    "sent": "A data type.",
                    "label": 0
                },
                {
                    "sent": "You are I for double or I mean or float and we handle it manually.",
                    "label": 0
                },
                {
                    "sent": "We do a runtime reflection to detect to detect the type of the data.",
                    "label": 0
                },
                {
                    "sent": "OK, got it.",
                    "label": 0
                }
            ]
        }
    }
}