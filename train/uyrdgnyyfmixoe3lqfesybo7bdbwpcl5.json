{
    "id": "uyrdgnyyfmixoe3lqfesybo7bdbwpcl5",
    "title": "Semi-supervised Structured Prediction Models",
    "info": {
        "author": [
            "Ulf Brefeld, Department of Computer Science, Humboldt University of Berlin"
        ],
        "published": "June 24, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Structured Output",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/icml07_brefeld_ssp/",
    "segmentation": [
        [
            "So with that, this is our first speaker or freifeld.",
            "He is among other things.",
            "Winner of the 2007 Best Paper award for ACNL and he'll be talking about semi supervised structured prediction models.",
            "Thank you.",
            "Yeah, welcome to my talk in parts of your disjoint work with Crystal Fisher.",
            "Two months Kettner Peter Hyderabad Schaeffer, Steven Global and Alex T."
        ],
        [
            "So what is it?",
            "Classical prediction models are binary, right?",
            "That is, they classify instances as positive or negative.",
            "Examples of the target concepts.",
            "We work problems, however, hardly binary, but there are other complex and they preserve an inherent structure, right and inputs and outputs are allowed to interact in multiple ways, and binary prediction models generally fail to capture these multiple way dependencies.",
            "It."
        ],
        [
            "Samples of these tests are for instance labeled sequence learning tests.",
            "So here we have, for instance, protein secondary structure prediction, named entity recognition.",
            "A part of speech tagging right in all of these tasks, we have a sequential input and we're looking for the corresponding sequential output of the same length.",
            "But of course, structured learning is not restricted to sequential learning other air."
        ],
        [
            "Areas include natural language parsing, where we have a tree structured output or classification with taxonomies where the output is a node in a tree."
        ],
        [
            "So the common approach when usually takes in structure learning is to learn a ranking function that is defined jointly on inputs and outputs.",
            "I think you're all familiar with that, so I will go through this rather quickly.",
            "So the decision value of actual input output pairs tells us something about how good they two guys fit together, right?",
            "So there is a prediction time.",
            "We're looking for the output for a given input that realises the highest decision value.",
            "So in order to find hypothesis that generalize well on new and unseen data, we can minimize the regularised empirical risk.",
            "And depending on the loss function we derive conditional random fields or large margin approaches.",
            "So all these approaches work pretty well when the sample size is large enough, or I should better say if the sample size is large and of course."
        ],
        [
            "In this structure learning settings we always have, we frequently have only a few labeled examples.",
            "For instance, think of a natural language parsing task where a linguist have to label the past three for a given sentence manually, right?",
            "And this is absolutely feasible for only a few sentences, but it becomes tedious when facing thousands of sentences.",
            "On the other hand, unclassified inputs are often abandoned and free.",
            "So in this natural language parsing task, we might extract unclassified inputs from the World Wide Web with actually no cost at all.",
            "So in structure learning."
        ],
        [
            "There's a real need for semi supervised technique that make use of these unlabeled data.",
            "Here's an overview over the next 40 minutes, so I will briefly introduce semi supervised learning with Co. Regularised regression and then we translate the lessons learned to the structure domain and investigate semi supervised structured prediction models.",
            "If there's enough time left, I will also show you briefly a study on case study on email batch detection."
        ],
        [
            "So let us start with Semi supervised learning."
        ],
        [
            "In addition to the label examples, we're in semi supervised learning, given additional Additionally unlabeled examples.",
            "So we assume that the unlabeled sample size outperform outnumbers the labeled ones, right?",
            "So we have a much more much larger number of unlabeled examples.",
            "And many approaches in semi supervised learning.",
            "Assume a class structure in the data.",
            "That is, they try to find decision boundaries that lie in low density regions of the data space.",
            "So however, this cluster assumption is frequently inappropriate, right?",
            "For instance, how does this assumption relate to a regression setting?",
            "So the question is, what can we do if we know that the cluster assumption is not appropriate?",
            "And."
        ],
        [
            "One possibility is to apply learning from multiple views, or it's also called Co learning.",
            "So here we split the attribute set in two disjoint sets that we call views.",
            "And a natural example might be web page classification.",
            "Right, so we can classify a webpage either based on its content or based on its context and the latter one can be for instance represented by the anchor text of its inbound links.",
            "So in each of these views we learn a hypothesis that provides its peer with predictions on the unlabeled examples.",
            "And the strategy is to maximize the consensus between both hypothesis."
        ],
        [
            "And why this is a good idea is illustrated here in terms of hypothesis spaces.",
            "So we in single view learning, we have a hypothesis space that contains the version space.",
            "That is all hypothesis that are consistent with the training data.",
            "Now it might if you're learning we have multiple hypothesis spaces and we're looking for a hypothesis in the intersection of all version spaces and hear the unlabeled data acts like an additional data driven regularization term.",
            "So we derive what we call the consensus maximization principle, and this is we minimize the training error for the labor examples and we minimize the disagreement for the unlabeled examples.",
            "So we can prove that we can minimize or in fact or in some we minimize an upper bound on the error.",
            "So."
        ],
        [
            "So the server closer look at Co optimization problems so.",
            "An objective function that implements this consensus maximization principle usually has the following form, so we have the empirical risk overall views.",
            "We have a regularization term for all views.",
            "And we have all pairwise disagreements among all views.",
            "So if the objective has this form, we can also prove an extended version of the Representer theorem that says that we can provide the minimizer in each view in terms of reproducing kernels.",
            "So."
        ],
        [
            "Now let's have a brief application to.",
            "Regularize the squares regression."
        ],
        [
            "So we can view regular rice, squares regression or univariate regression as a special case of structured output learning where the output space is equal to the set of real numbers.",
            "So in regression we usually have a squared loss function.",
            "And we realize the different views by applying different kernel functions.",
            "So the consensus maximization principle in this regression setting can now be stated as minimize the squared error for the labeled examples and minimize the squared differences.",
            "For the unlabeled examples."
        ],
        [
            "So and it goes as follows.",
            "We have here the single view regularised least squares regression and in Co learning we have multiple views and we have an additional disagreement term that measures all pairwise differences.",
            "No, the optimization problem that you see here has a solution, that is that has closed form solution and all involve matrices are strictly positive definite.",
            "If the kernel is positive definite.",
            "So."
        ],
        [
            "We can compute the solution in cubic time in the number of instances.",
            "And being cubic is neither good nor bad, right?",
            "So many approaches such as support vector machines or regularised least squares regression, a cubic in the number of instances.",
            "However, we assume that the unlabeled sample size outnumbers the labeled one.",
            "And.",
            "So being cubic in the number of unlabeled examples is not a desired property, right?",
            "So one can all."
        ],
        [
            "So study a parametric semiparametric approximation where the hypothesis bases are restricted to depend only on the labeled examples.",
            "So would you arrive also a convex objective function that has also a closed form solution?",
            "Of course it has less much fewer parameters.",
            "And this time we can compute the execution time in linear time in the number of examples of unlabeled examples.",
            "So."
        ],
        [
            "I'm up.",
            "Whenever we have labeled examples and unlabeled examples, we can plug them into one of the closed form solutions and compute the solution in, say, cubic time or linear time, right?",
            "But what happens if not all data is available at the same site?",
            "So you have also studied a distributed scenario.",
            "Think of for instance loan providers trying to prevent fraud using some prediction technique.",
            "So the easiest way would be to share the data."
        ],
        [
            "Right, so they have labeled data and easiest way to solve that problem would be to share the data.",
            "But due to privacy concerns or non disclosure agreements the companies are unlikely to do that.",
            "So, but now think of that they agree on a fixed set of unlabeled data, and they further agree on sharing predictions on these unlabeled data.",
            "And we can also devise an iterative update scheme that were only predictions on these unlabeled data have to be exchanged, and that converges to the same solution as if we would have applied with closed form.",
            "Solution.",
            "So."
        ],
        [
            "That's seven brief look at some empirical results.",
            "Here we compared the two semi supervised regressions with.",
            "The regularised least squares baseline, right?",
            "So we conducted inverse cross validations on 32 UCI datasets.",
            "And an inverse cross validation means that we use one fold as labeled example.",
            "An nine folds is unlabeled examples.",
            "So each of the points in the figures corresponds to the UCI data set and its location is determined by the root mean squared error of the two respective algorithms that are depicted on the X&Y axis.",
            "So we have, for instance, if a point lies above the diagonal, the diagonal represents equal performance.",
            "Then it means that the algorithm on the X axis performs better and vice versa.",
            "If the point lies below the diagonal, then this means that the algorithm on the Y axis has a lower root mean squared error.",
            "So here we clearly see that both semi supervised approaches perform much better than the Universal Single View baseline."
        ],
        [
            "And if we now compare both semi supervised approaches, we see that the exact solution is slightly but significantly better and leads to lower root mean squared errors than the semicircle, the semiparametric approximation.",
            "However, the exact solution."
        ],
        [
            "And as I mentioned before, is cubic in the number of unlabeled examples, and therefore intractable for large problems.",
            "But in this case we have to resort to the semi supervised approximate the semi parametric approximately."
        ],
        [
            "So now let's translate the lessons learned to the structure domain.",
            "Brighton we want to apply the consensus maximization principle with our two support vector machines for structured outputs.",
            "And here we go.",
            "So before we start, I should say that all the following results are easily generalizable to an arbitrary number of views, right?",
            "So I just focus on 2 for simplicity.",
            "I'm.",
            "So we have a joint decision function that is defined as the sum over 2 views.",
            "And here we model the views by distinct joint feature representations.",
            "And of course if we later on switch to the dual then this corresponds to using different kernels for different views.",
            "So I just want to remind you of the consensus maximization principle.",
            "Again, we want to minimize the error for the label examples and the disagreement for the under examples.",
            "So this involves computations.",
            "A lot of computations of the arc Max and these are carried out by either Viterbi algorithm in case of sequential outputs, or by CKY CKY algorithm in case of incursive grammar recursive grammars."
        ],
        [
            "So.",
            "This is the optimization problem for one view right.",
            "Note that we have two views.",
            "We now have two optimization problems, one in each view.",
            "And this is one of them.",
            "So essentially it is the regular support vector machine for structured outputs plus some additional terms that are associated with unlabeled examples.",
            "So these are indicated with a blue color.",
            "An in Gray you'll see.",
            "Hopefully yeah.",
            "The variables that are passed from the peer review, right?",
            "So on the one hand, we have the prediction of the peer review that is treated as the true labeling for the unlabeled examples.",
            "And we have the confidence of the peer review that is.",
            "The peer review is.",
            "Or, if this unlabeled example fulfills the margin constraint in the peer review, this confidence equals one and it has this unlabeled example.",
            "If the same influence as a labeled one in this view.",
            "So we can.",
            "Derive for dual representation where we can rewrite the weight vector in terms of linear combinations of different vectors.",
            "And these dual variables of the new dual variables are bound to input examples that is.",
            "Or an intern.",
            "This input examples can be associated with subspaces, right?",
            "So we can devise an optimization scheme, an iterative optimization scheme that maintains working sets that are associated with these subspaces, and that leads to sparse models.",
            "So here Central station how this works.",
            "So we have here a part of speech tagging problem.",
            "We have labeled example.",
            "John ate the cat and we have the true label sequence.",
            "It's noun, verb, determiner, noun.",
            "We have fixed working sets for all other examples and we start with an empty working set for simplicity.",
            "So this is the optimization in one view, right?",
            "So note that both optimized, so the optimization for label examples are independent and the optimization for the unlabeled examples are.",
            "Dependent on each other so.",
            "The."
        ],
        [
            "Turbie algorithm starts to predict the top scoring sequence, and in this case it says noun verb noun, right?",
            "It's not the correct one.",
            "And we have to perform an update that is.",
            "We add the most strongly violated constraint to the working set.",
            "And we instantiate the corresponding dual parameter.",
            "And since we only instantiate dual parameters if we see a constraint in the training process.",
            "We implicitly consider all others to be equal to 0 right, and therefore we have sparse models.",
            "Um?",
            "So after that we update the dual parameters or in this case this one dual parameter by solving aquatic program.",
            "And we can repeat this.",
            "If we if it turns out that the prediction is again incorrect now, it's now in determining determining known.",
            "We just repeat all the steps, that is, we add the most strongly violated constraint to the working set.",
            "We instantiate the corresponding Alpha we saw server quadratic program that optimizes both Alpha variables and.",
            "We can repeat this until we arrive.",
            "No more margin violations.",
            "So this is pretty much the regular support vector machine for structured output variables, right so?",
            "Um?"
        ],
        [
            "But what happens in the case of unlabeled examples?",
            "So here we have an unlimited example.",
            "It's John, went home and if you now have two views.",
            "And we have fixed working sets in each view for other examples and we start again with empty working sets for the current example.",
            "So both Kirby algorithms start to decode the top scoring prediction.",
            "One says none were determined.",
            "Verb, noun and one says noun, verb, verb.",
            "So both disagree on their prediction and we perform an update.",
            "And again we add the most strongly violated constraint to the working set in each view, but this time.",
            "We treat the prediction of the peer review as the true labeling.",
            "So after that we optimize the parameters by solving two quadratic programs, one in each view, and we can repeat this until the driver consensus."
        ],
        [
            "So let's have a look at some empirical results.",
            "The task in the bio creative data set is to detect gene and protein names in biomedical papers.",
            "We have a lot of features including letter N, gram surface clues and things like that.",
            "We apply a random feature split.",
            "And, um.",
            "Yeah, the baseline is of course, and all other following baselines are trained on the concatenated views.",
            "That is, they get all their valuable features."
        ],
        [
            "So here are the results.",
            "We have 410 labeled examples and a variant of varying number of unlabeled examples that is depicted on the wire on the X axis.",
            "Some we received some token accuracies for these.",
            "Training set.",
            "And first of all, we see that the semi supervised support vector machine leads to more accurate predictions as the baseline, right?",
            "So what we also see here is that.",
            "There seems to be a positive or the number of unlabeled examples seems to be kind of positively correlated with accuracy, right?",
            "That is.",
            "We might conclude that if we add more unlabeled examples, we might gain accuracy.",
            "We also Eve."
        ],
        [
            "I did the semi supervised SVM on a natural language parsing test.",
            "Here we use the Wall Street Journal corpus and the corpus.",
            "We transform the involved grammars into context free grammars in order to apply the CKY parser.",
            "We applied a loss based on the F1 measure and used local features."
        ],
        [
            "So here are the results we have.",
            "Again, the number of unlabeled examples on the on the X axis and the F1 score on the Y axis.",
            "And.",
            "Here we see that the semi supervised support vector machine leads to much better predictions in F1 score for even no unlabeled examples.",
            "And this is due to averaging two independently trained hypothesis, right?",
            "So without unlabeled examples, we train two regular support vector machines independently of each other and average them.",
            "So, but if we increase the number of unlimited examples, we further improve the of 1 scope.",
            "So, however."
        ],
        [
            "This gain in performance comes at the cost of a longer training time.",
            "Right, so the semi supervised SVM scales quadratically in the number of analytic samples and.",
            "Yeah, it's more expensive to optimize.",
            "So to sum up, we saw that we can make use of unlabeled data in discriminative structure learning without making a cluster assumption.",
            "However, it's not always fair to assume that the data has a class structure, and therefore I would like to also present you a transductive version of the structural support vector machine that makes an explicit cluster assumption."
        ],
        [
            "Required the kernels are different in different views, so how do you choose?",
            "So we split the attributes randomly, right?",
            "So we compute all kernels on the different sets of attributes.",
            "Right, right?",
            "Just based on different attributes.",
            "Sorry, sensitive.",
            "Yeah we we started with distinct splits.",
            "We started to have a token based split and a surface can split like on the one hand we have a letter N grams.",
            "The identity and things like that and on the other hand we had capitalization features or numbers or whatever and it turned out that the random feature split works much better than this service clue versus token you slip and so this means that.",
            "That there has to be a split.",
            "So if this is the average split split right and we.",
            "The results here are averages over 100 repetitions.",
            "And everything is random, right?",
            "The training data is random.",
            "The attributes players at random.",
            "So this means that we observe here kind of performance for an ad for an average split.",
            "So, but this also means that there has to be a split that even works better, right?",
            "The question is how to find that split and this remains an open challenge, though I haven't found him away to determine before hand if this player is good or not or what are the criteria for goods played right?",
            "So it's.",
            "True.",
            "The fact that you're only training on 40 labeled and 200 unlabeled examples.",
            "Yes, it is slow, but the slowest in this version is the CKY parser that we use.",
            "But the training is slow also, right?",
            "Anymore questions.",
            "Even when you don't have unlabeled examples, yeah, 'cause we have two independently trained hypothesis, right?",
            "And we just average then this kind of assembly effect here.",
            "Using the whole features together as a baseline, yeah, the baseline gets all the features right.",
            "So the baseline gets all the features and the green line.",
            "The semi supervised approach.",
            "We split the features into 2 views.",
            "We train 2 independently trained, trained independently to support vector machines and then we average their results for no unlabeled examples, right?",
            "Relations with both when you click yes, of course we can do anything.",
            "We can assume that the feature sets are correlated or not.",
            "Probably pretty correlated.",
            "Sure.",
            "Mini.",
            "Well, training 2 views is expensive in terms of execution time, so actually I didn't do it.",
            "I don't know.",
            "I never tried it out so I just focused on 2 views.",
            "So mathematically, you can write it down for an arbitrary number of views, but I just applied 2 views, sorry.",
            "Interesting.",
            "I'm.",
            "Yeah, probably we could have done pure code training as an additional baseline, right?",
            "But we didn't do that.",
            "That's a good idea, right?",
            "So I will think about including this one, yeah?",
            "Further questions or shall I go on?"
        ],
        [
            "So now the transactive variant, right?",
            "So you're probably all familiar with the binary transductive support vector machine that has these discrete variables for unlabeled instances.",
            "And due to this discrete due to these discrete variables, we have a combinatorial optimization problem that is very hard to solve if it's possible to solve it.",
            "And so, in order to translate this to the structural domain we have, we have to cope with these combinatorial optimization in order to be tractable.",
            "Or derive an optimization criterion that is tractable.",
            "So when we tackle this problem by removing the discrete variables and applying continuous and differentiable optimization schemes."
        ],
        [
            "So here we have.",
            "The regular support vector machine with constraints right?",
            "Now the simplest or easiest way to get rid of all these constraints is to solve the select variables for the constraints antauri, substitute them into the objective, right?",
            "So this is pretty straightforward, but of course we haven't won anything.",
            "So, um.",
            "It's we just rewrite the optimization problem."
        ],
        [
            "So.",
            "What we do now is we try to make it differentiable.",
            "And first of all we have the hinge loss.",
            "That is not differentiable, but so the hinge loss is not differentiable, But the Huber loss is right.",
            "So we just substitute the Huber loss for the hinge loss and.",
            "So this is it.",
            "But we still have a problem and this is the maximum in this equation.",
            "And the maximum can also be approximated by differential surrogate.",
            "And this is the softmax function.",
            "So if you substitute the Huber loss for the hinge loss and the softmax for the Maps for the Max, we get an unconstrained variant of the support vector machine.",
            "And this is differentiable and we can apply gradient based techniques in order to optimize it.",
            "So now let's let's have a look how to."
        ],
        [
            "Clude unlabeled examples in the support vector machine.",
            "So in according to the transductive principle, we want to include unlabeled instances such that they fulfill the margin constraint.",
            "Right, that is if we decode the top scoring output and we decode the best runner up.",
            "We want these two guys to fulfill the margin constraint.",
            "And therefore we design A loss function for the unlabeled examples that allows us to mitigate margin violations in two symmetric ways, right?",
            "One is associated with the top scoring output in one is associated with the best runner out.",
            "And once having this loss function, the inclusion of the unlabeled examples is straightforward.",
            "So we simply use.",
            "Uh.",
            "Yeah, the same approach As for the labor examples, we just switch or exchange the loss function and we spend another variable that measures the overall influence for this unlabeled examples.",
            "So unfortunately due to the shape of this new loss function, the problem is no longer convex.",
            "But before we get into details and have a look at empirical results, let me show you."
        ],
        [
            "The execution time.",
            "And here we see as the solid line is the gradient based optimization of the support vector machine and the dashed line is.",
            "The same program, so the same code, just the optimization is exchanged.",
            "And here we used the quadratic program approach with seaplex.",
            "So the gradient based optimization is a bit more efficient and a bit faster, but the real benefit we we can see the real benefit when we add unlabeled examples.",
            "So these dotted line here is the transductive version, and here we add five times as many unlabeled examples as it gets labeled examples.",
            "So.",
            "The inclusion of the unlabeled examples is pretty efficient, right?"
        ],
        [
            "So now let's have a look at some empirical results.",
            "We used the Spanish Newswire data set, where the task is to detect names of persons and locations, organizations, and other names.",
            "We have a lot of features again and."
        ],
        [
            "Here are the results.",
            "So the we have the number of unlabeled examples on the X axis and the token error on the Y axis.",
            "We applied the hidden Markov SPM bias mean altoon.",
            "As a baseline.",
            "And we see that the transductive variant has a bit lower error compared to that baseline.",
            "Yeah, I think that's it, and unfortunately this is the only task where it works out right so?"
        ],
        [
            "All in all, other chairs that we tried out, it was pretty much like this here.",
            "Here we have again a sequential task.",
            "It's Galaxy data set by left here, L. And we use this one in order to compare two different approaches of including unlabeled examples.",
            "So we wanted to compare our transductive support vector machine with the inclusion of unlabeled examples, paragraph Laplace link.",
            "So the data is generated by a local hidden Markov model, so the probability that observing another red state when we are in a red state is 80% and that we observe a blue state when we are in a red state is 20%.",
            "So, um.",
            "Since we assume.",
            "Before I tell you that I should say that we have 100 sequences of length 20, right so depicted on the X axis of these figures are the number of labeled examples and 100 minus the number of the X axis is the number of unlabeled examples and test data.",
            "I'm so.",
            "Since both semi supervised approaches, or we assume both semi supervised approaches to be orthogonal right?",
            "That is, we test on all four combinations of.",
            "Support vector machine transductive support vector machine and supervised RBF kernel and semi supervised Laplacian kernel.",
            "So when the results are as follows, first of all, I think we clearly see that the graph Laplacian is very well suited for the problem at hand, right?",
            "So it leads to an average decrease in error rate of, I think about 10%.",
            "So Moreover, we see that there's absolutely no difference between the transductive variant of the support vector machine and the regular support vector machine.",
            "So the question is now, why does the transductive support vector machine failed to make use of this unlabeled data?",
            "And besides the non convexity of the optimization problem, that might also.",
            "Be a point here.",
            "Um, we have.",
            "Another explanation would be different cluster assumptions.",
            "So.",
            "The data is generated in a local by this local model and the graph Laplacian captures this local neighborhood cause it's defined by nearest neighbor graph, right?",
            "I'm.",
            "The transductive support vector machine, on the other hand, makes a global or has a cluster assumption that is defined on complete sequences right, and this seems to be not.",
            "Not appropriate in this setting.",
            "So, but we have to.",
            "We need more studies on that so.",
            "Joe."
        ],
        [
            "I guess.",
            "Um, yeah, I think.",
            "10 minutes OK then I will quickly run through the last slides and present your case.",
            "Study an email back detection and that's the paper that won this award, right?"
        ],
        [
            "So before that I should say that.",
            "We have here a.",
            "A clustering problem.",
            "But we have access to a ground truth, right?",
            "That is, we have an unsupervised approach and we have a kind of supervised approach on at the same time and in some it is kind of a semi supervised approach and wonderfully fits into the store, right?",
            "So in order to motivate the problem setting.",
            "We have to have a closer look at the characteristics of electronic messaging, right?",
            "So about 80% of all emails in the world are considered to be spam.",
            "And approximately 80% of these 80% are generated by only a few spammers.",
            "And these spammers maintain one or more templates at a time to generate their spans automatically.",
            "And they exchange their templates rapidly.",
            "For an email server, this means that in a short time frame it receives many emails that are generated with the same template, so the goal is now to detect these batches in the data stream and the idea behind it is if we were having the best information, we could use it either from black or white listing, or we can use it for improving spam or non spam classification."
        ],
        [
            "So here's an example of two messages that are generated by the same template.",
            "At first said they look alike, but if we have a closer look then we see that they differ in, for instance, the address, the names we have, a random insertion of spaces here.",
            "The numbers are different.",
            "Even complete sentences are different.",
            "But the semantics stays always the same.",
            "So we can make use of the pairwise similarities between emails that are generated by the same badge.",
            "By re phrasing the chest of vegetation as a task of correlation clustering."
        ],
        [
            "Correlation clustering?",
            "Equivalent to the solution of correlation clustering is equivalent to a solution in a Poly cut of Poly cut in the fully connected graph, and the edges are weighted with the similarities of the connected nodes.",
            "So what correlation clustering essentially does is it maximizes the intra cluster similarity?",
            "So we identify the phone clusters with our batches and we're pretty much done.",
            "So the only problem is that we don't know which similarity measure leads to true clustering, right?",
            "And since we don't know that we use a parameterized similarity measure and we learn the weights of the of this similarity measure in the training process.",
            "So.",
            "We extract pairwise features."
        ],
        [
            "Office of Pairs of emails like the edit distance of the bodies or TF IDF similarities.",
            "Edit distance of the subject line.",
            "Sorry, and now an input looks as the following form.",
            "We have a data stream of key messages and the corresponding output is binary matrix where the elements indicate whether 2 messages are in the same cluster or not.",
            "Um?",
            "Of course, this is a combinatorial optimization problem.",
            "It's NP complete and all that an so we can sort of an approximate variant instead, where we use continuous variables for the disk."
        ],
        [
            "So plugging everything into a support vector machine, we derive the following optimization criterion.",
            "In order to to derive a more compact variant, we can apply the technique of law segmented influence by Ben Taskar.",
            "And we replace the computation of the worst margin violated by.",
            "The corresponding Lagrangian dual.",
            "And if we do that, we obtain a minimization within a minimization and we can combine them.",
            "And we obtain one quadratic program that has cubic number of constraints.",
            "So this means it's not tractable for practical applications, right?"
        ],
        [
            "But this solution does not exploit the data stream.",
            "So in order to do that, or to approximate the solution, we can assume that whenever a new email arrives, we only have to integrate this email into the existing clustering and the clustering of the previous emails remains fixed."
        ],
        [
            "So mathematically, this means that we decompose the objective into a constant part that depends only on the clustering of the previous emails and the part that depends only of the clustering of the latest email.",
            "So, um."
        ],
        [
            "In order to run finish quite soon, just tell you that we observed no significant differences.",
            "We tried out a lot of learning algorithms with a lot of different decomposing techniques and all perform pretty much the same.",
            "So neither no."
        ],
        [
            "Result was significant.",
            "However, we don't want to be better.",
            "We just want to be quicker, right?",
            "And he received at the sequential approximation in blue is very quick.",
            "Note that the X is on the Y axis, the Y axis is scaled in log scale, right?",
            "So."
        ],
        [
            "This is the final experiment for today.",
            "He wanted to find out if having the best information can improve the classification of spam or non spam units and.",
            "What we did is we used the sequential clustering approach.",
            "And, um, we extracted features of the found patches and these features were very simple, so we use the size of the badge or binary features, for instance, indicating if all subject lines are equivalent.",
            "So and if we add this features to a bag of words representation, we see that the misclassification risk reduces of about 40%, right?",
            "Anne.",
            "If you consider that there are 7 billion emails per day, this is an enormous amount, right?",
            "So what I think I will."
        ],
        [
            "Come to an end and.",
            "Let me conclude by summing up."
        ],
        [
            "I presented you today.",
            "I started with an introduction to semi supervised learning with Co. Regularised regression.",
            "Um, after that we lifted the lessons learned to the structure domain and device.",
            "The code VM that relies on the consensus maximization principle and the transductive support vector machine that relies on a cluster assumption.",
            "Empirically, it turns out that we actually can improve structure learning by including unlabeled examples, and sometimes adding more unlimited examples even improves performance.",
            "So in the supervised clustering test, we saw that.",
            "We saw how we can derive an efficient optimization by exploiting the streaming nature of the data.",
            "And we saw that the features extracted from the found patches can lead to a reduction in the misclassification error.",
            "So I'm looking for a great workshop and thanks for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So with that, this is our first speaker or freifeld.",
                    "label": 0
                },
                {
                    "sent": "He is among other things.",
                    "label": 0
                },
                {
                    "sent": "Winner of the 2007 Best Paper award for ACNL and he'll be talking about semi supervised structured prediction models.",
                    "label": 1
                },
                {
                    "sent": "Thank you.",
                    "label": 1
                },
                {
                    "sent": "Yeah, welcome to my talk in parts of your disjoint work with Crystal Fisher.",
                    "label": 0
                },
                {
                    "sent": "Two months Kettner Peter Hyderabad Schaeffer, Steven Global and Alex T.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is it?",
                    "label": 0
                },
                {
                    "sent": "Classical prediction models are binary, right?",
                    "label": 1
                },
                {
                    "sent": "That is, they classify instances as positive or negative.",
                    "label": 0
                },
                {
                    "sent": "Examples of the target concepts.",
                    "label": 0
                },
                {
                    "sent": "We work problems, however, hardly binary, but there are other complex and they preserve an inherent structure, right and inputs and outputs are allowed to interact in multiple ways, and binary prediction models generally fail to capture these multiple way dependencies.",
                    "label": 0
                },
                {
                    "sent": "It.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Samples of these tests are for instance labeled sequence learning tests.",
                    "label": 0
                },
                {
                    "sent": "So here we have, for instance, protein secondary structure prediction, named entity recognition.",
                    "label": 1
                },
                {
                    "sent": "A part of speech tagging right in all of these tasks, we have a sequential input and we're looking for the corresponding sequential output of the same length.",
                    "label": 0
                },
                {
                    "sent": "But of course, structured learning is not restricted to sequential learning other air.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Areas include natural language parsing, where we have a tree structured output or classification with taxonomies where the output is a node in a tree.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the common approach when usually takes in structure learning is to learn a ranking function that is defined jointly on inputs and outputs.",
                    "label": 1
                },
                {
                    "sent": "I think you're all familiar with that, so I will go through this rather quickly.",
                    "label": 1
                },
                {
                    "sent": "So the decision value of actual input output pairs tells us something about how good they two guys fit together, right?",
                    "label": 0
                },
                {
                    "sent": "So there is a prediction time.",
                    "label": 0
                },
                {
                    "sent": "We're looking for the output for a given input that realises the highest decision value.",
                    "label": 1
                },
                {
                    "sent": "So in order to find hypothesis that generalize well on new and unseen data, we can minimize the regularised empirical risk.",
                    "label": 0
                },
                {
                    "sent": "And depending on the loss function we derive conditional random fields or large margin approaches.",
                    "label": 0
                },
                {
                    "sent": "So all these approaches work pretty well when the sample size is large enough, or I should better say if the sample size is large and of course.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this structure learning settings we always have, we frequently have only a few labeled examples.",
                    "label": 0
                },
                {
                    "sent": "For instance, think of a natural language parsing task where a linguist have to label the past three for a given sentence manually, right?",
                    "label": 0
                },
                {
                    "sent": "And this is absolutely feasible for only a few sentences, but it becomes tedious when facing thousands of sentences.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, unclassified inputs are often abandoned and free.",
                    "label": 0
                },
                {
                    "sent": "So in this natural language parsing task, we might extract unclassified inputs from the World Wide Web with actually no cost at all.",
                    "label": 0
                },
                {
                    "sent": "So in structure learning.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a real need for semi supervised technique that make use of these unlabeled data.",
                    "label": 0
                },
                {
                    "sent": "Here's an overview over the next 40 minutes, so I will briefly introduce semi supervised learning with Co. Regularised regression and then we translate the lessons learned to the structure domain and investigate semi supervised structured prediction models.",
                    "label": 1
                },
                {
                    "sent": "If there's enough time left, I will also show you briefly a study on case study on email batch detection.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let us start with Semi supervised learning.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In addition to the label examples, we're in semi supervised learning, given additional Additionally unlabeled examples.",
                    "label": 1
                },
                {
                    "sent": "So we assume that the unlabeled sample size outperform outnumbers the labeled ones, right?",
                    "label": 0
                },
                {
                    "sent": "So we have a much more much larger number of unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "And many approaches in semi supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Assume a class structure in the data.",
                    "label": 0
                },
                {
                    "sent": "That is, they try to find decision boundaries that lie in low density regions of the data space.",
                    "label": 0
                },
                {
                    "sent": "So however, this cluster assumption is frequently inappropriate, right?",
                    "label": 1
                },
                {
                    "sent": "For instance, how does this assumption relate to a regression setting?",
                    "label": 0
                },
                {
                    "sent": "So the question is, what can we do if we know that the cluster assumption is not appropriate?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One possibility is to apply learning from multiple views, or it's also called Co learning.",
                    "label": 1
                },
                {
                    "sent": "So here we split the attribute set in two disjoint sets that we call views.",
                    "label": 1
                },
                {
                    "sent": "And a natural example might be web page classification.",
                    "label": 0
                },
                {
                    "sent": "Right, so we can classify a webpage either based on its content or based on its context and the latter one can be for instance represented by the anchor text of its inbound links.",
                    "label": 0
                },
                {
                    "sent": "So in each of these views we learn a hypothesis that provides its peer with predictions on the unlabeled examples.",
                    "label": 1
                },
                {
                    "sent": "And the strategy is to maximize the consensus between both hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And why this is a good idea is illustrated here in terms of hypothesis spaces.",
                    "label": 0
                },
                {
                    "sent": "So we in single view learning, we have a hypothesis space that contains the version space.",
                    "label": 1
                },
                {
                    "sent": "That is all hypothesis that are consistent with the training data.",
                    "label": 0
                },
                {
                    "sent": "Now it might if you're learning we have multiple hypothesis spaces and we're looking for a hypothesis in the intersection of all version spaces and hear the unlabeled data acts like an additional data driven regularization term.",
                    "label": 0
                },
                {
                    "sent": "So we derive what we call the consensus maximization principle, and this is we minimize the training error for the labor examples and we minimize the disagreement for the unlabeled examples.",
                    "label": 1
                },
                {
                    "sent": "So we can prove that we can minimize or in fact or in some we minimize an upper bound on the error.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the server closer look at Co optimization problems so.",
                    "label": 0
                },
                {
                    "sent": "An objective function that implements this consensus maximization principle usually has the following form, so we have the empirical risk overall views.",
                    "label": 1
                },
                {
                    "sent": "We have a regularization term for all views.",
                    "label": 1
                },
                {
                    "sent": "And we have all pairwise disagreements among all views.",
                    "label": 1
                },
                {
                    "sent": "So if the objective has this form, we can also prove an extended version of the Representer theorem that says that we can provide the minimizer in each view in terms of reproducing kernels.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's have a brief application to.",
                    "label": 0
                },
                {
                    "sent": "Regularize the squares regression.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can view regular rice, squares regression or univariate regression as a special case of structured output learning where the output space is equal to the set of real numbers.",
                    "label": 1
                },
                {
                    "sent": "So in regression we usually have a squared loss function.",
                    "label": 0
                },
                {
                    "sent": "And we realize the different views by applying different kernel functions.",
                    "label": 0
                },
                {
                    "sent": "So the consensus maximization principle in this regression setting can now be stated as minimize the squared error for the labeled examples and minimize the squared differences.",
                    "label": 1
                },
                {
                    "sent": "For the unlabeled examples.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So and it goes as follows.",
                    "label": 0
                },
                {
                    "sent": "We have here the single view regularised least squares regression and in Co learning we have multiple views and we have an additional disagreement term that measures all pairwise differences.",
                    "label": 0
                },
                {
                    "sent": "No, the optimization problem that you see here has a solution, that is that has closed form solution and all involve matrices are strictly positive definite.",
                    "label": 1
                },
                {
                    "sent": "If the kernel is positive definite.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can compute the solution in cubic time in the number of instances.",
                    "label": 0
                },
                {
                    "sent": "And being cubic is neither good nor bad, right?",
                    "label": 0
                },
                {
                    "sent": "So many approaches such as support vector machines or regularised least squares regression, a cubic in the number of instances.",
                    "label": 1
                },
                {
                    "sent": "However, we assume that the unlabeled sample size outnumbers the labeled one.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So being cubic in the number of unlabeled examples is not a desired property, right?",
                    "label": 0
                },
                {
                    "sent": "So one can all.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So study a parametric semiparametric approximation where the hypothesis bases are restricted to depend only on the labeled examples.",
                    "label": 0
                },
                {
                    "sent": "So would you arrive also a convex objective function that has also a closed form solution?",
                    "label": 1
                },
                {
                    "sent": "Of course it has less much fewer parameters.",
                    "label": 0
                },
                {
                    "sent": "And this time we can compute the execution time in linear time in the number of examples of unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm up.",
                    "label": 0
                },
                {
                    "sent": "Whenever we have labeled examples and unlabeled examples, we can plug them into one of the closed form solutions and compute the solution in, say, cubic time or linear time, right?",
                    "label": 0
                },
                {
                    "sent": "But what happens if not all data is available at the same site?",
                    "label": 0
                },
                {
                    "sent": "So you have also studied a distributed scenario.",
                    "label": 0
                },
                {
                    "sent": "Think of for instance loan providers trying to prevent fraud using some prediction technique.",
                    "label": 0
                },
                {
                    "sent": "So the easiest way would be to share the data.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so they have labeled data and easiest way to solve that problem would be to share the data.",
                    "label": 0
                },
                {
                    "sent": "But due to privacy concerns or non disclosure agreements the companies are unlikely to do that.",
                    "label": 0
                },
                {
                    "sent": "So, but now think of that they agree on a fixed set of unlabeled data, and they further agree on sharing predictions on these unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "And we can also devise an iterative update scheme that were only predictions on these unlabeled data have to be exchanged, and that converges to the same solution as if we would have applied with closed form.",
                    "label": 0
                },
                {
                    "sent": "Solution.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's seven brief look at some empirical results.",
                    "label": 0
                },
                {
                    "sent": "Here we compared the two semi supervised regressions with.",
                    "label": 0
                },
                {
                    "sent": "The regularised least squares baseline, right?",
                    "label": 0
                },
                {
                    "sent": "So we conducted inverse cross validations on 32 UCI datasets.",
                    "label": 1
                },
                {
                    "sent": "And an inverse cross validation means that we use one fold as labeled example.",
                    "label": 0
                },
                {
                    "sent": "An nine folds is unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "So each of the points in the figures corresponds to the UCI data set and its location is determined by the root mean squared error of the two respective algorithms that are depicted on the X&Y axis.",
                    "label": 0
                },
                {
                    "sent": "So we have, for instance, if a point lies above the diagonal, the diagonal represents equal performance.",
                    "label": 0
                },
                {
                    "sent": "Then it means that the algorithm on the X axis performs better and vice versa.",
                    "label": 0
                },
                {
                    "sent": "If the point lies below the diagonal, then this means that the algorithm on the Y axis has a lower root mean squared error.",
                    "label": 0
                },
                {
                    "sent": "So here we clearly see that both semi supervised approaches perform much better than the Universal Single View baseline.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if we now compare both semi supervised approaches, we see that the exact solution is slightly but significantly better and leads to lower root mean squared errors than the semicircle, the semiparametric approximation.",
                    "label": 0
                },
                {
                    "sent": "However, the exact solution.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as I mentioned before, is cubic in the number of unlabeled examples, and therefore intractable for large problems.",
                    "label": 0
                },
                {
                    "sent": "But in this case we have to resort to the semi supervised approximate the semi parametric approximately.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now let's translate the lessons learned to the structure domain.",
                    "label": 0
                },
                {
                    "sent": "Brighton we want to apply the consensus maximization principle with our two support vector machines for structured outputs.",
                    "label": 1
                },
                {
                    "sent": "And here we go.",
                    "label": 0
                },
                {
                    "sent": "So before we start, I should say that all the following results are easily generalizable to an arbitrary number of views, right?",
                    "label": 0
                },
                {
                    "sent": "So I just focus on 2 for simplicity.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 1
                },
                {
                    "sent": "So we have a joint decision function that is defined as the sum over 2 views.",
                    "label": 1
                },
                {
                    "sent": "And here we model the views by distinct joint feature representations.",
                    "label": 0
                },
                {
                    "sent": "And of course if we later on switch to the dual then this corresponds to using different kernels for different views.",
                    "label": 0
                },
                {
                    "sent": "So I just want to remind you of the consensus maximization principle.",
                    "label": 0
                },
                {
                    "sent": "Again, we want to minimize the error for the label examples and the disagreement for the under examples.",
                    "label": 1
                },
                {
                    "sent": "So this involves computations.",
                    "label": 0
                },
                {
                    "sent": "A lot of computations of the arc Max and these are carried out by either Viterbi algorithm in case of sequential outputs, or by CKY CKY algorithm in case of incursive grammar recursive grammars.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is the optimization problem for one view right.",
                    "label": 1
                },
                {
                    "sent": "Note that we have two views.",
                    "label": 0
                },
                {
                    "sent": "We now have two optimization problems, one in each view.",
                    "label": 0
                },
                {
                    "sent": "And this is one of them.",
                    "label": 0
                },
                {
                    "sent": "So essentially it is the regular support vector machine for structured outputs plus some additional terms that are associated with unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "So these are indicated with a blue color.",
                    "label": 0
                },
                {
                    "sent": "An in Gray you'll see.",
                    "label": 0
                },
                {
                    "sent": "Hopefully yeah.",
                    "label": 0
                },
                {
                    "sent": "The variables that are passed from the peer review, right?",
                    "label": 0
                },
                {
                    "sent": "So on the one hand, we have the prediction of the peer review that is treated as the true labeling for the unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "And we have the confidence of the peer review that is.",
                    "label": 0
                },
                {
                    "sent": "The peer review is.",
                    "label": 0
                },
                {
                    "sent": "Or, if this unlabeled example fulfills the margin constraint in the peer review, this confidence equals one and it has this unlabeled example.",
                    "label": 0
                },
                {
                    "sent": "If the same influence as a labeled one in this view.",
                    "label": 0
                },
                {
                    "sent": "So we can.",
                    "label": 0
                },
                {
                    "sent": "Derive for dual representation where we can rewrite the weight vector in terms of linear combinations of different vectors.",
                    "label": 0
                },
                {
                    "sent": "And these dual variables of the new dual variables are bound to input examples that is.",
                    "label": 1
                },
                {
                    "sent": "Or an intern.",
                    "label": 1
                },
                {
                    "sent": "This input examples can be associated with subspaces, right?",
                    "label": 0
                },
                {
                    "sent": "So we can devise an optimization scheme, an iterative optimization scheme that maintains working sets that are associated with these subspaces, and that leads to sparse models.",
                    "label": 0
                },
                {
                    "sent": "So here Central station how this works.",
                    "label": 0
                },
                {
                    "sent": "So we have here a part of speech tagging problem.",
                    "label": 0
                },
                {
                    "sent": "We have labeled example.",
                    "label": 0
                },
                {
                    "sent": "John ate the cat and we have the true label sequence.",
                    "label": 0
                },
                {
                    "sent": "It's noun, verb, determiner, noun.",
                    "label": 0
                },
                {
                    "sent": "We have fixed working sets for all other examples and we start with an empty working set for simplicity.",
                    "label": 0
                },
                {
                    "sent": "So this is the optimization in one view, right?",
                    "label": 0
                },
                {
                    "sent": "So note that both optimized, so the optimization for label examples are independent and the optimization for the unlabeled examples are.",
                    "label": 0
                },
                {
                    "sent": "Dependent on each other so.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Turbie algorithm starts to predict the top scoring sequence, and in this case it says noun verb noun, right?",
                    "label": 0
                },
                {
                    "sent": "It's not the correct one.",
                    "label": 0
                },
                {
                    "sent": "And we have to perform an update that is.",
                    "label": 0
                },
                {
                    "sent": "We add the most strongly violated constraint to the working set.",
                    "label": 0
                },
                {
                    "sent": "And we instantiate the corresponding dual parameter.",
                    "label": 0
                },
                {
                    "sent": "And since we only instantiate dual parameters if we see a constraint in the training process.",
                    "label": 0
                },
                {
                    "sent": "We implicitly consider all others to be equal to 0 right, and therefore we have sparse models.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So after that we update the dual parameters or in this case this one dual parameter by solving aquatic program.",
                    "label": 0
                },
                {
                    "sent": "And we can repeat this.",
                    "label": 0
                },
                {
                    "sent": "If we if it turns out that the prediction is again incorrect now, it's now in determining determining known.",
                    "label": 0
                },
                {
                    "sent": "We just repeat all the steps, that is, we add the most strongly violated constraint to the working set.",
                    "label": 0
                },
                {
                    "sent": "We instantiate the corresponding Alpha we saw server quadratic program that optimizes both Alpha variables and.",
                    "label": 0
                },
                {
                    "sent": "We can repeat this until we arrive.",
                    "label": 0
                },
                {
                    "sent": "No more margin violations.",
                    "label": 0
                },
                {
                    "sent": "So this is pretty much the regular support vector machine for structured output variables, right so?",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But what happens in the case of unlabeled examples?",
                    "label": 1
                },
                {
                    "sent": "So here we have an unlimited example.",
                    "label": 1
                },
                {
                    "sent": "It's John, went home and if you now have two views.",
                    "label": 0
                },
                {
                    "sent": "And we have fixed working sets in each view for other examples and we start again with empty working sets for the current example.",
                    "label": 0
                },
                {
                    "sent": "So both Kirby algorithms start to decode the top scoring prediction.",
                    "label": 0
                },
                {
                    "sent": "One says none were determined.",
                    "label": 0
                },
                {
                    "sent": "Verb, noun and one says noun, verb, verb.",
                    "label": 1
                },
                {
                    "sent": "So both disagree on their prediction and we perform an update.",
                    "label": 0
                },
                {
                    "sent": "And again we add the most strongly violated constraint to the working set in each view, but this time.",
                    "label": 0
                },
                {
                    "sent": "We treat the prediction of the peer review as the true labeling.",
                    "label": 0
                },
                {
                    "sent": "So after that we optimize the parameters by solving two quadratic programs, one in each view, and we can repeat this until the driver consensus.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's have a look at some empirical results.",
                    "label": 0
                },
                {
                    "sent": "The task in the bio creative data set is to detect gene and protein names in biomedical papers.",
                    "label": 1
                },
                {
                    "sent": "We have a lot of features including letter N, gram surface clues and things like that.",
                    "label": 1
                },
                {
                    "sent": "We apply a random feature split.",
                    "label": 1
                },
                {
                    "sent": "And, um.",
                    "label": 1
                },
                {
                    "sent": "Yeah, the baseline is of course, and all other following baselines are trained on the concatenated views.",
                    "label": 0
                },
                {
                    "sent": "That is, they get all their valuable features.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here are the results.",
                    "label": 0
                },
                {
                    "sent": "We have 410 labeled examples and a variant of varying number of unlabeled examples that is depicted on the wire on the X axis.",
                    "label": 0
                },
                {
                    "sent": "Some we received some token accuracies for these.",
                    "label": 0
                },
                {
                    "sent": "Training set.",
                    "label": 0
                },
                {
                    "sent": "And first of all, we see that the semi supervised support vector machine leads to more accurate predictions as the baseline, right?",
                    "label": 0
                },
                {
                    "sent": "So what we also see here is that.",
                    "label": 0
                },
                {
                    "sent": "There seems to be a positive or the number of unlabeled examples seems to be kind of positively correlated with accuracy, right?",
                    "label": 1
                },
                {
                    "sent": "That is.",
                    "label": 0
                },
                {
                    "sent": "We might conclude that if we add more unlabeled examples, we might gain accuracy.",
                    "label": 0
                },
                {
                    "sent": "We also Eve.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I did the semi supervised SVM on a natural language parsing test.",
                    "label": 0
                },
                {
                    "sent": "Here we use the Wall Street Journal corpus and the corpus.",
                    "label": 1
                },
                {
                    "sent": "We transform the involved grammars into context free grammars in order to apply the CKY parser.",
                    "label": 0
                },
                {
                    "sent": "We applied a loss based on the F1 measure and used local features.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are the results we have.",
                    "label": 0
                },
                {
                    "sent": "Again, the number of unlabeled examples on the on the X axis and the F1 score on the Y axis.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Here we see that the semi supervised support vector machine leads to much better predictions in F1 score for even no unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "And this is due to averaging two independently trained hypothesis, right?",
                    "label": 0
                },
                {
                    "sent": "So without unlabeled examples, we train two regular support vector machines independently of each other and average them.",
                    "label": 0
                },
                {
                    "sent": "So, but if we increase the number of unlimited examples, we further improve the of 1 scope.",
                    "label": 0
                },
                {
                    "sent": "So, however.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This gain in performance comes at the cost of a longer training time.",
                    "label": 0
                },
                {
                    "sent": "Right, so the semi supervised SVM scales quadratically in the number of analytic samples and.",
                    "label": 1
                },
                {
                    "sent": "Yeah, it's more expensive to optimize.",
                    "label": 0
                },
                {
                    "sent": "So to sum up, we saw that we can make use of unlabeled data in discriminative structure learning without making a cluster assumption.",
                    "label": 0
                },
                {
                    "sent": "However, it's not always fair to assume that the data has a class structure, and therefore I would like to also present you a transductive version of the structural support vector machine that makes an explicit cluster assumption.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Required the kernels are different in different views, so how do you choose?",
                    "label": 0
                },
                {
                    "sent": "So we split the attributes randomly, right?",
                    "label": 0
                },
                {
                    "sent": "So we compute all kernels on the different sets of attributes.",
                    "label": 0
                },
                {
                    "sent": "Right, right?",
                    "label": 0
                },
                {
                    "sent": "Just based on different attributes.",
                    "label": 0
                },
                {
                    "sent": "Sorry, sensitive.",
                    "label": 0
                },
                {
                    "sent": "Yeah we we started with distinct splits.",
                    "label": 0
                },
                {
                    "sent": "We started to have a token based split and a surface can split like on the one hand we have a letter N grams.",
                    "label": 0
                },
                {
                    "sent": "The identity and things like that and on the other hand we had capitalization features or numbers or whatever and it turned out that the random feature split works much better than this service clue versus token you slip and so this means that.",
                    "label": 0
                },
                {
                    "sent": "That there has to be a split.",
                    "label": 0
                },
                {
                    "sent": "So if this is the average split split right and we.",
                    "label": 0
                },
                {
                    "sent": "The results here are averages over 100 repetitions.",
                    "label": 0
                },
                {
                    "sent": "And everything is random, right?",
                    "label": 0
                },
                {
                    "sent": "The training data is random.",
                    "label": 0
                },
                {
                    "sent": "The attributes players at random.",
                    "label": 0
                },
                {
                    "sent": "So this means that we observe here kind of performance for an ad for an average split.",
                    "label": 0
                },
                {
                    "sent": "So, but this also means that there has to be a split that even works better, right?",
                    "label": 0
                },
                {
                    "sent": "The question is how to find that split and this remains an open challenge, though I haven't found him away to determine before hand if this player is good or not or what are the criteria for goods played right?",
                    "label": 0
                },
                {
                    "sent": "So it's.",
                    "label": 0
                },
                {
                    "sent": "True.",
                    "label": 0
                },
                {
                    "sent": "The fact that you're only training on 40 labeled and 200 unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "Yes, it is slow, but the slowest in this version is the CKY parser that we use.",
                    "label": 0
                },
                {
                    "sent": "But the training is slow also, right?",
                    "label": 0
                },
                {
                    "sent": "Anymore questions.",
                    "label": 0
                },
                {
                    "sent": "Even when you don't have unlabeled examples, yeah, 'cause we have two independently trained hypothesis, right?",
                    "label": 0
                },
                {
                    "sent": "And we just average then this kind of assembly effect here.",
                    "label": 0
                },
                {
                    "sent": "Using the whole features together as a baseline, yeah, the baseline gets all the features right.",
                    "label": 0
                },
                {
                    "sent": "So the baseline gets all the features and the green line.",
                    "label": 0
                },
                {
                    "sent": "The semi supervised approach.",
                    "label": 0
                },
                {
                    "sent": "We split the features into 2 views.",
                    "label": 0
                },
                {
                    "sent": "We train 2 independently trained, trained independently to support vector machines and then we average their results for no unlabeled examples, right?",
                    "label": 0
                },
                {
                    "sent": "Relations with both when you click yes, of course we can do anything.",
                    "label": 0
                },
                {
                    "sent": "We can assume that the feature sets are correlated or not.",
                    "label": 0
                },
                {
                    "sent": "Probably pretty correlated.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "Mini.",
                    "label": 0
                },
                {
                    "sent": "Well, training 2 views is expensive in terms of execution time, so actually I didn't do it.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I never tried it out so I just focused on 2 views.",
                    "label": 0
                },
                {
                    "sent": "So mathematically, you can write it down for an arbitrary number of views, but I just applied 2 views, sorry.",
                    "label": 0
                },
                {
                    "sent": "Interesting.",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "Yeah, probably we could have done pure code training as an additional baseline, right?",
                    "label": 0
                },
                {
                    "sent": "But we didn't do that.",
                    "label": 0
                },
                {
                    "sent": "That's a good idea, right?",
                    "label": 0
                },
                {
                    "sent": "So I will think about including this one, yeah?",
                    "label": 0
                },
                {
                    "sent": "Further questions or shall I go on?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now the transactive variant, right?",
                    "label": 0
                },
                {
                    "sent": "So you're probably all familiar with the binary transductive support vector machine that has these discrete variables for unlabeled instances.",
                    "label": 1
                },
                {
                    "sent": "And due to this discrete due to these discrete variables, we have a combinatorial optimization problem that is very hard to solve if it's possible to solve it.",
                    "label": 0
                },
                {
                    "sent": "And so, in order to translate this to the structural domain we have, we have to cope with these combinatorial optimization in order to be tractable.",
                    "label": 0
                },
                {
                    "sent": "Or derive an optimization criterion that is tractable.",
                    "label": 0
                },
                {
                    "sent": "So when we tackle this problem by removing the discrete variables and applying continuous and differentiable optimization schemes.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here we have.",
                    "label": 0
                },
                {
                    "sent": "The regular support vector machine with constraints right?",
                    "label": 1
                },
                {
                    "sent": "Now the simplest or easiest way to get rid of all these constraints is to solve the select variables for the constraints antauri, substitute them into the objective, right?",
                    "label": 0
                },
                {
                    "sent": "So this is pretty straightforward, but of course we haven't won anything.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "It's we just rewrite the optimization problem.",
                    "label": 1
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What we do now is we try to make it differentiable.",
                    "label": 0
                },
                {
                    "sent": "And first of all we have the hinge loss.",
                    "label": 0
                },
                {
                    "sent": "That is not differentiable, but so the hinge loss is not differentiable, But the Huber loss is right.",
                    "label": 0
                },
                {
                    "sent": "So we just substitute the Huber loss for the hinge loss and.",
                    "label": 0
                },
                {
                    "sent": "So this is it.",
                    "label": 0
                },
                {
                    "sent": "But we still have a problem and this is the maximum in this equation.",
                    "label": 0
                },
                {
                    "sent": "And the maximum can also be approximated by differential surrogate.",
                    "label": 0
                },
                {
                    "sent": "And this is the softmax function.",
                    "label": 0
                },
                {
                    "sent": "So if you substitute the Huber loss for the hinge loss and the softmax for the Maps for the Max, we get an unconstrained variant of the support vector machine.",
                    "label": 1
                },
                {
                    "sent": "And this is differentiable and we can apply gradient based techniques in order to optimize it.",
                    "label": 0
                },
                {
                    "sent": "So now let's let's have a look how to.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Clude unlabeled examples in the support vector machine.",
                    "label": 1
                },
                {
                    "sent": "So in according to the transductive principle, we want to include unlabeled instances such that they fulfill the margin constraint.",
                    "label": 0
                },
                {
                    "sent": "Right, that is if we decode the top scoring output and we decode the best runner up.",
                    "label": 0
                },
                {
                    "sent": "We want these two guys to fulfill the margin constraint.",
                    "label": 0
                },
                {
                    "sent": "And therefore we design A loss function for the unlabeled examples that allows us to mitigate margin violations in two symmetric ways, right?",
                    "label": 1
                },
                {
                    "sent": "One is associated with the top scoring output in one is associated with the best runner out.",
                    "label": 0
                },
                {
                    "sent": "And once having this loss function, the inclusion of the unlabeled examples is straightforward.",
                    "label": 0
                },
                {
                    "sent": "So we simply use.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the same approach As for the labor examples, we just switch or exchange the loss function and we spend another variable that measures the overall influence for this unlabeled examples.",
                    "label": 1
                },
                {
                    "sent": "So unfortunately due to the shape of this new loss function, the problem is no longer convex.",
                    "label": 0
                },
                {
                    "sent": "But before we get into details and have a look at empirical results, let me show you.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The execution time.",
                    "label": 0
                },
                {
                    "sent": "And here we see as the solid line is the gradient based optimization of the support vector machine and the dashed line is.",
                    "label": 0
                },
                {
                    "sent": "The same program, so the same code, just the optimization is exchanged.",
                    "label": 0
                },
                {
                    "sent": "And here we used the quadratic program approach with seaplex.",
                    "label": 0
                },
                {
                    "sent": "So the gradient based optimization is a bit more efficient and a bit faster, but the real benefit we we can see the real benefit when we add unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "So these dotted line here is the transductive version, and here we add five times as many unlabeled examples as it gets labeled examples.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The inclusion of the unlabeled examples is pretty efficient, right?",
                    "label": 1
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let's have a look at some empirical results.",
                    "label": 0
                },
                {
                    "sent": "We used the Spanish Newswire data set, where the task is to detect names of persons and locations, organizations, and other names.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of features again and.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here are the results.",
                    "label": 0
                },
                {
                    "sent": "So the we have the number of unlabeled examples on the X axis and the token error on the Y axis.",
                    "label": 1
                },
                {
                    "sent": "We applied the hidden Markov SPM bias mean altoon.",
                    "label": 0
                },
                {
                    "sent": "As a baseline.",
                    "label": 0
                },
                {
                    "sent": "And we see that the transductive variant has a bit lower error compared to that baseline.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think that's it, and unfortunately this is the only task where it works out right so?",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All in all, other chairs that we tried out, it was pretty much like this here.",
                    "label": 0
                },
                {
                    "sent": "Here we have again a sequential task.",
                    "label": 0
                },
                {
                    "sent": "It's Galaxy data set by left here, L. And we use this one in order to compare two different approaches of including unlabeled examples.",
                    "label": 0
                },
                {
                    "sent": "So we wanted to compare our transductive support vector machine with the inclusion of unlabeled examples, paragraph Laplace link.",
                    "label": 0
                },
                {
                    "sent": "So the data is generated by a local hidden Markov model, so the probability that observing another red state when we are in a red state is 80% and that we observe a blue state when we are in a red state is 20%.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "Since we assume.",
                    "label": 0
                },
                {
                    "sent": "Before I tell you that I should say that we have 100 sequences of length 20, right so depicted on the X axis of these figures are the number of labeled examples and 100 minus the number of the X axis is the number of unlabeled examples and test data.",
                    "label": 0
                },
                {
                    "sent": "I'm so.",
                    "label": 0
                },
                {
                    "sent": "Since both semi supervised approaches, or we assume both semi supervised approaches to be orthogonal right?",
                    "label": 0
                },
                {
                    "sent": "That is, we test on all four combinations of.",
                    "label": 0
                },
                {
                    "sent": "Support vector machine transductive support vector machine and supervised RBF kernel and semi supervised Laplacian kernel.",
                    "label": 1
                },
                {
                    "sent": "So when the results are as follows, first of all, I think we clearly see that the graph Laplacian is very well suited for the problem at hand, right?",
                    "label": 0
                },
                {
                    "sent": "So it leads to an average decrease in error rate of, I think about 10%.",
                    "label": 0
                },
                {
                    "sent": "So Moreover, we see that there's absolutely no difference between the transductive variant of the support vector machine and the regular support vector machine.",
                    "label": 0
                },
                {
                    "sent": "So the question is now, why does the transductive support vector machine failed to make use of this unlabeled data?",
                    "label": 0
                },
                {
                    "sent": "And besides the non convexity of the optimization problem, that might also.",
                    "label": 0
                },
                {
                    "sent": "Be a point here.",
                    "label": 0
                },
                {
                    "sent": "Um, we have.",
                    "label": 0
                },
                {
                    "sent": "Another explanation would be different cluster assumptions.",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The data is generated in a local by this local model and the graph Laplacian captures this local neighborhood cause it's defined by nearest neighbor graph, right?",
                    "label": 0
                },
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "The transductive support vector machine, on the other hand, makes a global or has a cluster assumption that is defined on complete sequences right, and this seems to be not.",
                    "label": 0
                },
                {
                    "sent": "Not appropriate in this setting.",
                    "label": 0
                },
                {
                    "sent": "So, but we have to.",
                    "label": 0
                },
                {
                    "sent": "We need more studies on that so.",
                    "label": 0
                },
                {
                    "sent": "Joe.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I guess.",
                    "label": 0
                },
                {
                    "sent": "Um, yeah, I think.",
                    "label": 0
                },
                {
                    "sent": "10 minutes OK then I will quickly run through the last slides and present your case.",
                    "label": 0
                },
                {
                    "sent": "Study an email back detection and that's the paper that won this award, right?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before that I should say that.",
                    "label": 0
                },
                {
                    "sent": "We have here a.",
                    "label": 0
                },
                {
                    "sent": "A clustering problem.",
                    "label": 0
                },
                {
                    "sent": "But we have access to a ground truth, right?",
                    "label": 0
                },
                {
                    "sent": "That is, we have an unsupervised approach and we have a kind of supervised approach on at the same time and in some it is kind of a semi supervised approach and wonderfully fits into the store, right?",
                    "label": 0
                },
                {
                    "sent": "So in order to motivate the problem setting.",
                    "label": 0
                },
                {
                    "sent": "We have to have a closer look at the characteristics of electronic messaging, right?",
                    "label": 0
                },
                {
                    "sent": "So about 80% of all emails in the world are considered to be spam.",
                    "label": 0
                },
                {
                    "sent": "And approximately 80% of these 80% are generated by only a few spammers.",
                    "label": 1
                },
                {
                    "sent": "And these spammers maintain one or more templates at a time to generate their spans automatically.",
                    "label": 0
                },
                {
                    "sent": "And they exchange their templates rapidly.",
                    "label": 1
                },
                {
                    "sent": "For an email server, this means that in a short time frame it receives many emails that are generated with the same template, so the goal is now to detect these batches in the data stream and the idea behind it is if we were having the best information, we could use it either from black or white listing, or we can use it for improving spam or non spam classification.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's an example of two messages that are generated by the same template.",
                    "label": 0
                },
                {
                    "sent": "At first said they look alike, but if we have a closer look then we see that they differ in, for instance, the address, the names we have, a random insertion of spaces here.",
                    "label": 0
                },
                {
                    "sent": "The numbers are different.",
                    "label": 0
                },
                {
                    "sent": "Even complete sentences are different.",
                    "label": 0
                },
                {
                    "sent": "But the semantics stays always the same.",
                    "label": 0
                },
                {
                    "sent": "So we can make use of the pairwise similarities between emails that are generated by the same badge.",
                    "label": 0
                },
                {
                    "sent": "By re phrasing the chest of vegetation as a task of correlation clustering.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Correlation clustering?",
                    "label": 0
                },
                {
                    "sent": "Equivalent to the solution of correlation clustering is equivalent to a solution in a Poly cut of Poly cut in the fully connected graph, and the edges are weighted with the similarities of the connected nodes.",
                    "label": 1
                },
                {
                    "sent": "So what correlation clustering essentially does is it maximizes the intra cluster similarity?",
                    "label": 0
                },
                {
                    "sent": "So we identify the phone clusters with our batches and we're pretty much done.",
                    "label": 0
                },
                {
                    "sent": "So the only problem is that we don't know which similarity measure leads to true clustering, right?",
                    "label": 0
                },
                {
                    "sent": "And since we don't know that we use a parameterized similarity measure and we learn the weights of the of this similarity measure in the training process.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We extract pairwise features.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Office of Pairs of emails like the edit distance of the bodies or TF IDF similarities.",
                    "label": 0
                },
                {
                    "sent": "Edit distance of the subject line.",
                    "label": 1
                },
                {
                    "sent": "Sorry, and now an input looks as the following form.",
                    "label": 0
                },
                {
                    "sent": "We have a data stream of key messages and the corresponding output is binary matrix where the elements indicate whether 2 messages are in the same cluster or not.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Of course, this is a combinatorial optimization problem.",
                    "label": 0
                },
                {
                    "sent": "It's NP complete and all that an so we can sort of an approximate variant instead, where we use continuous variables for the disk.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So plugging everything into a support vector machine, we derive the following optimization criterion.",
                    "label": 0
                },
                {
                    "sent": "In order to to derive a more compact variant, we can apply the technique of law segmented influence by Ben Taskar.",
                    "label": 0
                },
                {
                    "sent": "And we replace the computation of the worst margin violated by.",
                    "label": 0
                },
                {
                    "sent": "The corresponding Lagrangian dual.",
                    "label": 0
                },
                {
                    "sent": "And if we do that, we obtain a minimization within a minimization and we can combine them.",
                    "label": 0
                },
                {
                    "sent": "And we obtain one quadratic program that has cubic number of constraints.",
                    "label": 0
                },
                {
                    "sent": "So this means it's not tractable for practical applications, right?",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But this solution does not exploit the data stream.",
                    "label": 0
                },
                {
                    "sent": "So in order to do that, or to approximate the solution, we can assume that whenever a new email arrives, we only have to integrate this email into the existing clustering and the clustering of the previous emails remains fixed.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So mathematically, this means that we decompose the objective into a constant part that depends only on the clustering of the previous emails and the part that depends only of the clustering of the latest email.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In order to run finish quite soon, just tell you that we observed no significant differences.",
                    "label": 1
                },
                {
                    "sent": "We tried out a lot of learning algorithms with a lot of different decomposing techniques and all perform pretty much the same.",
                    "label": 0
                },
                {
                    "sent": "So neither no.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Result was significant.",
                    "label": 0
                },
                {
                    "sent": "However, we don't want to be better.",
                    "label": 0
                },
                {
                    "sent": "We just want to be quicker, right?",
                    "label": 0
                },
                {
                    "sent": "And he received at the sequential approximation in blue is very quick.",
                    "label": 1
                },
                {
                    "sent": "Note that the X is on the Y axis, the Y axis is scaled in log scale, right?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the final experiment for today.",
                    "label": 0
                },
                {
                    "sent": "He wanted to find out if having the best information can improve the classification of spam or non spam units and.",
                    "label": 0
                },
                {
                    "sent": "What we did is we used the sequential clustering approach.",
                    "label": 0
                },
                {
                    "sent": "And, um, we extracted features of the found patches and these features were very simple, so we use the size of the badge or binary features, for instance, indicating if all subject lines are equivalent.",
                    "label": 0
                },
                {
                    "sent": "So and if we add this features to a bag of words representation, we see that the misclassification risk reduces of about 40%, right?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "If you consider that there are 7 billion emails per day, this is an enormous amount, right?",
                    "label": 0
                },
                {
                    "sent": "So what I think I will.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Come to an end and.",
                    "label": 0
                },
                {
                    "sent": "Let me conclude by summing up.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I presented you today.",
                    "label": 0
                },
                {
                    "sent": "I started with an introduction to semi supervised learning with Co. Regularised regression.",
                    "label": 0
                },
                {
                    "sent": "Um, after that we lifted the lessons learned to the structure domain and device.",
                    "label": 0
                },
                {
                    "sent": "The code VM that relies on the consensus maximization principle and the transductive support vector machine that relies on a cluster assumption.",
                    "label": 1
                },
                {
                    "sent": "Empirically, it turns out that we actually can improve structure learning by including unlabeled examples, and sometimes adding more unlimited examples even improves performance.",
                    "label": 0
                },
                {
                    "sent": "So in the supervised clustering test, we saw that.",
                    "label": 1
                },
                {
                    "sent": "We saw how we can derive an efficient optimization by exploiting the streaming nature of the data.",
                    "label": 0
                },
                {
                    "sent": "And we saw that the features extracted from the found patches can lead to a reduction in the misclassification error.",
                    "label": 0
                },
                {
                    "sent": "So I'm looking for a great workshop and thanks for your attention.",
                    "label": 0
                }
            ]
        }
    }
}