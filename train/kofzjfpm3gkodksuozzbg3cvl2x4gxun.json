{
    "id": "kofzjfpm3gkodksuozzbg3cvl2x4gxun",
    "title": "Bayesian learning of sparse factor loadings",
    "info": {
        "author": [
            "Magnus Rattray, School of Mathematics, University of Manchester"
        ],
        "published": "Oct. 9, 2008",
        "recorded": "September 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/bark08_rattray_blospl/",
    "segmentation": [
        [
            "So thanks for providing me an.",
            "There could have been an even more difficult slot 'cause I thought it was 4:30 so I was just actually coming down here to finish the talk, but then found out I was already on so.",
            "So I'm going to talk about.",
            "Well, the plan for the talk actually was to do some theory about how good the sparsity priors were using what were and then present some biological results.",
            "My PhD students laughing 'cause he was supposed to provide the results.",
            "Luckily though, the theory showed that the sparsity priors were in fact not that good, and therefore that gave me something to talk about.",
            "But it may be a bit unfortunate for Kevin's project."
        ],
        [
            "So I'll give a brief overview of popular sparsity price Ann and I was sort of motivate things by talking about factor analysis, so we're using sort of sparse factor analysis to infer regulatory networks in biological models of cell.",
            "Anne.",
            "And then I'll talk about the theory an.",
            "And the theory will be for very simple single factor model that we can actually analyze an obeya kind of average case, statistical mechanics type theory.",
            "Our compare the theoretical results to MCMC and just look at different data distributions and how the relationship between the data distribution and the prior affect the problem of identifying sparsity.",
            "I have a quick look at the one prior that's like really, this is all quite recent work.",
            "It's not really finished yet, so I've just started looking at L1, so I'll show some quick results with that and then discuss some of the issues."
        ],
        [
            "So.",
            "I mean.",
            "Add zip in was talking about objective Bayesians and subjective Bayesians yesterday and.",
            "I think I'm going to subjective Bayesian, but I think in practice most people are pragmatic versions and they tend to use priors.",
            "Because it makes the modeling tractable as well as because they believe in the priors and sparsity priors definitely seem to fall into the category of convenient rather than realistic."
        ],
        [
            "Anne.",
            "So for instance, the L1 prior tends to be chosen.",
            "In non Bayesian settings, it's chosen because you can solve the parameter estimation problem.",
            "That sort of retains the convexity of the optimization and so on.",
            "And in Bayesian settings you can get for certain models.",
            "It gives you a unimodal posterior, so it's also nice in that case, so that's convenient really.",
            "I don't think people believe that.",
            "Primer isn't necessarily an."
        ],
        [
            "I look like that.",
            "Aard is very popular and so arodys is quite weird.",
            "I think you have a hyperparameter but you have as many hyperparameters as parameters and then you do type to maximum likelihood over them, which isn't very Bayesian but seems to.",
            "You know it works OK and people have shown you know Mike Tipping and people have shown that it has some nice properties and does hopefully something reasonable."
        ],
        [
            "Anne.",
            "An I'm mainly gonna be using mixture style priors so mixture prior where you have a sort of a bit of probability mass at 0.",
            "And then you add, say, zero mean Gaussian.",
            "So you can either be 0 or you can come from a Gaussian an.",
            "And perhaps these aren't so popular in.",
            "Machine learning because they get nasty discrete parameters.",
            "So maybe it doesn't work so well when you're doing variational Bayes in these sorts of things are quite popular in statistics where people are doing MCMC and also they lead to sort of thresholding rules as well for doing sparsity in a non Bayesian setting.",
            "So these are used quite a lot.",
            "Am but really the question."
        ],
        [
            "I want to address is.",
            "Is it OK not to worry about whether these actually fit the data?",
            "OK, so.",
            "Searching for sparse structures is sort of.",
            "I don't know Semiparametric in the sense that you only care about half of the parameters because you sort of.",
            "We believe the sparsity's there that's something 0.",
            "But the other stuff maybe?",
            "Where we don't know exactly what the distribution of it is.",
            "We hope that learning about the sparsity will be enough to give us good performance.",
            "So.",
            "But but our data maybe doesn't satisfy these sorts of conditions, so we should.",
            "And the other part is this part.",
            "OK, so the part parts that aren't sparse?",
            "We give them some sort of fairly vague prior.",
            "We don't really give them anything, so we believe the sparsity part is really zero perhaps, but maybe the other part.",
            "Is just catching everything else you know, so it's not very informative.",
            "Priors sort of quite uninformative.",
            "Anne."
        ],
        [
            "So just to motivate things we've been looking at factor analysis for network inference, so the idea here is that we have some log expression data Ann.",
            "This is a bit like some of this stuff Neil was talking about.",
            "I'm not going to talk about time series, but we can put time series priors and the latent variables.",
            "And the idea here is that we're going to treat the activity of transcription factors which regulate expression as latent because they might be regulated post transcriptionally, which means that we can't just use expression data as a proxy for the protein levels.",
            "So we can't measure the protein levels easily, but we can measure the expression levels of the targets and therefore we're going to treat the proteins as latent variables.",
            "And the factor loadings are this parameter matrix W. Anne.",
            "And in this application W is definitely sparse in the sense that genes tend to be regulated by a small number of transcription factors.",
            "So we really know that we're looking for sparse matrices W. An and there are bunch of methods for inferring sparse W from the data Y, and these were reviewed quite recently.",
            "So some things you can do is you can just do factor analysis and then rotate the factors into the most sparse.",
            "Rotation, but that doesn't seem to work too well.",
            "Or one can put in explicit priors and do something Bayesian to get sparsity in that matrix.",
            "Anne."
        ],
        [
            "So we've been using this mixture prior and it's quite nice because it leads to quite a nice Gibbs sampler for this problem.",
            "Anne.",
            "And in our application we actually have some knowledge about the hyperparameters, so we know from chip chip or motif data which genes are likely to be regulated by which transcription factors.",
            "So we can put that in so the whole Bayesian framework is very nice for us because we really have some priors.",
            "AM.",
            "If we don't have priors or if we don't have priors about everything, maybe we want to estimate some of the hyperparameters by MCMC.",
            "So if we're going to do any estimation by MCMC of the hyperparameters.",
            "Then we want to make sure that the marginal likelihood after we've marginalized at the parameters is doing something useful for us in terms of identifying these hyperparameters.",
            "And that's mainly what I'm going to focus on today, so we've been using a lot of empirical priors.",
            "But really, I want to start using more.",
            "Data driven sort of hyperparameter estimation, so I want to make sure that that's going to work OK, so that's wrong."
        ],
        [
            "I'm talking about today."
        ],
        [
            "Anne.",
            "Well, OK, there's another.",
            "So this is another thing we're interested in is actually treating measurements of Zed's data, but that's a separate issue, really.",
            "But we could have in this case.",
            "Why is the only data and any other information we have?",
            "Maybe we could put it into hyper prior so it could be like a beta distribution or something."
        ],
        [
            "So the way that Gibbs sampler works is that.",
            "You re parameterized, your weights as binary variables times Gaussians.",
            "And."
        ],
        [
            "The nice the reason it works is because you can integrate out the bees before sampling the axes.",
            "And that allows you to basically sample the WS like you don't have to sample.",
            "The X is in the bees sort of on the left and the right.",
            "You can sample the whole W 'cause you sample.",
            "The X is first and then the bees.",
            "Anne.",
            "I mean, if you have 10 factors or less, you can sample all the axes at once.",
            "But if you have more factors than you have to leave some of the X is on the right hand side when you're sampling some of the other X is OK, so so we have hundreds of factors, so we can't sample.",
            "RBX is an, but nevertheless it seems to converge quite well in the size of problems we're looking at there not sort of massive problems.",
            "It's like thousands of genes, hundreds of factors, so it's not huge, but you know it is non trivial.",
            "Anne.",
            "And the other thing that saved that makes this Gibbs sampling fast is that because of the sparsity.",
            "The sampling of being zed, which are Gaussians, those have sparse covariance matrices.",
            "So you can use, uh, sparse Cholesky decomposition and everything is really fast in the sampling, so effects remains sparse during the sampling procedure.",
            "Then, being zed sampling is also really fast, and everything goes nice and quickly.",
            "And so that's why this kind of mixture, prior with the binary variables seems to be quite nice for this kind of Gibbs sampling, whereas if we had some sort of continuous.",
            "Prior like an hour one or something like that, everything would remain dense and things would be perhaps a bit slower.",
            "And with if we use conjugate hyper priors, we can sample the hyperparameters as well and so that's fine as well.",
            "So we don't need to do type 2.",
            "Maximum likelihood.",
            "Either we can do everything by MCMC, which which always is nice."
        ],
        [
            "Anne.",
            "So I want to analyze that model so I'm not OK.",
            "I'm not going to present any results 'cause it's sort of ongoing.",
            "We've done some.",
            "Work which is already published, but I really want to focus for this workshop on the theoretical issues.",
            "So let's look at really simple case Bayesian PCA.",
            "With a single factor.",
            "An and a sparsity prior on the parameter vector.",
            "OK, so that's that's the model we're using."
        ],
        [
            "Anne.",
            "We want to study average behavior over datasets produced by some teacher distribution.",
            "So this is one of these sort of student teacher scenarios."
        ],
        [
            "Things.",
            "And the teacher is identical except for the parameter vector.",
            "Which comes from two different distributions and.",
            "When I did this, I chose the one."
        ],
        [
            "I'll show you this discussion first.",
            "OK, so once the same form.",
            "As the model, but with different parameters."
        ],
        [
            "Possibly.",
            "And another one is very similar.",
            "And but instead of using a Gaussian for the.",
            "Non sparse part.",
            "I just use a Delta function.",
            "But I put all the parameters one standard deviation away.",
            "In the gaussian.",
            "So instead of using a Gaussian optics user Delta function.",
            "But the length of the vectors is the same, so it's kind of a similar as I could make it really an.",
            "And I have to say that I only used this because.",
            "When I was doing the numerics in the theory, this made things go quicker.",
            "I thought it was going to be identical.",
            "An I I just it was just a speed up 'cause it's me doing some numerical integrals so it's kind of using this and everything was wrong and I sort of thought I keep checking my code, check my theory, check my code so in the end I thought well maybe I'll just use a Gaussian prior.",
            "And see if that works and then everything started working and then I realized that something weird was going on with this prior 'cause I thought that these two things would be identical.",
            "I mean they are identical for PCA without the sparsity.",
            "So I was."
        ],
        [
            "Little bit surprise.",
            "And that one can tell you.",
            "So the way the theory goes is we compute the marginal likelihood averaged over datasets drawn from either of those two distributions.",
            "And we take the limit of high dimensional data.",
            "Keeping the ratio of the number of training points to the dimension fixed, yeah.",
            "Tired.",
            "You would use two different teachers and.",
            "Yeah, I."
        ],
        [
            "I'm going to look at both teachers.",
            "Know that I'm going to use one or the other, OK?"
        ],
        [
            "I could make some.",
            "Yeah, OK so.",
            "So we're going to hold the ratio of the number of examples to that."
        ],
        [
            "It's dimension fixed.",
            "We're going to look at as well as looking at the marginal likelihood.",
            "We'll look at some functions of the mean posterior parameter, so we'll look at the.",
            "Cosine angle between the true parameter and the mean posterior estimate.",
            "And we'll look at the log likelihoods of test data.",
            "If you like data from the teacher distribution.",
            "An we could also average that.",
            "I mean, we've done it for the mean posterior.",
            "We could average over the posterior for that second one.",
            "It doesn't really affect the results.",
            "Is the same.",
            "I've just used this one because it actually appears later in in the marginal likelihood, so it's nice to introduce it here."
        ],
        [
            "Anne.",
            "And we got really good agreement with simulations.",
            "For the case we're interested in, which is small offer, which is where you have a small number of examples compared to the data dimension.",
            "So often we have data is thousands of dimensions and we maybe have 10s or hundreds of examples or something, so that's the kind of case we're interested in.",
            "Anne."
        ],
        [
            "So it's kind of similar to calculation that's already been done.",
            "On a classification problem.",
            "So it's already in.",
            "It's very similar, so we're using the replica method.",
            "And I don't really want to go into the details of it, but basically.",
            "The the solution has a has two parts for the marginal likelihood.",
            "One part is basically the log probability of data from the teacher distribution for the mean posterior parameter.",
            "And then we get some sort of entropic terms as well, and so you can see this first term here is proportional to Alpha, which is the number of training examples divided by the dimension.",
            "So as you get lots of training examples, you eventually converge to the log marginal likelihood being dominated by.",
            "The likely hood part.",
            "So really, it's in the nature of these entropic terms that we get differences in performance."
        ],
        [
            "And the average case becomes typical.",
            "If you have very large data set dimension.",
            "So there's a sort of self averaging behavior, and in these things."
        ],
        [
            "So first just look at standard PCA, so standard PCA is already was done ages ago an.",
            "In a slightly different setting, so people who studied this before did it for spherical distributions of parameters, so they constrain them onto the unit sphere and Bayesian inference for without different length parameter vectors.",
            "So the only difference here is that we reliably parameter vectors have different lengths.",
            "Anne.",
            "And we get this.",
            "These kind of phase transitions, which are quite interesting.",
            "So below certain number of training examples this is so called retarded learning regime where you can't learn anything about the directions in the data.",
            "There's no learning available.",
            "Manfreds also done some rigorous bounds on these kind of phase transitions as well.",
            "So the replica method for those in the know isn't rigorous, it's just.",
            "Correct?",
            "Often.",
            "But I see it's probably not correct in all these, although regions of this phase space.",
            "M. So then so when Alpha gets bigger than this.",
            "1 / T squared term.",
            "Then there's a second order phase transition and you begin to learn something and so your performance goes up and then converges.",
            "When you have lots of data to the true parameters.",
            "Anne.",
            "And then the the main thing here is that the learning performance for standard PCA only depends on the length of the teacher parameter vector.",
            "OK, so it doesn't depend on the particular distribution at all, so the two distributions are.",
            "They have the same length, so PCA does the same thing on those two, right?",
            "So there's no difference in performance for standard PCA.",
            "Yeah, yeah it is.",
            "I fix the noise to one but yeah.",
            "An yeah so in fact.",
            "Yeah, people have done.",
            "People got some rigorous results recently on the eigenvalue spectrum and the.",
            "What was the?",
            "When you study the eigenvalue Spectra, what you see actually is, it seems to be that the results are very universal.",
            "In terms of the distributions that you use, so for date, well, OK, that's for.",
            "Just data without signal in it.",
            "So data without a signal in it.",
            "It seems like you get the same spectrum as long as sort of your 4th moment exists or something for your data distribution, so those results are very robust to the particular distribution of data, and because PCA is a sort of variance based method, I think that's maybe not surprising.",
            "You know it's so it's really just using variance information.",
            "It's not really using any higher order information in the data.",
            "Man.",
            "And yeah, these transitions.",
            "They're mirrored in this the eigenvalue spectrum and people you know got rigorous results.",
            "Now for these kind of.",
            "Phase transition behaviors."
        ],
        [
            "Anne.",
            "So this is consistent.",
            "Other other sorry, that's not Bayesian PCA, though, right?",
            "So that's maximum likelihood PCA.",
            "Bayesian PCA, it's not just an eigenvalue problem.",
            "Anne.",
            "So it's consistent with the known results for this faculty."
        ],
        [
            "Ice.",
            "And the only new feature here is we got a new first order phase transition with increasing hyperparameter Lambda.",
            "So Lambda is the weight decay type term, so that's that.",
            "Basically means if you miss specify your prior on the length of these vectors, then if you miss specify it too much, then you can again get no performance.",
            "So you basically shrink your vectors to 0.",
            "Anne."
        ],
        [
            "So this is an example of that, so here we're just increasing this hyperparameter, which is this weight decay term.",
            "And if we use too much, then eventually performance just falls off to 0.",
            "Now with these sort of transitions, there always smoothed out when you look at real data.",
            "OK, you have to look at really high dimensional data to get it is sharp.",
            "There's a different scaling within.",
            "Yeah, yeah, yeah, yeah, that's right.",
            "And OK, but I don't think actually these phase transitions are really elegant in nice and everything, but I don't think they're actually that relevant to to us because I think we're usually quite far away from them.",
            "So if we're close to this retarded learning transition, then forget it.",
            "We're not going to learn anything anyway, 'cause that's in the region where we don't have enough data to learn.",
            "This transition happens far from the optimal hyperparameter.",
            "So for using any kind of marginal likelihood optimization or anything, we're going to true that good place for the hyperparameters here.",
            "And as long as we're not being completely crazy when we're setting our hyperparameters, we're not going to end up close to these transitions.",
            "The only time we end up close to this transition is when we're also close to the retarded learning transition, and at that point, this transition occurs at the optimal hyperparameter.",
            "So if we have.",
            "Only just enough data to learn.",
            "Then the this transition happens at the optimal point in the marginal likelihood, so that's.",
            "But it's a kind of only a theoretical interest, were never interested in learning in that regime."
        ],
        [
            "So where am I going to consider learning away from these transitions?"
        ],
        [
            "So I've already described it."
        ],
        [
            "Datasets."
        ],
        [
            "Same form is a prior, a different form an."
        ],
        [
            "And I've said they both give identical performance standard PC and they both give identical performance if we know where the sparsity is.",
            "So if someone tells us is sparsity, these things give the same."
        ],
        [
            "Performance.",
            "So this is a theory versus some MCMC results for one data set."
        ],
        [
            "And then this is an average over 10 datasets, so you see when you average you get better fit to the theory and this is increasing the size of the data set.",
            "And the teacher sparsity is not .2.",
            "So that means that 80% of the.",
            "Are zeros in 20% have some sort of signal?",
            "Of the parameter vectors.",
            "An and as you'd hope, the performance.",
            "So this row here is the overlap between the true parameter and the mean posterior an that improves an it's maximized close to the true sparsity.",
            "So if you're closer true sparsity, do well here.",
            "And so that's not surprising, that's."
        ],
        [
            "It.",
            "If you look at this row and you plotted against both of the hyperparameters, then you see that, well, the sort of.",
            "At likelihoods on test data.",
            "Is maximized at the right place?",
            "And the overlap between the parameter vectors is rather insensitive to the actual this hyperparameter, which is the length, and that's quite good, because often people when they do this MCMC, they just set Lambda really small.",
            "Don't really care about it, so they don't bother estimating it, so that's going to work OK in this setting."
        ],
        [
            "And then if you look at the marginal likelihoods from the theory, it's centered in the right place, and then you do MCMC on the hyperparameters in your sampling in the right place.",
            "So it looks like the theory agrees with the sampling and the samplings are working fine.",
            "That's great, so everything works."
        ],
        [
            "So let's look at the other distribution well.",
            "Split disappointing, so with the other distribution so the dashed lines are what we got when the distribution was of the same form as a prior.",
            "The solid lines now are for the Delta peak type distribution.",
            "And.",
            "It is getting better as we introduce the sparsity, but it's a much more subtle effect for small datasets, so we're getting a much smaller benefit by including this sparsity prior.",
            "Anne.",
            "So that's that's a little bit disappointing, but the other thing is that it's not maximized in the right place until you have quite a lot of data.",
            "And by the time you have this much data, you doing pretty well without this sparsity prior anyway, so it's not really giving you a big win.",
            "OK, but we might still be interested in finding this sparsity even if the overlap of the parameter vectors is very good, so we still maybe want to learn that the C is the right thing, so we're sampling these binary matrices correctly.",
            "So let's look at data from that top one.",
            "Here.",
            "Let's look at that case because we've got enough data there for the overlap to be maximized when the sparsity is roughly correct.",
            "Yeah, I mean.",
            "Well, you can infer the right spot.",
            "Convergence.",
            "An yeah, but they see is really the sparsity.",
            "So if the marginal likelihoods can marginal, I'm sampling binary vectors.",
            "If OK, you're right, I should maybe put in a source in the theory to actually measure the binary agreement, but the marginal likelihood if I'm going to see, right?",
            "I think that on the overlaps kids.",
            "Then I must be getting the right binary vectors.",
            "CAS CAS see I mean when you look at it, the binary vectors are being sampled in MCMC, their sparsity is almost exactly as predicted by the hyperparameter.",
            "So you're getting binary vectors of the right type.",
            "Teacher.",
            "Small.",
            "Oh, I'm going to go into that.",
            "I see 'cause that's the point.",
            "If you have everything that's true, well, well, let me get to that.",
            "I'm going to.",
            "I'm going to talk about that in a moment.",
            "So OK, the performance isn't great, but let's look at anyway.",
            "Whether the performance at least improves when we're in the right sparsity."
        ],
        [
            "Regime.",
            "So an.",
            "It does look like.",
            "The.",
            "At.",
            "The log likelihood in the test data is improved when we're in the right kind of region of sparsity.",
            "Anne.",
            "And similarly, the overlap between these parameter vectors seems to be maximized.",
            "In a region that's close to the true sparsity level, so it seems that if we set the sparsity hyperparameter.",
            "Close to the true value.",
            "Then we do well in terms of these metrics.",
            "OK, and this one is the log probability of test data, so it's not exactly an L2, you know, but I guess it does have a sort of L2 ish flavor to it."
        ],
        [
            "But when we look at the marginal likelihood.",
            "Then the marginal likelihood.",
            "Thinks that the solutions are twice as dense as they should be.",
            "And sampling gives us the same situation.",
            "So when we do the sampling, we sample these binary vectors and the binary vectors have have much more much more dense than they should be.",
            "If we're trying to describe the sparsity.",
            "OK, now it's true what you say so that.",
            "The.",
            "An so.",
            "So we have.",
            "Data.",
            "So we have a model like this.",
            "And we have data.",
            "And I've got the amount of sparsity wrong, but data like this.",
            "And OK so parameters producing data like this and then we have a model like this and so clearly what's happening in the marginal likelihood is it's trying to explain.",
            "Some of the stuff coming from here with some of the discussion OK.",
            "So, so I agree that you could maybe try and take that into account, but nevertheless it's a bit worrying that.",
            "The log test.",
            "The kind of the log likelihood of the testators optimized when you get the sparsity right.",
            "So.",
            "You know, so you could maybe correct for this.",
            "But it's not the optimal thing to do so if you're not a beige, and so you're going to do some sort of cross validation on a quantity like this, right?",
            "So you're going to try and estimate this on some left out data, and then you're going to optimize your hyperparameters on this left out data.",
            "You're going to get the hyperparameters right.",
            "And then, even though you've got the wrong prior and you're not Bayesian, you're still going to be doing quite well with this model.",
            "OK, so I'm not saying that's the solution, but I'm saying that that's just a concern that by the sum of the quantities are optimized with the correct sparsity.",
            "And if you did some sort of leave left out data estimate of the hyperparameters, you'd get the ones which optimize these quantities."
        ],
        [
            "So what about other priors, I mean?",
            "This is a kind of weird prior and.",
            "I mean when you look at this prior, first you think, well, there's basically infinitely more weight associated with zero than with this guy shooting at the sparsity point, so that's good.",
            "So, so it's going to really make everything use the Delta function, but clearly when you've got a noise model and you push it through that.",
            "Nice modeling your data is coming from that.",
            "There's there's a sufficient amount of.",
            "Area of this which is getting confused with this Delta peak.",
            "So this may be a bit of a weird case.",
            "So so are other sparsity priors.",
            "Also do they have problems?",
            "And this is kind of recent stuff, so if we look at."
        ],
        [
            "Yeah, one prior.",
            "We put a little bit of gas in and just so that we can.",
            "Do everything now when the Lambda 1 = 0."
        ],
        [
            "And it didn't really affect things.",
            "We take the same distribution as before."
        ],
        [
            "An hour.",
            "So this is kind of preliminary 'cause I'm done.",
            "The MCMC for this yet.",
            "But this was a stuff that already looked pretty bad.",
            "And now the other one is giving me a very small win in terms of this overlap between these parameter vectors.",
            "So OK, it's not looking."
        ],
        [
            "It's not looking great.",
            "But the marginal likelihood does seem to be actually picking the hyperparameters in the same place as these other error measures.",
            "So it does look like using the marginal likelihood with this L1 norm is actually choosing the hyperparameters in a reasonable place.",
            "Anne."
        ],
        [
            "And then this row.",
            "This overlap between the parameters.",
            "It doesn't have a peak, but it basically just flattens out over this way.",
            "So in a sense it doesn't have a good region as a.",
            "As long as this Lambda one is big enough.",
            "Yeah.",
            "2.",
            "No, I just the only reason I have it is 'cause the way I solved the saddle point equations is that I start off with the PCA result and then I kind of moved towards the other ones.",
            "So I just need it in there to be able to do the numerics so I could get rid of it.",
            "Replica results could be done without.",
            "Yeah yeah, it'll be fine, yeah."
        ],
        [
            "OK so the L1 stuffs very preliminary so I need to look at lots of other cases.",
            "So the mixture priors what we've been using it.",
            "It's quite nice for us 'cause it has this nice gift sampler.",
            "It really gives you binary vectors so you don't have to try and workout what sparsity is in a continuous variable.",
            "You actually have sparsity explicitly, so it's nice in that respect, so that's why we were using it.",
            "We thought is very nice."
        ],
        [
            "Anne.",
            "But I'm you have to just be careful with it because the marginal likelihood looks misleading.",
            "If the if the data doesn't come from the right kind of distribution and then.",
            "So I mean, but I think it's fixable.",
            "I don't think you have to start.",
            "You know.",
            "I'm not saying that there's anything wrong with the Bayesian framework.",
            "What I'm saying is that this kind of prior can have bad properties on certain types of data, so I guess one thing.",
            "For instance, you could do is not use a Gaussian distribution as your alternative, but you something that you really think believe reflects the data.",
            "I mean, so if you think about a gene regulation case.",
            "This model is not too bad because it says either you're not regulated or you're regulated by a significant amount.",
            "Or maybe you'd be.",
            "Down regulated by significant amount, but you're probably not mainly regulated.",
            "Close to zero.",
            "I don't think that's a reasonable model, so if I'm being a subjective Bayesian.",
            "Maybe I would have chosen this prior this prize really there, just because it gives sampler works easily with it.",
            "Well, that's another thing.",
            "It means it's equally likely to be positive and negative, but also why would the cell choose to do lots of weak regulation?",
            "I don't think that's how cells work.",
            "I think either your regulated by Gene or you're not, and it's probably by significant amount.",
            "If you are so, it's more likely.",
            "I think that the regulation is.",
            "Something like that.",
            "So the priors should really reflect what we think is the real situation and shouldn't just be chosen because it's convenient for the Gibbs sampler.",
            "And she's basically.",
            "Yeah, I don't.",
            "I don't yeah OK so you can do it with a mixture.",
            "I don't know about having a mixture 'cause they can overlap.",
            "It's a bit weird.",
            "I mean, if the Gaussian mixtures.",
            "Then you get.",
            "Well yeah, possibly yeah yeah.",
            "I mean, I was thinking of a mixture of a positive and a negative distribution or something, but the Gaussians would be more tractable, so yeah.",
            "And now I'm not saying it's intractable, but I'm saying that people were using this one because it was tractable, I think."
        ],
        [
            "For the L1L1 wasn't exactly given us great behavior results, but the marginal likelihood seems to be well behaved, so it doesn't seem like this problem is necessarily general.",
            "Maybe with these other sparse surprise of marginal likelihoods doing."
        ],
        [
            "Right thing.",
            "Anne."
        ],
        [
            "Now.",
            "This was looking at one factor and the reason for, well, one of the reasons for introducing things like sparsity is because it actually allows you to find the directions of that actually allows you to identify factors in factor analysis.",
            "When it's an identifiable.",
            "So if my if the factors have the same length then you can rotate them around and you have the same model you introduce.",
            "Sparsity, then you can actually really identify their directions.",
            "Now we've done the theory for PCA with multiple components, and it was kind of horrendous because.",
            "The finite size effects were massive and we had to do like.",
            "Or the one over in terms to the theory to to get it to work.",
            "And we thought that the theory was all wrong.",
            "But for fat analysis, it's probably actually better because factor analysis you can really identify these factors, so I think it might actually be a better theory for fact analysis and then for PCA."
        ],
        [
            "Ann should look at the full posterior.",
            "I mean, when we're assessing our methods, we actually use things like arosi curves on ranked.",
            "Probability of a link being on or not?",
            "You know?",
            "So.",
            "So that's how we actually assess things.",
            "We don't really use these kind of L2 norms or or whatever, so we should really put those kind of RC curves and get them out of the theory.",
            "You know, try and get area under the arosi out of the theory.",
            "I don't know if that's possible."
        ],
        [
            "And a lot of people are using map and maximum likelihood approaches, and you can obviously study those in the same framework and you know the L1 norm is hugely popular, so I think it be really interesting to have a look at that in the maximum likelihood setting."
        ],
        [
            "But basically we need good priors and that's the take home message, thanks.",
            "Great, I really like the results.",
            "They're quite consistent with what we got for GPU.",
            "The question where again if you if you get the covariance function of the shape long you could do very bad things.",
            "One thing is I found you didn't spend much time discussing with that, even if the prior had the wrong shade.",
            "By optimizing the test data likelihood, you could actually do quite well, yeah?",
            "Discussion yesterday and it seems to me that.",
            "That would be a good thing to do, but it's like they're doing a cloud optimization like cross validation, do an independent data.",
            "And it is with that ought to be more robust.",
            "In general.",
            "I was kind of hinting at that, but I didn't want to be too.",
            "Given the name of the workshop, I felt I was a dangerous thing to say, but yeah, I mean.",
            "If this material got into the wrong hands.",
            "No, I mean, it does seem to be the case that if you choose the hyperparameters to optimize some kind of test.",
            "Are you actually choose good hyperparameters?",
            "Well.",
            "Schubert Anne.",
            "Then again, I suspect if you do get your prior to be better than, you're still going to be better being Bayesian 'cause The thing is, I don't really want to type to maximum likelihood.",
            "I want to integrate over the hyperparameters.",
            "So I really want to fix my prior so I can do the full Bayesian treatment an.",
            "Is my answer.",
            "How well do you actually do it?",
            "I haven't done it 'cause this.",
            "This wasn't really the test error 'cause it was not estimated.",
            "From the same sample that I was, it's not like Aleve when I error, it's actually an estimate, as though you had an infinite amount of data from the teacher to estimate it.",
            "So I would really have to do a proper leave one out type estimate.",
            "Some of that makes me think of David with talk to the workshop.",
            "We had an approximate Bayesian inference that this last year and the tears he uses this past Bayesian learning stuff.",
            "Matthias was complaining about the model being wrong or something wrong.",
            "It works that I don't care about the model or cry or anything.",
            "All I care about is an algorithm that gives me a sparse solution.",
            "His work is analysis is really on that house, but it's going to be and why this is fast without which hints that he would probably in this case this cross validator.",
            "I'd be interested to see how they are D works 'cause I didn't do that one.",
            "Doesn't really fit into the theory so well because it has a hyperparameters.",
            "So, so I don't really know how to do it, but.",
            "It's a kind of weird thing, but maybe I think you can probably do it.",
            "In theory, you just have to treat the high performances like parameters.",
            "Like parameters, they're not really, but you're optimizing the marginal, so I don't really have to.",
            "Maybe do some sort of.",
            "I don't know how to do that.",
            "He probably knows that.",
            "What we do?",
            "Yeah, yeah.",
            "Right?",
            "I got in.",
            "Right, right, yeah?",
            "Technical one.",
            "Where are you going to publish this?",
            "Yeah, yeah, I've just done it like this week.",
            "So so so.",
            "Where am I going to publish it?",
            "I don't know.",
            "It's a bit difficult with this sort of thing Jameela.",
            "I'd like to actually publish it.",
            "Buried in a physics joke?",
            "No, I don't want to Bury in the physics Journal.",
            "That's what I tend to do with my theory.",
            "I've decided not to do that anymore.",
            "In fact, I'm never going to publish in a physics Journal again, 'cause I'm not a physicist.",
            "Well, actually I start reviewing for them as well, so you probably won't let me.",
            "I think it's quite important that the systematic studies of more than this man.",
            "Like that?",
            "Get some problems.",
            "Where is paperback?",
            "Yeah, and I probably won't publish it in the Journal of Statistical Physics Japan, which is where you then cappuccino decided to publish their results."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So thanks for providing me an.",
                    "label": 0
                },
                {
                    "sent": "There could have been an even more difficult slot 'cause I thought it was 4:30 so I was just actually coming down here to finish the talk, but then found out I was already on so.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "Well, the plan for the talk actually was to do some theory about how good the sparsity priors were using what were and then present some biological results.",
                    "label": 0
                },
                {
                    "sent": "My PhD students laughing 'cause he was supposed to provide the results.",
                    "label": 0
                },
                {
                    "sent": "Luckily though, the theory showed that the sparsity priors were in fact not that good, and therefore that gave me something to talk about.",
                    "label": 0
                },
                {
                    "sent": "But it may be a bit unfortunate for Kevin's project.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll give a brief overview of popular sparsity price Ann and I was sort of motivate things by talking about factor analysis, so we're using sort of sparse factor analysis to infer regulatory networks in biological models of cell.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And then I'll talk about the theory an.",
                    "label": 0
                },
                {
                    "sent": "And the theory will be for very simple single factor model that we can actually analyze an obeya kind of average case, statistical mechanics type theory.",
                    "label": 0
                },
                {
                    "sent": "Our compare the theoretical results to MCMC and just look at different data distributions and how the relationship between the data distribution and the prior affect the problem of identifying sparsity.",
                    "label": 0
                },
                {
                    "sent": "I have a quick look at the one prior that's like really, this is all quite recent work.",
                    "label": 0
                },
                {
                    "sent": "It's not really finished yet, so I've just started looking at L1, so I'll show some quick results with that and then discuss some of the issues.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "Add zip in was talking about objective Bayesians and subjective Bayesians yesterday and.",
                    "label": 0
                },
                {
                    "sent": "I think I'm going to subjective Bayesian, but I think in practice most people are pragmatic versions and they tend to use priors.",
                    "label": 0
                },
                {
                    "sent": "Because it makes the modeling tractable as well as because they believe in the priors and sparsity priors definitely seem to fall into the category of convenient rather than realistic.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So for instance, the L1 prior tends to be chosen.",
                    "label": 0
                },
                {
                    "sent": "In non Bayesian settings, it's chosen because you can solve the parameter estimation problem.",
                    "label": 0
                },
                {
                    "sent": "That sort of retains the convexity of the optimization and so on.",
                    "label": 0
                },
                {
                    "sent": "And in Bayesian settings you can get for certain models.",
                    "label": 0
                },
                {
                    "sent": "It gives you a unimodal posterior, so it's also nice in that case, so that's convenient really.",
                    "label": 0
                },
                {
                    "sent": "I don't think people believe that.",
                    "label": 0
                },
                {
                    "sent": "Primer isn't necessarily an.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I look like that.",
                    "label": 0
                },
                {
                    "sent": "Aard is very popular and so arodys is quite weird.",
                    "label": 0
                },
                {
                    "sent": "I think you have a hyperparameter but you have as many hyperparameters as parameters and then you do type to maximum likelihood over them, which isn't very Bayesian but seems to.",
                    "label": 0
                },
                {
                    "sent": "You know it works OK and people have shown you know Mike Tipping and people have shown that it has some nice properties and does hopefully something reasonable.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "An I'm mainly gonna be using mixture style priors so mixture prior where you have a sort of a bit of probability mass at 0.",
                    "label": 0
                },
                {
                    "sent": "And then you add, say, zero mean Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So you can either be 0 or you can come from a Gaussian an.",
                    "label": 0
                },
                {
                    "sent": "And perhaps these aren't so popular in.",
                    "label": 0
                },
                {
                    "sent": "Machine learning because they get nasty discrete parameters.",
                    "label": 0
                },
                {
                    "sent": "So maybe it doesn't work so well when you're doing variational Bayes in these sorts of things are quite popular in statistics where people are doing MCMC and also they lead to sort of thresholding rules as well for doing sparsity in a non Bayesian setting.",
                    "label": 0
                },
                {
                    "sent": "So these are used quite a lot.",
                    "label": 0
                },
                {
                    "sent": "Am but really the question.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I want to address is.",
                    "label": 0
                },
                {
                    "sent": "Is it OK not to worry about whether these actually fit the data?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Searching for sparse structures is sort of.",
                    "label": 0
                },
                {
                    "sent": "I don't know Semiparametric in the sense that you only care about half of the parameters because you sort of.",
                    "label": 0
                },
                {
                    "sent": "We believe the sparsity's there that's something 0.",
                    "label": 0
                },
                {
                    "sent": "But the other stuff maybe?",
                    "label": 0
                },
                {
                    "sent": "Where we don't know exactly what the distribution of it is.",
                    "label": 0
                },
                {
                    "sent": "We hope that learning about the sparsity will be enough to give us good performance.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "But but our data maybe doesn't satisfy these sorts of conditions, so we should.",
                    "label": 0
                },
                {
                    "sent": "And the other part is this part.",
                    "label": 0
                },
                {
                    "sent": "OK, so the part parts that aren't sparse?",
                    "label": 0
                },
                {
                    "sent": "We give them some sort of fairly vague prior.",
                    "label": 0
                },
                {
                    "sent": "We don't really give them anything, so we believe the sparsity part is really zero perhaps, but maybe the other part.",
                    "label": 0
                },
                {
                    "sent": "Is just catching everything else you know, so it's not very informative.",
                    "label": 0
                },
                {
                    "sent": "Priors sort of quite uninformative.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So just to motivate things we've been looking at factor analysis for network inference, so the idea here is that we have some log expression data Ann.",
                    "label": 1
                },
                {
                    "sent": "This is a bit like some of this stuff Neil was talking about.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about time series, but we can put time series priors and the latent variables.",
                    "label": 1
                },
                {
                    "sent": "And the idea here is that we're going to treat the activity of transcription factors which regulate expression as latent because they might be regulated post transcriptionally, which means that we can't just use expression data as a proxy for the protein levels.",
                    "label": 0
                },
                {
                    "sent": "So we can't measure the protein levels easily, but we can measure the expression levels of the targets and therefore we're going to treat the proteins as latent variables.",
                    "label": 1
                },
                {
                    "sent": "And the factor loadings are this parameter matrix W. Anne.",
                    "label": 0
                },
                {
                    "sent": "And in this application W is definitely sparse in the sense that genes tend to be regulated by a small number of transcription factors.",
                    "label": 0
                },
                {
                    "sent": "So we really know that we're looking for sparse matrices W. An and there are bunch of methods for inferring sparse W from the data Y, and these were reviewed quite recently.",
                    "label": 1
                },
                {
                    "sent": "So some things you can do is you can just do factor analysis and then rotate the factors into the most sparse.",
                    "label": 0
                },
                {
                    "sent": "Rotation, but that doesn't seem to work too well.",
                    "label": 0
                },
                {
                    "sent": "Or one can put in explicit priors and do something Bayesian to get sparsity in that matrix.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we've been using this mixture prior and it's quite nice because it leads to quite a nice Gibbs sampler for this problem.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And in our application we actually have some knowledge about the hyperparameters, so we know from chip chip or motif data which genes are likely to be regulated by which transcription factors.",
                    "label": 0
                },
                {
                    "sent": "So we can put that in so the whole Bayesian framework is very nice for us because we really have some priors.",
                    "label": 0
                },
                {
                    "sent": "AM.",
                    "label": 1
                },
                {
                    "sent": "If we don't have priors or if we don't have priors about everything, maybe we want to estimate some of the hyperparameters by MCMC.",
                    "label": 0
                },
                {
                    "sent": "So if we're going to do any estimation by MCMC of the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "Then we want to make sure that the marginal likelihood after we've marginalized at the parameters is doing something useful for us in terms of identifying these hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "And that's mainly what I'm going to focus on today, so we've been using a lot of empirical priors.",
                    "label": 0
                },
                {
                    "sent": "But really, I want to start using more.",
                    "label": 0
                },
                {
                    "sent": "Data driven sort of hyperparameter estimation, so I want to make sure that that's going to work OK, so that's wrong.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm talking about today.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, there's another.",
                    "label": 0
                },
                {
                    "sent": "So this is another thing we're interested in is actually treating measurements of Zed's data, but that's a separate issue, really.",
                    "label": 0
                },
                {
                    "sent": "But we could have in this case.",
                    "label": 0
                },
                {
                    "sent": "Why is the only data and any other information we have?",
                    "label": 0
                },
                {
                    "sent": "Maybe we could put it into hyper prior so it could be like a beta distribution or something.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the way that Gibbs sampler works is that.",
                    "label": 0
                },
                {
                    "sent": "You re parameterized, your weights as binary variables times Gaussians.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The nice the reason it works is because you can integrate out the bees before sampling the axes.",
                    "label": 1
                },
                {
                    "sent": "And that allows you to basically sample the WS like you don't have to sample.",
                    "label": 0
                },
                {
                    "sent": "The X is in the bees sort of on the left and the right.",
                    "label": 0
                },
                {
                    "sent": "You can sample the whole W 'cause you sample.",
                    "label": 1
                },
                {
                    "sent": "The X is first and then the bees.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you have 10 factors or less, you can sample all the axes at once.",
                    "label": 0
                },
                {
                    "sent": "But if you have more factors than you have to leave some of the X is on the right hand side when you're sampling some of the other X is OK, so so we have hundreds of factors, so we can't sample.",
                    "label": 0
                },
                {
                    "sent": "RBX is an, but nevertheless it seems to converge quite well in the size of problems we're looking at there not sort of massive problems.",
                    "label": 0
                },
                {
                    "sent": "It's like thousands of genes, hundreds of factors, so it's not huge, but you know it is non trivial.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And the other thing that saved that makes this Gibbs sampling fast is that because of the sparsity.",
                    "label": 0
                },
                {
                    "sent": "The sampling of being zed, which are Gaussians, those have sparse covariance matrices.",
                    "label": 0
                },
                {
                    "sent": "So you can use, uh, sparse Cholesky decomposition and everything is really fast in the sampling, so effects remains sparse during the sampling procedure.",
                    "label": 0
                },
                {
                    "sent": "Then, being zed sampling is also really fast, and everything goes nice and quickly.",
                    "label": 0
                },
                {
                    "sent": "And so that's why this kind of mixture, prior with the binary variables seems to be quite nice for this kind of Gibbs sampling, whereas if we had some sort of continuous.",
                    "label": 0
                },
                {
                    "sent": "Prior like an hour one or something like that, everything would remain dense and things would be perhaps a bit slower.",
                    "label": 0
                },
                {
                    "sent": "And with if we use conjugate hyper priors, we can sample the hyperparameters as well and so that's fine as well.",
                    "label": 0
                },
                {
                    "sent": "So we don't need to do type 2.",
                    "label": 0
                },
                {
                    "sent": "Maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "Either we can do everything by MCMC, which which always is nice.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So I want to analyze that model so I'm not OK.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to present any results 'cause it's sort of ongoing.",
                    "label": 0
                },
                {
                    "sent": "We've done some.",
                    "label": 0
                },
                {
                    "sent": "Work which is already published, but I really want to focus for this workshop on the theoretical issues.",
                    "label": 0
                },
                {
                    "sent": "So let's look at really simple case Bayesian PCA.",
                    "label": 0
                },
                {
                    "sent": "With a single factor.",
                    "label": 0
                },
                {
                    "sent": "An and a sparsity prior on the parameter vector.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's that's the model we're using.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "We want to study average behavior over datasets produced by some teacher distribution.",
                    "label": 1
                },
                {
                    "sent": "So this is one of these sort of student teacher scenarios.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things.",
                    "label": 0
                },
                {
                    "sent": "And the teacher is identical except for the parameter vector.",
                    "label": 1
                },
                {
                    "sent": "Which comes from two different distributions and.",
                    "label": 0
                },
                {
                    "sent": "When I did this, I chose the one.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll show you this discussion first.",
                    "label": 0
                },
                {
                    "sent": "OK, so once the same form.",
                    "label": 0
                },
                {
                    "sent": "As the model, but with different parameters.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Possibly.",
                    "label": 0
                },
                {
                    "sent": "And another one is very similar.",
                    "label": 0
                },
                {
                    "sent": "And but instead of using a Gaussian for the.",
                    "label": 0
                },
                {
                    "sent": "Non sparse part.",
                    "label": 0
                },
                {
                    "sent": "I just use a Delta function.",
                    "label": 0
                },
                {
                    "sent": "But I put all the parameters one standard deviation away.",
                    "label": 0
                },
                {
                    "sent": "In the gaussian.",
                    "label": 0
                },
                {
                    "sent": "So instead of using a Gaussian optics user Delta function.",
                    "label": 0
                },
                {
                    "sent": "But the length of the vectors is the same, so it's kind of a similar as I could make it really an.",
                    "label": 0
                },
                {
                    "sent": "And I have to say that I only used this because.",
                    "label": 0
                },
                {
                    "sent": "When I was doing the numerics in the theory, this made things go quicker.",
                    "label": 0
                },
                {
                    "sent": "I thought it was going to be identical.",
                    "label": 0
                },
                {
                    "sent": "An I I just it was just a speed up 'cause it's me doing some numerical integrals so it's kind of using this and everything was wrong and I sort of thought I keep checking my code, check my theory, check my code so in the end I thought well maybe I'll just use a Gaussian prior.",
                    "label": 0
                },
                {
                    "sent": "And see if that works and then everything started working and then I realized that something weird was going on with this prior 'cause I thought that these two things would be identical.",
                    "label": 0
                },
                {
                    "sent": "I mean they are identical for PCA without the sparsity.",
                    "label": 0
                },
                {
                    "sent": "So I was.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Little bit surprise.",
                    "label": 0
                },
                {
                    "sent": "And that one can tell you.",
                    "label": 0
                },
                {
                    "sent": "So the way the theory goes is we compute the marginal likelihood averaged over datasets drawn from either of those two distributions.",
                    "label": 0
                },
                {
                    "sent": "And we take the limit of high dimensional data.",
                    "label": 0
                },
                {
                    "sent": "Keeping the ratio of the number of training points to the dimension fixed, yeah.",
                    "label": 0
                },
                {
                    "sent": "Tired.",
                    "label": 0
                },
                {
                    "sent": "You would use two different teachers and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to look at both teachers.",
                    "label": 0
                },
                {
                    "sent": "Know that I'm going to use one or the other, OK?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I could make some.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK so.",
                    "label": 0
                },
                {
                    "sent": "So we're going to hold the ratio of the number of examples to that.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's dimension fixed.",
                    "label": 0
                },
                {
                    "sent": "We're going to look at as well as looking at the marginal likelihood.",
                    "label": 1
                },
                {
                    "sent": "We'll look at some functions of the mean posterior parameter, so we'll look at the.",
                    "label": 1
                },
                {
                    "sent": "Cosine angle between the true parameter and the mean posterior estimate.",
                    "label": 0
                },
                {
                    "sent": "And we'll look at the log likelihoods of test data.",
                    "label": 0
                },
                {
                    "sent": "If you like data from the teacher distribution.",
                    "label": 0
                },
                {
                    "sent": "An we could also average that.",
                    "label": 0
                },
                {
                    "sent": "I mean, we've done it for the mean posterior.",
                    "label": 0
                },
                {
                    "sent": "We could average over the posterior for that second one.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really affect the results.",
                    "label": 0
                },
                {
                    "sent": "Is the same.",
                    "label": 0
                },
                {
                    "sent": "I've just used this one because it actually appears later in in the marginal likelihood, so it's nice to introduce it here.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And we got really good agreement with simulations.",
                    "label": 1
                },
                {
                    "sent": "For the case we're interested in, which is small offer, which is where you have a small number of examples compared to the data dimension.",
                    "label": 0
                },
                {
                    "sent": "So often we have data is thousands of dimensions and we maybe have 10s or hundreds of examples or something, so that's the kind of case we're interested in.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's kind of similar to calculation that's already been done.",
                    "label": 0
                },
                {
                    "sent": "On a classification problem.",
                    "label": 0
                },
                {
                    "sent": "So it's already in.",
                    "label": 0
                },
                {
                    "sent": "It's very similar, so we're using the replica method.",
                    "label": 0
                },
                {
                    "sent": "And I don't really want to go into the details of it, but basically.",
                    "label": 0
                },
                {
                    "sent": "The the solution has a has two parts for the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "One part is basically the log probability of data from the teacher distribution for the mean posterior parameter.",
                    "label": 0
                },
                {
                    "sent": "And then we get some sort of entropic terms as well, and so you can see this first term here is proportional to Alpha, which is the number of training examples divided by the dimension.",
                    "label": 0
                },
                {
                    "sent": "So as you get lots of training examples, you eventually converge to the log marginal likelihood being dominated by.",
                    "label": 0
                },
                {
                    "sent": "The likely hood part.",
                    "label": 0
                },
                {
                    "sent": "So really, it's in the nature of these entropic terms that we get differences in performance.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the average case becomes typical.",
                    "label": 1
                },
                {
                    "sent": "If you have very large data set dimension.",
                    "label": 0
                },
                {
                    "sent": "So there's a sort of self averaging behavior, and in these things.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first just look at standard PCA, so standard PCA is already was done ages ago an.",
                    "label": 0
                },
                {
                    "sent": "In a slightly different setting, so people who studied this before did it for spherical distributions of parameters, so they constrain them onto the unit sphere and Bayesian inference for without different length parameter vectors.",
                    "label": 0
                },
                {
                    "sent": "So the only difference here is that we reliably parameter vectors have different lengths.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And we get this.",
                    "label": 0
                },
                {
                    "sent": "These kind of phase transitions, which are quite interesting.",
                    "label": 0
                },
                {
                    "sent": "So below certain number of training examples this is so called retarded learning regime where you can't learn anything about the directions in the data.",
                    "label": 0
                },
                {
                    "sent": "There's no learning available.",
                    "label": 0
                },
                {
                    "sent": "Manfreds also done some rigorous bounds on these kind of phase transitions as well.",
                    "label": 0
                },
                {
                    "sent": "So the replica method for those in the know isn't rigorous, it's just.",
                    "label": 0
                },
                {
                    "sent": "Correct?",
                    "label": 0
                },
                {
                    "sent": "Often.",
                    "label": 0
                },
                {
                    "sent": "But I see it's probably not correct in all these, although regions of this phase space.",
                    "label": 0
                },
                {
                    "sent": "M. So then so when Alpha gets bigger than this.",
                    "label": 0
                },
                {
                    "sent": "1 / T squared term.",
                    "label": 0
                },
                {
                    "sent": "Then there's a second order phase transition and you begin to learn something and so your performance goes up and then converges.",
                    "label": 0
                },
                {
                    "sent": "When you have lots of data to the true parameters.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And then the the main thing here is that the learning performance for standard PCA only depends on the length of the teacher parameter vector.",
                    "label": 0
                },
                {
                    "sent": "OK, so it doesn't depend on the particular distribution at all, so the two distributions are.",
                    "label": 0
                },
                {
                    "sent": "They have the same length, so PCA does the same thing on those two, right?",
                    "label": 0
                },
                {
                    "sent": "So there's no difference in performance for standard PCA.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah it is.",
                    "label": 0
                },
                {
                    "sent": "I fix the noise to one but yeah.",
                    "label": 0
                },
                {
                    "sent": "An yeah so in fact.",
                    "label": 0
                },
                {
                    "sent": "Yeah, people have done.",
                    "label": 0
                },
                {
                    "sent": "People got some rigorous results recently on the eigenvalue spectrum and the.",
                    "label": 0
                },
                {
                    "sent": "What was the?",
                    "label": 0
                },
                {
                    "sent": "When you study the eigenvalue Spectra, what you see actually is, it seems to be that the results are very universal.",
                    "label": 0
                },
                {
                    "sent": "In terms of the distributions that you use, so for date, well, OK, that's for.",
                    "label": 0
                },
                {
                    "sent": "Just data without signal in it.",
                    "label": 0
                },
                {
                    "sent": "So data without a signal in it.",
                    "label": 0
                },
                {
                    "sent": "It seems like you get the same spectrum as long as sort of your 4th moment exists or something for your data distribution, so those results are very robust to the particular distribution of data, and because PCA is a sort of variance based method, I think that's maybe not surprising.",
                    "label": 0
                },
                {
                    "sent": "You know it's so it's really just using variance information.",
                    "label": 0
                },
                {
                    "sent": "It's not really using any higher order information in the data.",
                    "label": 0
                },
                {
                    "sent": "Man.",
                    "label": 0
                },
                {
                    "sent": "And yeah, these transitions.",
                    "label": 0
                },
                {
                    "sent": "They're mirrored in this the eigenvalue spectrum and people you know got rigorous results.",
                    "label": 0
                },
                {
                    "sent": "Now for these kind of.",
                    "label": 0
                },
                {
                    "sent": "Phase transition behaviors.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So this is consistent.",
                    "label": 0
                },
                {
                    "sent": "Other other sorry, that's not Bayesian PCA, though, right?",
                    "label": 0
                },
                {
                    "sent": "So that's maximum likelihood PCA.",
                    "label": 0
                },
                {
                    "sent": "Bayesian PCA, it's not just an eigenvalue problem.",
                    "label": 1
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So it's consistent with the known results for this faculty.",
                    "label": 1
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ice.",
                    "label": 0
                },
                {
                    "sent": "And the only new feature here is we got a new first order phase transition with increasing hyperparameter Lambda.",
                    "label": 1
                },
                {
                    "sent": "So Lambda is the weight decay type term, so that's that.",
                    "label": 0
                },
                {
                    "sent": "Basically means if you miss specify your prior on the length of these vectors, then if you miss specify it too much, then you can again get no performance.",
                    "label": 0
                },
                {
                    "sent": "So you basically shrink your vectors to 0.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is an example of that, so here we're just increasing this hyperparameter, which is this weight decay term.",
                    "label": 0
                },
                {
                    "sent": "And if we use too much, then eventually performance just falls off to 0.",
                    "label": 0
                },
                {
                    "sent": "Now with these sort of transitions, there always smoothed out when you look at real data.",
                    "label": 0
                },
                {
                    "sent": "OK, you have to look at really high dimensional data to get it is sharp.",
                    "label": 0
                },
                {
                    "sent": "There's a different scaling within.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, yeah, yeah, that's right.",
                    "label": 0
                },
                {
                    "sent": "And OK, but I don't think actually these phase transitions are really elegant in nice and everything, but I don't think they're actually that relevant to to us because I think we're usually quite far away from them.",
                    "label": 0
                },
                {
                    "sent": "So if we're close to this retarded learning transition, then forget it.",
                    "label": 0
                },
                {
                    "sent": "We're not going to learn anything anyway, 'cause that's in the region where we don't have enough data to learn.",
                    "label": 0
                },
                {
                    "sent": "This transition happens far from the optimal hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "So for using any kind of marginal likelihood optimization or anything, we're going to true that good place for the hyperparameters here.",
                    "label": 0
                },
                {
                    "sent": "And as long as we're not being completely crazy when we're setting our hyperparameters, we're not going to end up close to these transitions.",
                    "label": 0
                },
                {
                    "sent": "The only time we end up close to this transition is when we're also close to the retarded learning transition, and at that point, this transition occurs at the optimal hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "So if we have.",
                    "label": 0
                },
                {
                    "sent": "Only just enough data to learn.",
                    "label": 0
                },
                {
                    "sent": "Then the this transition happens at the optimal point in the marginal likelihood, so that's.",
                    "label": 0
                },
                {
                    "sent": "But it's a kind of only a theoretical interest, were never interested in learning in that regime.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So where am I going to consider learning away from these transitions?",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I've already described it.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Datasets.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same form is a prior, a different form an.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I've said they both give identical performance standard PC and they both give identical performance if we know where the sparsity is.",
                    "label": 0
                },
                {
                    "sent": "So if someone tells us is sparsity, these things give the same.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Performance.",
                    "label": 0
                },
                {
                    "sent": "So this is a theory versus some MCMC results for one data set.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then this is an average over 10 datasets, so you see when you average you get better fit to the theory and this is increasing the size of the data set.",
                    "label": 0
                },
                {
                    "sent": "And the teacher sparsity is not .2.",
                    "label": 0
                },
                {
                    "sent": "So that means that 80% of the.",
                    "label": 0
                },
                {
                    "sent": "Are zeros in 20% have some sort of signal?",
                    "label": 0
                },
                {
                    "sent": "Of the parameter vectors.",
                    "label": 0
                },
                {
                    "sent": "An and as you'd hope, the performance.",
                    "label": 0
                },
                {
                    "sent": "So this row here is the overlap between the true parameter and the mean posterior an that improves an it's maximized close to the true sparsity.",
                    "label": 0
                },
                {
                    "sent": "So if you're closer true sparsity, do well here.",
                    "label": 0
                },
                {
                    "sent": "And so that's not surprising, that's.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It.",
                    "label": 0
                },
                {
                    "sent": "If you look at this row and you plotted against both of the hyperparameters, then you see that, well, the sort of.",
                    "label": 0
                },
                {
                    "sent": "At likelihoods on test data.",
                    "label": 0
                },
                {
                    "sent": "Is maximized at the right place?",
                    "label": 0
                },
                {
                    "sent": "And the overlap between the parameter vectors is rather insensitive to the actual this hyperparameter, which is the length, and that's quite good, because often people when they do this MCMC, they just set Lambda really small.",
                    "label": 0
                },
                {
                    "sent": "Don't really care about it, so they don't bother estimating it, so that's going to work OK in this setting.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then if you look at the marginal likelihoods from the theory, it's centered in the right place, and then you do MCMC on the hyperparameters in your sampling in the right place.",
                    "label": 0
                },
                {
                    "sent": "So it looks like the theory agrees with the sampling and the samplings are working fine.",
                    "label": 0
                },
                {
                    "sent": "That's great, so everything works.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let's look at the other distribution well.",
                    "label": 0
                },
                {
                    "sent": "Split disappointing, so with the other distribution so the dashed lines are what we got when the distribution was of the same form as a prior.",
                    "label": 0
                },
                {
                    "sent": "The solid lines now are for the Delta peak type distribution.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "It is getting better as we introduce the sparsity, but it's a much more subtle effect for small datasets, so we're getting a much smaller benefit by including this sparsity prior.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "So that's that's a little bit disappointing, but the other thing is that it's not maximized in the right place until you have quite a lot of data.",
                    "label": 0
                },
                {
                    "sent": "And by the time you have this much data, you doing pretty well without this sparsity prior anyway, so it's not really giving you a big win.",
                    "label": 0
                },
                {
                    "sent": "OK, but we might still be interested in finding this sparsity even if the overlap of the parameter vectors is very good, so we still maybe want to learn that the C is the right thing, so we're sampling these binary matrices correctly.",
                    "label": 0
                },
                {
                    "sent": "So let's look at data from that top one.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Let's look at that case because we've got enough data there for the overlap to be maximized when the sparsity is roughly correct.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I mean.",
                    "label": 0
                },
                {
                    "sent": "Well, you can infer the right spot.",
                    "label": 0
                },
                {
                    "sent": "Convergence.",
                    "label": 0
                },
                {
                    "sent": "An yeah, but they see is really the sparsity.",
                    "label": 0
                },
                {
                    "sent": "So if the marginal likelihoods can marginal, I'm sampling binary vectors.",
                    "label": 0
                },
                {
                    "sent": "If OK, you're right, I should maybe put in a source in the theory to actually measure the binary agreement, but the marginal likelihood if I'm going to see, right?",
                    "label": 0
                },
                {
                    "sent": "I think that on the overlaps kids.",
                    "label": 0
                },
                {
                    "sent": "Then I must be getting the right binary vectors.",
                    "label": 0
                },
                {
                    "sent": "CAS CAS see I mean when you look at it, the binary vectors are being sampled in MCMC, their sparsity is almost exactly as predicted by the hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "So you're getting binary vectors of the right type.",
                    "label": 0
                },
                {
                    "sent": "Teacher.",
                    "label": 0
                },
                {
                    "sent": "Small.",
                    "label": 0
                },
                {
                    "sent": "Oh, I'm going to go into that.",
                    "label": 0
                },
                {
                    "sent": "I see 'cause that's the point.",
                    "label": 0
                },
                {
                    "sent": "If you have everything that's true, well, well, let me get to that.",
                    "label": 0
                },
                {
                    "sent": "I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about that in a moment.",
                    "label": 0
                },
                {
                    "sent": "So OK, the performance isn't great, but let's look at anyway.",
                    "label": 0
                },
                {
                    "sent": "Whether the performance at least improves when we're in the right sparsity.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Regime.",
                    "label": 0
                },
                {
                    "sent": "So an.",
                    "label": 0
                },
                {
                    "sent": "It does look like.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "The log likelihood in the test data is improved when we're in the right kind of region of sparsity.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "And similarly, the overlap between these parameter vectors seems to be maximized.",
                    "label": 0
                },
                {
                    "sent": "In a region that's close to the true sparsity level, so it seems that if we set the sparsity hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "Close to the true value.",
                    "label": 0
                },
                {
                    "sent": "Then we do well in terms of these metrics.",
                    "label": 0
                },
                {
                    "sent": "OK, and this one is the log probability of test data, so it's not exactly an L2, you know, but I guess it does have a sort of L2 ish flavor to it.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But when we look at the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "Then the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "Thinks that the solutions are twice as dense as they should be.",
                    "label": 0
                },
                {
                    "sent": "And sampling gives us the same situation.",
                    "label": 0
                },
                {
                    "sent": "So when we do the sampling, we sample these binary vectors and the binary vectors have have much more much more dense than they should be.",
                    "label": 0
                },
                {
                    "sent": "If we're trying to describe the sparsity.",
                    "label": 0
                },
                {
                    "sent": "OK, now it's true what you say so that.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "An so.",
                    "label": 0
                },
                {
                    "sent": "So we have.",
                    "label": 0
                },
                {
                    "sent": "Data.",
                    "label": 0
                },
                {
                    "sent": "So we have a model like this.",
                    "label": 0
                },
                {
                    "sent": "And we have data.",
                    "label": 0
                },
                {
                    "sent": "And I've got the amount of sparsity wrong, but data like this.",
                    "label": 0
                },
                {
                    "sent": "And OK so parameters producing data like this and then we have a model like this and so clearly what's happening in the marginal likelihood is it's trying to explain.",
                    "label": 0
                },
                {
                    "sent": "Some of the stuff coming from here with some of the discussion OK.",
                    "label": 0
                },
                {
                    "sent": "So, so I agree that you could maybe try and take that into account, but nevertheless it's a bit worrying that.",
                    "label": 0
                },
                {
                    "sent": "The log test.",
                    "label": 0
                },
                {
                    "sent": "The kind of the log likelihood of the testators optimized when you get the sparsity right.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "You know, so you could maybe correct for this.",
                    "label": 0
                },
                {
                    "sent": "But it's not the optimal thing to do so if you're not a beige, and so you're going to do some sort of cross validation on a quantity like this, right?",
                    "label": 0
                },
                {
                    "sent": "So you're going to try and estimate this on some left out data, and then you're going to optimize your hyperparameters on this left out data.",
                    "label": 0
                },
                {
                    "sent": "You're going to get the hyperparameters right.",
                    "label": 0
                },
                {
                    "sent": "And then, even though you've got the wrong prior and you're not Bayesian, you're still going to be doing quite well with this model.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm not saying that's the solution, but I'm saying that that's just a concern that by the sum of the quantities are optimized with the correct sparsity.",
                    "label": 0
                },
                {
                    "sent": "And if you did some sort of leave left out data estimate of the hyperparameters, you'd get the ones which optimize these quantities.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what about other priors, I mean?",
                    "label": 0
                },
                {
                    "sent": "This is a kind of weird prior and.",
                    "label": 0
                },
                {
                    "sent": "I mean when you look at this prior, first you think, well, there's basically infinitely more weight associated with zero than with this guy shooting at the sparsity point, so that's good.",
                    "label": 0
                },
                {
                    "sent": "So, so it's going to really make everything use the Delta function, but clearly when you've got a noise model and you push it through that.",
                    "label": 0
                },
                {
                    "sent": "Nice modeling your data is coming from that.",
                    "label": 0
                },
                {
                    "sent": "There's there's a sufficient amount of.",
                    "label": 0
                },
                {
                    "sent": "Area of this which is getting confused with this Delta peak.",
                    "label": 0
                },
                {
                    "sent": "So this may be a bit of a weird case.",
                    "label": 0
                },
                {
                    "sent": "So so are other sparsity priors.",
                    "label": 0
                },
                {
                    "sent": "Also do they have problems?",
                    "label": 0
                },
                {
                    "sent": "And this is kind of recent stuff, so if we look at.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, one prior.",
                    "label": 0
                },
                {
                    "sent": "We put a little bit of gas in and just so that we can.",
                    "label": 0
                },
                {
                    "sent": "Do everything now when the Lambda 1 = 0.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it didn't really affect things.",
                    "label": 0
                },
                {
                    "sent": "We take the same distribution as before.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An hour.",
                    "label": 0
                },
                {
                    "sent": "So this is kind of preliminary 'cause I'm done.",
                    "label": 0
                },
                {
                    "sent": "The MCMC for this yet.",
                    "label": 0
                },
                {
                    "sent": "But this was a stuff that already looked pretty bad.",
                    "label": 0
                },
                {
                    "sent": "And now the other one is giving me a very small win in terms of this overlap between these parameter vectors.",
                    "label": 0
                },
                {
                    "sent": "So OK, it's not looking.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's not looking great.",
                    "label": 0
                },
                {
                    "sent": "But the marginal likelihood does seem to be actually picking the hyperparameters in the same place as these other error measures.",
                    "label": 0
                },
                {
                    "sent": "So it does look like using the marginal likelihood with this L1 norm is actually choosing the hyperparameters in a reasonable place.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then this row.",
                    "label": 0
                },
                {
                    "sent": "This overlap between the parameters.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have a peak, but it basically just flattens out over this way.",
                    "label": 0
                },
                {
                    "sent": "So in a sense it doesn't have a good region as a.",
                    "label": 0
                },
                {
                    "sent": "As long as this Lambda one is big enough.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "No, I just the only reason I have it is 'cause the way I solved the saddle point equations is that I start off with the PCA result and then I kind of moved towards the other ones.",
                    "label": 0
                },
                {
                    "sent": "So I just need it in there to be able to do the numerics so I could get rid of it.",
                    "label": 0
                },
                {
                    "sent": "Replica results could be done without.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, it'll be fine, yeah.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so the L1 stuffs very preliminary so I need to look at lots of other cases.",
                    "label": 0
                },
                {
                    "sent": "So the mixture priors what we've been using it.",
                    "label": 0
                },
                {
                    "sent": "It's quite nice for us 'cause it has this nice gift sampler.",
                    "label": 0
                },
                {
                    "sent": "It really gives you binary vectors so you don't have to try and workout what sparsity is in a continuous variable.",
                    "label": 0
                },
                {
                    "sent": "You actually have sparsity explicitly, so it's nice in that respect, so that's why we were using it.",
                    "label": 0
                },
                {
                    "sent": "We thought is very nice.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "But I'm you have to just be careful with it because the marginal likelihood looks misleading.",
                    "label": 1
                },
                {
                    "sent": "If the if the data doesn't come from the right kind of distribution and then.",
                    "label": 0
                },
                {
                    "sent": "So I mean, but I think it's fixable.",
                    "label": 0
                },
                {
                    "sent": "I don't think you have to start.",
                    "label": 0
                },
                {
                    "sent": "You know.",
                    "label": 0
                },
                {
                    "sent": "I'm not saying that there's anything wrong with the Bayesian framework.",
                    "label": 1
                },
                {
                    "sent": "What I'm saying is that this kind of prior can have bad properties on certain types of data, so I guess one thing.",
                    "label": 0
                },
                {
                    "sent": "For instance, you could do is not use a Gaussian distribution as your alternative, but you something that you really think believe reflects the data.",
                    "label": 0
                },
                {
                    "sent": "I mean, so if you think about a gene regulation case.",
                    "label": 0
                },
                {
                    "sent": "This model is not too bad because it says either you're not regulated or you're regulated by a significant amount.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you'd be.",
                    "label": 0
                },
                {
                    "sent": "Down regulated by significant amount, but you're probably not mainly regulated.",
                    "label": 0
                },
                {
                    "sent": "Close to zero.",
                    "label": 0
                },
                {
                    "sent": "I don't think that's a reasonable model, so if I'm being a subjective Bayesian.",
                    "label": 0
                },
                {
                    "sent": "Maybe I would have chosen this prior this prize really there, just because it gives sampler works easily with it.",
                    "label": 0
                },
                {
                    "sent": "Well, that's another thing.",
                    "label": 0
                },
                {
                    "sent": "It means it's equally likely to be positive and negative, but also why would the cell choose to do lots of weak regulation?",
                    "label": 0
                },
                {
                    "sent": "I don't think that's how cells work.",
                    "label": 0
                },
                {
                    "sent": "I think either your regulated by Gene or you're not, and it's probably by significant amount.",
                    "label": 0
                },
                {
                    "sent": "If you are so, it's more likely.",
                    "label": 0
                },
                {
                    "sent": "I think that the regulation is.",
                    "label": 0
                },
                {
                    "sent": "Something like that.",
                    "label": 0
                },
                {
                    "sent": "So the priors should really reflect what we think is the real situation and shouldn't just be chosen because it's convenient for the Gibbs sampler.",
                    "label": 0
                },
                {
                    "sent": "And she's basically.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't.",
                    "label": 0
                },
                {
                    "sent": "I don't yeah OK so you can do it with a mixture.",
                    "label": 0
                },
                {
                    "sent": "I don't know about having a mixture 'cause they can overlap.",
                    "label": 0
                },
                {
                    "sent": "It's a bit weird.",
                    "label": 0
                },
                {
                    "sent": "I mean, if the Gaussian mixtures.",
                    "label": 0
                },
                {
                    "sent": "Then you get.",
                    "label": 0
                },
                {
                    "sent": "Well yeah, possibly yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "I mean, I was thinking of a mixture of a positive and a negative distribution or something, but the Gaussians would be more tractable, so yeah.",
                    "label": 0
                },
                {
                    "sent": "And now I'm not saying it's intractable, but I'm saying that people were using this one because it was tractable, I think.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the L1L1 wasn't exactly given us great behavior results, but the marginal likelihood seems to be well behaved, so it doesn't seem like this problem is necessarily general.",
                    "label": 0
                },
                {
                    "sent": "Maybe with these other sparse surprise of marginal likelihoods doing.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right thing.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This was looking at one factor and the reason for, well, one of the reasons for introducing things like sparsity is because it actually allows you to find the directions of that actually allows you to identify factors in factor analysis.",
                    "label": 0
                },
                {
                    "sent": "When it's an identifiable.",
                    "label": 0
                },
                {
                    "sent": "So if my if the factors have the same length then you can rotate them around and you have the same model you introduce.",
                    "label": 0
                },
                {
                    "sent": "Sparsity, then you can actually really identify their directions.",
                    "label": 0
                },
                {
                    "sent": "Now we've done the theory for PCA with multiple components, and it was kind of horrendous because.",
                    "label": 0
                },
                {
                    "sent": "The finite size effects were massive and we had to do like.",
                    "label": 0
                },
                {
                    "sent": "Or the one over in terms to the theory to to get it to work.",
                    "label": 0
                },
                {
                    "sent": "And we thought that the theory was all wrong.",
                    "label": 0
                },
                {
                    "sent": "But for fat analysis, it's probably actually better because factor analysis you can really identify these factors, so I think it might actually be a better theory for fact analysis and then for PCA.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ann should look at the full posterior.",
                    "label": 1
                },
                {
                    "sent": "I mean, when we're assessing our methods, we actually use things like arosi curves on ranked.",
                    "label": 0
                },
                {
                    "sent": "Probability of a link being on or not?",
                    "label": 0
                },
                {
                    "sent": "You know?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So that's how we actually assess things.",
                    "label": 0
                },
                {
                    "sent": "We don't really use these kind of L2 norms or or whatever, so we should really put those kind of RC curves and get them out of the theory.",
                    "label": 0
                },
                {
                    "sent": "You know, try and get area under the arosi out of the theory.",
                    "label": 0
                },
                {
                    "sent": "I don't know if that's possible.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And a lot of people are using map and maximum likelihood approaches, and you can obviously study those in the same framework and you know the L1 norm is hugely popular, so I think it be really interesting to have a look at that in the maximum likelihood setting.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But basically we need good priors and that's the take home message, thanks.",
                    "label": 0
                },
                {
                    "sent": "Great, I really like the results.",
                    "label": 0
                },
                {
                    "sent": "They're quite consistent with what we got for GPU.",
                    "label": 0
                },
                {
                    "sent": "The question where again if you if you get the covariance function of the shape long you could do very bad things.",
                    "label": 0
                },
                {
                    "sent": "One thing is I found you didn't spend much time discussing with that, even if the prior had the wrong shade.",
                    "label": 0
                },
                {
                    "sent": "By optimizing the test data likelihood, you could actually do quite well, yeah?",
                    "label": 0
                },
                {
                    "sent": "Discussion yesterday and it seems to me that.",
                    "label": 0
                },
                {
                    "sent": "That would be a good thing to do, but it's like they're doing a cloud optimization like cross validation, do an independent data.",
                    "label": 0
                },
                {
                    "sent": "And it is with that ought to be more robust.",
                    "label": 0
                },
                {
                    "sent": "In general.",
                    "label": 0
                },
                {
                    "sent": "I was kind of hinting at that, but I didn't want to be too.",
                    "label": 0
                },
                {
                    "sent": "Given the name of the workshop, I felt I was a dangerous thing to say, but yeah, I mean.",
                    "label": 0
                },
                {
                    "sent": "If this material got into the wrong hands.",
                    "label": 0
                },
                {
                    "sent": "No, I mean, it does seem to be the case that if you choose the hyperparameters to optimize some kind of test.",
                    "label": 0
                },
                {
                    "sent": "Are you actually choose good hyperparameters?",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Schubert Anne.",
                    "label": 0
                },
                {
                    "sent": "Then again, I suspect if you do get your prior to be better than, you're still going to be better being Bayesian 'cause The thing is, I don't really want to type to maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "I want to integrate over the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "So I really want to fix my prior so I can do the full Bayesian treatment an.",
                    "label": 0
                },
                {
                    "sent": "Is my answer.",
                    "label": 0
                },
                {
                    "sent": "How well do you actually do it?",
                    "label": 0
                },
                {
                    "sent": "I haven't done it 'cause this.",
                    "label": 0
                },
                {
                    "sent": "This wasn't really the test error 'cause it was not estimated.",
                    "label": 0
                },
                {
                    "sent": "From the same sample that I was, it's not like Aleve when I error, it's actually an estimate, as though you had an infinite amount of data from the teacher to estimate it.",
                    "label": 0
                },
                {
                    "sent": "So I would really have to do a proper leave one out type estimate.",
                    "label": 0
                },
                {
                    "sent": "Some of that makes me think of David with talk to the workshop.",
                    "label": 0
                },
                {
                    "sent": "We had an approximate Bayesian inference that this last year and the tears he uses this past Bayesian learning stuff.",
                    "label": 0
                },
                {
                    "sent": "Matthias was complaining about the model being wrong or something wrong.",
                    "label": 0
                },
                {
                    "sent": "It works that I don't care about the model or cry or anything.",
                    "label": 0
                },
                {
                    "sent": "All I care about is an algorithm that gives me a sparse solution.",
                    "label": 0
                },
                {
                    "sent": "His work is analysis is really on that house, but it's going to be and why this is fast without which hints that he would probably in this case this cross validator.",
                    "label": 0
                },
                {
                    "sent": "I'd be interested to see how they are D works 'cause I didn't do that one.",
                    "label": 0
                },
                {
                    "sent": "Doesn't really fit into the theory so well because it has a hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "So, so I don't really know how to do it, but.",
                    "label": 0
                },
                {
                    "sent": "It's a kind of weird thing, but maybe I think you can probably do it.",
                    "label": 0
                },
                {
                    "sent": "In theory, you just have to treat the high performances like parameters.",
                    "label": 0
                },
                {
                    "sent": "Like parameters, they're not really, but you're optimizing the marginal, so I don't really have to.",
                    "label": 0
                },
                {
                    "sent": "Maybe do some sort of.",
                    "label": 0
                },
                {
                    "sent": "I don't know how to do that.",
                    "label": 0
                },
                {
                    "sent": "He probably knows that.",
                    "label": 0
                },
                {
                    "sent": "What we do?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "I got in.",
                    "label": 0
                },
                {
                    "sent": "Right, right, yeah?",
                    "label": 0
                },
                {
                    "sent": "Technical one.",
                    "label": 0
                },
                {
                    "sent": "Where are you going to publish this?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, I've just done it like this week.",
                    "label": 0
                },
                {
                    "sent": "So so so.",
                    "label": 0
                },
                {
                    "sent": "Where am I going to publish it?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "It's a bit difficult with this sort of thing Jameela.",
                    "label": 0
                },
                {
                    "sent": "I'd like to actually publish it.",
                    "label": 0
                },
                {
                    "sent": "Buried in a physics joke?",
                    "label": 0
                },
                {
                    "sent": "No, I don't want to Bury in the physics Journal.",
                    "label": 0
                },
                {
                    "sent": "That's what I tend to do with my theory.",
                    "label": 0
                },
                {
                    "sent": "I've decided not to do that anymore.",
                    "label": 0
                },
                {
                    "sent": "In fact, I'm never going to publish in a physics Journal again, 'cause I'm not a physicist.",
                    "label": 0
                },
                {
                    "sent": "Well, actually I start reviewing for them as well, so you probably won't let me.",
                    "label": 0
                },
                {
                    "sent": "I think it's quite important that the systematic studies of more than this man.",
                    "label": 0
                },
                {
                    "sent": "Like that?",
                    "label": 0
                },
                {
                    "sent": "Get some problems.",
                    "label": 0
                },
                {
                    "sent": "Where is paperback?",
                    "label": 0
                },
                {
                    "sent": "Yeah, and I probably won't publish it in the Journal of Statistical Physics Japan, which is where you then cappuccino decided to publish their results.",
                    "label": 0
                }
            ]
        }
    }
}