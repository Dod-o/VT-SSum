{
    "id": "y4mvp3xylrfoqvmqix6mbrkbpeqe7pku",
    "title": "Well-known shortcomings, advantages and computational challenges in Bayesian modelling: a few case stories",
    "info": {
        "author": [
            "Ole Winther, Technical University of Denmark"
        ],
        "published": "Oct. 9, 2008",
        "recorded": "September 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/bark08_winther_wksaacc/",
    "segmentation": [
        [
            "OK, so.",
            "I tried to be very loyal to Niels.",
            "Kind of outline for this.",
            "Meeting, but I think at this point I feel like kind of maybe kind of our need and energy for discussing these things have gone so.",
            "Yeah, I'll try.",
            "But do you mean?"
        ],
        [
            "Not too optimistic, let's see.",
            "OK, so.",
            "I actually have two case stories, so the one is is kind of taken from from population biology and it's about if you make a sample.",
            "You go out to the jungle and make a sample.",
            "Collect a lot of species.",
            "You want to know how many species do we have in total in this jungle.",
            "So you want to predict what you have not seen, so it's like.",
            "It's like a multi normal distribution you have drawn from, but you don't know how many pins you want to know how many pins.",
            "And I'll talk about.",
            "While that is an interesting question for biometrics as well, and I will talk about some of the ways we try to solve that problem.",
            "And I come to kind of two.",
            "Maybe not so surprising conclusions is that everything is much, much better than maximum likelihood.",
            "And to make Neil happy, the model is always wrong.",
            "And you can eyeball that by looking at some error bars, but you cannot.",
            "You space to tell that, especially not in this case, because I don't, I only have one model that are fit to the data.",
            "So I'm really I should go and hunt for other models and I hope that I can get help from the audience to come up with some better models.",
            "Then I'll change change subjects completely and talk about it for the rest of the talk.",
            "Talk about how to compute marginal likelihoods will limit the call assembly because I think.",
            "In my opinion, really, the strength.",
            "This is also because I've read David Mackay.",
            "He says, you know.",
            "Maybe sometimes base is doing more or less the same as many frequentists isn't made us because the frequentist estimators trying to imitate base, but the really advantage of base which we do not.",
            "Don't always explore fully is model comparison right?",
            "So the grand challenge here is to take an interesting model and actually being able to calculate the marginal likelihood.",
            "And the motivation I had for doing this was that I did some work with my friend and urish on computing corrections to EP.",
            "And it turned out, actually, that these corrections were many times quite small.",
            "So when.",
            "In order to really validate it.",
            "Then we had to make very very precise monocolor assessments of the marginal likelihood.",
            "And then I I kind of.",
            "Use caution process classification as a case story and I thought OK. Gibbs sampling should work and.",
            "I know why it doesn't work.",
            "Let's try to cure that.",
            "It turned out that the cure was not perfect, but that shows just about kind of my point about.",
            "It's really a challenge."
        ],
        [
            "To do this OK.",
            "So this is not very clear, but this is maybe it could.",
            "So imagine you go to the jungle.",
            "It's already said and you record some tap years and some elephants, and maybe there's a monkey up here in the tree.",
            "And you went from that to extrapolate to saying.",
            "How many different species do we actually have in this jungle?",
            "So that's the basic."
        ],
        [
            "Problem.",
            "And why does that arise in by Infomatics?",
            "This is maybe not so clear to see, but.",
            "In in the.",
            "At the very important development in biology.",
            "Is that we can now sequence very cheaply.",
            "And we can use this to assess different things.",
            "For example, we could we could.",
            "Sequence text Texas small pieces of DNA that has something to do with the expression level of a gene.",
            "That's one example, but I mean you can use many different enzymes to cut DNA and.",
            "And other stuff I don't know really.",
            "Many of the details.",
            "But in this case, I mean we have a.",
            "Very good associate professor now in biometrics in Copenhagen, and he's very interested in a technology called Cage and what Katie allows you to do is to.",
            "Pinpoint the exact location on the genome where you have the transcription start site, so where the reading of the gene starts and this is an example of.",
            "Hello side with several genes and you can see this is the indication of how many of these sequent tax has appeared in different positions and kind of the textbook.",
            "Kind of description of this is that you have a single.",
            "Transcription start sites.",
            "Maybe you have heard about that OtterBox, which is kind of a signal.",
            "It is just TATAA or something like that.",
            "Small variations of that that occurs like 30.",
            "Nucleotides before the transcription start sites so that is if you look in the genome in any textbook, you'll see that is the kind of classical picture this signal in the in the DNA and that initiates the transcription start.",
            "Like 30 base pairs longer downtown downstream, but the real picture is not as simple as that because you can.",
            "You can see that sometimes you have these single so-called single free peak promoters.",
            "But then you can also have really.",
            "Yeah.",
            "Brought distributions like this.",
            "But I'm impacted the question I want to ask is that if we maybe we want to.",
            "To go and do some modeling and then go back to the people who actually pay money to do this and tell them you need to do so much more in order to have seen everything right.",
            "That would be very satisfactory answer for them because then they know how much money they have to invest and what they get out of it.",
            "How much knew they would see?",
            "So there's a clear prediction and kind of inference task here.",
            "So when.",
            "This is clear, kind of clear what the problem is.",
            "So we could.",
            "I mean we could.",
            "I mean, we have kind of different things we could do.",
            "We can do different levels of resolution if we know all the annotated genes, then we could kind of lump things together and say this is just one position, right?",
            "This is 1 gene or we can also try to predict how many single positions.",
            "On the whole genome, what we see if we, if we for example doubled.",
            "The number of tax was sequenced."
        ],
        [
            "Yes, so I made this just before because after this discussion about that we are we actually allowed to look at the data and so on.",
            "I thought I actually didn't look very much the data and I think it doesn't make much.",
            "It doesn't really give you much information about what you want to predict just to look at the data.",
            "So what I've plotted here is.",
            "Is kind of the distribution over how many?",
            "Attacks to get in a certain position, right?",
            "So this means that.",
            "There are like in this for this data set, there's like almost let's say 2000.",
            "Jeans that only have 1 tag.",
            "Right and then so and so on.",
            "1002 and so on.",
            "And then you can see if you go out in the tail you have.",
            "Also one.",
            "One gene that has served 12 under tax.",
            "Right?",
            "So this is kind of multiplicity of.",
            "The different number of tags.",
            "You have seen first certainty, so this is the data and from this you want to be able to predict.",
            "If you doubled in how much?",
            "No sorry if you doubled, yeah.",
            "If you double the number of tags."
        ],
        [
            "How much more you would see.",
            "This is another data set which looks more smooth.",
            "I think it's a larger data set."
        ],
        [
            "Yes, OK so.",
            "We can do this very elegantly with kind of a nonparametric Bayesian method.",
            "So the idea now is that we have observed.",
            "Accounts and 12 NK&K pins.",
            "And then we can use the this sampling formula to just say something about what is the probability that we see account in a pin we haven't seen before.",
            "And so kind of the probability of observing something.",
            "Some account, one in the K + 1 pin is given by this formula here, so you can see this model has.",
            "Basically only two parameters.",
            "It does a theater which corresponds to, like a pseudo count.",
            "For observing you new things and then it has this Sigma which is kind of OK.",
            "I should explain.",
            "K is the number of pins and N is the total number of counts we have already.",
            "Right, so you can see this Sigma.",
            "If that's large then you kind of.",
            "If it's close to one, then you tend to see more and more things.",
            "Right, because then you will not.",
            "You're not even not the case so fast.",
            "OK, So what is the probability then?",
            "Our serving?",
            "Let's say the Jeff pin again.",
            "So the N + 1 count sample is in this pin.",
            "Then it's simply given by this probability here.",
            "So you can see it's proportional or disproportional to the number.",
            "Of counts we already seen, minus this signal, right?",
            "OK, so let's go back to our jungle example and see.",
            "Let's say that we have.",
            "We have observed two different sequences, maybe two elephants and monkey and two tapirs or this other sequence.",
            "So they had the same.",
            "You can see that two elephants in both into Pearson both and so on.",
            "So we can now use this sampling formula to write down what is the probability of observing this sequence.",
            "And then you do that simply by using this formula and then you will read.",
            "I've tried to indicate things that are new spaces, right?",
            "So that is elephant monkey to PS.",
            "So this is please.",
            "And you can do the same for the different sequence and what it turns out of course, which is very nice.",
            "Is that these two sequences have the same probability?",
            "Which is also reasonable because you believe that I mean.",
            "The way that you.",
            "Draw.",
            "Then I mean the probability of what you have seen in total with all the counts should not depend on the sequence.",
            "Of course you can ask within this kind of things where you do this tag based polity.",
            "Maybe there could be some errors due to amplification and so on that could violate this.",
            "But basically it's a very reasonable assumption, and what is very interesting is actually.",
            "That if you try to write down you say OK, you have only two parameters in this model you would like to have a model with more parameters then actually it turns out that you cannot satisfy this requirement.",
            "With more parameters I mean so this is so.",
            "Exchangeability is kind of restricted.",
            "We are only allowed to have these two parameters."
        ],
        [
            "Yes, OK, but let's let's try to be patient about this.",
            "You can see we're almost there because."
        ],
        [
            "We have this sampling formula so we can also use that to run into the future right?",
            "We can use it as a predicted distribution, given that we know.",
            "Given that we have.",
            "Some idea about these parameters?"
        ],
        [
            "Prompters OK, but it's very.",
            "Very easy, of course to to kind of generalize.",
            "This sampling formula and write down what is the.",
            "What is the likelihood function in general.",
            "You can see this term comes just from the normalizer.",
            "This term comes from the news pieces.",
            "The first counter phone news pieces and this is kind of the remaining counts.",
            "In any of this pieces.",
            "So the two parameters of the model is.",
            "As I said, this signal is between zero and one, which is kind of the decay of.",
            "Seeing news pieces in this suit account for.",
            "Phone.",
            "For seeing you.",
            "News pieces.",
            "So the predictive distribution.",
            "So let's say that we have.",
            "What is the probability of seeing and you count vector M. And that's very simple.",
            "We can simply write down.",
            "Bayes theorem here and then integrate over distribution.",
            "Yes.",
            "OK, so the question is of course what should I use here for the prior distribution?",
            "And.",
            "What I recent there was, I said simply, we have so much data.",
            "We have millions of these tags in some cases.",
            "So so actually it doesn't really matter because this.",
            "Likelihood here would be pretty.",
            "Repeat so so we can actually just take a flat prior here, but you can always question that.",
            "That was just my very.",
            "Innocent thing I I.",
            "A lot of this I've taken from some other statisticians which have written a series of nice papers on this.",
            "And they actually use.",
            "Maybe it be surpri or this where they kind of use.",
            "They use a little bit of the data to fit it, so I think that's not so, but it doesn't really give a big difference, I think."
        ],
        [
            "OK, so first the first point which will kind of yes if we had some more energy, we would take our hands.",
            "Is that actually averaging works so these two datasets I've shown you explain the example, you can see you had two very different situations.",
            "So let's start with this kind of most move datasets first, if I just kind of plot the.",
            "They look like they would come to us.",
            "Of this problem you can see that it's the maximum likelihood values here and then you have this kind of these kind of crunches here, and it's pretty peak.",
            "I mean you can see there's some variation.",
            "Of the parameters, but basically it's quite peaked and probably if I hadn't used to keep sampling to get.",
            "To get kind of different parameters to matsson, it would have made not such a big difference compared to just using the maximum likelihood.",
            "But for this other data set, you can see there's something going on here, because the maximum likelihood estimate is actually down here, very close to zero.",
            "It's like 10 to the minus 7.",
            "Right?",
            "And remember that this parameter is.",
            "Is.",
            "Is this?"
        ],
        [
            "Pretty important for predictions.",
            "Right, because it goes into here.",
            "Back here, right so?",
            "Chris so when Sigma Zero this is basically yes yes yes yes.",
            "Yes, you can see here this.",
            "If this signal is very small then you can see you get a really different behavior.",
            "So it makes a big difference whether it's 10 to the minus."
        ],
        [
            "Even or 10 to the minus three.",
            "And you can see when I do the Gibbs sampling.",
            "Of course I get some small values, but I also get something out here.",
            "Right so.",
            "So we're ignoring the uncertainty and the parameters would give completely different.",
            "Predictions and you can also see if some of the kind of more frequency statistics approaches to this.",
            "They actually say states sometimes that there's not information in the data sets to say anything about.",
            "Future samples, and I think the problem is.",
            "Kind of singularity."
        ],
        [
            "This.",
            "But the.",
            "We can actually just run this, and this is kind of the example, so this is.",
            "And here is the number total number of text we have.",
            "So this is the actual.",
            "The actual values so OK, let's focus on this top one first.",
            "So this is the actual values we have and this is kind of a prediction into the future.",
            "Simply running my keep sampling and running my running the sampling formula.",
            "Forwards so another thing I would also say here is that I've done something slightly over like it's not really non patient, but I have also done subsampling.",
            "So I've just.",
            "Subsample out, replacement down to half the size, the actual size I have, and then I want to predict.",
            "What is the?",
            "What is the?",
            "How many text do I get if I run up to the to this in?",
            "I actually happen?",
            "This is kind of this is a blown up of this now you can see there's something fun here because.",
            "I actually miss the true value and you can see I not only miss the true value, can also see that my error pass.",
            "Kind of the variation I get from two things.",
            "I get the variation from the variations in the sampling formula and I also get the variation.",
            "From the kind of the the averaging with parameters, it gives far too small error bars, right?",
            "So that I was not so happy when I saw this.",
            "And actually, I wrote down to this nice statisticians in Italy and they said that they could.",
            "They didn't believe this because they had kind of calculated it and they thought that the Airbus.",
            "Would go like some power of in.",
            "And after a few weeks, the road had written another paper where they had actually calculated this and they said this is basically correct, right?",
            "So you can see here just looking at this data.",
            "And.",
            "I mean, it's wrong.",
            "It's clearly wrong, right?",
            "I mean, I think I didn't even have to do cross validation to see this is not pretty stick that you have so narrow error bars.",
            "And what is going on here is that.",
            "We have a lot of data that's.",
            "So that was of course exposed that the model is wrong.",
            "But I mean it's still a very useful model because it gives.",
            "I think this this prediction sign right ballpark.",
            "I'm pretty sure about that.",
            "OK, so the other curves here on the slide is where I can also put a different threshold.",
            "Here I put a threshold saying I will only count things that have at least two tags as a new species and now you can see it starts to become a little bit more well behaved in here I've said I only will count things that have at least 30.",
            "30 text as a new species and then you can see it's right on.",
            "So of course it has something to do with kind of uncertainty in the in the really low count tags."
        ],
        [
            "This is the other data set.",
            "Yeah.",
            "So it's pretty far fetched."
        ],
        [
            "OK.",
            "So that was almost the conclusion of the first path.",
            "You still not very energetic.",
            "I don't know what I can do.",
            "I'm sorry, so parameter averaging works.",
            "Yes, OK, the model is always wrong.",
            "Yeah, well we knew that.",
            "And this is actually revealed and they have very much data.",
            "Also this nice paper I'm referring to all the time.",
            "I should have given the the reference they show it works very nice, but they typically work on datasets will only 1000 or at most 10,000 tags.",
            "Better show you know that the patient confidence intervals 95% of the time when you do subsampling.",
            "You get with it.",
            "I mean yeah, 95% of the subsampled things gets into within the hour parts and so on so it works.",
            "Small data set.",
            "You can also see.",
            "Another thing is I only considered one model and it's.",
            "I would really like that somebody in the audience told me OK, you actually using it much to a simple model.",
            "You should do Iraq.",
            "Some kind of hierarchical approach.",
            "So I would say yes, thank you.",
            "I mean that's then I can update my.",
            "Prior or models right?",
            "So I would yeah please do that if you can.",
            "So, but if I had two models then I would like to do what the second part of the talk is about.",
            "Being patient about model selection, right?",
            "Because then I would really compare these two.",
            "So maybe if I have time, I would also like to try any radical model which I could compare to this model with a flat prior and see.",
            "Which one has had the best?",
            "Had the best marginal likelihood.",
            "OK, any comments on this?",
            "Yes.",
            "The message is the message here that that's affecting your has.",
            "Underestimate the variance for the small small."
        ],
        [
            "Good job on the.",
            "Yeah.",
            "I think these error bars are pretty narrow in any case.",
            "So I think it probably.",
            "Yeah.",
            "I just wish somebody would tell me how can I get bigger class right?",
            "So that's that echo message."
        ],
        [
            "That I'm a little bit lost here.",
            "Yes.",
            "What you're doing here, and with respect to this previously holding back some data and then you know looking forward to see what your model predicts and and seeing how that compares.",
            "That's the example of this sort of within model model.",
            "Checking it.",
            "Yeah, yeah, sure no, I'm not against that at all.",
            "So I'm with you, Chris.",
            "I'm with you.",
            "I will you, but I mean I mean.",
            "But my point was simply I didn't need to do that here because I could just look at the predictions and see that the error bars are just.",
            "They are counter intuitive, but of course I mean.",
            "Example of war.",
            "You should do, maybe not the idea of holding outdated.",
            "Yeah.",
            "Saying is it.",
            "But you would get basically exactly what they would suggest you.",
            "How many chosen this subset of data and then project forward to your modeling people.",
            "It will predict and calculate some summaries statistic which.",
            "Yep.",
            "Yeah, I could calculate it, yeah?",
            "Yeah, I like it.",
            "I mean I don't mind and I cannot.",
            "I mean, I I I, I'm not in the church when I do this with other people so I have to.",
            "They would ask me to do cross validation all the time.",
            "Yeah.",
            "Yeah.",
            "But I mean maybe a point.",
            "It doesn't really help me to find a better model that.",
            "But hopefully it helps us to talk with people like you.",
            "Because.",
            "Whatever fancies.",
            "Gross.",
            "Presumably if you talk to the balance about these investments, they give you some immensely complicated story right about.",
            "You know how, yeah?",
            "But then I have a hammer to hit them with.",
            "And that is the exchangeability, right?",
            "I can say, but I mean you can come up with other models like this, But then not exchangeable.",
            "No, but I think I don't know.",
            "No matter.",
            "I mean, I could say maybe there's some noise in the process, like amplification.",
            "Will kind of create.",
            "Like the different things, there's also sequencing errors, so you get kind of a lot of random things on the genome, and I think also there's a lot of ways to try to clean up this data, but I mean that's.",
            "Liver cheating, right?",
            "Because yeah.",
            "Are you saying that the OK so you're saying that the model doesn't fit the data right?",
            "Yeah, but you're saying that it's the most general model which is expensed exchangeable.",
            "It's not a proof of the data.",
            "Is not exchanged.",
            "Did you say?",
            "You mean that the null hypothesis that the data is exchangeable?",
            "That's been the rejected having these things?",
            "The DNA button you just say I'll simplify that, like taking discount right and.",
            "Yeah, but I think the biologist.",
            "I mean, I think that the whole setup of recording this data is it's meant to be exchangeable, right?",
            "No, but I mean it's you you're trying to take independent samples.",
            "From the genome.",
            "Which is proportional to how much actually this specific start side is used, that is, that is kind of the idea, right?",
            "So this is like a random sample.",
            "Are you saying you need to be more than that process?",
            "Yeah.",
            "Yep.",
            "Really.",
            "But not according to the experimental procedures that they use.",
            "To get rid of things like sequencing errors in these kind of, get rid of all that experimental stuff.",
            "Taken real genome.",
            "Yeah, do some experiment where you sample DNA and see what they look like.",
            "That's a good point.",
            "Yeah, I think there's also some issues about that.",
            "You have something that Maps back to the team in multiple places, right?",
            "And maybe that has been.",
            "Actually removed in this data set.",
            "Maybe that could affect.",
            "But I mean call you almost did like rejected a null hypothesis here based on a single model.",
            "I almost heard you say that.",
            "Sorry, sorry card, I'm just.",
            "I'm going to.",
            "I'm actually going to use it and then I'm going to kind of make a disclaimer saying it's not completely precise, but the results are in the right ballpark, so it seems like that some of the results are useful.",
            "Purposes yeah.",
            "OK, and you don't want to replace it by a more complicated model, which would run for ages and the broader Arab are actually.",
            "You don't want to do that if if some if it's if somebody can give me an easy to implement model of course too that would be just happy to have sort of the model error.",
            "More realistic measure for them, I would rather have a having model ibbitson model.",
            "I mean it's.",
            "Anyway.",
            "Yeah.",
            "But I mean I should also say that there's some some statistics you can get out of this thing, which is more, maybe more more interesting than this.",
            "And that is what is called the coverage, which is how much of the complete probability.",
            "You have seen, right?",
            "Because of course.",
            "I mean, a lot of this.",
            "New jeans I record here that they only have very few tags, so that means they're not used very much.",
            "I mean, maybe that's not so interesting.",
            "It's more interesting to know.",
            "How much kind of?",
            "Completes the complete usage of.",
            "Of genes have I seen so far so that might be 95% here and that is a more robust statistic than the.",
            "Predicting the number of species.",
            "OK, but I think I have to speed up a little bit so I."
        ],
        [
            "It's completely to another problem.",
            "Very nice problem.",
            "You can state it in one line.",
            "You have want to calculate this integral.",
            "Right, and we also like to use model which is kind of informed which as we build in biology or whatever weather.",
            "And we don't want to kind of.",
            "We don't want to be so restrictive about the models we use.",
            "'cause we believe in putting in prior knowledge, right?",
            "So we should have kind of a general purpose.",
            "The.",
            "Machine here we could use have a nice black box.",
            "Actually.",
            "This is why we want to have a black box to calculate this integral.",
            "That would, I think, that would be.",
            "Actually, I think the only thing that could convert me to being non patient is that I actually give up on doing this.",
            "Too high enough precision, so so I mean that I simply have to spend too much time.",
            "I'm trying to do this.",
            "Without them in so that I mean there all the support vector machine guys, all the frequencies they have gone unsolved, more much more interesting problems while I'm still waiting for my sampler.",
            "Finishing it.",
            "Yeah, in this yeah no.",
            "Yeah yeah yeah.",
            "Yeah, that's a bias in the problems they want to solve, right?",
            "Yes, OK, so let's talk about approximate inference Montecarlo problems with slow mixing, so we have all the nice theorems, but we know many times in practice we have to wait, wait far too long and also it's nontrivial to get to.",
            "The marginal likelihood is much easier to get an average or some parameter or something like that.",
            "If you go to the deterministic methods then we have expectation propagation is popular.",
            "For example, for caution processes or variational Bayes.",
            "So loop plus these methods are sometimes precise, but you know we cannot control.",
            "It's not built in that we can say something about the approximation errors we have.",
            "And also I think that there may be more restricted than the Montecarlo, right?",
            "In this in this respect that we.",
            "We for example when we do hierarchical models, we have to have this conjugate families.",
            "And yeah.",
            "In general, also things like VP, which is kind of the most general applicable method, it kind of underestimate uncertainties.",
            "To degree where where it becomes useless in some applications."
        ],
        [
            "OK, So what is the motivation for trying to find very precise how much time do I lift?",
            "OK, that's fine.",
            "How the motivation for doing this?",
            "Is that we?",
            "We is manfreda.",
            "Norwich has me has come up with.",
            "A way to make perturbation corrections.",
            "To the two EP.",
            "So the idea there is that we can actually.",
            "We can actually run EP to convergence and then we can calculate with the statistics of the EP solution, we can calculate a correction and this is now what is.",
            "This plots are not very big, but this is here.",
            "We have taken the Coosa Rasmussen 2006 Yamaha setup where we have a pretty big data set.",
            "From under car list of tickets.",
            "From the USPS, and then we have a kernel function.",
            "And profit the classification and we have these two parameters, so we're looking at a grid.",
            "It's not so clear, but this is the lock.",
            "Length scale and this is the lock, kind of.",
            "Kind of strength prefactor of the kernel.",
            "And in this in this space.",
            "Parameters we want to calculate the kind of our correction term.",
            "And this is the difference between the lock.",
            "Normalize of EP and then the correction.",
            "And you can see what you cannot see, but what I can see here is that the corrections are kind of pretty smooth and they're pretty small, right?",
            "OK, so let's try to do some under Carlo and there already question resolution that did that and if we kind of try to subtract.",
            "Then MCMC, so subtract the EP from the MCMC.",
            "We get this picture which actually.",
            "Is pretty precise.",
            "If you look at the scales here, you can see that this white stuff is kind of almost 0 error, so it's pretty precise.",
            "I think.",
            "Also took a pretty long time to run.",
            "It was, yeah, I'll come back to what that is.",
            "OK, this is our best bet and I will not tell you how we did that right now.",
            "But you can see that this wide area up here, which is not cause the correctly predicted correction to be 0 simply because we cannot run our method up here because it's.",
            "Two non caution.",
            "It's an interesting region, so we also really.",
            "Yeah, we want to do this."
        ],
        [
            "Yes.",
            "OK, so let's talk about some methods to doing this.",
            "I mean, the kind of vanilla idea is just to do.",
            "Important sampling, so that means that we rewrite our matching likelihood.",
            "By introducing this.",
            "Q distribution and we divide by T as well.",
            "And then if we have.",
            "We run a sampler sampling from this distribution.",
            "Then we can.",
            "We can actually write an approximation to the marginal likelihood simply as.",
            "Is this ratio evaluated in all the samples?",
            "This has a really bad reputation becausw.",
            "This.",
            "So called important weights here.",
            "They tend to worry.",
            "Very much right?",
            "So I mean if maybe if you run.",
            "Yeah.",
            "Sample then there's only actually only one of them, which.",
            "Will be.",
            "Kind of dominating in this.",
            "This song.",
            "So it doesn't really work right, because you can also see that if these two are not really well matched, then it's.",
            "It's it's difficult."
        ],
        [
            "Yes, so better ideas which is.",
            "Which I know you guys use is.",
            "Is what is called thermodynamic integration and there's many variants of this.",
            "There's parallel tempering, simulated tempering and annealed importance sampling.",
            "The basic idea is that we kind of introduce kind of an interpolating distribution, right?",
            "So if you call.",
            "The likelihood times the prior H. Then we can write a new distribution which is conditioned on this tempering parameter.",
            "As this.",
            "This term here to the power beat and then a Q distribution to the power 1 minus beta.",
            "Then we can actually now write the difference between.",
            "And normalized, evaluated in two different inverse temperatures.",
            "This is kind of where the tempering work from this comes in.",
            "It has this integral.",
            "Over inverse temperatures and then.",
            "The derivative of the.",
            "After lock partition function.",
            "Right, this is just I mean going.",
            "From here to here is of course real because you.",
            "You can integrate derivative like this, but you can also go the other way and actually calculate this and then it turns out that you can write this.",
            "Quantitie here as as an average over this distribution and what you have to average is the lock.",
            "Of this ratio between these two.",
            "Qantas is here.",
            "And I think this is if you compare this to important family.",
            "This is of course nice in the way that.",
            "I mean locks are much more well behaved than actual.",
            "Actually, Rachel's but the downside is that we actually had to.",
            "Do an integral here right so we have to kind of.",
            "We have an extra integral that we have to do.",
            "Yes, but let's say now that we have.",
            "We can actually calculate this.",
            "Partition function for some value of beta.",
            "We could for example.",
            "Should be to want to be equal to 20.",
            "Then you can see that this reduces to this Q distribution, which we hopefully know the normalizer off.",
            "So that means that maybe.",
            "Maybe if this is already normalized, then this is actually one, so we can actually calculate.",
            "The marginal likelihood.",
            "By doing this.",
            "Let's say we have just make equal spacing.",
            "This is very primitive kind of interpolator integration.",
            "Then we can simply say if we run end beats and pizza chains at equidistant.",
            "Values here, then we can approximate the marginal likelihood with this expression, right?",
            "So you can see that we actually have.",
            "To draw samples from.",
            "All these change and evaluate this ratio here.",
            "I should just make one command.",
            "Is that this kind of in physics there's a method called multi Canonical where you go away from.",
            "From sampling from this distribution here.",
            "Which might actually work better.",
            "Much better than all this.",
            "But I mean, I haven't really any practical experience with it.",
            "I hope one day I will have time to."
        ],
        [
            "Do that.",
            "Yes."
        ],
        [
            "OK.",
            "But I mean you can.",
            "Still you can see, we still have to be able to sample from a distribution here, right?",
            "So we have to find some good old samples that could do this."
        ],
        [
            "I kind of like that.",
            "I kind of like the now.",
            "I kind of like Gibbs sampling.",
            "I don't know if I like it after.",
            "After this experience, I like it anymore, but I mean, let's discuss Gibbs sampling a little bit, yes."
        ],
        [
            "Yeah.",
            "Yeah.",
            "Very nice comparison.",
            "And.",
            "Yeah.",
            "Would work.",
            "Yeah, that's good.",
            "Yeah, I should look into that.",
            "Before I use all my time, yes.",
            "So basically I mean, did you find that multi Canonical in general was the best?",
            "General.",
            "Examples where.",
            "Yeah, yeah.",
            "Yeah, so I mean, I mean, if you kind of if you if you read the physics Ledger, it's clear that.",
            "Kind of tempering ideas would sometimes not work becausw.",
            "Becausw this distribution.",
            "Kind of changes very much if you change pizza just a little bit, right?",
            "So that's kind of.",
            "It can.",
            "It can kind of the relevant parameter settings can change.",
            "Very abruptly will the small change some pizza, so that's.",
            "Makes it hard to."
        ],
        [
            "Do this inspiration.",
            "And multi Canonical kind of avoids tries to avoid that problem.",
            "OK, but let's turn to this just the basic sampling problem.",
            "So here I have a little bit in mind that we're going to sample or caution process.",
            "So Gibbs sampling is very nice in the way that we cycle over the conditionals, and then we sample the conditional.",
            "So I've tried to draw this.",
            "To do this for just.",
            "And normal distribution in 2D.",
            "And here you can see the problem is that we we followed the coordinates, so that means that if this is very narrow then our kind of random walking from one end of this.",
            "Distribution to the other will take a long time.",
            "You can also quantify this right because you can kind of say what is the typical steps I should take.",
            "It would be kind of proportional to the.",
            "To the standard deviation."
        ],
        [
            "Along.",
            "The different directions so we can of course make a trivial curve when we sample and on.",
            "A normal distribution, that is that we instead of sampling.",
            "Instead of sampling our F here, we sample a different distribution, see which is just a.",
            "Completely independent components and then we make a linear transformation.",
            "Of C, so we get the F so you can see I've done this here now so I take my covariance matrix and decompose that with some.",
            "Decomposition for examples juliska decomposition.",
            "Then I draw samples here.",
            "With Gibbs sampling for C and then I transform and then I get these samples and they look like they have.",
            "Have a really good.",
            "Good mixing, so that seems to be a cure here.",
            "Of course.",
            "I mean we all know how to sample from multivariate normal."
        ],
        [
            "So it doesn't really matter.",
            "But if we go to.",
            "Gaussian process classification.",
            "Then we have a similar problem.",
            "So I mean the posterior distribution and caution.",
            "Process classification is that we have a profit likelihood function and then we have a GP prior.",
            "Here you can see now where this is this, because this tempering.",
            "Parameter that I talked about.",
            "Previously so it works like it works like it's one or the prefactor on the.",
            "Under Colonel.",
            "OK, we can go to.",
            "We can go to.",
            "I would like to go to a.",
            "For reasons that will be told later, I would like to go to a nice reformulation so that means I will actually use.",
            "Step function likelihood here instead of the profit and I could do that by writing the property like.",
            "I forgot the integration.",
            "There should be an integration here or this FN as an integral over.",
            "A normal distribution for new latent variable, which I call if and if for noise free.",
            "I can write down the joint distribution of this new latent variable, and then I can also integrate out.",
            "The old?",
            "If so, I get.",
            "A distribution for the noise free which has this step.",
            "Function here and then it has a new kernel function which is just.",
            "Hey got an additional idea.",
            "So now I have this one and if I want to sample from the good old F then I can.",
            "Use the conditional here, which is caution.",
            "To get those samples.",
            "Yes, OK, so the idea now is that I want to derive an efficient.",
            "Gibbs sampler.",
            "For this distribution here.",
            "How do we do that?",
            "Can we do that?"
        ],
        [
            "Yes, OK, so here's some.",
            "Here's a list of related work for this.",
            "So the efficient Gibbs sampler is this preprint.",
            "That's pretty.",
            "That's where I've got the method from.",
            "Will it sufficient?",
            "We'll see that.",
            "And.",
            "I should say that we are also working on different ways to do.",
            "And read for Neil's.",
            "Also worked in different ways to sample with all regularization.",
            "Another thing I thought about when I wrote down this is that in the good old days, before we were really fully patient Palfrey an invented this concept of playing billiards inversion space.",
            "And when I think about bagging that that is actually.",
            "Closely related, I mean to a kind of Monte Carlo method for.",
            "For for the GP problem, I don't know how precise that would be.",
            "I mean, it probably has some nice properties.",
            "I mean also has her push has.",
            "His current base point machine, but it's basically the same ideas, poorly and then call an multicluster.",
            "Use hybrid Monte Carlo in this white and space, an annealed importance sampling.",
            "Maybe call can give that thesis.",
            "If it's important, then our we also found some work by Kian Ming Koran.",
            "And a different method which."
        ],
        [
            "Also seem to have nice promises.",
            "Yes, OK, so let's try to see what happens if we just run a vanilla Gibbs sampler.",
            "On this truncated Goshen right, so there is now that we have.",
            "We have the caution from before, but now we are truncating it.",
            "So just for simplicity so we only looking at positive values.",
            "So basically we have the same problem as we have for sampling the normal distribution, right that we have we have slow mixing along these very.",
            "Highly correlated.",
            "The.",
            "Directions in space.",
            "If we look at a normal distribution which goes this way so we have negative correlations, you can see it doesn't seem to have equally big problems so.",
            "So this problem has very much to do with.",
            "For this kind of."
        ],
        [
            "Positive correlations.",
            "Yes, so let's try to just apply the same idea as we solve the for the normal distribution.",
            "We can just introduce these.",
            "Independent variables and then we can do this salesky of the of the covariance.",
            "And remember that we had now we just for simplicity.",
            "We look at the constraints that just says that we have only positive labels, so all the labels should be.",
            "Should be positive, so that translate because this is linear transformation.",
            "This also translates into a linear constraint.",
            "For the C variables, right?",
            "Yes, it's just a linear transformation and also becausw.",
            "This is a convex region for F. Then I mean the region we have to integrate overseas also convex region.",
            "When?"
        ],
        [
            "Drive OK, I'll I'll speed up OK.",
            "Sorry guys OK. OK, so.",
            "We want to keep sending once we want to sample from the conditional so we can write down what is this constraint for the gave conditional right?",
            "So we take this basic.",
            "This F greater than one constraint that becomes this constraint right?",
            "And now you can see that depending on whether this Elijah is positive and negative, we.",
            "Will get an upper or lower bound, right?",
            "So now it's actually a totally double truncated.",
            "The conditional we have a normal distribution.",
            "Yes, OK, this is pretty easy.",
            "We just find.",
            "And.",
            "Which.",
            "What is the slope anniversary of a pond by looking at all?",
            "All the ones in the set of positive here.",
            "Time to take the maximum value here and take a minimum value here and the actual sampling is like this.",
            "We let Matlab generate a random number between zero and one, and then we calculate these.",
            "These error functions and then we take the inverse error function of this combination and then we have a new sample.",
            "So let's have a look at how."
        ],
        [
            "Works.",
            "So you can see now this is.",
            "In C space so you can see the constraints here.",
            "If we have a very correlated thing, then the constraints becomes autumn almost.",
            "Collinear and we can sample pretty freely in this space, right?",
            "And we had really good.",
            "It looks like we have really good mixing in the.",
            "In the space, so it seems like it's solving."
        ],
        [
            "Problem is, let's see what happens if we have negative covariance.",
            "Then maybe it doesn't look so nice, right?",
            "Because maybe we're back here to the same problem as we had.",
            "In the original space.",
            "But it seems to give.",
            "If you just eyeball this, it seems to be."
        ],
        [
            "Pretty promising.",
            "Yes.",
            "Yes.",
            "OK.",
            "Yes, what happened now.",
            "I forgot to take the right figures, but I will not show the result of running this method actually because it doesn't work so well.",
            "So I would like to talk about this.",
            "That actually worked well for the regional parameters we can use.",
            "So this is the this is the corrections we have predicted which will appear at NIPS.",
            "And you can see that pretty small, actually .5 at most 1.6 maybe.",
            "And this is what we get.",
            "And this is actually what we get with important sampling.",
            "And we use as importance distribution reduce the EP.",
            "Cube distribution.",
            "So this is an example where actually.",
            "Important sampling works in very high dimensions, but not for all parameters, so I'm stealing.",
            "I'm still on the lookout for something that can.",
            "We're still on the lookout for something that can solve this.",
            "Part of of the parameter space."
        ],
        [
            "Let me stop now.",
            "Yes, OK.",
            "Yes, so I said I wrote here the trouble Gibbs sampling Anna cure.",
            "I don't know if it's a cure really.",
            "I think it's very unsatisfactory that you need to run.",
            "The Monte Carlo for months I think that is not a very strong selling point of patient methods, so I think it's worthwhile really looking into.",
            "Two better sampling methods, and I think it's you cannot just say.",
            "I don't believe that you can just write a program that does Gibbs sampling for any model.",
            "I think you really have to go in and try to understand the problem.",
            "And assign samplers that will work for the problem.",
            "Yes.",
            "And then I mean my last question was like what is machine learning actually?",
            "And I don't know.",
            "I feel like sometimes that I spent a lot of time on reading, you know, kind of patient statistics papers.",
            "So maybe I'm just.",
            "Becoming like an amateur patient statistician, I don't know.",
            "I don't know if you have any comments on that where where machine learning is moving, I mean.",
            "Yes, thank you.",
            "Performing to the Noise Feedback plantation you make."
        ],
        [
            "Problem easier by having you viginal latent function.",
            "Where the shop boundary.",
            "Space.",
            "You might have any Department.",
            "I mean is everything is this convex?",
            "I mean this is such a nice playing ground, right?",
            "Because it's completely convex and.",
            "I like the like that hard boundaries because it's really easy conceptually to understand what you want to do.",
            "So I don't know if maybe Kyle has some experience with the difference between going now.",
            "You want to use the baby EP approximation as the important temple.",
            "Then it would be much better to try to do that in this mood version, which is the one where you, yeah, we also do that.",
            "Yeah, we also do that.",
            "Yeah, we definitely do that, yes?",
            "Base simulation.",
            "Trying too hard spheres tonight man.",
            "So everyone who does Hospice based software yeah.",
            "Yeah, but actually I think the point is actually that that.",
            "This part of the parameter parameter space is really where it's more or less.",
            "Like shop boundaries.",
            "So I think we have to be able to do that.",
            "Yes, I'm wondering if there's some results on sampling from complex regions and whenever polynomial I don't know much about it.",
            "Does anybody know?",
            "This is the version.",
            "Remember.",
            "I've been, I think maybe some proofs, but they don't give kind of the actual algorithm right?",
            "I don't know.",
            "Different comment which is.",
            "Take a long time.",
            "He's getting the right result anyway.",
            "So yeah.",
            "And our corrections are basically our corrections are basically giving the wrist small path.",
            "Yeah, that's that's a good message, but I think that EP is this.",
            "I think GPS is kind of the EP.",
            "Greatest success, right?",
            "I mean, many other models where you?",
            "Well, I don't really trust the results as much.",
            "Why you?",
            "Why can you be my selected?",
            "Can you just sample over?",
            "They have friends.",
            "If you work.",
            "Integrate.",
            "Right?",
            "You're right, but I mean, I mean, I consider this as kind of two levels.",
            "So if I can solve the first level of integrating over the caution process.",
            "Then I can add the other level.",
            "Right so.",
            "Because you can also see I mean, OK, I haven't shown that.",
            "But I mean this is kind of the high marginal likelihood region, right?",
            "So I would be I should be able to sample that in order to really get the marginal likelihood where integrate also or the hyperparameters.",
            "So.",
            "No, I would definitely like to do that and I would like to take the neural network and do the same thing.",
            "And then I say OK, caution processes as the higher.",
            "The marginal likelihood, I mean that is my grenko.",
            "Or any other model.",
            "Sample.",
            "I mean, I would do a joint sample in the in this parameters and in.",
            "Caution process.",
            "Have the same problem.",
            "Still need to be.",
            "Something the region of the other plans as well, yeah.",
            "She would advise that he would arrive in time for ladies.",
            "So you're saying you want to be able to compare different models by computing marginal life?",
            "It's like normal network, yeah?",
            "So I guess.",
            "When I find a little difficult about that is that Marshall, like loser, play sensitive to the priors that you set up?",
            "That's what I want to test.",
            "I want to test my prize, yeah?",
            "Neural network is actually a very good model, but.",
            "With the priors on the way.",
            "Things like that.",
            "You integrate overload.",
            "Yep.",
            "OK, maybe I should have another example.",
            "I mean I would like to build models for biological data where I could say for example in transcription factor exams I could say for example, are there any real time dependence between?",
            "The measurements of transcription factors at different times, and then I could I could calculate my collected for model without the time dependence on one will and then I can go back to the biologist and say that.",
            "I mean I have statistical support for there's a time dependence question.",
            "Yes, that's the stuff I want to.",
            "That's yeah, yeah, yeah, yeah.",
            "I'd like to.",
            "Created exception actually that we can compute.",
            "Position with a method like EP.",
            "Computer these results.",
            "We actually spent a lot of time looking for the bug.",
            "Right answers were always exactly the same.",
            "Something wrong here, but it turns out that it actually doesn't compute the right hand side as stupid points out.",
            "Typically what happens is that you get hard likelihood.",
            "Better log scale.",
            "Usually you know 10 or 20 apart or something like that.",
            "Yeah, correction of .5.",
            "This is basically saying you're spot on.",
            "For practical purposes I agree.",
            "I mean.",
            "And it took us till 2006.",
            "They're doing our equivalent of classification in a way that we were happy was quick.",
            "And relatively efficient and it's kind of one of the things we know.",
            "It come back.",
            "I would really also as older really want to try and workout what the most likely referring network like.",
            "I don't know how to do that.",
            "Remarkable that it works, but not for profit.",
            "But I think we should.",
            "I mean, it's kind of depressing that you took it so long to work that out.",
            "The reason why we try to to calculate?",
            "Showing.",
            "The difference is small.",
            "Yep.",
            "OK thanks.",
            "Maybe maybe it kind of a linear more I mean.",
            "OK, maybe it's time to finish performing for completed takes over.",
            "OK. OK, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I tried to be very loyal to Niels.",
                    "label": 0
                },
                {
                    "sent": "Kind of outline for this.",
                    "label": 0
                },
                {
                    "sent": "Meeting, but I think at this point I feel like kind of maybe kind of our need and energy for discussing these things have gone so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I'll try.",
                    "label": 0
                },
                {
                    "sent": "But do you mean?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not too optimistic, let's see.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I actually have two case stories, so the one is is kind of taken from from population biology and it's about if you make a sample.",
                    "label": 0
                },
                {
                    "sent": "You go out to the jungle and make a sample.",
                    "label": 0
                },
                {
                    "sent": "Collect a lot of species.",
                    "label": 0
                },
                {
                    "sent": "You want to know how many species do we have in total in this jungle.",
                    "label": 0
                },
                {
                    "sent": "So you want to predict what you have not seen, so it's like.",
                    "label": 0
                },
                {
                    "sent": "It's like a multi normal distribution you have drawn from, but you don't know how many pins you want to know how many pins.",
                    "label": 0
                },
                {
                    "sent": "And I'll talk about.",
                    "label": 0
                },
                {
                    "sent": "While that is an interesting question for biometrics as well, and I will talk about some of the ways we try to solve that problem.",
                    "label": 0
                },
                {
                    "sent": "And I come to kind of two.",
                    "label": 0
                },
                {
                    "sent": "Maybe not so surprising conclusions is that everything is much, much better than maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "And to make Neil happy, the model is always wrong.",
                    "label": 1
                },
                {
                    "sent": "And you can eyeball that by looking at some error bars, but you cannot.",
                    "label": 0
                },
                {
                    "sent": "You space to tell that, especially not in this case, because I don't, I only have one model that are fit to the data.",
                    "label": 0
                },
                {
                    "sent": "So I'm really I should go and hunt for other models and I hope that I can get help from the audience to come up with some better models.",
                    "label": 0
                },
                {
                    "sent": "Then I'll change change subjects completely and talk about it for the rest of the talk.",
                    "label": 0
                },
                {
                    "sent": "Talk about how to compute marginal likelihoods will limit the call assembly because I think.",
                    "label": 0
                },
                {
                    "sent": "In my opinion, really, the strength.",
                    "label": 0
                },
                {
                    "sent": "This is also because I've read David Mackay.",
                    "label": 0
                },
                {
                    "sent": "He says, you know.",
                    "label": 0
                },
                {
                    "sent": "Maybe sometimes base is doing more or less the same as many frequentists isn't made us because the frequentist estimators trying to imitate base, but the really advantage of base which we do not.",
                    "label": 0
                },
                {
                    "sent": "Don't always explore fully is model comparison right?",
                    "label": 0
                },
                {
                    "sent": "So the grand challenge here is to take an interesting model and actually being able to calculate the marginal likelihood.",
                    "label": 1
                },
                {
                    "sent": "And the motivation I had for doing this was that I did some work with my friend and urish on computing corrections to EP.",
                    "label": 0
                },
                {
                    "sent": "And it turned out, actually, that these corrections were many times quite small.",
                    "label": 0
                },
                {
                    "sent": "So when.",
                    "label": 0
                },
                {
                    "sent": "In order to really validate it.",
                    "label": 0
                },
                {
                    "sent": "Then we had to make very very precise monocolor assessments of the marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "And then I I kind of.",
                    "label": 0
                },
                {
                    "sent": "Use caution process classification as a case story and I thought OK. Gibbs sampling should work and.",
                    "label": 0
                },
                {
                    "sent": "I know why it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Let's try to cure that.",
                    "label": 0
                },
                {
                    "sent": "It turned out that the cure was not perfect, but that shows just about kind of my point about.",
                    "label": 0
                },
                {
                    "sent": "It's really a challenge.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To do this OK.",
                    "label": 0
                },
                {
                    "sent": "So this is not very clear, but this is maybe it could.",
                    "label": 0
                },
                {
                    "sent": "So imagine you go to the jungle.",
                    "label": 0
                },
                {
                    "sent": "It's already said and you record some tap years and some elephants, and maybe there's a monkey up here in the tree.",
                    "label": 0
                },
                {
                    "sent": "And you went from that to extrapolate to saying.",
                    "label": 0
                },
                {
                    "sent": "How many different species do we actually have in this jungle?",
                    "label": 0
                },
                {
                    "sent": "So that's the basic.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "And why does that arise in by Infomatics?",
                    "label": 0
                },
                {
                    "sent": "This is maybe not so clear to see, but.",
                    "label": 0
                },
                {
                    "sent": "In in the.",
                    "label": 0
                },
                {
                    "sent": "At the very important development in biology.",
                    "label": 0
                },
                {
                    "sent": "Is that we can now sequence very cheaply.",
                    "label": 0
                },
                {
                    "sent": "And we can use this to assess different things.",
                    "label": 0
                },
                {
                    "sent": "For example, we could we could.",
                    "label": 0
                },
                {
                    "sent": "Sequence text Texas small pieces of DNA that has something to do with the expression level of a gene.",
                    "label": 0
                },
                {
                    "sent": "That's one example, but I mean you can use many different enzymes to cut DNA and.",
                    "label": 0
                },
                {
                    "sent": "And other stuff I don't know really.",
                    "label": 0
                },
                {
                    "sent": "Many of the details.",
                    "label": 0
                },
                {
                    "sent": "But in this case, I mean we have a.",
                    "label": 0
                },
                {
                    "sent": "Very good associate professor now in biometrics in Copenhagen, and he's very interested in a technology called Cage and what Katie allows you to do is to.",
                    "label": 0
                },
                {
                    "sent": "Pinpoint the exact location on the genome where you have the transcription start site, so where the reading of the gene starts and this is an example of.",
                    "label": 0
                },
                {
                    "sent": "Hello side with several genes and you can see this is the indication of how many of these sequent tax has appeared in different positions and kind of the textbook.",
                    "label": 0
                },
                {
                    "sent": "Kind of description of this is that you have a single.",
                    "label": 0
                },
                {
                    "sent": "Transcription start sites.",
                    "label": 0
                },
                {
                    "sent": "Maybe you have heard about that OtterBox, which is kind of a signal.",
                    "label": 0
                },
                {
                    "sent": "It is just TATAA or something like that.",
                    "label": 0
                },
                {
                    "sent": "Small variations of that that occurs like 30.",
                    "label": 0
                },
                {
                    "sent": "Nucleotides before the transcription start sites so that is if you look in the genome in any textbook, you'll see that is the kind of classical picture this signal in the in the DNA and that initiates the transcription start.",
                    "label": 0
                },
                {
                    "sent": "Like 30 base pairs longer downtown downstream, but the real picture is not as simple as that because you can.",
                    "label": 0
                },
                {
                    "sent": "You can see that sometimes you have these single so-called single free peak promoters.",
                    "label": 0
                },
                {
                    "sent": "But then you can also have really.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Brought distributions like this.",
                    "label": 0
                },
                {
                    "sent": "But I'm impacted the question I want to ask is that if we maybe we want to.",
                    "label": 0
                },
                {
                    "sent": "To go and do some modeling and then go back to the people who actually pay money to do this and tell them you need to do so much more in order to have seen everything right.",
                    "label": 0
                },
                {
                    "sent": "That would be very satisfactory answer for them because then they know how much money they have to invest and what they get out of it.",
                    "label": 0
                },
                {
                    "sent": "How much knew they would see?",
                    "label": 0
                },
                {
                    "sent": "So there's a clear prediction and kind of inference task here.",
                    "label": 0
                },
                {
                    "sent": "So when.",
                    "label": 0
                },
                {
                    "sent": "This is clear, kind of clear what the problem is.",
                    "label": 0
                },
                {
                    "sent": "So we could.",
                    "label": 0
                },
                {
                    "sent": "I mean we could.",
                    "label": 0
                },
                {
                    "sent": "I mean, we have kind of different things we could do.",
                    "label": 0
                },
                {
                    "sent": "We can do different levels of resolution if we know all the annotated genes, then we could kind of lump things together and say this is just one position, right?",
                    "label": 0
                },
                {
                    "sent": "This is 1 gene or we can also try to predict how many single positions.",
                    "label": 0
                },
                {
                    "sent": "On the whole genome, what we see if we, if we for example doubled.",
                    "label": 0
                },
                {
                    "sent": "The number of tax was sequenced.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, so I made this just before because after this discussion about that we are we actually allowed to look at the data and so on.",
                    "label": 0
                },
                {
                    "sent": "I thought I actually didn't look very much the data and I think it doesn't make much.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really give you much information about what you want to predict just to look at the data.",
                    "label": 1
                },
                {
                    "sent": "So what I've plotted here is.",
                    "label": 0
                },
                {
                    "sent": "Is kind of the distribution over how many?",
                    "label": 0
                },
                {
                    "sent": "Attacks to get in a certain position, right?",
                    "label": 0
                },
                {
                    "sent": "So this means that.",
                    "label": 0
                },
                {
                    "sent": "There are like in this for this data set, there's like almost let's say 2000.",
                    "label": 0
                },
                {
                    "sent": "Jeans that only have 1 tag.",
                    "label": 0
                },
                {
                    "sent": "Right and then so and so on.",
                    "label": 0
                },
                {
                    "sent": "1002 and so on.",
                    "label": 0
                },
                {
                    "sent": "And then you can see if you go out in the tail you have.",
                    "label": 0
                },
                {
                    "sent": "Also one.",
                    "label": 0
                },
                {
                    "sent": "One gene that has served 12 under tax.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "So this is kind of multiplicity of.",
                    "label": 0
                },
                {
                    "sent": "The different number of tags.",
                    "label": 0
                },
                {
                    "sent": "You have seen first certainty, so this is the data and from this you want to be able to predict.",
                    "label": 0
                },
                {
                    "sent": "If you doubled in how much?",
                    "label": 0
                },
                {
                    "sent": "No sorry if you doubled, yeah.",
                    "label": 0
                },
                {
                    "sent": "If you double the number of tags.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How much more you would see.",
                    "label": 0
                },
                {
                    "sent": "This is another data set which looks more smooth.",
                    "label": 0
                },
                {
                    "sent": "I think it's a larger data set.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, OK so.",
                    "label": 0
                },
                {
                    "sent": "We can do this very elegantly with kind of a nonparametric Bayesian method.",
                    "label": 0
                },
                {
                    "sent": "So the idea now is that we have observed.",
                    "label": 0
                },
                {
                    "sent": "Accounts and 12 NK&K pins.",
                    "label": 0
                },
                {
                    "sent": "And then we can use the this sampling formula to just say something about what is the probability that we see account in a pin we haven't seen before.",
                    "label": 0
                },
                {
                    "sent": "And so kind of the probability of observing something.",
                    "label": 0
                },
                {
                    "sent": "Some account, one in the K + 1 pin is given by this formula here, so you can see this model has.",
                    "label": 0
                },
                {
                    "sent": "Basically only two parameters.",
                    "label": 0
                },
                {
                    "sent": "It does a theater which corresponds to, like a pseudo count.",
                    "label": 0
                },
                {
                    "sent": "For observing you new things and then it has this Sigma which is kind of OK.",
                    "label": 0
                },
                {
                    "sent": "I should explain.",
                    "label": 0
                },
                {
                    "sent": "K is the number of pins and N is the total number of counts we have already.",
                    "label": 0
                },
                {
                    "sent": "Right, so you can see this Sigma.",
                    "label": 0
                },
                {
                    "sent": "If that's large then you kind of.",
                    "label": 0
                },
                {
                    "sent": "If it's close to one, then you tend to see more and more things.",
                    "label": 0
                },
                {
                    "sent": "Right, because then you will not.",
                    "label": 0
                },
                {
                    "sent": "You're not even not the case so fast.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is the probability then?",
                    "label": 0
                },
                {
                    "sent": "Our serving?",
                    "label": 0
                },
                {
                    "sent": "Let's say the Jeff pin again.",
                    "label": 0
                },
                {
                    "sent": "So the N + 1 count sample is in this pin.",
                    "label": 0
                },
                {
                    "sent": "Then it's simply given by this probability here.",
                    "label": 0
                },
                {
                    "sent": "So you can see it's proportional or disproportional to the number.",
                    "label": 0
                },
                {
                    "sent": "Of counts we already seen, minus this signal, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so let's go back to our jungle example and see.",
                    "label": 0
                },
                {
                    "sent": "Let's say that we have.",
                    "label": 0
                },
                {
                    "sent": "We have observed two different sequences, maybe two elephants and monkey and two tapirs or this other sequence.",
                    "label": 0
                },
                {
                    "sent": "So they had the same.",
                    "label": 0
                },
                {
                    "sent": "You can see that two elephants in both into Pearson both and so on.",
                    "label": 0
                },
                {
                    "sent": "So we can now use this sampling formula to write down what is the probability of observing this sequence.",
                    "label": 0
                },
                {
                    "sent": "And then you do that simply by using this formula and then you will read.",
                    "label": 0
                },
                {
                    "sent": "I've tried to indicate things that are new spaces, right?",
                    "label": 0
                },
                {
                    "sent": "So that is elephant monkey to PS.",
                    "label": 0
                },
                {
                    "sent": "So this is please.",
                    "label": 0
                },
                {
                    "sent": "And you can do the same for the different sequence and what it turns out of course, which is very nice.",
                    "label": 0
                },
                {
                    "sent": "Is that these two sequences have the same probability?",
                    "label": 0
                },
                {
                    "sent": "Which is also reasonable because you believe that I mean.",
                    "label": 0
                },
                {
                    "sent": "The way that you.",
                    "label": 0
                },
                {
                    "sent": "Draw.",
                    "label": 0
                },
                {
                    "sent": "Then I mean the probability of what you have seen in total with all the counts should not depend on the sequence.",
                    "label": 0
                },
                {
                    "sent": "Of course you can ask within this kind of things where you do this tag based polity.",
                    "label": 0
                },
                {
                    "sent": "Maybe there could be some errors due to amplification and so on that could violate this.",
                    "label": 0
                },
                {
                    "sent": "But basically it's a very reasonable assumption, and what is very interesting is actually.",
                    "label": 0
                },
                {
                    "sent": "That if you try to write down you say OK, you have only two parameters in this model you would like to have a model with more parameters then actually it turns out that you cannot satisfy this requirement.",
                    "label": 0
                },
                {
                    "sent": "With more parameters I mean so this is so.",
                    "label": 0
                },
                {
                    "sent": "Exchangeability is kind of restricted.",
                    "label": 0
                },
                {
                    "sent": "We are only allowed to have these two parameters.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, OK, but let's let's try to be patient about this.",
                    "label": 0
                },
                {
                    "sent": "You can see we're almost there because.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have this sampling formula so we can also use that to run into the future right?",
                    "label": 0
                },
                {
                    "sent": "We can use it as a predicted distribution, given that we know.",
                    "label": 0
                },
                {
                    "sent": "Given that we have.",
                    "label": 0
                },
                {
                    "sent": "Some idea about these parameters?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Prompters OK, but it's very.",
                    "label": 0
                },
                {
                    "sent": "Very easy, of course to to kind of generalize.",
                    "label": 0
                },
                {
                    "sent": "This sampling formula and write down what is the.",
                    "label": 0
                },
                {
                    "sent": "What is the likelihood function in general.",
                    "label": 1
                },
                {
                    "sent": "You can see this term comes just from the normalizer.",
                    "label": 0
                },
                {
                    "sent": "This term comes from the news pieces.",
                    "label": 0
                },
                {
                    "sent": "The first counter phone news pieces and this is kind of the remaining counts.",
                    "label": 0
                },
                {
                    "sent": "In any of this pieces.",
                    "label": 0
                },
                {
                    "sent": "So the two parameters of the model is.",
                    "label": 0
                },
                {
                    "sent": "As I said, this signal is between zero and one, which is kind of the decay of.",
                    "label": 0
                },
                {
                    "sent": "Seeing news pieces in this suit account for.",
                    "label": 0
                },
                {
                    "sent": "Phone.",
                    "label": 0
                },
                {
                    "sent": "For seeing you.",
                    "label": 0
                },
                {
                    "sent": "News pieces.",
                    "label": 0
                },
                {
                    "sent": "So the predictive distribution.",
                    "label": 0
                },
                {
                    "sent": "So let's say that we have.",
                    "label": 0
                },
                {
                    "sent": "What is the probability of seeing and you count vector M. And that's very simple.",
                    "label": 0
                },
                {
                    "sent": "We can simply write down.",
                    "label": 0
                },
                {
                    "sent": "Bayes theorem here and then integrate over distribution.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 1
                },
                {
                    "sent": "OK, so the question is of course what should I use here for the prior distribution?",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "What I recent there was, I said simply, we have so much data.",
                    "label": 0
                },
                {
                    "sent": "We have millions of these tags in some cases.",
                    "label": 0
                },
                {
                    "sent": "So so actually it doesn't really matter because this.",
                    "label": 1
                },
                {
                    "sent": "Likelihood here would be pretty.",
                    "label": 0
                },
                {
                    "sent": "Repeat so so we can actually just take a flat prior here, but you can always question that.",
                    "label": 0
                },
                {
                    "sent": "That was just my very.",
                    "label": 0
                },
                {
                    "sent": "Innocent thing I I.",
                    "label": 0
                },
                {
                    "sent": "A lot of this I've taken from some other statisticians which have written a series of nice papers on this.",
                    "label": 0
                },
                {
                    "sent": "And they actually use.",
                    "label": 0
                },
                {
                    "sent": "Maybe it be surpri or this where they kind of use.",
                    "label": 0
                },
                {
                    "sent": "They use a little bit of the data to fit it, so I think that's not so, but it doesn't really give a big difference, I think.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so first the first point which will kind of yes if we had some more energy, we would take our hands.",
                    "label": 0
                },
                {
                    "sent": "Is that actually averaging works so these two datasets I've shown you explain the example, you can see you had two very different situations.",
                    "label": 0
                },
                {
                    "sent": "So let's start with this kind of most move datasets first, if I just kind of plot the.",
                    "label": 0
                },
                {
                    "sent": "They look like they would come to us.",
                    "label": 0
                },
                {
                    "sent": "Of this problem you can see that it's the maximum likelihood values here and then you have this kind of these kind of crunches here, and it's pretty peak.",
                    "label": 0
                },
                {
                    "sent": "I mean you can see there's some variation.",
                    "label": 0
                },
                {
                    "sent": "Of the parameters, but basically it's quite peaked and probably if I hadn't used to keep sampling to get.",
                    "label": 0
                },
                {
                    "sent": "To get kind of different parameters to matsson, it would have made not such a big difference compared to just using the maximum likelihood.",
                    "label": 0
                },
                {
                    "sent": "But for this other data set, you can see there's something going on here, because the maximum likelihood estimate is actually down here, very close to zero.",
                    "label": 0
                },
                {
                    "sent": "It's like 10 to the minus 7.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "And remember that this parameter is.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Is this?",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty important for predictions.",
                    "label": 0
                },
                {
                    "sent": "Right, because it goes into here.",
                    "label": 0
                },
                {
                    "sent": "Back here, right so?",
                    "label": 0
                },
                {
                    "sent": "Chris so when Sigma Zero this is basically yes yes yes yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, you can see here this.",
                    "label": 0
                },
                {
                    "sent": "If this signal is very small then you can see you get a really different behavior.",
                    "label": 0
                },
                {
                    "sent": "So it makes a big difference whether it's 10 to the minus.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Even or 10 to the minus three.",
                    "label": 0
                },
                {
                    "sent": "And you can see when I do the Gibbs sampling.",
                    "label": 0
                },
                {
                    "sent": "Of course I get some small values, but I also get something out here.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "So we're ignoring the uncertainty and the parameters would give completely different.",
                    "label": 0
                },
                {
                    "sent": "Predictions and you can also see if some of the kind of more frequency statistics approaches to this.",
                    "label": 0
                },
                {
                    "sent": "They actually say states sometimes that there's not information in the data sets to say anything about.",
                    "label": 0
                },
                {
                    "sent": "Future samples, and I think the problem is.",
                    "label": 0
                },
                {
                    "sent": "Kind of singularity.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "But the.",
                    "label": 0
                },
                {
                    "sent": "We can actually just run this, and this is kind of the example, so this is.",
                    "label": 0
                },
                {
                    "sent": "And here is the number total number of text we have.",
                    "label": 0
                },
                {
                    "sent": "So this is the actual.",
                    "label": 0
                },
                {
                    "sent": "The actual values so OK, let's focus on this top one first.",
                    "label": 0
                },
                {
                    "sent": "So this is the actual values we have and this is kind of a prediction into the future.",
                    "label": 0
                },
                {
                    "sent": "Simply running my keep sampling and running my running the sampling formula.",
                    "label": 0
                },
                {
                    "sent": "Forwards so another thing I would also say here is that I've done something slightly over like it's not really non patient, but I have also done subsampling.",
                    "label": 0
                },
                {
                    "sent": "So I've just.",
                    "label": 0
                },
                {
                    "sent": "Subsample out, replacement down to half the size, the actual size I have, and then I want to predict.",
                    "label": 0
                },
                {
                    "sent": "What is the?",
                    "label": 0
                },
                {
                    "sent": "What is the?",
                    "label": 0
                },
                {
                    "sent": "How many text do I get if I run up to the to this in?",
                    "label": 0
                },
                {
                    "sent": "I actually happen?",
                    "label": 0
                },
                {
                    "sent": "This is kind of this is a blown up of this now you can see there's something fun here because.",
                    "label": 0
                },
                {
                    "sent": "I actually miss the true value and you can see I not only miss the true value, can also see that my error pass.",
                    "label": 0
                },
                {
                    "sent": "Kind of the variation I get from two things.",
                    "label": 0
                },
                {
                    "sent": "I get the variation from the variations in the sampling formula and I also get the variation.",
                    "label": 0
                },
                {
                    "sent": "From the kind of the the averaging with parameters, it gives far too small error bars, right?",
                    "label": 0
                },
                {
                    "sent": "So that I was not so happy when I saw this.",
                    "label": 0
                },
                {
                    "sent": "And actually, I wrote down to this nice statisticians in Italy and they said that they could.",
                    "label": 0
                },
                {
                    "sent": "They didn't believe this because they had kind of calculated it and they thought that the Airbus.",
                    "label": 0
                },
                {
                    "sent": "Would go like some power of in.",
                    "label": 0
                },
                {
                    "sent": "And after a few weeks, the road had written another paper where they had actually calculated this and they said this is basically correct, right?",
                    "label": 0
                },
                {
                    "sent": "So you can see here just looking at this data.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's wrong.",
                    "label": 0
                },
                {
                    "sent": "It's clearly wrong, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, I think I didn't even have to do cross validation to see this is not pretty stick that you have so narrow error bars.",
                    "label": 0
                },
                {
                    "sent": "And what is going on here is that.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of data that's.",
                    "label": 0
                },
                {
                    "sent": "So that was of course exposed that the model is wrong.",
                    "label": 0
                },
                {
                    "sent": "But I mean it's still a very useful model because it gives.",
                    "label": 0
                },
                {
                    "sent": "I think this this prediction sign right ballpark.",
                    "label": 0
                },
                {
                    "sent": "I'm pretty sure about that.",
                    "label": 0
                },
                {
                    "sent": "OK, so the other curves here on the slide is where I can also put a different threshold.",
                    "label": 0
                },
                {
                    "sent": "Here I put a threshold saying I will only count things that have at least two tags as a new species and now you can see it starts to become a little bit more well behaved in here I've said I only will count things that have at least 30.",
                    "label": 0
                },
                {
                    "sent": "30 text as a new species and then you can see it's right on.",
                    "label": 0
                },
                {
                    "sent": "So of course it has something to do with kind of uncertainty in the in the really low count tags.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the other data set.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So it's pretty far fetched.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that was almost the conclusion of the first path.",
                    "label": 0
                },
                {
                    "sent": "You still not very energetic.",
                    "label": 0
                },
                {
                    "sent": "I don't know what I can do.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry, so parameter averaging works.",
                    "label": 1
                },
                {
                    "sent": "Yes, OK, the model is always wrong.",
                    "label": 1
                },
                {
                    "sent": "Yeah, well we knew that.",
                    "label": 0
                },
                {
                    "sent": "And this is actually revealed and they have very much data.",
                    "label": 0
                },
                {
                    "sent": "Also this nice paper I'm referring to all the time.",
                    "label": 0
                },
                {
                    "sent": "I should have given the the reference they show it works very nice, but they typically work on datasets will only 1000 or at most 10,000 tags.",
                    "label": 0
                },
                {
                    "sent": "Better show you know that the patient confidence intervals 95% of the time when you do subsampling.",
                    "label": 0
                },
                {
                    "sent": "You get with it.",
                    "label": 0
                },
                {
                    "sent": "I mean yeah, 95% of the subsampled things gets into within the hour parts and so on so it works.",
                    "label": 0
                },
                {
                    "sent": "Small data set.",
                    "label": 0
                },
                {
                    "sent": "You can also see.",
                    "label": 0
                },
                {
                    "sent": "Another thing is I only considered one model and it's.",
                    "label": 0
                },
                {
                    "sent": "I would really like that somebody in the audience told me OK, you actually using it much to a simple model.",
                    "label": 0
                },
                {
                    "sent": "You should do Iraq.",
                    "label": 0
                },
                {
                    "sent": "Some kind of hierarchical approach.",
                    "label": 0
                },
                {
                    "sent": "So I would say yes, thank you.",
                    "label": 0
                },
                {
                    "sent": "I mean that's then I can update my.",
                    "label": 0
                },
                {
                    "sent": "Prior or models right?",
                    "label": 0
                },
                {
                    "sent": "So I would yeah please do that if you can.",
                    "label": 1
                },
                {
                    "sent": "So, but if I had two models then I would like to do what the second part of the talk is about.",
                    "label": 0
                },
                {
                    "sent": "Being patient about model selection, right?",
                    "label": 0
                },
                {
                    "sent": "Because then I would really compare these two.",
                    "label": 0
                },
                {
                    "sent": "So maybe if I have time, I would also like to try any radical model which I could compare to this model with a flat prior and see.",
                    "label": 0
                },
                {
                    "sent": "Which one has had the best?",
                    "label": 0
                },
                {
                    "sent": "Had the best marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "OK, any comments on this?",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "The message is the message here that that's affecting your has.",
                    "label": 0
                },
                {
                    "sent": "Underestimate the variance for the small small.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good job on the.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I think these error bars are pretty narrow in any case.",
                    "label": 0
                },
                {
                    "sent": "So I think it probably.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I just wish somebody would tell me how can I get bigger class right?",
                    "label": 0
                },
                {
                    "sent": "So that's that echo message.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That I'm a little bit lost here.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "What you're doing here, and with respect to this previously holding back some data and then you know looking forward to see what your model predicts and and seeing how that compares.",
                    "label": 0
                },
                {
                    "sent": "That's the example of this sort of within model model.",
                    "label": 0
                },
                {
                    "sent": "Checking it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, sure no, I'm not against that at all.",
                    "label": 0
                },
                {
                    "sent": "So I'm with you, Chris.",
                    "label": 0
                },
                {
                    "sent": "I'm with you.",
                    "label": 0
                },
                {
                    "sent": "I will you, but I mean I mean.",
                    "label": 0
                },
                {
                    "sent": "But my point was simply I didn't need to do that here because I could just look at the predictions and see that the error bars are just.",
                    "label": 0
                },
                {
                    "sent": "They are counter intuitive, but of course I mean.",
                    "label": 0
                },
                {
                    "sent": "Example of war.",
                    "label": 0
                },
                {
                    "sent": "You should do, maybe not the idea of holding outdated.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Saying is it.",
                    "label": 0
                },
                {
                    "sent": "But you would get basically exactly what they would suggest you.",
                    "label": 0
                },
                {
                    "sent": "How many chosen this subset of data and then project forward to your modeling people.",
                    "label": 0
                },
                {
                    "sent": "It will predict and calculate some summaries statistic which.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I could calculate it, yeah?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I like it.",
                    "label": 0
                },
                {
                    "sent": "I mean I don't mind and I cannot.",
                    "label": 0
                },
                {
                    "sent": "I mean, I I I, I'm not in the church when I do this with other people so I have to.",
                    "label": 0
                },
                {
                    "sent": "They would ask me to do cross validation all the time.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "But I mean maybe a point.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really help me to find a better model that.",
                    "label": 0
                },
                {
                    "sent": "But hopefully it helps us to talk with people like you.",
                    "label": 0
                },
                {
                    "sent": "Because.",
                    "label": 0
                },
                {
                    "sent": "Whatever fancies.",
                    "label": 0
                },
                {
                    "sent": "Gross.",
                    "label": 0
                },
                {
                    "sent": "Presumably if you talk to the balance about these investments, they give you some immensely complicated story right about.",
                    "label": 0
                },
                {
                    "sent": "You know how, yeah?",
                    "label": 0
                },
                {
                    "sent": "But then I have a hammer to hit them with.",
                    "label": 0
                },
                {
                    "sent": "And that is the exchangeability, right?",
                    "label": 0
                },
                {
                    "sent": "I can say, but I mean you can come up with other models like this, But then not exchangeable.",
                    "label": 0
                },
                {
                    "sent": "No, but I think I don't know.",
                    "label": 0
                },
                {
                    "sent": "No matter.",
                    "label": 0
                },
                {
                    "sent": "I mean, I could say maybe there's some noise in the process, like amplification.",
                    "label": 0
                },
                {
                    "sent": "Will kind of create.",
                    "label": 0
                },
                {
                    "sent": "Like the different things, there's also sequencing errors, so you get kind of a lot of random things on the genome, and I think also there's a lot of ways to try to clean up this data, but I mean that's.",
                    "label": 0
                },
                {
                    "sent": "Liver cheating, right?",
                    "label": 0
                },
                {
                    "sent": "Because yeah.",
                    "label": 0
                },
                {
                    "sent": "Are you saying that the OK so you're saying that the model doesn't fit the data right?",
                    "label": 0
                },
                {
                    "sent": "Yeah, but you're saying that it's the most general model which is expensed exchangeable.",
                    "label": 0
                },
                {
                    "sent": "It's not a proof of the data.",
                    "label": 0
                },
                {
                    "sent": "Is not exchanged.",
                    "label": 0
                },
                {
                    "sent": "Did you say?",
                    "label": 0
                },
                {
                    "sent": "You mean that the null hypothesis that the data is exchangeable?",
                    "label": 0
                },
                {
                    "sent": "That's been the rejected having these things?",
                    "label": 0
                },
                {
                    "sent": "The DNA button you just say I'll simplify that, like taking discount right and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but I think the biologist.",
                    "label": 0
                },
                {
                    "sent": "I mean, I think that the whole setup of recording this data is it's meant to be exchangeable, right?",
                    "label": 0
                },
                {
                    "sent": "No, but I mean it's you you're trying to take independent samples.",
                    "label": 0
                },
                {
                    "sent": "From the genome.",
                    "label": 0
                },
                {
                    "sent": "Which is proportional to how much actually this specific start side is used, that is, that is kind of the idea, right?",
                    "label": 0
                },
                {
                    "sent": "So this is like a random sample.",
                    "label": 0
                },
                {
                    "sent": "Are you saying you need to be more than that process?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Really.",
                    "label": 0
                },
                {
                    "sent": "But not according to the experimental procedures that they use.",
                    "label": 0
                },
                {
                    "sent": "To get rid of things like sequencing errors in these kind of, get rid of all that experimental stuff.",
                    "label": 0
                },
                {
                    "sent": "Taken real genome.",
                    "label": 0
                },
                {
                    "sent": "Yeah, do some experiment where you sample DNA and see what they look like.",
                    "label": 0
                },
                {
                    "sent": "That's a good point.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I think there's also some issues about that.",
                    "label": 0
                },
                {
                    "sent": "You have something that Maps back to the team in multiple places, right?",
                    "label": 0
                },
                {
                    "sent": "And maybe that has been.",
                    "label": 0
                },
                {
                    "sent": "Actually removed in this data set.",
                    "label": 0
                },
                {
                    "sent": "Maybe that could affect.",
                    "label": 0
                },
                {
                    "sent": "But I mean call you almost did like rejected a null hypothesis here based on a single model.",
                    "label": 0
                },
                {
                    "sent": "I almost heard you say that.",
                    "label": 0
                },
                {
                    "sent": "Sorry, sorry card, I'm just.",
                    "label": 0
                },
                {
                    "sent": "I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm actually going to use it and then I'm going to kind of make a disclaimer saying it's not completely precise, but the results are in the right ballpark, so it seems like that some of the results are useful.",
                    "label": 0
                },
                {
                    "sent": "Purposes yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, and you don't want to replace it by a more complicated model, which would run for ages and the broader Arab are actually.",
                    "label": 0
                },
                {
                    "sent": "You don't want to do that if if some if it's if somebody can give me an easy to implement model of course too that would be just happy to have sort of the model error.",
                    "label": 0
                },
                {
                    "sent": "More realistic measure for them, I would rather have a having model ibbitson model.",
                    "label": 0
                },
                {
                    "sent": "I mean it's.",
                    "label": 0
                },
                {
                    "sent": "Anyway.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "But I mean I should also say that there's some some statistics you can get out of this thing, which is more, maybe more more interesting than this.",
                    "label": 0
                },
                {
                    "sent": "And that is what is called the coverage, which is how much of the complete probability.",
                    "label": 0
                },
                {
                    "sent": "You have seen, right?",
                    "label": 0
                },
                {
                    "sent": "Because of course.",
                    "label": 0
                },
                {
                    "sent": "I mean, a lot of this.",
                    "label": 0
                },
                {
                    "sent": "New jeans I record here that they only have very few tags, so that means they're not used very much.",
                    "label": 0
                },
                {
                    "sent": "I mean, maybe that's not so interesting.",
                    "label": 0
                },
                {
                    "sent": "It's more interesting to know.",
                    "label": 0
                },
                {
                    "sent": "How much kind of?",
                    "label": 0
                },
                {
                    "sent": "Completes the complete usage of.",
                    "label": 0
                },
                {
                    "sent": "Of genes have I seen so far so that might be 95% here and that is a more robust statistic than the.",
                    "label": 0
                },
                {
                    "sent": "Predicting the number of species.",
                    "label": 0
                },
                {
                    "sent": "OK, but I think I have to speed up a little bit so I.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's completely to another problem.",
                    "label": 0
                },
                {
                    "sent": "Very nice problem.",
                    "label": 0
                },
                {
                    "sent": "You can state it in one line.",
                    "label": 0
                },
                {
                    "sent": "You have want to calculate this integral.",
                    "label": 0
                },
                {
                    "sent": "Right, and we also like to use model which is kind of informed which as we build in biology or whatever weather.",
                    "label": 0
                },
                {
                    "sent": "And we don't want to kind of.",
                    "label": 0
                },
                {
                    "sent": "We don't want to be so restrictive about the models we use.",
                    "label": 0
                },
                {
                    "sent": "'cause we believe in putting in prior knowledge, right?",
                    "label": 0
                },
                {
                    "sent": "So we should have kind of a general purpose.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Machine here we could use have a nice black box.",
                    "label": 0
                },
                {
                    "sent": "Actually.",
                    "label": 0
                },
                {
                    "sent": "This is why we want to have a black box to calculate this integral.",
                    "label": 0
                },
                {
                    "sent": "That would, I think, that would be.",
                    "label": 0
                },
                {
                    "sent": "Actually, I think the only thing that could convert me to being non patient is that I actually give up on doing this.",
                    "label": 0
                },
                {
                    "sent": "Too high enough precision, so so I mean that I simply have to spend too much time.",
                    "label": 0
                },
                {
                    "sent": "I'm trying to do this.",
                    "label": 0
                },
                {
                    "sent": "Without them in so that I mean there all the support vector machine guys, all the frequencies they have gone unsolved, more much more interesting problems while I'm still waiting for my sampler.",
                    "label": 0
                },
                {
                    "sent": "Finishing it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in this yeah no.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a bias in the problems they want to solve, right?",
                    "label": 0
                },
                {
                    "sent": "Yes, OK, so let's talk about approximate inference Montecarlo problems with slow mixing, so we have all the nice theorems, but we know many times in practice we have to wait, wait far too long and also it's nontrivial to get to.",
                    "label": 0
                },
                {
                    "sent": "The marginal likelihood is much easier to get an average or some parameter or something like that.",
                    "label": 1
                },
                {
                    "sent": "If you go to the deterministic methods then we have expectation propagation is popular.",
                    "label": 0
                },
                {
                    "sent": "For example, for caution processes or variational Bayes.",
                    "label": 0
                },
                {
                    "sent": "So loop plus these methods are sometimes precise, but you know we cannot control.",
                    "label": 0
                },
                {
                    "sent": "It's not built in that we can say something about the approximation errors we have.",
                    "label": 0
                },
                {
                    "sent": "And also I think that there may be more restricted than the Montecarlo, right?",
                    "label": 0
                },
                {
                    "sent": "In this in this respect that we.",
                    "label": 0
                },
                {
                    "sent": "We for example when we do hierarchical models, we have to have this conjugate families.",
                    "label": 0
                },
                {
                    "sent": "And yeah.",
                    "label": 0
                },
                {
                    "sent": "In general, also things like VP, which is kind of the most general applicable method, it kind of underestimate uncertainties.",
                    "label": 0
                },
                {
                    "sent": "To degree where where it becomes useless in some applications.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what is the motivation for trying to find very precise how much time do I lift?",
                    "label": 0
                },
                {
                    "sent": "OK, that's fine.",
                    "label": 0
                },
                {
                    "sent": "How the motivation for doing this?",
                    "label": 0
                },
                {
                    "sent": "Is that we?",
                    "label": 0
                },
                {
                    "sent": "We is manfreda.",
                    "label": 0
                },
                {
                    "sent": "Norwich has me has come up with.",
                    "label": 0
                },
                {
                    "sent": "A way to make perturbation corrections.",
                    "label": 0
                },
                {
                    "sent": "To the two EP.",
                    "label": 0
                },
                {
                    "sent": "So the idea there is that we can actually.",
                    "label": 0
                },
                {
                    "sent": "We can actually run EP to convergence and then we can calculate with the statistics of the EP solution, we can calculate a correction and this is now what is.",
                    "label": 0
                },
                {
                    "sent": "This plots are not very big, but this is here.",
                    "label": 0
                },
                {
                    "sent": "We have taken the Coosa Rasmussen 2006 Yamaha setup where we have a pretty big data set.",
                    "label": 0
                },
                {
                    "sent": "From under car list of tickets.",
                    "label": 0
                },
                {
                    "sent": "From the USPS, and then we have a kernel function.",
                    "label": 0
                },
                {
                    "sent": "And profit the classification and we have these two parameters, so we're looking at a grid.",
                    "label": 0
                },
                {
                    "sent": "It's not so clear, but this is the lock.",
                    "label": 0
                },
                {
                    "sent": "Length scale and this is the lock, kind of.",
                    "label": 0
                },
                {
                    "sent": "Kind of strength prefactor of the kernel.",
                    "label": 0
                },
                {
                    "sent": "And in this in this space.",
                    "label": 0
                },
                {
                    "sent": "Parameters we want to calculate the kind of our correction term.",
                    "label": 0
                },
                {
                    "sent": "And this is the difference between the lock.",
                    "label": 0
                },
                {
                    "sent": "Normalize of EP and then the correction.",
                    "label": 0
                },
                {
                    "sent": "And you can see what you cannot see, but what I can see here is that the corrections are kind of pretty smooth and they're pretty small, right?",
                    "label": 0
                },
                {
                    "sent": "OK, so let's try to do some under Carlo and there already question resolution that did that and if we kind of try to subtract.",
                    "label": 0
                },
                {
                    "sent": "Then MCMC, so subtract the EP from the MCMC.",
                    "label": 0
                },
                {
                    "sent": "We get this picture which actually.",
                    "label": 0
                },
                {
                    "sent": "Is pretty precise.",
                    "label": 0
                },
                {
                    "sent": "If you look at the scales here, you can see that this white stuff is kind of almost 0 error, so it's pretty precise.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "Also took a pretty long time to run.",
                    "label": 0
                },
                {
                    "sent": "It was, yeah, I'll come back to what that is.",
                    "label": 0
                },
                {
                    "sent": "OK, this is our best bet and I will not tell you how we did that right now.",
                    "label": 0
                },
                {
                    "sent": "But you can see that this wide area up here, which is not cause the correctly predicted correction to be 0 simply because we cannot run our method up here because it's.",
                    "label": 0
                },
                {
                    "sent": "Two non caution.",
                    "label": 0
                },
                {
                    "sent": "It's an interesting region, so we also really.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we want to do this.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's talk about some methods to doing this.",
                    "label": 0
                },
                {
                    "sent": "I mean, the kind of vanilla idea is just to do.",
                    "label": 0
                },
                {
                    "sent": "Important sampling, so that means that we rewrite our matching likelihood.",
                    "label": 0
                },
                {
                    "sent": "By introducing this.",
                    "label": 0
                },
                {
                    "sent": "Q distribution and we divide by T as well.",
                    "label": 0
                },
                {
                    "sent": "And then if we have.",
                    "label": 0
                },
                {
                    "sent": "We run a sampler sampling from this distribution.",
                    "label": 0
                },
                {
                    "sent": "Then we can.",
                    "label": 0
                },
                {
                    "sent": "We can actually write an approximation to the marginal likelihood simply as.",
                    "label": 0
                },
                {
                    "sent": "Is this ratio evaluated in all the samples?",
                    "label": 0
                },
                {
                    "sent": "This has a really bad reputation becausw.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "So called important weights here.",
                    "label": 0
                },
                {
                    "sent": "They tend to worry.",
                    "label": 0
                },
                {
                    "sent": "Very much right?",
                    "label": 0
                },
                {
                    "sent": "So I mean if maybe if you run.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Sample then there's only actually only one of them, which.",
                    "label": 0
                },
                {
                    "sent": "Will be.",
                    "label": 0
                },
                {
                    "sent": "Kind of dominating in this.",
                    "label": 0
                },
                {
                    "sent": "This song.",
                    "label": 0
                },
                {
                    "sent": "So it doesn't really work right, because you can also see that if these two are not really well matched, then it's.",
                    "label": 0
                },
                {
                    "sent": "It's it's difficult.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, so better ideas which is.",
                    "label": 0
                },
                {
                    "sent": "Which I know you guys use is.",
                    "label": 0
                },
                {
                    "sent": "Is what is called thermodynamic integration and there's many variants of this.",
                    "label": 1
                },
                {
                    "sent": "There's parallel tempering, simulated tempering and annealed importance sampling.",
                    "label": 1
                },
                {
                    "sent": "The basic idea is that we kind of introduce kind of an interpolating distribution, right?",
                    "label": 0
                },
                {
                    "sent": "So if you call.",
                    "label": 0
                },
                {
                    "sent": "The likelihood times the prior H. Then we can write a new distribution which is conditioned on this tempering parameter.",
                    "label": 0
                },
                {
                    "sent": "As this.",
                    "label": 0
                },
                {
                    "sent": "This term here to the power beat and then a Q distribution to the power 1 minus beta.",
                    "label": 0
                },
                {
                    "sent": "Then we can actually now write the difference between.",
                    "label": 0
                },
                {
                    "sent": "And normalized, evaluated in two different inverse temperatures.",
                    "label": 0
                },
                {
                    "sent": "This is kind of where the tempering work from this comes in.",
                    "label": 0
                },
                {
                    "sent": "It has this integral.",
                    "label": 0
                },
                {
                    "sent": "Over inverse temperatures and then.",
                    "label": 0
                },
                {
                    "sent": "The derivative of the.",
                    "label": 0
                },
                {
                    "sent": "After lock partition function.",
                    "label": 0
                },
                {
                    "sent": "Right, this is just I mean going.",
                    "label": 0
                },
                {
                    "sent": "From here to here is of course real because you.",
                    "label": 0
                },
                {
                    "sent": "You can integrate derivative like this, but you can also go the other way and actually calculate this and then it turns out that you can write this.",
                    "label": 0
                },
                {
                    "sent": "Quantitie here as as an average over this distribution and what you have to average is the lock.",
                    "label": 0
                },
                {
                    "sent": "Of this ratio between these two.",
                    "label": 0
                },
                {
                    "sent": "Qantas is here.",
                    "label": 0
                },
                {
                    "sent": "And I think this is if you compare this to important family.",
                    "label": 0
                },
                {
                    "sent": "This is of course nice in the way that.",
                    "label": 0
                },
                {
                    "sent": "I mean locks are much more well behaved than actual.",
                    "label": 0
                },
                {
                    "sent": "Actually, Rachel's but the downside is that we actually had to.",
                    "label": 0
                },
                {
                    "sent": "Do an integral here right so we have to kind of.",
                    "label": 0
                },
                {
                    "sent": "We have an extra integral that we have to do.",
                    "label": 0
                },
                {
                    "sent": "Yes, but let's say now that we have.",
                    "label": 0
                },
                {
                    "sent": "We can actually calculate this.",
                    "label": 0
                },
                {
                    "sent": "Partition function for some value of beta.",
                    "label": 0
                },
                {
                    "sent": "We could for example.",
                    "label": 0
                },
                {
                    "sent": "Should be to want to be equal to 20.",
                    "label": 0
                },
                {
                    "sent": "Then you can see that this reduces to this Q distribution, which we hopefully know the normalizer off.",
                    "label": 0
                },
                {
                    "sent": "So that means that maybe.",
                    "label": 0
                },
                {
                    "sent": "Maybe if this is already normalized, then this is actually one, so we can actually calculate.",
                    "label": 0
                },
                {
                    "sent": "The marginal likelihood.",
                    "label": 0
                },
                {
                    "sent": "By doing this.",
                    "label": 0
                },
                {
                    "sent": "Let's say we have just make equal spacing.",
                    "label": 0
                },
                {
                    "sent": "This is very primitive kind of interpolator integration.",
                    "label": 1
                },
                {
                    "sent": "Then we can simply say if we run end beats and pizza chains at equidistant.",
                    "label": 0
                },
                {
                    "sent": "Values here, then we can approximate the marginal likelihood with this expression, right?",
                    "label": 0
                },
                {
                    "sent": "So you can see that we actually have.",
                    "label": 0
                },
                {
                    "sent": "To draw samples from.",
                    "label": 0
                },
                {
                    "sent": "All these change and evaluate this ratio here.",
                    "label": 0
                },
                {
                    "sent": "I should just make one command.",
                    "label": 0
                },
                {
                    "sent": "Is that this kind of in physics there's a method called multi Canonical where you go away from.",
                    "label": 0
                },
                {
                    "sent": "From sampling from this distribution here.",
                    "label": 0
                },
                {
                    "sent": "Which might actually work better.",
                    "label": 0
                },
                {
                    "sent": "Much better than all this.",
                    "label": 0
                },
                {
                    "sent": "But I mean, I haven't really any practical experience with it.",
                    "label": 0
                },
                {
                    "sent": "I hope one day I will have time to.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do that.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But I mean you can.",
                    "label": 0
                },
                {
                    "sent": "Still you can see, we still have to be able to sample from a distribution here, right?",
                    "label": 0
                },
                {
                    "sent": "So we have to find some good old samples that could do this.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I kind of like that.",
                    "label": 0
                },
                {
                    "sent": "I kind of like the now.",
                    "label": 0
                },
                {
                    "sent": "I kind of like Gibbs sampling.",
                    "label": 0
                },
                {
                    "sent": "I don't know if I like it after.",
                    "label": 0
                },
                {
                    "sent": "After this experience, I like it anymore, but I mean, let's discuss Gibbs sampling a little bit, yes.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Very nice comparison.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Would work.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's good.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I should look into that.",
                    "label": 0
                },
                {
                    "sent": "Before I use all my time, yes.",
                    "label": 0
                },
                {
                    "sent": "So basically I mean, did you find that multi Canonical in general was the best?",
                    "label": 0
                },
                {
                    "sent": "General.",
                    "label": 0
                },
                {
                    "sent": "Examples where.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I mean, I mean, if you kind of if you if you read the physics Ledger, it's clear that.",
                    "label": 0
                },
                {
                    "sent": "Kind of tempering ideas would sometimes not work becausw.",
                    "label": 0
                },
                {
                    "sent": "Becausw this distribution.",
                    "label": 0
                },
                {
                    "sent": "Kind of changes very much if you change pizza just a little bit, right?",
                    "label": 0
                },
                {
                    "sent": "So that's kind of.",
                    "label": 0
                },
                {
                    "sent": "It can.",
                    "label": 0
                },
                {
                    "sent": "It can kind of the relevant parameter settings can change.",
                    "label": 0
                },
                {
                    "sent": "Very abruptly will the small change some pizza, so that's.",
                    "label": 0
                },
                {
                    "sent": "Makes it hard to.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Do this inspiration.",
                    "label": 0
                },
                {
                    "sent": "And multi Canonical kind of avoids tries to avoid that problem.",
                    "label": 0
                },
                {
                    "sent": "OK, but let's turn to this just the basic sampling problem.",
                    "label": 0
                },
                {
                    "sent": "So here I have a little bit in mind that we're going to sample or caution process.",
                    "label": 0
                },
                {
                    "sent": "So Gibbs sampling is very nice in the way that we cycle over the conditionals, and then we sample the conditional.",
                    "label": 1
                },
                {
                    "sent": "So I've tried to draw this.",
                    "label": 0
                },
                {
                    "sent": "To do this for just.",
                    "label": 0
                },
                {
                    "sent": "And normal distribution in 2D.",
                    "label": 0
                },
                {
                    "sent": "And here you can see the problem is that we we followed the coordinates, so that means that if this is very narrow then our kind of random walking from one end of this.",
                    "label": 0
                },
                {
                    "sent": "Distribution to the other will take a long time.",
                    "label": 0
                },
                {
                    "sent": "You can also quantify this right because you can kind of say what is the typical steps I should take.",
                    "label": 0
                },
                {
                    "sent": "It would be kind of proportional to the.",
                    "label": 0
                },
                {
                    "sent": "To the standard deviation.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Along.",
                    "label": 0
                },
                {
                    "sent": "The different directions so we can of course make a trivial curve when we sample and on.",
                    "label": 0
                },
                {
                    "sent": "A normal distribution, that is that we instead of sampling.",
                    "label": 0
                },
                {
                    "sent": "Instead of sampling our F here, we sample a different distribution, see which is just a.",
                    "label": 0
                },
                {
                    "sent": "Completely independent components and then we make a linear transformation.",
                    "label": 0
                },
                {
                    "sent": "Of C, so we get the F so you can see I've done this here now so I take my covariance matrix and decompose that with some.",
                    "label": 0
                },
                {
                    "sent": "Decomposition for examples juliska decomposition.",
                    "label": 0
                },
                {
                    "sent": "Then I draw samples here.",
                    "label": 0
                },
                {
                    "sent": "With Gibbs sampling for C and then I transform and then I get these samples and they look like they have.",
                    "label": 0
                },
                {
                    "sent": "Have a really good.",
                    "label": 0
                },
                {
                    "sent": "Good mixing, so that seems to be a cure here.",
                    "label": 0
                },
                {
                    "sent": "Of course.",
                    "label": 0
                },
                {
                    "sent": "I mean we all know how to sample from multivariate normal.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it doesn't really matter.",
                    "label": 0
                },
                {
                    "sent": "But if we go to.",
                    "label": 0
                },
                {
                    "sent": "Gaussian process classification.",
                    "label": 0
                },
                {
                    "sent": "Then we have a similar problem.",
                    "label": 0
                },
                {
                    "sent": "So I mean the posterior distribution and caution.",
                    "label": 0
                },
                {
                    "sent": "Process classification is that we have a profit likelihood function and then we have a GP prior.",
                    "label": 0
                },
                {
                    "sent": "Here you can see now where this is this, because this tempering.",
                    "label": 0
                },
                {
                    "sent": "Parameter that I talked about.",
                    "label": 0
                },
                {
                    "sent": "Previously so it works like it works like it's one or the prefactor on the.",
                    "label": 0
                },
                {
                    "sent": "Under Colonel.",
                    "label": 0
                },
                {
                    "sent": "OK, we can go to.",
                    "label": 0
                },
                {
                    "sent": "We can go to.",
                    "label": 0
                },
                {
                    "sent": "I would like to go to a.",
                    "label": 0
                },
                {
                    "sent": "For reasons that will be told later, I would like to go to a nice reformulation so that means I will actually use.",
                    "label": 0
                },
                {
                    "sent": "Step function likelihood here instead of the profit and I could do that by writing the property like.",
                    "label": 0
                },
                {
                    "sent": "I forgot the integration.",
                    "label": 0
                },
                {
                    "sent": "There should be an integration here or this FN as an integral over.",
                    "label": 0
                },
                {
                    "sent": "A normal distribution for new latent variable, which I call if and if for noise free.",
                    "label": 0
                },
                {
                    "sent": "I can write down the joint distribution of this new latent variable, and then I can also integrate out.",
                    "label": 0
                },
                {
                    "sent": "The old?",
                    "label": 0
                },
                {
                    "sent": "If so, I get.",
                    "label": 0
                },
                {
                    "sent": "A distribution for the noise free which has this step.",
                    "label": 0
                },
                {
                    "sent": "Function here and then it has a new kernel function which is just.",
                    "label": 0
                },
                {
                    "sent": "Hey got an additional idea.",
                    "label": 0
                },
                {
                    "sent": "So now I have this one and if I want to sample from the good old F then I can.",
                    "label": 0
                },
                {
                    "sent": "Use the conditional here, which is caution.",
                    "label": 0
                },
                {
                    "sent": "To get those samples.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK, so the idea now is that I want to derive an efficient.",
                    "label": 0
                },
                {
                    "sent": "Gibbs sampler.",
                    "label": 0
                },
                {
                    "sent": "For this distribution here.",
                    "label": 0
                },
                {
                    "sent": "How do we do that?",
                    "label": 0
                },
                {
                    "sent": "Can we do that?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yes, OK, so here's some.",
                    "label": 0
                },
                {
                    "sent": "Here's a list of related work for this.",
                    "label": 0
                },
                {
                    "sent": "So the efficient Gibbs sampler is this preprint.",
                    "label": 0
                },
                {
                    "sent": "That's pretty.",
                    "label": 0
                },
                {
                    "sent": "That's where I've got the method from.",
                    "label": 0
                },
                {
                    "sent": "Will it sufficient?",
                    "label": 0
                },
                {
                    "sent": "We'll see that.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "I should say that we are also working on different ways to do.",
                    "label": 0
                },
                {
                    "sent": "And read for Neil's.",
                    "label": 0
                },
                {
                    "sent": "Also worked in different ways to sample with all regularization.",
                    "label": 0
                },
                {
                    "sent": "Another thing I thought about when I wrote down this is that in the good old days, before we were really fully patient Palfrey an invented this concept of playing billiards inversion space.",
                    "label": 0
                },
                {
                    "sent": "And when I think about bagging that that is actually.",
                    "label": 0
                },
                {
                    "sent": "Closely related, I mean to a kind of Monte Carlo method for.",
                    "label": 0
                },
                {
                    "sent": "For for the GP problem, I don't know how precise that would be.",
                    "label": 0
                },
                {
                    "sent": "I mean, it probably has some nice properties.",
                    "label": 0
                },
                {
                    "sent": "I mean also has her push has.",
                    "label": 0
                },
                {
                    "sent": "His current base point machine, but it's basically the same ideas, poorly and then call an multicluster.",
                    "label": 0
                },
                {
                    "sent": "Use hybrid Monte Carlo in this white and space, an annealed importance sampling.",
                    "label": 1
                },
                {
                    "sent": "Maybe call can give that thesis.",
                    "label": 0
                },
                {
                    "sent": "If it's important, then our we also found some work by Kian Ming Koran.",
                    "label": 0
                },
                {
                    "sent": "And a different method which.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also seem to have nice promises.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK, so let's try to see what happens if we just run a vanilla Gibbs sampler.",
                    "label": 0
                },
                {
                    "sent": "On this truncated Goshen right, so there is now that we have.",
                    "label": 0
                },
                {
                    "sent": "We have the caution from before, but now we are truncating it.",
                    "label": 0
                },
                {
                    "sent": "So just for simplicity so we only looking at positive values.",
                    "label": 0
                },
                {
                    "sent": "So basically we have the same problem as we have for sampling the normal distribution, right that we have we have slow mixing along these very.",
                    "label": 0
                },
                {
                    "sent": "Highly correlated.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Directions in space.",
                    "label": 0
                },
                {
                    "sent": "If we look at a normal distribution which goes this way so we have negative correlations, you can see it doesn't seem to have equally big problems so.",
                    "label": 0
                },
                {
                    "sent": "So this problem has very much to do with.",
                    "label": 0
                },
                {
                    "sent": "For this kind of.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Positive correlations.",
                    "label": 0
                },
                {
                    "sent": "Yes, so let's try to just apply the same idea as we solve the for the normal distribution.",
                    "label": 0
                },
                {
                    "sent": "We can just introduce these.",
                    "label": 0
                },
                {
                    "sent": "Independent variables and then we can do this salesky of the of the covariance.",
                    "label": 0
                },
                {
                    "sent": "And remember that we had now we just for simplicity.",
                    "label": 0
                },
                {
                    "sent": "We look at the constraints that just says that we have only positive labels, so all the labels should be.",
                    "label": 0
                },
                {
                    "sent": "Should be positive, so that translate because this is linear transformation.",
                    "label": 0
                },
                {
                    "sent": "This also translates into a linear constraint.",
                    "label": 0
                },
                {
                    "sent": "For the C variables, right?",
                    "label": 0
                },
                {
                    "sent": "Yes, it's just a linear transformation and also becausw.",
                    "label": 1
                },
                {
                    "sent": "This is a convex region for F. Then I mean the region we have to integrate overseas also convex region.",
                    "label": 0
                },
                {
                    "sent": "When?",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Drive OK, I'll I'll speed up OK.",
                    "label": 0
                },
                {
                    "sent": "Sorry guys OK. OK, so.",
                    "label": 0
                },
                {
                    "sent": "We want to keep sending once we want to sample from the conditional so we can write down what is this constraint for the gave conditional right?",
                    "label": 0
                },
                {
                    "sent": "So we take this basic.",
                    "label": 0
                },
                {
                    "sent": "This F greater than one constraint that becomes this constraint right?",
                    "label": 0
                },
                {
                    "sent": "And now you can see that depending on whether this Elijah is positive and negative, we.",
                    "label": 0
                },
                {
                    "sent": "Will get an upper or lower bound, right?",
                    "label": 0
                },
                {
                    "sent": "So now it's actually a totally double truncated.",
                    "label": 0
                },
                {
                    "sent": "The conditional we have a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK, this is pretty easy.",
                    "label": 0
                },
                {
                    "sent": "We just find.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Which.",
                    "label": 0
                },
                {
                    "sent": "What is the slope anniversary of a pond by looking at all?",
                    "label": 0
                },
                {
                    "sent": "All the ones in the set of positive here.",
                    "label": 0
                },
                {
                    "sent": "Time to take the maximum value here and take a minimum value here and the actual sampling is like this.",
                    "label": 0
                },
                {
                    "sent": "We let Matlab generate a random number between zero and one, and then we calculate these.",
                    "label": 0
                },
                {
                    "sent": "These error functions and then we take the inverse error function of this combination and then we have a new sample.",
                    "label": 0
                },
                {
                    "sent": "So let's have a look at how.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Works.",
                    "label": 0
                },
                {
                    "sent": "So you can see now this is.",
                    "label": 0
                },
                {
                    "sent": "In C space so you can see the constraints here.",
                    "label": 0
                },
                {
                    "sent": "If we have a very correlated thing, then the constraints becomes autumn almost.",
                    "label": 0
                },
                {
                    "sent": "Collinear and we can sample pretty freely in this space, right?",
                    "label": 0
                },
                {
                    "sent": "And we had really good.",
                    "label": 0
                },
                {
                    "sent": "It looks like we have really good mixing in the.",
                    "label": 0
                },
                {
                    "sent": "In the space, so it seems like it's solving.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem is, let's see what happens if we have negative covariance.",
                    "label": 0
                },
                {
                    "sent": "Then maybe it doesn't look so nice, right?",
                    "label": 0
                },
                {
                    "sent": "Because maybe we're back here to the same problem as we had.",
                    "label": 0
                },
                {
                    "sent": "In the original space.",
                    "label": 0
                },
                {
                    "sent": "But it seems to give.",
                    "label": 0
                },
                {
                    "sent": "If you just eyeball this, it seems to be.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty promising.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Yes, what happened now.",
                    "label": 0
                },
                {
                    "sent": "I forgot to take the right figures, but I will not show the result of running this method actually because it doesn't work so well.",
                    "label": 0
                },
                {
                    "sent": "So I would like to talk about this.",
                    "label": 0
                },
                {
                    "sent": "That actually worked well for the regional parameters we can use.",
                    "label": 0
                },
                {
                    "sent": "So this is the this is the corrections we have predicted which will appear at NIPS.",
                    "label": 0
                },
                {
                    "sent": "And you can see that pretty small, actually .5 at most 1.6 maybe.",
                    "label": 0
                },
                {
                    "sent": "And this is what we get.",
                    "label": 0
                },
                {
                    "sent": "And this is actually what we get with important sampling.",
                    "label": 0
                },
                {
                    "sent": "And we use as importance distribution reduce the EP.",
                    "label": 0
                },
                {
                    "sent": "Cube distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is an example where actually.",
                    "label": 0
                },
                {
                    "sent": "Important sampling works in very high dimensions, but not for all parameters, so I'm stealing.",
                    "label": 0
                },
                {
                    "sent": "I'm still on the lookout for something that can.",
                    "label": 0
                },
                {
                    "sent": "We're still on the lookout for something that can solve this.",
                    "label": 0
                },
                {
                    "sent": "Part of of the parameter space.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me stop now.",
                    "label": 0
                },
                {
                    "sent": "Yes, OK.",
                    "label": 0
                },
                {
                    "sent": "Yes, so I said I wrote here the trouble Gibbs sampling Anna cure.",
                    "label": 1
                },
                {
                    "sent": "I don't know if it's a cure really.",
                    "label": 0
                },
                {
                    "sent": "I think it's very unsatisfactory that you need to run.",
                    "label": 0
                },
                {
                    "sent": "The Monte Carlo for months I think that is not a very strong selling point of patient methods, so I think it's worthwhile really looking into.",
                    "label": 1
                },
                {
                    "sent": "Two better sampling methods, and I think it's you cannot just say.",
                    "label": 0
                },
                {
                    "sent": "I don't believe that you can just write a program that does Gibbs sampling for any model.",
                    "label": 0
                },
                {
                    "sent": "I think you really have to go in and try to understand the problem.",
                    "label": 0
                },
                {
                    "sent": "And assign samplers that will work for the problem.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 1
                },
                {
                    "sent": "And then I mean my last question was like what is machine learning actually?",
                    "label": 0
                },
                {
                    "sent": "And I don't know.",
                    "label": 0
                },
                {
                    "sent": "I feel like sometimes that I spent a lot of time on reading, you know, kind of patient statistics papers.",
                    "label": 0
                },
                {
                    "sent": "So maybe I'm just.",
                    "label": 0
                },
                {
                    "sent": "Becoming like an amateur patient statistician, I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't know if you have any comments on that where where machine learning is moving, I mean.",
                    "label": 0
                },
                {
                    "sent": "Yes, thank you.",
                    "label": 0
                },
                {
                    "sent": "Performing to the Noise Feedback plantation you make.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem easier by having you viginal latent function.",
                    "label": 0
                },
                {
                    "sent": "Where the shop boundary.",
                    "label": 0
                },
                {
                    "sent": "Space.",
                    "label": 0
                },
                {
                    "sent": "You might have any Department.",
                    "label": 0
                },
                {
                    "sent": "I mean is everything is this convex?",
                    "label": 0
                },
                {
                    "sent": "I mean this is such a nice playing ground, right?",
                    "label": 0
                },
                {
                    "sent": "Because it's completely convex and.",
                    "label": 0
                },
                {
                    "sent": "I like the like that hard boundaries because it's really easy conceptually to understand what you want to do.",
                    "label": 0
                },
                {
                    "sent": "So I don't know if maybe Kyle has some experience with the difference between going now.",
                    "label": 0
                },
                {
                    "sent": "You want to use the baby EP approximation as the important temple.",
                    "label": 0
                },
                {
                    "sent": "Then it would be much better to try to do that in this mood version, which is the one where you, yeah, we also do that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we also do that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, we definitely do that, yes?",
                    "label": 0
                },
                {
                    "sent": "Base simulation.",
                    "label": 0
                },
                {
                    "sent": "Trying too hard spheres tonight man.",
                    "label": 0
                },
                {
                    "sent": "So everyone who does Hospice based software yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but actually I think the point is actually that that.",
                    "label": 0
                },
                {
                    "sent": "This part of the parameter parameter space is really where it's more or less.",
                    "label": 0
                },
                {
                    "sent": "Like shop boundaries.",
                    "label": 0
                },
                {
                    "sent": "So I think we have to be able to do that.",
                    "label": 0
                },
                {
                    "sent": "Yes, I'm wondering if there's some results on sampling from complex regions and whenever polynomial I don't know much about it.",
                    "label": 0
                },
                {
                    "sent": "Does anybody know?",
                    "label": 0
                },
                {
                    "sent": "This is the version.",
                    "label": 0
                },
                {
                    "sent": "Remember.",
                    "label": 0
                },
                {
                    "sent": "I've been, I think maybe some proofs, but they don't give kind of the actual algorithm right?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "Different comment which is.",
                    "label": 0
                },
                {
                    "sent": "Take a long time.",
                    "label": 0
                },
                {
                    "sent": "He's getting the right result anyway.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "And our corrections are basically our corrections are basically giving the wrist small path.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's that's a good message, but I think that EP is this.",
                    "label": 0
                },
                {
                    "sent": "I think GPS is kind of the EP.",
                    "label": 0
                },
                {
                    "sent": "Greatest success, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, many other models where you?",
                    "label": 0
                },
                {
                    "sent": "Well, I don't really trust the results as much.",
                    "label": 0
                },
                {
                    "sent": "Why you?",
                    "label": 0
                },
                {
                    "sent": "Why can you be my selected?",
                    "label": 0
                },
                {
                    "sent": "Can you just sample over?",
                    "label": 0
                },
                {
                    "sent": "They have friends.",
                    "label": 0
                },
                {
                    "sent": "If you work.",
                    "label": 0
                },
                {
                    "sent": "Integrate.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "You're right, but I mean, I mean, I consider this as kind of two levels.",
                    "label": 0
                },
                {
                    "sent": "So if I can solve the first level of integrating over the caution process.",
                    "label": 0
                },
                {
                    "sent": "Then I can add the other level.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "Because you can also see I mean, OK, I haven't shown that.",
                    "label": 0
                },
                {
                    "sent": "But I mean this is kind of the high marginal likelihood region, right?",
                    "label": 0
                },
                {
                    "sent": "So I would be I should be able to sample that in order to really get the marginal likelihood where integrate also or the hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "No, I would definitely like to do that and I would like to take the neural network and do the same thing.",
                    "label": 0
                },
                {
                    "sent": "And then I say OK, caution processes as the higher.",
                    "label": 0
                },
                {
                    "sent": "The marginal likelihood, I mean that is my grenko.",
                    "label": 0
                },
                {
                    "sent": "Or any other model.",
                    "label": 0
                },
                {
                    "sent": "Sample.",
                    "label": 0
                },
                {
                    "sent": "I mean, I would do a joint sample in the in this parameters and in.",
                    "label": 0
                },
                {
                    "sent": "Caution process.",
                    "label": 0
                },
                {
                    "sent": "Have the same problem.",
                    "label": 0
                },
                {
                    "sent": "Still need to be.",
                    "label": 0
                },
                {
                    "sent": "Something the region of the other plans as well, yeah.",
                    "label": 0
                },
                {
                    "sent": "She would advise that he would arrive in time for ladies.",
                    "label": 0
                },
                {
                    "sent": "So you're saying you want to be able to compare different models by computing marginal life?",
                    "label": 0
                },
                {
                    "sent": "It's like normal network, yeah?",
                    "label": 0
                },
                {
                    "sent": "So I guess.",
                    "label": 0
                },
                {
                    "sent": "When I find a little difficult about that is that Marshall, like loser, play sensitive to the priors that you set up?",
                    "label": 0
                },
                {
                    "sent": "That's what I want to test.",
                    "label": 0
                },
                {
                    "sent": "I want to test my prize, yeah?",
                    "label": 0
                },
                {
                    "sent": "Neural network is actually a very good model, but.",
                    "label": 0
                },
                {
                    "sent": "With the priors on the way.",
                    "label": 0
                },
                {
                    "sent": "Things like that.",
                    "label": 0
                },
                {
                    "sent": "You integrate overload.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "OK, maybe I should have another example.",
                    "label": 0
                },
                {
                    "sent": "I mean I would like to build models for biological data where I could say for example in transcription factor exams I could say for example, are there any real time dependence between?",
                    "label": 0
                },
                {
                    "sent": "The measurements of transcription factors at different times, and then I could I could calculate my collected for model without the time dependence on one will and then I can go back to the biologist and say that.",
                    "label": 0
                },
                {
                    "sent": "I mean I have statistical support for there's a time dependence question.",
                    "label": 0
                },
                {
                    "sent": "Yes, that's the stuff I want to.",
                    "label": 0
                },
                {
                    "sent": "That's yeah, yeah, yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "I'd like to.",
                    "label": 0
                },
                {
                    "sent": "Created exception actually that we can compute.",
                    "label": 0
                },
                {
                    "sent": "Position with a method like EP.",
                    "label": 0
                },
                {
                    "sent": "Computer these results.",
                    "label": 0
                },
                {
                    "sent": "We actually spent a lot of time looking for the bug.",
                    "label": 0
                },
                {
                    "sent": "Right answers were always exactly the same.",
                    "label": 0
                },
                {
                    "sent": "Something wrong here, but it turns out that it actually doesn't compute the right hand side as stupid points out.",
                    "label": 0
                },
                {
                    "sent": "Typically what happens is that you get hard likelihood.",
                    "label": 0
                },
                {
                    "sent": "Better log scale.",
                    "label": 0
                },
                {
                    "sent": "Usually you know 10 or 20 apart or something like that.",
                    "label": 0
                },
                {
                    "sent": "Yeah, correction of .5.",
                    "label": 0
                },
                {
                    "sent": "This is basically saying you're spot on.",
                    "label": 0
                },
                {
                    "sent": "For practical purposes I agree.",
                    "label": 0
                },
                {
                    "sent": "I mean.",
                    "label": 0
                },
                {
                    "sent": "And it took us till 2006.",
                    "label": 0
                },
                {
                    "sent": "They're doing our equivalent of classification in a way that we were happy was quick.",
                    "label": 0
                },
                {
                    "sent": "And relatively efficient and it's kind of one of the things we know.",
                    "label": 0
                },
                {
                    "sent": "It come back.",
                    "label": 0
                },
                {
                    "sent": "I would really also as older really want to try and workout what the most likely referring network like.",
                    "label": 0
                },
                {
                    "sent": "I don't know how to do that.",
                    "label": 0
                },
                {
                    "sent": "Remarkable that it works, but not for profit.",
                    "label": 0
                },
                {
                    "sent": "But I think we should.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's kind of depressing that you took it so long to work that out.",
                    "label": 0
                },
                {
                    "sent": "The reason why we try to to calculate?",
                    "label": 0
                },
                {
                    "sent": "Showing.",
                    "label": 0
                },
                {
                    "sent": "The difference is small.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "OK thanks.",
                    "label": 0
                },
                {
                    "sent": "Maybe maybe it kind of a linear more I mean.",
                    "label": 0
                },
                {
                    "sent": "OK, maybe it's time to finish performing for completed takes over.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, thank you.",
                    "label": 0
                }
            ]
        }
    }
}