{
    "id": "yolkcsaxxhw5nje3zvla2p62eggz4z6r",
    "title": "Multiple-Instance Learning with Instance Selection via Dominant Sets",
    "info": {
        "author": [
            "\u0130brahim Aykut Erdem, Computer Engineering Department, Hacettepe University"
        ],
        "published": "Oct. 17, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Instance-based Learning"
        ]
    },
    "url": "http://videolectures.net/simbad2011_erdem_dominant/",
    "segmentation": [
        [
            "OK, today I will present you any instance selection strategy for multiple instance learning and which is based on Domino sets framework and it is a joint work with Mike Timber.",
            "Other error codes."
        ],
        [
            "OK, in the traditional supervised single distance learning problems, we have a training set of instances and each instance is associated with the specific labels and our aim is to discriminate between these two classes so that newly observed data comes in, we can predict its label.",
            "On the other hand, in the multiple instance learning setting.",
            "We instead have backs off labels, where in this case the backs not instances, but the bags are associated with some labels.",
            "And of course here the aim is to classify an unseen back.",
            "Until we have two basic assumptions, the first one is that a BEC is considered negative if all of its instances are negative, and Secondly.",
            "Back is positive if it contains contains at this one positive instance.",
            "So in these positive facts we indeed have some ambiguity, so we indeed have a learning problem with ambiguously labeled data."
        ],
        [
            "In the instance, selection based approaches to multiple instance learning, they transform a given multiple instance learning problem into standard learning problem just by embedding the bags into some points in some embedding space, and then train a classifier in that space.",
            "Until all the variables, metals can be differentiated off.",
            "They can perform this, forming this embedding space.",
            "Basically they select a set of representative instances from the training packs.",
            "So we do indeed have a similarity based representation.",
            "Of course there are some critical questions here, for example, which instances best model the positive and negative data?",
            "How many prototypes do we need in order to correctly represent these classes and also the robustness to outliers and labeling noise is another."
        ],
        [
            "Issue.",
            "Just to compare and contrast Availables methods, here we have four different methods.",
            "In the first one in the DDS firm, it's selects single prototype from each training back and this is based on a diverse density function which somehow measures the Co occurrences of similar instances with different bags but having to the same labels and in the classifications stage, it just uses a standard support vector machine with radial basis function kernels.",
            "But its drawback is this diverse tested function in selecting two prototypes that it is quite sensitive to the labeling noise.",
            "Two years later, she told modified their approach and in miles it stand for multiple instance learning with.",
            "Emitted instant selection.",
            "They use all the instances in the training backs in order to generate this embedding quarter feature space and instant selection process is passed to the classification stage where they use a one normal SVM.",
            "So it implies an implicit instance selection.",
            "And as you can guess, its main drawback is this how they form this embedding space and it is exponentially expensive as the volume of the training data increases.",
            "And after that, the last two methods is actually more similar to our approach, they just use the observation that there is no ambiguity in the negative training packs.",
            "All backs are composed of negative instances.",
            "So for example, in wild stay here, choose one prototype from just the positive training packs.",
            "And this is based on the conditional probability models so that they can choose the most positive instances in this back and forth classification.",
            "Again they use support vector machines with RBF kernels.",
            "Until about its throwback, we can say that they only choose positive prototypes, so there's no instances modeling to negative data in the embedding space.",
            "So as you will see in our experiments, its performance is a bit poor compared to other approaches.",
            "And plus the Mills chooses one prototype from each training back and this time it is based on for will to density function on the negative data which is based on kernel density estimation.",
            "And they use linear SVM for classification and also.",
            "They proposed an alternating instance selection and process, and the training classification cycle so that they can go back and update their initial instance instance prototypes.",
            "So in a way where we can state this throwback cost compared to 1st three approaches, their training stages of it's expensive.",
            "So in this work we also start with the same observation on the negative training packs, but compared to others we will have a clustering based approach and for the classic we will be especially using the dominant sets framework because of its advantages."
        ],
        [
            "Just to briefly talk about Dominicis framework, it is a pairwise clustering approach proposed by iwanan pale yellow.",
            "It makes no assumption on the underlying data representation, and it can detect the proper number of clusters, and it is quite robust to labeling noise out lives.",
            "And it imposes no constraint on the structural input similarity metrics that it can naturally deal with both symmetric and negative similarities.",
            "And Lastly, it can handle unseen data in appearance will play."
        ],
        [
            "More formally, dominicis can be considered as generalizations of maximal clique to edge weighted graph.",
            "Basically, domains set satisfy two properties of clusters.",
            "The first one is internal homogeneity that the instances in a cluster are highly similar to each other and Secondly.",
            "The instances outside of a cluster is are highly dissimilar to the elements of that clusters.",
            "So just to give a simple example here, the set 123 is dominant sets.",
            "As you can see the pairwise similarities between these objects are quite high.",
            "The other elements are quite dissimilar to the elements of this set."
        ],
        [
            "And computationally adamin sets can be computed as the support of of the local maximizers of this standard quadratic problem, subject to the constraint that the solution vectors should lie on a simplex.",
            "And we can state some properties of domain set framework to begin with.",
            "The value of the objective function gives us a measure of the internal coherence is of a cluster.",
            "While the components of the characteristic vectors gives us measures of the participation of the corresponding data points in.",
            "The clusters and also the similarity of unseen elements to a cluster can be direct computed by using a weighted similarity using the weights in the characteristic vectors."
        ],
        [
            "So now I can start talking about our approach.",
            "As I said earlier, our main observation is that there is no ambiguity in the negative backs, so our main assumption is that the negative instances form clusters.",
            "Of course, this assumption may not always be valid.",
            "For example, in the case of outliers or labeling noise.",
            "So to put into perspective or approach with.",
            "Other methods?",
            "We first cluster the negative data in the negative training packs and then choose a single instance from these extracted clusters and turn based on their relation to these exact clusters.",
            "We choose some instances from the positive Max.",
            "UN tiers we will make use of the dominant sets clustering framework.",
            "And for the classification we use linear support vector machine."
        ],
        [
            "Here are some basic notations.",
            "EI stand forced to beg.",
            "I wear the IG knowledge the Jade instance in back I of course in the training set the backs have some labels.",
            "And we indeed have set off training packs.",
            "Some of them are positively labeled, some have negative labels, and since we will cluster the negative data in the negative training packs, we do not collection of the negative instances.",
            "We take capital."
        ],
        [
            "Location or instance selection strategy starts with computing the pay rise similarities among these negative instances.",
            "Once we have this similarity matrix, we just extract the domino sets that is the clusters in this set.",
            "By using a feeling of strategy that we just extract a single cluster from the input data than we exclude that set and rate to rate this process until all the dominant cells have been found.",
            "Once we extract all the clusters done using the internal queries measures, we sort the clusters according to their internal coherence is and we just select.",
            "Maximum.",
            "Number of clusters Barbie.",
            "Put it upper limit based on the number of negative Max.",
            "Anti racism artificial examples here.",
            "The colors do not the labels off the backs, whereas the circus and the crosses denote which class is the instances are coming from here we have a full strength, eight negative bags.",
            "And and.",
            "Once we extracted Domino sets, we have correctly found three clusters here."
        ],
        [
            "OK, so we have the clusters from the negative training packs.",
            "Now how we extract the instance prototypes?",
            "Actually for the negative class is quite straightforward.",
            "We just select a single prototype from each cluster.",
            "Based on this formula here, not that or the clusters.",
            "The components of the characters Quaestors gives us a measure of the participation of the corresponding instances.",
            "So for example, for this simple toy examples we have just three instances to model the negative data, and as you can see here, there are quite close to the same traits of these clusters."
        ],
        [
            "However, for the positive Baxter, since there is an ambiguity there, we cannot directly use this costing based approach because it can fail if the negative instances are forming clusters here.",
            "So here we just.",
            "Compare the instances in these positive bags.",
            "Bit extracted clusters so we just choose a single prototype from each positive back and trying to actually.",
            "Finding the most positive or you know way to least negative examples in this packs.",
            "Here's the a matrix is actually the similarities between the instances in a positive back to the clusters and three.",
            "Just take here rated similarities, so indent, we just choose the most distance instant from the extracted negative clusters.",
            "So for this example, we have just eight instance prototypes and they are from the correct."
        ],
        [
            "Glasses.",
            "Unforced classification since we have now the instant prototypes you have to define an embedding function based on similarity measure.",
            "Here we use this similarity measures of head back to instance prototype which is basically based on the distance between instance prototype to its nearest neighbor in the back.",
            "So we have this embedding function based on the similarities.",
            "For the classification we just used linear as firm, so we have indeed two parameters to optimize.",
            "One is the scale parameter used involved clustering phase, and in this similarity measure here and also we have the acquisition parameters for the support."
        ],
        [
            "Permission.",
            "Not for the multiclass cases, of course.",
            "Naive way is to use one versus rest strategy where we train C number of binary classifier, see denoting the total number of crosses.",
            "One for each class against the remaining funds and the classification is performed based on the classifier with highest decision value.",
            "This strategy.",
            "We will use a different instance based embedding for each binary subproblem, so."
        ],
        [
            "We come up with a second alternative marriage.",
            "The idea is to construct an embedding space, come for all the classes.",
            "Of course you do number of instances selected for each class may differ, but for the training data we have, the training data is kept fixed for all the binary solve problems and on the labels differ.",
            "So compared to the former approach, this makes the training phase much more efficient."
        ],
        [
            "Tommy, compute this instances for each class.",
            "We simply consider the instances coming from the back send those labels and then we extract clusters.",
            "For this set.",
            "Downton Abbey again.",
            "Choose one through prototypes from each cluster based on this formula here, which is based on both the degree of the participation and the similarity to all the remaining clusters.",
            "So that's the most similar instance to other training data from other classes is chosen while it has a higher degree of participation.",
            "Frontiers for the similarity to the remaining classes, we just consider the most."
        ],
        [
            "English.",
            "Class.",
            "To test the performance of our approach, we just consider to kind of task.",
            "The first one is to use the standard benchmark datasets in mill literature, which are all two class problems and Secondly, we test on multiclass image classification problem."
        ],
        [
            "English benchmark datasets.",
            "The common setting is to perform time, timestamp, fold cross validation, and here are the results we.",
            "Obtained.",
            "By the way.",
            "Line device.",
            "Stonemark mean algorithms from the instant selection based approaches and compared to the state of the art approaches, we do indeed have.",
            "Competitive results in the in their papers in Millers they did not specify the results on the elephant focus on Tiger data.",
            "Datasets we.",
            "Implemented this algorithm by our phone and the results are a bit smaller than the reports at once."
        ],
        [
            "And for the dimensions of the embedding spaces, here are the details of these benchmark datasets, and here are the averages of the dimensions of the embedding spaces for.",
            "With this instance selection based approaches.",
            "First of all, my smiles has the highest embedding space dimension, since it uses all the instances in the embedding, whereas Smile tells the lowest dimension.",
            "But since it does not make use of the.",
            "Prototypes from the negative training facts, and as you can see in the previous table, it's performance is a bit poor compared to other approaches.",
            "Answer For our methods compared to millisecond DDS firm we have less dimension.",
            "Approximately 622 until percentage smaller than those algorithms.",
            "And of course, we can even reduce these dimensions by employing sparse linear SVM in training, spelt, or eliminating the dimensions with."
        ],
        [
            "Very small lights.",
            "Unfolded multiclass problem.",
            "We use the experimental setup using the DDS from paper.",
            "It consists of 2000 images from 2 antique categories, each having hundreds examples and it is considered as a back and these images are segment into regions and these regions are actually corresponding to the instances in this Max or.",
            "Images that each class has some distinct region of interests.",
            "Understand in their experiments they use make use of two different.",
            "Setups in the first one, they just consider the first time categories and in the second one they use all the existing categories until we test 22 possible extensions of our approach to the multiclass."
        ],
        [
            "Case.",
            "Frontier should the standard set up is to use 5 * 2 fold cross validation.",
            "And here are the results we get.",
            "Again, we obtain competitive results compared to state of the art until one for the larger data set to 2000 image datasets or second version gives the best results."
        ],
        [
            "Also, we can look into these results in a more deeply in the first approach that we use for less stress strategy, each classifier is trained to distinguish a specific category from the rest.",
            "So again, to repeat a different embedding space is built for each subproblem, but the selected the set of sex protais, Verizon every subproblem.",
            "What's here, we can say that the positive purple prototypes are mostly selected from the discriminator regions for those classes, and here are some.",
            "Results for explaining."
        ],
        [
            "That's.",
            "And for our SEC multiclass strategy, the set of selected instance prototypes is the same for all the subproblems.",
            "And in this way we actually provided rich failed to include context as well.",
            "Here we are showing just.",
            "To class is the horizontal metal ship categories.",
            "Here in both tables the ones on the left shows the actual prototypes which are coming from a specific clusters, whereas the three examples on the right shows others members in those classes.",
            "As you can see here, we can come up with different clusters corresponding to horses with different appearance properties.",
            "But also for example at the last line here we have some regions corresponding to the cross region and that similar type of explanation can be made for the battleship class as well.",
            "And in a way this.",
            "They off finding the instant prototypes resembles vocabulary generation step of the so called bag of words approach is."
        ],
        [
            "Lastly, we investigate the sensitivity to labeling noise again with the set up in the mice paper where we have a two class problem distinguishing historical buildings from the horses very artificially introduce noisy labels for each noise levels.",
            "D percent of the positive anti percent of the negative images are randomly selected and their labels are exchanged and again we performed.",
            "5 * 2 fold Cross validation here, and this is our results.",
            "What about the conclusions here at the lower levels for the cases with the noise level less than 5%, there is no constable.",
            "This difference in the performance of both algorithms, but at high levels, especially when the noise level is higher than or equal to 25%, our approach is the most robust bar.",
            "And this is expected to cause domains that is known to be quite robust to outliers."
        ],
        [
            "Hunter is a.",
            "Summary In this study, we propose a new instance selection strategy which is based on domain sets.",
            "Our approach can identify the most representative examples in both positive and negative training packs and our experimental results show that our approach is competitive with the stumps.",
            "State of the art methods.",
            "Lastly, it is quite robust to labeling noise and for future work we would like to investigate to related problems.",
            "The first one is multiple instance multi label learning problem where this case the banks have more than one labels associated with them and Secondly here we all consider samples in a bag.",
            "From IID samples.",
            "But for example, in the case for some computer vision problems like the image categorization problem I mentioned here, they do have indeed some structures in this specs, so there are a couple of works which treats the instances in the backs as long IID samples, and we would like to investigate these.",
            "Both of these problems from an instance selection based perspective."
        ],
        [
            "And thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, today I will present you any instance selection strategy for multiple instance learning and which is based on Domino sets framework and it is a joint work with Mike Timber.",
                    "label": 0
                },
                {
                    "sent": "Other error codes.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, in the traditional supervised single distance learning problems, we have a training set of instances and each instance is associated with the specific labels and our aim is to discriminate between these two classes so that newly observed data comes in, we can predict its label.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, in the multiple instance learning setting.",
                    "label": 0
                },
                {
                    "sent": "We instead have backs off labels, where in this case the backs not instances, but the bags are associated with some labels.",
                    "label": 0
                },
                {
                    "sent": "And of course here the aim is to classify an unseen back.",
                    "label": 0
                },
                {
                    "sent": "Until we have two basic assumptions, the first one is that a BEC is considered negative if all of its instances are negative, and Secondly.",
                    "label": 1
                },
                {
                    "sent": "Back is positive if it contains contains at this one positive instance.",
                    "label": 1
                },
                {
                    "sent": "So in these positive facts we indeed have some ambiguity, so we indeed have a learning problem with ambiguously labeled data.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the instance, selection based approaches to multiple instance learning, they transform a given multiple instance learning problem into standard learning problem just by embedding the bags into some points in some embedding space, and then train a classifier in that space.",
                    "label": 0
                },
                {
                    "sent": "Until all the variables, metals can be differentiated off.",
                    "label": 0
                },
                {
                    "sent": "They can perform this, forming this embedding space.",
                    "label": 0
                },
                {
                    "sent": "Basically they select a set of representative instances from the training packs.",
                    "label": 1
                },
                {
                    "sent": "So we do indeed have a similarity based representation.",
                    "label": 1
                },
                {
                    "sent": "Of course there are some critical questions here, for example, which instances best model the positive and negative data?",
                    "label": 0
                },
                {
                    "sent": "How many prototypes do we need in order to correctly represent these classes and also the robustness to outliers and labeling noise is another.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Issue.",
                    "label": 0
                },
                {
                    "sent": "Just to compare and contrast Availables methods, here we have four different methods.",
                    "label": 0
                },
                {
                    "sent": "In the first one in the DDS firm, it's selects single prototype from each training back and this is based on a diverse density function which somehow measures the Co occurrences of similar instances with different bags but having to the same labels and in the classifications stage, it just uses a standard support vector machine with radial basis function kernels.",
                    "label": 0
                },
                {
                    "sent": "But its drawback is this diverse tested function in selecting two prototypes that it is quite sensitive to the labeling noise.",
                    "label": 0
                },
                {
                    "sent": "Two years later, she told modified their approach and in miles it stand for multiple instance learning with.",
                    "label": 0
                },
                {
                    "sent": "Emitted instant selection.",
                    "label": 0
                },
                {
                    "sent": "They use all the instances in the training backs in order to generate this embedding quarter feature space and instant selection process is passed to the classification stage where they use a one normal SVM.",
                    "label": 1
                },
                {
                    "sent": "So it implies an implicit instance selection.",
                    "label": 0
                },
                {
                    "sent": "And as you can guess, its main drawback is this how they form this embedding space and it is exponentially expensive as the volume of the training data increases.",
                    "label": 1
                },
                {
                    "sent": "And after that, the last two methods is actually more similar to our approach, they just use the observation that there is no ambiguity in the negative training packs.",
                    "label": 0
                },
                {
                    "sent": "All backs are composed of negative instances.",
                    "label": 0
                },
                {
                    "sent": "So for example, in wild stay here, choose one prototype from just the positive training packs.",
                    "label": 0
                },
                {
                    "sent": "And this is based on the conditional probability models so that they can choose the most positive instances in this back and forth classification.",
                    "label": 0
                },
                {
                    "sent": "Again they use support vector machines with RBF kernels.",
                    "label": 1
                },
                {
                    "sent": "Until about its throwback, we can say that they only choose positive prototypes, so there's no instances modeling to negative data in the embedding space.",
                    "label": 0
                },
                {
                    "sent": "So as you will see in our experiments, its performance is a bit poor compared to other approaches.",
                    "label": 0
                },
                {
                    "sent": "And plus the Mills chooses one prototype from each training back and this time it is based on for will to density function on the negative data which is based on kernel density estimation.",
                    "label": 0
                },
                {
                    "sent": "And they use linear SVM for classification and also.",
                    "label": 0
                },
                {
                    "sent": "They proposed an alternating instance selection and process, and the training classification cycle so that they can go back and update their initial instance instance prototypes.",
                    "label": 0
                },
                {
                    "sent": "So in a way where we can state this throwback cost compared to 1st three approaches, their training stages of it's expensive.",
                    "label": 0
                },
                {
                    "sent": "So in this work we also start with the same observation on the negative training packs, but compared to others we will have a clustering based approach and for the classic we will be especially using the dominant sets framework because of its advantages.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to briefly talk about Dominicis framework, it is a pairwise clustering approach proposed by iwanan pale yellow.",
                    "label": 0
                },
                {
                    "sent": "It makes no assumption on the underlying data representation, and it can detect the proper number of clusters, and it is quite robust to labeling noise out lives.",
                    "label": 1
                },
                {
                    "sent": "And it imposes no constraint on the structural input similarity metrics that it can naturally deal with both symmetric and negative similarities.",
                    "label": 0
                },
                {
                    "sent": "And Lastly, it can handle unseen data in appearance will play.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "More formally, dominicis can be considered as generalizations of maximal clique to edge weighted graph.",
                    "label": 0
                },
                {
                    "sent": "Basically, domains set satisfy two properties of clusters.",
                    "label": 0
                },
                {
                    "sent": "The first one is internal homogeneity that the instances in a cluster are highly similar to each other and Secondly.",
                    "label": 0
                },
                {
                    "sent": "The instances outside of a cluster is are highly dissimilar to the elements of that clusters.",
                    "label": 0
                },
                {
                    "sent": "So just to give a simple example here, the set 123 is dominant sets.",
                    "label": 0
                },
                {
                    "sent": "As you can see the pairwise similarities between these objects are quite high.",
                    "label": 0
                },
                {
                    "sent": "The other elements are quite dissimilar to the elements of this set.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And computationally adamin sets can be computed as the support of of the local maximizers of this standard quadratic problem, subject to the constraint that the solution vectors should lie on a simplex.",
                    "label": 1
                },
                {
                    "sent": "And we can state some properties of domain set framework to begin with.",
                    "label": 0
                },
                {
                    "sent": "The value of the objective function gives us a measure of the internal coherence is of a cluster.",
                    "label": 1
                },
                {
                    "sent": "While the components of the characteristic vectors gives us measures of the participation of the corresponding data points in.",
                    "label": 0
                },
                {
                    "sent": "The clusters and also the similarity of unseen elements to a cluster can be direct computed by using a weighted similarity using the weights in the characteristic vectors.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now I can start talking about our approach.",
                    "label": 1
                },
                {
                    "sent": "As I said earlier, our main observation is that there is no ambiguity in the negative backs, so our main assumption is that the negative instances form clusters.",
                    "label": 1
                },
                {
                    "sent": "Of course, this assumption may not always be valid.",
                    "label": 1
                },
                {
                    "sent": "For example, in the case of outliers or labeling noise.",
                    "label": 1
                },
                {
                    "sent": "So to put into perspective or approach with.",
                    "label": 0
                },
                {
                    "sent": "Other methods?",
                    "label": 1
                },
                {
                    "sent": "We first cluster the negative data in the negative training packs and then choose a single instance from these extracted clusters and turn based on their relation to these exact clusters.",
                    "label": 0
                },
                {
                    "sent": "We choose some instances from the positive Max.",
                    "label": 0
                },
                {
                    "sent": "UN tiers we will make use of the dominant sets clustering framework.",
                    "label": 0
                },
                {
                    "sent": "And for the classification we use linear support vector machine.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are some basic notations.",
                    "label": 0
                },
                {
                    "sent": "EI stand forced to beg.",
                    "label": 0
                },
                {
                    "sent": "I wear the IG knowledge the Jade instance in back I of course in the training set the backs have some labels.",
                    "label": 0
                },
                {
                    "sent": "And we indeed have set off training packs.",
                    "label": 0
                },
                {
                    "sent": "Some of them are positively labeled, some have negative labels, and since we will cluster the negative data in the negative training packs, we do not collection of the negative instances.",
                    "label": 0
                },
                {
                    "sent": "We take capital.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Location or instance selection strategy starts with computing the pay rise similarities among these negative instances.",
                    "label": 0
                },
                {
                    "sent": "Once we have this similarity matrix, we just extract the domino sets that is the clusters in this set.",
                    "label": 0
                },
                {
                    "sent": "By using a feeling of strategy that we just extract a single cluster from the input data than we exclude that set and rate to rate this process until all the dominant cells have been found.",
                    "label": 0
                },
                {
                    "sent": "Once we extract all the clusters done using the internal queries measures, we sort the clusters according to their internal coherence is and we just select.",
                    "label": 0
                },
                {
                    "sent": "Maximum.",
                    "label": 0
                },
                {
                    "sent": "Number of clusters Barbie.",
                    "label": 0
                },
                {
                    "sent": "Put it upper limit based on the number of negative Max.",
                    "label": 0
                },
                {
                    "sent": "Anti racism artificial examples here.",
                    "label": 0
                },
                {
                    "sent": "The colors do not the labels off the backs, whereas the circus and the crosses denote which class is the instances are coming from here we have a full strength, eight negative bags.",
                    "label": 0
                },
                {
                    "sent": "And and.",
                    "label": 0
                },
                {
                    "sent": "Once we extracted Domino sets, we have correctly found three clusters here.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we have the clusters from the negative training packs.",
                    "label": 1
                },
                {
                    "sent": "Now how we extract the instance prototypes?",
                    "label": 0
                },
                {
                    "sent": "Actually for the negative class is quite straightforward.",
                    "label": 1
                },
                {
                    "sent": "We just select a single prototype from each cluster.",
                    "label": 0
                },
                {
                    "sent": "Based on this formula here, not that or the clusters.",
                    "label": 0
                },
                {
                    "sent": "The components of the characters Quaestors gives us a measure of the participation of the corresponding instances.",
                    "label": 1
                },
                {
                    "sent": "So for example, for this simple toy examples we have just three instances to model the negative data, and as you can see here, there are quite close to the same traits of these clusters.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "However, for the positive Baxter, since there is an ambiguity there, we cannot directly use this costing based approach because it can fail if the negative instances are forming clusters here.",
                    "label": 1
                },
                {
                    "sent": "So here we just.",
                    "label": 1
                },
                {
                    "sent": "Compare the instances in these positive bags.",
                    "label": 1
                },
                {
                    "sent": "Bit extracted clusters so we just choose a single prototype from each positive back and trying to actually.",
                    "label": 0
                },
                {
                    "sent": "Finding the most positive or you know way to least negative examples in this packs.",
                    "label": 0
                },
                {
                    "sent": "Here's the a matrix is actually the similarities between the instances in a positive back to the clusters and three.",
                    "label": 1
                },
                {
                    "sent": "Just take here rated similarities, so indent, we just choose the most distance instant from the extracted negative clusters.",
                    "label": 0
                },
                {
                    "sent": "So for this example, we have just eight instance prototypes and they are from the correct.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Glasses.",
                    "label": 0
                },
                {
                    "sent": "Unforced classification since we have now the instant prototypes you have to define an embedding function based on similarity measure.",
                    "label": 0
                },
                {
                    "sent": "Here we use this similarity measures of head back to instance prototype which is basically based on the distance between instance prototype to its nearest neighbor in the back.",
                    "label": 0
                },
                {
                    "sent": "So we have this embedding function based on the similarities.",
                    "label": 0
                },
                {
                    "sent": "For the classification we just used linear as firm, so we have indeed two parameters to optimize.",
                    "label": 0
                },
                {
                    "sent": "One is the scale parameter used involved clustering phase, and in this similarity measure here and also we have the acquisition parameters for the support.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Permission.",
                    "label": 0
                },
                {
                    "sent": "Not for the multiclass cases, of course.",
                    "label": 0
                },
                {
                    "sent": "Naive way is to use one versus rest strategy where we train C number of binary classifier, see denoting the total number of crosses.",
                    "label": 0
                },
                {
                    "sent": "One for each class against the remaining funds and the classification is performed based on the classifier with highest decision value.",
                    "label": 1
                },
                {
                    "sent": "This strategy.",
                    "label": 0
                },
                {
                    "sent": "We will use a different instance based embedding for each binary subproblem, so.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We come up with a second alternative marriage.",
                    "label": 1
                },
                {
                    "sent": "The idea is to construct an embedding space, come for all the classes.",
                    "label": 1
                },
                {
                    "sent": "Of course you do number of instances selected for each class may differ, but for the training data we have, the training data is kept fixed for all the binary solve problems and on the labels differ.",
                    "label": 1
                },
                {
                    "sent": "So compared to the former approach, this makes the training phase much more efficient.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tommy, compute this instances for each class.",
                    "label": 0
                },
                {
                    "sent": "We simply consider the instances coming from the back send those labels and then we extract clusters.",
                    "label": 0
                },
                {
                    "sent": "For this set.",
                    "label": 0
                },
                {
                    "sent": "Downton Abbey again.",
                    "label": 0
                },
                {
                    "sent": "Choose one through prototypes from each cluster based on this formula here, which is based on both the degree of the participation and the similarity to all the remaining clusters.",
                    "label": 0
                },
                {
                    "sent": "So that's the most similar instance to other training data from other classes is chosen while it has a higher degree of participation.",
                    "label": 0
                },
                {
                    "sent": "Frontiers for the similarity to the remaining classes, we just consider the most.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "English.",
                    "label": 0
                },
                {
                    "sent": "Class.",
                    "label": 0
                },
                {
                    "sent": "To test the performance of our approach, we just consider to kind of task.",
                    "label": 0
                },
                {
                    "sent": "The first one is to use the standard benchmark datasets in mill literature, which are all two class problems and Secondly, we test on multiclass image classification problem.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "English benchmark datasets.",
                    "label": 0
                },
                {
                    "sent": "The common setting is to perform time, timestamp, fold cross validation, and here are the results we.",
                    "label": 0
                },
                {
                    "sent": "Obtained.",
                    "label": 0
                },
                {
                    "sent": "By the way.",
                    "label": 0
                },
                {
                    "sent": "Line device.",
                    "label": 0
                },
                {
                    "sent": "Stonemark mean algorithms from the instant selection based approaches and compared to the state of the art approaches, we do indeed have.",
                    "label": 0
                },
                {
                    "sent": "Competitive results in the in their papers in Millers they did not specify the results on the elephant focus on Tiger data.",
                    "label": 0
                },
                {
                    "sent": "Datasets we.",
                    "label": 0
                },
                {
                    "sent": "Implemented this algorithm by our phone and the results are a bit smaller than the reports at once.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And for the dimensions of the embedding spaces, here are the details of these benchmark datasets, and here are the averages of the dimensions of the embedding spaces for.",
                    "label": 1
                },
                {
                    "sent": "With this instance selection based approaches.",
                    "label": 1
                },
                {
                    "sent": "First of all, my smiles has the highest embedding space dimension, since it uses all the instances in the embedding, whereas Smile tells the lowest dimension.",
                    "label": 1
                },
                {
                    "sent": "But since it does not make use of the.",
                    "label": 0
                },
                {
                    "sent": "Prototypes from the negative training facts, and as you can see in the previous table, it's performance is a bit poor compared to other approaches.",
                    "label": 0
                },
                {
                    "sent": "Answer For our methods compared to millisecond DDS firm we have less dimension.",
                    "label": 1
                },
                {
                    "sent": "Approximately 622 until percentage smaller than those algorithms.",
                    "label": 0
                },
                {
                    "sent": "And of course, we can even reduce these dimensions by employing sparse linear SVM in training, spelt, or eliminating the dimensions with.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very small lights.",
                    "label": 0
                },
                {
                    "sent": "Unfolded multiclass problem.",
                    "label": 0
                },
                {
                    "sent": "We use the experimental setup using the DDS from paper.",
                    "label": 1
                },
                {
                    "sent": "It consists of 2000 images from 2 antique categories, each having hundreds examples and it is considered as a back and these images are segment into regions and these regions are actually corresponding to the instances in this Max or.",
                    "label": 1
                },
                {
                    "sent": "Images that each class has some distinct region of interests.",
                    "label": 0
                },
                {
                    "sent": "Understand in their experiments they use make use of two different.",
                    "label": 0
                },
                {
                    "sent": "Setups in the first one, they just consider the first time categories and in the second one they use all the existing categories until we test 22 possible extensions of our approach to the multiclass.",
                    "label": 1
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Case.",
                    "label": 0
                },
                {
                    "sent": "Frontier should the standard set up is to use 5 * 2 fold cross validation.",
                    "label": 0
                },
                {
                    "sent": "And here are the results we get.",
                    "label": 0
                },
                {
                    "sent": "Again, we obtain competitive results compared to state of the art until one for the larger data set to 2000 image datasets or second version gives the best results.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also, we can look into these results in a more deeply in the first approach that we use for less stress strategy, each classifier is trained to distinguish a specific category from the rest.",
                    "label": 1
                },
                {
                    "sent": "So again, to repeat a different embedding space is built for each subproblem, but the selected the set of sex protais, Verizon every subproblem.",
                    "label": 1
                },
                {
                    "sent": "What's here, we can say that the positive purple prototypes are mostly selected from the discriminator regions for those classes, and here are some.",
                    "label": 0
                },
                {
                    "sent": "Results for explaining.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's.",
                    "label": 0
                },
                {
                    "sent": "And for our SEC multiclass strategy, the set of selected instance prototypes is the same for all the subproblems.",
                    "label": 1
                },
                {
                    "sent": "And in this way we actually provided rich failed to include context as well.",
                    "label": 0
                },
                {
                    "sent": "Here we are showing just.",
                    "label": 0
                },
                {
                    "sent": "To class is the horizontal metal ship categories.",
                    "label": 0
                },
                {
                    "sent": "Here in both tables the ones on the left shows the actual prototypes which are coming from a specific clusters, whereas the three examples on the right shows others members in those classes.",
                    "label": 0
                },
                {
                    "sent": "As you can see here, we can come up with different clusters corresponding to horses with different appearance properties.",
                    "label": 0
                },
                {
                    "sent": "But also for example at the last line here we have some regions corresponding to the cross region and that similar type of explanation can be made for the battleship class as well.",
                    "label": 0
                },
                {
                    "sent": "And in a way this.",
                    "label": 1
                },
                {
                    "sent": "They off finding the instant prototypes resembles vocabulary generation step of the so called bag of words approach is.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lastly, we investigate the sensitivity to labeling noise again with the set up in the mice paper where we have a two class problem distinguishing historical buildings from the horses very artificially introduce noisy labels for each noise levels.",
                    "label": 1
                },
                {
                    "sent": "D percent of the positive anti percent of the negative images are randomly selected and their labels are exchanged and again we performed.",
                    "label": 0
                },
                {
                    "sent": "5 * 2 fold Cross validation here, and this is our results.",
                    "label": 1
                },
                {
                    "sent": "What about the conclusions here at the lower levels for the cases with the noise level less than 5%, there is no constable.",
                    "label": 1
                },
                {
                    "sent": "This difference in the performance of both algorithms, but at high levels, especially when the noise level is higher than or equal to 25%, our approach is the most robust bar.",
                    "label": 0
                },
                {
                    "sent": "And this is expected to cause domains that is known to be quite robust to outliers.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hunter is a.",
                    "label": 0
                },
                {
                    "sent": "Summary In this study, we propose a new instance selection strategy which is based on domain sets.",
                    "label": 1
                },
                {
                    "sent": "Our approach can identify the most representative examples in both positive and negative training packs and our experimental results show that our approach is competitive with the stumps.",
                    "label": 1
                },
                {
                    "sent": "State of the art methods.",
                    "label": 0
                },
                {
                    "sent": "Lastly, it is quite robust to labeling noise and for future work we would like to investigate to related problems.",
                    "label": 0
                },
                {
                    "sent": "The first one is multiple instance multi label learning problem where this case the banks have more than one labels associated with them and Secondly here we all consider samples in a bag.",
                    "label": 0
                },
                {
                    "sent": "From IID samples.",
                    "label": 0
                },
                {
                    "sent": "But for example, in the case for some computer vision problems like the image categorization problem I mentioned here, they do have indeed some structures in this specs, so there are a couple of works which treats the instances in the backs as long IID samples, and we would like to investigate these.",
                    "label": 0
                },
                {
                    "sent": "Both of these problems from an instance selection based perspective.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And thank you.",
                    "label": 0
                }
            ]
        }
    }
}