{
    "id": "nti26im2bqxeuwxg2r4jf5fosxgaj4rq",
    "title": "Statistical Change Detection for Multi-Dimensional Data",
    "info": {
        "author": [
            "Xiuyao Song, University of Florida"
        ],
        "published": "Aug. 15, 2007",
        "recorded": "August 2007",
        "category": [
            "Top->Computer Science->Data Mining",
            "Top->Computer Science->Machine Learning->Density Estimation"
        ]
    },
    "url": "http://videolectures.net/kdd07_song_scd/",
    "segmentation": [
        [
            "Hi good afternoon everyone.",
            "My name is young and this work is collaborated with me.",
            "She and supervised by Christian and Center anchor.",
            "The problem we're considering is to build a status test to detect the change in distribution for multidimensional data.",
            "This test could have numerous applications in a while."
        ],
        [
            "Of disciplines.",
            "One of the motivation example consider is to detect antibiotic resistant pattern change.",
            "Assume we're culturing the specimen of E. Coli bacterial and then we testing is resistant to multiple antibiotics drugs.",
            "A typical resistance data could be a vector like this.",
            "Each dimension of the vector could indicate the E. Coli bacteria is resistant, susceptible, or have undetermined reaction to a specific drug.",
            "Now we are given a baseline data set and a recently observed data set.",
            "The question of interest is, does equalize show different resistant pattern than what we have observed before?",
            "Now we need a change detection method to answer this question.",
            "If a change is detected.",
            "It might be caused by the presence of some new equalize strength.",
            "In such a case, will raise an alarm for further investigations."
        ],
        [
            "You high level the problem was solved is like this.",
            "Given two data set as an S prime, we assume they are randomly sampled from 2 unknown underlying distributions respectively.",
            "The question we aim to answer is others to distribution."
        ],
        [
            "The same.",
            "For uni dimensional data, we have many existing tests to solve this problem, such as the chaos test and Chi Square test.",
            "But unfortunately, for multidimensional data there has been little attention paid to this case.",
            "Actually, we found only two tests to detect a generic distribution change in multidimensional space.",
            "One of them is called PDQ tree test.",
            "This test relies on the discretization scheme of the data space.",
            "So basically this test.",
            "Tends to be suffer from the curse of dimensionality.",
            "The second one is cross match test.",
            "This test is basically computationally expensive because it includes a maximum matching algorithm.",
            "And it's worth mentioning that both the kid cute retest and cross match test and even the test we propose later."
        ],
        [
            "Will be based on the hypothesis test framework.",
            "The null hypothesis states that the two underlying distributions are the same.",
            "Then a test status Delta is defined and it's not distributions derived.",
            "If the null hypothesis holds, the Delta should be a sample from the non distribution.",
            "Otherwise, if the observed Delta is far out in the tales of the non distribution, it's highly likely that the null hypothesis does not hold."
        ],
        [
            "So here is a density test we proposed before we conduct test, we first randomly partition the baseline data set into two subsets as one as two.",
            "Then our density test consists of four steps.",
            "Step one will use Gaussian color density estimate to infer the underlying distribution from S1.",
            "And the density estimate result.",
            "We call it PS1.",
            "The second step we will define a test status Delta.",
            "Basically our density test.",
            "The Delta is a function of X prime as two and the density estimate from S1.",
            "The third step we will derive within our distribution and last step we calculate the critical value town for our density test and then we make a conclusion.",
            "Next I will go over this first."
        ],
        [
            "1 by 1.",
            "Step one is using the kernel density estimate technique.",
            "The idea behind the KD is put a corner over each data point and then sum the bumps up to obtain the density estimate.",
            "The most important thing in the Kitty technique is to choose a the appropriate bandwidth.",
            "There are two basic approach to choose the bandwidth.",
            "The first one is plug in bandwidth.",
            "This bound with assume each corner all the corners share the same fixed bandwidth.",
            "This approach is a symptomatic inefficient, but is not accurate in high dimensional space.",
            "But the other one, the data driven boundaries is more accurate to convert better to the true distribution.",
            "So which bound with selection methods should we choose?",
            "We basically have two observations.",
            "First, the correctness of our density test does not depend on any specific bound with selection method.",
            "Actually, it even does not depend on the kernel density estimation technique as we will show later because now distribution of our density test is always a normal distribution which is centered at 0.",
            "The second observation is that the power of our density test will be increased if the estimate is converge closely to a true distribution.",
            "So based on these two observations we will go for a data driven bond with selection."
        ],
        [
            "Message.",
            "Specifically, in arc density test, we will choose the bandwidth for each corner by the maximum maximum likelihood estimation technique.",
            "In traditional me method.",
            "We will maximize the log like function of the data set over this density estimate.",
            "But in our case, we cannot maximize this local function directly, why?",
            "You can refer to this figure.",
            "Because each corner is centered on one data point.",
            "So as the bandwidth of the corner is converted to zero, the likelihood of this data point over his corner will be become infinite.",
            "As one term becomes infinite, the whole log like function will become infinite.",
            "So as you can see, if we maximize this log like function directly, the optimal bandwidth will always shrink to 0, so this does not make sense.",
            "What we did to fix this problem is to add an extra constraint.",
            "The constant states that the likelihood of the data point over its kernel is set to 0 and we did not count that likelihood.",
            "Now we can combine the log likelihood function with this adding constraint.",
            "We got a pseudo log like the object function and we will maximize this object function with the expectation maximization algorithm.",
            "Or we could MA."
        ],
        [
            "Some for short.",
            "And these figures shows the effectiveness of REM bandwidth.",
            "The first figure is the true distribution.",
            "The second and third one are the samples from the density estimate.",
            "With the plug-in bandwidth and R.E.M.",
            "Bond with respectively.",
            "Is Connie that REM bond with in the third graph converted very close needs to a true distribution?",
            "So that means our density test, the power of our density test is fully explored with this year bandwidth.",
            "OK, now I finished."
        ],
        [
            "First step, now we come to second step.",
            "Define and calculate this test statistic.",
            "The test status Delta is defined in this formula where H stands for the log likelihood.",
            "Basically, it's not like hood of this as prime over the density estimate of S1 minus the skilled like hood of S 2 / K S 1.",
            "As you can see, for the second term, since S1 and S2 are coming from the same distribution, so this term will always take very large value.",
            "As for the first term, if as prime and as one are coming from different distribution, the value will be very small, otherwise it will have much value.",
            "So we know that if the underlying distribution of SNS Prime are different, the Delta will take a very small value.",
            "That means our density test will be a 1 sided loyalty or hypothesis test."
        ],
        [
            "After we calculate the test steps value, we will derive its corresponding now distribution.",
            "This is our Step 3.",
            "Assume the big Delta here is a random variable of the non distribution.",
            "It consists of two independent terms.",
            "Each of these terms.",
            "How can be viewed as the sum of some IID random verbs where the TI in the formula means is a random variable from distribution FS?",
            "By the central Limit Theorem, each of these terms will follow a normal distribution.",
            "And finally we know that this big Delta than our distribution will be a normal distribution.",
            "After some derivations, we found that the expectation of this non distribution is exactly 0 and variance of the distribution is proportional to some Sigma square.",
            "As for the Sigma square one with element, TI has unknown underlying distribution FS, so we have to estimate this Sigma."
        ],
        [
            "Here.",
            "Before estimating, we should be aware of that this estimation might introduce some additional Type 1 error.",
            "As you refer to a figure on the right.",
            "If the thing my square is under estimated, that means the virus of the non distribution will be under estimated.",
            "So the estimated not distribution will improperly shrink towards the mean.",
            "That means a normal sample from the true, not distribution could become a plumber in the estimated loss distribution.",
            "In this case we incorrectly reject null hypothesis, so that's why I where we introduce a Type 1 error.",
            "But unfortunately we can use the bootstrap percentile method to bond this typo error.",
            "In this algorithm we use Bootstrap method to get a series of estimate of Sigma Square and then we pick a upper confidence limit on the true value of Sigma Square and use that value as the estimate and the probability of underestimating Sigma Square is bound by beta.",
            "With this algorithm."
        ],
        [
            "OK, the next step of our intensive test is to calculate the critical value and make a decision.",
            "Assume Alpha is a typo error of the basic hypothesis test and beta is error we introduced when we when we estimate than our distribution.",
            "So that all the critical value town can be calculated by its definition.",
            "That means the probability of the null distribution less than tall is bound by Alpha.",
            "OK, after Tao is calculated, we can make a decision of the detection.",
            "That means if the observed Delta the calculate Delta in the second step is necessary and how we can reject null hypothesis and we declare a distribution of change.",
            "Note that in our density test, the total type 1 error is bound by Alpha plus beta.",
            "And also the critical value Tau is related to both Alpha and beta.",
            "We will choose Alpha and beta in such a way.",
            "The towel is maximized because with nature towel we are more likely to reject null hypothesis, so the detection power is maximized."
        ],
        [
            "OK, sure, now we finish all the four steps of our density test.",
            "And now the test is completely ready to be."
        ],
        [
            "Post.",
            "But if you want to increase the detection power, you might run want to run the density test in two directions.",
            "OK, as in these figures show because the Delta is not defined symmetrically as regard to S&S prime, so changes in One Direction might be much easier to detect than the changes on the other direction."
        ],
        [
            "So finally we will give some experimental result.",
            "We will compare our density test with the other two.",
            "Test petticoat, retest and cross match test.",
            "Our experiment datasets include 13 data set.",
            "We will divide the data set into two groups, no dimensional group and high Dimensional Group.",
            "We will use large datasets for high dimensional Group due to the curse of dimensionality.",
            "Here is a table, gave the false positive of all the methods.",
            "You can see.",
            "All the methods can correctly find the typo error by the user given P value 8%.",
            "And also notice that cross match does not have results for high dimensional Group because this test has limited scalability."
        ],
        [
            "Then as regards to the power of the test.",
            "In order to test the power of the test, we create five types of changes as given in the X axis.",
            "As you can see, the density test represented by the blue bar always gives the lowest false negative on all five type of changes.",
            "And this is a result for the low dimensional group."
        ],
        [
            "For the high dimensional group, again, the density test shows the lowest false negative.",
            "That means it's most powerful on all five type of changes."
        ],
        [
            "Next we will compare the scalability of all the three methods these two plots gives the running time of each method versus the data size.",
            "As you can see, the cross match has the worst scalability and kinda country has best scalability.",
            "An artistic test falls in between.",
            "But it's worth mentioning that our density test has a one time cost for constructing the kernel density estimate.",
            "Once the density estimate is constructed, we can save it and reduce it, and then our test can repeatedly be right in a smaller time fraction.",
            "And actually this one time cost occupies 84% of the whole running time."
        ],
        [
            "In conclusion, our density test can correctly bound the Type 1 error, and it is the most powerful on all five type of changes, and it can easily scale to large data size and has an amortized overtime costs.",
            "And that concludes my talk.",
            "Thank you very much."
        ],
        [
            "Explain your constraint again.",
            "Colonel.",
            "Each color is centered on one data point that is part of the.",
            "Function, we want to maximize.",
            "Actually, the we use normal life functions, that is by the maximization MLE algorithm want maximize this option function.",
            "But we if we maximize it directly, it will have a problem.",
            "About this why the kernel centered at X would be serious.",
            "Oh, that's the constraint we add to make the object function.",
            "I mean more meaningful.",
            "App that we set each other.",
            "If the bandwidth sharing through the likelihood of each data point over 0 because we don't want count that into the log likelihood function.",
            "Change one out.",
            "Yeah.",
            "We just ignore that part by the corner.",
            "Thanks.",
            "You mean?",
            "The ID we randomly partitioned the baseline setting S1S2.",
            "And in the experiment of the non distribution, the Big Delta TI, the random variable T is always a random variable with underlying distribution FS.",
            "So basically these two terms of the big Delta is independent and the TSR TSR independent because they are from FS.",
            "They're random variable from the scene from the unknown distribution FS and these do not relate to each other.",
            "So the idea assumption is a whole for the night distribution.",
            "All the way back.",
            "Katie.",
            "Place.",
            "Sorry I missed your question.",
            "So you mean the clinical trial tests are used?",
            "Sometimes a purple better than.",
            "The price match or.",
            "And could you please just waiting to compute one of the many ways you can compute?",
            "You are using it then how that would compare to.",
            "The comparison we did a comparison three methods is based on the five type changes we created, but it yes because it's a change is like it's in settings change.",
            "We made these changes about the changes is extensive with kind of extensive.",
            "We made changes on our dimension of the data set and sometimes we make changes on one dimension.",
            "So if you use different disruptions, maybe yeah maybe you have a different result.",
            "But for our results in the year.",
            "To the best of power over all other methods.",
            "Speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi good afternoon everyone.",
                    "label": 0
                },
                {
                    "sent": "My name is young and this work is collaborated with me.",
                    "label": 0
                },
                {
                    "sent": "She and supervised by Christian and Center anchor.",
                    "label": 0
                },
                {
                    "sent": "The problem we're considering is to build a status test to detect the change in distribution for multidimensional data.",
                    "label": 1
                },
                {
                    "sent": "This test could have numerous applications in a while.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of disciplines.",
                    "label": 0
                },
                {
                    "sent": "One of the motivation example consider is to detect antibiotic resistant pattern change.",
                    "label": 0
                },
                {
                    "sent": "Assume we're culturing the specimen of E. Coli bacterial and then we testing is resistant to multiple antibiotics drugs.",
                    "label": 1
                },
                {
                    "sent": "A typical resistance data could be a vector like this.",
                    "label": 0
                },
                {
                    "sent": "Each dimension of the vector could indicate the E. Coli bacteria is resistant, susceptible, or have undetermined reaction to a specific drug.",
                    "label": 0
                },
                {
                    "sent": "Now we are given a baseline data set and a recently observed data set.",
                    "label": 1
                },
                {
                    "sent": "The question of interest is, does equalize show different resistant pattern than what we have observed before?",
                    "label": 1
                },
                {
                    "sent": "Now we need a change detection method to answer this question.",
                    "label": 0
                },
                {
                    "sent": "If a change is detected.",
                    "label": 1
                },
                {
                    "sent": "It might be caused by the presence of some new equalize strength.",
                    "label": 0
                },
                {
                    "sent": "In such a case, will raise an alarm for further investigations.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You high level the problem was solved is like this.",
                    "label": 0
                },
                {
                    "sent": "Given two data set as an S prime, we assume they are randomly sampled from 2 unknown underlying distributions respectively.",
                    "label": 0
                },
                {
                    "sent": "The question we aim to answer is others to distribution.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The same.",
                    "label": 0
                },
                {
                    "sent": "For uni dimensional data, we have many existing tests to solve this problem, such as the chaos test and Chi Square test.",
                    "label": 0
                },
                {
                    "sent": "But unfortunately, for multidimensional data there has been little attention paid to this case.",
                    "label": 0
                },
                {
                    "sent": "Actually, we found only two tests to detect a generic distribution change in multidimensional space.",
                    "label": 1
                },
                {
                    "sent": "One of them is called PDQ tree test.",
                    "label": 0
                },
                {
                    "sent": "This test relies on the discretization scheme of the data space.",
                    "label": 0
                },
                {
                    "sent": "So basically this test.",
                    "label": 1
                },
                {
                    "sent": "Tends to be suffer from the curse of dimensionality.",
                    "label": 1
                },
                {
                    "sent": "The second one is cross match test.",
                    "label": 0
                },
                {
                    "sent": "This test is basically computationally expensive because it includes a maximum matching algorithm.",
                    "label": 0
                },
                {
                    "sent": "And it's worth mentioning that both the kid cute retest and cross match test and even the test we propose later.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will be based on the hypothesis test framework.",
                    "label": 1
                },
                {
                    "sent": "The null hypothesis states that the two underlying distributions are the same.",
                    "label": 0
                },
                {
                    "sent": "Then a test status Delta is defined and it's not distributions derived.",
                    "label": 1
                },
                {
                    "sent": "If the null hypothesis holds, the Delta should be a sample from the non distribution.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, if the observed Delta is far out in the tales of the non distribution, it's highly likely that the null hypothesis does not hold.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here is a density test we proposed before we conduct test, we first randomly partition the baseline data set into two subsets as one as two.",
                    "label": 1
                },
                {
                    "sent": "Then our density test consists of four steps.",
                    "label": 0
                },
                {
                    "sent": "Step one will use Gaussian color density estimate to infer the underlying distribution from S1.",
                    "label": 0
                },
                {
                    "sent": "And the density estimate result.",
                    "label": 1
                },
                {
                    "sent": "We call it PS1.",
                    "label": 0
                },
                {
                    "sent": "The second step we will define a test status Delta.",
                    "label": 0
                },
                {
                    "sent": "Basically our density test.",
                    "label": 0
                },
                {
                    "sent": "The Delta is a function of X prime as two and the density estimate from S1.",
                    "label": 0
                },
                {
                    "sent": "The third step we will derive within our distribution and last step we calculate the critical value town for our density test and then we make a conclusion.",
                    "label": 1
                },
                {
                    "sent": "Next I will go over this first.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "1 by 1.",
                    "label": 0
                },
                {
                    "sent": "Step one is using the kernel density estimate technique.",
                    "label": 1
                },
                {
                    "sent": "The idea behind the KD is put a corner over each data point and then sum the bumps up to obtain the density estimate.",
                    "label": 0
                },
                {
                    "sent": "The most important thing in the Kitty technique is to choose a the appropriate bandwidth.",
                    "label": 0
                },
                {
                    "sent": "There are two basic approach to choose the bandwidth.",
                    "label": 0
                },
                {
                    "sent": "The first one is plug in bandwidth.",
                    "label": 0
                },
                {
                    "sent": "This bound with assume each corner all the corners share the same fixed bandwidth.",
                    "label": 0
                },
                {
                    "sent": "This approach is a symptomatic inefficient, but is not accurate in high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "But the other one, the data driven boundaries is more accurate to convert better to the true distribution.",
                    "label": 1
                },
                {
                    "sent": "So which bound with selection methods should we choose?",
                    "label": 0
                },
                {
                    "sent": "We basically have two observations.",
                    "label": 0
                },
                {
                    "sent": "First, the correctness of our density test does not depend on any specific bound with selection method.",
                    "label": 1
                },
                {
                    "sent": "Actually, it even does not depend on the kernel density estimation technique as we will show later because now distribution of our density test is always a normal distribution which is centered at 0.",
                    "label": 0
                },
                {
                    "sent": "The second observation is that the power of our density test will be increased if the estimate is converge closely to a true distribution.",
                    "label": 0
                },
                {
                    "sent": "So based on these two observations we will go for a data driven bond with selection.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Message.",
                    "label": 0
                },
                {
                    "sent": "Specifically, in arc density test, we will choose the bandwidth for each corner by the maximum maximum likelihood estimation technique.",
                    "label": 1
                },
                {
                    "sent": "In traditional me method.",
                    "label": 0
                },
                {
                    "sent": "We will maximize the log like function of the data set over this density estimate.",
                    "label": 0
                },
                {
                    "sent": "But in our case, we cannot maximize this local function directly, why?",
                    "label": 0
                },
                {
                    "sent": "You can refer to this figure.",
                    "label": 0
                },
                {
                    "sent": "Because each corner is centered on one data point.",
                    "label": 0
                },
                {
                    "sent": "So as the bandwidth of the corner is converted to zero, the likelihood of this data point over his corner will be become infinite.",
                    "label": 0
                },
                {
                    "sent": "As one term becomes infinite, the whole log like function will become infinite.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, if we maximize this log like function directly, the optimal bandwidth will always shrink to 0, so this does not make sense.",
                    "label": 0
                },
                {
                    "sent": "What we did to fix this problem is to add an extra constraint.",
                    "label": 0
                },
                {
                    "sent": "The constant states that the likelihood of the data point over its kernel is set to 0 and we did not count that likelihood.",
                    "label": 1
                },
                {
                    "sent": "Now we can combine the log likelihood function with this adding constraint.",
                    "label": 1
                },
                {
                    "sent": "We got a pseudo log like the object function and we will maximize this object function with the expectation maximization algorithm.",
                    "label": 0
                },
                {
                    "sent": "Or we could MA.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some for short.",
                    "label": 0
                },
                {
                    "sent": "And these figures shows the effectiveness of REM bandwidth.",
                    "label": 1
                },
                {
                    "sent": "The first figure is the true distribution.",
                    "label": 0
                },
                {
                    "sent": "The second and third one are the samples from the density estimate.",
                    "label": 1
                },
                {
                    "sent": "With the plug-in bandwidth and R.E.M.",
                    "label": 0
                },
                {
                    "sent": "Bond with respectively.",
                    "label": 0
                },
                {
                    "sent": "Is Connie that REM bond with in the third graph converted very close needs to a true distribution?",
                    "label": 0
                },
                {
                    "sent": "So that means our density test, the power of our density test is fully explored with this year bandwidth.",
                    "label": 0
                },
                {
                    "sent": "OK, now I finished.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First step, now we come to second step.",
                    "label": 0
                },
                {
                    "sent": "Define and calculate this test statistic.",
                    "label": 1
                },
                {
                    "sent": "The test status Delta is defined in this formula where H stands for the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "Basically, it's not like hood of this as prime over the density estimate of S1 minus the skilled like hood of S 2 / K S 1.",
                    "label": 0
                },
                {
                    "sent": "As you can see, for the second term, since S1 and S2 are coming from the same distribution, so this term will always take very large value.",
                    "label": 0
                },
                {
                    "sent": "As for the first term, if as prime and as one are coming from different distribution, the value will be very small, otherwise it will have much value.",
                    "label": 0
                },
                {
                    "sent": "So we know that if the underlying distribution of SNS Prime are different, the Delta will take a very small value.",
                    "label": 0
                },
                {
                    "sent": "That means our density test will be a 1 sided loyalty or hypothesis test.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After we calculate the test steps value, we will derive its corresponding now distribution.",
                    "label": 0
                },
                {
                    "sent": "This is our Step 3.",
                    "label": 0
                },
                {
                    "sent": "Assume the big Delta here is a random variable of the non distribution.",
                    "label": 0
                },
                {
                    "sent": "It consists of two independent terms.",
                    "label": 0
                },
                {
                    "sent": "Each of these terms.",
                    "label": 0
                },
                {
                    "sent": "How can be viewed as the sum of some IID random verbs where the TI in the formula means is a random variable from distribution FS?",
                    "label": 0
                },
                {
                    "sent": "By the central Limit Theorem, each of these terms will follow a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "And finally we know that this big Delta than our distribution will be a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "After some derivations, we found that the expectation of this non distribution is exactly 0 and variance of the distribution is proportional to some Sigma square.",
                    "label": 0
                },
                {
                    "sent": "As for the Sigma square one with element, TI has unknown underlying distribution FS, so we have to estimate this Sigma.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Before estimating, we should be aware of that this estimation might introduce some additional Type 1 error.",
                    "label": 1
                },
                {
                    "sent": "As you refer to a figure on the right.",
                    "label": 0
                },
                {
                    "sent": "If the thing my square is under estimated, that means the virus of the non distribution will be under estimated.",
                    "label": 0
                },
                {
                    "sent": "So the estimated not distribution will improperly shrink towards the mean.",
                    "label": 0
                },
                {
                    "sent": "That means a normal sample from the true, not distribution could become a plumber in the estimated loss distribution.",
                    "label": 0
                },
                {
                    "sent": "In this case we incorrectly reject null hypothesis, so that's why I where we introduce a Type 1 error.",
                    "label": 0
                },
                {
                    "sent": "But unfortunately we can use the bootstrap percentile method to bond this typo error.",
                    "label": 1
                },
                {
                    "sent": "In this algorithm we use Bootstrap method to get a series of estimate of Sigma Square and then we pick a upper confidence limit on the true value of Sigma Square and use that value as the estimate and the probability of underestimating Sigma Square is bound by beta.",
                    "label": 0
                },
                {
                    "sent": "With this algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, the next step of our intensive test is to calculate the critical value and make a decision.",
                    "label": 1
                },
                {
                    "sent": "Assume Alpha is a typo error of the basic hypothesis test and beta is error we introduced when we when we estimate than our distribution.",
                    "label": 0
                },
                {
                    "sent": "So that all the critical value town can be calculated by its definition.",
                    "label": 0
                },
                {
                    "sent": "That means the probability of the null distribution less than tall is bound by Alpha.",
                    "label": 0
                },
                {
                    "sent": "OK, after Tao is calculated, we can make a decision of the detection.",
                    "label": 0
                },
                {
                    "sent": "That means if the observed Delta the calculate Delta in the second step is necessary and how we can reject null hypothesis and we declare a distribution of change.",
                    "label": 0
                },
                {
                    "sent": "Note that in our density test, the total type 1 error is bound by Alpha plus beta.",
                    "label": 1
                },
                {
                    "sent": "And also the critical value Tau is related to both Alpha and beta.",
                    "label": 0
                },
                {
                    "sent": "We will choose Alpha and beta in such a way.",
                    "label": 0
                },
                {
                    "sent": "The towel is maximized because with nature towel we are more likely to reject null hypothesis, so the detection power is maximized.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, sure, now we finish all the four steps of our density test.",
                    "label": 0
                },
                {
                    "sent": "And now the test is completely ready to be.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Post.",
                    "label": 0
                },
                {
                    "sent": "But if you want to increase the detection power, you might run want to run the density test in two directions.",
                    "label": 1
                },
                {
                    "sent": "OK, as in these figures show because the Delta is not defined symmetrically as regard to S&S prime, so changes in One Direction might be much easier to detect than the changes on the other direction.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So finally we will give some experimental result.",
                    "label": 0
                },
                {
                    "sent": "We will compare our density test with the other two.",
                    "label": 0
                },
                {
                    "sent": "Test petticoat, retest and cross match test.",
                    "label": 0
                },
                {
                    "sent": "Our experiment datasets include 13 data set.",
                    "label": 0
                },
                {
                    "sent": "We will divide the data set into two groups, no dimensional group and high Dimensional Group.",
                    "label": 1
                },
                {
                    "sent": "We will use large datasets for high dimensional Group due to the curse of dimensionality.",
                    "label": 0
                },
                {
                    "sent": "Here is a table, gave the false positive of all the methods.",
                    "label": 1
                },
                {
                    "sent": "You can see.",
                    "label": 0
                },
                {
                    "sent": "All the methods can correctly find the typo error by the user given P value 8%.",
                    "label": 0
                },
                {
                    "sent": "And also notice that cross match does not have results for high dimensional Group because this test has limited scalability.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then as regards to the power of the test.",
                    "label": 0
                },
                {
                    "sent": "In order to test the power of the test, we create five types of changes as given in the X axis.",
                    "label": 0
                },
                {
                    "sent": "As you can see, the density test represented by the blue bar always gives the lowest false negative on all five type of changes.",
                    "label": 1
                },
                {
                    "sent": "And this is a result for the low dimensional group.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the high dimensional group, again, the density test shows the lowest false negative.",
                    "label": 0
                },
                {
                    "sent": "That means it's most powerful on all five type of changes.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next we will compare the scalability of all the three methods these two plots gives the running time of each method versus the data size.",
                    "label": 1
                },
                {
                    "sent": "As you can see, the cross match has the worst scalability and kinda country has best scalability.",
                    "label": 0
                },
                {
                    "sent": "An artistic test falls in between.",
                    "label": 0
                },
                {
                    "sent": "But it's worth mentioning that our density test has a one time cost for constructing the kernel density estimate.",
                    "label": 1
                },
                {
                    "sent": "Once the density estimate is constructed, we can save it and reduce it, and then our test can repeatedly be right in a smaller time fraction.",
                    "label": 0
                },
                {
                    "sent": "And actually this one time cost occupies 84% of the whole running time.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In conclusion, our density test can correctly bound the Type 1 error, and it is the most powerful on all five type of changes, and it can easily scale to large data size and has an amortized overtime costs.",
                    "label": 1
                },
                {
                    "sent": "And that concludes my talk.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Explain your constraint again.",
                    "label": 0
                },
                {
                    "sent": "Colonel.",
                    "label": 0
                },
                {
                    "sent": "Each color is centered on one data point that is part of the.",
                    "label": 0
                },
                {
                    "sent": "Function, we want to maximize.",
                    "label": 0
                },
                {
                    "sent": "Actually, the we use normal life functions, that is by the maximization MLE algorithm want maximize this option function.",
                    "label": 0
                },
                {
                    "sent": "But we if we maximize it directly, it will have a problem.",
                    "label": 0
                },
                {
                    "sent": "About this why the kernel centered at X would be serious.",
                    "label": 0
                },
                {
                    "sent": "Oh, that's the constraint we add to make the object function.",
                    "label": 0
                },
                {
                    "sent": "I mean more meaningful.",
                    "label": 0
                },
                {
                    "sent": "App that we set each other.",
                    "label": 0
                },
                {
                    "sent": "If the bandwidth sharing through the likelihood of each data point over 0 because we don't want count that into the log likelihood function.",
                    "label": 0
                },
                {
                    "sent": "Change one out.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "We just ignore that part by the corner.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "You mean?",
                    "label": 0
                },
                {
                    "sent": "The ID we randomly partitioned the baseline setting S1S2.",
                    "label": 0
                },
                {
                    "sent": "And in the experiment of the non distribution, the Big Delta TI, the random variable T is always a random variable with underlying distribution FS.",
                    "label": 0
                },
                {
                    "sent": "So basically these two terms of the big Delta is independent and the TSR TSR independent because they are from FS.",
                    "label": 0
                },
                {
                    "sent": "They're random variable from the scene from the unknown distribution FS and these do not relate to each other.",
                    "label": 0
                },
                {
                    "sent": "So the idea assumption is a whole for the night distribution.",
                    "label": 0
                },
                {
                    "sent": "All the way back.",
                    "label": 0
                },
                {
                    "sent": "Katie.",
                    "label": 0
                },
                {
                    "sent": "Place.",
                    "label": 0
                },
                {
                    "sent": "Sorry I missed your question.",
                    "label": 0
                },
                {
                    "sent": "So you mean the clinical trial tests are used?",
                    "label": 0
                },
                {
                    "sent": "Sometimes a purple better than.",
                    "label": 0
                },
                {
                    "sent": "The price match or.",
                    "label": 0
                },
                {
                    "sent": "And could you please just waiting to compute one of the many ways you can compute?",
                    "label": 0
                },
                {
                    "sent": "You are using it then how that would compare to.",
                    "label": 0
                },
                {
                    "sent": "The comparison we did a comparison three methods is based on the five type changes we created, but it yes because it's a change is like it's in settings change.",
                    "label": 0
                },
                {
                    "sent": "We made these changes about the changes is extensive with kind of extensive.",
                    "label": 0
                },
                {
                    "sent": "We made changes on our dimension of the data set and sometimes we make changes on one dimension.",
                    "label": 0
                },
                {
                    "sent": "So if you use different disruptions, maybe yeah maybe you have a different result.",
                    "label": 0
                },
                {
                    "sent": "But for our results in the year.",
                    "label": 0
                },
                {
                    "sent": "To the best of power over all other methods.",
                    "label": 0
                },
                {
                    "sent": "Speaker.",
                    "label": 0
                }
            ]
        }
    }
}