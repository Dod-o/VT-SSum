{
    "id": "ex7o646wm4lrcmbvad4k2754yl5yp37n",
    "title": "Sequential Bayesian Prediction in the Presence of Changepoints",
    "info": {
        "author": [
            "Michael Osborne, Department of Engineering Science, University of Oxford"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Bayesian Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_osborne_sbp/",
    "segmentation": [
        [
            "OK, so yeah, sequential Bayesian prediction in the presence of changepoints."
        ],
        [
            "What do I mean by changepoint?",
            "Well, Simply put an abrupt change in the underlying characteristics of the data we're trying to track.",
            "So here's a very simple example in which this data we're trying to form predictions about undergoes some sort of change in its amplitude and its smoothness.",
            "So we need some sort of way to deal with that and particular tool.",
            "We're going to be using is a Gaussian process sound, some Gaussian process basics.",
            "My apologies if you."
        ],
        [
            "Hopefully with it, so my Gaussian well, I'm a Gaussian distribution has a number of different properties that we're going to exploit, and I'd like to highlight a couple of them here using this simple contour plot.",
            "Those are part of this bivariate Gaussian agasi and over 2 variables X2 and X1 in these contours and are the first thing I want to do that to illustrate is the property of the Gaussian that its marginal distributions are themselves Gaussian and can be determined analytically.",
            "Said will straight that have produced here the distribution for X1 loan which contained by basically projecting down the probability mass in each column down to the next one axis and you get this nice blue curve, which as you can see is itself Gaussian.",
            "So the second property I want identifies that the conditional distributions this Gaussian are themselves Gaussian and sort of to illustrate that I've taken the distribution for X1 conditioned on the fact that X2 is equal to minus 5.",
            "Which we obtained by just taking the probability mass along that dark black line and we get this nice red curve, which again is gas in its own right.",
            "So now I'd like to take that second plot the conditional distrib."
        ],
        [
            "And and rearrange it.",
            "I'd like to take their probability mass over X2 and project that into this second column.",
            "Now, because we've observed X2, we basically have a Delta distribution for what's going on with X2, hence that thin red line and we take the probability distribution for X, one.",
            "That is what's given by this red curve on the left hand side and put that in a slightly different way in the first column.",
            "There that is, we put simply it's mean and that dark red line and plus minus the single single standard deviation in the shaded pink.",
            "OK, so if you're happy with."
        ],
        [
            "Let's try generalizing it.",
            "To begin with, let's try generalizing to say, 10 different variables and have produced the conditional distribution for X1 through X10 conditioned on the fact that we've observed X, 2X5 and X9.",
            "And if you're happy with that, let's imagine generalizing one step further to a potentially infinite number of variables, and that's basically what a Gaussian processes.",
            "Now that's very interesting, because we're going to try treating functions as basically an infinitely infinite dimensional quantity.",
            "Basically, a list of all the possible functional outputs function evaluated at every single point in the domain, and hopefully it's clear now while those two properties identified earlier are important, because obviously we're never going to be able to observe this function at every single point, so we constantly have to be marginalizing out over all these other function values that we don't observe only because we've got this Gaussian distribution.",
            "Is that going to be.",
            "Analytic and nice.",
            "OK, so yeah, we can do inference about functions."
        ],
        [
            "That's pretty good throughout the remainder of the talk, I'm going to be focusing on functions of time XLT, but obviously everything will apply to any other functions you could come up with.",
            "So the first thing the Gaussian process gives us as a means."
        ],
        [
            "Best estimate for what that function is doing between and away from our observations, but."
        ],
        [
            "Top that, it also gives an indication of the uncertainty in that function, which obviously is small close.",
            "We've got observations, but grows larger elsewhere."
        ],
        [
            "Now JPS orgastic processes are often used for regression or classification on fixed assets.",
            "That is, we've got a fixed set of data when making predictions about a fixed set of points, but in fact, that's not what we're going to do throughout the remote."
        ],
        [
            "This talk we're going to use it.",
            "We're going to use GPS for tracking that is becoming constantly getting new data coming in and the point about which we're trying to make predictions."
        ],
        [
            "Going to be moving ever further forwards into the future.",
            "But Fortunately we can come up with that nice algorithms to allow us to do."
        ],
        [
            "Actually this week casting process as well.",
            "So."
        ],
        [
            "OK, now the beating heart of all this wonderful Gaussian process inference is what is called a covariance function, which basically tells the GP about how we expect our function to vary with input.",
            "So in our case time and there an enormous variety of different covariance functions, we can choose all expressing slightly different sets of prior information about the function we're trying to track.",
            "Now, one very common piece of prior information we want to express is that our function is smooth in some sort of sense.",
            "So if a function is very smooth, such As for example, this one plotted in dots on the right hand side there, we might use a covariance function like for example this squared exponential which I've plotted in dots on the left hand side here and you can see that that gives us very strong correlations for small separation in small separations in input space, and hopefully that's clear to you.",
            "That is exactly what we mean by smoothness.",
            "Basically that two values of a function that are not separated by much in input space are going to be very strongly correlated and thereby similar.",
            "If we expect that our function could be less smooth, we've got.",
            "Various other choices to allow us to do that.",
            "Here I've plotted few examples of the Materne covariance which has this variable parameter new, allowing us to control exactly how smooth we expect our function to be.",
            "But there's all kinds of other things we can."
        ],
        [
            "With that covariance functions as well.",
            "They're very, very flexible if we expect that our function might be periodic and potentially combined with some sort of long term drift, we can build covariance functions to model that.",
            "If we have multiple sensors or multiple channel channels, we can use covariance functions to express the fact that they might be correlated or delayed relative to one another."
        ],
        [
            "Because the focus of this talk is changepoints modeling, change points, and in our paper we've introduced a number of different families of covariance functions for exactly that.",
            "In particular, what we've done is to modify any standard stationary covariance function and turn it into a nonstationary changepoint covariance function.",
            "So the examples are part in these next couple of slides are taken from the modifications of the standard squared exponential covariance function which are plotted here.",
            "In this light black line anyway.",
            "So obviously the particular type of covariance function we choose is dependent on the kind of change points we expect to see.",
            "And the simplest type of change point you could expect is what we call a drastic change .1 in which the function values after the change point are completely uncorrelated with functions before you can see there's this nice clean break here.",
            "So an appropriate covariance function to model that has this son simple break in the correlations, it simply drops to 0 after the change point.",
            "Another common type of change point is 1 input scale.",
            "That is 1 in which are function suddenly becomes much smoother or less smooth.",
            "So here's an example in which that occurs and an appropriate covariance function is depicted on the top here, and you can see that after that change point, the degree of correlation we're expressing obviously get stronger due to that longer input scale.",
            "So again, I want to emphasize this is just a A."
        ],
        [
            "A sampling of the different types of covariance functions that were able to generate another one is, for example change point in output scale, which we might use a covariance function such as depicted here to represent and all of those covariance functions can be combined together and modified in various ways.",
            "So on the right hand side there have depicted covariance function.",
            "You might use to model A function which experiences a change point in both input and output scale simultaneously.",
            "So that might look something like this."
        ],
        [
            "OK, so.",
            "The cost of all this wonderful, you know, amount of flexibility that we have with covariance functions is that for each new degree of freedom we want to give them.",
            "Basically we have to introduce a new hyperparameter to control that degree of freedom.",
            "So if we want to talk about a periodic function, we have to introduce a period hyper hyper parameter and also an amplitude hyperparameter.",
            "If we want to talk about multiple correlated sensors, we have to introduce correlation high parameters to express exactly what those correlations are."
        ],
        [
            "And that's equally true for these changepoint covariances that we're proposing.",
            "So there are a number of parameters.",
            "These introduced the 1st and most important of course is the location of that change point or the the number of change points we might have.",
            "We also then have to introduce further high parameters to specify the characteristics of our data both before and after that change point.",
            "So in this example here, in which we've gotta change point in input scale, we need to have one high parameter to say what the input scale is before the change point another to say what is afterwards."
        ],
        [
            "OK, so we have all these high parameters floating around.",
            "What are we going to do them?",
            "Well, Fortunately we're good Bayesians.",
            "We're going to do the correct thing, which is to assign a prior distribution to all of them and then to marginalized them out.",
            "That is, to integrate out any uncertainty remains to possess about them.",
            "After having observed data.",
            "So the actual quantity we're interested in is something like this.",
            "That is the predictive distribution for extra our product ants conditioned on the fact that we've observed a number of credit doors that center so that requires us to form these two integrals, the ratio of integrals you can see there over the values of our hyperparameters.",
            "And those integrals have two kind of problematic terms there.",
            "There's the likelihood which I've shaded in red, and then the predictions shaded in blue.",
            "Now both of those terms separately have highly nontrivial dependence upon exactly what the value of those half parameters are.",
            "So unfortunately these integrals are going to turn out to be non analytic, and as such we're going to be forced to resort some kind of numerical integration quadrature.",
            "So what quarter inevitably involves is."
        ],
        [
            "Evaluation of average grand at a set of different samples.",
            "5 minutes left.",
            "OK.",
            "Sorry, sorry on.",
            "So we have to evaluate Orange.",
            "Granted a set of different samples.",
            "Now for this I'm going to focus on the second of those terms in our integrand, which is the predictions.",
            "So we're going to have to evaluate our predictions for a number of different samples in our high parameter space.",
            "So to illustrate that I'm going to take this simple example in which trying to perform prediction about very simple periodic function which has a real period of 1.",
            "Unfortunately, of course that's unknown to us a priority, and such we're going to have to take samples like for example .6, which gives us these very, very poor predictions.",
            "As you might expect, we might also take a sample of 2.08, which gives us better predictions, 'cause of course that's closer to a real harmonic of the rule.",
            "10.97 but note that even though this .97 sample is pretty close to the real period of 1 after sequels, 4 our predictions have this significant discrepancy from the real values, and worse, the very very arrogant you know that byte equals 4 error bars here shrunk down to basically 0.",
            "Due to the fact that you know when we give a Gaussian process just a single sample, it's assuming it knows exactly what that high parameter is, which is not the case."
        ],
        [
            "So the particular flavor of quadrature we're going to use is known as Bayesian Monte Carlo or base Hermite quadrature.",
            "And what that does is to assign another a second Gaussian process to our integrand as a function of those hyperparameters.",
            "So now focused on the second of the terms and the likelihood, and applauded here the log likelihood as a function of that high parameter though.",
            "So you see, we get high likelihoods for that .97 sample in the 2.08 sample for the GP also gives us an indication of what's going on between and away from those samples, giving us the best possible idea about what's going on over the whole domain."
        ],
        [
            "Right so at the end of the day, basically what color is just a very fancy way of assigning weights to our samples and to continue the example here we see how the weights change and a revised as we gather more data about our function.",
            "So to begin with, we have this very evenly weighted set of samples, but soon enough we workout that .97 sample is pretty much the best and we assign almost all of the weight to it.",
            "But we retain some weight with the samples either side and that allows us to improve upon the inference we obtained by just taking that .97 sample.",
            "In particular, note that our predictions that equals four both much closer to the real value and also have much more realistic error bars, much more representative of our real uncertainty.",
            "OK, so we're able to form integration too."
        ],
        [
            "Make predictions, but another kind of integral.",
            "We often want to do is that in order to produce posterior distributions in particular, posterior distributions for high parameters.",
            "So for that last example, something we might very well be interested in is the posterior distribution for our period high parameter after having observed all that data.",
            "And using Bayesian Monte Carlo techniques, we can both evaluate that full posterior and also it's mean giving us a nice kind of summary for what's going on."
        ],
        [
            "OK, and very quickly we use various iterative tricks in order to allow us to do all this online in some kind of sense."
        ],
        [
            "OK, so now back to our changepoint example.",
            "This is the data represented on the very first slide change in both input and output scale and on the bottom.",
            "Here I've got a standard GP that is a GP using a simple squared exponential covariance.",
            "And you can see that does fine, right up until the point that change point kicks in and then it just throws in the towel.",
            "It basically loses the plot completely, wiggles all over the place.",
            "As you can see and also again gives us very arrogant predictions predictions with a very small, very small Airbus on the top.",
            "There we've got the same example, but with predictions produced using our changepoint covariance and you can see they give us not only more accurate predictions but also much more representative error bars.",
            "Now coming back to that production."
        ],
        [
            "Of posteriors for high parameters.",
            "Remember the location of the change point is itself just a hyperparameter, so whatever produced here is the posterior distribution for that high parameter overtime.",
            "So to read this plot, the X axis is obviously time and the Y axis here is the time since the last change point.",
            "And you can see that once that change point kicks in, we latch onto the fact that it's code straight away and just follow it right through until it falls past the trailing edge of our window.",
            "No problems whatsoever.",
            "So now onto her."
        ],
        [
            "Real example, Brown moment is a network have been working on throughout my PhD.",
            "It's a network of wireless weather sensors deployed off the South Coast of England, making measurements of Tide Heights, air temperatures, all kinds of other things."
        ],
        [
            "And people like the port authorities and recreational sailors interested in it because it does give these very local recent observations of the weather.",
            "We're not going to do any kind of fancy climate modeling, or we're going to do is use our GPS to try and make accurate forecasts into the short term future.",
            "So a brief demo.",
            "Now here's our Bramble met demonstrator.",
            "So now I'm just putting tide height readings from this single set sensor in the just in Southampton itself.",
            "And the Red Cross is here represent those observations of the tide height on top of that I'm going to overlay the predictions made by a standard GP.",
            "That is the GP with a squared exponential covariance.",
            "So that's represented by Blue line giving it's mean and shaded region, giving it some standard deviation.",
            "So it does pretty well.",
            "Obviously it is as you'd expect, we're using a periodic covariance, so it's able to make very accurate predictions, but then after a little while there we get this sensor fault, which is actually a real fault we observed in the system.",
            "The ratings basically gets stuck out of reading of four meters, when obviously we know that I'd is not just being stuck at 4 meters.",
            "Unfortunately, JP here doesn't benefit from that knowledge, so after that fault occurs, it goes completely wild.",
            "It's got no idea what's going on.",
            "It thinks it should be periodic, so it tries to repeat that periodic pattern, but it's getting absolutely nowhere.",
            "On the other hand, I can flash up the predictions made using our changepoint covariance.",
            "And you can see that once in this case, once it sees that sensor fault, it works out exactly what's going on.",
            "It knows that those faulty readings are not at all informative about the real tight height, and as such it just continues to merrily make predictions using its knowledge of the periodicity in this data.",
            "And once the fault ends, it latches right back on Bob's, your uncle.",
            "Totally cool.",
            "Okie doke, so the final thing there unfortunate.",
            "Couldn't get this into the demo in time but.",
            "We can also produce the posterior discrete."
        ],
        [
            "On that example, for the location of change point.",
            "So there's the same data set plotted in slightly different way, and here's the posterior distribution for the location of that change point, and you can see that sure enough, as soon as it occurs, we latched right onto it, and when it stops, we latch onto that as well.",
            "Performing inference exactly as we should.",
            "OK, so that's more or less.",
            "All I wanted to say we've introduced this sequential Gaussian process algorithm, which is allowed us to perform prediction in the presence of change points.",
            "And yeah, that's it, thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so yeah, sequential Bayesian prediction in the presence of changepoints.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What do I mean by changepoint?",
                    "label": 0
                },
                {
                    "sent": "Well, Simply put an abrupt change in the underlying characteristics of the data we're trying to track.",
                    "label": 0
                },
                {
                    "sent": "So here's a very simple example in which this data we're trying to form predictions about undergoes some sort of change in its amplitude and its smoothness.",
                    "label": 0
                },
                {
                    "sent": "So we need some sort of way to deal with that and particular tool.",
                    "label": 0
                },
                {
                    "sent": "We're going to be using is a Gaussian process sound, some Gaussian process basics.",
                    "label": 0
                },
                {
                    "sent": "My apologies if you.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hopefully with it, so my Gaussian well, I'm a Gaussian distribution has a number of different properties that we're going to exploit, and I'd like to highlight a couple of them here using this simple contour plot.",
                    "label": 0
                },
                {
                    "sent": "Those are part of this bivariate Gaussian agasi and over 2 variables X2 and X1 in these contours and are the first thing I want to do that to illustrate is the property of the Gaussian that its marginal distributions are themselves Gaussian and can be determined analytically.",
                    "label": 0
                },
                {
                    "sent": "Said will straight that have produced here the distribution for X1 loan which contained by basically projecting down the probability mass in each column down to the next one axis and you get this nice blue curve, which as you can see is itself Gaussian.",
                    "label": 0
                },
                {
                    "sent": "So the second property I want identifies that the conditional distributions this Gaussian are themselves Gaussian and sort of to illustrate that I've taken the distribution for X1 conditioned on the fact that X2 is equal to minus 5.",
                    "label": 0
                },
                {
                    "sent": "Which we obtained by just taking the probability mass along that dark black line and we get this nice red curve, which again is gas in its own right.",
                    "label": 0
                },
                {
                    "sent": "So now I'd like to take that second plot the conditional distrib.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And and rearrange it.",
                    "label": 0
                },
                {
                    "sent": "I'd like to take their probability mass over X2 and project that into this second column.",
                    "label": 0
                },
                {
                    "sent": "Now, because we've observed X2, we basically have a Delta distribution for what's going on with X2, hence that thin red line and we take the probability distribution for X, one.",
                    "label": 0
                },
                {
                    "sent": "That is what's given by this red curve on the left hand side and put that in a slightly different way in the first column.",
                    "label": 0
                },
                {
                    "sent": "There that is, we put simply it's mean and that dark red line and plus minus the single single standard deviation in the shaded pink.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you're happy with.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's try generalizing it.",
                    "label": 0
                },
                {
                    "sent": "To begin with, let's try generalizing to say, 10 different variables and have produced the conditional distribution for X1 through X10 conditioned on the fact that we've observed X, 2X5 and X9.",
                    "label": 0
                },
                {
                    "sent": "And if you're happy with that, let's imagine generalizing one step further to a potentially infinite number of variables, and that's basically what a Gaussian processes.",
                    "label": 1
                },
                {
                    "sent": "Now that's very interesting, because we're going to try treating functions as basically an infinitely infinite dimensional quantity.",
                    "label": 0
                },
                {
                    "sent": "Basically, a list of all the possible functional outputs function evaluated at every single point in the domain, and hopefully it's clear now while those two properties identified earlier are important, because obviously we're never going to be able to observe this function at every single point, so we constantly have to be marginalizing out over all these other function values that we don't observe only because we've got this Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "Is that going to be.",
                    "label": 0
                },
                {
                    "sent": "Analytic and nice.",
                    "label": 0
                },
                {
                    "sent": "OK, so yeah, we can do inference about functions.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's pretty good throughout the remainder of the talk, I'm going to be focusing on functions of time XLT, but obviously everything will apply to any other functions you could come up with.",
                    "label": 0
                },
                {
                    "sent": "So the first thing the Gaussian process gives us as a means.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Best estimate for what that function is doing between and away from our observations, but.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Top that, it also gives an indication of the uncertainty in that function, which obviously is small close.",
                    "label": 0
                },
                {
                    "sent": "We've got observations, but grows larger elsewhere.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now JPS orgastic processes are often used for regression or classification on fixed assets.",
                    "label": 0
                },
                {
                    "sent": "That is, we've got a fixed set of data when making predictions about a fixed set of points, but in fact, that's not what we're going to do throughout the remote.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This talk we're going to use it.",
                    "label": 0
                },
                {
                    "sent": "We're going to use GPS for tracking that is becoming constantly getting new data coming in and the point about which we're trying to make predictions.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to be moving ever further forwards into the future.",
                    "label": 0
                },
                {
                    "sent": "But Fortunately we can come up with that nice algorithms to allow us to do.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually this week casting process as well.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now the beating heart of all this wonderful Gaussian process inference is what is called a covariance function, which basically tells the GP about how we expect our function to vary with input.",
                    "label": 1
                },
                {
                    "sent": "So in our case time and there an enormous variety of different covariance functions, we can choose all expressing slightly different sets of prior information about the function we're trying to track.",
                    "label": 0
                },
                {
                    "sent": "Now, one very common piece of prior information we want to express is that our function is smooth in some sort of sense.",
                    "label": 0
                },
                {
                    "sent": "So if a function is very smooth, such As for example, this one plotted in dots on the right hand side there, we might use a covariance function like for example this squared exponential which I've plotted in dots on the left hand side here and you can see that that gives us very strong correlations for small separation in small separations in input space, and hopefully that's clear to you.",
                    "label": 0
                },
                {
                    "sent": "That is exactly what we mean by smoothness.",
                    "label": 0
                },
                {
                    "sent": "Basically that two values of a function that are not separated by much in input space are going to be very strongly correlated and thereby similar.",
                    "label": 0
                },
                {
                    "sent": "If we expect that our function could be less smooth, we've got.",
                    "label": 0
                },
                {
                    "sent": "Various other choices to allow us to do that.",
                    "label": 0
                },
                {
                    "sent": "Here I've plotted few examples of the Materne covariance which has this variable parameter new, allowing us to control exactly how smooth we expect our function to be.",
                    "label": 0
                },
                {
                    "sent": "But there's all kinds of other things we can.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "With that covariance functions as well.",
                    "label": 1
                },
                {
                    "sent": "They're very, very flexible if we expect that our function might be periodic and potentially combined with some sort of long term drift, we can build covariance functions to model that.",
                    "label": 0
                },
                {
                    "sent": "If we have multiple sensors or multiple channel channels, we can use covariance functions to express the fact that they might be correlated or delayed relative to one another.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Because the focus of this talk is changepoints modeling, change points, and in our paper we've introduced a number of different families of covariance functions for exactly that.",
                    "label": 1
                },
                {
                    "sent": "In particular, what we've done is to modify any standard stationary covariance function and turn it into a nonstationary changepoint covariance function.",
                    "label": 0
                },
                {
                    "sent": "So the examples are part in these next couple of slides are taken from the modifications of the standard squared exponential covariance function which are plotted here.",
                    "label": 0
                },
                {
                    "sent": "In this light black line anyway.",
                    "label": 0
                },
                {
                    "sent": "So obviously the particular type of covariance function we choose is dependent on the kind of change points we expect to see.",
                    "label": 0
                },
                {
                    "sent": "And the simplest type of change point you could expect is what we call a drastic change .1 in which the function values after the change point are completely uncorrelated with functions before you can see there's this nice clean break here.",
                    "label": 0
                },
                {
                    "sent": "So an appropriate covariance function to model that has this son simple break in the correlations, it simply drops to 0 after the change point.",
                    "label": 0
                },
                {
                    "sent": "Another common type of change point is 1 input scale.",
                    "label": 0
                },
                {
                    "sent": "That is 1 in which are function suddenly becomes much smoother or less smooth.",
                    "label": 0
                },
                {
                    "sent": "So here's an example in which that occurs and an appropriate covariance function is depicted on the top here, and you can see that after that change point, the degree of correlation we're expressing obviously get stronger due to that longer input scale.",
                    "label": 0
                },
                {
                    "sent": "So again, I want to emphasize this is just a A.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A sampling of the different types of covariance functions that were able to generate another one is, for example change point in output scale, which we might use a covariance function such as depicted here to represent and all of those covariance functions can be combined together and modified in various ways.",
                    "label": 0
                },
                {
                    "sent": "So on the right hand side there have depicted covariance function.",
                    "label": 0
                },
                {
                    "sent": "You might use to model A function which experiences a change point in both input and output scale simultaneously.",
                    "label": 0
                },
                {
                    "sent": "So that might look something like this.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "The cost of all this wonderful, you know, amount of flexibility that we have with covariance functions is that for each new degree of freedom we want to give them.",
                    "label": 0
                },
                {
                    "sent": "Basically we have to introduce a new hyperparameter to control that degree of freedom.",
                    "label": 0
                },
                {
                    "sent": "So if we want to talk about a periodic function, we have to introduce a period hyper hyper parameter and also an amplitude hyperparameter.",
                    "label": 0
                },
                {
                    "sent": "If we want to talk about multiple correlated sensors, we have to introduce correlation high parameters to express exactly what those correlations are.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's equally true for these changepoint covariances that we're proposing.",
                    "label": 0
                },
                {
                    "sent": "So there are a number of parameters.",
                    "label": 0
                },
                {
                    "sent": "These introduced the 1st and most important of course is the location of that change point or the the number of change points we might have.",
                    "label": 0
                },
                {
                    "sent": "We also then have to introduce further high parameters to specify the characteristics of our data both before and after that change point.",
                    "label": 0
                },
                {
                    "sent": "So in this example here, in which we've gotta change point in input scale, we need to have one high parameter to say what the input scale is before the change point another to say what is afterwards.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so we have all these high parameters floating around.",
                    "label": 0
                },
                {
                    "sent": "What are we going to do them?",
                    "label": 0
                },
                {
                    "sent": "Well, Fortunately we're good Bayesians.",
                    "label": 0
                },
                {
                    "sent": "We're going to do the correct thing, which is to assign a prior distribution to all of them and then to marginalized them out.",
                    "label": 1
                },
                {
                    "sent": "That is, to integrate out any uncertainty remains to possess about them.",
                    "label": 0
                },
                {
                    "sent": "After having observed data.",
                    "label": 0
                },
                {
                    "sent": "So the actual quantity we're interested in is something like this.",
                    "label": 0
                },
                {
                    "sent": "That is the predictive distribution for extra our product ants conditioned on the fact that we've observed a number of credit doors that center so that requires us to form these two integrals, the ratio of integrals you can see there over the values of our hyperparameters.",
                    "label": 0
                },
                {
                    "sent": "And those integrals have two kind of problematic terms there.",
                    "label": 0
                },
                {
                    "sent": "There's the likelihood which I've shaded in red, and then the predictions shaded in blue.",
                    "label": 0
                },
                {
                    "sent": "Now both of those terms separately have highly nontrivial dependence upon exactly what the value of those half parameters are.",
                    "label": 1
                },
                {
                    "sent": "So unfortunately these integrals are going to turn out to be non analytic, and as such we're going to be forced to resort some kind of numerical integration quadrature.",
                    "label": 0
                },
                {
                    "sent": "So what quarter inevitably involves is.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Evaluation of average grand at a set of different samples.",
                    "label": 0
                },
                {
                    "sent": "5 minutes left.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Sorry, sorry on.",
                    "label": 0
                },
                {
                    "sent": "So we have to evaluate Orange.",
                    "label": 0
                },
                {
                    "sent": "Granted a set of different samples.",
                    "label": 1
                },
                {
                    "sent": "Now for this I'm going to focus on the second of those terms in our integrand, which is the predictions.",
                    "label": 0
                },
                {
                    "sent": "So we're going to have to evaluate our predictions for a number of different samples in our high parameter space.",
                    "label": 1
                },
                {
                    "sent": "So to illustrate that I'm going to take this simple example in which trying to perform prediction about very simple periodic function which has a real period of 1.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, of course that's unknown to us a priority, and such we're going to have to take samples like for example .6, which gives us these very, very poor predictions.",
                    "label": 0
                },
                {
                    "sent": "As you might expect, we might also take a sample of 2.08, which gives us better predictions, 'cause of course that's closer to a real harmonic of the rule.",
                    "label": 0
                },
                {
                    "sent": "10.97 but note that even though this .97 sample is pretty close to the real period of 1 after sequels, 4 our predictions have this significant discrepancy from the real values, and worse, the very very arrogant you know that byte equals 4 error bars here shrunk down to basically 0.",
                    "label": 0
                },
                {
                    "sent": "Due to the fact that you know when we give a Gaussian process just a single sample, it's assuming it knows exactly what that high parameter is, which is not the case.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the particular flavor of quadrature we're going to use is known as Bayesian Monte Carlo or base Hermite quadrature.",
                    "label": 0
                },
                {
                    "sent": "And what that does is to assign another a second Gaussian process to our integrand as a function of those hyperparameters.",
                    "label": 1
                },
                {
                    "sent": "So now focused on the second of the terms and the likelihood, and applauded here the log likelihood as a function of that high parameter though.",
                    "label": 0
                },
                {
                    "sent": "So you see, we get high likelihoods for that .97 sample in the 2.08 sample for the GP also gives us an indication of what's going on between and away from those samples, giving us the best possible idea about what's going on over the whole domain.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right so at the end of the day, basically what color is just a very fancy way of assigning weights to our samples and to continue the example here we see how the weights change and a revised as we gather more data about our function.",
                    "label": 1
                },
                {
                    "sent": "So to begin with, we have this very evenly weighted set of samples, but soon enough we workout that .97 sample is pretty much the best and we assign almost all of the weight to it.",
                    "label": 0
                },
                {
                    "sent": "But we retain some weight with the samples either side and that allows us to improve upon the inference we obtained by just taking that .97 sample.",
                    "label": 0
                },
                {
                    "sent": "In particular, note that our predictions that equals four both much closer to the real value and also have much more realistic error bars, much more representative of our real uncertainty.",
                    "label": 1
                },
                {
                    "sent": "OK, so we're able to form integration too.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Make predictions, but another kind of integral.",
                    "label": 0
                },
                {
                    "sent": "We often want to do is that in order to produce posterior distributions in particular, posterior distributions for high parameters.",
                    "label": 1
                },
                {
                    "sent": "So for that last example, something we might very well be interested in is the posterior distribution for our period high parameter after having observed all that data.",
                    "label": 0
                },
                {
                    "sent": "And using Bayesian Monte Carlo techniques, we can both evaluate that full posterior and also it's mean giving us a nice kind of summary for what's going on.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and very quickly we use various iterative tricks in order to allow us to do all this online in some kind of sense.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so now back to our changepoint example.",
                    "label": 0
                },
                {
                    "sent": "This is the data represented on the very first slide change in both input and output scale and on the bottom.",
                    "label": 0
                },
                {
                    "sent": "Here I've got a standard GP that is a GP using a simple squared exponential covariance.",
                    "label": 1
                },
                {
                    "sent": "And you can see that does fine, right up until the point that change point kicks in and then it just throws in the towel.",
                    "label": 0
                },
                {
                    "sent": "It basically loses the plot completely, wiggles all over the place.",
                    "label": 0
                },
                {
                    "sent": "As you can see and also again gives us very arrogant predictions predictions with a very small, very small Airbus on the top.",
                    "label": 0
                },
                {
                    "sent": "There we've got the same example, but with predictions produced using our changepoint covariance and you can see they give us not only more accurate predictions but also much more representative error bars.",
                    "label": 0
                },
                {
                    "sent": "Now coming back to that production.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of posteriors for high parameters.",
                    "label": 0
                },
                {
                    "sent": "Remember the location of the change point is itself just a hyperparameter, so whatever produced here is the posterior distribution for that high parameter overtime.",
                    "label": 1
                },
                {
                    "sent": "So to read this plot, the X axis is obviously time and the Y axis here is the time since the last change point.",
                    "label": 0
                },
                {
                    "sent": "And you can see that once that change point kicks in, we latch onto the fact that it's code straight away and just follow it right through until it falls past the trailing edge of our window.",
                    "label": 0
                },
                {
                    "sent": "No problems whatsoever.",
                    "label": 0
                },
                {
                    "sent": "So now onto her.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Real example, Brown moment is a network have been working on throughout my PhD.",
                    "label": 0
                },
                {
                    "sent": "It's a network of wireless weather sensors deployed off the South Coast of England, making measurements of Tide Heights, air temperatures, all kinds of other things.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And people like the port authorities and recreational sailors interested in it because it does give these very local recent observations of the weather.",
                    "label": 1
                },
                {
                    "sent": "We're not going to do any kind of fancy climate modeling, or we're going to do is use our GPS to try and make accurate forecasts into the short term future.",
                    "label": 0
                },
                {
                    "sent": "So a brief demo.",
                    "label": 0
                },
                {
                    "sent": "Now here's our Bramble met demonstrator.",
                    "label": 0
                },
                {
                    "sent": "So now I'm just putting tide height readings from this single set sensor in the just in Southampton itself.",
                    "label": 0
                },
                {
                    "sent": "And the Red Cross is here represent those observations of the tide height on top of that I'm going to overlay the predictions made by a standard GP.",
                    "label": 0
                },
                {
                    "sent": "That is the GP with a squared exponential covariance.",
                    "label": 0
                },
                {
                    "sent": "So that's represented by Blue line giving it's mean and shaded region, giving it some standard deviation.",
                    "label": 0
                },
                {
                    "sent": "So it does pretty well.",
                    "label": 0
                },
                {
                    "sent": "Obviously it is as you'd expect, we're using a periodic covariance, so it's able to make very accurate predictions, but then after a little while there we get this sensor fault, which is actually a real fault we observed in the system.",
                    "label": 0
                },
                {
                    "sent": "The ratings basically gets stuck out of reading of four meters, when obviously we know that I'd is not just being stuck at 4 meters.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, JP here doesn't benefit from that knowledge, so after that fault occurs, it goes completely wild.",
                    "label": 0
                },
                {
                    "sent": "It's got no idea what's going on.",
                    "label": 0
                },
                {
                    "sent": "It thinks it should be periodic, so it tries to repeat that periodic pattern, but it's getting absolutely nowhere.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, I can flash up the predictions made using our changepoint covariance.",
                    "label": 0
                },
                {
                    "sent": "And you can see that once in this case, once it sees that sensor fault, it works out exactly what's going on.",
                    "label": 0
                },
                {
                    "sent": "It knows that those faulty readings are not at all informative about the real tight height, and as such it just continues to merrily make predictions using its knowledge of the periodicity in this data.",
                    "label": 0
                },
                {
                    "sent": "And once the fault ends, it latches right back on Bob's, your uncle.",
                    "label": 0
                },
                {
                    "sent": "Totally cool.",
                    "label": 0
                },
                {
                    "sent": "Okie doke, so the final thing there unfortunate.",
                    "label": 0
                },
                {
                    "sent": "Couldn't get this into the demo in time but.",
                    "label": 0
                },
                {
                    "sent": "We can also produce the posterior discrete.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On that example, for the location of change point.",
                    "label": 0
                },
                {
                    "sent": "So there's the same data set plotted in slightly different way, and here's the posterior distribution for the location of that change point, and you can see that sure enough, as soon as it occurs, we latched right onto it, and when it stops, we latch onto that as well.",
                    "label": 1
                },
                {
                    "sent": "Performing inference exactly as we should.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's more or less.",
                    "label": 0
                },
                {
                    "sent": "All I wanted to say we've introduced this sequential Gaussian process algorithm, which is allowed us to perform prediction in the presence of change points.",
                    "label": 0
                },
                {
                    "sent": "And yeah, that's it, thanks.",
                    "label": 0
                }
            ]
        }
    }
}