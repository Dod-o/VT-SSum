{
    "id": "6wtj2d44ll6syzbrv6lpptrbeowfaotx",
    "title": "Topic Models",
    "info": {
        "author": [
            "David Blei, Computer Science Department, Princeton University"
        ],
        "published": "Nov. 2, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlss09uk_blei_tm/",
    "segmentation": [
        [
            "OK, welcome to September 1st.",
            "My name is Dave Bly.",
            "I'm going to talk about topic models.",
            "I'm from Whoops, I didn't mean to do that.",
            "I'm from Princeton University in New Jersey where it's 4:00 o'clock in the morning.",
            "And so.",
            "This is what it's going to look like to teach at 4:00 in the morning.",
            "Um?",
            "OK, and please interrupt me a lot in this talk so that it will help.",
            "Stimulate me to wake me up.",
            "OK. Good So what topic modeling is about is that as."
        ],
        [
            "You all know it's almost redundant.",
            "More information becomes available to us.",
            "It becomes more difficult for us to be able to quickly access it and search for things in it and understand it and basically get something out of it.",
            "And so we need new tools, new algorithmic tools to help us organize, search and understand these vast amounts of information.",
            "Things like text, archives and image archives, and all different kinds of data that we're all just.",
            "Both creating and having access to constantly.",
            "So what topic modeling provides are?"
        ],
        [
            "Our methods for automatically organizing, understanding, searching, summarizing, exploiting these large electronic archives, and the basic idea are is, are these three steps.",
            "First we take a big corpus like Wikipedia say, and uncover the hidden topical patterns that pervade it so.",
            "Wikipedia's got some number of millions of articles and there about some things.",
            "Those things overlap and we want to uncover what those things are.",
            "What are the different topics that pervade this collection?",
            "Then we want to annotate the documents according to those topics, so I understand what they say.",
            "You know, 250 topics in Wikipedia are now.",
            "If I pluck a document out of Wikipedia, what topics is that article about?",
            "How can I annotate that article according to the topics I've discovered in step one and finally?",
            "We want to use these annotations to organize, summarize, and do whatever it is we want to do.",
            "Obviously will not obviously, but doing this in a vacuum is only so interesting.",
            "We really want to say OK, Now that I've been able to essentially mark up my collection according to these automatically found topics I want to use that marked up collection.",
            "You can think of it as if I had.",
            "10,000,000 people to go through Wikipedia and Anne carefully organize it, then what would I do with their organization since I don't have those 10,000,000 people?",
            "I built algorithms to do that for me.",
            "OK. Oops.",
            "Oh, this is backwards.",
            "It's the same.",
            "Have been almost hit by cars about 6 times already, so same idea, OK?",
            "So what?"
        ],
        [
            "You can do with with topic models or things like discovered topics from corpus.",
            "So here are some topics.",
            "So a topic is going to be a distribution over terms in a vocabulary, and these are the top most probable terms and four topics that were uncovered by analyzing the text in the journal Science.",
            "So here's a topic.",
            "Human genome DNA, genetic evolution, evolutionary species, organisms disease, host bacteria, diseases, computer models, information data, computers.",
            "These are words that seem to go together in some kind of thematic, thematically coherent way.",
            "But I should say that part of this talk probably on Thursday we're going to discuss the pitfalls of overly interpreting these probability distributions over words, But for now, since we're learning about it, we can pretend like those pitfalls don't exist and interpret them.",
            "So these are four topics discovered from the journal Science.",
            "OK. Oh yeah.",
            "Don't do standing like.",
            "There's no good reason not to send.",
            "Yeah, so you know sometimes stemming.",
            "I mean, when I when I fit a topic model, sometimes I use stemming.",
            "It can be when you're looking at these topics.",
            "If you're doing this for like a system for someone to be able to to quickly browse and navigate through documents, some stemmers can be overly word aggressive and you can't really recognize the routes that they they give back.",
            "But then there's one that I know my student likes.",
            "I can't remember what it's called, but it's a more conservative stemmer.",
            "Just it will remove things like computer and computers.",
            "OK.",
            "But no, there's stemming is a good idea.",
            "Other questions."
        ],
        [
            "Things like so discovering topics, discovering topics, and understanding how the words change, how within the topic through time.",
            "So here are here is time from 1880 until 2000 and here are two different topics that I've named again ignoring the pitfalls of naming topics.",
            "And here are some words in those topics and you can see how their probability is changing overtime.",
            "OK, we can we can uncover these time changing topics with top."
        ],
        [
            "Models we can model connections between topics so.",
            "Here again are lists of words that we are interpreting as topics.",
            "And again, this is from science.",
            "The science articles, and here are topics and how they tend to Co occur with other topics.",
            "So you see here we have this topic.",
            "Ancient found impact million years ago in Africa, and it connects to this topic, fossil record birds, fossils, dinosaurs, fossil.",
            "OK, those two topics tend to Co occur, whereas this Dinosaur topic everyone loves dinosaurs doesn't.",
            "Connect as much with, say, this topic about mice antigen T cells, antigens.",
            "There's this issue again and immune response.",
            "OK, so finding connections between topics.",
            "And finally, we can use these tools not just on."
        ],
        [
            "Text data, but on lots of different kinds of data.",
            "For example on images.",
            "Topic modeling has has helped make some progress in computer vision, and we can also combine these different types of data type.",
            "These different types of data.",
            "Here we're using the topic model to automatically annotate images.",
            "So in this model you the input to the algorithm are images and words that describe them, and again by using this set of ideas that we're going to be discussing for the next while, you can take a raw image after fitting the model.",
            "And automatically annotate it.",
            "Here we have this image of a fish and the annotation is fish, water, ocean tree, coral.",
            "OK, it's not perfect.",
            "There's I don't think there's a tree in here, but.",
            "This is another another application of topic modeling and there the idea is that the same way that we think of.",
            "I don't have a picture of a document anywhere the same way we think of a document is a collection of words you can think of an image as a collection of image features, either by running some algorithms on it in advance to segment it and then computing features of each segment, or by doing something silly like just gritting it and computing features of each square in the grid.",
            "And then you can treat the image like a document.",
            "OK, I was deciding between coffee and water just then.",
            "OK, so."
        ],
        [
            "That's kind of 1 perspective or motivation for topic modeling, since this is a machine learning summer school.",
            "You know, sometimes I think of topic modeling.",
            "On one hand, it's about taking our intuitions and maybe other scholarship about the structure of language and images and encoding it into good machine learning algorithms.",
            "But from the machine learning perspective, topic modeling can be seen as a case study in applying basically hierarchical Bayesian models to group data like documents or images and and.",
            "Thinking of topic modeling as an application of these ideas, it really touches on a lot of different pieces in the applied machine statistical machine learning world, things like directed graphical models conjugate priors, an non conjugate priors time series modeling modeling with graphs, hierarchical Bayesian methods, fast approximate posterior inference like MCMC or variational methods, exploratory data analysis, model selection, nonparametric Bayesian methods, an mixed membership models so.",
            "In this talk, we're going to touch on all of these.",
            "All these topics I know you're going to go into these topics in more detail in other lectures in the school, but.",
            "Princeton any of these things I would encourage you to think of topic modeling as a possible application for for your ideas.",
            "In one of these contexts, it's a.",
            "It's a nice way, for example, to test out new methods of approximate posterior inference and so on.",
            "OK. OK, so.",
            "The way I had planned this is that we'd start by talking about latent dearest."
        ],
        [
            "Allocation, which is like the simplest topic model I guess.",
            "Then discuss in some detail approximate posterior inference.",
            "I was going to talk about Gibbs sampling and variational inference and then talk a little bit about comparing the different approximate posterior inference algorithms that are available to you and give some advice.",
            "Apparently I didn't.",
            "I don't know.",
            "I think I might have written that before I made the slides, 'cause I don't think there's much advice in there.",
            "And then in the second part I want to talk about.",
            "Taking the assumptions of LDA and relaxing them in various ways, so one is to build topic models for prediction relation and supervised topic models.",
            "The other is to relax the Dirichlet assumption and hence the conjugacy assumption and look at the logistic normal as a tool to building different kinds of topic models.",
            "There will discuss dynamic and correlated topic models, and finally, briefly, I'll talk about Infinite Topic models which is otherwise known as or, which is an application of the hierarchical Dirichlet process.",
            "I know ETA will.",
            "Discuss that in much more detail next week.",
            "Then finally I want to discuss interpreting and evaluating topic models, which is kind of a thorn in the side of topic models to be Frank, but we'll talk about it.",
            "Yeah.",
            "Make the assumption that documents are bags of words all the time.",
            "You're sort of making it.",
            "Well, I haven't talked about anything yet, but"
        ],
        [
            "Right, it does seem like I'm going to make that assumption the whole time.",
            "And yes, I will.",
            "Yeah, but I'll be.",
            "But I'll be totally honest with you about it.",
            "Yeah, that's right, yeah, yeah, no, I I, that's right.",
            "Um?",
            "Yeah, so to answer the question, I like that question.",
            "Yes, I think we're going to assume that documents are bags of words everywhere, but I'll try to point you to some.",
            "There's been some excellent work out there that takes the same ideas and relaxes that assumption.",
            "Basically allows documents to be to have Markovian structure, for example.",
            "But yes, we will be explicit and we will assume that documents are bags of words here.",
            "These are exchangeable, exchangeable models.",
            "Good question.",
            "I'm sorry, I answered it with a joke.",
            "I often do that.",
            "It's no offense.",
            "Other questions that will be politely answered.",
            "OK. Um?",
            "OK. Talk about that, alright?"
        ],
        [
            "So now we'll start this talk in earnest.",
            "So."
        ],
        [
            "Latent seriously allocation is a probabilistic model, and I'm sure you."
        ],
        [
            "Heard this already in this in this series, but the idea behind probabilistic modeling is to 1 treat your data as observations that arise from some kind of generative probabilistic process.",
            "This is generative probabilistic modeling I should say.",
            "And one that includes hidden variables structure that that we want to find in the data.",
            "So for documents those hidden variables reflect the thematic structure of the collection that we don't have access to.",
            "Step 2 is to then infer that hidden structure using posterior inference.",
            "Basically, we're going to contemplate and compute the conditional distribution of the hidden variables given the observations, which is which are the documents themselves.",
            "Third, we want to situate new data into the estimated model.",
            "Typically, sometimes you might want to just analyze your documents and then look at the hidden variables as is and never, never believe that you'll see another piece of data again, but often you want to take your model and then you have some new data coming in that you want to do something with.",
            "And so you need to be able to situate that new data into the estimated model.",
            "In other words, how does this query or new document fit into the topic structure that I learned in the first part OK?",
            "OK, so the intuition so with."
        ],
        [
            "This kind of general recipe for probabilistic modeling, the intuition behind latent garishly allocation or LDA, is simply that documents exhibit multiple topics.",
            "OK, so here is an example.",
            "I think there's a way to do this.",
            "Oh, OK. Um?",
            "Doesn't matter.",
            "Oh good so.",
            "Here's an example, so this is an article from the journal Science called Seeking Life's bare genetic necessities, and this article is basically about computing the number of genes that an Organism needs to survive.",
            "Missionary evolutionarily and what I've done is I've highlighted different words in this article with different colors, so words like predictions, computer analysis, computational computer numbers.",
            "I manually highlighted those in blue.",
            "These are words about data analysis.",
            "Words like genes genomes sequenced.",
            "These are highlighted in yellow.",
            "These are words about genomics and words like life and organisms and survive words about evolutionary biology.",
            "I've highlighted these words in pink.",
            "OK, so the intuition that I'm trying to convey is that this document somehow combines words about computer analysis.",
            "Words about evolutionary biology and words about genomics.",
            "OK, in contrast to the assumptions made by a mixture model, which says that.",
            "All of these words come from a single component.",
            "A single topic.",
            "OK, so with that intuition in mind, the idea is to.",
            "Express it as a generative probabilistic process and the way that works is."
        ],
        [
            "So.",
            "First, we are going to posit that there are some number of topics which will now formally define that live outside of the document collection.",
            "OK, so here I have four topics listed.",
            "There might be 96 underneath it, and each topic is a distribution over terms in the vocabulary, right?",
            "So there's a fixed vocabulary.",
            "We're going to assume that and every topic is a distribution over that fixed vocabulary, but different topics.",
            "Have different words with different probabilities.",
            "So for example, at the top I see a topic that has worked here.",
            "Let's say I've ordered these words in order of their probabilities.",
            "So at the top we have a topic that has gene with probability .4, DNA with probability .02.",
            "Genetically probability .1 and so on.",
            "OK, so I'm calling about the genetics topic underneath it, the pink topic, the word life is probably .02 evolve .1 an Organism .1.",
            "Then I have a topic and green about neuroscience.",
            "With words like brain and neuron and nerve with high probability and finally a topic data number in computer with high probability.",
            "OK so when I say topic, what I'm going to mean is distribution over fixed vocabulary, but that's cumbersome.",
            "So we say topic.",
            "Yeah.",
            "Can they wet?",
            "Yeah, that's an important point that every topic contains a probability for every word.",
            "So even though it doesn't have high probability, the word data has some probability in the yellow topic, and it might be that if you have a word that a word can have high probability in two topics.",
            "For example, example, you're probably sick of the word bank, could have probability in a topic about financial instruments, an also high probability in a topic about bodies of water.",
            "For riverbank other questions.",
            "OK, good.",
            "I will will get there.",
            "Yep, good question.",
            "That's going to be the next the next thing, OK, so let's assume for now that those probabilities are all there, and we've got our topics, and there's 100 of them.",
            "The generative process for each document then works like this.",
            "First, we're going to choose a distribution over our topics.",
            "So while a topic is a distribution over terms, a distribution over topics is a distribution over these 100 elements.",
            "OK, so there are 100 topics.",
            "Then this distribution.",
            "Has 100 possible possible values, each one color coded by its topic, and here I've chosen one that has pink with some probability yellow with some probability and blue with some probability clear, and we're going to draw that from a Dirichlet distribution.",
            "Have you seen the Dirichlet yet?",
            "Have you seen the dearsley yet in this series?",
            "No, OK, so good.",
            "We'll talk about that.",
            "This is drawn from a distribution over distributions called Adi Richley and.",
            "That's the first step in generating a document.",
            "The second step is to repeatedly draw a coin from this distribution.",
            "So here I drew a blue coin, look up what topic that blue coin refers to, the blue topic.",
            "That's why you gotta color code.",
            "It's really important and then choose the word from that distribution.",
            "OK, so here I rolled this 100 sided die.",
            "I got blue.",
            "I drew a word analysis from the blue distribution.",
            "OK here I drew.",
            "I can't point this way.",
            "Can see.",
            "I'm not good with Shadow, so will do it that way.",
            "Here I chose yellow, I got the yellow coin and then I looked up the yellow distribution and got the word genome and somewhere else I chose the pink coin and I got the word Organism in life and now going back to your question you can see that.",
            "No thanks, even the jumping kind of was good, but this can't last with that also.",
            "Oh, that's nice.",
            "It's green.",
            "Green is good, you know it's more positive than red, which is like stop green is go.",
            "What were we saying?",
            "Oh yeah, so.",
            "So we choose these coins and then we choose the word from the corresponding distribution.",
            "Here we chose the yellow coin, we choose the word genetic.",
            "Here we chose the pink coin, we choose the word Organism and so on and so forth.",
            "And this gets to your question.",
            "I'm implicitly assuming now that the order of words doesn't matter.",
            "OK, because I'm choosing these coins independently of each other.",
            "Now, of course, documents aren't really made this way.",
            "If they were, they would be totally unreadable, but if you.",
            "I should say when they are, they are totally unreadable, but.",
            "If you had a document whose words were all shuffled and you looked at looked at those words, you might be able to get a sense that, oh, this is a document about genetics, computation and evolutionary biology by looking at the various types of words that occur in the document.",
            "OK, so it's important that the generative process doesn't have to be.",
            "A precise description or a plausable description of how the document arose, in fact doesn't even have to generate documents that look realistic.",
            "It just has to be a process that makes sense for your goal, which is in looking at the posterior finding these.",
            "Thematically coherent terms and a little later, we'll discuss how one well, we won't discuss it much, but will try to think about how this generative process leads to these types of probabilities in the posterior.",
            "But we will get there.",
            "But is this generative process clear?",
            "So this is important, the whole rest of the talk depends on it.",
            "First choose a distribution over topics.",
            "Then for each for each word, draw a colored coin from this distribution look up the distribution over terms associated with that coin and draw the word from that distribution.",
            "And then that's the generative process for a single document, and then for another document we repeat the same process.",
            "So notice that documents are going to have different distributions over topics, so while this document is about the pink.",
            "Yellow and blue topic.",
            "Another document might be about the green and blue topic, a document about computational neuroscience, for example.",
            "OK, we repeat this process for every document clear.",
            "Any questions yeah?",
            "We're going to get right.",
            "We're going to get this is this is just the imaginary generative probabilistic process that we're assuming our data came from, and what we're assuming in particular is that we've got these, and we've got a process for this.",
            "OK, other questions.",
            "OK.",
            "So the problem is of course we don't get to see any of this, right?",
            "This is what we're imagining is there, and this is what we'd like to exploit and use.",
            "But in reality all we have is a big stack of 10,000,000 documents.",
            "So our goal."
        ],
        [
            "Is going to be to infer the underlying topic structure given all these documents?",
            "First of all, possibly most interesting.",
            "What are the topics that generated them under?",
            "This is under these assumptions.",
            "What are the distributions over terms that generated them under these assumptions and for each document, what is the distribution over topics associated with that document?",
            "And maybe we care about for each word which topic generated each word.",
            "OK, and that's the algorithmic problem here.",
            "Is to is to compute this this posterior distribution.",
            "This is a conditional distribution of all of these latent variables given the observations, which are the words of the documents.",
            "Clear.",
            "Not clear.",
            "No, not not clear good.",
            "OK.",
            "So that's sort of the fun part and it's over.",
            "Um?"
        ],
        [
            "I'm serious so.",
            "We're going to code this model in a graphical model directed graphical model, so I want to I know you've seen these already.",
            "I want to just quickly review them so that we're on the same page about the semantics of directed graphical models.",
            "In directed graphical model, a node is a random variable.",
            "So here I have nodes Y&X one or XN edges denote possible dependence between random variables.",
            "So here the edge between Y and X1 means that X1 might depend on YXN might depend on Y as well.",
            "Observed variables are shaded, so here I've observed X one through NI have not observed why?",
            "Why is a hidden variable or a latent variable?",
            "Those you can use interchangeably and plates denote replicated structure.",
            "So instead of having to waste your ink on this big figure, you can shorthand it with.",
            "Y goes to XN and draw a box around XN, which is called a plate and put big N in the lower right hand corner to denote that we have Big Ten of these little X ends, and they're all dependent, possibly on Y.",
            "Clear.",
            "OK.",
            "So the structure of this graph."
        ],
        [
            "Defines the pattern of conditional independence ease that are encoded in the joint distribution of these random variables.",
            "So from this graph we can read off all of the possible conditional independence.",
            "Ease that among these random variables and and the structure of the graph also defines a factorization of the joint distribution of these random variables.",
            "In particular here.",
            "This here is the joint distribution of all of those random variables Y&X one through XN, and this graph means that this joint distribution can be written as.",
            "P of Y times the product from N1 to N of P of XN given Y, so you notice that.",
            "That just from this joint distribution you can see that X the X is are all conditionally independent given Y. OK, so so this and this and this all mean exactly the same thing.",
            "Questions.",
            "Good.",
            "So with this simple little language in our hands, we can write down the latent."
        ],
        [
            "Seriously, allocation model that I just described for you.",
            "OK, and you can see why we need those plates so.",
            "Each piece of this structure now is a random variable and let's the easiest way to understand this is to sandwich in from the outside into the middle.",
            "OK, so the first thing I'll tell you is to ignore Ada and Alpha for now and Theta Theta sub DD is the document replication are the topic proportions that was that cartoon histogram that I drew for you with the pink in the blue and the yellow bars?",
            "OK so that's called Theta D topic proportions.",
            "We have one of these for every document it's in the D plate.",
            "Now.",
            "Oh, I should have started over here.",
            "So beta K are the topics themselves.",
            "That's where we start.",
            "So each beta is a distribution over terms and we have K of them.",
            "So K might be 100.",
            "Alright so beta sub 96 is some distribution over words so beta lives on what we call the simplex.",
            "The vocabulary simplex.",
            "It's the space of all possible distributions, OK, and I'm assuming here that beta comes from a Dirichlet distribution, which is a distribution over these over in this type of space.",
            "OK, so these are our 100 topics and that's the K plate.",
            "Now we have the document plate.",
            "This is the corpus and we first have Theta D. These are the topic proportions as I mentioned the cartoon histogram.",
            "And it is of dimension K. Because there are K topics.",
            "That's not really illustrated here.",
            "Then for each word that's the end plate inside the D plate.",
            "If you want to be really picky, you might want to put a little D here.",
            "Um?",
            "We have ZDN called at the topic assignment.",
            "That's the colored coin from the picture.",
            "OK, so you can see that that depends on Theta because it's drawn from a distribution with parameter Theta.",
            "So if Theta has probabilities for blue and yellow and pink then ZDN might be pink and it's drawn from that particular Theta.",
            "Clear and and there is a Z for every word.",
            "Remember, there's a colored coin for every word.",
            "OK WDN depends on Z DN and beta all the betas.",
            "WDN is the NTH word in the death document, and notice that WDN, which is difficult to say, is the only observed random variable in this whole model.",
            "OK, all we observe are a bunch of words organized by document.",
            "Now, why does W depend on Z and beta?",
            "That's not rhetorical.",
            "It could have been.",
            "That's right, selected from the topic and and so why is it?",
            "Does it depend on all the betas?",
            "Topics.",
            "OK, so because all the topics contain this word and what did you say?",
            "All the topics you said all words.",
            "That's right, all right, and so.",
            "If this is going to encode the distribution of the words, then how do I?",
            "What is the probability of this word given Z and beta?",
            "Probability to that topic.",
            "Existed.",
            "Work with the chosen.",
            "And the probability that that word would have been chosen from the top.",
            "Right, so well, I'll say close, that's almost right so we're conditioning on Z an.",
            "All the betas so the probability of the word.",
            "See if I can do this.",
            "So.",
            "In the in the joint distribution that this graphical model represents, we have probably the word given Z, so this is DN given Z DN and all the betas.",
            "And what you said is that it's the probability of seeing that topic and the probability of seeing that word under that topic.",
            "Is that what you said?",
            "No, what did you say?",
            "Probability of observing that word given the topic and the probability of observing that word given the topic assignment.",
            "And then yeah, topics like a prior to that.",
            "Right, So what?",
            "It is?",
            "What this is is I think, I think you said what it is.",
            "It's the probability of observing this word from.",
            "Remember, ZDN is a number from 1 to K because it comes from Theta and so you index the ZT NTH topic from beta one through K and you look up the probability of WDN from that topic.",
            "OK, so it's beta.",
            "ZDN, WDN.",
            "OK, it's basically we've got our K topics.",
            "Each one is a distribution over words so.",
            "If this is V and this is K, then.",
            "Our topics this is sometimes called a topic matrix.",
            "Our topics are each a distribution over all the V words.",
            "Like we said, they all sum up to one.",
            "And in the generative process, once we've selected Z, we basically know which topic this word is coming from, so it might come from this topic here and then we look up the particular cell of this word.",
            "So if this is WDN, that cell is WDN this row.",
            "And this column is ZDN.",
            "Then we basically look up in beta the.",
            "In in the CDF column the WDF word and get its probability from there.",
            "OK, so that's why we have the observed WDN depend both on CDN and all the betas because without observing them we don't know which one it is.",
            "We just know that it depends on both those things, and you can kind of see that functionally here the probability of the word depends on both Z and beta.",
            "Clear.",
            "Red is actually probably the probability of the word is the, so the probability of the word given the topic assignment and all the topics is the WD NTH entry in the ZD NTH beta.",
            "It's the word WDN entry in the topic assignment.",
            "Sorry in the topic beta with index topic assignment, CDN.",
            "Did that make things less clear?",
            "Possibly?",
            "Do you have a question?",
            "Yeah.",
            "Is forming a year you're doing?",
            "Is it said the end and only one value?",
            "And so you're saying the word depends like have been generated only by one that's right and it said that the end probably is like is a distribution and therefore we're going to.",
            "Merchant results, he said.",
            "Some point yes, OK.",
            "Generate because I work might be coming from several different clubs.",
            "OK. We try to repeat the questions.",
            "Oh yeah, sorry.",
            "Sure yeah, the question is that it looks like in this formula I'm assuming that, well, the question was with Z, but I say Z but we could say zed is zed, Zed, NA distribution?",
            "Or is it a?",
            "I'm assuming it's a single value?",
            "I'm saying it's a topic assignment, a number from 1 to K when really we don't observe it, so it's a distribution that was the question and the answer is very glad you asked that question.",
            "So here we're still.",
            "Living in the fantastical magical world of treating our hidden variables as observed.",
            "OK, so here I'm asking for the distribution of WDN given Z DN and beta one through K. Now we know you and I both know secretly they're not observed, so we're going to be putting posterior distributions.",
            "We're going to have a posterior over these things, but for formulating the model, the conditional, the conditional distribution that this graphical model begs us for.",
            "Is the probability of W given that I've observed the topic assignment and all of the topics and in that fantastical world ZDN is just a number from 1 to K?",
            "That's an important distinction.",
            "Theta D is a distribution over topics even in the fantastical world.",
            "Because each Z comes from Theta and there can be multiple these for different words in the same document.",
            "But ZDN is a number from 1 to K. It's an index from one to K. Is that clear to everyone?",
            "OK, yeah.",
            "W. Exactly, so that's not be included here.",
            "Encoded, where the graphical model doesn't tell you that, that's right.",
            "So the graphical model really just tells you what kinds of conditional in dependencies you can.",
            "You can assume here and write the particular functional form of W given Z and beta, I'm specifying at the board here good point.",
            "Yeah.",
            "Kind of aggregate functions from the package of WD into the only like the maximum probability that it can belong to one of the topics.",
            "I'm talking something disorder or even learning just from the data.",
            "So there might be.",
            "And the other one.",
            "Aggregate maximum.",
            "No, let me write down the joint distribution here.",
            "That'll clear up everything.",
            "The answer is no, we're not assuming that there's any maximums or aggregate functions at play.",
            "So yeah, I'll turn the light on.",
            "Binding there were two.",
            "I did exactly what you been told me to do earlier so.",
            "It's gotta work alright.",
            "Good, alright?",
            "So let's just just let's just put everything in bright lights.",
            "So.",
            "Again, fantastical world everything is observed.",
            "What is the joint distribution of all of the hidden and observed variables according to this model first?",
            "We have each topic coming from some distribution that's appropriate over topics.",
            "That's going to be the richley OK, and so those are all independent of anything else.",
            "And that's because these betas only depend on Ada.",
            "OK, that's the first part of this joint distribution.",
            "Now we have our documents.",
            "We have the documents and what's the first random variable we generate for each document the topic proportions?",
            "Which depend on Alpha.",
            "Again, that's a distribution.",
            "The dearest lady distribution that's appropriate for distributions over distributions.",
            "That's easier to understand if it's written down, then within each document we have the words of the document.",
            "OK, this prints may not be necessary, but they make it all clear.",
            "So for each document there is N words in the document.",
            "Just for simplicity, we assume that all documents have exactly 552 words, and.",
            "We first draw the topic assignment from Theta D, sorry.",
            "Right, that's akin to rolling the 100 sided die.",
            "OK.",
            "Finally, we draw the word.",
            "Condition on Z DN and beta one through K. OK. That's the joint distribution of the observed and hidden random variables.",
            "And just so I said that this comes from a deer sleigh.",
            "And this comes from a dear sleigh.",
            "Which we're going to talk about in a second.",
            "What is the probability of ZDN given Theta D?",
            "So if Theta D is here, we have 100 topics.",
            "And here we have our probabilities of various of those topics.",
            "This is 1326 thirty nine and 42 total coincidence, and let's say Z DN is 42.",
            "What's P of ZDN given Theta D?",
            ".33 right in general, what is P of ZDN given Theta D?",
            "What's that?",
            "Exactly well, Theta, it's Theta D, ZDN.",
            "It's this CDN indexes data D OK, same trick is before and.",
            "I should say that in the in the in the nitpicky mathematics of this, it's not exactly how you represent these random variables, but don't worry about it.",
            "For now.",
            "We'll get there.",
            "Maybe we won't, but you'll get there.",
            "So that's OPZDN given Theta D equals that and P of WDN.",
            "Given ZDN an beta went through K we discussed was a slightly more complicated thing.",
            "Beta ZDN, WDN.",
            "OK so with this and this and these two facts and this joint distribution here we fully specified the model.",
            "Subject to telling you about the rich late distribution.",
            "Clear good OK?",
            "So other questions.",
            "OK, good.",
            "Possibly have demotivated you to ask questions now, but please know I like those questions.",
            "It's important OK.",
            "Right?",
            "It's been awhile since I've spoken.",
            "Some are good.",
            "The restriction that's convenient, so this is the this is.",
            "This process described as a graphical model and fully specified, and now I just want to tell you about that."
        ],
        [
            "Seriously, distribution.",
            "So the Dirichlet distribution is an exponential family distribution over the simplex.",
            "Have you seen the exponential family yet?",
            "Yes, OK, so it's an exponential family distribution page.",
            "In all of that information over the simplex, which are positive vectors that sum to one OK and.",
            "The deer sleep distribution so Theta.",
            "Here is a point on the it's really called the K -- 1 simplex.",
            "It's the distributions over K elements and.",
            "The Dirichlet is parameterized by a K vector Alpha.",
            "Alpha is just K positive values.",
            "That parameter is the deer sleigh and the density function of the richly is given here, so it's a product of each component to the power of Alpha I -- 1 times.",
            "Guy times this, which is the normalizing constant and this gamma function is a is one of the special functions and it's essentially like a real valued extension of the of the factorial function.",
            "You can think of it as the factorial function.",
            "This constant here.",
            "This is just a function of Alpha, so it's a constant with respect to the density with respect to the random variable Theta.",
            "This constant here just make sure that this whole thing integrates to one, which as you know densities must do so.",
            "This is the Dirichlet distribution and like I said, it's a distribution over the simplex over positive vectors that sum to one which are basically parameters to discrete distributions to multinomial distributions OK. And the Dirichlet is special because it's conjugate to the multinomial.",
            "Have you learned about conjugacy?",
            "I think I know you did 'cause I came.",
            "I was here for that.",
            "Dearsley is conjugate to the multinomial.",
            "What that means is that given multinomial multinomial observation or multinomial observations, the posterior distribution of Theta given those observations is still a Dirichlet.",
            "I'm going, I think it's worth giving you the details of that in a second.",
            "The parameter Alpha controls the mean shape and the sparsity of data by sparsity of data I mean how many atoms in this distribution over atoms tend to have positive probability or or?",
            "They all will have some positive probability, but tend to have high probability.",
            "And.",
            "Yeah, and so we're using the deer sleigh twice in this model.",
            "The topic proportions are K dimensional.",
            "Dirichlet and the topics are V dimensional dearsley OK, want to hear more details about the deer sleigh?",
            "Yes, good.",
            "Zubin does so.",
            "I feel like he speaks for everything for you.",
            "I think you want to.",
            "That's really fascinating.",
            "OK, so.",
            "Dearsley oh I I I used to have like a really lame figure showing.",
            "Different examples of deer slays.",
            "And then I looked on Wikipedia and found a much cooler figure and took it so."
        ],
        [
            "And.",
            "Anyway, so I'll explain this figure.",
            "OK, so this figure shows a couple.",
            "Let's forget the K -- 1 nonsense for second, let's call them 3 dimensional Irish lace.",
            "OK really, it's 2 dimensional.",
            "Dear slays, I'm calling them 3 dimensional displays.",
            "So here's what these figures look like.",
            "So remember the deer sleigh represents a distribution over some number of elements, and let's say Theta comes from a deer sleigh with parameters 111.",
            "OK, that means that as I draw a random variables from Theta as I draw from this theorist way, I'm going to get distributions over three elements.",
            "OK, so one such distribution, for example.",
            "So here's element AB&C.",
            "One such distribution over three elements is the distribution that places all of its mass on a. OK, on this triangle that's here.",
            "Hey, that place is all of its massane.",
            "Another distribution places all of its mass on B.",
            "That's here placing all of its mass on be OK.",
            "I won't insult your intelligence and draw it again.",
            "The third one places all of its mass on.",
            "See now.",
            "The way this triangle looks works is that.",
            "Every points a between A&B here represents.",
            "See having probability zero and.",
            "A having some positive probability and B having one minus that probability.",
            "OK, so in other words, if I clamp the probability of C at zero and I put some probability on A and some probability on B and let's assume that sums to one, then that point might be.",
            "Let's see B has a little more than a.",
            "That point might be right here on the simplex.",
            "OK, analogous the points between A&C analogous.",
            "The points between B&C.",
            "Clear so we can clamp one of these at zero.",
            "Contemplate all the possible essentially binary distributions between the other two points and that represents the continuum along the edges of this triangle.",
            "OK, and the points inside the triangle.",
            "Are the points where a has some probability, B has some probability and C has some probability, so that might be that point.",
            "OK, every point in the triangle is some point in the distribution space over distributions of three items.",
            "OK.",
            "The Dirichlet Place is a distribution over that space.",
            "And the Deer Slayer 111 is very special if all the alphas are equal to 1 then the richly 111 is the uniform distribution on this space.",
            "Every there's a nuanced here which is that these actual edges, the points on the line are not possible.",
            "You can't have something have zero probability under dearsley that has that has zero probability.",
            "But you can get arbitrarily close, so 111 on the interior of this triangle puts units the same probability anywhere.",
            "It's called the uniform distribution.",
            "Alright, and in general so where in this triangle is the space where a B&C have equal probability?",
            "Yeah.",
            "I.",
            "Let me OK.",
            "Thank you so no 111.",
            "So remember the Dirichlet is parameterized by Alpha and Alpha is a vector of length K of positive values.",
            "They can be anything.",
            "So in the 3D, richly Alpha has three components.",
            "An 111 refers to A1 equal A2 equal A3 equal 1.",
            "OK, and so those are the parameters to this distribution and what I'm saying is that when you set those parameters exactly to one like that, that leads to a distribution where any point on the simplex has the same probability.",
            "OK, good.",
            "But where is the point where Alpha AB&C all have probability 1/3?",
            "Where is that point?",
            "It's in the middle, right?",
            "It's right here.",
            "OK, and so if you have a deer sleigh where A1 equals A2 equals A3 equals say 5.",
            "What that does is puts a bump in the middle and spreads the contours around that bump like this somehow.",
            "OK, in general some properties of the deer slay that are worth knowing.",
            "The expectation of Theta given Alpha.",
            "Equals so Theta I the expectation of the ith component.",
            "So we have a random.",
            "We have a distribution over this space.",
            "We can contemplate the expectation of any of these components.",
            "Just a multivariate distribution and expectation of Theta.",
            "I given Alpha equals Alpha I over the sum of the Alpha.",
            "OK, so with 555, the expectation of each of the components is going to be 1/3 because 5 / 15 is 1/3.",
            "OK, and that's always true.",
            "The expectation of data I given Alpha is this, and so if you have a an asymmetric theoretically distribution, if you have like A1 equals 10 Alpha 2 equals, well, I don't say it.",
            "So here we have our simplex and here is a distribution centered somewhere, not in the middle.",
            "Right?",
            "Here is a distribution center not in the middle.",
            "These are all distribution center dot in the middle.",
            "OK, so that determines the location of this hump.",
            "Yeah.",
            "Alpha yes, that's the next thing I'm going to.",
            "No, yes and no are the answers to your questions.",
            "The first question was the greater Alpha, the less Peaky, yes, the next question was you can't have Alpha less than one, no, but I'm going to explain those in that a bit OK, but first just to look for the location of the hump, the location of the hump is determined by this expectation.",
            "OK, now let's get to the penis of the hump so.",
            "And let's assume for now that Alpha is greater than one.",
            "The Alpha less than one case will deal with separately.",
            "OK, I'll go over here.",
            "Can you see that?",
            "Can you all see the board over there?",
            "That way I only have to work with one set of lights.",
            "OK, good alright?",
            "So the.",
            "So this is 1 important piece of the Dirichlet parameterisation.",
            "The other important piece is the sum of the alphas.",
            "OK, so the sum of the alphas determines the Peaky Ness of the Dirichlet when the sum of the alphas is is is.",
            "Small, then the dearest lady is going to be very spread out and the greater the sum of the alphas, the more Peaky the dearest lady comes.",
            "At this point at its expectation.",
            "OK. And sometimes you see this called.",
            "Sometimes called S. And this is sometimes called M. OK, so this is like the mean and this is the scaling OK and just to to so an alternative parameterisation of the Dirichlet is as a point on this simplex, the mean and a scaling parameter S which is which is which determines how peak it is around the mean.",
            "And I'm saying this to foreshadow the various lectures on nonparametric Bayesian methods that are coming up next week, where you parameterise the infinite dimensional directly in precisely the same way.",
            "OK, but just.",
            "Put that away in your brains somewhere safe.",
            "OK. Um?",
            "Now I need to go to my safe brain space.",
            "Think about what to say next.",
            "Penis Alpha, lesson 1 and then posterior OK?",
            "This, I think will be useful.",
            "Later.",
            "This is.",
            "These boards are heavier than at my University.",
            "I can see why you guys are so buff here.",
            "It's serious.",
            "This is very.",
            "This is very solid stuff.",
            "This not all mathematics is for these boards, OK?",
            "Good.",
            "It's kind of invigorating.",
            "Alpha less than one, what happens?",
            "OK, equivalently.",
            "S lesson 1.",
            "Is an Alpha is less than one?",
            "Is you get sparsity.",
            "OK, so on the three simplex.",
            "Here's our our friendly triangle so you know I've been drawing.",
            "These deer slays like hump somewhere in the middle of the simplex.",
            "But on the when Alpha is less than one, you end up with a different shape.",
            "You end up with a shape that that.",
            "Places increased probability at the corners of the simplex.",
            "OK, so its contours.",
            "Hey, let's see let me think as contours.",
            "This looks like this.",
            "For example, OK, so it's like a shape like like that, yeah, that I draw that wrong.",
            "What's that?",
            "Why's it going to S less than one?",
            "As less than K. Thank you, yeah.",
            "It's not.",
            "It's S less than K, thanks.",
            "The board gets heavier as you make more mistakes actually.",
            "So.",
            "Is there another question or if that's it, did you have a question?",
            "Like that?",
            "They are to the outside, OK.",
            "However, you're supposed to draw this, let's say, in two dimensions.",
            "It's really easy.",
            "So in two dimensions you have between zero and one right.",
            "You just have the probability of one thing and then probably 1 minus the other thing.",
            "So when in the deer size that I've been drawing, it looks like this where you've got some mean and a spread around the mean, and when alphas are less than one you get this kind of shape.",
            "However, that looks in three dimensions.",
            "I'm not sure, OK?",
            "Exactly, it's like this.",
            "Yes yes fine very good.",
            "OK now I can't even move it.",
            "Alright erase this horrible picture except pretty in kind of a spirograph sense.",
            "And yes, yes, just take those contours as pictures.",
            "The same.",
            "I don't know.",
            "It still doesn't seem right to me, but it doesn't matter.",
            "Let's move on.",
            "OK, so this is what happens when office one.",
            "But really, when Alphas lesson 1 don't waste your time with two and three dimensional Dirichlet's.",
            "Think about 50 and 100 dimensional Dirichlet's OK.",
            "So now let's draw that space so.",
            "The way you do that.",
            "Is of course you can't, but what we can do is I can.",
            "So let's say we have a 50 dimensional directly, so there's 50 components here.",
            "OK, and.",
            "I encourage you.",
            "In you know, in R to sample abunch of dearest Lazan, plot them so that you can see what happens with different parameterisations of the Dirichlet.",
            "And in saying that I'm encouraging you to do two things.",
            "One is to do that and two is to use R, so I can really go on and on about R Now.",
            "So let's say Alpha was greater than one.",
            "So let's call let's say Alpha look like this.",
            "OK, and let's say this 50 of them or whatever OK, so let's say that's Alpha alright.",
            "Then when we draw from this dearest way, we're going to get shapes that sort of look like this, but that vary from this in some way or another, subject to how Big Alpha is, right?",
            "So again, if you do this in R, you can easily plot 100 points on the simplex drawn from this distribution and see how it kind of looks like this.",
            "But sometimes this one will be a little bigger.",
            "Sometimes this one will be a little smaller.",
            "Sometimes this one will be a little bigger and so on.",
            "Is that clear?",
            "I'm not going to do it because I think it would be too time consuming.",
            "OK?",
            "When Alpha is much smaller than one.",
            "However, when each Alpha is smaller than one or some of the alphas are smaller than one, let's let's be simple and let's talk about what's called the exchange obliviously.",
            "Exchangeable deer sleigh is dearest way AAA and so on.",
            "OK, there's just one parameter, one scalar, and we assume that M is the is right in the center of the simplex.",
            "OK, so when Alpha is smaller than one in the exchangeable dearsley, what you get are sparse distributions, so you get things like this.",
            "OK, we're only some of the atoms have positive probability and many of them have probability very very close to 0.",
            "Again, file this away in your brains 'cause this is going to have an important connection in nonparametric Bayesian methods.",
            "Now, which especially in the exchangeable dearsley which components have positive masks, those are.",
            "Up to the those are totally at random.",
            "However, as Alpha gets smaller and smaller and smaller, fewer components will have positive mass, so this is a sparse, exchangeable Dirichlet.",
            "Then answer question about Alpha lesson 1.",
            "OK.",
            "Finally, any other questions?",
            "Yeah.",
            "One per uniform distribution.",
            "That's right.",
            "It's fine, it's also summer peak where it's not respond precisely between Alpha is less than one.",
            "As you get further and further below towards closer to 0, you get sparser and sparser and sparser draws from your dear ishly.",
            "So when Theta comes from a deer sleigh.",
            "And Z comes from Theta OK, or let's say XN comes from.",
            "So that's that's like a little piece of our LDA model, right?",
            "Data comes from the rich, lazy and comes from a multinomial with parameter Theta.",
            "This is also called the discrete distribution.",
            "Technically, it's not a multinomial less, you have another number there, which is how many draws.",
            "But don't worry bout it says again comes from multinomial data comes from Italy.",
            "You can ask the question what is P of Theta given Z1 through N?",
            "Other words vibe serve N topic assignments.",
            "What is the conditional distribution of the topic proportions given those topic assignments OK, and what conjugacy means is that if we let.",
            "NZ 12 Big NB accounts.",
            "Of each Atom.",
            "In other words, if I saw a topic 13 six times, then end sub 13 of Z1 through N = 6 OK, it's just a little counting function clear.",
            "OK then later given Z1 through N is a dearest leg with parameters Alpha plus AN-12.",
            "OK, we just add the number of times we saw topic 13 to the Alpha parameter for topic 13 and that's the new D richley parameter.",
            "So notice that you will rarely have sparse well.",
            "Now don't notice anything.",
            "Yeah, that's the posterior Dirichlet.",
            "OK, and so notice that your instinct earlier that as the as the sum of the alphas gets larger you get a peak and peak your distribution.",
            "That's mirrored here as we see more and more observations are posterior, get speaker and peak here in peakier and that's very intuitive.",
            "As I roll the die more and more and more.",
            "My idea of what the distribution of the faces are is going to become more.",
            "I'm going to come more and more confident about it and so it's the whole idea of the prior speaking so loudly in the data speaking.",
            "Loud as loud, proportional to how much data you saw.",
            "As you see more data you become more and more confident in the estimate that that data gives you.",
            "OK, we've totally diverged from topic modeling.",
            "So any questions about this and then we can get back to it for 15 minutes.",
            "OK. Yep.",
            "Brian, I mean when you say sports, do you mean that the other, the other ones are exactly no close to 0?",
            "They're going to be like .001?",
            "You can see that in our.",
            "I will turn the lights off.",
            "By the way, when you sample from a dear slate in order to do it well, I'll tell you how to do it later.",
            "Well, let's keep going now, OK?",
            "OK, very good, we're back so.",
            "I'm going to turn the lights on in one second."
        ],
        [
            "So here we go again.",
            "We have the LDA model.",
            "LDA is a mixed.",
            "It's called a mixed membership model in statistics and it really builds on the work.",
            "The seminal LSA work latent semantic analysis of Dearwester at all, and probabilistic latent semantic analysis from Thomas Hoffman.",
            "And.",
            "Actually, let's not go into details about this.",
            "The reason it's called a mixed membership model is that you can think of the model where each document comes from a single cluster as like it.",
            "That's a mixture model where we have each document is associated with a single Z.",
            "And here since documents are associated with Theta, a distribution over clusters, then each document can be associated with multiple components.",
            "That's what that's what the mixed membership idea is.",
            "So when you're reading the Journal of Bayesian analysis or the Annals of Applied Statistics, and you hear about mixed membership models of ranked data in this data and that data.",
            "You can think about LDA as an instance of a mixed membership model.",
            "OK, that's in statistics, that's what this is called.",
            "And for document collections, another grouped data, I should embolden that group data.",
            "This is rather than thinking of a document as a single data point, it's really a group of data points.",
            "The data are the words and the document represents the group of words for group data.",
            "Often the mixed membership assumption is more appropriate than a simple finite mixture, which is the natural alternative.",
            "OK. And I should mention that in statistics this same model was invented for population genetics analysis and had a lot of impact there by Stevens and Pritchard.",
            "So there they don't care bout documents.",
            "What they care about is real science, and they're modeling people as being mixtures of their various.",
            "Ancestry, so you know it's all my my case is a bad case 'cause everybody's from the same small town in Hungary, but my wife like her family, her mom's from Denmark, and her dad's from.",
            "He's a pretty.",
            "He's a character.",
            "He's from Canada.",
            "Basically, let's say and.",
            "And.",
            "And so you know her genes are kind of a mixture of the very of the different populations that she comes from.",
            "It's a lovely combination.",
            "I should say that to the video.",
            "And and so this model is a model for for looking at how people's genes mix and then what are the results of it.",
            "You can think of the populations as the topics and the people as mixing over the topics.",
            "OK, so let's get on before I make some kind of marriage error alright?",
            "Good."
        ],
        [
            "So again, the from a collection of documents.",
            "This is all a nice story, but we don't get to observe any of this stuff.",
            "We don't get to observe the topics or the topic proportions or the topic assignments, and that's really the central algorithmic goal of of working with a model like this.",
            "You want to infer all of this nice structure so we can use it.",
            "So the idea is to infer the per word topic assignment CDN the per document topic proportions Theta D and the per corpus topic distributions beta K, and then to use posterior expectations.",
            "Basically the expectation.",
            "Of all those things, given, the words here is the hidden variables.",
            "Here are the words.",
            "Use the posterior expectations of these things to perform whatever task it is we care about, such as information retrieval, document similarity classification, whatever it is you're doing.",
            "OK, see if this works good, whoops."
        ],
        [
            "OK, so there are a lot of approximate OK we will in the second part of this talk we're going to see how we can actually compute the exact posterior like we can here in this nice conjugate model.",
            "And so a lot of approximate posterior inference algorithms for this model have been developed including mean field variational methods.",
            "This is what we're going to describe later on expectation propagation, which I know you learned about a bit yesterday.",
            "Collapsed Gibbs sampling, which I'll talk about later on and collapsed.",
            "Variational inference, which is very exciting, but I will only allude to it later on, and there's also been some work on how to compare these different types of inference algorithms and how well they do.",
            "There's a great paper from I smell this year comparing them and also some theoretical work about collapsed variational inference versus mean field variational inference that I worked on with a student.",
            "We're going to get into details of approximate posterior inference later on.",
            "I guess on Thursday, but for now I want to show you a little bit of.",
            "Let's assume we have an approximate posterior algorithm that we like and let's look at some real data and what this model does with real data.",
            "OK, any questions about this?",
            "So this is the model.",
            "The only things that are observed, other words, and so we want to fill in the rest of this with approximate posterior inference, OK?",
            "OK, so."
        ],
        [
            "For these first pieces, I want to look at the OC Art collection of Science Magazine from 1990 to 2000.",
            "So just to give you an idea of what this data.",
            "Is the basically have you all seen J store.org?",
            "OK, good so Jay Store is a an online archive of scientific articles and what they do.",
            "It's an amazing project is they take the original bound articles and they scan them and then they run them through optical character recognition software and then they index the original scans via the output of the OCR.",
            "Algorithm so the it's cool because so you know it'll screw stuff up like this is 2 columns and maybe their OCR software can't handle two columns and it maybe it doesn't get all the words right, but it's pretty accurate and maybe the punctuation is totally bogus, but that's OK, but for doing search when you want to search for a word like.",
            "Phenotype, then, as long as it got phenotype a bunch of times in the article, you're going to get the right scan back.",
            "And since you never interact specifically with the noisy OCR, it's a very effective system for searching online for searching articles online that are archived and they have science all the way back to 1880 will in fact see that later on.",
            "OK, but of course J store has problem, which is that they have millions and millions of articles, but they're only organized by Journal and by date and what they want is a system where people can go through and browse and examine these articles in a topic oriented way.",
            "But since they've scanned so many articles, it's out of the question to annotate these scans with keywords.",
            "Moreover, that's in they want this to be automatic.",
            "They don't want to have to read every article an assignment, keywords.",
            "The whole point is that you don't have to do that, you can just put them through a machine.",
            "And then let scholars use them effectively so they're interested in looking at the kind of topic decomposition of their archives.",
            "So here we're taking their collection of Science magazine for 10 years.",
            "We're looking at 17,000 documents.",
            "This is 11,000,000 words, and we used a vocabulary of 20,000 terms.",
            "Basically, as a practical point, you typically remove stop words words like the but or and, and very rare words which don't affect the inference much.",
            "Or somebody recently told me might affect the inference badly.",
            "We can talk about that later.",
            "But so this is, the documents are analyzed and we fit a 100 topic LDA model using variational inference, but could have been anything.",
            "OK so here is that same art."
        ],
        [
            "So that I showed you, I should say that this article was actually in a test set, so this article we didn't.",
            "We didn't fit with this is the article.",
            "How many genes as an Organism needs to survive in an evolutionary sense and using posterior inference.",
            "So here are the WD NS for this article, right?",
            "I didn't draw the graph model, but here are the W. DNN and with posterior inference this is the expected value of Theta.",
            "Given those WS.",
            "OK, so.",
            "Or town lights of this, but it's just the expectation of Theta D given W one through N and the topics.",
            "That we estimated.",
            "OK, that's what this is an approximation of, so this is like the real version of the cartoon histogram that I drew for you before it's saying, look, I've got these hundred topics we haven't looked at them yet, and given this new article here is, here are the topic proportions that are associated with that article.",
            "Clear.",
            "OK, and you can see that it's not like it's using every topic for this particular article.",
            "It's saying this topic has high probability.",
            "This topic has high probability.",
            "This topic has high probability and so on.",
            "And even though it has 100 topics to choose from, it's not as though it's using all of them to describe this article.",
            "So we're getting some description of this article in terms of these topics.",
            "Hey.",
            "What's that?",
            "We we so this was all done through post your inference.",
            "So let me just loops going the wrong way.",
            "Don't look, don't look.",
            "This is secret.",
            "OK, so."
        ],
        [
            "Remember, these things are all latent.",
            "The topics are latent.",
            "The topic assignments are late in the topic.",
            "Proportions are late and we never observe them.",
            "So with approximate posterior inference, we infer them just from the collection of articles, and we're going to look at them in a second.",
            "Unless you look before at the secret stuff.",
            "OK.",
            "So here are the topic proportions and now."
        ],
        [
            "Now let's finally look at these topics.",
            "Let's look at the top words from the top topics in this article, and these are the top."
        ],
        [
            "Pics I showed you before you can see.",
            "So what I've done here instead of showing you all 20,000 words and the probabilities and, uh, squinting and looking at the numbers, I've ordered the words by probability.",
            "So the most probable words in this topic, which is the most probable topic in that document.",
            "Our words, like human genome, DNA genetic gene sequence, gene molecular and so on.",
            "Another top topic are words like evolution, evolutionary species, organisms, life origin, biology and so on.",
            "And here is disease.",
            "Host bacteria, diseases, resistance.",
            "And the computer modeling words OK and I want to emphasize that we didn't have to set any probabilities here.",
            "All we did was was arrange painstakingly with J store and their lawyers to get their data.",
            "Then we ran our algorithm on that data and this topical decomposition of this article comes out of running that algorithm.",
            "There's no probabilities need to be set anywhere.",
            "This is what's called unsupervised learning.",
            "I should have mentioned that 16 times by now and I didn't.",
            "OK. You do still need to set the number of topics which we can talk about how to do that in the later part of the series, yeah.",
            "Yeah.",
            "Why do you not use any impression aggressive staining algorithms so I can see again?",
            "Gene gene.",
            "Yeah, so again we should we.",
            "You could stem and and if this were done in some kind of industrial grade way then we would probably stem to make the best possible interpretable topics for the JSTOR users.",
            "Yeah.",
            "Other questions.",
            "OK, and so you can see that this is capturing what seems to be thematically related words which I'm going to want us to contemplate in a little bit.",
            "OK, here's another example.",
            "This is."
        ],
        [
            "About chaotic Beatles and Michael Hassle and and this is about mathematical models of insects and beetles, and again we can play the same game we do.",
            "Post your inference.",
            "We get back topic proportions and we look at the most probable words from the."
        ],
        [
            "Same model for this document and you see words like words about mathematical problems worth about words about models and statistics, words about like selection and.",
            "Don't even know what the word is for this.",
            "Population theory and words about ecology and so on.",
            "Going with The Beatles.",
            "And so again.",
            "These these topics and these topics are all in the same model and the approximate posterior inference algorithm is deciding to use them in different ways depending on the different observed words.",
            "OK, and so.",
            "These tools then, once you've marked up your entire corpus with topics and with topic, proportions and so on, you can use these tools to browse."
        ],
        [
            "Collection so here for example is another article from Science.",
            "Here are some top topics from this from this article.",
            "Here I'm I'm showing which words were assigned to different topics.",
            "This usually makes a little bit less sense because it has to assign every word to a topic, even ones that have low probability.",
            "And but you can see things like statistics evaluating data, statistical means comparisons in one topic, words like acid, an proteins and spacing, and amino in one topic and words like.",
            "General Text provides recent.",
            "That seems like a bogus topic or not a bogus topic, but seems like these words it had no better place to put them, but you can see that it's decomposed this article into multiple overlapping recurring patterns of words, and then you can do things like ask for similar documents according to these topic proportions.",
            "So rather than use words to find document similarity, we can use the topics to find document similarity and here are the titles of the of the documents that are most similar to this document.",
            "By topic, and that can be a form of what's called query expansion.",
            "For example, where if I write an article about my cat and I use the word cat, and you write an article about your cat and you use the word feline, then traditional document similarity metric won't say that our articles are similar, even though our cats might be extremely similar and whereas topic model knowing that cat and feline or somehow in the same group will say that our articles are similar in that indeed our cats are very similar personalities.",
            "So this is the way that you can use these types of models, and in fact, if you look at rexha.org, I don't know if you've seen this.",
            "This is a project that came out of UMass Amherst from Andrew Mccallum's Group, and they are they use topic models to help you browse a large collection of computer science academic articles, so it's worth checking that out.",
            "Questions, yeah, does it turn out?",
            "Similar.",
            "You know where little meaning good question, so I said that we remove stop words words like of but and or the but really well not really, but what that means is that we really remove words that occur in all of the articles.",
            "So if a word occurs in all of the articles then we remove it.",
            "In fact, if it occurs in more than 90% of the articles then we remove it because your intuition is correct that the model is going to want to place those words with high probability in all the topics because they occur so frequently.",
            "That they don't help decompose the collection.",
            "That's typically why we do that.",
            "There are ways of doing that post hoc after you get the topics looking at the instead of ordering the topics by probable, most probable words, there are other scores you can use to order the words.",
            "I can talk about that on Thursday if you like, but that's a good question and leads."
        ],
        [
            "To this question for you so actually I won't show the answer like yeah, yeah.",
            "And that is.",
            "It's not.",
            "Why doesn't it happen that rather than putting those words into all of the topics, yes, in the model?",
            "Discover a new topic which only has those areas and this topic will appear in all the documents.",
            "There is an intuition for that, but I'm I'm not going to answer it 'cause it relates to my next question, which is for you.",
            "You plural.",
            "What, why on Earth does this work?",
            "That's the question, so I told you don't look at that secret I.",
            "You know, I showed you this generative model and.",
            "Even let's look at this picture better."
        ],
        [
            "Look at this picture, so here's the generative process and you know we seem to just make it up.",
            "And consistently we get back these.",
            "Distributions over words that look like the matically.",
            "Related terms.",
            "And when you're doing applied Bayesian modeling.",
            "It's important to think both about the prior, which is what we're specifying here.",
            "Essentially an to think about the posterior.",
            "So why could we expect to find these kinds of combinations of words as highly probable words under the posterior, and if so, why so?",
            "It's time to go, unfortunately, but I want you to contemplate that and then on Thursday will begin by hearing your answers to those to that question.",
            "I have an answer, but who knows if it's right and I'm interested in you thinking about what the posterior distribution means here and why it might do something like this and what it's actually.",
            "This is capturing somehow in plain English.",
            "I mean, I want the description to be in plain English, you don't have to analyze English documents.",
            "What is this capturing?",
            "I want to know that answer in plain English.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, welcome to September 1st.",
                    "label": 0
                },
                {
                    "sent": "My name is Dave Bly.",
                    "label": 0
                },
                {
                    "sent": "I'm going to talk about topic models.",
                    "label": 1
                },
                {
                    "sent": "I'm from Whoops, I didn't mean to do that.",
                    "label": 1
                },
                {
                    "sent": "I'm from Princeton University in New Jersey where it's 4:00 o'clock in the morning.",
                    "label": 0
                },
                {
                    "sent": "And so.",
                    "label": 0
                },
                {
                    "sent": "This is what it's going to look like to teach at 4:00 in the morning.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, and please interrupt me a lot in this talk so that it will help.",
                    "label": 0
                },
                {
                    "sent": "Stimulate me to wake me up.",
                    "label": 0
                },
                {
                    "sent": "OK. Good So what topic modeling is about is that as.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You all know it's almost redundant.",
                    "label": 0
                },
                {
                    "sent": "More information becomes available to us.",
                    "label": 1
                },
                {
                    "sent": "It becomes more difficult for us to be able to quickly access it and search for things in it and understand it and basically get something out of it.",
                    "label": 0
                },
                {
                    "sent": "And so we need new tools, new algorithmic tools to help us organize, search and understand these vast amounts of information.",
                    "label": 1
                },
                {
                    "sent": "Things like text, archives and image archives, and all different kinds of data that we're all just.",
                    "label": 0
                },
                {
                    "sent": "Both creating and having access to constantly.",
                    "label": 0
                },
                {
                    "sent": "So what topic modeling provides are?",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our methods for automatically organizing, understanding, searching, summarizing, exploiting these large electronic archives, and the basic idea are is, are these three steps.",
                    "label": 1
                },
                {
                    "sent": "First we take a big corpus like Wikipedia say, and uncover the hidden topical patterns that pervade it so.",
                    "label": 0
                },
                {
                    "sent": "Wikipedia's got some number of millions of articles and there about some things.",
                    "label": 0
                },
                {
                    "sent": "Those things overlap and we want to uncover what those things are.",
                    "label": 0
                },
                {
                    "sent": "What are the different topics that pervade this collection?",
                    "label": 1
                },
                {
                    "sent": "Then we want to annotate the documents according to those topics, so I understand what they say.",
                    "label": 0
                },
                {
                    "sent": "You know, 250 topics in Wikipedia are now.",
                    "label": 1
                },
                {
                    "sent": "If I pluck a document out of Wikipedia, what topics is that article about?",
                    "label": 0
                },
                {
                    "sent": "How can I annotate that article according to the topics I've discovered in step one and finally?",
                    "label": 0
                },
                {
                    "sent": "We want to use these annotations to organize, summarize, and do whatever it is we want to do.",
                    "label": 0
                },
                {
                    "sent": "Obviously will not obviously, but doing this in a vacuum is only so interesting.",
                    "label": 0
                },
                {
                    "sent": "We really want to say OK, Now that I've been able to essentially mark up my collection according to these automatically found topics I want to use that marked up collection.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as if I had.",
                    "label": 0
                },
                {
                    "sent": "10,000,000 people to go through Wikipedia and Anne carefully organize it, then what would I do with their organization since I don't have those 10,000,000 people?",
                    "label": 0
                },
                {
                    "sent": "I built algorithms to do that for me.",
                    "label": 0
                },
                {
                    "sent": "OK. Oops.",
                    "label": 0
                },
                {
                    "sent": "Oh, this is backwards.",
                    "label": 0
                },
                {
                    "sent": "It's the same.",
                    "label": 0
                },
                {
                    "sent": "Have been almost hit by cars about 6 times already, so same idea, OK?",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can do with with topic models or things like discovered topics from corpus.",
                    "label": 0
                },
                {
                    "sent": "So here are some topics.",
                    "label": 0
                },
                {
                    "sent": "So a topic is going to be a distribution over terms in a vocabulary, and these are the top most probable terms and four topics that were uncovered by analyzing the text in the journal Science.",
                    "label": 0
                },
                {
                    "sent": "So here's a topic.",
                    "label": 0
                },
                {
                    "sent": "Human genome DNA, genetic evolution, evolutionary species, organisms disease, host bacteria, diseases, computer models, information data, computers.",
                    "label": 0
                },
                {
                    "sent": "These are words that seem to go together in some kind of thematic, thematically coherent way.",
                    "label": 0
                },
                {
                    "sent": "But I should say that part of this talk probably on Thursday we're going to discuss the pitfalls of overly interpreting these probability distributions over words, But for now, since we're learning about it, we can pretend like those pitfalls don't exist and interpret them.",
                    "label": 0
                },
                {
                    "sent": "So these are four topics discovered from the journal Science.",
                    "label": 0
                },
                {
                    "sent": "OK. Oh yeah.",
                    "label": 0
                },
                {
                    "sent": "Don't do standing like.",
                    "label": 0
                },
                {
                    "sent": "There's no good reason not to send.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so you know sometimes stemming.",
                    "label": 0
                },
                {
                    "sent": "I mean, when I when I fit a topic model, sometimes I use stemming.",
                    "label": 0
                },
                {
                    "sent": "It can be when you're looking at these topics.",
                    "label": 0
                },
                {
                    "sent": "If you're doing this for like a system for someone to be able to to quickly browse and navigate through documents, some stemmers can be overly word aggressive and you can't really recognize the routes that they they give back.",
                    "label": 0
                },
                {
                    "sent": "But then there's one that I know my student likes.",
                    "label": 0
                },
                {
                    "sent": "I can't remember what it's called, but it's a more conservative stemmer.",
                    "label": 0
                },
                {
                    "sent": "Just it will remove things like computer and computers.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "But no, there's stemming is a good idea.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Things like so discovering topics, discovering topics, and understanding how the words change, how within the topic through time.",
                    "label": 0
                },
                {
                    "sent": "So here are here is time from 1880 until 2000 and here are two different topics that I've named again ignoring the pitfalls of naming topics.",
                    "label": 0
                },
                {
                    "sent": "And here are some words in those topics and you can see how their probability is changing overtime.",
                    "label": 0
                },
                {
                    "sent": "OK, we can we can uncover these time changing topics with top.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Models we can model connections between topics so.",
                    "label": 1
                },
                {
                    "sent": "Here again are lists of words that we are interpreting as topics.",
                    "label": 0
                },
                {
                    "sent": "And again, this is from science.",
                    "label": 0
                },
                {
                    "sent": "The science articles, and here are topics and how they tend to Co occur with other topics.",
                    "label": 0
                },
                {
                    "sent": "So you see here we have this topic.",
                    "label": 0
                },
                {
                    "sent": "Ancient found impact million years ago in Africa, and it connects to this topic, fossil record birds, fossils, dinosaurs, fossil.",
                    "label": 1
                },
                {
                    "sent": "OK, those two topics tend to Co occur, whereas this Dinosaur topic everyone loves dinosaurs doesn't.",
                    "label": 1
                },
                {
                    "sent": "Connect as much with, say, this topic about mice antigen T cells, antigens.",
                    "label": 0
                },
                {
                    "sent": "There's this issue again and immune response.",
                    "label": 0
                },
                {
                    "sent": "OK, so finding connections between topics.",
                    "label": 0
                },
                {
                    "sent": "And finally, we can use these tools not just on.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Text data, but on lots of different kinds of data.",
                    "label": 0
                },
                {
                    "sent": "For example on images.",
                    "label": 0
                },
                {
                    "sent": "Topic modeling has has helped make some progress in computer vision, and we can also combine these different types of data type.",
                    "label": 0
                },
                {
                    "sent": "These different types of data.",
                    "label": 0
                },
                {
                    "sent": "Here we're using the topic model to automatically annotate images.",
                    "label": 0
                },
                {
                    "sent": "So in this model you the input to the algorithm are images and words that describe them, and again by using this set of ideas that we're going to be discussing for the next while, you can take a raw image after fitting the model.",
                    "label": 0
                },
                {
                    "sent": "And automatically annotate it.",
                    "label": 0
                },
                {
                    "sent": "Here we have this image of a fish and the annotation is fish, water, ocean tree, coral.",
                    "label": 0
                },
                {
                    "sent": "OK, it's not perfect.",
                    "label": 0
                },
                {
                    "sent": "There's I don't think there's a tree in here, but.",
                    "label": 0
                },
                {
                    "sent": "This is another another application of topic modeling and there the idea is that the same way that we think of.",
                    "label": 0
                },
                {
                    "sent": "I don't have a picture of a document anywhere the same way we think of a document is a collection of words you can think of an image as a collection of image features, either by running some algorithms on it in advance to segment it and then computing features of each segment, or by doing something silly like just gritting it and computing features of each square in the grid.",
                    "label": 0
                },
                {
                    "sent": "And then you can treat the image like a document.",
                    "label": 0
                },
                {
                    "sent": "OK, I was deciding between coffee and water just then.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's kind of 1 perspective or motivation for topic modeling, since this is a machine learning summer school.",
                    "label": 0
                },
                {
                    "sent": "You know, sometimes I think of topic modeling.",
                    "label": 0
                },
                {
                    "sent": "On one hand, it's about taking our intuitions and maybe other scholarship about the structure of language and images and encoding it into good machine learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "But from the machine learning perspective, topic modeling can be seen as a case study in applying basically hierarchical Bayesian models to group data like documents or images and and.",
                    "label": 1
                },
                {
                    "sent": "Thinking of topic modeling as an application of these ideas, it really touches on a lot of different pieces in the applied machine statistical machine learning world, things like directed graphical models conjugate priors, an non conjugate priors time series modeling modeling with graphs, hierarchical Bayesian methods, fast approximate posterior inference like MCMC or variational methods, exploratory data analysis, model selection, nonparametric Bayesian methods, an mixed membership models so.",
                    "label": 1
                },
                {
                    "sent": "In this talk, we're going to touch on all of these.",
                    "label": 0
                },
                {
                    "sent": "All these topics I know you're going to go into these topics in more detail in other lectures in the school, but.",
                    "label": 0
                },
                {
                    "sent": "Princeton any of these things I would encourage you to think of topic modeling as a possible application for for your ideas.",
                    "label": 0
                },
                {
                    "sent": "In one of these contexts, it's a.",
                    "label": 0
                },
                {
                    "sent": "It's a nice way, for example, to test out new methods of approximate posterior inference and so on.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, so.",
                    "label": 0
                },
                {
                    "sent": "The way I had planned this is that we'd start by talking about latent dearest.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Allocation, which is like the simplest topic model I guess.",
                    "label": 0
                },
                {
                    "sent": "Then discuss in some detail approximate posterior inference.",
                    "label": 0
                },
                {
                    "sent": "I was going to talk about Gibbs sampling and variational inference and then talk a little bit about comparing the different approximate posterior inference algorithms that are available to you and give some advice.",
                    "label": 0
                },
                {
                    "sent": "Apparently I didn't.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I think I might have written that before I made the slides, 'cause I don't think there's much advice in there.",
                    "label": 0
                },
                {
                    "sent": "And then in the second part I want to talk about.",
                    "label": 0
                },
                {
                    "sent": "Taking the assumptions of LDA and relaxing them in various ways, so one is to build topic models for prediction relation and supervised topic models.",
                    "label": 0
                },
                {
                    "sent": "The other is to relax the Dirichlet assumption and hence the conjugacy assumption and look at the logistic normal as a tool to building different kinds of topic models.",
                    "label": 0
                },
                {
                    "sent": "There will discuss dynamic and correlated topic models, and finally, briefly, I'll talk about Infinite Topic models which is otherwise known as or, which is an application of the hierarchical Dirichlet process.",
                    "label": 1
                },
                {
                    "sent": "I know ETA will.",
                    "label": 0
                },
                {
                    "sent": "Discuss that in much more detail next week.",
                    "label": 0
                },
                {
                    "sent": "Then finally I want to discuss interpreting and evaluating topic models, which is kind of a thorn in the side of topic models to be Frank, but we'll talk about it.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Make the assumption that documents are bags of words all the time.",
                    "label": 0
                },
                {
                    "sent": "You're sort of making it.",
                    "label": 0
                },
                {
                    "sent": "Well, I haven't talked about anything yet, but",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, it does seem like I'm going to make that assumption the whole time.",
                    "label": 0
                },
                {
                    "sent": "And yes, I will.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but I'll be.",
                    "label": 0
                },
                {
                    "sent": "But I'll be totally honest with you about it.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's right, yeah, yeah, no, I I, that's right.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so to answer the question, I like that question.",
                    "label": 0
                },
                {
                    "sent": "Yes, I think we're going to assume that documents are bags of words everywhere, but I'll try to point you to some.",
                    "label": 0
                },
                {
                    "sent": "There's been some excellent work out there that takes the same ideas and relaxes that assumption.",
                    "label": 0
                },
                {
                    "sent": "Basically allows documents to be to have Markovian structure, for example.",
                    "label": 0
                },
                {
                    "sent": "But yes, we will be explicit and we will assume that documents are bags of words here.",
                    "label": 0
                },
                {
                    "sent": "These are exchangeable, exchangeable models.",
                    "label": 0
                },
                {
                    "sent": "Good question.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry, I answered it with a joke.",
                    "label": 0
                },
                {
                    "sent": "I often do that.",
                    "label": 0
                },
                {
                    "sent": "It's no offense.",
                    "label": 0
                },
                {
                    "sent": "Other questions that will be politely answered.",
                    "label": 0
                },
                {
                    "sent": "OK. Um?",
                    "label": 0
                },
                {
                    "sent": "OK. Talk about that, alright?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we'll start this talk in earnest.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Latent seriously allocation is a probabilistic model, and I'm sure you.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Heard this already in this in this series, but the idea behind probabilistic modeling is to 1 treat your data as observations that arise from some kind of generative probabilistic process.",
                    "label": 1
                },
                {
                    "sent": "This is generative probabilistic modeling I should say.",
                    "label": 0
                },
                {
                    "sent": "And one that includes hidden variables structure that that we want to find in the data.",
                    "label": 1
                },
                {
                    "sent": "So for documents those hidden variables reflect the thematic structure of the collection that we don't have access to.",
                    "label": 1
                },
                {
                    "sent": "Step 2 is to then infer that hidden structure using posterior inference.",
                    "label": 0
                },
                {
                    "sent": "Basically, we're going to contemplate and compute the conditional distribution of the hidden variables given the observations, which is which are the documents themselves.",
                    "label": 1
                },
                {
                    "sent": "Third, we want to situate new data into the estimated model.",
                    "label": 0
                },
                {
                    "sent": "Typically, sometimes you might want to just analyze your documents and then look at the hidden variables as is and never, never believe that you'll see another piece of data again, but often you want to take your model and then you have some new data coming in that you want to do something with.",
                    "label": 0
                },
                {
                    "sent": "And so you need to be able to situate that new data into the estimated model.",
                    "label": 0
                },
                {
                    "sent": "In other words, how does this query or new document fit into the topic structure that I learned in the first part OK?",
                    "label": 1
                },
                {
                    "sent": "OK, so the intuition so with.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This kind of general recipe for probabilistic modeling, the intuition behind latent garishly allocation or LDA, is simply that documents exhibit multiple topics.",
                    "label": 1
                },
                {
                    "sent": "OK, so here is an example.",
                    "label": 0
                },
                {
                    "sent": "I think there's a way to do this.",
                    "label": 0
                },
                {
                    "sent": "Oh, OK. Um?",
                    "label": 0
                },
                {
                    "sent": "Doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "Oh good so.",
                    "label": 0
                },
                {
                    "sent": "Here's an example, so this is an article from the journal Science called Seeking Life's bare genetic necessities, and this article is basically about computing the number of genes that an Organism needs to survive.",
                    "label": 0
                },
                {
                    "sent": "Missionary evolutionarily and what I've done is I've highlighted different words in this article with different colors, so words like predictions, computer analysis, computational computer numbers.",
                    "label": 0
                },
                {
                    "sent": "I manually highlighted those in blue.",
                    "label": 0
                },
                {
                    "sent": "These are words about data analysis.",
                    "label": 0
                },
                {
                    "sent": "Words like genes genomes sequenced.",
                    "label": 0
                },
                {
                    "sent": "These are highlighted in yellow.",
                    "label": 0
                },
                {
                    "sent": "These are words about genomics and words like life and organisms and survive words about evolutionary biology.",
                    "label": 0
                },
                {
                    "sent": "I've highlighted these words in pink.",
                    "label": 0
                },
                {
                    "sent": "OK, so the intuition that I'm trying to convey is that this document somehow combines words about computer analysis.",
                    "label": 0
                },
                {
                    "sent": "Words about evolutionary biology and words about genomics.",
                    "label": 0
                },
                {
                    "sent": "OK, in contrast to the assumptions made by a mixture model, which says that.",
                    "label": 0
                },
                {
                    "sent": "All of these words come from a single component.",
                    "label": 0
                },
                {
                    "sent": "A single topic.",
                    "label": 0
                },
                {
                    "sent": "OK, so with that intuition in mind, the idea is to.",
                    "label": 0
                },
                {
                    "sent": "Express it as a generative probabilistic process and the way that works is.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "First, we are going to posit that there are some number of topics which will now formally define that live outside of the document collection.",
                    "label": 0
                },
                {
                    "sent": "OK, so here I have four topics listed.",
                    "label": 0
                },
                {
                    "sent": "There might be 96 underneath it, and each topic is a distribution over terms in the vocabulary, right?",
                    "label": 0
                },
                {
                    "sent": "So there's a fixed vocabulary.",
                    "label": 0
                },
                {
                    "sent": "We're going to assume that and every topic is a distribution over that fixed vocabulary, but different topics.",
                    "label": 0
                },
                {
                    "sent": "Have different words with different probabilities.",
                    "label": 0
                },
                {
                    "sent": "So for example, at the top I see a topic that has worked here.",
                    "label": 0
                },
                {
                    "sent": "Let's say I've ordered these words in order of their probabilities.",
                    "label": 0
                },
                {
                    "sent": "So at the top we have a topic that has gene with probability .4, DNA with probability .02.",
                    "label": 0
                },
                {
                    "sent": "Genetically probability .1 and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm calling about the genetics topic underneath it, the pink topic, the word life is probably .02 evolve .1 an Organism .1.",
                    "label": 0
                },
                {
                    "sent": "Then I have a topic and green about neuroscience.",
                    "label": 0
                },
                {
                    "sent": "With words like brain and neuron and nerve with high probability and finally a topic data number in computer with high probability.",
                    "label": 0
                },
                {
                    "sent": "OK so when I say topic, what I'm going to mean is distribution over fixed vocabulary, but that's cumbersome.",
                    "label": 0
                },
                {
                    "sent": "So we say topic.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Can they wet?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's an important point that every topic contains a probability for every word.",
                    "label": 0
                },
                {
                    "sent": "So even though it doesn't have high probability, the word data has some probability in the yellow topic, and it might be that if you have a word that a word can have high probability in two topics.",
                    "label": 0
                },
                {
                    "sent": "For example, example, you're probably sick of the word bank, could have probability in a topic about financial instruments, an also high probability in a topic about bodies of water.",
                    "label": 0
                },
                {
                    "sent": "For riverbank other questions.",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                },
                {
                    "sent": "I will will get there.",
                    "label": 0
                },
                {
                    "sent": "Yep, good question.",
                    "label": 0
                },
                {
                    "sent": "That's going to be the next the next thing, OK, so let's assume for now that those probabilities are all there, and we've got our topics, and there's 100 of them.",
                    "label": 0
                },
                {
                    "sent": "The generative process for each document then works like this.",
                    "label": 1
                },
                {
                    "sent": "First, we're going to choose a distribution over our topics.",
                    "label": 0
                },
                {
                    "sent": "So while a topic is a distribution over terms, a distribution over topics is a distribution over these 100 elements.",
                    "label": 0
                },
                {
                    "sent": "OK, so there are 100 topics.",
                    "label": 0
                },
                {
                    "sent": "Then this distribution.",
                    "label": 0
                },
                {
                    "sent": "Has 100 possible possible values, each one color coded by its topic, and here I've chosen one that has pink with some probability yellow with some probability and blue with some probability clear, and we're going to draw that from a Dirichlet distribution.",
                    "label": 0
                },
                {
                    "sent": "Have you seen the Dirichlet yet?",
                    "label": 0
                },
                {
                    "sent": "Have you seen the dearsley yet in this series?",
                    "label": 0
                },
                {
                    "sent": "No, OK, so good.",
                    "label": 0
                },
                {
                    "sent": "We'll talk about that.",
                    "label": 0
                },
                {
                    "sent": "This is drawn from a distribution over distributions called Adi Richley and.",
                    "label": 1
                },
                {
                    "sent": "That's the first step in generating a document.",
                    "label": 0
                },
                {
                    "sent": "The second step is to repeatedly draw a coin from this distribution.",
                    "label": 0
                },
                {
                    "sent": "So here I drew a blue coin, look up what topic that blue coin refers to, the blue topic.",
                    "label": 0
                },
                {
                    "sent": "That's why you gotta color code.",
                    "label": 0
                },
                {
                    "sent": "It's really important and then choose the word from that distribution.",
                    "label": 0
                },
                {
                    "sent": "OK, so here I rolled this 100 sided die.",
                    "label": 0
                },
                {
                    "sent": "I got blue.",
                    "label": 0
                },
                {
                    "sent": "I drew a word analysis from the blue distribution.",
                    "label": 0
                },
                {
                    "sent": "OK here I drew.",
                    "label": 0
                },
                {
                    "sent": "I can't point this way.",
                    "label": 0
                },
                {
                    "sent": "Can see.",
                    "label": 0
                },
                {
                    "sent": "I'm not good with Shadow, so will do it that way.",
                    "label": 0
                },
                {
                    "sent": "Here I chose yellow, I got the yellow coin and then I looked up the yellow distribution and got the word genome and somewhere else I chose the pink coin and I got the word Organism in life and now going back to your question you can see that.",
                    "label": 0
                },
                {
                    "sent": "No thanks, even the jumping kind of was good, but this can't last with that also.",
                    "label": 0
                },
                {
                    "sent": "Oh, that's nice.",
                    "label": 0
                },
                {
                    "sent": "It's green.",
                    "label": 0
                },
                {
                    "sent": "Green is good, you know it's more positive than red, which is like stop green is go.",
                    "label": 0
                },
                {
                    "sent": "What were we saying?",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, so.",
                    "label": 0
                },
                {
                    "sent": "So we choose these coins and then we choose the word from the corresponding distribution.",
                    "label": 0
                },
                {
                    "sent": "Here we chose the yellow coin, we choose the word genetic.",
                    "label": 0
                },
                {
                    "sent": "Here we chose the pink coin, we choose the word Organism and so on and so forth.",
                    "label": 0
                },
                {
                    "sent": "And this gets to your question.",
                    "label": 0
                },
                {
                    "sent": "I'm implicitly assuming now that the order of words doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "OK, because I'm choosing these coins independently of each other.",
                    "label": 0
                },
                {
                    "sent": "Now, of course, documents aren't really made this way.",
                    "label": 0
                },
                {
                    "sent": "If they were, they would be totally unreadable, but if you.",
                    "label": 0
                },
                {
                    "sent": "I should say when they are, they are totally unreadable, but.",
                    "label": 0
                },
                {
                    "sent": "If you had a document whose words were all shuffled and you looked at looked at those words, you might be able to get a sense that, oh, this is a document about genetics, computation and evolutionary biology by looking at the various types of words that occur in the document.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's important that the generative process doesn't have to be.",
                    "label": 0
                },
                {
                    "sent": "A precise description or a plausable description of how the document arose, in fact doesn't even have to generate documents that look realistic.",
                    "label": 0
                },
                {
                    "sent": "It just has to be a process that makes sense for your goal, which is in looking at the posterior finding these.",
                    "label": 0
                },
                {
                    "sent": "Thematically coherent terms and a little later, we'll discuss how one well, we won't discuss it much, but will try to think about how this generative process leads to these types of probabilities in the posterior.",
                    "label": 0
                },
                {
                    "sent": "But we will get there.",
                    "label": 0
                },
                {
                    "sent": "But is this generative process clear?",
                    "label": 0
                },
                {
                    "sent": "So this is important, the whole rest of the talk depends on it.",
                    "label": 0
                },
                {
                    "sent": "First choose a distribution over topics.",
                    "label": 0
                },
                {
                    "sent": "Then for each for each word, draw a colored coin from this distribution look up the distribution over terms associated with that coin and draw the word from that distribution.",
                    "label": 1
                },
                {
                    "sent": "And then that's the generative process for a single document, and then for another document we repeat the same process.",
                    "label": 0
                },
                {
                    "sent": "So notice that documents are going to have different distributions over topics, so while this document is about the pink.",
                    "label": 0
                },
                {
                    "sent": "Yellow and blue topic.",
                    "label": 0
                },
                {
                    "sent": "Another document might be about the green and blue topic, a document about computational neuroscience, for example.",
                    "label": 0
                },
                {
                    "sent": "OK, we repeat this process for every document clear.",
                    "label": 0
                },
                {
                    "sent": "Any questions yeah?",
                    "label": 0
                },
                {
                    "sent": "We're going to get right.",
                    "label": 0
                },
                {
                    "sent": "We're going to get this is this is just the imaginary generative probabilistic process that we're assuming our data came from, and what we're assuming in particular is that we've got these, and we've got a process for this.",
                    "label": 0
                },
                {
                    "sent": "OK, other questions.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the problem is of course we don't get to see any of this, right?",
                    "label": 1
                },
                {
                    "sent": "This is what we're imagining is there, and this is what we'd like to exploit and use.",
                    "label": 0
                },
                {
                    "sent": "But in reality all we have is a big stack of 10,000,000 documents.",
                    "label": 0
                },
                {
                    "sent": "So our goal.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is going to be to infer the underlying topic structure given all these documents?",
                    "label": 1
                },
                {
                    "sent": "First of all, possibly most interesting.",
                    "label": 0
                },
                {
                    "sent": "What are the topics that generated them under?",
                    "label": 0
                },
                {
                    "sent": "This is under these assumptions.",
                    "label": 0
                },
                {
                    "sent": "What are the distributions over terms that generated them under these assumptions and for each document, what is the distribution over topics associated with that document?",
                    "label": 0
                },
                {
                    "sent": "And maybe we care about for each word which topic generated each word.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's the algorithmic problem here.",
                    "label": 1
                },
                {
                    "sent": "Is to is to compute this this posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "This is a conditional distribution of all of these latent variables given the observations, which are the words of the documents.",
                    "label": 0
                },
                {
                    "sent": "Clear.",
                    "label": 0
                },
                {
                    "sent": "Not clear.",
                    "label": 0
                },
                {
                    "sent": "No, not not clear good.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's sort of the fun part and it's over.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm serious so.",
                    "label": 0
                },
                {
                    "sent": "We're going to code this model in a graphical model directed graphical model, so I want to I know you've seen these already.",
                    "label": 0
                },
                {
                    "sent": "I want to just quickly review them so that we're on the same page about the semantics of directed graphical models.",
                    "label": 0
                },
                {
                    "sent": "In directed graphical model, a node is a random variable.",
                    "label": 0
                },
                {
                    "sent": "So here I have nodes Y&X one or XN edges denote possible dependence between random variables.",
                    "label": 1
                },
                {
                    "sent": "So here the edge between Y and X1 means that X1 might depend on YXN might depend on Y as well.",
                    "label": 1
                },
                {
                    "sent": "Observed variables are shaded, so here I've observed X one through NI have not observed why?",
                    "label": 0
                },
                {
                    "sent": "Why is a hidden variable or a latent variable?",
                    "label": 1
                },
                {
                    "sent": "Those you can use interchangeably and plates denote replicated structure.",
                    "label": 0
                },
                {
                    "sent": "So instead of having to waste your ink on this big figure, you can shorthand it with.",
                    "label": 0
                },
                {
                    "sent": "Y goes to XN and draw a box around XN, which is called a plate and put big N in the lower right hand corner to denote that we have Big Ten of these little X ends, and they're all dependent, possibly on Y.",
                    "label": 0
                },
                {
                    "sent": "Clear.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the structure of this graph.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Defines the pattern of conditional independence ease that are encoded in the joint distribution of these random variables.",
                    "label": 1
                },
                {
                    "sent": "So from this graph we can read off all of the possible conditional independence.",
                    "label": 0
                },
                {
                    "sent": "Ease that among these random variables and and the structure of the graph also defines a factorization of the joint distribution of these random variables.",
                    "label": 0
                },
                {
                    "sent": "In particular here.",
                    "label": 0
                },
                {
                    "sent": "This here is the joint distribution of all of those random variables Y&X one through XN, and this graph means that this joint distribution can be written as.",
                    "label": 0
                },
                {
                    "sent": "P of Y times the product from N1 to N of P of XN given Y, so you notice that.",
                    "label": 0
                },
                {
                    "sent": "That just from this joint distribution you can see that X the X is are all conditionally independent given Y. OK, so so this and this and this all mean exactly the same thing.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "So with this simple little language in our hands, we can write down the latent.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Seriously, allocation model that I just described for you.",
                    "label": 0
                },
                {
                    "sent": "OK, and you can see why we need those plates so.",
                    "label": 0
                },
                {
                    "sent": "Each piece of this structure now is a random variable and let's the easiest way to understand this is to sandwich in from the outside into the middle.",
                    "label": 1
                },
                {
                    "sent": "OK, so the first thing I'll tell you is to ignore Ada and Alpha for now and Theta Theta sub DD is the document replication are the topic proportions that was that cartoon histogram that I drew for you with the pink in the blue and the yellow bars?",
                    "label": 0
                },
                {
                    "sent": "OK so that's called Theta D topic proportions.",
                    "label": 0
                },
                {
                    "sent": "We have one of these for every document it's in the D plate.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Oh, I should have started over here.",
                    "label": 0
                },
                {
                    "sent": "So beta K are the topics themselves.",
                    "label": 0
                },
                {
                    "sent": "That's where we start.",
                    "label": 0
                },
                {
                    "sent": "So each beta is a distribution over terms and we have K of them.",
                    "label": 0
                },
                {
                    "sent": "So K might be 100.",
                    "label": 0
                },
                {
                    "sent": "Alright so beta sub 96 is some distribution over words so beta lives on what we call the simplex.",
                    "label": 0
                },
                {
                    "sent": "The vocabulary simplex.",
                    "label": 0
                },
                {
                    "sent": "It's the space of all possible distributions, OK, and I'm assuming here that beta comes from a Dirichlet distribution, which is a distribution over these over in this type of space.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are our 100 topics and that's the K plate.",
                    "label": 0
                },
                {
                    "sent": "Now we have the document plate.",
                    "label": 0
                },
                {
                    "sent": "This is the corpus and we first have Theta D. These are the topic proportions as I mentioned the cartoon histogram.",
                    "label": 0
                },
                {
                    "sent": "And it is of dimension K. Because there are K topics.",
                    "label": 0
                },
                {
                    "sent": "That's not really illustrated here.",
                    "label": 0
                },
                {
                    "sent": "Then for each word that's the end plate inside the D plate.",
                    "label": 0
                },
                {
                    "sent": "If you want to be really picky, you might want to put a little D here.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We have ZDN called at the topic assignment.",
                    "label": 0
                },
                {
                    "sent": "That's the colored coin from the picture.",
                    "label": 0
                },
                {
                    "sent": "OK, so you can see that that depends on Theta because it's drawn from a distribution with parameter Theta.",
                    "label": 0
                },
                {
                    "sent": "So if Theta has probabilities for blue and yellow and pink then ZDN might be pink and it's drawn from that particular Theta.",
                    "label": 0
                },
                {
                    "sent": "Clear and and there is a Z for every word.",
                    "label": 0
                },
                {
                    "sent": "Remember, there's a colored coin for every word.",
                    "label": 0
                },
                {
                    "sent": "OK WDN depends on Z DN and beta all the betas.",
                    "label": 0
                },
                {
                    "sent": "WDN is the NTH word in the death document, and notice that WDN, which is difficult to say, is the only observed random variable in this whole model.",
                    "label": 0
                },
                {
                    "sent": "OK, all we observe are a bunch of words organized by document.",
                    "label": 0
                },
                {
                    "sent": "Now, why does W depend on Z and beta?",
                    "label": 0
                },
                {
                    "sent": "That's not rhetorical.",
                    "label": 0
                },
                {
                    "sent": "It could have been.",
                    "label": 0
                },
                {
                    "sent": "That's right, selected from the topic and and so why is it?",
                    "label": 0
                },
                {
                    "sent": "Does it depend on all the betas?",
                    "label": 0
                },
                {
                    "sent": "Topics.",
                    "label": 0
                },
                {
                    "sent": "OK, so because all the topics contain this word and what did you say?",
                    "label": 0
                },
                {
                    "sent": "All the topics you said all words.",
                    "label": 0
                },
                {
                    "sent": "That's right, all right, and so.",
                    "label": 0
                },
                {
                    "sent": "If this is going to encode the distribution of the words, then how do I?",
                    "label": 0
                },
                {
                    "sent": "What is the probability of this word given Z and beta?",
                    "label": 0
                },
                {
                    "sent": "Probability to that topic.",
                    "label": 0
                },
                {
                    "sent": "Existed.",
                    "label": 0
                },
                {
                    "sent": "Work with the chosen.",
                    "label": 0
                },
                {
                    "sent": "And the probability that that word would have been chosen from the top.",
                    "label": 0
                },
                {
                    "sent": "Right, so well, I'll say close, that's almost right so we're conditioning on Z an.",
                    "label": 0
                },
                {
                    "sent": "All the betas so the probability of the word.",
                    "label": 0
                },
                {
                    "sent": "See if I can do this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "In the in the joint distribution that this graphical model represents, we have probably the word given Z, so this is DN given Z DN and all the betas.",
                    "label": 0
                },
                {
                    "sent": "And what you said is that it's the probability of seeing that topic and the probability of seeing that word under that topic.",
                    "label": 0
                },
                {
                    "sent": "Is that what you said?",
                    "label": 0
                },
                {
                    "sent": "No, what did you say?",
                    "label": 0
                },
                {
                    "sent": "Probability of observing that word given the topic and the probability of observing that word given the topic assignment.",
                    "label": 0
                },
                {
                    "sent": "And then yeah, topics like a prior to that.",
                    "label": 0
                },
                {
                    "sent": "Right, So what?",
                    "label": 0
                },
                {
                    "sent": "It is?",
                    "label": 0
                },
                {
                    "sent": "What this is is I think, I think you said what it is.",
                    "label": 0
                },
                {
                    "sent": "It's the probability of observing this word from.",
                    "label": 0
                },
                {
                    "sent": "Remember, ZDN is a number from 1 to K because it comes from Theta and so you index the ZT NTH topic from beta one through K and you look up the probability of WDN from that topic.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's beta.",
                    "label": 0
                },
                {
                    "sent": "ZDN, WDN.",
                    "label": 0
                },
                {
                    "sent": "OK, it's basically we've got our K topics.",
                    "label": 0
                },
                {
                    "sent": "Each one is a distribution over words so.",
                    "label": 0
                },
                {
                    "sent": "If this is V and this is K, then.",
                    "label": 0
                },
                {
                    "sent": "Our topics this is sometimes called a topic matrix.",
                    "label": 0
                },
                {
                    "sent": "Our topics are each a distribution over all the V words.",
                    "label": 0
                },
                {
                    "sent": "Like we said, they all sum up to one.",
                    "label": 0
                },
                {
                    "sent": "And in the generative process, once we've selected Z, we basically know which topic this word is coming from, so it might come from this topic here and then we look up the particular cell of this word.",
                    "label": 0
                },
                {
                    "sent": "So if this is WDN, that cell is WDN this row.",
                    "label": 0
                },
                {
                    "sent": "And this column is ZDN.",
                    "label": 0
                },
                {
                    "sent": "Then we basically look up in beta the.",
                    "label": 0
                },
                {
                    "sent": "In in the CDF column the WDF word and get its probability from there.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's why we have the observed WDN depend both on CDN and all the betas because without observing them we don't know which one it is.",
                    "label": 0
                },
                {
                    "sent": "We just know that it depends on both those things, and you can kind of see that functionally here the probability of the word depends on both Z and beta.",
                    "label": 0
                },
                {
                    "sent": "Clear.",
                    "label": 0
                },
                {
                    "sent": "Red is actually probably the probability of the word is the, so the probability of the word given the topic assignment and all the topics is the WD NTH entry in the ZD NTH beta.",
                    "label": 0
                },
                {
                    "sent": "It's the word WDN entry in the topic assignment.",
                    "label": 0
                },
                {
                    "sent": "Sorry in the topic beta with index topic assignment, CDN.",
                    "label": 0
                },
                {
                    "sent": "Did that make things less clear?",
                    "label": 0
                },
                {
                    "sent": "Possibly?",
                    "label": 0
                },
                {
                    "sent": "Do you have a question?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Is forming a year you're doing?",
                    "label": 0
                },
                {
                    "sent": "Is it said the end and only one value?",
                    "label": 0
                },
                {
                    "sent": "And so you're saying the word depends like have been generated only by one that's right and it said that the end probably is like is a distribution and therefore we're going to.",
                    "label": 0
                },
                {
                    "sent": "Merchant results, he said.",
                    "label": 0
                },
                {
                    "sent": "Some point yes, OK.",
                    "label": 0
                },
                {
                    "sent": "Generate because I work might be coming from several different clubs.",
                    "label": 0
                },
                {
                    "sent": "OK. We try to repeat the questions.",
                    "label": 0
                },
                {
                    "sent": "Oh yeah, sorry.",
                    "label": 0
                },
                {
                    "sent": "Sure yeah, the question is that it looks like in this formula I'm assuming that, well, the question was with Z, but I say Z but we could say zed is zed, Zed, NA distribution?",
                    "label": 0
                },
                {
                    "sent": "Or is it a?",
                    "label": 0
                },
                {
                    "sent": "I'm assuming it's a single value?",
                    "label": 0
                },
                {
                    "sent": "I'm saying it's a topic assignment, a number from 1 to K when really we don't observe it, so it's a distribution that was the question and the answer is very glad you asked that question.",
                    "label": 0
                },
                {
                    "sent": "So here we're still.",
                    "label": 0
                },
                {
                    "sent": "Living in the fantastical magical world of treating our hidden variables as observed.",
                    "label": 0
                },
                {
                    "sent": "OK, so here I'm asking for the distribution of WDN given Z DN and beta one through K. Now we know you and I both know secretly they're not observed, so we're going to be putting posterior distributions.",
                    "label": 0
                },
                {
                    "sent": "We're going to have a posterior over these things, but for formulating the model, the conditional, the conditional distribution that this graphical model begs us for.",
                    "label": 0
                },
                {
                    "sent": "Is the probability of W given that I've observed the topic assignment and all of the topics and in that fantastical world ZDN is just a number from 1 to K?",
                    "label": 0
                },
                {
                    "sent": "That's an important distinction.",
                    "label": 0
                },
                {
                    "sent": "Theta D is a distribution over topics even in the fantastical world.",
                    "label": 0
                },
                {
                    "sent": "Because each Z comes from Theta and there can be multiple these for different words in the same document.",
                    "label": 0
                },
                {
                    "sent": "But ZDN is a number from 1 to K. It's an index from one to K. Is that clear to everyone?",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "W. Exactly, so that's not be included here.",
                    "label": 0
                },
                {
                    "sent": "Encoded, where the graphical model doesn't tell you that, that's right.",
                    "label": 0
                },
                {
                    "sent": "So the graphical model really just tells you what kinds of conditional in dependencies you can.",
                    "label": 0
                },
                {
                    "sent": "You can assume here and write the particular functional form of W given Z and beta, I'm specifying at the board here good point.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Kind of aggregate functions from the package of WD into the only like the maximum probability that it can belong to one of the topics.",
                    "label": 0
                },
                {
                    "sent": "I'm talking something disorder or even learning just from the data.",
                    "label": 0
                },
                {
                    "sent": "So there might be.",
                    "label": 0
                },
                {
                    "sent": "And the other one.",
                    "label": 0
                },
                {
                    "sent": "Aggregate maximum.",
                    "label": 0
                },
                {
                    "sent": "No, let me write down the joint distribution here.",
                    "label": 0
                },
                {
                    "sent": "That'll clear up everything.",
                    "label": 0
                },
                {
                    "sent": "The answer is no, we're not assuming that there's any maximums or aggregate functions at play.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I'll turn the light on.",
                    "label": 0
                },
                {
                    "sent": "Binding there were two.",
                    "label": 0
                },
                {
                    "sent": "I did exactly what you been told me to do earlier so.",
                    "label": 0
                },
                {
                    "sent": "It's gotta work alright.",
                    "label": 0
                },
                {
                    "sent": "Good, alright?",
                    "label": 0
                },
                {
                    "sent": "So let's just just let's just put everything in bright lights.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Again, fantastical world everything is observed.",
                    "label": 0
                },
                {
                    "sent": "What is the joint distribution of all of the hidden and observed variables according to this model first?",
                    "label": 0
                },
                {
                    "sent": "We have each topic coming from some distribution that's appropriate over topics.",
                    "label": 0
                },
                {
                    "sent": "That's going to be the richley OK, and so those are all independent of anything else.",
                    "label": 0
                },
                {
                    "sent": "And that's because these betas only depend on Ada.",
                    "label": 0
                },
                {
                    "sent": "OK, that's the first part of this joint distribution.",
                    "label": 0
                },
                {
                    "sent": "Now we have our documents.",
                    "label": 0
                },
                {
                    "sent": "We have the documents and what's the first random variable we generate for each document the topic proportions?",
                    "label": 0
                },
                {
                    "sent": "Which depend on Alpha.",
                    "label": 0
                },
                {
                    "sent": "Again, that's a distribution.",
                    "label": 0
                },
                {
                    "sent": "The dearest lady distribution that's appropriate for distributions over distributions.",
                    "label": 0
                },
                {
                    "sent": "That's easier to understand if it's written down, then within each document we have the words of the document.",
                    "label": 0
                },
                {
                    "sent": "OK, this prints may not be necessary, but they make it all clear.",
                    "label": 0
                },
                {
                    "sent": "So for each document there is N words in the document.",
                    "label": 0
                },
                {
                    "sent": "Just for simplicity, we assume that all documents have exactly 552 words, and.",
                    "label": 0
                },
                {
                    "sent": "We first draw the topic assignment from Theta D, sorry.",
                    "label": 0
                },
                {
                    "sent": "Right, that's akin to rolling the 100 sided die.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Finally, we draw the word.",
                    "label": 0
                },
                {
                    "sent": "Condition on Z DN and beta one through K. OK. That's the joint distribution of the observed and hidden random variables.",
                    "label": 0
                },
                {
                    "sent": "And just so I said that this comes from a deer sleigh.",
                    "label": 0
                },
                {
                    "sent": "And this comes from a dear sleigh.",
                    "label": 0
                },
                {
                    "sent": "Which we're going to talk about in a second.",
                    "label": 0
                },
                {
                    "sent": "What is the probability of ZDN given Theta D?",
                    "label": 0
                },
                {
                    "sent": "So if Theta D is here, we have 100 topics.",
                    "label": 0
                },
                {
                    "sent": "And here we have our probabilities of various of those topics.",
                    "label": 0
                },
                {
                    "sent": "This is 1326 thirty nine and 42 total coincidence, and let's say Z DN is 42.",
                    "label": 0
                },
                {
                    "sent": "What's P of ZDN given Theta D?",
                    "label": 0
                },
                {
                    "sent": ".33 right in general, what is P of ZDN given Theta D?",
                    "label": 0
                },
                {
                    "sent": "What's that?",
                    "label": 0
                },
                {
                    "sent": "Exactly well, Theta, it's Theta D, ZDN.",
                    "label": 0
                },
                {
                    "sent": "It's this CDN indexes data D OK, same trick is before and.",
                    "label": 0
                },
                {
                    "sent": "I should say that in the in the in the nitpicky mathematics of this, it's not exactly how you represent these random variables, but don't worry about it.",
                    "label": 0
                },
                {
                    "sent": "For now.",
                    "label": 0
                },
                {
                    "sent": "We'll get there.",
                    "label": 0
                },
                {
                    "sent": "Maybe we won't, but you'll get there.",
                    "label": 0
                },
                {
                    "sent": "So that's OPZDN given Theta D equals that and P of WDN.",
                    "label": 0
                },
                {
                    "sent": "Given ZDN an beta went through K we discussed was a slightly more complicated thing.",
                    "label": 0
                },
                {
                    "sent": "Beta ZDN, WDN.",
                    "label": 0
                },
                {
                    "sent": "OK so with this and this and these two facts and this joint distribution here we fully specified the model.",
                    "label": 0
                },
                {
                    "sent": "Subject to telling you about the rich late distribution.",
                    "label": 0
                },
                {
                    "sent": "Clear good OK?",
                    "label": 0
                },
                {
                    "sent": "So other questions.",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                },
                {
                    "sent": "Possibly have demotivated you to ask questions now, but please know I like those questions.",
                    "label": 0
                },
                {
                    "sent": "It's important OK.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "It's been awhile since I've spoken.",
                    "label": 0
                },
                {
                    "sent": "Some are good.",
                    "label": 0
                },
                {
                    "sent": "The restriction that's convenient, so this is the this is.",
                    "label": 0
                },
                {
                    "sent": "This process described as a graphical model and fully specified, and now I just want to tell you about that.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Seriously, distribution.",
                    "label": 0
                },
                {
                    "sent": "So the Dirichlet distribution is an exponential family distribution over the simplex.",
                    "label": 1
                },
                {
                    "sent": "Have you seen the exponential family yet?",
                    "label": 0
                },
                {
                    "sent": "Yes, OK, so it's an exponential family distribution page.",
                    "label": 0
                },
                {
                    "sent": "In all of that information over the simplex, which are positive vectors that sum to one OK and.",
                    "label": 0
                },
                {
                    "sent": "The deer sleep distribution so Theta.",
                    "label": 0
                },
                {
                    "sent": "Here is a point on the it's really called the K -- 1 simplex.",
                    "label": 0
                },
                {
                    "sent": "It's the distributions over K elements and.",
                    "label": 0
                },
                {
                    "sent": "The Dirichlet is parameterized by a K vector Alpha.",
                    "label": 0
                },
                {
                    "sent": "Alpha is just K positive values.",
                    "label": 0
                },
                {
                    "sent": "That parameter is the deer sleigh and the density function of the richly is given here, so it's a product of each component to the power of Alpha I -- 1 times.",
                    "label": 0
                },
                {
                    "sent": "Guy times this, which is the normalizing constant and this gamma function is a is one of the special functions and it's essentially like a real valued extension of the of the factorial function.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as the factorial function.",
                    "label": 0
                },
                {
                    "sent": "This constant here.",
                    "label": 0
                },
                {
                    "sent": "This is just a function of Alpha, so it's a constant with respect to the density with respect to the random variable Theta.",
                    "label": 0
                },
                {
                    "sent": "This constant here just make sure that this whole thing integrates to one, which as you know densities must do so.",
                    "label": 0
                },
                {
                    "sent": "This is the Dirichlet distribution and like I said, it's a distribution over the simplex over positive vectors that sum to one which are basically parameters to discrete distributions to multinomial distributions OK. And the Dirichlet is special because it's conjugate to the multinomial.",
                    "label": 1
                },
                {
                    "sent": "Have you learned about conjugacy?",
                    "label": 0
                },
                {
                    "sent": "I think I know you did 'cause I came.",
                    "label": 0
                },
                {
                    "sent": "I was here for that.",
                    "label": 0
                },
                {
                    "sent": "Dearsley is conjugate to the multinomial.",
                    "label": 0
                },
                {
                    "sent": "What that means is that given multinomial multinomial observation or multinomial observations, the posterior distribution of Theta given those observations is still a Dirichlet.",
                    "label": 0
                },
                {
                    "sent": "I'm going, I think it's worth giving you the details of that in a second.",
                    "label": 0
                },
                {
                    "sent": "The parameter Alpha controls the mean shape and the sparsity of data by sparsity of data I mean how many atoms in this distribution over atoms tend to have positive probability or or?",
                    "label": 0
                },
                {
                    "sent": "They all will have some positive probability, but tend to have high probability.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "Yeah, and so we're using the deer sleigh twice in this model.",
                    "label": 0
                },
                {
                    "sent": "The topic proportions are K dimensional.",
                    "label": 0
                },
                {
                    "sent": "Dirichlet and the topics are V dimensional dearsley OK, want to hear more details about the deer sleigh?",
                    "label": 0
                },
                {
                    "sent": "Yes, good.",
                    "label": 0
                },
                {
                    "sent": "Zubin does so.",
                    "label": 0
                },
                {
                    "sent": "I feel like he speaks for everything for you.",
                    "label": 0
                },
                {
                    "sent": "I think you want to.",
                    "label": 0
                },
                {
                    "sent": "That's really fascinating.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Dearsley oh I I I used to have like a really lame figure showing.",
                    "label": 0
                },
                {
                    "sent": "Different examples of deer slays.",
                    "label": 0
                },
                {
                    "sent": "And then I looked on Wikipedia and found a much cooler figure and took it so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so I'll explain this figure.",
                    "label": 0
                },
                {
                    "sent": "OK, so this figure shows a couple.",
                    "label": 0
                },
                {
                    "sent": "Let's forget the K -- 1 nonsense for second, let's call them 3 dimensional Irish lace.",
                    "label": 0
                },
                {
                    "sent": "OK really, it's 2 dimensional.",
                    "label": 0
                },
                {
                    "sent": "Dear slays, I'm calling them 3 dimensional displays.",
                    "label": 0
                },
                {
                    "sent": "So here's what these figures look like.",
                    "label": 0
                },
                {
                    "sent": "So remember the deer sleigh represents a distribution over some number of elements, and let's say Theta comes from a deer sleigh with parameters 111.",
                    "label": 0
                },
                {
                    "sent": "OK, that means that as I draw a random variables from Theta as I draw from this theorist way, I'm going to get distributions over three elements.",
                    "label": 0
                },
                {
                    "sent": "OK, so one such distribution, for example.",
                    "label": 0
                },
                {
                    "sent": "So here's element AB&C.",
                    "label": 0
                },
                {
                    "sent": "One such distribution over three elements is the distribution that places all of its mass on a. OK, on this triangle that's here.",
                    "label": 0
                },
                {
                    "sent": "Hey, that place is all of its massane.",
                    "label": 0
                },
                {
                    "sent": "Another distribution places all of its mass on B.",
                    "label": 0
                },
                {
                    "sent": "That's here placing all of its mass on be OK.",
                    "label": 0
                },
                {
                    "sent": "I won't insult your intelligence and draw it again.",
                    "label": 0
                },
                {
                    "sent": "The third one places all of its mass on.",
                    "label": 0
                },
                {
                    "sent": "See now.",
                    "label": 0
                },
                {
                    "sent": "The way this triangle looks works is that.",
                    "label": 0
                },
                {
                    "sent": "Every points a between A&B here represents.",
                    "label": 0
                },
                {
                    "sent": "See having probability zero and.",
                    "label": 0
                },
                {
                    "sent": "A having some positive probability and B having one minus that probability.",
                    "label": 0
                },
                {
                    "sent": "OK, so in other words, if I clamp the probability of C at zero and I put some probability on A and some probability on B and let's assume that sums to one, then that point might be.",
                    "label": 0
                },
                {
                    "sent": "Let's see B has a little more than a.",
                    "label": 0
                },
                {
                    "sent": "That point might be right here on the simplex.",
                    "label": 0
                },
                {
                    "sent": "OK, analogous the points between A&C analogous.",
                    "label": 0
                },
                {
                    "sent": "The points between B&C.",
                    "label": 0
                },
                {
                    "sent": "Clear so we can clamp one of these at zero.",
                    "label": 0
                },
                {
                    "sent": "Contemplate all the possible essentially binary distributions between the other two points and that represents the continuum along the edges of this triangle.",
                    "label": 0
                },
                {
                    "sent": "OK, and the points inside the triangle.",
                    "label": 0
                },
                {
                    "sent": "Are the points where a has some probability, B has some probability and C has some probability, so that might be that point.",
                    "label": 0
                },
                {
                    "sent": "OK, every point in the triangle is some point in the distribution space over distributions of three items.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "The Dirichlet Place is a distribution over that space.",
                    "label": 1
                },
                {
                    "sent": "And the Deer Slayer 111 is very special if all the alphas are equal to 1 then the richly 111 is the uniform distribution on this space.",
                    "label": 0
                },
                {
                    "sent": "Every there's a nuanced here which is that these actual edges, the points on the line are not possible.",
                    "label": 0
                },
                {
                    "sent": "You can't have something have zero probability under dearsley that has that has zero probability.",
                    "label": 0
                },
                {
                    "sent": "But you can get arbitrarily close, so 111 on the interior of this triangle puts units the same probability anywhere.",
                    "label": 0
                },
                {
                    "sent": "It's called the uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "Alright, and in general so where in this triangle is the space where a B&C have equal probability?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Let me OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you so no 111.",
                    "label": 0
                },
                {
                    "sent": "So remember the Dirichlet is parameterized by Alpha and Alpha is a vector of length K of positive values.",
                    "label": 0
                },
                {
                    "sent": "They can be anything.",
                    "label": 0
                },
                {
                    "sent": "So in the 3D, richly Alpha has three components.",
                    "label": 0
                },
                {
                    "sent": "An 111 refers to A1 equal A2 equal A3 equal 1.",
                    "label": 0
                },
                {
                    "sent": "OK, and so those are the parameters to this distribution and what I'm saying is that when you set those parameters exactly to one like that, that leads to a distribution where any point on the simplex has the same probability.",
                    "label": 0
                },
                {
                    "sent": "OK, good.",
                    "label": 0
                },
                {
                    "sent": "But where is the point where Alpha AB&C all have probability 1/3?",
                    "label": 0
                },
                {
                    "sent": "Where is that point?",
                    "label": 0
                },
                {
                    "sent": "It's in the middle, right?",
                    "label": 0
                },
                {
                    "sent": "It's right here.",
                    "label": 0
                },
                {
                    "sent": "OK, and so if you have a deer sleigh where A1 equals A2 equals A3 equals say 5.",
                    "label": 0
                },
                {
                    "sent": "What that does is puts a bump in the middle and spreads the contours around that bump like this somehow.",
                    "label": 0
                },
                {
                    "sent": "OK, in general some properties of the deer slay that are worth knowing.",
                    "label": 0
                },
                {
                    "sent": "The expectation of Theta given Alpha.",
                    "label": 0
                },
                {
                    "sent": "Equals so Theta I the expectation of the ith component.",
                    "label": 0
                },
                {
                    "sent": "So we have a random.",
                    "label": 0
                },
                {
                    "sent": "We have a distribution over this space.",
                    "label": 0
                },
                {
                    "sent": "We can contemplate the expectation of any of these components.",
                    "label": 0
                },
                {
                    "sent": "Just a multivariate distribution and expectation of Theta.",
                    "label": 0
                },
                {
                    "sent": "I given Alpha equals Alpha I over the sum of the Alpha.",
                    "label": 0
                },
                {
                    "sent": "OK, so with 555, the expectation of each of the components is going to be 1/3 because 5 / 15 is 1/3.",
                    "label": 0
                },
                {
                    "sent": "OK, and that's always true.",
                    "label": 0
                },
                {
                    "sent": "The expectation of data I given Alpha is this, and so if you have a an asymmetric theoretically distribution, if you have like A1 equals 10 Alpha 2 equals, well, I don't say it.",
                    "label": 0
                },
                {
                    "sent": "So here we have our simplex and here is a distribution centered somewhere, not in the middle.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Here is a distribution center not in the middle.",
                    "label": 0
                },
                {
                    "sent": "These are all distribution center dot in the middle.",
                    "label": 0
                },
                {
                    "sent": "OK, so that determines the location of this hump.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Alpha yes, that's the next thing I'm going to.",
                    "label": 0
                },
                {
                    "sent": "No, yes and no are the answers to your questions.",
                    "label": 0
                },
                {
                    "sent": "The first question was the greater Alpha, the less Peaky, yes, the next question was you can't have Alpha less than one, no, but I'm going to explain those in that a bit OK, but first just to look for the location of the hump, the location of the hump is determined by this expectation.",
                    "label": 0
                },
                {
                    "sent": "OK, now let's get to the penis of the hump so.",
                    "label": 0
                },
                {
                    "sent": "And let's assume for now that Alpha is greater than one.",
                    "label": 0
                },
                {
                    "sent": "The Alpha less than one case will deal with separately.",
                    "label": 0
                },
                {
                    "sent": "OK, I'll go over here.",
                    "label": 0
                },
                {
                    "sent": "Can you see that?",
                    "label": 0
                },
                {
                    "sent": "Can you all see the board over there?",
                    "label": 0
                },
                {
                    "sent": "That way I only have to work with one set of lights.",
                    "label": 0
                },
                {
                    "sent": "OK, good alright?",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 important piece of the Dirichlet parameterisation.",
                    "label": 0
                },
                {
                    "sent": "The other important piece is the sum of the alphas.",
                    "label": 0
                },
                {
                    "sent": "OK, so the sum of the alphas determines the Peaky Ness of the Dirichlet when the sum of the alphas is is is.",
                    "label": 0
                },
                {
                    "sent": "Small, then the dearest lady is going to be very spread out and the greater the sum of the alphas, the more Peaky the dearest lady comes.",
                    "label": 0
                },
                {
                    "sent": "At this point at its expectation.",
                    "label": 0
                },
                {
                    "sent": "OK. And sometimes you see this called.",
                    "label": 0
                },
                {
                    "sent": "Sometimes called S. And this is sometimes called M. OK, so this is like the mean and this is the scaling OK and just to to so an alternative parameterisation of the Dirichlet is as a point on this simplex, the mean and a scaling parameter S which is which is which determines how peak it is around the mean.",
                    "label": 0
                },
                {
                    "sent": "And I'm saying this to foreshadow the various lectures on nonparametric Bayesian methods that are coming up next week, where you parameterise the infinite dimensional directly in precisely the same way.",
                    "label": 0
                },
                {
                    "sent": "OK, but just.",
                    "label": 0
                },
                {
                    "sent": "Put that away in your brains somewhere safe.",
                    "label": 0
                },
                {
                    "sent": "OK. Um?",
                    "label": 0
                },
                {
                    "sent": "Now I need to go to my safe brain space.",
                    "label": 0
                },
                {
                    "sent": "Think about what to say next.",
                    "label": 0
                },
                {
                    "sent": "Penis Alpha, lesson 1 and then posterior OK?",
                    "label": 0
                },
                {
                    "sent": "This, I think will be useful.",
                    "label": 0
                },
                {
                    "sent": "Later.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                },
                {
                    "sent": "These boards are heavier than at my University.",
                    "label": 0
                },
                {
                    "sent": "I can see why you guys are so buff here.",
                    "label": 0
                },
                {
                    "sent": "It's serious.",
                    "label": 0
                },
                {
                    "sent": "This is very.",
                    "label": 0
                },
                {
                    "sent": "This is very solid stuff.",
                    "label": 0
                },
                {
                    "sent": "This not all mathematics is for these boards, OK?",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                },
                {
                    "sent": "It's kind of invigorating.",
                    "label": 0
                },
                {
                    "sent": "Alpha less than one, what happens?",
                    "label": 0
                },
                {
                    "sent": "OK, equivalently.",
                    "label": 0
                },
                {
                    "sent": "S lesson 1.",
                    "label": 0
                },
                {
                    "sent": "Is an Alpha is less than one?",
                    "label": 0
                },
                {
                    "sent": "Is you get sparsity.",
                    "label": 0
                },
                {
                    "sent": "OK, so on the three simplex.",
                    "label": 0
                },
                {
                    "sent": "Here's our our friendly triangle so you know I've been drawing.",
                    "label": 0
                },
                {
                    "sent": "These deer slays like hump somewhere in the middle of the simplex.",
                    "label": 0
                },
                {
                    "sent": "But on the when Alpha is less than one, you end up with a different shape.",
                    "label": 0
                },
                {
                    "sent": "You end up with a shape that that.",
                    "label": 0
                },
                {
                    "sent": "Places increased probability at the corners of the simplex.",
                    "label": 0
                },
                {
                    "sent": "OK, so its contours.",
                    "label": 0
                },
                {
                    "sent": "Hey, let's see let me think as contours.",
                    "label": 0
                },
                {
                    "sent": "This looks like this.",
                    "label": 0
                },
                {
                    "sent": "For example, OK, so it's like a shape like like that, yeah, that I draw that wrong.",
                    "label": 0
                },
                {
                    "sent": "What's that?",
                    "label": 0
                },
                {
                    "sent": "Why's it going to S less than one?",
                    "label": 0
                },
                {
                    "sent": "As less than K. Thank you, yeah.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "It's S less than K, thanks.",
                    "label": 0
                },
                {
                    "sent": "The board gets heavier as you make more mistakes actually.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Is there another question or if that's it, did you have a question?",
                    "label": 0
                },
                {
                    "sent": "Like that?",
                    "label": 0
                },
                {
                    "sent": "They are to the outside, OK.",
                    "label": 0
                },
                {
                    "sent": "However, you're supposed to draw this, let's say, in two dimensions.",
                    "label": 0
                },
                {
                    "sent": "It's really easy.",
                    "label": 0
                },
                {
                    "sent": "So in two dimensions you have between zero and one right.",
                    "label": 0
                },
                {
                    "sent": "You just have the probability of one thing and then probably 1 minus the other thing.",
                    "label": 0
                },
                {
                    "sent": "So when in the deer size that I've been drawing, it looks like this where you've got some mean and a spread around the mean, and when alphas are less than one you get this kind of shape.",
                    "label": 0
                },
                {
                    "sent": "However, that looks in three dimensions.",
                    "label": 0
                },
                {
                    "sent": "I'm not sure, OK?",
                    "label": 0
                },
                {
                    "sent": "Exactly, it's like this.",
                    "label": 0
                },
                {
                    "sent": "Yes yes fine very good.",
                    "label": 0
                },
                {
                    "sent": "OK now I can't even move it.",
                    "label": 0
                },
                {
                    "sent": "Alright erase this horrible picture except pretty in kind of a spirograph sense.",
                    "label": 0
                },
                {
                    "sent": "And yes, yes, just take those contours as pictures.",
                    "label": 0
                },
                {
                    "sent": "The same.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "It still doesn't seem right to me, but it doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "Let's move on.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what happens when office one.",
                    "label": 0
                },
                {
                    "sent": "But really, when Alphas lesson 1 don't waste your time with two and three dimensional Dirichlet's.",
                    "label": 0
                },
                {
                    "sent": "Think about 50 and 100 dimensional Dirichlet's OK.",
                    "label": 0
                },
                {
                    "sent": "So now let's draw that space so.",
                    "label": 0
                },
                {
                    "sent": "The way you do that.",
                    "label": 0
                },
                {
                    "sent": "Is of course you can't, but what we can do is I can.",
                    "label": 0
                },
                {
                    "sent": "So let's say we have a 50 dimensional directly, so there's 50 components here.",
                    "label": 0
                },
                {
                    "sent": "OK, and.",
                    "label": 0
                },
                {
                    "sent": "I encourage you.",
                    "label": 0
                },
                {
                    "sent": "In you know, in R to sample abunch of dearest Lazan, plot them so that you can see what happens with different parameterisations of the Dirichlet.",
                    "label": 0
                },
                {
                    "sent": "And in saying that I'm encouraging you to do two things.",
                    "label": 0
                },
                {
                    "sent": "One is to do that and two is to use R, so I can really go on and on about R Now.",
                    "label": 0
                },
                {
                    "sent": "So let's say Alpha was greater than one.",
                    "label": 0
                },
                {
                    "sent": "So let's call let's say Alpha look like this.",
                    "label": 0
                },
                {
                    "sent": "OK, and let's say this 50 of them or whatever OK, so let's say that's Alpha alright.",
                    "label": 0
                },
                {
                    "sent": "Then when we draw from this dearest way, we're going to get shapes that sort of look like this, but that vary from this in some way or another, subject to how Big Alpha is, right?",
                    "label": 0
                },
                {
                    "sent": "So again, if you do this in R, you can easily plot 100 points on the simplex drawn from this distribution and see how it kind of looks like this.",
                    "label": 0
                },
                {
                    "sent": "But sometimes this one will be a little bigger.",
                    "label": 0
                },
                {
                    "sent": "Sometimes this one will be a little smaller.",
                    "label": 0
                },
                {
                    "sent": "Sometimes this one will be a little bigger and so on.",
                    "label": 0
                },
                {
                    "sent": "Is that clear?",
                    "label": 0
                },
                {
                    "sent": "I'm not going to do it because I think it would be too time consuming.",
                    "label": 0
                },
                {
                    "sent": "OK?",
                    "label": 0
                },
                {
                    "sent": "When Alpha is much smaller than one.",
                    "label": 0
                },
                {
                    "sent": "However, when each Alpha is smaller than one or some of the alphas are smaller than one, let's let's be simple and let's talk about what's called the exchange obliviously.",
                    "label": 0
                },
                {
                    "sent": "Exchangeable deer sleigh is dearest way AAA and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, there's just one parameter, one scalar, and we assume that M is the is right in the center of the simplex.",
                    "label": 0
                },
                {
                    "sent": "OK, so when Alpha is smaller than one in the exchangeable dearsley, what you get are sparse distributions, so you get things like this.",
                    "label": 0
                },
                {
                    "sent": "OK, we're only some of the atoms have positive probability and many of them have probability very very close to 0.",
                    "label": 0
                },
                {
                    "sent": "Again, file this away in your brains 'cause this is going to have an important connection in nonparametric Bayesian methods.",
                    "label": 0
                },
                {
                    "sent": "Now, which especially in the exchangeable dearsley which components have positive masks, those are.",
                    "label": 0
                },
                {
                    "sent": "Up to the those are totally at random.",
                    "label": 0
                },
                {
                    "sent": "However, as Alpha gets smaller and smaller and smaller, fewer components will have positive mass, so this is a sparse, exchangeable Dirichlet.",
                    "label": 0
                },
                {
                    "sent": "Then answer question about Alpha lesson 1.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Finally, any other questions?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "One per uniform distribution.",
                    "label": 0
                },
                {
                    "sent": "That's right.",
                    "label": 0
                },
                {
                    "sent": "It's fine, it's also summer peak where it's not respond precisely between Alpha is less than one.",
                    "label": 0
                },
                {
                    "sent": "As you get further and further below towards closer to 0, you get sparser and sparser and sparser draws from your dear ishly.",
                    "label": 0
                },
                {
                    "sent": "So when Theta comes from a deer sleigh.",
                    "label": 0
                },
                {
                    "sent": "And Z comes from Theta OK, or let's say XN comes from.",
                    "label": 0
                },
                {
                    "sent": "So that's that's like a little piece of our LDA model, right?",
                    "label": 0
                },
                {
                    "sent": "Data comes from the rich, lazy and comes from a multinomial with parameter Theta.",
                    "label": 0
                },
                {
                    "sent": "This is also called the discrete distribution.",
                    "label": 0
                },
                {
                    "sent": "Technically, it's not a multinomial less, you have another number there, which is how many draws.",
                    "label": 0
                },
                {
                    "sent": "But don't worry bout it says again comes from multinomial data comes from Italy.",
                    "label": 0
                },
                {
                    "sent": "You can ask the question what is P of Theta given Z1 through N?",
                    "label": 0
                },
                {
                    "sent": "Other words vibe serve N topic assignments.",
                    "label": 0
                },
                {
                    "sent": "What is the conditional distribution of the topic proportions given those topic assignments OK, and what conjugacy means is that if we let.",
                    "label": 0
                },
                {
                    "sent": "NZ 12 Big NB accounts.",
                    "label": 0
                },
                {
                    "sent": "Of each Atom.",
                    "label": 0
                },
                {
                    "sent": "In other words, if I saw a topic 13 six times, then end sub 13 of Z1 through N = 6 OK, it's just a little counting function clear.",
                    "label": 0
                },
                {
                    "sent": "OK then later given Z1 through N is a dearest leg with parameters Alpha plus AN-12.",
                    "label": 0
                },
                {
                    "sent": "OK, we just add the number of times we saw topic 13 to the Alpha parameter for topic 13 and that's the new D richley parameter.",
                    "label": 0
                },
                {
                    "sent": "So notice that you will rarely have sparse well.",
                    "label": 0
                },
                {
                    "sent": "Now don't notice anything.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's the posterior Dirichlet.",
                    "label": 0
                },
                {
                    "sent": "OK, and so notice that your instinct earlier that as the as the sum of the alphas gets larger you get a peak and peak your distribution.",
                    "label": 0
                },
                {
                    "sent": "That's mirrored here as we see more and more observations are posterior, get speaker and peak here in peakier and that's very intuitive.",
                    "label": 0
                },
                {
                    "sent": "As I roll the die more and more and more.",
                    "label": 0
                },
                {
                    "sent": "My idea of what the distribution of the faces are is going to become more.",
                    "label": 0
                },
                {
                    "sent": "I'm going to come more and more confident about it and so it's the whole idea of the prior speaking so loudly in the data speaking.",
                    "label": 0
                },
                {
                    "sent": "Loud as loud, proportional to how much data you saw.",
                    "label": 0
                },
                {
                    "sent": "As you see more data you become more and more confident in the estimate that that data gives you.",
                    "label": 0
                },
                {
                    "sent": "OK, we've totally diverged from topic modeling.",
                    "label": 0
                },
                {
                    "sent": "So any questions about this and then we can get back to it for 15 minutes.",
                    "label": 0
                },
                {
                    "sent": "OK. Yep.",
                    "label": 0
                },
                {
                    "sent": "Brian, I mean when you say sports, do you mean that the other, the other ones are exactly no close to 0?",
                    "label": 0
                },
                {
                    "sent": "They're going to be like .001?",
                    "label": 0
                },
                {
                    "sent": "You can see that in our.",
                    "label": 0
                },
                {
                    "sent": "I will turn the lights off.",
                    "label": 0
                },
                {
                    "sent": "By the way, when you sample from a dear slate in order to do it well, I'll tell you how to do it later.",
                    "label": 0
                },
                {
                    "sent": "Well, let's keep going now, OK?",
                    "label": 0
                },
                {
                    "sent": "OK, very good, we're back so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to turn the lights on in one second.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here we go again.",
                    "label": 0
                },
                {
                    "sent": "We have the LDA model.",
                    "label": 0
                },
                {
                    "sent": "LDA is a mixed.",
                    "label": 0
                },
                {
                    "sent": "It's called a mixed membership model in statistics and it really builds on the work.",
                    "label": 1
                },
                {
                    "sent": "The seminal LSA work latent semantic analysis of Dearwester at all, and probabilistic latent semantic analysis from Thomas Hoffman.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Actually, let's not go into details about this.",
                    "label": 0
                },
                {
                    "sent": "The reason it's called a mixed membership model is that you can think of the model where each document comes from a single cluster as like it.",
                    "label": 0
                },
                {
                    "sent": "That's a mixture model where we have each document is associated with a single Z.",
                    "label": 0
                },
                {
                    "sent": "And here since documents are associated with Theta, a distribution over clusters, then each document can be associated with multiple components.",
                    "label": 0
                },
                {
                    "sent": "That's what that's what the mixed membership idea is.",
                    "label": 0
                },
                {
                    "sent": "So when you're reading the Journal of Bayesian analysis or the Annals of Applied Statistics, and you hear about mixed membership models of ranked data in this data and that data.",
                    "label": 0
                },
                {
                    "sent": "You can think about LDA as an instance of a mixed membership model.",
                    "label": 0
                },
                {
                    "sent": "OK, that's in statistics, that's what this is called.",
                    "label": 1
                },
                {
                    "sent": "And for document collections, another grouped data, I should embolden that group data.",
                    "label": 0
                },
                {
                    "sent": "This is rather than thinking of a document as a single data point, it's really a group of data points.",
                    "label": 0
                },
                {
                    "sent": "The data are the words and the document represents the group of words for group data.",
                    "label": 1
                },
                {
                    "sent": "Often the mixed membership assumption is more appropriate than a simple finite mixture, which is the natural alternative.",
                    "label": 1
                },
                {
                    "sent": "OK. And I should mention that in statistics this same model was invented for population genetics analysis and had a lot of impact there by Stevens and Pritchard.",
                    "label": 0
                },
                {
                    "sent": "So there they don't care bout documents.",
                    "label": 0
                },
                {
                    "sent": "What they care about is real science, and they're modeling people as being mixtures of their various.",
                    "label": 0
                },
                {
                    "sent": "Ancestry, so you know it's all my my case is a bad case 'cause everybody's from the same small town in Hungary, but my wife like her family, her mom's from Denmark, and her dad's from.",
                    "label": 0
                },
                {
                    "sent": "He's a pretty.",
                    "label": 0
                },
                {
                    "sent": "He's a character.",
                    "label": 0
                },
                {
                    "sent": "He's from Canada.",
                    "label": 0
                },
                {
                    "sent": "Basically, let's say and.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And so you know her genes are kind of a mixture of the very of the different populations that she comes from.",
                    "label": 0
                },
                {
                    "sent": "It's a lovely combination.",
                    "label": 0
                },
                {
                    "sent": "I should say that to the video.",
                    "label": 0
                },
                {
                    "sent": "And and so this model is a model for for looking at how people's genes mix and then what are the results of it.",
                    "label": 0
                },
                {
                    "sent": "You can think of the populations as the topics and the people as mixing over the topics.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's get on before I make some kind of marriage error alright?",
                    "label": 0
                },
                {
                    "sent": "Good.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, the from a collection of documents.",
                    "label": 1
                },
                {
                    "sent": "This is all a nice story, but we don't get to observe any of this stuff.",
                    "label": 0
                },
                {
                    "sent": "We don't get to observe the topics or the topic proportions or the topic assignments, and that's really the central algorithmic goal of of working with a model like this.",
                    "label": 0
                },
                {
                    "sent": "You want to infer all of this nice structure so we can use it.",
                    "label": 1
                },
                {
                    "sent": "So the idea is to infer the per word topic assignment CDN the per document topic proportions Theta D and the per corpus topic distributions beta K, and then to use posterior expectations.",
                    "label": 0
                },
                {
                    "sent": "Basically the expectation.",
                    "label": 0
                },
                {
                    "sent": "Of all those things, given, the words here is the hidden variables.",
                    "label": 0
                },
                {
                    "sent": "Here are the words.",
                    "label": 1
                },
                {
                    "sent": "Use the posterior expectations of these things to perform whatever task it is we care about, such as information retrieval, document similarity classification, whatever it is you're doing.",
                    "label": 0
                },
                {
                    "sent": "OK, see if this works good, whoops.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so there are a lot of approximate OK we will in the second part of this talk we're going to see how we can actually compute the exact posterior like we can here in this nice conjugate model.",
                    "label": 0
                },
                {
                    "sent": "And so a lot of approximate posterior inference algorithms for this model have been developed including mean field variational methods.",
                    "label": 1
                },
                {
                    "sent": "This is what we're going to describe later on expectation propagation, which I know you learned about a bit yesterday.",
                    "label": 1
                },
                {
                    "sent": "Collapsed Gibbs sampling, which I'll talk about later on and collapsed.",
                    "label": 0
                },
                {
                    "sent": "Variational inference, which is very exciting, but I will only allude to it later on, and there's also been some work on how to compare these different types of inference algorithms and how well they do.",
                    "label": 0
                },
                {
                    "sent": "There's a great paper from I smell this year comparing them and also some theoretical work about collapsed variational inference versus mean field variational inference that I worked on with a student.",
                    "label": 0
                },
                {
                    "sent": "We're going to get into details of approximate posterior inference later on.",
                    "label": 0
                },
                {
                    "sent": "I guess on Thursday, but for now I want to show you a little bit of.",
                    "label": 0
                },
                {
                    "sent": "Let's assume we have an approximate posterior algorithm that we like and let's look at some real data and what this model does with real data.",
                    "label": 0
                },
                {
                    "sent": "OK, any questions about this?",
                    "label": 0
                },
                {
                    "sent": "So this is the model.",
                    "label": 0
                },
                {
                    "sent": "The only things that are observed, other words, and so we want to fill in the rest of this with approximate posterior inference, OK?",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For these first pieces, I want to look at the OC Art collection of Science Magazine from 1990 to 2000.",
                    "label": 0
                },
                {
                    "sent": "So just to give you an idea of what this data.",
                    "label": 0
                },
                {
                    "sent": "Is the basically have you all seen J store.org?",
                    "label": 0
                },
                {
                    "sent": "OK, good so Jay Store is a an online archive of scientific articles and what they do.",
                    "label": 0
                },
                {
                    "sent": "It's an amazing project is they take the original bound articles and they scan them and then they run them through optical character recognition software and then they index the original scans via the output of the OCR.",
                    "label": 0
                },
                {
                    "sent": "Algorithm so the it's cool because so you know it'll screw stuff up like this is 2 columns and maybe their OCR software can't handle two columns and it maybe it doesn't get all the words right, but it's pretty accurate and maybe the punctuation is totally bogus, but that's OK, but for doing search when you want to search for a word like.",
                    "label": 0
                },
                {
                    "sent": "Phenotype, then, as long as it got phenotype a bunch of times in the article, you're going to get the right scan back.",
                    "label": 0
                },
                {
                    "sent": "And since you never interact specifically with the noisy OCR, it's a very effective system for searching online for searching articles online that are archived and they have science all the way back to 1880 will in fact see that later on.",
                    "label": 0
                },
                {
                    "sent": "OK, but of course J store has problem, which is that they have millions and millions of articles, but they're only organized by Journal and by date and what they want is a system where people can go through and browse and examine these articles in a topic oriented way.",
                    "label": 0
                },
                {
                    "sent": "But since they've scanned so many articles, it's out of the question to annotate these scans with keywords.",
                    "label": 0
                },
                {
                    "sent": "Moreover, that's in they want this to be automatic.",
                    "label": 0
                },
                {
                    "sent": "They don't want to have to read every article an assignment, keywords.",
                    "label": 0
                },
                {
                    "sent": "The whole point is that you don't have to do that, you can just put them through a machine.",
                    "label": 0
                },
                {
                    "sent": "And then let scholars use them effectively so they're interested in looking at the kind of topic decomposition of their archives.",
                    "label": 0
                },
                {
                    "sent": "So here we're taking their collection of Science magazine for 10 years.",
                    "label": 1
                },
                {
                    "sent": "We're looking at 17,000 documents.",
                    "label": 0
                },
                {
                    "sent": "This is 11,000,000 words, and we used a vocabulary of 20,000 terms.",
                    "label": 0
                },
                {
                    "sent": "Basically, as a practical point, you typically remove stop words words like the but or and, and very rare words which don't affect the inference much.",
                    "label": 0
                },
                {
                    "sent": "Or somebody recently told me might affect the inference badly.",
                    "label": 0
                },
                {
                    "sent": "We can talk about that later.",
                    "label": 0
                },
                {
                    "sent": "But so this is, the documents are analyzed and we fit a 100 topic LDA model using variational inference, but could have been anything.",
                    "label": 1
                },
                {
                    "sent": "OK so here is that same art.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that I showed you, I should say that this article was actually in a test set, so this article we didn't.",
                    "label": 0
                },
                {
                    "sent": "We didn't fit with this is the article.",
                    "label": 0
                },
                {
                    "sent": "How many genes as an Organism needs to survive in an evolutionary sense and using posterior inference.",
                    "label": 0
                },
                {
                    "sent": "So here are the WD NS for this article, right?",
                    "label": 0
                },
                {
                    "sent": "I didn't draw the graph model, but here are the W. DNN and with posterior inference this is the expected value of Theta.",
                    "label": 0
                },
                {
                    "sent": "Given those WS.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Or town lights of this, but it's just the expectation of Theta D given W one through N and the topics.",
                    "label": 0
                },
                {
                    "sent": "That we estimated.",
                    "label": 0
                },
                {
                    "sent": "OK, that's what this is an approximation of, so this is like the real version of the cartoon histogram that I drew for you before it's saying, look, I've got these hundred topics we haven't looked at them yet, and given this new article here is, here are the topic proportions that are associated with that article.",
                    "label": 0
                },
                {
                    "sent": "Clear.",
                    "label": 0
                },
                {
                    "sent": "OK, and you can see that it's not like it's using every topic for this particular article.",
                    "label": 0
                },
                {
                    "sent": "It's saying this topic has high probability.",
                    "label": 0
                },
                {
                    "sent": "This topic has high probability.",
                    "label": 0
                },
                {
                    "sent": "This topic has high probability and so on.",
                    "label": 0
                },
                {
                    "sent": "And even though it has 100 topics to choose from, it's not as though it's using all of them to describe this article.",
                    "label": 0
                },
                {
                    "sent": "So we're getting some description of this article in terms of these topics.",
                    "label": 0
                },
                {
                    "sent": "Hey.",
                    "label": 0
                },
                {
                    "sent": "What's that?",
                    "label": 0
                },
                {
                    "sent": "We we so this was all done through post your inference.",
                    "label": 0
                },
                {
                    "sent": "So let me just loops going the wrong way.",
                    "label": 0
                },
                {
                    "sent": "Don't look, don't look.",
                    "label": 0
                },
                {
                    "sent": "This is secret.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Remember, these things are all latent.",
                    "label": 0
                },
                {
                    "sent": "The topics are latent.",
                    "label": 0
                },
                {
                    "sent": "The topic assignments are late in the topic.",
                    "label": 0
                },
                {
                    "sent": "Proportions are late and we never observe them.",
                    "label": 0
                },
                {
                    "sent": "So with approximate posterior inference, we infer them just from the collection of articles, and we're going to look at them in a second.",
                    "label": 0
                },
                {
                    "sent": "Unless you look before at the secret stuff.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So here are the topic proportions and now.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's finally look at these topics.",
                    "label": 0
                },
                {
                    "sent": "Let's look at the top words from the top topics in this article, and these are the top.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pics I showed you before you can see.",
                    "label": 0
                },
                {
                    "sent": "So what I've done here instead of showing you all 20,000 words and the probabilities and, uh, squinting and looking at the numbers, I've ordered the words by probability.",
                    "label": 0
                },
                {
                    "sent": "So the most probable words in this topic, which is the most probable topic in that document.",
                    "label": 0
                },
                {
                    "sent": "Our words, like human genome, DNA genetic gene sequence, gene molecular and so on.",
                    "label": 0
                },
                {
                    "sent": "Another top topic are words like evolution, evolutionary species, organisms, life origin, biology and so on.",
                    "label": 0
                },
                {
                    "sent": "And here is disease.",
                    "label": 0
                },
                {
                    "sent": "Host bacteria, diseases, resistance.",
                    "label": 0
                },
                {
                    "sent": "And the computer modeling words OK and I want to emphasize that we didn't have to set any probabilities here.",
                    "label": 0
                },
                {
                    "sent": "All we did was was arrange painstakingly with J store and their lawyers to get their data.",
                    "label": 0
                },
                {
                    "sent": "Then we ran our algorithm on that data and this topical decomposition of this article comes out of running that algorithm.",
                    "label": 0
                },
                {
                    "sent": "There's no probabilities need to be set anywhere.",
                    "label": 0
                },
                {
                    "sent": "This is what's called unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "I should have mentioned that 16 times by now and I didn't.",
                    "label": 0
                },
                {
                    "sent": "OK. You do still need to set the number of topics which we can talk about how to do that in the later part of the series, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Why do you not use any impression aggressive staining algorithms so I can see again?",
                    "label": 0
                },
                {
                    "sent": "Gene gene.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so again we should we.",
                    "label": 0
                },
                {
                    "sent": "You could stem and and if this were done in some kind of industrial grade way then we would probably stem to make the best possible interpretable topics for the JSTOR users.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Other questions.",
                    "label": 0
                },
                {
                    "sent": "OK, and so you can see that this is capturing what seems to be thematically related words which I'm going to want us to contemplate in a little bit.",
                    "label": 0
                },
                {
                    "sent": "OK, here's another example.",
                    "label": 0
                },
                {
                    "sent": "This is.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About chaotic Beatles and Michael Hassle and and this is about mathematical models of insects and beetles, and again we can play the same game we do.",
                    "label": 0
                },
                {
                    "sent": "Post your inference.",
                    "label": 0
                },
                {
                    "sent": "We get back topic proportions and we look at the most probable words from the.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same model for this document and you see words like words about mathematical problems worth about words about models and statistics, words about like selection and.",
                    "label": 0
                },
                {
                    "sent": "Don't even know what the word is for this.",
                    "label": 0
                },
                {
                    "sent": "Population theory and words about ecology and so on.",
                    "label": 0
                },
                {
                    "sent": "Going with The Beatles.",
                    "label": 0
                },
                {
                    "sent": "And so again.",
                    "label": 0
                },
                {
                    "sent": "These these topics and these topics are all in the same model and the approximate posterior inference algorithm is deciding to use them in different ways depending on the different observed words.",
                    "label": 0
                },
                {
                    "sent": "OK, and so.",
                    "label": 0
                },
                {
                    "sent": "These tools then, once you've marked up your entire corpus with topics and with topic, proportions and so on, you can use these tools to browse.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Collection so here for example is another article from Science.",
                    "label": 0
                },
                {
                    "sent": "Here are some top topics from this from this article.",
                    "label": 0
                },
                {
                    "sent": "Here I'm I'm showing which words were assigned to different topics.",
                    "label": 0
                },
                {
                    "sent": "This usually makes a little bit less sense because it has to assign every word to a topic, even ones that have low probability.",
                    "label": 0
                },
                {
                    "sent": "And but you can see things like statistics evaluating data, statistical means comparisons in one topic, words like acid, an proteins and spacing, and amino in one topic and words like.",
                    "label": 0
                },
                {
                    "sent": "General Text provides recent.",
                    "label": 0
                },
                {
                    "sent": "That seems like a bogus topic or not a bogus topic, but seems like these words it had no better place to put them, but you can see that it's decomposed this article into multiple overlapping recurring patterns of words, and then you can do things like ask for similar documents according to these topic proportions.",
                    "label": 0
                },
                {
                    "sent": "So rather than use words to find document similarity, we can use the topics to find document similarity and here are the titles of the of the documents that are most similar to this document.",
                    "label": 0
                },
                {
                    "sent": "By topic, and that can be a form of what's called query expansion.",
                    "label": 0
                },
                {
                    "sent": "For example, where if I write an article about my cat and I use the word cat, and you write an article about your cat and you use the word feline, then traditional document similarity metric won't say that our articles are similar, even though our cats might be extremely similar and whereas topic model knowing that cat and feline or somehow in the same group will say that our articles are similar in that indeed our cats are very similar personalities.",
                    "label": 0
                },
                {
                    "sent": "So this is the way that you can use these types of models, and in fact, if you look at rexha.org, I don't know if you've seen this.",
                    "label": 0
                },
                {
                    "sent": "This is a project that came out of UMass Amherst from Andrew Mccallum's Group, and they are they use topic models to help you browse a large collection of computer science academic articles, so it's worth checking that out.",
                    "label": 0
                },
                {
                    "sent": "Questions, yeah, does it turn out?",
                    "label": 0
                },
                {
                    "sent": "Similar.",
                    "label": 0
                },
                {
                    "sent": "You know where little meaning good question, so I said that we remove stop words words like of but and or the but really well not really, but what that means is that we really remove words that occur in all of the articles.",
                    "label": 0
                },
                {
                    "sent": "So if a word occurs in all of the articles then we remove it.",
                    "label": 0
                },
                {
                    "sent": "In fact, if it occurs in more than 90% of the articles then we remove it because your intuition is correct that the model is going to want to place those words with high probability in all the topics because they occur so frequently.",
                    "label": 0
                },
                {
                    "sent": "That they don't help decompose the collection.",
                    "label": 0
                },
                {
                    "sent": "That's typically why we do that.",
                    "label": 0
                },
                {
                    "sent": "There are ways of doing that post hoc after you get the topics looking at the instead of ordering the topics by probable, most probable words, there are other scores you can use to order the words.",
                    "label": 0
                },
                {
                    "sent": "I can talk about that on Thursday if you like, but that's a good question and leads.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To this question for you so actually I won't show the answer like yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "And that is.",
                    "label": 0
                },
                {
                    "sent": "It's not.",
                    "label": 0
                },
                {
                    "sent": "Why doesn't it happen that rather than putting those words into all of the topics, yes, in the model?",
                    "label": 0
                },
                {
                    "sent": "Discover a new topic which only has those areas and this topic will appear in all the documents.",
                    "label": 0
                },
                {
                    "sent": "There is an intuition for that, but I'm I'm not going to answer it 'cause it relates to my next question, which is for you.",
                    "label": 0
                },
                {
                    "sent": "You plural.",
                    "label": 0
                },
                {
                    "sent": "What, why on Earth does this work?",
                    "label": 0
                },
                {
                    "sent": "That's the question, so I told you don't look at that secret I.",
                    "label": 0
                },
                {
                    "sent": "You know, I showed you this generative model and.",
                    "label": 0
                },
                {
                    "sent": "Even let's look at this picture better.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at this picture, so here's the generative process and you know we seem to just make it up.",
                    "label": 0
                },
                {
                    "sent": "And consistently we get back these.",
                    "label": 0
                },
                {
                    "sent": "Distributions over words that look like the matically.",
                    "label": 0
                },
                {
                    "sent": "Related terms.",
                    "label": 0
                },
                {
                    "sent": "And when you're doing applied Bayesian modeling.",
                    "label": 0
                },
                {
                    "sent": "It's important to think both about the prior, which is what we're specifying here.",
                    "label": 0
                },
                {
                    "sent": "Essentially an to think about the posterior.",
                    "label": 0
                },
                {
                    "sent": "So why could we expect to find these kinds of combinations of words as highly probable words under the posterior, and if so, why so?",
                    "label": 0
                },
                {
                    "sent": "It's time to go, unfortunately, but I want you to contemplate that and then on Thursday will begin by hearing your answers to those to that question.",
                    "label": 0
                },
                {
                    "sent": "I have an answer, but who knows if it's right and I'm interested in you thinking about what the posterior distribution means here and why it might do something like this and what it's actually.",
                    "label": 0
                },
                {
                    "sent": "This is capturing somehow in plain English.",
                    "label": 0
                },
                {
                    "sent": "I mean, I want the description to be in plain English, you don't have to analyze English documents.",
                    "label": 0
                },
                {
                    "sent": "What is this capturing?",
                    "label": 0
                },
                {
                    "sent": "I want to know that answer in plain English.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}