{
    "id": "scv43tfd7qw6bng7hqdprlcfy5tewrvr",
    "title": "Discriminative Learning of Sum-Product Networks",
    "info": {
        "author": [
            "Robert Gens, Department of Computer Science and Engineering, University of Washington"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Deep Learning"
        ]
    },
    "url": "http://videolectures.net/nips2012_gens_discriminative_learning/",
    "segmentation": [
        [
            "This is a talk about learning some product networks discriminatively.",
            "I'm Rob guns and this is joint work with Pedro Dominguez at the University of Washington."
        ],
        [
            "Some product networks are new deep probabilistic architecture with tractable inference."
        ],
        [
            "Now you might not be familiar with SPN, so I'll give you a brief guided tour of a sample.",
            "SPN over some image."
        ],
        [
            "Pixels so at the bottom we have our leaf distributions which model those pixels and then products of those distributions would be considered."
        ],
        [
            "Image caps features which of?"
        ],
        [
            "So we have small and large and then sums over those products would be."
        ],
        [
            "Considered mixture models over image Patch features.",
            "And then products over those sums could."
        ],
        [
            "Be considered part decompositions of which you have one."
        ],
        [
            "Like this or one like this?",
            "And then finally our route.",
            "Some node can."
        ],
        [
            "Be considered as a pooling function over party compositions.",
            "And this whole structure could be repeated the features, the mixtures, the party compositions as many layers as you want.",
            "So as you can see, SPN's are quite expressive, despite their tracked ability."
        ],
        [
            "So in this talk, I'm first going to motivate the use of sum product networks.",
            "Then I'm going to review how they work.",
            "Then I'm going to show you how to add feature functions to SPMS and train them discriminatively.",
            "And finally I'll show some experiments where sum product networks outperform support vector machines and deep models on image classification bench."
        ],
        [
            "Let's first start with some motivation.",
            "Many of these graphical models, because they allow us to compactly represent the joint."
        ],
        [
            "Distribution of many variables.",
            "Unfortunately, inference in graphical models is intractable, which often forces us to reduce the expressivity of our model.",
            "Often with low tree with.",
            "Although many approximate inference schemes have been proposed, they're all fraught with difficulties.",
            "In contrast, some product networks perform fast exact inference on hightree with models."
        ],
        [
            "Deep architectures use layers of hidden variables to increase their representational power.",
            "However, when we add layers of hidden variables to graphical models, this compounds the already intractable inference problem.",
            "Some product network can be viewed as a deep model that has full probabilistic semantics and tractable inference over many layers."
        ],
        [
            "Up until now, we've only seen generative training of sum product networks.",
            "However, for many applications, we find that discriminative learning is performance better.",
            "This is our contribution.",
            "With conditional random fields, we can use flexible feature functions.",
            "But and yet the conditional partition function remains intractable.",
            "In this talk, we're going to combine the advantages."
        ],
        [
            "SPMS with those of conditional models, namely, flexible feature functions with exact inference over high tree."
        ],
        [
            "So what is an SPN?",
            "Let's define espions recursively."
        ],
        [
            "You know, very distribution is an SPN.",
            "You can pick your favorite disc."
        ],
        [
            "Treat or continuous distribution for the same."
        ],
        [
            "For this example."
        ],
        [
            "Will define X as being a multinomial.",
            "Now."
        ],
        [
            "Now one variable is not that interesting, so only."
        ],
        [
            "Our next definition.",
            "Which is that?"
        ],
        [
            "A product of SPN's over disjoint variables is also an SPN.",
            "So we just brought over another variable Y, which could be any distribution.",
            "But in this example we also have at a multi node."
        ],
        [
            "No.",
            "Now you might notice that the product of two variables is just independent, so we need our last definition."
        ],
        [
            "Which is that?"
        ],
        [
            "A weighted sum of SPN's over the same variables is also an SPN.",
            "So now we just brought over another distribution over these same two variables and formed a weighted sum.",
            "And that now establishes dependencies.",
            "So this is."
        ],
        [
            "Some node sums out a hidden variable, just like in a mixture model.",
            "And.",
            "So what we have now is that sum product networks are rooted directed acyclic graph with univariate distributions at the leaves and sums and product nodes as internal nodes.",
            "Now we can also use multivariate distributions at the leaves, but you have to be able to ensure that you can compute their partition functions and modes in closed form."
        ],
        [
            "And the beauty of this definition is that it's recursive, so we can build up layers and layers of more and more and more structure.",
            "And yet we're always remaining tractable."
        ],
        [
            "The key property of SPN's is that all marginals are computable in linear time, and it's easy to see why."
        ],
        [
            "Because the partition function of a product node over disjoint sets of variables is just the product of the partition functions of its children.",
            "And the."
        ],
        [
            "Partition function of a sum note over the same sets of variables is just the weighted sum of the partition functions of its children."
        ],
        [
            "A marginal is just the partition function of an SPN, where you replace those leaf distributions with Evan over evidence variables with Delta functions or the probability of evidence."
        ],
        [
            "So for example, if we were curious about the probability that X is zero in this model with set, those least leaf distributions that we have evidence for with the probability."
        ],
        [
            "That evidence, and then for all other variables we."
        ],
        [
            "Marginalized them by setting those leaf distributions to be their partition functions."
        ],
        [
            "We then compute the values of notes going bottom."
        ],
        [
            "And this would give us our answer.",
            "If the leaf distributions and weights in the model are normalized, then the partition function is just one."
        ],
        [
            "If not, the partition function, like other marginals, is computable in linear time."
        ],
        [
            "A related prop."
        ],
        [
            "Pretty is that all mapu states are computable in linear time, we just replaced."
        ],
        [
            "Some nodes with Max nodes."
        ],
        [
            "And so we're using the Max product semiring instead of the sum product semiring.",
            "And we can do this because the ideas behind some product networks apply to any semiring."
        ],
        [
            "So for example, if we're interested in the maximum probability with evidence X = 0, we would set evidence on our leaf distributions just like before, but then for those nodes that we don't."
        ],
        [
            "Have evidence for we wouldn't marginalized them will set them to be the probability of their."
        ],
        [
            "Woods we then compute the values of those modes of values of net."
        ],
        [
            "So it's going bottom up and that would give us the maximum probability."
        ],
        [
            "And then compute a trace back.",
            "Just like with Kirby."
        ],
        [
            "And this would lead to the argmax."
        ],
        [
            "You might think that because their definition is so simple that espions are quite limited, when in fact there are many models that are special cases of spins and spins are more general than each of them for."
        ],
        [
            "Apple Junction trees harkle mixture models non recursive PCF.",
            "These models with context specific independence models with determinism.",
            "Another hightree with models."
        ],
        [
            "In the world of compactly represent about probability distributions."
        ],
        [
            "Their graphical model."
        ],
        [
            "And some product networks."
        ],
        [
            "At the intersection, lie existing tractable models which would include hierarchical mixture models and thin junction trees.",
            "Now when I say compactly represent aghbal, what I mean is that if I took that SPN that I showed you at the beginning of the talk and try to represent it as a as a graphical model, it's conditional independence graph would be a complete click, so that would not be compactly represent Abaline which solely exist in SPN territory."
        ],
        [
            "Sum product networks were introduced in the paper last year by Putin Domingo's.",
            "They explored updates for EM and gradient descent.",
            "And since emigrating to send both involve inference, they explored soft and hard variance of inference.",
            "Soft entrance you compute marginals or expectations and with hard inference you compute the counts from MLP states.",
            "And by the end of this talk, we're going to complete this table."
        ],
        [
            "With discriminative espions, we optimized the conditional likelihood of Y given X or Y."
        ],
        [
            "Error query variables."
        ],
        [
            "HR hidden variables corresponding to some nodes."
        ],
        [
            "And X are evidence variables."
        ],
        [
            "Now the trick is that we're going to treat these evidence variables as constants in the network."
        ],
        [
            "We can take any generative SPN and turn it into a discriminative SPN by adding feature functions.",
            "So here we have a generative SPN over variables Y1Y2, and we have a hidden variable that some doubt at the root we also."
        ],
        [
            "Other evidence variables, which are the pixels of an image."
        ],
        [
            "We can compute any feature functions we like and then place them in the network's children of product nodes.",
            "The reason we can do this is because when those features take on a value, those values get folded into the weights of the parents, some node and we have a generative SPN."
        ],
        [
            "Again, so because we can do this process to any generative SPM, it means that there's a greater variety of discriminative espions."
        ],
        [
            "With discriminative training, we're going to follow the."
        ],
        [
            "Radiance of the conditional log likelihood, which takes on a form that identical to when we train CRF's where."
        ],
        [
            "Have a positive first term where we're given the correctly."
        ],
        [
            "And a negative second term where we sum over all labels.",
            "The key."
        ],
        [
            "The difference between training CRF's and training SPN's is that these summations are always tractable for SPMS.",
            "We don't have to reduce the expressivity of our model in order to make these things tractable."
        ],
        [
            "We compute gradients in our model using back propagate."
        ],
        [
            "Where we can we set the values of our leaf distribution?"
        ],
        [
            "And then compute the values of nodes going bottom up and back propagation starts."
        ],
        [
            "Through we're going to use the gradient symbol to note the partial derivative of the network with respect to."
        ],
        [
            "Node which."
        ],
        [
            "For the route is of course one."
        ],
        [
            "The gradient down the branch of a."
        ],
        [
            "Download is just the weight of that branch times the gradient of the parents on node, so the gradient would flow down this some node as follows."
        ],
        [
            "Now let's follow the gradient down.",
            "The child on the left."
        ],
        [
            "As follows from the derivative, the grading down the branch of a."
        ],
        [
            "Product node is just equal to the product of the."
        ],
        [
            "Eyes of the other children multiplied by the gradient of the."
        ],
        [
            "Current product so that gradient would flow."
        ],
        [
            "Like this?"
        ],
        [
            "But there's a huge problem with backpropagation, which is that as the gradient flows from the root down to the leaves, the signal gets washed out, it becomes uniform.",
            "This is known as."
        ],
        [
            "Gradient diffusion, that's what makes deep learning hard and espions are no exception to that."
        ],
        [
            "We found when training some product networks."
        ],
        [
            "Hard inference can overcome the gradient diffusion problem.",
            "So on the left we have soft inference where we compute marginals or expectations.",
            "And on the right we have hard inference where we compute counselor, maybe states.",
            "Now."
        ],
        [
            "There are other reasons to use hard inference."
        ],
        [
            "Besides overcoming great infusion."
        ],
        [
            "Such as when our goal is to predict the most probable structure and."
        ],
        [
            "In general, we might replace some nations with maximization's.",
            "For speed attract ability, but this is not as important.",
            "Recipes which are always tractable."
        ],
        [
            "The hard gradient is derived from the soft gradient, but we replaced also."
        ],
        [
            "Animations with maximization's.",
            "Again."
        ],
        [
            "Have a positive first term that's given the correct label and a negative."
        ],
        [
            "Second term, when we maximize overall labels, and intuitively when our model guesses the correct label, the two terms will be equal and the gradient for this example would be 0."
        ],
        [
            "The hard gradient is best explained graphically."
        ],
        [
            "Where the two maximization's are computed using with MLP inference in our network."
        ],
        [
            "We notice that the two backtraces tracebacks are just products.",
            "And since the log of a product is just a sum, corresponding terms will cancel out."
        ],
        [
            "What we're left with are these green lines, which indicate those weights that should be increased because they lead to the correct label and the red lines, which are those which that should be decreased because they lead to the incorrect label."
        ],
        [
            "So now we have an intuitive wait update for the hard gradient, which is that the partial derivative with respect to await WI is equal to Delta CI over WI."
        ],
        [
            "Jealousy is the difference between the number of times the weight WI appears in the traceback, with the correct label and the number of times the weight WI appears in the traceback when the model guesses.",
            "You might notice that this update looks like it looks like structured perceptron, and in our paper we actually show a re parameterisation that makes that happen."
        ],
        [
            "So to our weight update table, we've added discriminative gradient descent with soft and hard updates."
        ],
        [
            "And you'll notice that all of the soft inference updates.",
            "Are proportional to the partial derivative of the network with respect to apparent some node SKU, meaning it's going to suffer from great infusion.",
            "And notice how all of the hardinfo."
        ],
        [
            "Updates involve the term CI, which are the counselor maybe in France."
        ],
        [
            "Now for experiments."
        ],
        [
            "We tested discriminatively trained espions on two image classification benchmarks, Cifar 10 and STL 10 and for this task were given a fixed network architecture and we're going to discriminate learning."
        ],
        [
            "To be able to compare our method to other methods, we're going to use the same feature extraction pipeline proposed by Coates at."
        ],
        [
            "Where we extract the Dictionary of Image Patch features using K means."
        ],
        [
            "And then for any image on our data set, we compare Patch with that image to our diction."
        ],
        [
            "Using the triangle encoding, this deal is a vector of length K. So when we apply this."
        ],
        [
            "So all patches of an image.",
            "With a 32 by 32 image and six by six pixel patches, this will deal with a tensor of size 27 by 27 by K that."
        ],
        [
            "Answers then downsampled by Max pooling to be a G by G by K tensor."
        ],
        [
            "That tensor is then fed into architecture."
        ],
        [
            "We use an SPN architecture that's inspired by this successful star models of files and swap little where we have a root some node of."
        ],
        [
            "Attend classes and each week."
        ],
        [
            "US is a product of its parts.",
            "And each part is."
        ],
        [
            "The mixture over parts, subtypes, and then each parts."
        ],
        [
            "Type is summed over all locations in the image where that part could exist.",
            "We use the leaf distribution of the logistic function E to the X dot F, where F is the portion of our image sensor that falls under apart at that location.",
            "So for example, a class might be a bird, and hypothetically speaking apart might be a beak, and you might have two types of beaks, like curved beaks and straight beaks, and then you add some overall locations in the image where the beak could occur."
        ],
        [
            "We compare against methods that downsample their image feature tensor to be a four by 4 by K tensor.",
            "So here we have 3."
        ],
        [
            "The results that were presented by Coates at all where the where we have a support vector machine over RK means features and then we have.",
            "Two method, we have an autoencoder generated features and restricted Boltzmann machine generated features.",
            "And on the X axis, we're varying the dictionary size K. And we notice that in general, as we increase the size of the dictionary, the test accuracy increases as well."
        ],
        [
            "There is a recent CPR paper by Gia at all that learns pooling functions over these features."
        ],
        [
            "And we find our discriminative espions do even better than that.",
            "So in particular."
        ],
        [
            "We"
        ],
        [
            "That are espions are able to do better than the pooling functions using a 5th as many features and when we compare ourselves to support vector machine."
        ],
        [
            "We find that we're able to use 20 * 2 year features than they do."
        ],
        [
            "And we believe this is the case because the SPN is modeling spatial structure among features that the other methods are not modeling.",
            "Moving on to a final."
        ],
        [
            "The features."
        ],
        [
            "We achieved the best published results at the time on Cifar 10 using a fraction of the features of the other methods.",
            "The."
        ],
        [
            "Story is similar for STL 10 where we compare against three methods that learn unsupervised features.",
            "The first 2 methods learn image Patch dictionaries using two different penalties on the dictionary and the third method learns three layers of locally of local receptive fields that are learned by correlation among features.",
            "Our discriminatively trained espions do even better than these methods, even though we don't use the unlabeled data."
        ],
        [
            "Future work."
        ],
        [
            "Includes Max margin SPM."
        ],
        [
            "Learning espian structure."
        ],
        [
            "Applying discriminative structure prediction."
        ],
        [
            "An approximate inference using SPN Switch.",
            "For example, you might, if you formulated an intractable spin, you might want to sample from it.",
            "Or if you prefer variational inference, you might want to use SPSS the approximating distribution."
        ],
        [
            "In summary"
        ],
        [
            "Discriminative, espions combined."
        ],
        [
            "Images of tractable."
        ],
        [
            "Difference, deep architectures and."
        ],
        [
            "Discriminative learning.",
            "We show."
        ],
        [
            "How the hard gradient combats the diffusion problem in deep models and final?"
        ],
        [
            "We showed discriminative espions outperforming support vector machines and deep models on image classification benchmarks.",
            "We look forward to discussing our work with you and can't wait to see what our poster tomorrow night poster W 15.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is a talk about learning some product networks discriminatively.",
                    "label": 0
                },
                {
                    "sent": "I'm Rob guns and this is joint work with Pedro Dominguez at the University of Washington.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some product networks are new deep probabilistic architecture with tractable inference.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now you might not be familiar with SPN, so I'll give you a brief guided tour of a sample.",
                    "label": 0
                },
                {
                    "sent": "SPN over some image.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pixels so at the bottom we have our leaf distributions which model those pixels and then products of those distributions would be considered.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Image caps features which of?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have small and large and then sums over those products would be.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Considered mixture models over image Patch features.",
                    "label": 0
                },
                {
                    "sent": "And then products over those sums could.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be considered part decompositions of which you have one.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this or one like this?",
                    "label": 0
                },
                {
                    "sent": "And then finally our route.",
                    "label": 0
                },
                {
                    "sent": "Some node can.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Be considered as a pooling function over party compositions.",
                    "label": 0
                },
                {
                    "sent": "And this whole structure could be repeated the features, the mixtures, the party compositions as many layers as you want.",
                    "label": 0
                },
                {
                    "sent": "So as you can see, SPN's are quite expressive, despite their tracked ability.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in this talk, I'm first going to motivate the use of sum product networks.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to review how they work.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to show you how to add feature functions to SPMS and train them discriminatively.",
                    "label": 0
                },
                {
                    "sent": "And finally I'll show some experiments where sum product networks outperform support vector machines and deep models on image classification bench.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's first start with some motivation.",
                    "label": 0
                },
                {
                    "sent": "Many of these graphical models, because they allow us to compactly represent the joint.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Distribution of many variables.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, inference in graphical models is intractable, which often forces us to reduce the expressivity of our model.",
                    "label": 0
                },
                {
                    "sent": "Often with low tree with.",
                    "label": 0
                },
                {
                    "sent": "Although many approximate inference schemes have been proposed, they're all fraught with difficulties.",
                    "label": 0
                },
                {
                    "sent": "In contrast, some product networks perform fast exact inference on hightree with models.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Deep architectures use layers of hidden variables to increase their representational power.",
                    "label": 0
                },
                {
                    "sent": "However, when we add layers of hidden variables to graphical models, this compounds the already intractable inference problem.",
                    "label": 0
                },
                {
                    "sent": "Some product network can be viewed as a deep model that has full probabilistic semantics and tractable inference over many layers.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Up until now, we've only seen generative training of sum product networks.",
                    "label": 0
                },
                {
                    "sent": "However, for many applications, we find that discriminative learning is performance better.",
                    "label": 0
                },
                {
                    "sent": "This is our contribution.",
                    "label": 0
                },
                {
                    "sent": "With conditional random fields, we can use flexible feature functions.",
                    "label": 0
                },
                {
                    "sent": "But and yet the conditional partition function remains intractable.",
                    "label": 0
                },
                {
                    "sent": "In this talk, we're going to combine the advantages.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "SPMS with those of conditional models, namely, flexible feature functions with exact inference over high tree.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what is an SPN?",
                    "label": 0
                },
                {
                    "sent": "Let's define espions recursively.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You know, very distribution is an SPN.",
                    "label": 0
                },
                {
                    "sent": "You can pick your favorite disc.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Treat or continuous distribution for the same.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this example.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will define X as being a multinomial.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now one variable is not that interesting, so only.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our next definition.",
                    "label": 0
                },
                {
                    "sent": "Which is that?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A product of SPN's over disjoint variables is also an SPN.",
                    "label": 1
                },
                {
                    "sent": "So we just brought over another variable Y, which could be any distribution.",
                    "label": 0
                },
                {
                    "sent": "But in this example we also have at a multi node.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Now you might notice that the product of two variables is just independent, so we need our last definition.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Which is that?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A weighted sum of SPN's over the same variables is also an SPN.",
                    "label": 1
                },
                {
                    "sent": "So now we just brought over another distribution over these same two variables and formed a weighted sum.",
                    "label": 0
                },
                {
                    "sent": "And that now establishes dependencies.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some node sums out a hidden variable, just like in a mixture model.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So what we have now is that sum product networks are rooted directed acyclic graph with univariate distributions at the leaves and sums and product nodes as internal nodes.",
                    "label": 0
                },
                {
                    "sent": "Now we can also use multivariate distributions at the leaves, but you have to be able to ensure that you can compute their partition functions and modes in closed form.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the beauty of this definition is that it's recursive, so we can build up layers and layers of more and more and more structure.",
                    "label": 0
                },
                {
                    "sent": "And yet we're always remaining tractable.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The key property of SPN's is that all marginals are computable in linear time, and it's easy to see why.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because the partition function of a product node over disjoint sets of variables is just the product of the partition functions of its children.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Partition function of a sum note over the same sets of variables is just the weighted sum of the partition functions of its children.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A marginal is just the partition function of an SPN, where you replace those leaf distributions with Evan over evidence variables with Delta functions or the probability of evidence.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for example, if we were curious about the probability that X is zero in this model with set, those least leaf distributions that we have evidence for with the probability.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That evidence, and then for all other variables we.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Marginalized them by setting those leaf distributions to be their partition functions.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We then compute the values of notes going bottom.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this would give us our answer.",
                    "label": 0
                },
                {
                    "sent": "If the leaf distributions and weights in the model are normalized, then the partition function is just one.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If not, the partition function, like other marginals, is computable in linear time.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A related prop.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty is that all mapu states are computable in linear time, we just replaced.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some nodes with Max nodes.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we're using the Max product semiring instead of the sum product semiring.",
                    "label": 0
                },
                {
                    "sent": "And we can do this because the ideas behind some product networks apply to any semiring.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for example, if we're interested in the maximum probability with evidence X = 0, we would set evidence on our leaf distributions just like before, but then for those nodes that we don't.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have evidence for we wouldn't marginalized them will set them to be the probability of their.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Woods we then compute the values of those modes of values of net.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's going bottom up and that would give us the maximum probability.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then compute a trace back.",
                    "label": 0
                },
                {
                    "sent": "Just like with Kirby.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this would lead to the argmax.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You might think that because their definition is so simple that espions are quite limited, when in fact there are many models that are special cases of spins and spins are more general than each of them for.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Apple Junction trees harkle mixture models non recursive PCF.",
                    "label": 1
                },
                {
                    "sent": "These models with context specific independence models with determinism.",
                    "label": 1
                },
                {
                    "sent": "Another hightree with models.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the world of compactly represent about probability distributions.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Their graphical model.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And some product networks.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the intersection, lie existing tractable models which would include hierarchical mixture models and thin junction trees.",
                    "label": 0
                },
                {
                    "sent": "Now when I say compactly represent aghbal, what I mean is that if I took that SPN that I showed you at the beginning of the talk and try to represent it as a as a graphical model, it's conditional independence graph would be a complete click, so that would not be compactly represent Abaline which solely exist in SPN territory.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sum product networks were introduced in the paper last year by Putin Domingo's.",
                    "label": 0
                },
                {
                    "sent": "They explored updates for EM and gradient descent.",
                    "label": 0
                },
                {
                    "sent": "And since emigrating to send both involve inference, they explored soft and hard variance of inference.",
                    "label": 0
                },
                {
                    "sent": "Soft entrance you compute marginals or expectations and with hard inference you compute the counts from MLP states.",
                    "label": 1
                },
                {
                    "sent": "And by the end of this talk, we're going to complete this table.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With discriminative espions, we optimized the conditional likelihood of Y given X or Y.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Error query variables.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "HR hidden variables corresponding to some nodes.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And X are evidence variables.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the trick is that we're going to treat these evidence variables as constants in the network.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can take any generative SPN and turn it into a discriminative SPN by adding feature functions.",
                    "label": 0
                },
                {
                    "sent": "So here we have a generative SPN over variables Y1Y2, and we have a hidden variable that some doubt at the root we also.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other evidence variables, which are the pixels of an image.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can compute any feature functions we like and then place them in the network's children of product nodes.",
                    "label": 0
                },
                {
                    "sent": "The reason we can do this is because when those features take on a value, those values get folded into the weights of the parents, some node and we have a generative SPN.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, so because we can do this process to any generative SPM, it means that there's a greater variety of discriminative espions.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With discriminative training, we're going to follow the.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Radiance of the conditional log likelihood, which takes on a form that identical to when we train CRF's where.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have a positive first term where we're given the correctly.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And a negative second term where we sum over all labels.",
                    "label": 0
                },
                {
                    "sent": "The key.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The difference between training CRF's and training SPN's is that these summations are always tractable for SPMS.",
                    "label": 0
                },
                {
                    "sent": "We don't have to reduce the expressivity of our model in order to make these things tractable.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We compute gradients in our model using back propagate.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where we can we set the values of our leaf distribution?",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then compute the values of nodes going bottom up and back propagation starts.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Through we're going to use the gradient symbol to note the partial derivative of the network with respect to.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Node which.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the route is of course one.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The gradient down the branch of a.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Download is just the weight of that branch times the gradient of the parents on node, so the gradient would flow down this some node as follows.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's follow the gradient down.",
                    "label": 0
                },
                {
                    "sent": "The child on the left.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As follows from the derivative, the grading down the branch of a.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Product node is just equal to the product of the.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Eyes of the other children multiplied by the gradient of the.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Current product so that gradient would flow.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this?",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there's a huge problem with backpropagation, which is that as the gradient flows from the root down to the leaves, the signal gets washed out, it becomes uniform.",
                    "label": 0
                },
                {
                    "sent": "This is known as.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gradient diffusion, that's what makes deep learning hard and espions are no exception to that.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We found when training some product networks.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hard inference can overcome the gradient diffusion problem.",
                    "label": 1
                },
                {
                    "sent": "So on the left we have soft inference where we compute marginals or expectations.",
                    "label": 0
                },
                {
                    "sent": "And on the right we have hard inference where we compute counselor, maybe states.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are other reasons to use hard inference.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Besides overcoming great infusion.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Such as when our goal is to predict the most probable structure and.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In general, we might replace some nations with maximization's.",
                    "label": 0
                },
                {
                    "sent": "For speed attract ability, but this is not as important.",
                    "label": 0
                },
                {
                    "sent": "Recipes which are always tractable.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The hard gradient is derived from the soft gradient, but we replaced also.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Animations with maximization's.",
                    "label": 0
                },
                {
                    "sent": "Again.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Have a positive first term that's given the correct label and a negative.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Second term, when we maximize overall labels, and intuitively when our model guesses the correct label, the two terms will be equal and the gradient for this example would be 0.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The hard gradient is best explained graphically.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where the two maximization's are computed using with MLP inference in our network.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We notice that the two backtraces tracebacks are just products.",
                    "label": 0
                },
                {
                    "sent": "And since the log of a product is just a sum, corresponding terms will cancel out.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we're left with are these green lines, which indicate those weights that should be increased because they lead to the correct label and the red lines, which are those which that should be decreased because they lead to the incorrect label.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we have an intuitive wait update for the hard gradient, which is that the partial derivative with respect to await WI is equal to Delta CI over WI.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Jealousy is the difference between the number of times the weight WI appears in the traceback, with the correct label and the number of times the weight WI appears in the traceback when the model guesses.",
                    "label": 0
                },
                {
                    "sent": "You might notice that this update looks like it looks like structured perceptron, and in our paper we actually show a re parameterisation that makes that happen.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to our weight update table, we've added discriminative gradient descent with soft and hard updates.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you'll notice that all of the soft inference updates.",
                    "label": 0
                },
                {
                    "sent": "Are proportional to the partial derivative of the network with respect to apparent some node SKU, meaning it's going to suffer from great infusion.",
                    "label": 0
                },
                {
                    "sent": "And notice how all of the hardinfo.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Updates involve the term CI, which are the counselor maybe in France.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now for experiments.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We tested discriminatively trained espions on two image classification benchmarks, Cifar 10 and STL 10 and for this task were given a fixed network architecture and we're going to discriminate learning.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To be able to compare our method to other methods, we're going to use the same feature extraction pipeline proposed by Coates at.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Where we extract the Dictionary of Image Patch features using K means.",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then for any image on our data set, we compare Patch with that image to our diction.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using the triangle encoding, this deal is a vector of length K. So when we apply this.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So all patches of an image.",
                    "label": 0
                },
                {
                    "sent": "With a 32 by 32 image and six by six pixel patches, this will deal with a tensor of size 27 by 27 by K that.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Answers then downsampled by Max pooling to be a G by G by K tensor.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That tensor is then fed into architecture.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We use an SPN architecture that's inspired by this successful star models of files and swap little where we have a root some node of.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Attend classes and each week.",
                    "label": 0
                }
            ]
        },
        "clip_119": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "US is a product of its parts.",
                    "label": 0
                },
                {
                    "sent": "And each part is.",
                    "label": 0
                }
            ]
        },
        "clip_120": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The mixture over parts, subtypes, and then each parts.",
                    "label": 0
                }
            ]
        },
        "clip_121": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Type is summed over all locations in the image where that part could exist.",
                    "label": 0
                },
                {
                    "sent": "We use the leaf distribution of the logistic function E to the X dot F, where F is the portion of our image sensor that falls under apart at that location.",
                    "label": 0
                },
                {
                    "sent": "So for example, a class might be a bird, and hypothetically speaking apart might be a beak, and you might have two types of beaks, like curved beaks and straight beaks, and then you add some overall locations in the image where the beak could occur.",
                    "label": 0
                }
            ]
        },
        "clip_122": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_123": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We compare against methods that downsample their image feature tensor to be a four by 4 by K tensor.",
                    "label": 0
                },
                {
                    "sent": "So here we have 3.",
                    "label": 0
                }
            ]
        },
        "clip_124": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The results that were presented by Coates at all where the where we have a support vector machine over RK means features and then we have.",
                    "label": 0
                },
                {
                    "sent": "Two method, we have an autoencoder generated features and restricted Boltzmann machine generated features.",
                    "label": 0
                },
                {
                    "sent": "And on the X axis, we're varying the dictionary size K. And we notice that in general, as we increase the size of the dictionary, the test accuracy increases as well.",
                    "label": 1
                }
            ]
        },
        "clip_125": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There is a recent CPR paper by Gia at all that learns pooling functions over these features.",
                    "label": 0
                }
            ]
        },
        "clip_126": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we find our discriminative espions do even better than that.",
                    "label": 0
                },
                {
                    "sent": "So in particular.",
                    "label": 0
                }
            ]
        },
        "clip_127": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We",
                    "label": 0
                }
            ]
        },
        "clip_128": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That are espions are able to do better than the pooling functions using a 5th as many features and when we compare ourselves to support vector machine.",
                    "label": 0
                }
            ]
        },
        "clip_129": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We find that we're able to use 20 * 2 year features than they do.",
                    "label": 0
                }
            ]
        },
        "clip_130": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we believe this is the case because the SPN is modeling spatial structure among features that the other methods are not modeling.",
                    "label": 0
                },
                {
                    "sent": "Moving on to a final.",
                    "label": 0
                }
            ]
        },
        "clip_131": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The features.",
                    "label": 0
                }
            ]
        },
        "clip_132": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We achieved the best published results at the time on Cifar 10 using a fraction of the features of the other methods.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_133": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Story is similar for STL 10 where we compare against three methods that learn unsupervised features.",
                    "label": 0
                },
                {
                    "sent": "The first 2 methods learn image Patch dictionaries using two different penalties on the dictionary and the third method learns three layers of locally of local receptive fields that are learned by correlation among features.",
                    "label": 0
                },
                {
                    "sent": "Our discriminatively trained espions do even better than these methods, even though we don't use the unlabeled data.",
                    "label": 0
                }
            ]
        },
        "clip_134": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Future work.",
                    "label": 0
                }
            ]
        },
        "clip_135": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Includes Max margin SPM.",
                    "label": 0
                }
            ]
        },
        "clip_136": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning espian structure.",
                    "label": 0
                }
            ]
        },
        "clip_137": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Applying discriminative structure prediction.",
                    "label": 0
                }
            ]
        },
        "clip_138": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An approximate inference using SPN Switch.",
                    "label": 0
                },
                {
                    "sent": "For example, you might, if you formulated an intractable spin, you might want to sample from it.",
                    "label": 0
                },
                {
                    "sent": "Or if you prefer variational inference, you might want to use SPSS the approximating distribution.",
                    "label": 0
                }
            ]
        },
        "clip_139": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In summary",
                    "label": 0
                }
            ]
        },
        "clip_140": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Discriminative, espions combined.",
                    "label": 0
                }
            ]
        },
        "clip_141": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Images of tractable.",
                    "label": 0
                }
            ]
        },
        "clip_142": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Difference, deep architectures and.",
                    "label": 0
                }
            ]
        },
        "clip_143": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Discriminative learning.",
                    "label": 0
                },
                {
                    "sent": "We show.",
                    "label": 0
                }
            ]
        },
        "clip_144": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How the hard gradient combats the diffusion problem in deep models and final?",
                    "label": 0
                }
            ]
        },
        "clip_145": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We showed discriminative espions outperforming support vector machines and deep models on image classification benchmarks.",
                    "label": 1
                },
                {
                    "sent": "We look forward to discussing our work with you and can't wait to see what our poster tomorrow night poster W 15.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}