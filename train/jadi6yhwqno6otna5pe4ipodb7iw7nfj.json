{
    "id": "jadi6yhwqno6otna5pe4ipodb7iw7nfj",
    "title": "Synergies in learning words and their referents",
    "info": {
        "author": [
            "Mark Johnson, Department of Computing, Macquarie University"
        ],
        "published": "March 25, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Text Mining"
        ]
    },
    "url": "http://videolectures.net/nips2010_johnson_slw/",
    "segmentation": [
        [
            "Hello, I'm a computational linguists.",
            "I'm interested in how languages are learned an I studied this by studying grammatical inference last year at NIPS at the topic Modeling Workshop.",
            "The guys there actually asked me to talk about combining topic models and grammars, and that's actually sort of led to.",
            "I think some very interesting work.",
            "Basically, at that workshop, we showed that it's possible to reduce LDA topic models to PCF, geez.",
            "And that's interesting, because that suggests a variety of extensions to topic models, where essentially by stating the topic model as a grammar, then you can extend it to make it sensitive to structural information, and you'll actually see some of the work being used here.",
            "So here we're actually studying the problem of language learning, where the input to the learner consists of unsegmented utterances.",
            "So speech is actually much more like written Chinese or written Japanese than it is written English.",
            "There's no spaces in between words.",
            "And so you can see at the bottom corner down there that the you know the input there is actually the phonemic representation of the sentence.",
            "Is that the pig?",
            "And also what I'm studying here is let's imagine also that the kid.",
            "Ideally I'd actually like the learner to be presented with the image that's shown here where the you know the the mothers showing the kid both are toy pig in a toy dog until image recognition gets a little bit better.",
            "What will do as well?",
            "Just simply instead of actually using image recognition, will this actually present the learner also with just the information that there's a pig and a dog in the context?",
            "And so the goal of learning is to actually take the sequence of speech sounds and segment it into words so actually identified and the segmentations actually shown there in blue and then also to actually work out that the word pig actually is referring to the object pig here.",
            "So."
        ],
        [
            "So this work is actually based on earlier work that Mike Frank did.",
            "Mike actually sort of studied the way in which topic models could actually be learned?",
            "Could be used to model the way in which children might actually learn to associate words with their reference.",
            "So the idea is that the objects in the long linguistic context you know the pig or the dog would be represented essentially as potential sentence topics.",
            "And one observation as I said is because of this reduction between topic models and PCF.",
            "Geez, you can actually express Franks topic models as probabilistic context free grammars and the grammar is written in such a way that basically the grammar has to sort of choose a topic from the possible topic markers and propagated up through the sentence.",
            "Every word is either generated by the sentence topic or a special null topic.",
            "And although I don't show it here, it turns out that if you change the grammar so it requires at most one topic per sentence, this in."
        ],
        [
            "Lose accuracy.",
            "OK, in separate work we studied the this final logical segmentation problem.",
            "That's the problem of given a a corpus of sentence utterances represented as a sequence of speech sounds.",
            "How can we segment that into Woods and we studied this using adaptive grammars, which are a generalization of probabilistic context free grammars that are actually based on hierarchical Dursley or Pitman yor processes, and basically it gives the grammar the ability to learn the probabilities of not just rules but entire subtrees.",
            "So here for example, you'll see that the tree on the right represents a parse of the sequence the pig into the words, the.",
            "And pig and the model here would be learning the probability of the two words subtrees.",
            "And it turns out the segmentation accuracy improves if you change the grammar in such a way that as well as learning words.",
            "It also learns dependencies between adjacent words that we represent by colocation."
        ],
        [
            "So in the poster, you'll actually see how we combine these two grammars.",
            "One advantage of working with grammars is that it's easy to actually combine and generalize these things, so we could actually combine Franks topic PCF cheese with the word segmentation adapter grammars.",
            "And when you do that, what's interesting is that it turns out that these models actually learn better than either of the two models that they were combined from.",
            "So in particular, if if the model is actually capable of learning topics, figuring out whether the mother is talking about the pig or the dog that improves the word segmentation models.",
            "So the accuracies go from 70 to 75% accuracy, and if we change the model so that it improves word segmentation, that also improves topic detection.",
            "As well, so it looks basically as if what we're seeing is that there's a real benefit to performing joint inference in actually in actual learning, and I think that raises a whole host of other questions.",
            "Are there other similar synergies and learning other aspects of language and to humans exploit such synergies?",
            "Thanks very much."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello, I'm a computational linguists.",
                    "label": 0
                },
                {
                    "sent": "I'm interested in how languages are learned an I studied this by studying grammatical inference last year at NIPS at the topic Modeling Workshop.",
                    "label": 0
                },
                {
                    "sent": "The guys there actually asked me to talk about combining topic models and grammars, and that's actually sort of led to.",
                    "label": 0
                },
                {
                    "sent": "I think some very interesting work.",
                    "label": 0
                },
                {
                    "sent": "Basically, at that workshop, we showed that it's possible to reduce LDA topic models to PCF, geez.",
                    "label": 0
                },
                {
                    "sent": "And that's interesting, because that suggests a variety of extensions to topic models, where essentially by stating the topic model as a grammar, then you can extend it to make it sensitive to structural information, and you'll actually see some of the work being used here.",
                    "label": 0
                },
                {
                    "sent": "So here we're actually studying the problem of language learning, where the input to the learner consists of unsegmented utterances.",
                    "label": 0
                },
                {
                    "sent": "So speech is actually much more like written Chinese or written Japanese than it is written English.",
                    "label": 0
                },
                {
                    "sent": "There's no spaces in between words.",
                    "label": 0
                },
                {
                    "sent": "And so you can see at the bottom corner down there that the you know the input there is actually the phonemic representation of the sentence.",
                    "label": 0
                },
                {
                    "sent": "Is that the pig?",
                    "label": 0
                },
                {
                    "sent": "And also what I'm studying here is let's imagine also that the kid.",
                    "label": 0
                },
                {
                    "sent": "Ideally I'd actually like the learner to be presented with the image that's shown here where the you know the the mothers showing the kid both are toy pig in a toy dog until image recognition gets a little bit better.",
                    "label": 0
                },
                {
                    "sent": "What will do as well?",
                    "label": 0
                },
                {
                    "sent": "Just simply instead of actually using image recognition, will this actually present the learner also with just the information that there's a pig and a dog in the context?",
                    "label": 0
                },
                {
                    "sent": "And so the goal of learning is to actually take the sequence of speech sounds and segment it into words so actually identified and the segmentations actually shown there in blue and then also to actually work out that the word pig actually is referring to the object pig here.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this work is actually based on earlier work that Mike Frank did.",
                    "label": 0
                },
                {
                    "sent": "Mike actually sort of studied the way in which topic models could actually be learned?",
                    "label": 0
                },
                {
                    "sent": "Could be used to model the way in which children might actually learn to associate words with their reference.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that the objects in the long linguistic context you know the pig or the dog would be represented essentially as potential sentence topics.",
                    "label": 0
                },
                {
                    "sent": "And one observation as I said is because of this reduction between topic models and PCF.",
                    "label": 0
                },
                {
                    "sent": "Geez, you can actually express Franks topic models as probabilistic context free grammars and the grammar is written in such a way that basically the grammar has to sort of choose a topic from the possible topic markers and propagated up through the sentence.",
                    "label": 1
                },
                {
                    "sent": "Every word is either generated by the sentence topic or a special null topic.",
                    "label": 1
                },
                {
                    "sent": "And although I don't show it here, it turns out that if you change the grammar so it requires at most one topic per sentence, this in.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Lose accuracy.",
                    "label": 0
                },
                {
                    "sent": "OK, in separate work we studied the this final logical segmentation problem.",
                    "label": 0
                },
                {
                    "sent": "That's the problem of given a a corpus of sentence utterances represented as a sequence of speech sounds.",
                    "label": 0
                },
                {
                    "sent": "How can we segment that into Woods and we studied this using adaptive grammars, which are a generalization of probabilistic context free grammars that are actually based on hierarchical Dursley or Pitman yor processes, and basically it gives the grammar the ability to learn the probabilities of not just rules but entire subtrees.",
                    "label": 0
                },
                {
                    "sent": "So here for example, you'll see that the tree on the right represents a parse of the sequence the pig into the words, the.",
                    "label": 0
                },
                {
                    "sent": "And pig and the model here would be learning the probability of the two words subtrees.",
                    "label": 1
                },
                {
                    "sent": "And it turns out the segmentation accuracy improves if you change the grammar in such a way that as well as learning words.",
                    "label": 1
                },
                {
                    "sent": "It also learns dependencies between adjacent words that we represent by colocation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the poster, you'll actually see how we combine these two grammars.",
                    "label": 0
                },
                {
                    "sent": "One advantage of working with grammars is that it's easy to actually combine and generalize these things, so we could actually combine Franks topic PCF cheese with the word segmentation adapter grammars.",
                    "label": 0
                },
                {
                    "sent": "And when you do that, what's interesting is that it turns out that these models actually learn better than either of the two models that they were combined from.",
                    "label": 0
                },
                {
                    "sent": "So in particular, if if the model is actually capable of learning topics, figuring out whether the mother is talking about the pig or the dog that improves the word segmentation models.",
                    "label": 0
                },
                {
                    "sent": "So the accuracies go from 70 to 75% accuracy, and if we change the model so that it improves word segmentation, that also improves topic detection.",
                    "label": 1
                },
                {
                    "sent": "As well, so it looks basically as if what we're seeing is that there's a real benefit to performing joint inference in actually in actual learning, and I think that raises a whole host of other questions.",
                    "label": 0
                },
                {
                    "sent": "Are there other similar synergies and learning other aspects of language and to humans exploit such synergies?",
                    "label": 1
                },
                {
                    "sent": "Thanks very much.",
                    "label": 0
                }
            ]
        }
    }
}