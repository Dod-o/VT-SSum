{
    "id": "h4emlsqc3sxnpv7p7xqycqfimnpzyyb6",
    "title": "Statistical Predicate Invention",
    "info": {
        "author": [
            "Stanley Kok, University of Washington"
        ],
        "published": "July 27, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Machine Learning->Inductive Logic Programming"
        ]
    },
    "url": "http://videolectures.net/icml07_kok_spi/",
    "segmentation": [
        [
            "Hi Mr Leacock from the University of Washington.",
            "My talk is on statistical predicate invention.",
            "This is joint work with."
        ],
        [
            "Angus so this is the overview of my talk.",
            "I'll begin by motivating the problem.",
            "Then I'll give some technical background.",
            "Then I'll describe all approach multiple relational clusterings.",
            "Next, I'll present some experimental results, then of any."
        ],
        [
            "Slight on future work.",
            "So in recent years, the statistical, relational learning SRL community.",
            "Has recognized the importance of combining the strengths of statistical learning and relational learning.",
            "Also known as inductive logic programming, they have developed novel representations.",
            "Come up with efficient algorithms to learn the parameters and structure of such representations, however."
        ],
        [
            "They have not paid much attention to the problem of statistical predicate invention SPI.",
            "So what SPI SPI is the discovery of new concepts, properties and relations from data express in terms of observable ones using statistical techniques to guide the discovery of the new predicates and to represent the uncertainty in them.",
            "SPI has been looked at by the statistical learning and relational learning communities separately and with limited success.",
            "In the statistical learning Community is known as a problem or latent variable discovery.",
            "In the relational learning community is known as a problem of predicate invention."
        ],
        [
            "So YSPI was so great about it.",
            "SPI give us more compact and comprehensible models by efficiently representing dependencies among observed predicates.",
            "No.",
            "This reduces the number of parameters for all models, which in turn.",
            "Reduces the risk of overfitting and reduces the amount of memory required to represent our model.",
            "This means that we could potentially speed up inference.",
            "But representing an observer specs of a data generating process, SPI could also gives better, better accuracy by giving a truer representation of that process.",
            "And invented predicate also allow us to take largest search steps in a search space.",
            "That by allowing us to create more complex models to model more complex phenomena."
        ],
        [
            "Few approaches to date combine.",
            "Statistical and relational learning for SPI.",
            "Most only our most invent predicates as clusters by clustering objects, but not relations or they only predict a single target predicate.",
            "The latest in this is the latest in the line of approaches is the infinite relational model.",
            "The IRM proposed by Cam at all shared all proposal closely related model.",
            "The IRM classes, objects and relations simultaneously.",
            "It can handle multiple types of objects.",
            "The relations can be of any error T and the number of clusters need not be determined.",
            "In advance, one thing to note about IRM is that it finds a single clustering."
        ],
        [
            "It is in this context that we introduced our approach multiple relational clusterings.",
            "Like the IRM, the MLC clusters objects and relations simultaneously.",
            "It can handle multiple types of objects.",
            "Relations can be of any error T, and the number of clusters need not be specified in advance in addition.",
            "Within multiple cross cutting clusterings.",
            "In our experiments, we find that this is useful in capturing the dependencies present in rich relational domains.",
            "Atmos is based on Finite 2nd order Markov logic, an Alpha step towards a general framework for SPI."
        ],
        [
            "Some background.",
            "A logical knowledge base is a set of hard constraints on the set of possible worlds.",
            "Now This is why it is so brittle.",
            "To fix that we can make the constraint soft so that when the world values of formula it becomes less probable, but not totally impossible.",
            "MLN's do this by giving each formula weight.",
            "The higher the weight, the stronger the constraint.",
            "This makes the probability of a world proportional to the exponentiated some of the weights of the formulas that the world satisfies."
        ],
        [
            "More concretely.",
            "This is the model of an orphan MLN, probably of X default.",
            "Either world where X is a vector of truth assignments today items.",
            "These partition function that sums over all possible troop assignments to grind terms and it serves to normalize the expression.",
            "The right to make it valid probability with some overall formulas.",
            "WI is the weight of the eye formula and number of true groundings of the eye formula.",
            "It is interesting to note that first order logic and most statistical models are special cases of."
        ],
        [
            "Alliance."
        ],
        [
            "Moving on to our approach, multiple relational clusterings.",
            "MRC MRC is our approach.",
            "We invent unary predicates as clusters.",
            "We invent multiple cross cutting clusterings to capture dependencies in relational domains with clusters relations by the objects they relate.",
            "An objects by their related objects and the relations in which they appear.",
            "Because objects of the same type and we cluster relationship the same error T an argument type."
        ],
        [
            "So consider this example illustrating the importance of multiple clusterings.",
            "People have friends, coworkers, hobbies and technical skills.",
            "A person hobbies are best predicted by her friends, hobbies, person skills are best protected by her coworkers skills.",
            "If you long everybody together in one huge cluster, our ability to predict both hobbies and skills will be heard instead, which is simultaneously.",
            "Classes together people who are friends and people who are coworkers.",
            "the French clusters are highly predictive of hobbies and the coworker, coworkers, clusters are highly predictive of skills."
        ],
        [
            "Now MSE is based on finite function free 2nd order Markov logic, so it's not full blown.",
            "2nd Order logic in MRC variables range over relations, that is predicates and objects that is constant.",
            "Note that I use the terms relations and predicates interchangeably.",
            "Likewise for the terms, objects and constants.",
            "On the ground atoms, if all possible predicate symbols as well as constant symbols, we use 2nd order Markov logic because it's able to represent some models more compactly than first order Markov logic, and we needed to specify how predicate symbols are clustered."
        ],
        [
            "These are the symbols used in the rules that I'll show you in the next slide.",
            "We use small gamma I to indicate a cluster.",
            "Bit gamma to indicate a clustering that is a partitioning of a set of symbols.",
            "Over here.",
            "Are this is an Atom, but R is a relation predicate symbol X one to XN.",
            "The arguments of that Atom.",
            "XI the symbol XI in cluster gamma is also an Atom.",
            "This should become clearer.",
            "When you look at the rules.",
            "Let me talk about a cluster combination.",
            "We're referring to the combination of clusters to which the constituent symbols in an Atom belongs."
        ],
        [
            "Now we have 5 rules that we use.",
            "The first fixed that each symbol belongs to at least one cluster.",
            "This is a hard rule, so it's given infinite weight.",
            "That means it cannot be violated.",
            "The second rule states that a symbol cannot belong to more than one cluster in the same cluster ring.",
            "Again, this is a hard rule and given infinite wait.",
            "The third rule states that each Atom appears in exactly 1 combination of clusters.",
            "These two is a hard rule and given infinite weight."
        ],
        [
            "The next rule we called Atom Prediction Rule, states the true value of an Atom is determined by the cluster combination to which it belongs.",
            "Now let's pass this rule in more detail.",
            "Over here we see a + This is nothing but syntactic sugar.",
            "Emili states that there's an instance of this rule.",
            "For every possible cluster combination and every instance of the rule has his own weight.",
            "To restate the rule, the probability that this Atom is true.",
            "Is determined by the cluster membership of is constituent symbols.",
            "The larger the width.",
            "The higher the probability that the Atom is true.",
            "Now, this way it has to be learned.",
            "It's not fixed.",
            "The last rule is the exponential prior rule.",
            "On the number of clusters, it seems to penalize the number of clusters in our in our model, is given fixed negative negative Lambda."
        ],
        [
            "Learning the MLC model consists of finding two things that maximize log posterior probability.",
            "The first cluster first is cluster assignment.",
            "That is the assignment of truth values to the R in gamma R&X in gamma X atoms, the 2nd.",
            "Is which of the Atom prediction rules over here you see the expression?",
            "For the log, posterior probabilities are is the vector of proof assignments to all ground observed ground atoms."
        ],
        [
            "Now show you how the five rules map to these two components of the log posterior probability.",
            "The Tryhard rules an exponential prior rule Maps to the first component and is evaluated as shown on the slide is value is negative Infinity if the cluster assignment violates a hard rule because the probability of the cluster assignment is 0 for all other cluster assignments is merely the number of clusters multiplied by the Lambda penalty."
        ],
        [
            "The other predictions rules map to the second component.",
            "Now given a cluster assignment, the MLN containing the Atom prediction rules decomposes into one MLN.",
            "For each rule.",
            "This is why the width of the rule can be computed in close form, simply as the lots of the Atom in the cluster combination being true T&F.",
            "Are the number and true and false atoms in the cluster combination?",
            "Since the weights can be computed in close form, the second component can also be computed in closed form now.",
            "Better over here just smoothing parameter that we use."
        ],
        [
            "Now that I've shown you our now than you did last year, probability, which is our objective function, I'll show you how we search for the MLP.",
            "The best cluster assignment.",
            "We make an approximation.",
            "We make hot assignments of symbols to classes.",
            "Now I've shown you that estimating the weights given the cluster assignment is easy.",
            "What's hard is finding MFP cluster assignments because the search space is so huge Now, this is intractable, so we have to use, so we use greedy search.",
            "Our algorithm can be seen as a form of topdown defensive refinement algorithm.",
            "As we further refine clusters of symbols, clusters of objects, we can further refine clusters of related objects in clusters of relations.",
            "Every algorithm can be seen to have two levels.",
            "The top finds cluster rings, the bottom finds clusters."
        ],
        [
            "Illustrate this with."
        ],
        [
            "10 fold cross validation train on 9 and tests on the remaining one.",
            "We measured the average conditional log likelihood of the test ground items, the CLL and the average area under the precision, precision recall curves of the test ground atoms, the AUC."
        ],
        [
            "We compared with the infinite relational model, the IRM an MLN structure, learning.",
            "We use the default IRM parameters downloadable from the web.",
            "We ran it for 10 hours.",
            "We set the MRC parameters of Lambda and better both to one without any tuning.",
            "We ran mercy for 10 hours for the first level of clustering.",
            "That is the root note of the hierarchy that I've shown you.",
            "For subsequent levels, we permitted MRC 100 steps we took in total about 3 to 10 minutes MLN structured learning.",
            "It's allowed to run for 24 hours and its parameter settings are in an online appendix."
        ],
        [
            "No, for the results.",
            "The top rule shows the results for the conditional log likelihood and the bottom shows the results for area under the precision recall curves.",
            "The error bars indicate one standard deviation in either direction.",
            "Now let's look first look, compare IRM and MRC the white bar in the purple bar.",
            "So as you can see.",
            "I am at MRC performs comparably on the animal.",
            "Any nations data set.",
            "However, on the UMLS and kinship datasets, MRC performs considerably better than the IRM.",
            "Note that the M, LS and kinship datasets are one to two orders of magnitude larger than the animals and nations datasets.",
            "This suggests that using MRC finding multiple clusterings is helpful on large, rich relational domains.",
            "Now let's compare.",
            "MSE with MLA structure.",
            "Learning the light blue bar.",
            "As you can see from the slide, MRC does better than MLN structure learning all datasets.",
            "SFO nations or nations that does about equally well?",
            "The thing about nations is that most of their relationships are symmetric, so the country a goes to war Country B, country B, Jolly Bell goes toward country A.",
            "So Emma instructor, learning found such a rule and consequently does well.",
            "Now we can look at.",
            "I must see compared to init, init is just mercy with the first leave, first level of clustering.",
            "So am I see does better finding multiple clusterings does better than finding one clustering or all datasets except for the animals data set.",
            "So animals that's really small, we don't expect to gain much from multiple clusterings.",
            "OK tomorrow."
        ],
        [
            "So move on now I'll show you some examples on multiple clusterings that we learn on the M LS data set over here.",
            "This is the first clustering that we learned.",
            "For example, virus, fungus, LG plant found in bio active.",
            "Sorry by active substances biology mines are found in virus fungus LG implant but they are not found in and feeding birds, vertebrate animals.",
            "Bara spongy invertebrate causes diseases.",
            "Cell dysfunction.",
            "LG plant and feed bin.",
            "But do not cost.",
            "These are cells dysfunction now this is."
        ],
        [
            "According to the journal's database.",
            "Virus fungus LG implant.",
            "They're not even different animals and feeding birds fish.",
            "They are vertebrate animals.",
            "Now MRC is not perfect.",
            "Over here is this data.",
            "Invertebrate is of interpret, which is wrong."
        ],
        [
            "So over here we have an example of."
        ],
        [
            "Triple cross cutting."
        ],
        [
            "For future work, future work would like to experiment on larger datasets.",
            "Right now we are looking at inducing ontologies from web text using MRC.",
            "Would like to use the clusters that we learn as primitives in structured learning.",
            "Would like to retain the entire hierarchy of multiple clusterings and perform shrinkage over it.",
            "Would like to cluster predicates with different arities.",
            "An argument types and we speculate that all relational structure learning can be accomplished with SPI alone.",
            "Now this is our ultimate goal."
        ],
        [
            "In conclusion, I presented I proposed statistical predicate invention as a key problem for SRL.",
            "I've presented multiple relational clusterings.",
            "Our approach offers step towards a general framework for SPI.",
            "This based on finite 2nd order Markov logic.",
            "It creates multiple cross cut during class cross cutting clusterings of symbols in data and empirical comparisons with.",
            "At mountain structure learning and ERM, shows the promise of our approach.",
            "I'm unclear why you're calling this.",
            "Because.",
            "Conventional's right banks on introduced it in.",
            "Constructs a definition of a predicate.",
            "It doesn't just introduce constructed definition.",
            "And it also all of the examples you are giving are simply hierarchical clustering, which introduces us inherently one credit.",
            "So I mean, it's much more limited than the concept of predicate convention.",
            "Which makes me doubt your intention that this is going to solve over relational learning.",
            "In terms of.",
            "Right this is I do agree that right now we are not learning the kind of rich theories that kind of we're not accomplishing the same richness that is accomplished by predicate invention.",
            "Data agreed, so this is this is like a set of first step towards SPI.",
            "So the point is that we are not merely finding hierarchical clustering because the clusters cross cut.",
            "So so just not.",
            "So it's not simply just hierarchical clustering.",
            "Um?",
            "Yeah, you're right, this is not full blown predicate invention like what's done in IOP and this is a first step.",
            "Objects are higher order predicates one predicate, but you're not learning.",
            "Multi.",
            "Area is one of the big things is missing.",
            "There's learning new relations across objects, right, right so.",
            "This is not.",
            "Multi right.",
            "Totally true as I stated, but introduce MRC.",
            "We're learning unary predicates as clusters, so the next step would be learning multi arity predicates.",
            "Comparison between your technique and IRL.",
            "You showed us that on some datasets, in performs better.",
            "Do you know why it performs better?",
            "So for these two datasets.",
            "It performs better because.",
            "It finds multiple clusterings.",
            "So S. You're not convinced, yeah?",
            "Does that.",
            "Why do we know it?",
            "But I agree, I agree with you.",
            "Why do we know that result?",
            "Well, one thing that we did is more that.",
            "Provide evidence to that is, we also compared to the case where we do not find multiple clusterings.",
            "How long?",
            "Actually, I mentioned that we let you run for 10 hours.",
            "But I still try experiments running for 24 and 48 hours, and there's not much of a difference.",
            "So so possibly we could have really run for 72 hours and we could have better results, but we don't really know, so this is.",
            "Greedy search.",
            "The algorithm is never.",
            "Yeah, because it's greedy search we restart so we can just keep on restarting."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hi Mr Leacock from the University of Washington.",
                    "label": 1
                },
                {
                    "sent": "My talk is on statistical predicate invention.",
                    "label": 1
                },
                {
                    "sent": "This is joint work with.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Angus so this is the overview of my talk.",
                    "label": 0
                },
                {
                    "sent": "I'll begin by motivating the problem.",
                    "label": 0
                },
                {
                    "sent": "Then I'll give some technical background.",
                    "label": 0
                },
                {
                    "sent": "Then I'll describe all approach multiple relational clusterings.",
                    "label": 1
                },
                {
                    "sent": "Next, I'll present some experimental results, then of any.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Slight on future work.",
                    "label": 0
                },
                {
                    "sent": "So in recent years, the statistical, relational learning SRL community.",
                    "label": 1
                },
                {
                    "sent": "Has recognized the importance of combining the strengths of statistical learning and relational learning.",
                    "label": 0
                },
                {
                    "sent": "Also known as inductive logic programming, they have developed novel representations.",
                    "label": 0
                },
                {
                    "sent": "Come up with efficient algorithms to learn the parameters and structure of such representations, however.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They have not paid much attention to the problem of statistical predicate invention SPI.",
                    "label": 1
                },
                {
                    "sent": "So what SPI SPI is the discovery of new concepts, properties and relations from data express in terms of observable ones using statistical techniques to guide the discovery of the new predicates and to represent the uncertainty in them.",
                    "label": 1
                },
                {
                    "sent": "SPI has been looked at by the statistical learning and relational learning communities separately and with limited success.",
                    "label": 1
                },
                {
                    "sent": "In the statistical learning Community is known as a problem or latent variable discovery.",
                    "label": 0
                },
                {
                    "sent": "In the relational learning community is known as a problem of predicate invention.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So YSPI was so great about it.",
                    "label": 0
                },
                {
                    "sent": "SPI give us more compact and comprehensible models by efficiently representing dependencies among observed predicates.",
                    "label": 1
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "This reduces the number of parameters for all models, which in turn.",
                    "label": 0
                },
                {
                    "sent": "Reduces the risk of overfitting and reduces the amount of memory required to represent our model.",
                    "label": 0
                },
                {
                    "sent": "This means that we could potentially speed up inference.",
                    "label": 0
                },
                {
                    "sent": "But representing an observer specs of a data generating process, SPI could also gives better, better accuracy by giving a truer representation of that process.",
                    "label": 0
                },
                {
                    "sent": "And invented predicate also allow us to take largest search steps in a search space.",
                    "label": 1
                },
                {
                    "sent": "That by allowing us to create more complex models to model more complex phenomena.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Few approaches to date combine.",
                    "label": 1
                },
                {
                    "sent": "Statistical and relational learning for SPI.",
                    "label": 1
                },
                {
                    "sent": "Most only our most invent predicates as clusters by clustering objects, but not relations or they only predict a single target predicate.",
                    "label": 0
                },
                {
                    "sent": "The latest in this is the latest in the line of approaches is the infinite relational model.",
                    "label": 0
                },
                {
                    "sent": "The IRM proposed by Cam at all shared all proposal closely related model.",
                    "label": 1
                },
                {
                    "sent": "The IRM classes, objects and relations simultaneously.",
                    "label": 0
                },
                {
                    "sent": "It can handle multiple types of objects.",
                    "label": 0
                },
                {
                    "sent": "The relations can be of any error T and the number of clusters need not be determined.",
                    "label": 1
                },
                {
                    "sent": "In advance, one thing to note about IRM is that it finds a single clustering.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is in this context that we introduced our approach multiple relational clusterings.",
                    "label": 0
                },
                {
                    "sent": "Like the IRM, the MLC clusters objects and relations simultaneously.",
                    "label": 1
                },
                {
                    "sent": "It can handle multiple types of objects.",
                    "label": 1
                },
                {
                    "sent": "Relations can be of any error T, and the number of clusters need not be specified in advance in addition.",
                    "label": 1
                },
                {
                    "sent": "Within multiple cross cutting clusterings.",
                    "label": 0
                },
                {
                    "sent": "In our experiments, we find that this is useful in capturing the dependencies present in rich relational domains.",
                    "label": 0
                },
                {
                    "sent": "Atmos is based on Finite 2nd order Markov logic, an Alpha step towards a general framework for SPI.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some background.",
                    "label": 0
                },
                {
                    "sent": "A logical knowledge base is a set of hard constraints on the set of possible worlds.",
                    "label": 1
                },
                {
                    "sent": "Now This is why it is so brittle.",
                    "label": 0
                },
                {
                    "sent": "To fix that we can make the constraint soft so that when the world values of formula it becomes less probable, but not totally impossible.",
                    "label": 0
                },
                {
                    "sent": "MLN's do this by giving each formula weight.",
                    "label": 0
                },
                {
                    "sent": "The higher the weight, the stronger the constraint.",
                    "label": 0
                },
                {
                    "sent": "This makes the probability of a world proportional to the exponentiated some of the weights of the formulas that the world satisfies.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More concretely.",
                    "label": 0
                },
                {
                    "sent": "This is the model of an orphan MLN, probably of X default.",
                    "label": 0
                },
                {
                    "sent": "Either world where X is a vector of truth assignments today items.",
                    "label": 1
                },
                {
                    "sent": "These partition function that sums over all possible troop assignments to grind terms and it serves to normalize the expression.",
                    "label": 1
                },
                {
                    "sent": "The right to make it valid probability with some overall formulas.",
                    "label": 0
                },
                {
                    "sent": "WI is the weight of the eye formula and number of true groundings of the eye formula.",
                    "label": 0
                },
                {
                    "sent": "It is interesting to note that first order logic and most statistical models are special cases of.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alliance.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Moving on to our approach, multiple relational clusterings.",
                    "label": 1
                },
                {
                    "sent": "MRC MRC is our approach.",
                    "label": 1
                },
                {
                    "sent": "We invent unary predicates as clusters.",
                    "label": 0
                },
                {
                    "sent": "We invent multiple cross cutting clusterings to capture dependencies in relational domains with clusters relations by the objects they relate.",
                    "label": 1
                },
                {
                    "sent": "An objects by their related objects and the relations in which they appear.",
                    "label": 0
                },
                {
                    "sent": "Because objects of the same type and we cluster relationship the same error T an argument type.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So consider this example illustrating the importance of multiple clusterings.",
                    "label": 1
                },
                {
                    "sent": "People have friends, coworkers, hobbies and technical skills.",
                    "label": 0
                },
                {
                    "sent": "A person hobbies are best predicted by her friends, hobbies, person skills are best protected by her coworkers skills.",
                    "label": 0
                },
                {
                    "sent": "If you long everybody together in one huge cluster, our ability to predict both hobbies and skills will be heard instead, which is simultaneously.",
                    "label": 1
                },
                {
                    "sent": "Classes together people who are friends and people who are coworkers.",
                    "label": 0
                },
                {
                    "sent": "the French clusters are highly predictive of hobbies and the coworker, coworkers, clusters are highly predictive of skills.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now MSE is based on finite function free 2nd order Markov logic, so it's not full blown.",
                    "label": 0
                },
                {
                    "sent": "2nd Order logic in MRC variables range over relations, that is predicates and objects that is constant.",
                    "label": 1
                },
                {
                    "sent": "Note that I use the terms relations and predicates interchangeably.",
                    "label": 0
                },
                {
                    "sent": "Likewise for the terms, objects and constants.",
                    "label": 0
                },
                {
                    "sent": "On the ground atoms, if all possible predicate symbols as well as constant symbols, we use 2nd order Markov logic because it's able to represent some models more compactly than first order Markov logic, and we needed to specify how predicate symbols are clustered.",
                    "label": 1
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These are the symbols used in the rules that I'll show you in the next slide.",
                    "label": 0
                },
                {
                    "sent": "We use small gamma I to indicate a cluster.",
                    "label": 0
                },
                {
                    "sent": "Bit gamma to indicate a clustering that is a partitioning of a set of symbols.",
                    "label": 0
                },
                {
                    "sent": "Over here.",
                    "label": 0
                },
                {
                    "sent": "Are this is an Atom, but R is a relation predicate symbol X one to XN.",
                    "label": 0
                },
                {
                    "sent": "The arguments of that Atom.",
                    "label": 0
                },
                {
                    "sent": "XI the symbol XI in cluster gamma is also an Atom.",
                    "label": 0
                },
                {
                    "sent": "This should become clearer.",
                    "label": 0
                },
                {
                    "sent": "When you look at the rules.",
                    "label": 0
                },
                {
                    "sent": "Let me talk about a cluster combination.",
                    "label": 1
                },
                {
                    "sent": "We're referring to the combination of clusters to which the constituent symbols in an Atom belongs.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we have 5 rules that we use.",
                    "label": 0
                },
                {
                    "sent": "The first fixed that each symbol belongs to at least one cluster.",
                    "label": 1
                },
                {
                    "sent": "This is a hard rule, so it's given infinite weight.",
                    "label": 0
                },
                {
                    "sent": "That means it cannot be violated.",
                    "label": 0
                },
                {
                    "sent": "The second rule states that a symbol cannot belong to more than one cluster in the same cluster ring.",
                    "label": 0
                },
                {
                    "sent": "Again, this is a hard rule and given infinite wait.",
                    "label": 1
                },
                {
                    "sent": "The third rule states that each Atom appears in exactly 1 combination of clusters.",
                    "label": 0
                },
                {
                    "sent": "These two is a hard rule and given infinite weight.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The next rule we called Atom Prediction Rule, states the true value of an Atom is determined by the cluster combination to which it belongs.",
                    "label": 1
                },
                {
                    "sent": "Now let's pass this rule in more detail.",
                    "label": 0
                },
                {
                    "sent": "Over here we see a + This is nothing but syntactic sugar.",
                    "label": 0
                },
                {
                    "sent": "Emili states that there's an instance of this rule.",
                    "label": 0
                },
                {
                    "sent": "For every possible cluster combination and every instance of the rule has his own weight.",
                    "label": 0
                },
                {
                    "sent": "To restate the rule, the probability that this Atom is true.",
                    "label": 0
                },
                {
                    "sent": "Is determined by the cluster membership of is constituent symbols.",
                    "label": 0
                },
                {
                    "sent": "The larger the width.",
                    "label": 0
                },
                {
                    "sent": "The higher the probability that the Atom is true.",
                    "label": 0
                },
                {
                    "sent": "Now, this way it has to be learned.",
                    "label": 0
                },
                {
                    "sent": "It's not fixed.",
                    "label": 0
                },
                {
                    "sent": "The last rule is the exponential prior rule.",
                    "label": 0
                },
                {
                    "sent": "On the number of clusters, it seems to penalize the number of clusters in our in our model, is given fixed negative negative Lambda.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Learning the MLC model consists of finding two things that maximize log posterior probability.",
                    "label": 1
                },
                {
                    "sent": "The first cluster first is cluster assignment.",
                    "label": 1
                },
                {
                    "sent": "That is the assignment of truth values to the R in gamma R&X in gamma X atoms, the 2nd.",
                    "label": 1
                },
                {
                    "sent": "Is which of the Atom prediction rules over here you see the expression?",
                    "label": 0
                },
                {
                    "sent": "For the log, posterior probabilities are is the vector of proof assignments to all ground observed ground atoms.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now show you how the five rules map to these two components of the log posterior probability.",
                    "label": 0
                },
                {
                    "sent": "The Tryhard rules an exponential prior rule Maps to the first component and is evaluated as shown on the slide is value is negative Infinity if the cluster assignment violates a hard rule because the probability of the cluster assignment is 0 for all other cluster assignments is merely the number of clusters multiplied by the Lambda penalty.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other predictions rules map to the second component.",
                    "label": 0
                },
                {
                    "sent": "Now given a cluster assignment, the MLN containing the Atom prediction rules decomposes into one MLN.",
                    "label": 0
                },
                {
                    "sent": "For each rule.",
                    "label": 0
                },
                {
                    "sent": "This is why the width of the rule can be computed in close form, simply as the lots of the Atom in the cluster combination being true T&F.",
                    "label": 1
                },
                {
                    "sent": "Are the number and true and false atoms in the cluster combination?",
                    "label": 0
                },
                {
                    "sent": "Since the weights can be computed in close form, the second component can also be computed in closed form now.",
                    "label": 0
                },
                {
                    "sent": "Better over here just smoothing parameter that we use.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now that I've shown you our now than you did last year, probability, which is our objective function, I'll show you how we search for the MLP.",
                    "label": 0
                },
                {
                    "sent": "The best cluster assignment.",
                    "label": 0
                },
                {
                    "sent": "We make an approximation.",
                    "label": 0
                },
                {
                    "sent": "We make hot assignments of symbols to classes.",
                    "label": 1
                },
                {
                    "sent": "Now I've shown you that estimating the weights given the cluster assignment is easy.",
                    "label": 0
                },
                {
                    "sent": "What's hard is finding MFP cluster assignments because the search space is so huge Now, this is intractable, so we have to use, so we use greedy search.",
                    "label": 1
                },
                {
                    "sent": "Our algorithm can be seen as a form of topdown defensive refinement algorithm.",
                    "label": 0
                },
                {
                    "sent": "As we further refine clusters of symbols, clusters of objects, we can further refine clusters of related objects in clusters of relations.",
                    "label": 1
                },
                {
                    "sent": "Every algorithm can be seen to have two levels.",
                    "label": 0
                },
                {
                    "sent": "The top finds cluster rings, the bottom finds clusters.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Illustrate this with.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "10 fold cross validation train on 9 and tests on the remaining one.",
                    "label": 0
                },
                {
                    "sent": "We measured the average conditional log likelihood of the test ground items, the CLL and the average area under the precision, precision recall curves of the test ground atoms, the AUC.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We compared with the infinite relational model, the IRM an MLN structure, learning.",
                    "label": 1
                },
                {
                    "sent": "We use the default IRM parameters downloadable from the web.",
                    "label": 0
                },
                {
                    "sent": "We ran it for 10 hours.",
                    "label": 0
                },
                {
                    "sent": "We set the MRC parameters of Lambda and better both to one without any tuning.",
                    "label": 0
                },
                {
                    "sent": "We ran mercy for 10 hours for the first level of clustering.",
                    "label": 1
                },
                {
                    "sent": "That is the root note of the hierarchy that I've shown you.",
                    "label": 0
                },
                {
                    "sent": "For subsequent levels, we permitted MRC 100 steps we took in total about 3 to 10 minutes MLN structured learning.",
                    "label": 0
                },
                {
                    "sent": "It's allowed to run for 24 hours and its parameter settings are in an online appendix.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No, for the results.",
                    "label": 0
                },
                {
                    "sent": "The top rule shows the results for the conditional log likelihood and the bottom shows the results for area under the precision recall curves.",
                    "label": 0
                },
                {
                    "sent": "The error bars indicate one standard deviation in either direction.",
                    "label": 0
                },
                {
                    "sent": "Now let's look first look, compare IRM and MRC the white bar in the purple bar.",
                    "label": 0
                },
                {
                    "sent": "So as you can see.",
                    "label": 0
                },
                {
                    "sent": "I am at MRC performs comparably on the animal.",
                    "label": 0
                },
                {
                    "sent": "Any nations data set.",
                    "label": 0
                },
                {
                    "sent": "However, on the UMLS and kinship datasets, MRC performs considerably better than the IRM.",
                    "label": 0
                },
                {
                    "sent": "Note that the M, LS and kinship datasets are one to two orders of magnitude larger than the animals and nations datasets.",
                    "label": 0
                },
                {
                    "sent": "This suggests that using MRC finding multiple clusterings is helpful on large, rich relational domains.",
                    "label": 0
                },
                {
                    "sent": "Now let's compare.",
                    "label": 0
                },
                {
                    "sent": "MSE with MLA structure.",
                    "label": 0
                },
                {
                    "sent": "Learning the light blue bar.",
                    "label": 0
                },
                {
                    "sent": "As you can see from the slide, MRC does better than MLN structure learning all datasets.",
                    "label": 0
                },
                {
                    "sent": "SFO nations or nations that does about equally well?",
                    "label": 0
                },
                {
                    "sent": "The thing about nations is that most of their relationships are symmetric, so the country a goes to war Country B, country B, Jolly Bell goes toward country A.",
                    "label": 0
                },
                {
                    "sent": "So Emma instructor, learning found such a rule and consequently does well.",
                    "label": 0
                },
                {
                    "sent": "Now we can look at.",
                    "label": 0
                },
                {
                    "sent": "I must see compared to init, init is just mercy with the first leave, first level of clustering.",
                    "label": 0
                },
                {
                    "sent": "So am I see does better finding multiple clusterings does better than finding one clustering or all datasets except for the animals data set.",
                    "label": 0
                },
                {
                    "sent": "So animals that's really small, we don't expect to gain much from multiple clusterings.",
                    "label": 0
                },
                {
                    "sent": "OK tomorrow.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So move on now I'll show you some examples on multiple clusterings that we learn on the M LS data set over here.",
                    "label": 1
                },
                {
                    "sent": "This is the first clustering that we learned.",
                    "label": 0
                },
                {
                    "sent": "For example, virus, fungus, LG plant found in bio active.",
                    "label": 0
                },
                {
                    "sent": "Sorry by active substances biology mines are found in virus fungus LG implant but they are not found in and feeding birds, vertebrate animals.",
                    "label": 1
                },
                {
                    "sent": "Bara spongy invertebrate causes diseases.",
                    "label": 0
                },
                {
                    "sent": "Cell dysfunction.",
                    "label": 0
                },
                {
                    "sent": "LG plant and feed bin.",
                    "label": 0
                },
                {
                    "sent": "But do not cost.",
                    "label": 0
                },
                {
                    "sent": "These are cells dysfunction now this is.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "According to the journal's database.",
                    "label": 0
                },
                {
                    "sent": "Virus fungus LG implant.",
                    "label": 0
                },
                {
                    "sent": "They're not even different animals and feeding birds fish.",
                    "label": 0
                },
                {
                    "sent": "They are vertebrate animals.",
                    "label": 0
                },
                {
                    "sent": "Now MRC is not perfect.",
                    "label": 0
                },
                {
                    "sent": "Over here is this data.",
                    "label": 0
                },
                {
                    "sent": "Invertebrate is of interpret, which is wrong.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So over here we have an example of.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Triple cross cutting.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For future work, future work would like to experiment on larger datasets.",
                    "label": 1
                },
                {
                    "sent": "Right now we are looking at inducing ontologies from web text using MRC.",
                    "label": 0
                },
                {
                    "sent": "Would like to use the clusters that we learn as primitives in structured learning.",
                    "label": 1
                },
                {
                    "sent": "Would like to retain the entire hierarchy of multiple clusterings and perform shrinkage over it.",
                    "label": 0
                },
                {
                    "sent": "Would like to cluster predicates with different arities.",
                    "label": 1
                },
                {
                    "sent": "An argument types and we speculate that all relational structure learning can be accomplished with SPI alone.",
                    "label": 1
                },
                {
                    "sent": "Now this is our ultimate goal.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In conclusion, I presented I proposed statistical predicate invention as a key problem for SRL.",
                    "label": 1
                },
                {
                    "sent": "I've presented multiple relational clusterings.",
                    "label": 1
                },
                {
                    "sent": "Our approach offers step towards a general framework for SPI.",
                    "label": 1
                },
                {
                    "sent": "This based on finite 2nd order Markov logic.",
                    "label": 0
                },
                {
                    "sent": "It creates multiple cross cut during class cross cutting clusterings of symbols in data and empirical comparisons with.",
                    "label": 0
                },
                {
                    "sent": "At mountain structure learning and ERM, shows the promise of our approach.",
                    "label": 0
                },
                {
                    "sent": "I'm unclear why you're calling this.",
                    "label": 0
                },
                {
                    "sent": "Because.",
                    "label": 0
                },
                {
                    "sent": "Conventional's right banks on introduced it in.",
                    "label": 0
                },
                {
                    "sent": "Constructs a definition of a predicate.",
                    "label": 0
                },
                {
                    "sent": "It doesn't just introduce constructed definition.",
                    "label": 0
                },
                {
                    "sent": "And it also all of the examples you are giving are simply hierarchical clustering, which introduces us inherently one credit.",
                    "label": 0
                },
                {
                    "sent": "So I mean, it's much more limited than the concept of predicate convention.",
                    "label": 0
                },
                {
                    "sent": "Which makes me doubt your intention that this is going to solve over relational learning.",
                    "label": 0
                },
                {
                    "sent": "In terms of.",
                    "label": 0
                },
                {
                    "sent": "Right this is I do agree that right now we are not learning the kind of rich theories that kind of we're not accomplishing the same richness that is accomplished by predicate invention.",
                    "label": 0
                },
                {
                    "sent": "Data agreed, so this is this is like a set of first step towards SPI.",
                    "label": 0
                },
                {
                    "sent": "So the point is that we are not merely finding hierarchical clustering because the clusters cross cut.",
                    "label": 0
                },
                {
                    "sent": "So so just not.",
                    "label": 0
                },
                {
                    "sent": "So it's not simply just hierarchical clustering.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Yeah, you're right, this is not full blown predicate invention like what's done in IOP and this is a first step.",
                    "label": 0
                },
                {
                    "sent": "Objects are higher order predicates one predicate, but you're not learning.",
                    "label": 0
                },
                {
                    "sent": "Multi.",
                    "label": 0
                },
                {
                    "sent": "Area is one of the big things is missing.",
                    "label": 0
                },
                {
                    "sent": "There's learning new relations across objects, right, right so.",
                    "label": 0
                },
                {
                    "sent": "This is not.",
                    "label": 0
                },
                {
                    "sent": "Multi right.",
                    "label": 0
                },
                {
                    "sent": "Totally true as I stated, but introduce MRC.",
                    "label": 0
                },
                {
                    "sent": "We're learning unary predicates as clusters, so the next step would be learning multi arity predicates.",
                    "label": 0
                },
                {
                    "sent": "Comparison between your technique and IRL.",
                    "label": 0
                },
                {
                    "sent": "You showed us that on some datasets, in performs better.",
                    "label": 0
                },
                {
                    "sent": "Do you know why it performs better?",
                    "label": 0
                },
                {
                    "sent": "So for these two datasets.",
                    "label": 0
                },
                {
                    "sent": "It performs better because.",
                    "label": 0
                },
                {
                    "sent": "It finds multiple clusterings.",
                    "label": 0
                },
                {
                    "sent": "So S. You're not convinced, yeah?",
                    "label": 0
                },
                {
                    "sent": "Does that.",
                    "label": 0
                },
                {
                    "sent": "Why do we know it?",
                    "label": 0
                },
                {
                    "sent": "But I agree, I agree with you.",
                    "label": 0
                },
                {
                    "sent": "Why do we know that result?",
                    "label": 0
                },
                {
                    "sent": "Well, one thing that we did is more that.",
                    "label": 0
                },
                {
                    "sent": "Provide evidence to that is, we also compared to the case where we do not find multiple clusterings.",
                    "label": 0
                },
                {
                    "sent": "How long?",
                    "label": 0
                },
                {
                    "sent": "Actually, I mentioned that we let you run for 10 hours.",
                    "label": 0
                },
                {
                    "sent": "But I still try experiments running for 24 and 48 hours, and there's not much of a difference.",
                    "label": 0
                },
                {
                    "sent": "So so possibly we could have really run for 72 hours and we could have better results, but we don't really know, so this is.",
                    "label": 0
                },
                {
                    "sent": "Greedy search.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is never.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because it's greedy search we restart so we can just keep on restarting.",
                    "label": 0
                }
            ]
        }
    }
}