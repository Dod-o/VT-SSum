{
    "id": "7w765d2vxhqpqcviuowdushq4y4j5via",
    "title": "Multimodal Integration for Meeting Group Action Segmentation and Recognition",
    "info": {
        "author": [
            "Marc Al Hames, TU Munich"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "June 2005",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/mlmi04uk_hames_mimga/",
    "segmentation": [
        [
            "From the media in Switzerland and also University in Edinburgh.",
            "This has been done in the framework of me."
        ],
        [
            "First, short overview, I will give you an introduction what is structuring of meetings.",
            "Then roughly describe audiovisual features.",
            "Then come to four different models we came up and finally a summary.",
            "Unfortunately time is very limited.",
            "Each of the models would take about 20 minutes to explain in details, so I will."
        ],
        [
            "Have to rush a bit first of all to give you an idea what we like to do is, we say a meeting can be structured as a sequence of actions.",
            "This can be individual actions or group actions.",
            "And they can be from any set that we have defined here as A and you see, we now have a sequence there.",
            "A2A 1A2.",
            "To give you."
        ],
        [
            "More detail with different sets.",
            "We can represent different meeting views.",
            "So on the bottom level, for example, you see group task and currently we're doing information sharing.",
            "I'm sharing my information with you.",
            "We could go to another level and see we have in a discussion phase, currently a presentation and thereafter discussion and then again in presentation.",
            "We could go even higher and say we model for example, that the group interest is yet another set an I hope the group interest.",
            "Is high currently.",
            "And finally we can model each person in the meeting or in the lecture on its own."
        ],
        [
            "So more formally, we can define meeting views and first of all we can split them into group meeting views and individual meeting views, and then we can define whatever we like most important discussion phases.",
            "Tasks like brainstorming, decision making or the interest level and the same we can do for the individual level and just a very short remark on the bottom.",
            "Here we can also.",
            "With further sets represent the current agenda item or, for example, the topic discussed.",
            "But this might require higher semantic knowledge which we don't use in this work, at least not too much."
        ],
        [
            "OK, for this work we have a very limited action lexicon.",
            "We only consider the group discussion phases and we defined 2 action lexica.",
            "The first one has 8 action classes, basically discussion, monologue and the monologue comes with the speaker ID that makes the number 8 together, notetaking presentation and whiteboard and then we have a second X action lexicon which comprehends the full action lexicon.",
            "One plus combinations of the action lexicon.",
            "1.",
            "Or"
        ],
        [
            "I's data SDM data has has been ready for this work.",
            "We have as yet used the M for meeting corpus, which is, I guess, well known recorded TV app with three fixed cameras, labor microphones, microphone, airing each meeting has four participants.",
            "They're quite well structured so they are quite good for this work.",
            "We have 59 videos and each has five minutes.",
            "Now."
        ],
        [
            "My system overview.",
            "This is a multimodal problem because you can't just use ASR and see what has been spoken.",
            "You have to see how other people that don't speak, for example, act.",
            "So we use all three streams, like the cameras, the label microphones, and the microphone area.",
            "We extract some multimodal features and then we do action segmentation and classification, and in the end we hope to get out.",
            "Hopefully the right sequence of actions.",
            "Now let me summarize roughly the features we use."
        ],
        [
            "For the visual features we use basically two sets.",
            "First, our global motions for each position in the meeting room we derives from subtraction of subsequent images, center of motion, changes of motion, and so on.",
            "And for each person we derive skin color blobs with just GMM model and then we get head orientation, motion, hands position, size, motion and also moving blocks from background subtraction.",
            "So you see the.",
            "For each position, the global motions can be represented, represented as such ellipse in this picture, and you see on the right hand side the skin color blobs.",
            "For each person."
        ],
        [
            "As audio features we use form from the microphone area again for each position.",
            "Basically speech and silence segmentation on you.",
            "See something on the bottom here.",
            "So you see from time 0 till time 80s speaker three is very active and then the white board position is very active and from the label microphones that is information for each person we use.",
            "We use pitch energy speaking rate and MFT coefficients.",
            "So very standard methods basically."
        ],
        [
            "And finally, to a very limited degree, we use also lexical features.",
            "I just mentioned speech speech transcription could be used that has not been done in this work.",
            "Orchestra transcription for example.",
            "So if we have output of a guest recognizer, we could also feed our models.",
            "Of course with against the inventory like writing, pointing and so on.",
            "OK, so now I have to explain you for models and unfortunately have to be very fast so."
        ],
        [
            "I'm sorry for this.",
            "The first approach we have is a bit like big and it's based on semantic features.",
            "And you see an image on the bottom here.",
            "It's basically you have two windows with variable length shifted over the timescale and you see the inner border is shifted from left to right and then it use any classifier to classify the left and the right window.",
            "And if you get a different result you say.",
            "Well, this is a border.",
            "And action border.",
            "And if you don't have any boundary in the complete window, you enlarge your whole window well, and then you repeat this until you reach the end of the meeting and you see we found two boundaries here on the bottom.",
            "OK."
        ],
        [
            "Doing we will always have the same procedure to class to rate our results.",
            "We will have insertion rate deletion rate accuracy which is very similar to speech recognition and then a recognition error.",
            "So this could be compared to a word error rate and for this big like approach, 57 meetings with the lexicon, one which is 8 action classes.",
            "And you see on the left hand side the different classifiers were used to classify the different Windows an the best deletion rate can be obtained with the support vector machine you see.",
            "This is below 1%.",
            "However the best accuracy.",
            "It's given with radial basis function right here, and so is the best.",
            "Recognition rates are lower numbers, of course, better here, but this is highly variable depending on the classifier.",
            "But you can see works quite well.",
            "Now."
        ],
        [
            "Now quite a heavy shift to another model which is a multi layer hidden Markov model and in this approach the problem has been divided into 2 independent layers.",
            "First an individual layer and then a group layer and both are model with HMM.",
            "And the individual action layer represents each person in the meeting.",
            "So we have four hmm, each one representing one person, and in these individual HMM the input is the person features and then the output of the individual layer is fed into a group action.",
            "Hmm, together with some group features and we can those train each layer independently and finally do simultaneous segmentation and recognition.",
            "This has some advantage."
        ],
        [
            "As you can see on the next slide.",
            "We have a small observation space, so even for limited training data we are quite stable and the main advantage is the individual layer.",
            "Hmm's are person independent, of course.",
            "Then you have more training data to train this layer.",
            "And finally the group action hmm is less sensitive to changes in the low level feature space.",
            "While and as the two layers are trained in dependently, we can explore different combinations of HMM such as in Crohn's or early integration Hmm's.",
            "Well."
        ],
        [
            "And again, some results.",
            "So this is just the actual right again and 1st resolve.",
            "If you compare single layer Hmm's on the top different single layer Hmm's with the different multilayer hmm approaches, you see that the single layer Hmm's are always worse than the multilayer hmm.",
            "Just as an example for the asynchronous case, you see 7% improvement.",
            "If you now compare just for the multilayer hmm, the single modality features with the multimodality features you find that the single modality features are always worse than the multimodal features.",
            "This is something we found for nearly all models, and of course this is a multi model problem while and finally to summarize the best overall result can be achieved with the multilayer HMM inasan Cronus case, which indicates there is some other quality in our data."
        ],
        [
            "Now 1/3 model we've explored which is a multi stream DBN model so this fits well.",
            "After the last talk.",
            "And this is in hierarchical approach.",
            "You see on the bottom we have independent modality processing.",
            "So these are the input nodes.",
            "In dark blue here and then, in the next level there is a state space decomposition.",
            "These are the green nodes and they are trained unsupervised and then they are summarized in the action nodes a that are actually the actual classes we like to detect.",
            "This would already be.",
            "And finalized model yet to improve the state duration modeling to prevent the model from jumping from state to state to different actual classes.",
            "There is a counter structure that improves the state duration modeling."
        ],
        [
            "And again, some short results.",
            "On the left hand side you see different features in apps.",
            "I haven't mentioned this during the features.",
            "We have three different features that apps you see, just the standard HM, then a multi stream approach and the margin stream approaches the counter structure.",
            "And for the results you can see.",
            "We couldn't get quite a heavy improvement if you compare 92% to 21%, that is quite heavily.",
            "And especially one feature set up PDF feature setups.",
            "Benefits quite a lot from the counter structure.",
            "Well, I finally the best recognition rate can be achieved with the edenburg feature set up in the multi stream approach.",
            "This is because this has been developed at Edenburg and they have tuned it to their feature set.",
            "Should mention.",
            "OK."
        ],
        [
            "And finally, we also developed something that can be occlusions and disturbances in the in the data an we developed a model that can handle such occlusions and disturbances.",
            "This fits well into the last talk.",
            "Again we couple and hmm with linear dynamical system and we use the Hmm's driving input for the LDS and those we can compensate disturbance system.",
            "You can see it on the right hand side.",
            "We can represent this as a dynamic Bayesian network.",
            "So we have a multi stream hmm on the top and linear dynamical system on the bottom and they are coupled time goes from left to right.",
            "And if we do this, we can improve our recognition results for disturbed data.",
            "So in this case higher numbers are better."
        ],
        [
            "As we haven't done segmentation, you're just classification, yet segmentation would be possible.",
            "So first we found the same.",
            "The individual.",
            "Features audio area and video stream are always worse than using a multimodal approach.",
            "I said we found this for all models.",
            "Then if you see what happens if we just drop the Lake with stream.",
            "So we add some background bubble, you can see that for example, the audio only single model HMM drops quite heavily in the recognition rate by more than 20%, while the multimodal systems keep up quite high with recognition rate and especially the mixed state DBN.",
            "Nearly with this, nothing is less than 1% compared to the HMM.",
            "Well, and finally, if you see the same, we have to stop all streams.",
            "The visual streams with some black bars and the label with some background bubble.",
            "You see that we still we're still better with our approach using these improvement with the LDS compared to a standard HMM.",
            "OK to summarize."
        ],
        [
            "This thing I know this was very quick because I had to put in quite a lot in one short talk.",
            "This was a joint work of three institutes within the ME Project and we modeled meeting as a sequence of individual and group actions.",
            "And here in this special case we have taken the group action discussion phases.",
            "Yet the models can easily be adapted to all kind of sets.",
            "Will actually do to model meetings.",
            "We have derived large number of audiovisual features.",
            "We've tested various combinations of these.",
            "We've come up with four different group action modeling frameworks I've explained to you roughly, and you've seen all of them have a good performance yet there's still much space for improvements both in the feature domain and in the model domain, especially if you like to enlarge the set of action classes.",
            "And if you like to go to layout approaches, those with the mediator, we are now going to to investigate combinations of the proposed systems to use the complementary strengths.",
            "Thanks for your attention.",
            "Time for a number of questions.",
            "Preston.",
            "Question for you yeah.",
            "You mentioned that you were using his orientation.",
            "Your feature vector slides.",
            "I yes, that's right, this has not been real.",
            "Part of this work, but it's basically we're using GMM models to extract skin color and then skin blocks and then head orientation is derived from there.",
            "But please don't ask me how executives has been worked because this was not part of the work.",
            "Well, I have a second ship.",
            "It was all an 88 classes right?",
            "Yes 8 or 14 depending on which action lexicon.",
            "The classes are.",
            "Could you have two activities?",
            "The active at the same time in your data?",
            "I guess we can't come up with a slight again.",
            "OK, I can explain well the two actual lexicon and the first one you have 8 action classes.",
            "They are disjunct so you have somebody talking as a monologue or giving a presentation or discussion.",
            "Yet with the second X actual lexicon you also have combinations like for example I'm speaking and people take notes at the same time, so it depends on the actual lexicon.",
            "And in a future, that is what I mentioned for the approach we're going more to investigate these combinations, because obviously people act while somebody is acting at the same time.",
            "OK, thank you again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From the media in Switzerland and also University in Edinburgh.",
                    "label": 0
                },
                {
                    "sent": "This has been done in the framework of me.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, short overview, I will give you an introduction what is structuring of meetings.",
                    "label": 1
                },
                {
                    "sent": "Then roughly describe audiovisual features.",
                    "label": 0
                },
                {
                    "sent": "Then come to four different models we came up and finally a summary.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately time is very limited.",
                    "label": 0
                },
                {
                    "sent": "Each of the models would take about 20 minutes to explain in details, so I will.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Have to rush a bit first of all to give you an idea what we like to do is, we say a meeting can be structured as a sequence of actions.",
                    "label": 1
                },
                {
                    "sent": "This can be individual actions or group actions.",
                    "label": 0
                },
                {
                    "sent": "And they can be from any set that we have defined here as A and you see, we now have a sequence there.",
                    "label": 0
                },
                {
                    "sent": "A2A 1A2.",
                    "label": 0
                },
                {
                    "sent": "To give you.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More detail with different sets.",
                    "label": 1
                },
                {
                    "sent": "We can represent different meeting views.",
                    "label": 1
                },
                {
                    "sent": "So on the bottom level, for example, you see group task and currently we're doing information sharing.",
                    "label": 0
                },
                {
                    "sent": "I'm sharing my information with you.",
                    "label": 0
                },
                {
                    "sent": "We could go to another level and see we have in a discussion phase, currently a presentation and thereafter discussion and then again in presentation.",
                    "label": 0
                },
                {
                    "sent": "We could go even higher and say we model for example, that the group interest is yet another set an I hope the group interest.",
                    "label": 0
                },
                {
                    "sent": "Is high currently.",
                    "label": 0
                },
                {
                    "sent": "And finally we can model each person in the meeting or in the lecture on its own.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So more formally, we can define meeting views and first of all we can split them into group meeting views and individual meeting views, and then we can define whatever we like most important discussion phases.",
                    "label": 0
                },
                {
                    "sent": "Tasks like brainstorming, decision making or the interest level and the same we can do for the individual level and just a very short remark on the bottom.",
                    "label": 1
                },
                {
                    "sent": "Here we can also.",
                    "label": 0
                },
                {
                    "sent": "With further sets represent the current agenda item or, for example, the topic discussed.",
                    "label": 1
                },
                {
                    "sent": "But this might require higher semantic knowledge which we don't use in this work, at least not too much.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, for this work we have a very limited action lexicon.",
                    "label": 0
                },
                {
                    "sent": "We only consider the group discussion phases and we defined 2 action lexica.",
                    "label": 1
                },
                {
                    "sent": "The first one has 8 action classes, basically discussion, monologue and the monologue comes with the speaker ID that makes the number 8 together, notetaking presentation and whiteboard and then we have a second X action lexicon which comprehends the full action lexicon.",
                    "label": 1
                },
                {
                    "sent": "One plus combinations of the action lexicon.",
                    "label": 0
                },
                {
                    "sent": "1.",
                    "label": 0
                },
                {
                    "sent": "Or",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I's data SDM data has has been ready for this work.",
                    "label": 0
                },
                {
                    "sent": "We have as yet used the M for meeting corpus, which is, I guess, well known recorded TV app with three fixed cameras, labor microphones, microphone, airing each meeting has four participants.",
                    "label": 1
                },
                {
                    "sent": "They're quite well structured so they are quite good for this work.",
                    "label": 1
                },
                {
                    "sent": "We have 59 videos and each has five minutes.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My system overview.",
                    "label": 0
                },
                {
                    "sent": "This is a multimodal problem because you can't just use ASR and see what has been spoken.",
                    "label": 0
                },
                {
                    "sent": "You have to see how other people that don't speak, for example, act.",
                    "label": 0
                },
                {
                    "sent": "So we use all three streams, like the cameras, the label microphones, and the microphone area.",
                    "label": 0
                },
                {
                    "sent": "We extract some multimodal features and then we do action segmentation and classification, and in the end we hope to get out.",
                    "label": 1
                },
                {
                    "sent": "Hopefully the right sequence of actions.",
                    "label": 0
                },
                {
                    "sent": "Now let me summarize roughly the features we use.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the visual features we use basically two sets.",
                    "label": 1
                },
                {
                    "sent": "First, our global motions for each position in the meeting room we derives from subtraction of subsequent images, center of motion, changes of motion, and so on.",
                    "label": 1
                },
                {
                    "sent": "And for each person we derive skin color blobs with just GMM model and then we get head orientation, motion, hands position, size, motion and also moving blocks from background subtraction.",
                    "label": 1
                },
                {
                    "sent": "So you see the.",
                    "label": 0
                },
                {
                    "sent": "For each position, the global motions can be represented, represented as such ellipse in this picture, and you see on the right hand side the skin color blobs.",
                    "label": 0
                },
                {
                    "sent": "For each person.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As audio features we use form from the microphone area again for each position.",
                    "label": 1
                },
                {
                    "sent": "Basically speech and silence segmentation on you.",
                    "label": 1
                },
                {
                    "sent": "See something on the bottom here.",
                    "label": 1
                },
                {
                    "sent": "So you see from time 0 till time 80s speaker three is very active and then the white board position is very active and from the label microphones that is information for each person we use.",
                    "label": 0
                },
                {
                    "sent": "We use pitch energy speaking rate and MFT coefficients.",
                    "label": 0
                },
                {
                    "sent": "So very standard methods basically.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally, to a very limited degree, we use also lexical features.",
                    "label": 1
                },
                {
                    "sent": "I just mentioned speech speech transcription could be used that has not been done in this work.",
                    "label": 0
                },
                {
                    "sent": "Orchestra transcription for example.",
                    "label": 1
                },
                {
                    "sent": "So if we have output of a guest recognizer, we could also feed our models.",
                    "label": 1
                },
                {
                    "sent": "Of course with against the inventory like writing, pointing and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so now I have to explain you for models and unfortunately have to be very fast so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm sorry for this.",
                    "label": 0
                },
                {
                    "sent": "The first approach we have is a bit like big and it's based on semantic features.",
                    "label": 0
                },
                {
                    "sent": "And you see an image on the bottom here.",
                    "label": 0
                },
                {
                    "sent": "It's basically you have two windows with variable length shifted over the timescale and you see the inner border is shifted from left to right and then it use any classifier to classify the left and the right window.",
                    "label": 0
                },
                {
                    "sent": "And if you get a different result you say.",
                    "label": 0
                },
                {
                    "sent": "Well, this is a border.",
                    "label": 0
                },
                {
                    "sent": "And action border.",
                    "label": 0
                },
                {
                    "sent": "And if you don't have any boundary in the complete window, you enlarge your whole window well, and then you repeat this until you reach the end of the meeting and you see we found two boundaries here on the bottom.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Doing we will always have the same procedure to class to rate our results.",
                    "label": 0
                },
                {
                    "sent": "We will have insertion rate deletion rate accuracy which is very similar to speech recognition and then a recognition error.",
                    "label": 1
                },
                {
                    "sent": "So this could be compared to a word error rate and for this big like approach, 57 meetings with the lexicon, one which is 8 action classes.",
                    "label": 0
                },
                {
                    "sent": "And you see on the left hand side the different classifiers were used to classify the different Windows an the best deletion rate can be obtained with the support vector machine you see.",
                    "label": 0
                },
                {
                    "sent": "This is below 1%.",
                    "label": 0
                },
                {
                    "sent": "However the best accuracy.",
                    "label": 0
                },
                {
                    "sent": "It's given with radial basis function right here, and so is the best.",
                    "label": 0
                },
                {
                    "sent": "Recognition rates are lower numbers, of course, better here, but this is highly variable depending on the classifier.",
                    "label": 0
                },
                {
                    "sent": "But you can see works quite well.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now quite a heavy shift to another model which is a multi layer hidden Markov model and in this approach the problem has been divided into 2 independent layers.",
                    "label": 1
                },
                {
                    "sent": "First an individual layer and then a group layer and both are model with HMM.",
                    "label": 0
                },
                {
                    "sent": "And the individual action layer represents each person in the meeting.",
                    "label": 0
                },
                {
                    "sent": "So we have four hmm, each one representing one person, and in these individual HMM the input is the person features and then the output of the individual layer is fed into a group action.",
                    "label": 0
                },
                {
                    "sent": "Hmm, together with some group features and we can those train each layer independently and finally do simultaneous segmentation and recognition.",
                    "label": 1
                },
                {
                    "sent": "This has some advantage.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As you can see on the next slide.",
                    "label": 0
                },
                {
                    "sent": "We have a small observation space, so even for limited training data we are quite stable and the main advantage is the individual layer.",
                    "label": 0
                },
                {
                    "sent": "Hmm's are person independent, of course.",
                    "label": 0
                },
                {
                    "sent": "Then you have more training data to train this layer.",
                    "label": 0
                },
                {
                    "sent": "And finally the group action hmm is less sensitive to changes in the low level feature space.",
                    "label": 0
                },
                {
                    "sent": "While and as the two layers are trained in dependently, we can explore different combinations of HMM such as in Crohn's or early integration Hmm's.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again, some results.",
                    "label": 0
                },
                {
                    "sent": "So this is just the actual right again and 1st resolve.",
                    "label": 0
                },
                {
                    "sent": "If you compare single layer Hmm's on the top different single layer Hmm's with the different multilayer hmm approaches, you see that the single layer Hmm's are always worse than the multilayer hmm.",
                    "label": 0
                },
                {
                    "sent": "Just as an example for the asynchronous case, you see 7% improvement.",
                    "label": 0
                },
                {
                    "sent": "If you now compare just for the multilayer hmm, the single modality features with the multimodality features you find that the single modality features are always worse than the multimodal features.",
                    "label": 0
                },
                {
                    "sent": "This is something we found for nearly all models, and of course this is a multi model problem while and finally to summarize the best overall result can be achieved with the multilayer HMM inasan Cronus case, which indicates there is some other quality in our data.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now 1/3 model we've explored which is a multi stream DBN model so this fits well.",
                    "label": 1
                },
                {
                    "sent": "After the last talk.",
                    "label": 0
                },
                {
                    "sent": "And this is in hierarchical approach.",
                    "label": 1
                },
                {
                    "sent": "You see on the bottom we have independent modality processing.",
                    "label": 1
                },
                {
                    "sent": "So these are the input nodes.",
                    "label": 0
                },
                {
                    "sent": "In dark blue here and then, in the next level there is a state space decomposition.",
                    "label": 0
                },
                {
                    "sent": "These are the green nodes and they are trained unsupervised and then they are summarized in the action nodes a that are actually the actual classes we like to detect.",
                    "label": 0
                },
                {
                    "sent": "This would already be.",
                    "label": 1
                },
                {
                    "sent": "And finalized model yet to improve the state duration modeling to prevent the model from jumping from state to state to different actual classes.",
                    "label": 0
                },
                {
                    "sent": "There is a counter structure that improves the state duration modeling.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And again, some short results.",
                    "label": 0
                },
                {
                    "sent": "On the left hand side you see different features in apps.",
                    "label": 0
                },
                {
                    "sent": "I haven't mentioned this during the features.",
                    "label": 0
                },
                {
                    "sent": "We have three different features that apps you see, just the standard HM, then a multi stream approach and the margin stream approaches the counter structure.",
                    "label": 0
                },
                {
                    "sent": "And for the results you can see.",
                    "label": 0
                },
                {
                    "sent": "We couldn't get quite a heavy improvement if you compare 92% to 21%, that is quite heavily.",
                    "label": 0
                },
                {
                    "sent": "And especially one feature set up PDF feature setups.",
                    "label": 0
                },
                {
                    "sent": "Benefits quite a lot from the counter structure.",
                    "label": 0
                },
                {
                    "sent": "Well, I finally the best recognition rate can be achieved with the edenburg feature set up in the multi stream approach.",
                    "label": 0
                },
                {
                    "sent": "This is because this has been developed at Edenburg and they have tuned it to their feature set.",
                    "label": 0
                },
                {
                    "sent": "Should mention.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And finally, we also developed something that can be occlusions and disturbances in the in the data an we developed a model that can handle such occlusions and disturbances.",
                    "label": 0
                },
                {
                    "sent": "This fits well into the last talk.",
                    "label": 0
                },
                {
                    "sent": "Again we couple and hmm with linear dynamical system and we use the Hmm's driving input for the LDS and those we can compensate disturbance system.",
                    "label": 0
                },
                {
                    "sent": "You can see it on the right hand side.",
                    "label": 0
                },
                {
                    "sent": "We can represent this as a dynamic Bayesian network.",
                    "label": 0
                },
                {
                    "sent": "So we have a multi stream hmm on the top and linear dynamical system on the bottom and they are coupled time goes from left to right.",
                    "label": 0
                },
                {
                    "sent": "And if we do this, we can improve our recognition results for disturbed data.",
                    "label": 0
                },
                {
                    "sent": "So in this case higher numbers are better.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we haven't done segmentation, you're just classification, yet segmentation would be possible.",
                    "label": 0
                },
                {
                    "sent": "So first we found the same.",
                    "label": 0
                },
                {
                    "sent": "The individual.",
                    "label": 0
                },
                {
                    "sent": "Features audio area and video stream are always worse than using a multimodal approach.",
                    "label": 0
                },
                {
                    "sent": "I said we found this for all models.",
                    "label": 0
                },
                {
                    "sent": "Then if you see what happens if we just drop the Lake with stream.",
                    "label": 0
                },
                {
                    "sent": "So we add some background bubble, you can see that for example, the audio only single model HMM drops quite heavily in the recognition rate by more than 20%, while the multimodal systems keep up quite high with recognition rate and especially the mixed state DBN.",
                    "label": 0
                },
                {
                    "sent": "Nearly with this, nothing is less than 1% compared to the HMM.",
                    "label": 0
                },
                {
                    "sent": "Well, and finally, if you see the same, we have to stop all streams.",
                    "label": 0
                },
                {
                    "sent": "The visual streams with some black bars and the label with some background bubble.",
                    "label": 0
                },
                {
                    "sent": "You see that we still we're still better with our approach using these improvement with the LDS compared to a standard HMM.",
                    "label": 0
                },
                {
                    "sent": "OK to summarize.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This thing I know this was very quick because I had to put in quite a lot in one short talk.",
                    "label": 0
                },
                {
                    "sent": "This was a joint work of three institutes within the ME Project and we modeled meeting as a sequence of individual and group actions.",
                    "label": 1
                },
                {
                    "sent": "And here in this special case we have taken the group action discussion phases.",
                    "label": 0
                },
                {
                    "sent": "Yet the models can easily be adapted to all kind of sets.",
                    "label": 0
                },
                {
                    "sent": "Will actually do to model meetings.",
                    "label": 1
                },
                {
                    "sent": "We have derived large number of audiovisual features.",
                    "label": 0
                },
                {
                    "sent": "We've tested various combinations of these.",
                    "label": 0
                },
                {
                    "sent": "We've come up with four different group action modeling frameworks I've explained to you roughly, and you've seen all of them have a good performance yet there's still much space for improvements both in the feature domain and in the model domain, especially if you like to enlarge the set of action classes.",
                    "label": 1
                },
                {
                    "sent": "And if you like to go to layout approaches, those with the mediator, we are now going to to investigate combinations of the proposed systems to use the complementary strengths.",
                    "label": 0
                },
                {
                    "sent": "Thanks for your attention.",
                    "label": 0
                },
                {
                    "sent": "Time for a number of questions.",
                    "label": 0
                },
                {
                    "sent": "Preston.",
                    "label": 0
                },
                {
                    "sent": "Question for you yeah.",
                    "label": 0
                },
                {
                    "sent": "You mentioned that you were using his orientation.",
                    "label": 0
                },
                {
                    "sent": "Your feature vector slides.",
                    "label": 0
                },
                {
                    "sent": "I yes, that's right, this has not been real.",
                    "label": 0
                },
                {
                    "sent": "Part of this work, but it's basically we're using GMM models to extract skin color and then skin blocks and then head orientation is derived from there.",
                    "label": 0
                },
                {
                    "sent": "But please don't ask me how executives has been worked because this was not part of the work.",
                    "label": 0
                },
                {
                    "sent": "Well, I have a second ship.",
                    "label": 0
                },
                {
                    "sent": "It was all an 88 classes right?",
                    "label": 0
                },
                {
                    "sent": "Yes 8 or 14 depending on which action lexicon.",
                    "label": 0
                },
                {
                    "sent": "The classes are.",
                    "label": 0
                },
                {
                    "sent": "Could you have two activities?",
                    "label": 0
                },
                {
                    "sent": "The active at the same time in your data?",
                    "label": 0
                },
                {
                    "sent": "I guess we can't come up with a slight again.",
                    "label": 0
                },
                {
                    "sent": "OK, I can explain well the two actual lexicon and the first one you have 8 action classes.",
                    "label": 0
                },
                {
                    "sent": "They are disjunct so you have somebody talking as a monologue or giving a presentation or discussion.",
                    "label": 0
                },
                {
                    "sent": "Yet with the second X actual lexicon you also have combinations like for example I'm speaking and people take notes at the same time, so it depends on the actual lexicon.",
                    "label": 0
                },
                {
                    "sent": "And in a future, that is what I mentioned for the approach we're going more to investigate these combinations, because obviously people act while somebody is acting at the same time.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you again.",
                    "label": 0
                }
            ]
        }
    }
}