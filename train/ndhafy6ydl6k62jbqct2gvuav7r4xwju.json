{
    "id": "ndhafy6ydl6k62jbqct2gvuav7r4xwju",
    "title": "Open Information Extraction from the Web",
    "info": {
        "author": [
            "Oren Etzioni, Allen Institute for Artificial Intelligence (AI2)"
        ],
        "recorded by": [
            "AKBC-WEKEX"
        ],
        "published": "July 13, 2012",
        "recorded": "June 2012",
        "category": [
            "Top->Computer Science->Information Extraction",
            "Top->Computer Science->Natural Language Processing",
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Text Mining"
        ]
    },
    "url": "http://videolectures.net/akbcwekex2012_etzioni_information_extraction/",
    "segmentation": [
        [
            "I appreciate being here, and I volunteered to be the opening act with the idea of injecting a note of controversy and maybe even a note of humor just to get us going on a nice informal note for the workshop."
        ],
        [
            "Before I do that, I wanted to thank my collaborators.",
            "There's about 10 or 11 of us in the know it all research group right now, and several folks here.",
            "And then there's six years deal on.",
            "They have escaped over the years.",
            "Also wanted to thank our sponsors, funders, Dark by Arpina, cell phone number, and Google has been a great supporter of the project.",
            "So here's what I'm going to do this morning."
        ],
        [
            "I'm going to start with an unabashedly scruffy view of machine reading, and then I'll give a quick overview of open A. I think most of you are familiar with it.",
            "I'll discuss some of our recent progress, and then if the gods of the Internet cooperate, I'll show you our brand new demo.",
            "That's as it's worldwide opening in a few in a few minutes.",
            "I'll then get into a prevalent.",
            "Critique of open IE that I've heard over the years and which is completely miss founded and I'll explain to you in detail.",
            "While it is so, and I'll end with some discussion of future work for open a open is going to become more open than ever with something we're calling open open IE.",
            "So that's the plan.",
            "When I started to put together this."
        ],
        [
            "Like I went back to our AAA 06 paper and pull out a few quotes on machine reading that I think still capture how we think about it today.",
            "It's machinery is an exploratory open ended serendipitous process in contrast with many NLP tasks.",
            "Machine reading is inherently unsupervised, and the point here is that when I think of machine reading, let's say picking up the newspaper in the morning or.",
            "Processing the last you know, 100 million tweets.",
            "I don't think of it as somebody looking over your shoulder and say, OK, I want you to find instances of this concept and I want you to find evidence for this predicate.",
            "It's much more exploratory, much more serendipitous.",
            "You encounter new ideas, new concepts, new arguments all the time, and in our work, cramming to start to capture the feeling of that process, obviously.",
            "We're interested in very large scale.",
            "If you want to author a small amount of knowledge, I recommend you do it by hand.",
            "Is not the way to do it, so I think the game for machine reading is being at large scale and it's very much even back then.",
            "He said not about just what's there, explicitly extracted, but it's information that's implicit where you can generalize based on the extracted assertions.",
            "So those are some of the ideas back then.",
            "Still believe in them.",
            "One of the problems is that if you read through that, you'll find that there's no notion of ontology.",
            "Nothing like that in there and and again over the years.",
            "As I was working on open A which is our instantiation machine reading I became increasingly chagrined physically.",
            "Why does everybody else have an ontology but but we don't?",
            "And I was getting kind of an adult dreams about this and I was reminded of of a story about a guy, let's call him Joe and Joe's 30 year old guy and he has.",
            "An awkward problem.",
            "It's embarrassing.",
            "He's 30 years old and he still wets his bed.",
            "And you know, because increasingly depressed about this.",
            "Finally, he runs into a friend who sees him, you know, so down, and he says, Joe, look, you gotta go see a therapist to deal with this.",
            "So Joe goes to see a therapist.",
            "He goes into intensive therapy.",
            "He runs into his friend three weeks later and Joe is upbeat.",
            "He's happy smiling and his friend said, well, did the therapist help you know with the with the bed win?",
            "And just says no.",
            "But now I'm proud of it so.",
            "So after being chagrined about the lack of ontology in an opening for awhile, I realized that actually what we are is."
        ],
        [
            "We're not laughing, then tells you were entology free and now I'm proud of it.",
            "So what, why?",
            "Why am I proud of it?",
            "But really, there's lessons from decades of research on."
        ],
        [
            "Databases, knowledge representation, semantic heterogeneous database integration.",
            "Really a lot of stuff I got from my good friend along Halevi and basically creating declarative knowledge representations is extremely hard, difficult, error prone, slow putting formal semantics on top of them is again great at small scale, but realistically it's extremely hard to do with a broad scope, extremely hard to do if you have distributed.",
            "Authorship, and so it's a great aspiration, but it's very, very difficult to achieve the way alone.",
            "Put it in a paper back in 2003.",
            "Candies that we build their brutal and they can only be used for tasks whose knowledge needs have been anticipated in advance, right?",
            "So another challenge with ontology, right is that the knowledge is often very implicit ways structured for a particular use.",
            "And when you try to use it in a different way, things don't work nearly as well and there's a lot more here, right?",
            "People have given endless talks on this topic and decades of research have gone in to putting together knowledge bases in this way, and at the very least.",
            "It's extremely hard, right?",
            "This is the the field of database integration.",
            "Now if the."
        ],
        [
            "The basis for knowledge base you're trying to put together from distributed sources isn't the collection of 510 or 100 databases, but you're actually trying to put together put together the knowledge base from texts, and this text is authored by thousands, millions of people.",
            "It's not much hard.",
            "Right, so I think that the notion of ontology."
        ],
        [
            "Here is is at the very least, very challenging, and frankly, I believe a universal ontology.",
            "One of the covers everything in any kind of ambitious sense is impossible.",
            "I think that consistency over this non existent universal ontology is is like World Peace and consistency.",
            "I mean a logical consistency or even a probabilistic notion where we develop a coherent probabilistic model over that.",
            "Doing that at scale is like World Peace, OK?",
            "It's something that we all want, but we're just not going to see it in our lifetime.",
            "Now, what about Micron technologies?",
            "Is that is that possible and the answer is absolutely right.",
            "We see examples of those.",
            "The challenge with Micron Technologies is scaling them interconnecting right?",
            "So my chronologies is 1 approach but it has its own challenges.",
            "Another thing that I really worry about is what I called the uncle logical glass ceiling.",
            "So if you have some ontology, even if it's ambitious as.",
            "Freebase or Google's knowledge graph.",
            "Things equivalent whenever you have a limited vocabulary, particularly around a predetermined set of predicates, you have a problem, right?",
            "There's a whole bunch of things that you can't express, and if you're reading at any nontrivial scale and broad scope, your ontology is going to be swamped by what you're reading, right?",
            "So we're able to read extremely fast.",
            "Our technology process is on the order of hundreds of sentences per second.",
            "Right, if you have multiple machines right?",
            "We can easily read large chunks of the web corpus.",
            "And how does that fit with with a small ontology.",
            "So all these are our issues for.",
            "That kind of more formal approach."
        ],
        [
            "So in response to these challenges, and by the way, I don't claim to have all the answers we have our approach and it has its own challenges which I'll talk about some and we're happy to discuss more.",
            "My point is that these challenges motivated us to formulate open IE where.",
            "Open a looks as follows and again, the best way to understand it is in contrast with more traditional IE.",
            "In more traditionally you have as input a corpus and then you have hand labeled data per relation.",
            "You have a million relations, then you need.",
            "To label a million concepts.",
            "Whereas an opening, we start with the concept and no label data on a per relation basis.",
            "Obviously traditionally the relations have to be specified in advance.",
            "Otherwise how do you know what concepts the labeled data for in opening these are discovered automatically, right?",
            "And this is key to the serendipity and extensibility of open aid.",
            "And then the extractor does produce in.",
            "Traditionally is a relation specific extractor right 14.",
            "Management succession one for this element of nutrition, etc etc.",
            "Whereas ours is relation independent.",
            "We're really trying to capture actually certain things about relationships in language and use that to do the extraction and I'll talk more about that in a second.",
            "So really the question comes up OK if you're going to do this, how is that even possible in any nontrivial way?"
        ],
        [
            "And it turns out that it's possible because we believe, and this is a hypothesis we articulated awhile ago, but now have years of evidence for there is a subset of English that's a relatively easy to understand, and it goes way beyond extraction.",
            "But for the purposes of today, let's just say that there's a subset of English where we're able to characterize relations and their arguments syntactically.",
            "OK, so that's without having to learn lexical notions that are specific to particular relation or particular domain.",
            "We've actually had a series of papers characterize relations in arguments and shown that the characterization is a compact, be domain independent, and empirically based on a large sample of sentence is we've shown that our characterization covers 85% of binary verb based relations, and in more recent work we're extending that to nouns.",
            "Adjectives and so on.",
            "So this is the key right?",
            "And the basic idea here is very simple.",
            "Is there really such a difference between talking about company acquisitions and talking about which foods contain vitamin C?",
            "Well, obviously there's a difference in the lexemes, but."
        ],
        [
            "Tactically they are quite similar to give you the intuition here without going into the complex expressions that are in the paper.",
            "Here's a sample of some simple relation phrases and you quickly see the across a huge range of topics of verb can do a great job of capturing a relationship, invented, downloaded, what have you.",
            "The verb could be followed by preposition, acquired by voted for, born in.",
            "Or maybe there's some intervening gun now is determined as etc is in this.",
            "Light verb constructions has a PhD in inhibits tumor growth in mastered the art of so again, completely disparate domains, but very economical way of expressing information.",
            "And this is what open any hooks onto to do its extraction from arbitrary senses, arbitrary domains.",
            "What this buys us."
        ],
        [
            "As.",
            "Is the ability to operate over much larger sets of relations than in previous things, so I apologize if your system is not listed here or if I got the number of relations off by some amount right?",
            "All of these are dynamic projects and these are moving targets, but the point that I'm making is that our systems are doing two to three orders of magnitude more relations relative to previous work.",
            "There's a catch though.",
            "When I say relations.",
            "Maybe a more appropriate term would be relation phrases, right?",
            "So we capture the phrases in the language that denote relations.",
            "What we don't do is map them to an ontology, right?",
            "We don't normalize them, at least not yet.",
            "OK, so there's definitely much bigger, but it's also a weaker notion of relation, OK?"
        ],
        [
            "So where, where are we going with that in 2007 we feel the text running was, which was the first web scale open.",
            "A system used ideas like distant supervision to generate a model, training model of language, then developed a CRF model of relations and our notion of relations.",
            "Rules of the forum argument.",
            "Wanna relation phrase?",
            "An argument to were able to generate a billion distinct extractions?",
            "And do so with nontrivial levels of precision over the year."
        ],
        [
            "As we and others have been consistently improving the area under the precision recall curve.",
            "Most recent system.",
            "Here is a reverb and I'll talk to you about an even more impressive system called Holly in a few minutes, but the general trend is up, which is which is not surprising."
        ],
        [
            "What else have we've done?",
            "We've made an open source version of the extractor and I was pleased to see a number of the papers in the workshop using that and making it better.",
            "We've looked into synonym detection and we've also looked into much more intensive analysis of language using parsers using relations that aren't just based on verbs.",
            "Using the context of the relation.",
            "And again, today we haven't seen something that is inconsistent with our semantic tractability hypothesis.",
            "On the contrary, what we see is that whatever linguistic phenomena we're interested in, we're able to identify a semantically tractable corner, and then often bootstrap from there to have a more general notion of language.",
            "So, for example, all E takes as input.",
            "Millions of high quality extractions from reverb and then uses that to generalize."
        ],
        [
            "And let me just give you a couple of examples.",
            "So we go from the simple verbas relations to being able to handle sentence is like after beating the heat.",
            "the Celtics, another top dog in the NBA.",
            "Little prediction embedded in there so we can get the Celtics beat the heat."
        ],
        [
            "Or if we have the sentence, if he wins five key states, Romney will be president.",
            "That's not a prediction.",
            "That's a counter factual.",
            "And again, we're able to capture that counterfactual at least as a phrase.",
            "If he wins 5 States and this is a paper that's going to appear in email P. 12, So the sophistication of extraction from language is a major focus for us.",
            "We're continuing to push really hard on getting more and more from individual sentence is.",
            "Sentence is in combination information that's explicit information that's implicit, but the key is to do this based on domain independent regularity's in language not to do this based on lexical patterns, simply because the structure is there.",
            "So you want to exploit that before you move on to more domain specific stuff.",
            "OK, so I hope you're convinced that this is a valid direction of research, and I encourage you to read the papers for the details.",
            "It still raises the question of OK, what about entities types, ontologies, and all that good stuff.",
            "So what, in fact we've done is."
        ],
        [
            "Rather than just a proud of being ontology free, we've started moving in the direction of Anthologizing Open A and the first step, and this is the work of Tomlin who's who's here and has a paper on this.",
            "At the workshop we started to link the extraction arguments to Freebase when possible, and nothing particularly magical about Freebase.",
            "That's what we had at hand when possible.",
            "Part is really key here.",
            "'cause we're achieving graceful degradation, right?",
            "If you can link an argument, Freebase, great, but we don't want to restrict the extraction, so only what's in the ontology right?",
            "That would suffer from all the that would suffer from the ontological glass ceiling that I talked about, right?",
            "So when you can't link, we don't, and the system operates just fine when we do have, those links were able to associate types with the arguments and I'll show you a demo of that in a second and then in.",
            "Another paper by Tom that's coming out later this summer.",
            "He wrote a paper known noun phrase left behind where he can detect arguments that aren't in Freebase and figure out their their types by generalizing from the freebase arguments right?",
            "So again, we're able to carry over some of the semantic information that's in Freebase.",
            "Two things that aren't aren't there that are not in Freebase and again therefore extend this notion of types beyond what's in the original.",
            "Colleges."
        ],
        [
            "So with that in mind, here's what our system architecture looks like.",
            "We start with the web corpus.",
            "We run an extractor.",
            "This used to be text runner, then re verb.",
            "Then all the the demo I'm going to show you is based on reverb, but we're continually improving this piece.",
            "As I mentioned, we get out of that raw topples.",
            "We do a number of processes over them like synonym detection like counting and detecting redundancy.",
            "We do deduplication.",
            "All that allows us to, so to assess our confidence in the extractions.",
            "Then we come up with extraction sets.",
            "We do the entity linking of the arguments to Freebase and we get something that looks like this and you see the bolded arguments like Albert Einstein, other ones that have a link, and that's the Convention will use some demo ensuring a second and once we've gone through this process now we have a query processor.",
            "That person can use, so let me.",
            "Show you what that looks like.",
            "And maybe before I start, let me just.",
            "Give you my favorite example query, but I'll start by showing you what it looks like on on Google.",
            "So let's say I want to answer the question what kills bacteria.",
            "If I go to Google, let's take a minute.",
            "With this I see a bunch of snippets from various answer sites.",
            "As you can see, I've clicked on some of these so you gotta wiki answers and you find a fairly decent answer.",
            "Also some some problems.",
            "But antibiotics, antiseptics, bleach disinfected OK. That's a pretty good set of answers if I click.",
            "Let's see if I click on answers from s.com, what are they going to tell us?",
            "I'll see a bunch of ads and then I'll see some obscure substance.",
            "OK, so I think if I spent an hour here I could come up with a pretty comprehensive set of answers to this question.",
            "If you go to open information extraction.",
            "By the way, this demo is generally available at openai.cs.washington.edu.",
            "We have links here to some key papers and also to our.",
            "Textual resources that I'll talk about later, but we have that one location for all your open information extraction needs anyway.",
            "So what you have here is an interface that allows you to query by argument, query by relation or or combinations thereof.",
            "So, and we have a simple ability to map from questions to these sufe say what kills bacteria in Maps it to a question of query.",
            "The formulation is killed argument to the bacterium.",
            "We don't know what argument one is.",
            "I press search, this is cashed and so other queries would definitely take longer and you see a pretty comprehensive set of answers.",
            "We have more and more here as you go, and it's distill this down to an answer the the number you see next to the answer is the number of sentences this is based on.",
            "I can click on it and bring up the sentence is we have provenance right?",
            "I can click through to the underlying URL and see where it comes from.",
            "And the corpus we used here is a combination of 500 million web pages that Google kindly provided us as well as we've recently run it on Clue Web as well.",
            "And what you're seeing here is based on the combination now what's nice here is that if I look at a particular place, if I hover over that, I can see here.",
            "Let's take this one.",
            "I can see the freebase information about this, the various types.",
            "I can see the various synonyms that we've extracted right?",
            "We sub vinegar.",
            "38 times etc, but also in cases where I failed to do that like high temperatures is not something that's in Freebase.",
            "The system works just fine and I'll at least get the extracted synonyms.",
            "Another nice thing is I have the benefit of.",
            "Freebase is type system so if I want to focus on let's say I want to look at foods that kill bacteria I could click on that see if I successfully click there.",
            "Oh, I did write an icy various foods, and I obviously I also suffer right from whatever idiosyncrasy's are in anthology.",
            "For example, I see book subject a lot, which is not particularly meaningful in this context, so you get a sense for how this works.",
            "Let me just show you one more query to give you a sense of this.",
            "Another thing that we can do is do type queries, so again.",
            "We're leveraging the type systems if that's in Freebase.",
            "If I want to know what actor starred in which films, I'll say type: actor starred in as the relation type: film.",
            "I'll search and again I get a whole variety of answers from a whole bunch of sentence is from Bruce Lee in Enter the dragon to Heath Ledger Anson's Son and I'll just show you my favorite examples days when I ask what is the symbol of which country?",
            "So again relation is symbol of.",
            "Type is country.",
            "I live the first argument blank and I get a wide variety of answers.",
            "Chinese dragon.",
            "I didn't know that leak, for example, was the symbol of Wales or the milk Thistle for Scotland.",
            "And again all these have their links and I have the Uri that I can I can pursue etc etc.",
            "So this is the current state of open information extraction at at large scale.",
            "I would have to take a deep breath and a sip after you did over the web demo and it actually works.",
            "Bread.",
            "So."
        ],
        [
            "OK, so that's that's the good news.",
            "If you will, what's the bad news?",
            "Well, we don't have a formal ontology, as I've been emphasizing over Cabul airy.",
            "We have a lot of inconsistent extractions, right?",
            "I showed you some of our best examples.",
            "If you play with our demo, you'll easily find much worse examples.",
            "We have extraction errors, we have inconsistencies.",
            "So can this hodgepodge of structured textual stuff?",
            "Can IT support recently?",
            "And if it can't, what's the point of open a?",
            "Aren't you just playing games with words on the page?",
            "And I have several answers to these, but let me just immediately address that one.",
            "It turns out that open a can support reason.",
            "We do generalization.",
            "We do reasoning on top of this, and the point of it will get to in a second.",
            "But absolutely we do not need an ontology to do reason, it's just it's a different kind of reasoning.",
            "We get that in a second, so."
        ],
        [
            "In response to these questions, which are fair questions, I thought of three key answers.",
            "What's the point of opening one is?",
            "I do think that it suggests a direction for search, at least for certain kinds of search too.",
            "We're releasing a number of textual resources that are based on open A and I'll list those in a minute and then I'll talk to you about several efforts that we have for reasoning over extractions.",
            "Generated through this process so."
        ],
        [
            "I think that there is a potential for a new paradigm for search that goes way beyond the Google Knowledge Graph and so on.",
            "And I called it a long time ago, moving up the information food chain and the idea is really to move from information retrieval to information extraction to move from snippets and documents to entities and relations and Google Knowledge Graph is an important step in that direction, but also to move from keyword queries to questions and from list of documents to answers.",
            "And you saw a simple example of that in the open a demo.",
            "You know, if you take out your iPhone, you can access the open a demo and it actually works surprisingly well again, for a non commercial prototypes because we take a huge amount of information and crystallize it into succinct answers.",
            "So really think of this ideas as Siri makes Watson, we want to have the sophistication of Lawson of analyzing enormous amount of text, but we want to have it available via Siri like interface.",
            "That would be a search engine that I'd like to use from my from my iPhone.",
            "OK, and I think we're taking steps in that direction."
        ],
        [
            "We show you a case study we did over Yelp reviews.",
            "Well we did is.",
            "We took several 100,000 Yelp reviews only for restaurants in Seattle.",
            "I would love to show this for you in the context Of Montreal, but we didn't get access to that data.",
            "We mapped the review corpus to a set of attribute values and we did this automatically with very minimal hand labeling of data.",
            "So we can tell that a review says the sushi is fresh.",
            "In his free etc etc.",
            "We then allow natural language queries like well.",
            "Where is the best sushi in Seattle?",
            "And we answer that not with a literal mesh, right?",
            "Information retrieval would look for the phrase best sushi.",
            "We answer that by looking at the attributes, sushi and various synonyms for it and doing sentiment analysis on the values that get returned so we can figure out that exquisite is better than very good.",
            "Very good is better than so, so and."
        ],
        [
            "As a result, we're able to answer questions like that and able to compress the information into something that fits on a mobile phone.",
            "So this is actually our we have this both on iPhone and we have an Android app you can see here about the restaurant serious Pie in Seattle.",
            "You have the extracted attributes and it automatically figures out the things like crust is particularly important, and then again very similar.",
            "You'll see values you will see how many times you saw those.",
            "You can click through.",
            "To get the sentence and the actual reviews, we do the sentiment analysis using color.",
            "So green is positive, red is negative and we played with a variety of of interfaces.",
            "The point is, instead of spending hours reading reviews which you really do not want to do on your smartphone, this extractive interface gives you a concise sum."
        ],
        [
            "That's highly actionable.",
            "This is available, or if minor.com we have a desktop interface you want to play with and the Android app is downloadable.",
            "So if you're ever in Seattle, which some of you are there frequently, it's better you should be finding your restaurants using Revel minor.com."
        ],
        [
            "OK.",
            "So second point, what resources can we create leveraging open 81 that we're unveiling at the workshop today and is what we call rella Grams?",
            "It's a probabilistic model of relations and text and their sequences, right?",
            "That's very similar to integration.",
            "Anagrams are probabilistic model over characters or words.",
            "Here we have these models over relations.",
            "We have a database with 94 million rella grams that's available.",
            "And Steven will be Stephen Siren will be talking about that in a later in Manning."
        ],
        [
            "His poster, that kind of thing you'll see is things like, well police, investigate X.",
            "Then it turns out that it's often followed by they charge YX&Y.",
            "Here are variables and not necessarily the same variable.",
            "Anas Tom Mitchell pointed out at dinner last night.",
            "This is actually a good thing.",
            "I was like, why is that a good thing?",
            "But well, that means that the investigation had some value, right?",
            "If it always charge the same person they investigated, or if they always did, then what's the point of?",
            "Of the investigation.",
            "Actually in some countries, right?",
            "That's not the case.",
            "So so here we learned an important role.",
            "In fact, from this kind of narrative structure, and in fact we're very interested in using that in extraction and using that in a number of ways and using that to to create rich structures that describe events and so on so."
        ],
        [
            "We also released 600,000 of our favorite relation phrases.",
            "If people want to play with a rich set of relationships, is we generated a bunch of metadata over relations.",
            "For example, the domain and range for 50,000 relations, 10,000 functional relations, 30,000 Learn horn clauses, all these are available at that website.",
            "Open a.",
            "Is the Washington Edu and one of the most impressive resources is coming soon and this is the work of Jonathan Brandt who's a student of E to the gun in Israel.",
            "Jonathan came to visit us and got excited about the work and he's built a resource that he calls clean.",
            "It's 10 million 10,000,000 entailment rules, very similar to the ones you find in dirt, but the precision, According to him, is double that of dirt.",
            "Using using their metrics, so that's coming soon and I think should be an exciting resource.",
            "So second use of opening."
        ],
        [
            "No, Thirdly to go to the point about reasoning over extractions, we've had a number of efforts and I'll just go through them in a little bit just to give you the flavor of them and to show you the different kinds of reasoning you can do so.",
            "I already talked about how we map arguments into our we mapped extracting arguments into Freebase entities.",
            "I won't go into that war.",
            "We also identify synonyms automatically, both synonymous arguments, anonymous relations.",
            "We were able to do linear time 1st order form clause inference and we're also able to learn types via generative model."
        ],
        [
            "So how do we do synonym detection?",
            "We have an unsupervised probabilistic model and the basic idea is extremely simple.",
            "If we want to decide what's the probability that Bill Clinton is the same as President Clinton, well, we're going to count how many relations and our twos these two share.",
            "If we wanted to side that acquired and bought are synonymous relations, we're going to count how many argument pairs they share, and that gets us along that direction.",
            "There's an obvious.",
            "Mutual recursion here, right?",
            "Between this, so we can go back and forth.",
            "And obviously, when you're counting shared relations, you might want to wait functions more heavily, right?",
            "So if I and somebody else are married to the same person, that's more important than the fact that we both visited France, right?",
            "So not all relations are created equal, and this is part of Alex uses thesis and.",
            "This is done some nice work there.",
            "We've also now looked at OK as you start to unify with Freebase and you don't just have textual arguments.",
            "You can do this even even better.",
            "Unscalable"
        ],
        [
            "Actual inference, again, I'll just give you a real hit here in text.",
            "That kind of inference you want to do is probabilistic, and if you're dealing with web scale corpora, it has to be linear time in the size of the corpus, right?",
            "These are massive massive corpora, and in fact we've identified a property of argument distributions of textual relations that actually gives us both of these.",
            "The inference is actually probably linear, and even more important, because you know the proof.",
            "Sometimes smuggle in a few assumptions.",
            "Here in there, we actually did a series."
        ],
        [
            "Experiments with a system called homes, where we showed that as we scale over larger and larger corpora going to hundreds of millions of pages, the inference process remains linear time so."
        ],
        [
            "So another example and then Lastly thinking about mapping from extractions to the domain and range of these relations.",
            "There's been extensive previous work on this problem.",
            "It's called Selectional preferences.",
            "It's called various things like that, and we've been able to utilize generative models to do this.",
            "And the basic idea is we take the instances of extractions of relation.",
            "We model those as a document and then we think of the domain and range as topics.",
            "Here we're understand."
        ],
        [
            "Weird, so here's relation.",
            "Viewed as a document, right?",
            "We sort our extractions by topic and then we're able to find regularity's in the argument sets as well as constraints between different relate."
        ],
        [
            "Oceans we run a generative model won't go through into the details."
        ],
        [
            "Here's the sort of thing we extract.",
            "Right elect country tends to elect the person.",
            "Experts predict events, people download software, etc etc.",
            "Again, obviously these are the best cases, but we've made these available on the web free to play with and for people to download and use."
        ],
        [
            "So quick summary.",
            "I've gone through a lot of stuff rather quickly.",
            "Here's the summary of the trajectory of the project over the last nine years and really focused on the last five.",
            "We start out with text Runner and a billion ontology free extractions.",
            "We've worked on inference over the extractions and getting metadata from there.",
            "We've released open source extractors.",
            "We've released public textual resources and recently we have been busy on both ontology Ising it.",
            "By by intersecting.",
            "Open a with freebase.",
            "Undoing deeper analysis of the sentence is as an oly and much more work we're doing now to get more implicit information out of the sentences, and also on I eBay search.",
            "I think very practically there's a lot more."
        ],
        [
            "Or to do there.",
            "So to end, I want to leave room for questions.",
            "Where are we going?",
            "Again a number of directions, but when I want to emphasize is becoming more open, one of the things that we realized is that our approach is monolithic, right?",
            "We have our extractor and we generate couples based on that, but because we're so relatively lightweight, it's very easy for us to adjust couples from any source.",
            "So if you give me a tuple, some notion of confidence and again we can normalize this.",
            "If your notion of confidence.",
            "Is different than ours.",
            "Anna source.",
            "We can start ingesting those couples.",
            "It creates some interesting algorithmic problems.",
            "How do you surface that in our system?",
            "But something that I really want to do is take nails, or somebody else is couples and just bring them into our system, open input and then the second thing is I'm very enamored with what's happening in the linked open data community.",
            "I feel like they've gotten the model of distribution of authorship right.",
            "They have some other things that are more challenging, like they don't have a notion of uncertainty yet, but I think linking our extractions into VLD cloud is something that we're going to do and that's going to require us to think hard about relation normalization, something that we don't have results on yet, but I like here is that again.",
            "Instead of legislating something, they have the idea of using best practices, which is really all you can do if you want to do.",
            "Things at this level of scale and distribution and then specialize reasoners, which is really what I was showing you open a, uses specialized reasoners are going to operate over portions of this lovr club so."
        ],
        [
            "To conclude, I argued ontology is not necessary for reasoning, however open he's becoming gracefully antologi gracefully.",
            "I mean right not fully just wear wear appropriate were boosting our analysis of text, and I think that Elodie has distribution and scale but in a funny way tax if you look in the Elodie cloud right, they haven't incorporated the results of extraction by large and that's a great opportunity for us and for this community.",
            "Or Bud, thank you.",
            "We have some time for questions.",
            "Indoor DARPA.",
            "Yeah, I just was curious for I like this top when you at some point in their very inside about inferring implicit information, but a lot of what's going on, at least in examples that you presented, pivoted off of explicit information that was somehow conveyed.",
            "And so for, you know, for example, Ray Mooney's Idol presentation where he said John needed money.",
            "He called his parents.",
            "There are couple of relations there, including the parents having the money or.",
            "Potentially in the future, transferring the money with some probability and those are not explicitly represented in either of those two sentence is that the direction that you're headed in or use.",
            "Are you saying right now the ability for reasoning about type is already embedded in here.",
            "It is largely the direction we're heading rather than stuff that I demo right.",
            "The demo I showed it scale is very much about explicit information, but let me give you an example right if you think about relative grams right, generating expectations, right?",
            "Those are the sorts of things where you say you know what this wasn't mentioned in the text, and typically a textual description of an event will omit right many elements.",
            "However, based on our expectations, right?",
            "Based on the model of these kinds of events.",
            "That we've recovered from reading, you know, millions of stories like this, we're going to expect that assign some degree of belief to that, so that's future work.",
            "Holograms are here, and the implicit stuff is future, right?",
            "So if you're so yeah, so this is future.",
            "I'm stuffing stuff I have.",
            "I want to point out a few things about the use of ontology.",
            "The terminology I know that it's used in very ambiguous ways, and so the way you used it is also quite common, but it doesn't quite reflect the way this used in the semantic web community now.",
            "And I would say there are two things.",
            "One thing is to distinguish your apology as a conceptual model from the language you use.",
            "So if you replace your use of ontology by, let's say description logics right, then I would fully agree with what you said.",
            "That maybe you don't want to have all this kind of general purpose.",
            "Very expensive reasons.",
            "So I must also say there are also like lightweight and cheap description logic reasons that run polynomial time over.",
            "So but but I would rather agree when you replace or told you by description logics.",
            "A lot of what you do I would call from my perspective and my background was when I started as a PhD student to do information extraction using ontologies.",
            "I would allow what you do.",
            "Call ontology based information extraction or doing information extraction to extend the ontology right when you formalize what is the relation, what is the domain and range?",
            "From my point of view from my use of the word ontology that well.",
            "Construction.",
            "And Furthermore, last push.",
            "Ontology now is using.",
            "The semantic web is not that you have like this one big thing, that's everything need.",
            "But people really take as you showing this link data cloud where you have a different technologies and ontologies are interlinked.",
            "And that's a very important metric for, because it says, well, I cannot like capture everything that's too much for me.",
            "But what I can do is other people have worked on using or people or documents.",
            "And so I I use what other people have achieved in describing this domain that I'm interested in, and one of these domains actually is also text.",
            "Annotations only, last week's repeats many web conference.",
            "There were people talking about using this many web form.",
            "It's just like text annotation.",
            "Which is then just one possible limitation.",
            "I want one plus the application of semantic web forms, so I don't disagree with what you say, but I just want to point out that.",
            "The use of the word apology in this medical community is a bit different than what you, how you would use so.",
            "And I'm glad to hear is somebody who comes in our community that you see the opportunities for text and semantic web two to match up.",
            "The only other thing I would add is that since you said that we were doing ontology based extraction and I'm just deeply, deeply insulted by that.",
            "Instruction right.",
            "The picture you have in mind with ontology based information extraction is you have this kind of templates like Old Mac style right?",
            "And that you don't do I fully agree.",
            "So if you consider like Mac with the template S that is apology based information extraction, I would never say that this week.",
            "So I won't use the over by.",
            "Listening to to what you were saying on, I was trying to sort out.",
            "To what degree is what we saw based on text tokens and inference and manipulation on text tokens versus inference and manipulation of latent concepts like Albert Einstein the person?",
            "Who might be referred to by many pets tokens and you mentioned synonym finding.",
            "You didn't mention polysemy, but.",
            "Aside from the question of whether you want to start with an ontology or not, there's the question of.",
            "To what degree does the processing in the inferencing of the system.",
            "Occur at the level of text tokens versus at the level of some latent, not directly observed collection of concepts that you reason about.",
            "Yeah, that's a great question because I think that question is at the core of this absolutely, and what I would say is that.",
            "Obviously a lot of the work that we've done has been doing reasoning at the level of the text tokens, although again, if you look at the generative models that are latent variables that are the domain and range is right.",
            "Those are the topics related topics.",
            "However, they are not using formal tokens percent.",
            "It's also the case that I think if we had more formal tokens.",
            "There's good reason to believe that the inference would progress more smoothly effectively with higher precision, right?",
            "And that's why we've been moving in that direction.",
            "So I think is important is to not lose the textual richness.",
            "So I would say the goal of where we're headed at least, is to do inference based on a combination of both, as opposed to being restricted to one or the other.",
            "So I'll follow up on Tom's idea.",
            "What have you done to deal with polysemy so you know I say Microsoft bought farecast.",
            "That's one thing.",
            "If I say you know I bought Orange idea that open IE is a great idea.",
            "I mean, you know those are completely different notions of bot.",
            "How do you not just put together 2 words but tear them apart into sensitive?",
            "So so for example, right?",
            "We could link in the end of the link in case we allow linking.",
            "The token to different entities in Freebase in different contexts.",
            "So polysemy is a very real problem.",
            "I don't claim that we've solved it, but we're not ignoring it either, and we have mechanisms.",
            "Alright, one question.",
            "Read review.",
            "Maybe it's a Seattle.",
            "Sushi is an attribute of the restaurant, so.",
            "For a broader sense is how do you determine what are the attributes you supposed to extract?",
            "It's not as the other thing, the the approach that we've taken is that various dishes sooner traditions, you know, like the margaritas at a Mexican restaurant or whatever you are attributes of the entity, and then what people say about the more value.",
            "So the values are opinions, adjectives you know, like the margaritas here, very strong, and the attributes are aspects of the restaurants of parking and sushi.",
            "From our point of view, the same so you do.",
            "That's the beauty of it.",
            "We don't know that that is the key to the approach that we take, and that's why this stuff is related to what we have is an underlying model of language that's syntactic, and that tells us.",
            "Here's here's what we're looking for, but the system automatically figures out what are the attributes, and in fact we've run it.",
            "I showed it on restaurant.",
            "We've run another product categories, and itself figures out what are the things that people talk about, and that's really, really important, because you can start with an upper anthology.",
            "We say, well, you know.",
            "Laptop people are going to talk about the following five things, but in fact you finally talk about different things, so that's thanks for asking.",
            "That's the strength of this approach is serve certain.",
            "Definitely figuring out what it is that the attributes are in that can generate surprises.",
            "Alright, thank you very much.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I appreciate being here, and I volunteered to be the opening act with the idea of injecting a note of controversy and maybe even a note of humor just to get us going on a nice informal note for the workshop.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Before I do that, I wanted to thank my collaborators.",
                    "label": 0
                },
                {
                    "sent": "There's about 10 or 11 of us in the know it all research group right now, and several folks here.",
                    "label": 0
                },
                {
                    "sent": "And then there's six years deal on.",
                    "label": 0
                },
                {
                    "sent": "They have escaped over the years.",
                    "label": 0
                },
                {
                    "sent": "Also wanted to thank our sponsors, funders, Dark by Arpina, cell phone number, and Google has been a great supporter of the project.",
                    "label": 0
                },
                {
                    "sent": "So here's what I'm going to do this morning.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to start with an unabashedly scruffy view of machine reading, and then I'll give a quick overview of open A. I think most of you are familiar with it.",
                    "label": 1
                },
                {
                    "sent": "I'll discuss some of our recent progress, and then if the gods of the Internet cooperate, I'll show you our brand new demo.",
                    "label": 0
                },
                {
                    "sent": "That's as it's worldwide opening in a few in a few minutes.",
                    "label": 0
                },
                {
                    "sent": "I'll then get into a prevalent.",
                    "label": 0
                },
                {
                    "sent": "Critique of open IE that I've heard over the years and which is completely miss founded and I'll explain to you in detail.",
                    "label": 1
                },
                {
                    "sent": "While it is so, and I'll end with some discussion of future work for open a open is going to become more open than ever with something we're calling open open IE.",
                    "label": 0
                },
                {
                    "sent": "So that's the plan.",
                    "label": 0
                },
                {
                    "sent": "When I started to put together this.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like I went back to our AAA 06 paper and pull out a few quotes on machine reading that I think still capture how we think about it today.",
                    "label": 0
                },
                {
                    "sent": "It's machinery is an exploratory open ended serendipitous process in contrast with many NLP tasks.",
                    "label": 1
                },
                {
                    "sent": "Machine reading is inherently unsupervised, and the point here is that when I think of machine reading, let's say picking up the newspaper in the morning or.",
                    "label": 0
                },
                {
                    "sent": "Processing the last you know, 100 million tweets.",
                    "label": 0
                },
                {
                    "sent": "I don't think of it as somebody looking over your shoulder and say, OK, I want you to find instances of this concept and I want you to find evidence for this predicate.",
                    "label": 0
                },
                {
                    "sent": "It's much more exploratory, much more serendipitous.",
                    "label": 0
                },
                {
                    "sent": "You encounter new ideas, new concepts, new arguments all the time, and in our work, cramming to start to capture the feeling of that process, obviously.",
                    "label": 0
                },
                {
                    "sent": "We're interested in very large scale.",
                    "label": 0
                },
                {
                    "sent": "If you want to author a small amount of knowledge, I recommend you do it by hand.",
                    "label": 0
                },
                {
                    "sent": "Is not the way to do it, so I think the game for machine reading is being at large scale and it's very much even back then.",
                    "label": 0
                },
                {
                    "sent": "He said not about just what's there, explicitly extracted, but it's information that's implicit where you can generalize based on the extracted assertions.",
                    "label": 0
                },
                {
                    "sent": "So those are some of the ideas back then.",
                    "label": 0
                },
                {
                    "sent": "Still believe in them.",
                    "label": 0
                },
                {
                    "sent": "One of the problems is that if you read through that, you'll find that there's no notion of ontology.",
                    "label": 0
                },
                {
                    "sent": "Nothing like that in there and and again over the years.",
                    "label": 0
                },
                {
                    "sent": "As I was working on open A which is our instantiation machine reading I became increasingly chagrined physically.",
                    "label": 0
                },
                {
                    "sent": "Why does everybody else have an ontology but but we don't?",
                    "label": 0
                },
                {
                    "sent": "And I was getting kind of an adult dreams about this and I was reminded of of a story about a guy, let's call him Joe and Joe's 30 year old guy and he has.",
                    "label": 0
                },
                {
                    "sent": "An awkward problem.",
                    "label": 0
                },
                {
                    "sent": "It's embarrassing.",
                    "label": 0
                },
                {
                    "sent": "He's 30 years old and he still wets his bed.",
                    "label": 0
                },
                {
                    "sent": "And you know, because increasingly depressed about this.",
                    "label": 0
                },
                {
                    "sent": "Finally, he runs into a friend who sees him, you know, so down, and he says, Joe, look, you gotta go see a therapist to deal with this.",
                    "label": 0
                },
                {
                    "sent": "So Joe goes to see a therapist.",
                    "label": 0
                },
                {
                    "sent": "He goes into intensive therapy.",
                    "label": 0
                },
                {
                    "sent": "He runs into his friend three weeks later and Joe is upbeat.",
                    "label": 0
                },
                {
                    "sent": "He's happy smiling and his friend said, well, did the therapist help you know with the with the bed win?",
                    "label": 0
                },
                {
                    "sent": "And just says no.",
                    "label": 0
                },
                {
                    "sent": "But now I'm proud of it so.",
                    "label": 0
                },
                {
                    "sent": "So after being chagrined about the lack of ontology in an opening for awhile, I realized that actually what we are is.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're not laughing, then tells you were entology free and now I'm proud of it.",
                    "label": 0
                },
                {
                    "sent": "So what, why?",
                    "label": 0
                },
                {
                    "sent": "Why am I proud of it?",
                    "label": 0
                },
                {
                    "sent": "But really, there's lessons from decades of research on.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Databases, knowledge representation, semantic heterogeneous database integration.",
                    "label": 0
                },
                {
                    "sent": "Really a lot of stuff I got from my good friend along Halevi and basically creating declarative knowledge representations is extremely hard, difficult, error prone, slow putting formal semantics on top of them is again great at small scale, but realistically it's extremely hard to do with a broad scope, extremely hard to do if you have distributed.",
                    "label": 0
                },
                {
                    "sent": "Authorship, and so it's a great aspiration, but it's very, very difficult to achieve the way alone.",
                    "label": 0
                },
                {
                    "sent": "Put it in a paper back in 2003.",
                    "label": 0
                },
                {
                    "sent": "Candies that we build their brutal and they can only be used for tasks whose knowledge needs have been anticipated in advance, right?",
                    "label": 1
                },
                {
                    "sent": "So another challenge with ontology, right is that the knowledge is often very implicit ways structured for a particular use.",
                    "label": 0
                },
                {
                    "sent": "And when you try to use it in a different way, things don't work nearly as well and there's a lot more here, right?",
                    "label": 0
                },
                {
                    "sent": "People have given endless talks on this topic and decades of research have gone in to putting together knowledge bases in this way, and at the very least.",
                    "label": 0
                },
                {
                    "sent": "It's extremely hard, right?",
                    "label": 0
                },
                {
                    "sent": "This is the the field of database integration.",
                    "label": 0
                },
                {
                    "sent": "Now if the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The basis for knowledge base you're trying to put together from distributed sources isn't the collection of 510 or 100 databases, but you're actually trying to put together put together the knowledge base from texts, and this text is authored by thousands, millions of people.",
                    "label": 0
                },
                {
                    "sent": "It's not much hard.",
                    "label": 0
                },
                {
                    "sent": "Right, so I think that the notion of ontology.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here is is at the very least, very challenging, and frankly, I believe a universal ontology.",
                    "label": 0
                },
                {
                    "sent": "One of the covers everything in any kind of ambitious sense is impossible.",
                    "label": 0
                },
                {
                    "sent": "I think that consistency over this non existent universal ontology is is like World Peace and consistency.",
                    "label": 1
                },
                {
                    "sent": "I mean a logical consistency or even a probabilistic notion where we develop a coherent probabilistic model over that.",
                    "label": 1
                },
                {
                    "sent": "Doing that at scale is like World Peace, OK?",
                    "label": 0
                },
                {
                    "sent": "It's something that we all want, but we're just not going to see it in our lifetime.",
                    "label": 0
                },
                {
                    "sent": "Now, what about Micron technologies?",
                    "label": 0
                },
                {
                    "sent": "Is that is that possible and the answer is absolutely right.",
                    "label": 0
                },
                {
                    "sent": "We see examples of those.",
                    "label": 0
                },
                {
                    "sent": "The challenge with Micron Technologies is scaling them interconnecting right?",
                    "label": 0
                },
                {
                    "sent": "So my chronologies is 1 approach but it has its own challenges.",
                    "label": 0
                },
                {
                    "sent": "Another thing that I really worry about is what I called the uncle logical glass ceiling.",
                    "label": 0
                },
                {
                    "sent": "So if you have some ontology, even if it's ambitious as.",
                    "label": 0
                },
                {
                    "sent": "Freebase or Google's knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "Things equivalent whenever you have a limited vocabulary, particularly around a predetermined set of predicates, you have a problem, right?",
                    "label": 0
                },
                {
                    "sent": "There's a whole bunch of things that you can't express, and if you're reading at any nontrivial scale and broad scope, your ontology is going to be swamped by what you're reading, right?",
                    "label": 0
                },
                {
                    "sent": "So we're able to read extremely fast.",
                    "label": 0
                },
                {
                    "sent": "Our technology process is on the order of hundreds of sentences per second.",
                    "label": 0
                },
                {
                    "sent": "Right, if you have multiple machines right?",
                    "label": 0
                },
                {
                    "sent": "We can easily read large chunks of the web corpus.",
                    "label": 0
                },
                {
                    "sent": "And how does that fit with with a small ontology.",
                    "label": 0
                },
                {
                    "sent": "So all these are our issues for.",
                    "label": 0
                },
                {
                    "sent": "That kind of more formal approach.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in response to these challenges, and by the way, I don't claim to have all the answers we have our approach and it has its own challenges which I'll talk about some and we're happy to discuss more.",
                    "label": 0
                },
                {
                    "sent": "My point is that these challenges motivated us to formulate open IE where.",
                    "label": 1
                },
                {
                    "sent": "Open a looks as follows and again, the best way to understand it is in contrast with more traditional IE.",
                    "label": 0
                },
                {
                    "sent": "In more traditionally you have as input a corpus and then you have hand labeled data per relation.",
                    "label": 0
                },
                {
                    "sent": "You have a million relations, then you need.",
                    "label": 0
                },
                {
                    "sent": "To label a million concepts.",
                    "label": 0
                },
                {
                    "sent": "Whereas an opening, we start with the concept and no label data on a per relation basis.",
                    "label": 0
                },
                {
                    "sent": "Obviously traditionally the relations have to be specified in advance.",
                    "label": 1
                },
                {
                    "sent": "Otherwise how do you know what concepts the labeled data for in opening these are discovered automatically, right?",
                    "label": 0
                },
                {
                    "sent": "And this is key to the serendipity and extensibility of open aid.",
                    "label": 0
                },
                {
                    "sent": "And then the extractor does produce in.",
                    "label": 0
                },
                {
                    "sent": "Traditionally is a relation specific extractor right 14.",
                    "label": 0
                },
                {
                    "sent": "Management succession one for this element of nutrition, etc etc.",
                    "label": 1
                },
                {
                    "sent": "Whereas ours is relation independent.",
                    "label": 0
                },
                {
                    "sent": "We're really trying to capture actually certain things about relationships in language and use that to do the extraction and I'll talk more about that in a second.",
                    "label": 0
                },
                {
                    "sent": "So really the question comes up OK if you're going to do this, how is that even possible in any nontrivial way?",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And it turns out that it's possible because we believe, and this is a hypothesis we articulated awhile ago, but now have years of evidence for there is a subset of English that's a relatively easy to understand, and it goes way beyond extraction.",
                    "label": 0
                },
                {
                    "sent": "But for the purposes of today, let's just say that there's a subset of English where we're able to characterize relations and their arguments syntactically.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's without having to learn lexical notions that are specific to particular relation or particular domain.",
                    "label": 0
                },
                {
                    "sent": "We've actually had a series of papers characterize relations in arguments and shown that the characterization is a compact, be domain independent, and empirically based on a large sample of sentence is we've shown that our characterization covers 85% of binary verb based relations, and in more recent work we're extending that to nouns.",
                    "label": 0
                },
                {
                    "sent": "Adjectives and so on.",
                    "label": 0
                },
                {
                    "sent": "So this is the key right?",
                    "label": 0
                },
                {
                    "sent": "And the basic idea here is very simple.",
                    "label": 0
                },
                {
                    "sent": "Is there really such a difference between talking about company acquisitions and talking about which foods contain vitamin C?",
                    "label": 0
                },
                {
                    "sent": "Well, obviously there's a difference in the lexemes, but.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Tactically they are quite similar to give you the intuition here without going into the complex expressions that are in the paper.",
                    "label": 0
                },
                {
                    "sent": "Here's a sample of some simple relation phrases and you quickly see the across a huge range of topics of verb can do a great job of capturing a relationship, invented, downloaded, what have you.",
                    "label": 0
                },
                {
                    "sent": "The verb could be followed by preposition, acquired by voted for, born in.",
                    "label": 0
                },
                {
                    "sent": "Or maybe there's some intervening gun now is determined as etc is in this.",
                    "label": 0
                },
                {
                    "sent": "Light verb constructions has a PhD in inhibits tumor growth in mastered the art of so again, completely disparate domains, but very economical way of expressing information.",
                    "label": 1
                },
                {
                    "sent": "And this is what open any hooks onto to do its extraction from arbitrary senses, arbitrary domains.",
                    "label": 0
                },
                {
                    "sent": "What this buys us.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As.",
                    "label": 0
                },
                {
                    "sent": "Is the ability to operate over much larger sets of relations than in previous things, so I apologize if your system is not listed here or if I got the number of relations off by some amount right?",
                    "label": 1
                },
                {
                    "sent": "All of these are dynamic projects and these are moving targets, but the point that I'm making is that our systems are doing two to three orders of magnitude more relations relative to previous work.",
                    "label": 0
                },
                {
                    "sent": "There's a catch though.",
                    "label": 0
                },
                {
                    "sent": "When I say relations.",
                    "label": 0
                },
                {
                    "sent": "Maybe a more appropriate term would be relation phrases, right?",
                    "label": 0
                },
                {
                    "sent": "So we capture the phrases in the language that denote relations.",
                    "label": 0
                },
                {
                    "sent": "What we don't do is map them to an ontology, right?",
                    "label": 0
                },
                {
                    "sent": "We don't normalize them, at least not yet.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's definitely much bigger, but it's also a weaker notion of relation, OK?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So where, where are we going with that in 2007 we feel the text running was, which was the first web scale open.",
                    "label": 0
                },
                {
                    "sent": "A system used ideas like distant supervision to generate a model, training model of language, then developed a CRF model of relations and our notion of relations.",
                    "label": 1
                },
                {
                    "sent": "Rules of the forum argument.",
                    "label": 0
                },
                {
                    "sent": "Wanna relation phrase?",
                    "label": 1
                },
                {
                    "sent": "An argument to were able to generate a billion distinct extractions?",
                    "label": 0
                },
                {
                    "sent": "And do so with nontrivial levels of precision over the year.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As we and others have been consistently improving the area under the precision recall curve.",
                    "label": 0
                },
                {
                    "sent": "Most recent system.",
                    "label": 0
                },
                {
                    "sent": "Here is a reverb and I'll talk to you about an even more impressive system called Holly in a few minutes, but the general trend is up, which is which is not surprising.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What else have we've done?",
                    "label": 0
                },
                {
                    "sent": "We've made an open source version of the extractor and I was pleased to see a number of the papers in the workshop using that and making it better.",
                    "label": 0
                },
                {
                    "sent": "We've looked into synonym detection and we've also looked into much more intensive analysis of language using parsers using relations that aren't just based on verbs.",
                    "label": 0
                },
                {
                    "sent": "Using the context of the relation.",
                    "label": 0
                },
                {
                    "sent": "And again, today we haven't seen something that is inconsistent with our semantic tractability hypothesis.",
                    "label": 0
                },
                {
                    "sent": "On the contrary, what we see is that whatever linguistic phenomena we're interested in, we're able to identify a semantically tractable corner, and then often bootstrap from there to have a more general notion of language.",
                    "label": 0
                },
                {
                    "sent": "So, for example, all E takes as input.",
                    "label": 0
                },
                {
                    "sent": "Millions of high quality extractions from reverb and then uses that to generalize.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And let me just give you a couple of examples.",
                    "label": 0
                },
                {
                    "sent": "So we go from the simple verbas relations to being able to handle sentence is like after beating the heat.",
                    "label": 0
                },
                {
                    "sent": "the Celtics, another top dog in the NBA.",
                    "label": 1
                },
                {
                    "sent": "Little prediction embedded in there so we can get the Celtics beat the heat.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or if we have the sentence, if he wins five key states, Romney will be president.",
                    "label": 1
                },
                {
                    "sent": "That's not a prediction.",
                    "label": 0
                },
                {
                    "sent": "That's a counter factual.",
                    "label": 0
                },
                {
                    "sent": "And again, we're able to capture that counterfactual at least as a phrase.",
                    "label": 1
                },
                {
                    "sent": "If he wins 5 States and this is a paper that's going to appear in email P. 12, So the sophistication of extraction from language is a major focus for us.",
                    "label": 0
                },
                {
                    "sent": "We're continuing to push really hard on getting more and more from individual sentence is.",
                    "label": 0
                },
                {
                    "sent": "Sentence is in combination information that's explicit information that's implicit, but the key is to do this based on domain independent regularity's in language not to do this based on lexical patterns, simply because the structure is there.",
                    "label": 0
                },
                {
                    "sent": "So you want to exploit that before you move on to more domain specific stuff.",
                    "label": 1
                },
                {
                    "sent": "OK, so I hope you're convinced that this is a valid direction of research, and I encourage you to read the papers for the details.",
                    "label": 0
                },
                {
                    "sent": "It still raises the question of OK, what about entities types, ontologies, and all that good stuff.",
                    "label": 0
                },
                {
                    "sent": "So what, in fact we've done is.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rather than just a proud of being ontology free, we've started moving in the direction of Anthologizing Open A and the first step, and this is the work of Tomlin who's who's here and has a paper on this.",
                    "label": 0
                },
                {
                    "sent": "At the workshop we started to link the extraction arguments to Freebase when possible, and nothing particularly magical about Freebase.",
                    "label": 1
                },
                {
                    "sent": "That's what we had at hand when possible.",
                    "label": 0
                },
                {
                    "sent": "Part is really key here.",
                    "label": 0
                },
                {
                    "sent": "'cause we're achieving graceful degradation, right?",
                    "label": 0
                },
                {
                    "sent": "If you can link an argument, Freebase, great, but we don't want to restrict the extraction, so only what's in the ontology right?",
                    "label": 0
                },
                {
                    "sent": "That would suffer from all the that would suffer from the ontological glass ceiling that I talked about, right?",
                    "label": 0
                },
                {
                    "sent": "So when you can't link, we don't, and the system operates just fine when we do have, those links were able to associate types with the arguments and I'll show you a demo of that in a second and then in.",
                    "label": 0
                },
                {
                    "sent": "Another paper by Tom that's coming out later this summer.",
                    "label": 1
                },
                {
                    "sent": "He wrote a paper known noun phrase left behind where he can detect arguments that aren't in Freebase and figure out their their types by generalizing from the freebase arguments right?",
                    "label": 0
                },
                {
                    "sent": "So again, we're able to carry over some of the semantic information that's in Freebase.",
                    "label": 0
                },
                {
                    "sent": "Two things that aren't aren't there that are not in Freebase and again therefore extend this notion of types beyond what's in the original.",
                    "label": 0
                },
                {
                    "sent": "Colleges.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So with that in mind, here's what our system architecture looks like.",
                    "label": 0
                },
                {
                    "sent": "We start with the web corpus.",
                    "label": 0
                },
                {
                    "sent": "We run an extractor.",
                    "label": 0
                },
                {
                    "sent": "This used to be text runner, then re verb.",
                    "label": 0
                },
                {
                    "sent": "Then all the the demo I'm going to show you is based on reverb, but we're continually improving this piece.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, we get out of that raw topples.",
                    "label": 0
                },
                {
                    "sent": "We do a number of processes over them like synonym detection like counting and detecting redundancy.",
                    "label": 0
                },
                {
                    "sent": "We do deduplication.",
                    "label": 0
                },
                {
                    "sent": "All that allows us to, so to assess our confidence in the extractions.",
                    "label": 0
                },
                {
                    "sent": "Then we come up with extraction sets.",
                    "label": 0
                },
                {
                    "sent": "We do the entity linking of the arguments to Freebase and we get something that looks like this and you see the bolded arguments like Albert Einstein, other ones that have a link, and that's the Convention will use some demo ensuring a second and once we've gone through this process now we have a query processor.",
                    "label": 0
                },
                {
                    "sent": "That person can use, so let me.",
                    "label": 0
                },
                {
                    "sent": "Show you what that looks like.",
                    "label": 0
                },
                {
                    "sent": "And maybe before I start, let me just.",
                    "label": 0
                },
                {
                    "sent": "Give you my favorite example query, but I'll start by showing you what it looks like on on Google.",
                    "label": 0
                },
                {
                    "sent": "So let's say I want to answer the question what kills bacteria.",
                    "label": 0
                },
                {
                    "sent": "If I go to Google, let's take a minute.",
                    "label": 0
                },
                {
                    "sent": "With this I see a bunch of snippets from various answer sites.",
                    "label": 0
                },
                {
                    "sent": "As you can see, I've clicked on some of these so you gotta wiki answers and you find a fairly decent answer.",
                    "label": 0
                },
                {
                    "sent": "Also some some problems.",
                    "label": 0
                },
                {
                    "sent": "But antibiotics, antiseptics, bleach disinfected OK. That's a pretty good set of answers if I click.",
                    "label": 0
                },
                {
                    "sent": "Let's see if I click on answers from s.com, what are they going to tell us?",
                    "label": 0
                },
                {
                    "sent": "I'll see a bunch of ads and then I'll see some obscure substance.",
                    "label": 0
                },
                {
                    "sent": "OK, so I think if I spent an hour here I could come up with a pretty comprehensive set of answers to this question.",
                    "label": 0
                },
                {
                    "sent": "If you go to open information extraction.",
                    "label": 0
                },
                {
                    "sent": "By the way, this demo is generally available at openai.cs.washington.edu.",
                    "label": 0
                },
                {
                    "sent": "We have links here to some key papers and also to our.",
                    "label": 0
                },
                {
                    "sent": "Textual resources that I'll talk about later, but we have that one location for all your open information extraction needs anyway.",
                    "label": 0
                },
                {
                    "sent": "So what you have here is an interface that allows you to query by argument, query by relation or or combinations thereof.",
                    "label": 0
                },
                {
                    "sent": "So, and we have a simple ability to map from questions to these sufe say what kills bacteria in Maps it to a question of query.",
                    "label": 0
                },
                {
                    "sent": "The formulation is killed argument to the bacterium.",
                    "label": 0
                },
                {
                    "sent": "We don't know what argument one is.",
                    "label": 0
                },
                {
                    "sent": "I press search, this is cashed and so other queries would definitely take longer and you see a pretty comprehensive set of answers.",
                    "label": 0
                },
                {
                    "sent": "We have more and more here as you go, and it's distill this down to an answer the the number you see next to the answer is the number of sentences this is based on.",
                    "label": 0
                },
                {
                    "sent": "I can click on it and bring up the sentence is we have provenance right?",
                    "label": 0
                },
                {
                    "sent": "I can click through to the underlying URL and see where it comes from.",
                    "label": 0
                },
                {
                    "sent": "And the corpus we used here is a combination of 500 million web pages that Google kindly provided us as well as we've recently run it on Clue Web as well.",
                    "label": 0
                },
                {
                    "sent": "And what you're seeing here is based on the combination now what's nice here is that if I look at a particular place, if I hover over that, I can see here.",
                    "label": 0
                },
                {
                    "sent": "Let's take this one.",
                    "label": 0
                },
                {
                    "sent": "I can see the freebase information about this, the various types.",
                    "label": 0
                },
                {
                    "sent": "I can see the various synonyms that we've extracted right?",
                    "label": 0
                },
                {
                    "sent": "We sub vinegar.",
                    "label": 0
                },
                {
                    "sent": "38 times etc, but also in cases where I failed to do that like high temperatures is not something that's in Freebase.",
                    "label": 0
                },
                {
                    "sent": "The system works just fine and I'll at least get the extracted synonyms.",
                    "label": 0
                },
                {
                    "sent": "Another nice thing is I have the benefit of.",
                    "label": 0
                },
                {
                    "sent": "Freebase is type system so if I want to focus on let's say I want to look at foods that kill bacteria I could click on that see if I successfully click there.",
                    "label": 0
                },
                {
                    "sent": "Oh, I did write an icy various foods, and I obviously I also suffer right from whatever idiosyncrasy's are in anthology.",
                    "label": 0
                },
                {
                    "sent": "For example, I see book subject a lot, which is not particularly meaningful in this context, so you get a sense for how this works.",
                    "label": 0
                },
                {
                    "sent": "Let me just show you one more query to give you a sense of this.",
                    "label": 0
                },
                {
                    "sent": "Another thing that we can do is do type queries, so again.",
                    "label": 0
                },
                {
                    "sent": "We're leveraging the type systems if that's in Freebase.",
                    "label": 0
                },
                {
                    "sent": "If I want to know what actor starred in which films, I'll say type: actor starred in as the relation type: film.",
                    "label": 0
                },
                {
                    "sent": "I'll search and again I get a whole variety of answers from a whole bunch of sentence is from Bruce Lee in Enter the dragon to Heath Ledger Anson's Son and I'll just show you my favorite examples days when I ask what is the symbol of which country?",
                    "label": 0
                },
                {
                    "sent": "So again relation is symbol of.",
                    "label": 0
                },
                {
                    "sent": "Type is country.",
                    "label": 0
                },
                {
                    "sent": "I live the first argument blank and I get a wide variety of answers.",
                    "label": 0
                },
                {
                    "sent": "Chinese dragon.",
                    "label": 0
                },
                {
                    "sent": "I didn't know that leak, for example, was the symbol of Wales or the milk Thistle for Scotland.",
                    "label": 0
                },
                {
                    "sent": "And again all these have their links and I have the Uri that I can I can pursue etc etc.",
                    "label": 0
                },
                {
                    "sent": "So this is the current state of open information extraction at at large scale.",
                    "label": 0
                },
                {
                    "sent": "I would have to take a deep breath and a sip after you did over the web demo and it actually works.",
                    "label": 0
                },
                {
                    "sent": "Bread.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's that's the good news.",
                    "label": 0
                },
                {
                    "sent": "If you will, what's the bad news?",
                    "label": 0
                },
                {
                    "sent": "Well, we don't have a formal ontology, as I've been emphasizing over Cabul airy.",
                    "label": 0
                },
                {
                    "sent": "We have a lot of inconsistent extractions, right?",
                    "label": 1
                },
                {
                    "sent": "I showed you some of our best examples.",
                    "label": 0
                },
                {
                    "sent": "If you play with our demo, you'll easily find much worse examples.",
                    "label": 0
                },
                {
                    "sent": "We have extraction errors, we have inconsistencies.",
                    "label": 0
                },
                {
                    "sent": "So can this hodgepodge of structured textual stuff?",
                    "label": 0
                },
                {
                    "sent": "Can IT support recently?",
                    "label": 0
                },
                {
                    "sent": "And if it can't, what's the point of open a?",
                    "label": 1
                },
                {
                    "sent": "Aren't you just playing games with words on the page?",
                    "label": 0
                },
                {
                    "sent": "And I have several answers to these, but let me just immediately address that one.",
                    "label": 0
                },
                {
                    "sent": "It turns out that open a can support reason.",
                    "label": 0
                },
                {
                    "sent": "We do generalization.",
                    "label": 0
                },
                {
                    "sent": "We do reasoning on top of this, and the point of it will get to in a second.",
                    "label": 0
                },
                {
                    "sent": "But absolutely we do not need an ontology to do reason, it's just it's a different kind of reasoning.",
                    "label": 0
                },
                {
                    "sent": "We get that in a second, so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In response to these questions, which are fair questions, I thought of three key answers.",
                    "label": 0
                },
                {
                    "sent": "What's the point of opening one is?",
                    "label": 0
                },
                {
                    "sent": "I do think that it suggests a direction for search, at least for certain kinds of search too.",
                    "label": 0
                },
                {
                    "sent": "We're releasing a number of textual resources that are based on open A and I'll list those in a minute and then I'll talk to you about several efforts that we have for reasoning over extractions.",
                    "label": 1
                },
                {
                    "sent": "Generated through this process so.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I think that there is a potential for a new paradigm for search that goes way beyond the Google Knowledge Graph and so on.",
                    "label": 1
                },
                {
                    "sent": "And I called it a long time ago, moving up the information food chain and the idea is really to move from information retrieval to information extraction to move from snippets and documents to entities and relations and Google Knowledge Graph is an important step in that direction, but also to move from keyword queries to questions and from list of documents to answers.",
                    "label": 1
                },
                {
                    "sent": "And you saw a simple example of that in the open a demo.",
                    "label": 0
                },
                {
                    "sent": "You know, if you take out your iPhone, you can access the open a demo and it actually works surprisingly well again, for a non commercial prototypes because we take a huge amount of information and crystallize it into succinct answers.",
                    "label": 0
                },
                {
                    "sent": "So really think of this ideas as Siri makes Watson, we want to have the sophistication of Lawson of analyzing enormous amount of text, but we want to have it available via Siri like interface.",
                    "label": 0
                },
                {
                    "sent": "That would be a search engine that I'd like to use from my from my iPhone.",
                    "label": 0
                },
                {
                    "sent": "OK, and I think we're taking steps in that direction.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We show you a case study we did over Yelp reviews.",
                    "label": 1
                },
                {
                    "sent": "Well we did is.",
                    "label": 0
                },
                {
                    "sent": "We took several 100,000 Yelp reviews only for restaurants in Seattle.",
                    "label": 0
                },
                {
                    "sent": "I would love to show this for you in the context Of Montreal, but we didn't get access to that data.",
                    "label": 0
                },
                {
                    "sent": "We mapped the review corpus to a set of attribute values and we did this automatically with very minimal hand labeling of data.",
                    "label": 0
                },
                {
                    "sent": "So we can tell that a review says the sushi is fresh.",
                    "label": 0
                },
                {
                    "sent": "In his free etc etc.",
                    "label": 0
                },
                {
                    "sent": "We then allow natural language queries like well.",
                    "label": 0
                },
                {
                    "sent": "Where is the best sushi in Seattle?",
                    "label": 1
                },
                {
                    "sent": "And we answer that not with a literal mesh, right?",
                    "label": 0
                },
                {
                    "sent": "Information retrieval would look for the phrase best sushi.",
                    "label": 0
                },
                {
                    "sent": "We answer that by looking at the attributes, sushi and various synonyms for it and doing sentiment analysis on the values that get returned so we can figure out that exquisite is better than very good.",
                    "label": 0
                },
                {
                    "sent": "Very good is better than so, so and.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As a result, we're able to answer questions like that and able to compress the information into something that fits on a mobile phone.",
                    "label": 0
                },
                {
                    "sent": "So this is actually our we have this both on iPhone and we have an Android app you can see here about the restaurant serious Pie in Seattle.",
                    "label": 0
                },
                {
                    "sent": "You have the extracted attributes and it automatically figures out the things like crust is particularly important, and then again very similar.",
                    "label": 0
                },
                {
                    "sent": "You'll see values you will see how many times you saw those.",
                    "label": 0
                },
                {
                    "sent": "You can click through.",
                    "label": 0
                },
                {
                    "sent": "To get the sentence and the actual reviews, we do the sentiment analysis using color.",
                    "label": 0
                },
                {
                    "sent": "So green is positive, red is negative and we played with a variety of of interfaces.",
                    "label": 0
                },
                {
                    "sent": "The point is, instead of spending hours reading reviews which you really do not want to do on your smartphone, this extractive interface gives you a concise sum.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's highly actionable.",
                    "label": 0
                },
                {
                    "sent": "This is available, or if minor.com we have a desktop interface you want to play with and the Android app is downloadable.",
                    "label": 0
                },
                {
                    "sent": "So if you're ever in Seattle, which some of you are there frequently, it's better you should be finding your restaurants using Revel minor.com.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So second point, what resources can we create leveraging open 81 that we're unveiling at the workshop today and is what we call rella Grams?",
                    "label": 0
                },
                {
                    "sent": "It's a probabilistic model of relations and text and their sequences, right?",
                    "label": 0
                },
                {
                    "sent": "That's very similar to integration.",
                    "label": 0
                },
                {
                    "sent": "Anagrams are probabilistic model over characters or words.",
                    "label": 0
                },
                {
                    "sent": "Here we have these models over relations.",
                    "label": 0
                },
                {
                    "sent": "We have a database with 94 million rella grams that's available.",
                    "label": 0
                },
                {
                    "sent": "And Steven will be Stephen Siren will be talking about that in a later in Manning.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "His poster, that kind of thing you'll see is things like, well police, investigate X.",
                    "label": 0
                },
                {
                    "sent": "Then it turns out that it's often followed by they charge YX&Y.",
                    "label": 0
                },
                {
                    "sent": "Here are variables and not necessarily the same variable.",
                    "label": 0
                },
                {
                    "sent": "Anas Tom Mitchell pointed out at dinner last night.",
                    "label": 0
                },
                {
                    "sent": "This is actually a good thing.",
                    "label": 0
                },
                {
                    "sent": "I was like, why is that a good thing?",
                    "label": 0
                },
                {
                    "sent": "But well, that means that the investigation had some value, right?",
                    "label": 0
                },
                {
                    "sent": "If it always charge the same person they investigated, or if they always did, then what's the point of?",
                    "label": 0
                },
                {
                    "sent": "Of the investigation.",
                    "label": 0
                },
                {
                    "sent": "Actually in some countries, right?",
                    "label": 0
                },
                {
                    "sent": "That's not the case.",
                    "label": 0
                },
                {
                    "sent": "So so here we learned an important role.",
                    "label": 0
                },
                {
                    "sent": "In fact, from this kind of narrative structure, and in fact we're very interested in using that in extraction and using that in a number of ways and using that to to create rich structures that describe events and so on so.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also released 600,000 of our favorite relation phrases.",
                    "label": 1
                },
                {
                    "sent": "If people want to play with a rich set of relationships, is we generated a bunch of metadata over relations.",
                    "label": 0
                },
                {
                    "sent": "For example, the domain and range for 50,000 relations, 10,000 functional relations, 30,000 Learn horn clauses, all these are available at that website.",
                    "label": 1
                },
                {
                    "sent": "Open a.",
                    "label": 0
                },
                {
                    "sent": "Is the Washington Edu and one of the most impressive resources is coming soon and this is the work of Jonathan Brandt who's a student of E to the gun in Israel.",
                    "label": 1
                },
                {
                    "sent": "Jonathan came to visit us and got excited about the work and he's built a resource that he calls clean.",
                    "label": 0
                },
                {
                    "sent": "It's 10 million 10,000,000 entailment rules, very similar to the ones you find in dirt, but the precision, According to him, is double that of dirt.",
                    "label": 1
                },
                {
                    "sent": "Using using their metrics, so that's coming soon and I think should be an exciting resource.",
                    "label": 0
                },
                {
                    "sent": "So second use of opening.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No, Thirdly to go to the point about reasoning over extractions, we've had a number of efforts and I'll just go through them in a little bit just to give you the flavor of them and to show you the different kinds of reasoning you can do so.",
                    "label": 0
                },
                {
                    "sent": "I already talked about how we map arguments into our we mapped extracting arguments into Freebase entities.",
                    "label": 0
                },
                {
                    "sent": "I won't go into that war.",
                    "label": 0
                },
                {
                    "sent": "We also identify synonyms automatically, both synonymous arguments, anonymous relations.",
                    "label": 1
                },
                {
                    "sent": "We were able to do linear time 1st order form clause inference and we're also able to learn types via generative model.",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we do synonym detection?",
                    "label": 0
                },
                {
                    "sent": "We have an unsupervised probabilistic model and the basic idea is extremely simple.",
                    "label": 1
                },
                {
                    "sent": "If we want to decide what's the probability that Bill Clinton is the same as President Clinton, well, we're going to count how many relations and our twos these two share.",
                    "label": 0
                },
                {
                    "sent": "If we wanted to side that acquired and bought are synonymous relations, we're going to count how many argument pairs they share, and that gets us along that direction.",
                    "label": 0
                },
                {
                    "sent": "There's an obvious.",
                    "label": 0
                },
                {
                    "sent": "Mutual recursion here, right?",
                    "label": 0
                },
                {
                    "sent": "Between this, so we can go back and forth.",
                    "label": 0
                },
                {
                    "sent": "And obviously, when you're counting shared relations, you might want to wait functions more heavily, right?",
                    "label": 0
                },
                {
                    "sent": "So if I and somebody else are married to the same person, that's more important than the fact that we both visited France, right?",
                    "label": 0
                },
                {
                    "sent": "So not all relations are created equal, and this is part of Alex uses thesis and.",
                    "label": 0
                },
                {
                    "sent": "This is done some nice work there.",
                    "label": 1
                },
                {
                    "sent": "We've also now looked at OK as you start to unify with Freebase and you don't just have textual arguments.",
                    "label": 0
                },
                {
                    "sent": "You can do this even even better.",
                    "label": 0
                },
                {
                    "sent": "Unscalable",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Actual inference, again, I'll just give you a real hit here in text.",
                    "label": 0
                },
                {
                    "sent": "That kind of inference you want to do is probabilistic, and if you're dealing with web scale corpora, it has to be linear time in the size of the corpus, right?",
                    "label": 0
                },
                {
                    "sent": "These are massive massive corpora, and in fact we've identified a property of argument distributions of textual relations that actually gives us both of these.",
                    "label": 1
                },
                {
                    "sent": "The inference is actually probably linear, and even more important, because you know the proof.",
                    "label": 0
                },
                {
                    "sent": "Sometimes smuggle in a few assumptions.",
                    "label": 0
                },
                {
                    "sent": "Here in there, we actually did a series.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Experiments with a system called homes, where we showed that as we scale over larger and larger corpora going to hundreds of millions of pages, the inference process remains linear time so.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So another example and then Lastly thinking about mapping from extractions to the domain and range of these relations.",
                    "label": 0
                },
                {
                    "sent": "There's been extensive previous work on this problem.",
                    "label": 1
                },
                {
                    "sent": "It's called Selectional preferences.",
                    "label": 1
                },
                {
                    "sent": "It's called various things like that, and we've been able to utilize generative models to do this.",
                    "label": 1
                },
                {
                    "sent": "And the basic idea is we take the instances of extractions of relation.",
                    "label": 0
                },
                {
                    "sent": "We model those as a document and then we think of the domain and range as topics.",
                    "label": 0
                },
                {
                    "sent": "Here we're understand.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Weird, so here's relation.",
                    "label": 0
                },
                {
                    "sent": "Viewed as a document, right?",
                    "label": 0
                },
                {
                    "sent": "We sort our extractions by topic and then we're able to find regularity's in the argument sets as well as constraints between different relate.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oceans we run a generative model won't go through into the details.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's the sort of thing we extract.",
                    "label": 0
                },
                {
                    "sent": "Right elect country tends to elect the person.",
                    "label": 0
                },
                {
                    "sent": "Experts predict events, people download software, etc etc.",
                    "label": 0
                },
                {
                    "sent": "Again, obviously these are the best cases, but we've made these available on the web free to play with and for people to download and use.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So quick summary.",
                    "label": 0
                },
                {
                    "sent": "I've gone through a lot of stuff rather quickly.",
                    "label": 0
                },
                {
                    "sent": "Here's the summary of the trajectory of the project over the last nine years and really focused on the last five.",
                    "label": 0
                },
                {
                    "sent": "We start out with text Runner and a billion ontology free extractions.",
                    "label": 1
                },
                {
                    "sent": "We've worked on inference over the extractions and getting metadata from there.",
                    "label": 1
                },
                {
                    "sent": "We've released open source extractors.",
                    "label": 1
                },
                {
                    "sent": "We've released public textual resources and recently we have been busy on both ontology Ising it.",
                    "label": 0
                },
                {
                    "sent": "By by intersecting.",
                    "label": 0
                },
                {
                    "sent": "Open a with freebase.",
                    "label": 0
                },
                {
                    "sent": "Undoing deeper analysis of the sentence is as an oly and much more work we're doing now to get more implicit information out of the sentences, and also on I eBay search.",
                    "label": 0
                },
                {
                    "sent": "I think very practically there's a lot more.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or to do there.",
                    "label": 0
                },
                {
                    "sent": "So to end, I want to leave room for questions.",
                    "label": 0
                },
                {
                    "sent": "Where are we going?",
                    "label": 0
                },
                {
                    "sent": "Again a number of directions, but when I want to emphasize is becoming more open, one of the things that we realized is that our approach is monolithic, right?",
                    "label": 0
                },
                {
                    "sent": "We have our extractor and we generate couples based on that, but because we're so relatively lightweight, it's very easy for us to adjust couples from any source.",
                    "label": 1
                },
                {
                    "sent": "So if you give me a tuple, some notion of confidence and again we can normalize this.",
                    "label": 0
                },
                {
                    "sent": "If your notion of confidence.",
                    "label": 0
                },
                {
                    "sent": "Is different than ours.",
                    "label": 0
                },
                {
                    "sent": "Anna source.",
                    "label": 0
                },
                {
                    "sent": "We can start ingesting those couples.",
                    "label": 0
                },
                {
                    "sent": "It creates some interesting algorithmic problems.",
                    "label": 0
                },
                {
                    "sent": "How do you surface that in our system?",
                    "label": 1
                },
                {
                    "sent": "But something that I really want to do is take nails, or somebody else is couples and just bring them into our system, open input and then the second thing is I'm very enamored with what's happening in the linked open data community.",
                    "label": 0
                },
                {
                    "sent": "I feel like they've gotten the model of distribution of authorship right.",
                    "label": 1
                },
                {
                    "sent": "They have some other things that are more challenging, like they don't have a notion of uncertainty yet, but I think linking our extractions into VLD cloud is something that we're going to do and that's going to require us to think hard about relation normalization, something that we don't have results on yet, but I like here is that again.",
                    "label": 0
                },
                {
                    "sent": "Instead of legislating something, they have the idea of using best practices, which is really all you can do if you want to do.",
                    "label": 0
                },
                {
                    "sent": "Things at this level of scale and distribution and then specialize reasoners, which is really what I was showing you open a, uses specialized reasoners are going to operate over portions of this lovr club so.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To conclude, I argued ontology is not necessary for reasoning, however open he's becoming gracefully antologi gracefully.",
                    "label": 1
                },
                {
                    "sent": "I mean right not fully just wear wear appropriate were boosting our analysis of text, and I think that Elodie has distribution and scale but in a funny way tax if you look in the Elodie cloud right, they haven't incorporated the results of extraction by large and that's a great opportunity for us and for this community.",
                    "label": 0
                },
                {
                    "sent": "Or Bud, thank you.",
                    "label": 0
                },
                {
                    "sent": "We have some time for questions.",
                    "label": 0
                },
                {
                    "sent": "Indoor DARPA.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I just was curious for I like this top when you at some point in their very inside about inferring implicit information, but a lot of what's going on, at least in examples that you presented, pivoted off of explicit information that was somehow conveyed.",
                    "label": 0
                },
                {
                    "sent": "And so for, you know, for example, Ray Mooney's Idol presentation where he said John needed money.",
                    "label": 0
                },
                {
                    "sent": "He called his parents.",
                    "label": 0
                },
                {
                    "sent": "There are couple of relations there, including the parents having the money or.",
                    "label": 0
                },
                {
                    "sent": "Potentially in the future, transferring the money with some probability and those are not explicitly represented in either of those two sentence is that the direction that you're headed in or use.",
                    "label": 0
                },
                {
                    "sent": "Are you saying right now the ability for reasoning about type is already embedded in here.",
                    "label": 0
                },
                {
                    "sent": "It is largely the direction we're heading rather than stuff that I demo right.",
                    "label": 0
                },
                {
                    "sent": "The demo I showed it scale is very much about explicit information, but let me give you an example right if you think about relative grams right, generating expectations, right?",
                    "label": 0
                },
                {
                    "sent": "Those are the sorts of things where you say you know what this wasn't mentioned in the text, and typically a textual description of an event will omit right many elements.",
                    "label": 0
                },
                {
                    "sent": "However, based on our expectations, right?",
                    "label": 0
                },
                {
                    "sent": "Based on the model of these kinds of events.",
                    "label": 0
                },
                {
                    "sent": "That we've recovered from reading, you know, millions of stories like this, we're going to expect that assign some degree of belief to that, so that's future work.",
                    "label": 0
                },
                {
                    "sent": "Holograms are here, and the implicit stuff is future, right?",
                    "label": 0
                },
                {
                    "sent": "So if you're so yeah, so this is future.",
                    "label": 0
                },
                {
                    "sent": "I'm stuffing stuff I have.",
                    "label": 0
                },
                {
                    "sent": "I want to point out a few things about the use of ontology.",
                    "label": 0
                },
                {
                    "sent": "The terminology I know that it's used in very ambiguous ways, and so the way you used it is also quite common, but it doesn't quite reflect the way this used in the semantic web community now.",
                    "label": 0
                },
                {
                    "sent": "And I would say there are two things.",
                    "label": 0
                },
                {
                    "sent": "One thing is to distinguish your apology as a conceptual model from the language you use.",
                    "label": 0
                },
                {
                    "sent": "So if you replace your use of ontology by, let's say description logics right, then I would fully agree with what you said.",
                    "label": 0
                },
                {
                    "sent": "That maybe you don't want to have all this kind of general purpose.",
                    "label": 0
                },
                {
                    "sent": "Very expensive reasons.",
                    "label": 0
                },
                {
                    "sent": "So I must also say there are also like lightweight and cheap description logic reasons that run polynomial time over.",
                    "label": 0
                },
                {
                    "sent": "So but but I would rather agree when you replace or told you by description logics.",
                    "label": 0
                },
                {
                    "sent": "A lot of what you do I would call from my perspective and my background was when I started as a PhD student to do information extraction using ontologies.",
                    "label": 0
                },
                {
                    "sent": "I would allow what you do.",
                    "label": 0
                },
                {
                    "sent": "Call ontology based information extraction or doing information extraction to extend the ontology right when you formalize what is the relation, what is the domain and range?",
                    "label": 0
                },
                {
                    "sent": "From my point of view from my use of the word ontology that well.",
                    "label": 0
                },
                {
                    "sent": "Construction.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, last push.",
                    "label": 0
                },
                {
                    "sent": "Ontology now is using.",
                    "label": 0
                },
                {
                    "sent": "The semantic web is not that you have like this one big thing, that's everything need.",
                    "label": 0
                },
                {
                    "sent": "But people really take as you showing this link data cloud where you have a different technologies and ontologies are interlinked.",
                    "label": 0
                },
                {
                    "sent": "And that's a very important metric for, because it says, well, I cannot like capture everything that's too much for me.",
                    "label": 0
                },
                {
                    "sent": "But what I can do is other people have worked on using or people or documents.",
                    "label": 0
                },
                {
                    "sent": "And so I I use what other people have achieved in describing this domain that I'm interested in, and one of these domains actually is also text.",
                    "label": 0
                },
                {
                    "sent": "Annotations only, last week's repeats many web conference.",
                    "label": 0
                },
                {
                    "sent": "There were people talking about using this many web form.",
                    "label": 0
                },
                {
                    "sent": "It's just like text annotation.",
                    "label": 0
                },
                {
                    "sent": "Which is then just one possible limitation.",
                    "label": 0
                },
                {
                    "sent": "I want one plus the application of semantic web forms, so I don't disagree with what you say, but I just want to point out that.",
                    "label": 0
                },
                {
                    "sent": "The use of the word apology in this medical community is a bit different than what you, how you would use so.",
                    "label": 0
                },
                {
                    "sent": "And I'm glad to hear is somebody who comes in our community that you see the opportunities for text and semantic web two to match up.",
                    "label": 0
                },
                {
                    "sent": "The only other thing I would add is that since you said that we were doing ontology based extraction and I'm just deeply, deeply insulted by that.",
                    "label": 0
                },
                {
                    "sent": "Instruction right.",
                    "label": 0
                },
                {
                    "sent": "The picture you have in mind with ontology based information extraction is you have this kind of templates like Old Mac style right?",
                    "label": 0
                },
                {
                    "sent": "And that you don't do I fully agree.",
                    "label": 0
                },
                {
                    "sent": "So if you consider like Mac with the template S that is apology based information extraction, I would never say that this week.",
                    "label": 0
                },
                {
                    "sent": "So I won't use the over by.",
                    "label": 0
                },
                {
                    "sent": "Listening to to what you were saying on, I was trying to sort out.",
                    "label": 0
                },
                {
                    "sent": "To what degree is what we saw based on text tokens and inference and manipulation on text tokens versus inference and manipulation of latent concepts like Albert Einstein the person?",
                    "label": 0
                },
                {
                    "sent": "Who might be referred to by many pets tokens and you mentioned synonym finding.",
                    "label": 0
                },
                {
                    "sent": "You didn't mention polysemy, but.",
                    "label": 0
                },
                {
                    "sent": "Aside from the question of whether you want to start with an ontology or not, there's the question of.",
                    "label": 0
                },
                {
                    "sent": "To what degree does the processing in the inferencing of the system.",
                    "label": 0
                },
                {
                    "sent": "Occur at the level of text tokens versus at the level of some latent, not directly observed collection of concepts that you reason about.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a great question because I think that question is at the core of this absolutely, and what I would say is that.",
                    "label": 0
                },
                {
                    "sent": "Obviously a lot of the work that we've done has been doing reasoning at the level of the text tokens, although again, if you look at the generative models that are latent variables that are the domain and range is right.",
                    "label": 0
                },
                {
                    "sent": "Those are the topics related topics.",
                    "label": 0
                },
                {
                    "sent": "However, they are not using formal tokens percent.",
                    "label": 0
                },
                {
                    "sent": "It's also the case that I think if we had more formal tokens.",
                    "label": 0
                },
                {
                    "sent": "There's good reason to believe that the inference would progress more smoothly effectively with higher precision, right?",
                    "label": 0
                },
                {
                    "sent": "And that's why we've been moving in that direction.",
                    "label": 0
                },
                {
                    "sent": "So I think is important is to not lose the textual richness.",
                    "label": 0
                },
                {
                    "sent": "So I would say the goal of where we're headed at least, is to do inference based on a combination of both, as opposed to being restricted to one or the other.",
                    "label": 0
                },
                {
                    "sent": "So I'll follow up on Tom's idea.",
                    "label": 0
                },
                {
                    "sent": "What have you done to deal with polysemy so you know I say Microsoft bought farecast.",
                    "label": 0
                },
                {
                    "sent": "That's one thing.",
                    "label": 1
                },
                {
                    "sent": "If I say you know I bought Orange idea that open IE is a great idea.",
                    "label": 0
                },
                {
                    "sent": "I mean, you know those are completely different notions of bot.",
                    "label": 0
                },
                {
                    "sent": "How do you not just put together 2 words but tear them apart into sensitive?",
                    "label": 0
                },
                {
                    "sent": "So so for example, right?",
                    "label": 0
                },
                {
                    "sent": "We could link in the end of the link in case we allow linking.",
                    "label": 0
                },
                {
                    "sent": "The token to different entities in Freebase in different contexts.",
                    "label": 0
                },
                {
                    "sent": "So polysemy is a very real problem.",
                    "label": 0
                },
                {
                    "sent": "I don't claim that we've solved it, but we're not ignoring it either, and we have mechanisms.",
                    "label": 0
                },
                {
                    "sent": "Alright, one question.",
                    "label": 0
                },
                {
                    "sent": "Read review.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's a Seattle.",
                    "label": 0
                },
                {
                    "sent": "Sushi is an attribute of the restaurant, so.",
                    "label": 0
                },
                {
                    "sent": "For a broader sense is how do you determine what are the attributes you supposed to extract?",
                    "label": 0
                },
                {
                    "sent": "It's not as the other thing, the the approach that we've taken is that various dishes sooner traditions, you know, like the margaritas at a Mexican restaurant or whatever you are attributes of the entity, and then what people say about the more value.",
                    "label": 0
                },
                {
                    "sent": "So the values are opinions, adjectives you know, like the margaritas here, very strong, and the attributes are aspects of the restaurants of parking and sushi.",
                    "label": 0
                },
                {
                    "sent": "From our point of view, the same so you do.",
                    "label": 0
                },
                {
                    "sent": "That's the beauty of it.",
                    "label": 0
                },
                {
                    "sent": "We don't know that that is the key to the approach that we take, and that's why this stuff is related to what we have is an underlying model of language that's syntactic, and that tells us.",
                    "label": 0
                },
                {
                    "sent": "Here's here's what we're looking for, but the system automatically figures out what are the attributes, and in fact we've run it.",
                    "label": 0
                },
                {
                    "sent": "I showed it on restaurant.",
                    "label": 0
                },
                {
                    "sent": "We've run another product categories, and itself figures out what are the things that people talk about, and that's really, really important, because you can start with an upper anthology.",
                    "label": 0
                },
                {
                    "sent": "We say, well, you know.",
                    "label": 0
                },
                {
                    "sent": "Laptop people are going to talk about the following five things, but in fact you finally talk about different things, so that's thanks for asking.",
                    "label": 0
                },
                {
                    "sent": "That's the strength of this approach is serve certain.",
                    "label": 0
                },
                {
                    "sent": "Definitely figuring out what it is that the attributes are in that can generate surprises.",
                    "label": 0
                },
                {
                    "sent": "Alright, thank you very much.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}