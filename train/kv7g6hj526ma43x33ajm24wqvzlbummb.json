{
    "id": "kv7g6hj526ma43x33ajm24wqvzlbummb",
    "title": "Machine Reading at Web Scale",
    "info": {
        "author": [
            "Oren Etzioni, Allen Institute for Artificial Intelligence (AI2)"
        ],
        "published": "Feb. 25, 2008",
        "recorded": "February 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning",
            "Top->Computer Science->Natural Language Processing",
            "Top->Computer Science->Web Mining"
        ]
    },
    "url": "http://videolectures.net/wsdm08_etzioni_mrws/",
    "segmentation": [
        [
            "So now love.",
            "We're very pleased to welcome Professor or an engineer from the University of Washington who is the founder and director of the university's touring center.",
            "His Atripla fellow and the founder of several successful startups including Hamlet to buy or not to buy.",
            "That's the question.",
            "And he's interested in solving fundamental problems in the study of intelligence web search, machine reading and data mining.",
            "So thanks for inference accepting an impression.",
            "Thank you.",
            "Thank you, it's a pleasure to be here.",
            "Hamlet is now called Farecast.",
            "I don't know if that's an improvement, but I won't talk about that.",
            "Let me start here."
        ],
        [
            "Can anybody tell me what this is?",
            "Thank you, it's a Rorschach test.",
            "What I've done is I've given."
        ],
        [
            "As a Rorschach test for computer scientists.",
            "So if you look at this picture, you have to ask yourself what's that is that a?"
        ],
        [
            "Depiction of Moores law."
        ],
        [
            "Storage cap"
        ],
        [
            "City number of web pages."
        ],
        [
            "Facebook users?"
        ],
        [
            "Etc.",
            "We live in a in an exponential world and the Turing Center is meant to investigate a set of questions around search natural language.",
            "In that context, I'll just say a couple of words about the Turing Center and then I'll get to the."
        ],
        [
            "The topic of my talk.",
            "So we have 3 research foci.",
            "The first one is to scale machine translation to the large number of languages on the order of 7000 languages spoken on Earth.",
            "That means 49 million language pairs standards, statistical machine translation technology that doesn't scale to this number of language pairs.",
            "The reason is that they rely on aligned corpora, which are manually generated and you can't have this many align corporate scale.",
            "What we've done is looked at an easier problem.",
            "Lexical translation mapping words from one language to another, with the idea that we could do that for a much larger set of languages.",
            "And so we've built something we called the translation graph.",
            "Right now it has on the order of 2.5 million words as nodes and edges indicate translation between one word and another in a particular sense, and the question that we ask in that project is the inference.",
            "Question is if you start in a word, say in Vietnamese, and you can translate it to French and from French.",
            "Let's say you can go to Catalan.",
            "Well, is that translation path is that truth preserving?",
            "In other words, what's the probability that if you go from Vietnamese all the way to Catalan, you end up with the same word that you started with?",
            "So interesting problem, let me just show you a quick search application built on top of this lexical translation.",
            "One of the things we realized is that image search experience is not nearly as good if you speak a relatively small language like Turkish or Hebrew.",
            "Or Vietnamese compared to our experience in English, simply the size of the webs there are smaller, so actually makes sense to take a query, let's say in Turkish and translate it to a different language before you do your image search.",
            "So we built this engine called pan images that uses this translation technology to do exactly that and let me just give you a sense of how it works.",
            "I'll do a query in English so you can click translate.",
            "And what it does is it uses this graph and inference over that graph and computes a set of translations to different languages and you can see that the notion of breakfast across different cultures.",
            "So a Japanese breakfast looks quite different than typical American ones, so this is query sent in parallel to Flickr and to Google or trying to be fair and you can see the images there.",
            "So that's a fun.",
            "A fun project at the touring center and you can play with this offline.",
            "It set pan images.org.",
            "A second project that has more of an AI flavor is we're asking, how do you accumulate massive amounts of knowledge from from the web age old AI problem?",
            "How do you build an AI system with an enormous amount of knowledge?",
            "And our hypothesis is well, if you can extract it from the web, that would be one way to build it, presumably more efficient than say what's happening in Psych where you have knowledge engineers hand entering that knowledge over what's been there more than 20 years.",
            "What I want to talk about today though, is our third effort, which is to attempt to investigate and sketch out a new paradigm for."
        ],
        [
            "Search so the outline of my talk.",
            "I'll talk about this new paradigm.",
            "What I mean by that I'll talk about our brand if you will, of information extraction, which we call open information extraction and and then I'll talk about the various ways we try to do inference over that extracted content."
        ],
        [
            "And I'll conclude.",
            "So the question that we start with is what is web search going to look like far into the future?",
            "OK, further than I think most of us usually think so I picked 2020 could be 20 thirty 2018, but let's think far ahead.",
            "What's it going to look like?",
            "Are we still going to be typing keywords into the search box?",
            "Maybe?",
            "Is it going to be a social type of thing is going to be running on top of Facebook is it going to be something like Mahalo or Cha Cha?",
            "That's our Yahoo answers.",
            "That's human powered.",
            "Or that social?",
            "Is it going to be the semantic web, right?",
            "Tim Berners Lee's vision.",
            "Are we going to get there?",
            "By 2020, what about the technology exponentials?",
            "In my little Rawr shack test?",
            "OK, how do they change the equation?",
            "How do we build search systems that use exponentially more powerful disks and machines?",
            "So these are good questions and I think the best answer is to borrow Alan Kay's line.",
            "The best way to predict the future is to invent it, and I think that's what most of us here, if not all of us are trying to do.",
            "And again, I'm going to focus on our."
        ],
        [
            "Our way of inventing this future so our hypothesis hypothesis is that instead of merely retreiving and indexing web pages, we can read them with some level of understanding.",
            "So what I mean by machine reading is some combination of information extraction, and I'll be clear about that term in a second, combined with some tractable inference.",
            "OK, so we're not talking about reading Shakespeare and understanding the nuances of poetry.",
            "But if machine sees the sentence.",
            "Paris is the capital of France.",
            "Why shouldn't it be able to realize?",
            "OK, that is a particular very well defined meaning about.",
            "Paris and France and the relationship between them.",
            "So in general information extraction of a sentence is extracting the basic facts.",
            "The basic relationships in that sentence, who did what, to whom and so.",
            "What you end up with some tuple like alone, Halevi is going to be the speaker at University of Washington and then a lot of the information you find is not explicitly stated in the text.",
            "So then you, if you ask the question well, will alone visit Seattle.",
            "Often we're sitting waiting with bated breath, is he coming or not?",
            "Presumably you could infer that from from this tuple if he's going to be speaking at UW, presumably he's going to be visiting Seattle and even some of the subtleties already starting to creep in here, right?",
            "UW?",
            "Maybe that's.",
            "University of Wisconsin.",
            "In which case he's not visiting Seattle, etc, etc.",
            "So there's a lot of technical difficulties in doing this kind of processing."
        ],
        [
            "So let's suppose you could do this kind of processing.",
            "So what?",
            "What is this by you above and beyond what we have today and is really actually a tremendous number of applications of this sort of technology.",
            "Let me focus on what I like to call information Fusion pulling together information from lots of different sources, lots of different textual sources to show you the potential if you can do this kind of extraction inference.",
            "So imagine you asked the question what kills bacteria if you go to Google.",
            "You'll get an answer.",
            "You go to Wikipedia, you'll get antibiotics, kill bacteria.",
            "OK, that's great, but there's actually hundreds if not thousands of things that kill bacteria, and different people talk about them in different contexts.",
            "And I'll show you a demo shortly of the kind of answer that we're able to get drawn from all over the web for this kind of question.",
            "Here's another one.",
            "What West Coast nanotechnology companies are hiring today?",
            "Again, I'm not aware of a single page on the web that has the answer to that, but if you go to a variety of pages, right, different companies, different company directory's, then you go to these companies, jobs, pages and so on.",
            "You can pull together and synthesize a good answer to that question, right?",
            "That's not something you get from a search engine today, but to something you could get from an information extraction engine.",
            "Even more topical, right?",
            "Let's say we want to compare Obama's buzz to Hillary's.",
            "And let's say we want to do that specifically for pages that were generated after his recent victory in Maine.",
            "How do we do that?",
            "That's not something a search engine gives you, right?",
            "We want to be able to go to a relevant set of pages, extract what people are saying him about him, and about her, and put that information side by side.",
            "And then the last example that has maybe more of an information integration flavor.",
            "Let's say I want a quiet, inexpensive four Star hotel in Vancouver, and maybe I'm going to get quiet out of trip adviser.",
            "You actually have to dig pretty deep to figure out which hotels are quiet and which ones aren't.",
            "If to read a lot of reviews and let's say inexpensive, I might get from a site like Farecast.",
            "We also have an hotel engine in a ranking of inexpensiveness four star.",
            "Maybe I'll get from the hotel website, but I need to go to all these different places and put the pieces together.",
            "So if you're an informed traveler.",
            "Can take you an hour to do to find the right hotel in a place where as a system like this could do most of the work automatically for you, reduce that to to maybe a minute or."
        ],
        [
            "So let me just show you one application that we did build on top of this kind of extraction and this is the part of the dissertation work of Anna Maria Popescu.",
            "She built an opinion minor that we called opine and the idea was to apply information extraction to product reviews right there.",
            "Highly informative there textual.",
            "There's lots of them.",
            "They are highly varied.",
            "Thank you.",
            "And what we wanted to do is summarize the reviews without any prior knowledge of the product category.",
            "So people have built one specifically for printers or specifically for hotels we wanted to build a general one with the idea of applying to arbiter reviews on the web.",
            "And again, there's a paper on this and we got nice precision."
        ],
        [
            "And recall results.",
            "Let me just give you a snapshot then I apologize for the threadbare interface.",
            "Here is just a way of displaying the information.",
            "It's not really an interface, but what you see there in black on the left are the various attributes that automatically extracted and you see that some of the names make a lot of sense, like staffon location.",
            "Otherwise, don't these are all automatically generated and then you see the opinion phrases it extracted, so you could tell at a glance this is off of 17 reviews of the.",
            "Particular hotel in New York and you could see at a glance that people felt pretty good about the staff.",
            "Seven people said excellent.",
            "Somebody said poor, that's probably somebody from the competition and somebody had a particularly bad experience.",
            "So you can really tell at a glance.",
            "The staff is great.",
            "The location is great, etc etc.",
            "If you're interested in drilling down more.",
            "So let's say I'm interested in whether the rooms are clean.",
            "I can actually click on the extraction and it opens the sentence.",
            "Is that the extraction was taken from?",
            "I can click on read more and actually go to the to the review so it gives me a way to slice and dice the information that allows me to drill down to the actual text, but also gives me this very handy summary.",
            "The one other thing that was nice about this system it not only did this kind of extraction, but then it took the different hotels and place them on a graph.",
            "So imagine a graph where the nodes are the different product, so then those are different hotels and then the edges are these different attributes like cleanliness and they're ordered by strength.",
            "So basically if I look in a hotel and I see well there are two reviews that say that the rooms are clean and I must, you know, very fussy.",
            "So I say, well I want a hotel that's cleaner than that that's rated as cleaner.",
            "Well, I can click in this user."
        ],
        [
            "Office and it will take me to another hotel where it realizes that Spotlessly clean is actually stronger than clean and fairly fancy mechanics that used to do that.",
            "So again what I'm trying to show here is that on top of this kind of extraction, with some inference layered in about say what's stronger, spotlessly clean or clean on top of that you can really build fairly sophisticated applications that are more powerful than the kind we see today today in Google.",
            "You type in a product name and it will happily collect for you a bunch of reviews, but it's up to you to you to sift through them and figure out the different attributes and how H1 compares to H2."
        ],
        [
            "Sign.",
            "OK, so that's so.",
            "Hopefully I've persuaded you that if we could do this well, particularly web scale, life will be good.",
            "Turns out the reading the web is a tough problem.",
            "First of all, traditional information extraction has been applied to very narrow, very precise information needs.",
            "The classical example is finding the location and the time from a corpus of CMU seminar announcements.",
            "OK, so it's a very homogeneous corpus, very clear and narrow request.",
            "And this is not going to scale to the web, so the 1st Order of business for us has been to scale information extraction to the web.",
            "It's hard because no parser achieves high accuracy on the full web corpus.",
            "It's hard because typically extraction relies on named entity taggers, right saying this is a location.",
            "This is a person.",
            "This is an organization.",
            "You can't assume that when you're dealing with arbitrary web text because the number of types that we're dealing with the different types of entities is arbitrary and unbounded.",
            "We don't even know in advance.",
            "What types of objects, what types of entities we're going to be dealing with, and Lastly, you can't do supervised learning in such a massive space.",
            "What are the concepts that you'd label?",
            "How many examples do you need right?",
            "It's just happening on a massive scale.",
            "You might say, OK, you can do supervised learning.",
            "What about semi supervised learning where we only need a small amount of data, hand labeled data, and then we leverage the unlabeled data that's available on the web."
        ],
        [
            "Well, it's true that in semi supervised learning you only need a small number of hand labeled examples, but that's a small number per concept, right?",
            "And the issue is that that place is an inherent limit on the number of concepts.",
            "Think about it.",
            "If I have a if I need let's say 20 and labeled examples per concept, and I have a million concepts all of a sudden I'm dealing with 20 million hand labeled examples and that starts to be too big.",
            "Furthermore, Semi supervised learning assumes that the concept is specified ahead of time.",
            "And again, because of the long tail, because of the diversity on the web, we don't know the concepts ahead of time.",
            "We don't know what the relations of interest are when we start, so we can't really do semisupervised learning either.",
            "So the approach that we've taken is what we call self supervised learning.",
            "The idea is that the learner discovers concepts on the fly and the learner automatically labels examples on its own, and I'll show you a little bit of how that's done and I'll refer you to to papers for more."
        ],
        [
            "The details.",
            "So really, the kind of extraction that we need to do that we call open information extraction open IE is quite different than the traditional kind.",
            "So here I put them side by side.",
            "Traditionally you have as input a corpus and some hand labeled data in open A.",
            "All we can afford is the corpus in.",
            "Traditionally the relations of interest are specified in in advance opening we discover them automatically.",
            "In traditional, either complexities, order the number of documents times the number of relations, right?",
            "We make multiple passes over the corpus, one for each relation.",
            "In open a it's not a function of the relations is constant in the number of relations, and again that's key because we could we have thousands or hundreds of thousands of relations that we're dealing with.",
            "And then Lastly the machinery for text analysis.",
            "In traditionally, is pretty heavy, right?",
            "There's a parser.",
            "There's a named entity tagger.",
            "All we use an opening is a noun phrase chunker, which is fairly robust."
        ],
        [
            "OK, so how does the extractor work?",
            "And this is the work of Michel Bank Owen.",
            "A recent paper.",
            "The basic idea is we start with a very simple model of relationships in English.",
            "If you take a step back and think about it, we don't have to have a relation specific extractor to a first order.",
            "This is oversimplified, but to 1st order you can say verbs tend to capture relationships in English, right?",
            "So Thomas Edison invented the light bulb, invented is meant to capture the relationship and we have a subject and an object so we can start with a very simple model and label ourselves a whole bunch of.",
            "Negative and positive examples of extraction and then we can bootstrap from that.",
            "A much more fine grained model and that's that's what we've done and the the more general model is encoded as a conditional random field.",
            "And then the next step is you decompose each sentence you come across into a noun.",
            "Phrases and verb phrase into chunks, and then you go through those and identify exactly the entities and the relation expression using the conditional random field.",
            "So the key to this is that this process, while it's language specific.",
            "OK, we did this for English.",
            "We haven't tried this for other languages.",
            "We believe that you could try it for other languages, but we haven't.",
            "The key here is that it's a relation independent extractor, which is very different than."
        ],
        [
            "Previous work in this area so, So what does this look like for text runner?",
            "Well, we tend to extract triples representing binary relations.",
            "We've actually extended it recently to Enoree relations, but let me stay with binaries, so we're going to have argument one the relation, and then argument too.",
            "So given a sentence like Internet powerhouse, eBay was originally founded by Pierre Omidyar.",
            "What's going to happen is it's going to figure out?",
            "OK, eBay is 1 entity here.",
            "The relation is founded by pure, mature is the other argument.",
            "And that's the triple that we're going to extract from the sentence.",
            "And again, you can immediately think of lots of examples that are much trickier, right?",
            "50 word sentence is.",
            "And most of these 50 word sentence is we're just going to leave behind.",
            "Either will take a simple piece of them and extract from that.",
            "Or we might say, you know what we can't work on such a complicated sentence.",
            "That's OK.",
            "I've got 100 billion other sentences to work with, so that's one of the nice things about working with."
        ],
        [
            "The web corpus.",
            "There's still a ton of challenges here, and this is some of the work that we've done, and again, I'll just hint at some of them.",
            "You have to drop nonessential information so it was originally founded by goes to founded by.",
            "If you don't drop this kind of information, you're going to have a ton of relations that you think are different, but really all mean the same thing, so it's important to do some compression here.",
            "As you're doing compression and dropping information, it's still important to retain key distinctions, right?",
            "So you can't just say I'm going to look at the verbs, right?",
            "eBay, founded by Pierre, is not the same as eBay founder Pierre, Right?",
            "The word by there is actually critical, so so we need to know that, by the way, a bunch of relationships are not actually captured by verb, so George Bush, president of the United States, right?",
            "There's a relationship there.",
            "It's not a verb, and we also have to think a lot about problems of synonymy and aliasing.",
            "So we need to realize that Albert Einstein.",
            "Is the same as Einstein, but not the same as Einstein brothers was a leading bagel manufacturer on the on the web."
        ],
        [
            "OK, so here's how we put the pieces together.",
            "Text Runner, which is our open a system I'm going to demo to in a second.",
            "It has a self supervised learner that learns an extractor, then that extractor makes a single Passover.",
            "The corpus just goes sentence by sentence over over webpages.",
            "Extracts as much as it can from each sentence and then it indexes it.",
            "We put it into Lucene and now we have something that.",
            "Can answer queries at close."
        ],
        [
            "Close to interactive speeds, so let me show you.",
            "Hopefully my demo.",
            "Will work here over the Internet and what I'm showing you here.",
            "Is iaccessible for my homepage or just type in text runner into your favorite search engine?",
            "You would find this.",
            "So this is publicly available.",
            "You can play with."
        ],
        [
            "Obviously I'm going to show you a good example.",
            "You know it.",
            "It's it's.",
            "Performance is variable, but to give you a sense of the potential of this kind of thing and this is run over about 120 million web pages of varying quality.",
            "We have Wikipedia in there.",
            "We've got some pretty random pages, so again, this is just a proof of concept, so let's ask it what kills bacteria as a simple question processor that basically tells it OK, we want kills as the relationship and bacteria is the object not telling me what you find an we see a variety.",
            "Of answers here and here, it says 175 more, so I'm going to click on that.",
            "And the."
        ],
        [
            "And open it up and you see the various things that found that killed bacteria from the.",
            "From from the obvious ones like antibiotics and if you look below we see chlorine heat amoxicillin pasteurization.",
            "They start to get more obscure like garlic, alcohol, honey and if you're not sure that you believe in it, you can click on the number here and see the different sentences that it came from or let me do it here so it's more visible, right?",
            "So you see different senses you can click through and actually go to the web pages and then another nice thing is you see there's a lot of compression going here underneath the scenes so.",
            "All these different ways of referring to antibiotics.",
            "We're all compressed to realize is really just saying antibiotics, so you're not swamped with a bunch of different answers that really mean.",
            "The same thing.",
            "OK, so.",
            "So that gives you a flavor of how tannic text runner works, and.",
            "Usually I open it up, but since everybody has their laptops here you can you can play with it yourselves and again this this prototype is not particularly optimized.",
            "It takes, you know 10 to 30 seconds to give you an answer.",
            "We could easily make that be a lot faster with just some elbow grease.",
            "There's nothing inherently slow about the processing at.",
            "At query time, right?",
            "The hard work happened at compile time."
        ],
        [
            "The crawler, so to give you a more quantitative sense, we took a sample of 9 million web pages that contained about 11.3 million triples and using samples we assessed OK. How many of these are actually any good and we found that of the 11.3 million there about 9.3 million with a well informed relation.",
            "And of those there are 7.8 million that had both well formed relations and well informed entities.",
            "There were meaningful.",
            "And then what we find that of these there were six point 8 million that contained.",
            "Abstract observations things like fruit contain vitamins.",
            "Those might be good for ontology building various activities like that and then they were on the order of a million.",
            "There were concrete facts like Oppenheimer taught at Berkeley and you see that the precision levels that we're achieving in here are actually pretty high, right?",
            "So if you type in Oppenheimer to text runner, you're going to get a mixture of abstract facts and concrete facts, but with fairly high accuracy.",
            "We tend to sort them by frequency and so the ones that are more often repeated more likely to be correct filter."
        ],
        [
            "Up to the top.",
            "OK, so so that's extraction and now what I want to talk about is OK if you have this kind of extraction, what can you do on top of that?",
            "What kind of inference you can do, because again initially we got into this and we solve these billions and billions of senses.",
            "We thought great, anything you could ever possibly want to know is in there.",
            "But in fact, what we've realized overtime is a lot of the information that you want is actually implicit in some form, and so I'll talk about three kinds of inferences that.",
            "We and other people have done over extraction and those are entity and predicate resolution, and this is the old problem, right of deduping has lots of different names, But the basic idea is to do this kind of compression and realize the two different sentences are actually talking about the same object.",
            "And then I'll talk about how we assess the probability of correctness of a sentence.",
            "That's pretty important.",
            "And Lastly, I'll talk about our most recent work.",
            "How do you put 2 + 2 to get 4?",
            "How do you compose different facts you see on the web to draw a conclusion that's not explicitly stated at all?"
        ],
        [
            "So let's start with entity resolution.",
            "Again, this is part of the dissertation of Alex Yates.",
            "Our basic idea is that we can determine the two objects are likely to be the same one based on the relationships we find the text runner is found and this is related to earlier work by Pentel and Lynn.",
            "There's some technical differences, but it's not particular important right now, so the basic idea is if I have a bunch of these triples about X, and I know that X was born in 1941, is a citizen of the US is a friend of Joe.",
            "And I have another entity, let's call it M, where some of these facts are true, but M is also a friend of Marys.",
            "Basically, the observation is that the probability that X&M are the same is some function of the number of shared relations, right?",
            "And we can make this a lot more sophisticated.",
            "We can look at functional relationships, right?",
            "So if X&M have the same spouse, then that further increases the likelihood that they are the same person.",
            "And so on and so on.",
            "So basic relational model.",
            "Nice thing about this again is this is completely generic.",
            "We don't need hand labeled data, we don't need specificity to a particular domain.",
            "This is for arbitrary entities in the web corpus, and in fact we have."
        ],
        [
            "Resolve are on our entire corpus.",
            "Another elegant thing here is you can do the dual of this and apply to relations.",
            "So here's a schematic example.",
            "I have two relations are in are primed and if they both apply between one and 2, two and four etc etc.",
            "I'm going to conclude that the probability that the two relations are the same is again a direct function of the number of shared argument pairs, and in fact there's a natural mutual recursion here between these two parts of the algorithm, right as you discover.",
            "The more entities are the same.",
            "You can then use that to discover the more relationships are likely to be the same and you can go backwards and forwards, and so we have here.",
            "This unsupervised probabilistic model.",
            "The algorithm here is actually order N log N, so it's not entirely linear, but still easy to run on millions and 10s of millions of docs."
        ],
        [
            "OK, so next one probability of correctness.",
            "So how likely is it that an extraction that we get is is correct and that's kind of a subtle question?",
            "Even what does correct mean this correct mean true?",
            "Or does it mean a true truth preserving transformation on the original sentence, right?",
            "So if I see the sentence you know, JFK was killed by Elvis?",
            "What exactly am I supposed to extract from that sentence, right?",
            "And we've mainly focused on an truth preserving notion of correctness, right?",
            "We're going to be trying to do an accurate reflection of what's in the text, and there are a lot of factors to consider if you do this analysis.",
            "There's the authoritativeness of the source.",
            "There's your confidence in the extraction method.",
            "What I'm going to show you now is how we utilized the redundancy in the web corpus we ask.",
            "OK, what's the number of independent extractions that you've gotten, and how is likelihood of correctness of function of of the?"
        ],
        [
            "Parameter.",
            "And this idea of counting extractions is actually not a new idea.",
            "If you start with lexico syntactic patterns, which is an old idea, goes back to Marty Herst back in 90 two.",
            "The idea is you have a sentence.",
            "Cities such as Seattle and Boston and that contains a clue right that Seattle and Boston are cities, right?",
            "Such as their means that we have some class in Seattle and Boston are members of the class.",
            "Well, those kinds of patterns.",
            "Our hints as to the meaning of the sentence they suggest extractions.",
            "Now Peter turning back in 2002, an ACL paper developed the PMI.",
            "Our algorithm.",
            "The basic idea there was that he could take pointwise mutual information which we can just approximate as Co occurrence frequencies of different strings.",
            "His idea was you can estimate those from the number of results you get from a search engine OK and then based on the number of results you can compute.",
            "Confidence of membership of the class.",
            "So basically what this means is we're going to go to our favorite search engine and type in a phrase.",
            "For example, cities such as Seattle and we look at the number of results that comes back in.",
            "If that number is higher than it is for some other string.",
            "I don't know cities such as avocado, then we're going to be more confident that Seattle is a city than that avocado is a city.",
            "And if you play these kinds of games and lots of people, have you find that these Co occurrence frequencies are actually very telling.",
            "About what's what among."
        ],
        [
            "Strength, so the next step is we actually formalize this, and this gets very complicated to hurry, but you can actually ask the following precise question if you have some extraction X and it occurs K times in a set of N distinct sentences, and each of these sentence has this kind of lexico syntactic extraction pattern, so each of them is basically suggesting that X is a member of some class, what actually the probability that X belongs to the class, and again by class I mean something like cities or can be.",
            "Strings that belong to relation like mayor of a city.",
            "And by the way, one really important thing here is that we only count in this, and that's what I'm trying to highlight there.",
            "This thing sentences right if you have.",
            "There's a lot of copying on the web and we're aware of that obviously, and so you don't want to be counting the same sentence is just because it's been copied lots of times.",
            "But if we."
        ],
        [
            "Count distinct sentence is.",
            "It turns out that we can develop a commentarial model.",
            "We called the urns model, 'cause it's a balls and urns type of model.",
            "I won't go through into the formal details, but I'll just show you the the punchline.",
            "Here we can actually have a close form expression 4 if X appears K times in an draws of balls from the urn.",
            "In other words in it appears K times as an extraction from out of an extractions that we conclude that the probability is this.",
            "Nasty looking expression.",
            "I haven't even defined the notation here.",
            "I'm not expecting you to read this, I just want you to be impressed by our our equations and I'll refer you to the paper for for the details.",
            "If you do look at it for a second though, you see that it does behave in sort of reasonable ways.",
            "You see that the odds here are the probability do increase exponentially with K right?",
            "In other words, the more times.",
            "We see Seattle being appearing in a sentence.",
            "Cities such as Seattle, we become exponentially more confident in the fact that Seattle is a city.",
            "The odds also decrease exponentially with the number of extractions.",
            "So if I saw Seattle being extracted as a city five times, the question is 5 times out of 10 sentences or five times out of 100 million sentences, right?",
            "That's a very different probability.",
            "So this expression is actually an interplay between these two exponentials, and it turns out to probably give.",
            "The right probability now, why do we care about the fancy math here?"
        ],
        [
            "Well, if we compare the performance in practice to previous approaches so so this graph is showing the urns model compared to attorneys pointwise mutual information based model an compared to noisy or which is another model that was previously used in extraction work and you see this is for a sample for relations.",
            "He ran on the.",
            "Y axis you have deviation from ideal log likelihood, so this is a log scale and lower here is actually better, right?",
            "How far we from from the right answer and you see that in dark black we have the urns model very close to the right answer and compared to the other ones we're getting an order of 1500% improvement or more.",
            "So this is much, much, much better than than previous work, and again no hand labeled data completely domain independent.",
            "In other words, methods that scale to trying to tackle the full web."
        ],
        [
            "Now it turns out that this urns model works great when you have lots of information.",
            "When you have this kind of redundancy.",
            "So if we want to know whether we believe the Michael Bloomberg is the mayor of New York, that kind of thing tends to be correct.",
            "And if you play with text runner, you'll see that the statements that it extracts.",
            "At the top of its results are often correct, but when things get sparse, right, well familiar with this load distribution we have on the order of 50% of the extractions, there only occur once, and then what do you do with that?",
            "And you find that in those there's a mixture of statements that are rather obscure, but correct like Dave Shavers, the mayor of some small town called Pickerington, and also statements that are blatantly false like Ronald McDonald, is the mayor of Mcdonaldland where.",
            "We all know it's.",
            "I'm sorry.",
            "Thank you, I appreciate that.",
            "So maybe not everybody knows that, but it's actually running.",
            "This is not the mirror McDonald, so there's don't say you didn't learn anything from my talk, right?",
            "So so how do we tease these apart, right?",
            "How do we tease the?"
        ],
        [
            "The good ones apart from the bad ones and in more recent work by dug down and stuff show markers.",
            "What they said is instead of just looking at these extraction patterns, these lexico syntactic patterns, why don't we, when we're considering an entity look at all the context that it occurs in so affectively?",
            "If we're wondering, is Shaver Amayreh?",
            "Why don't we ask the question?",
            "Does he behave like a mayor and what does it mean?",
            "Does he behave like a Mary means if we take all the sentence is that Shaver appears in.",
            "Do we see statements like he was elected and you know he blocked the motion and he attended the City Council meeting etc etc.",
            "Right so if we see him in the presence of the right kinds of strings then we're going to be more and more likely to believe that he is the mayor and the same for printing 10 if we see streets oven mayor of Pinkerton?",
            "Going to more likely believe it's a city.",
            "Now.",
            "This is not an explicit computation.",
            "What we do is we build a language model and in fact the language model is captured by a hierarchical Markov model.",
            "It's built once per corpus, and what that model does is it really takes the entire corpus and projects it into a 20 dimensional space.",
            "A lower dimensional space, an what happens then is each string is a point in this lower dimensional space, and what you can do is you can measure distances between these strings and these distances are really referring to there.",
            "Behavior in context to the statistical properties of their behavior in context.",
            "Again, I'm glossing over a lot of technical detail here to give you the flavor, so the idea is if I'm trying to figure out if Pinkerton is a city I'm going to map it into this space and I'm going to ask what's the proximity between Pickerington and other cities that earns the urns model tells us are known cities like Seattle and Boston and so on, so using a model like this I can tell the Pickerington is relatively likely to be a city, certainly compared to other strings.",
            "OK, so again more of the details in the paper, but the point is by using language models over the corpus we can handle the more Sparks cases as well and that's."
        ],
        [
            "That's very important.",
            "OK, so let me turn to the last example of inference and this is compositional inference, and this is work very much work in progress, so I'll just give you 1 slide on this.",
            "Again, our idea here is to get an implicit information and what we're looking for is very short inference chains.",
            "OK, we're not going to prove Fermat's last theorem using the simple mechanism.",
            "I'm bout to show you, but we would like to get some basic facts that aren't in the corpus.",
            "So let's say text Runner notes that Turing was born in London.",
            "And let's say we know from word Net could get this from text renders well, easier to get out of London.",
            "London is a part of England.",
            "And Furthermore, let's say we have a rule that says born in is transitive through part of what that basically means is we can conclude the Turing is born in England OK even though the corpus might not say this and in fact in the 120 million web pages that are in the text runner.",
            "Corpus it says that he was born in London, but doesn't say that is born in England, right?",
            "So if you just did a simple question answering thing and ask it.",
            "Hey was during born in England, it will say no.",
            "He was born in London.",
            "So that's not the behavior that we want out of these systems.",
            "So what the mechanism that we built does is, it instantiates a Markov logic network on the fly based on the extraction.",
            "Then it knows about based on rules learned from the corpus.",
            "What I'm about to show you actually the rules were coded by hand, the automatic learning of rules is actually future work, but we get an inference simple system that hopefully I can show you here.",
            "If I can find my mouse.",
            "And again, so you can ask it a question like was Alan Turing born in England and the first thing it says?",
            "Well, I don't have any direct evidence for that zero piece of evidence in the corpus, but it instantiates it's MLN and it comes to conclusion well, with some probability, I do believe that he was born in England, and if I click on the number I can see the again this chain of reasoning that led to that conclusion, and it's doing some probabilistic computation over that.",
            "And again, this could be optimized further, so we're seeing here really over hundreds of millions of assertions, some very simple inference to get beyond what's explicitly stated in the text.",
            "And again, everything here the probability values that are in green and the different modes of reasoning everything can be refined.",
            "I just urge Steph to put something together just so I could show you something brand new and this is I would refer you to the paper on this, except we haven't written it yet, so.",
            "I can't quite do that.",
            "Whoops"
        ],
        [
            "OK, so.",
            "Here's excuse me the family tree.",
            "The different systems that I talked about here are in green.",
            "Some systems that we built that are fun that I didn't talk about are in red.",
            "We were influenced to start this node all project by three systems that I should mention.",
            "One is Tom Mitchell's project at CMU, the web project, which again very much shares this motivation of trying to get knowledge from text or a lens terminology.",
            "It crossing the structure chasm.",
            "Very important challenge for for this kind of work also by attorneys PMI our algorithm and then also by we had one of the early question answering systems from the web system back in 2001 was called Mulder.",
            "You would ask it a question.",
            "It would go through the search engines and get you your answer and this was a one off system and we realized instead of doing this kind of question answering thing one at a time, why don't we just go and apply the kind of extraction machinery we can too.",
            "Every sentence on the web, and something that in fact we're very excited about, is exactly that.",
            "Scaling text runner from the 120 million pages to the full web corpus, and we believe that if you do that, as you often see with these things with two orders of magnitude more data, the quality of what you get will go up substantially."
        ],
        [
            "So I want to acknowledge the team a number of people there dug down his name is in red.",
            "Aside from his great contributions.",
            "He's also on the job market today, so if people are interested they should contact him directly.",
            "He's a great guy."
        ],
        [
            "In"
        ],
        [
            "And let me just I want to leave time for questions.",
            "So let me just end with again the high level conclusion.",
            "So what I'm asking you to think about is search systems that operate over a much more semantic space than today's search system.",
            "So instead of keywords and documents, we can have these kinds of extractions instead of TF IDF fan page rank.",
            "We can have these relational models like the one I showed you that resolve are was using and instead of web pages and hyperlinks we can think of.",
            "Entities and the relationships between them, and I guess I shouldn't even say instead of right?",
            "I mean, these two spaces can coexist side-by-side, right?",
            "You could ask, could we use extractions relational models to help build a better ranking function, right?",
            "Could we use them to do better spam detection?",
            "Could we use them alongside?",
            "Imagine some model where somebody showed me a startup that's trying to do this recently where you have the pages, but the different entities are highlighted on the side and you can click the same way you can click.",
            "Show me similar pages you can click on the entity and say show me more pages about this entity, right?",
            "So these are not opposing but complementary, and what I hope I've persuaded use that this is not a pipe dream or even a long term super ambitious vision like Tim Berners Lee's vision of the Semantic Web.",
            "This is something that we have prototypes of today, and other people are building even even more advancements as we speak, so this notion of reading the web.",
            "Suggests a new search paradigm."
        ],
        [
            "Thank you.",
            "So one question.",
            "So did these techniques are interesting and can give quite impressive results.",
            "But let's say this is still corpus level of understanding of the triples usually, so you have the statistics page before roughly one triple per page or even.",
            "A pervert picture or even less so.",
            "Oh, how do you imagine that to the future?",
            "In on the going down to document level understanding.",
            "So where you would get?",
            "I don't know 10s of triples or tense effects per page which would be reliable, contextualized, and everything else which which needs to be done basically to deal with this information.",
            "So factually we get on the order of 1.5 extractions per sentence on average.",
            "So in fact I think we've already.",
            "Achieve part of what you're saying, which is we are very much working at the sentence level and getting extractions.",
            "However, you're right that there's a lot more work needs to be done, so you said contextualize.",
            "Then you said reliable and those are very good points.",
            "We ignore the context and we do that at our Pearl, so if you have a web page entitled you know common misconceptions about the topic will happily dive into that an extract the misconceptions and believe them as if they were true, so.",
            "And there are all kinds of even context within the sentence.",
            "If you have a sentence that says, I believe that London is the capital of the United States, or I don't believe that London is the capital of United States, we're likely to get that.",
            "London is the capital United States, so there's a lot of issues there now.",
            "The massive redundancy really helps here.",
            "OK, so the that London is the capital United States in those kinds of weird sentences is swamped by other sentence.",
            "Is that say that what the capital actually is, right?",
            "So that's part of it.",
            "But there is a lot more work to be done.",
            "On UN better extraction and better inference from my point of view.",
            "As a professor, that's good news, because you know smart students are walking into my office and we have lots to work on.",
            "I'm not shipping a product.",
            "Yeah, urns model does very well in there like 15 times better than the previous model.",
            "Then for the sparse data you mentioned that language models to the rescue.",
            "So what?",
            "Would you expect like if you use the language models, could you further improve this orange model?",
            "So we've done some measurements on that.",
            "Again, I didn't have time to get into it 'cause it gets rather involved, but there are in the ACL paper.",
            "Basically, we've found that on sparse extractions it increases the precision at a fixed level of recall, so precision at the top 20 to be precise.",
            "By 90% so it really improves things a lot on sparse extractions.",
            "So, so you mentioned that in the case of facts that are incorrect, you rely on the fact that there are a large number of sentences, so it's probably more correct ones than incorrect ones.",
            "So in terms of what if you have facts that are correct at different points in time, so you mentioned Bloomberg as the mayor, right?",
            "So maybe the day after a new mayor is elected, there's still more sentences that talk about Bloomberg as a mayor?",
            "Yeah, that's a very good point.",
            "Along a number of dimensions, the first one is that this notion of redundancy.",
            "It's a very powerful tool, but it's a heuristic.",
            "OK, it's often wrong, so if there's a conspiracy theory that's very popular.",
            "Again, text Runner will will happily believe that, right?",
            "So first of all, it can easily be wrong, and again, there's countervailing forces, like if we look at authoritativeness of a source and so on.",
            "But the point that you brought up in particular temporally scoped facts and so.",
            "Text Runner is not very good with that, but that's not a function of the paradigm.",
            "OK, we just need to invest the time to temporally scope the facts, and that's nontrivial because often the temporal scoping is not in that sentence, so we really need to extend extraction to go beyond single sentence to put in like you were saying.",
            "Context an, for example, to associate.",
            "You can think of this as metadata with every sentence.",
            "Is it always true?",
            "Is a true over a particular time interval, and so on and so on, and so that's very much.",
            "A problem for future research that we haven't solved.",
            "Hi, you made a really interesting comment near the end of your talk where you said if we had a couple more orders of magnitude of data then you'd see that the performance would be just hugely more and you also had a graph a little bit before that really showed that you have a lot of common things that are repeated very often in this huge tail.",
            "So if you went up a couple orders of magnitude than your data is much more sparse.",
            "The question I have is do you have a sense or have you actually measured?",
            "What's the critical mass of information you have to be able to answer certain kinds of questions and.",
            "How do you know that?",
            "How much do you really need and what really is going to happen if you have a couple orders of magnitude more data?",
            "So those are great great questions, so let me see if I can take him first of all, of course, you're right that however much more data we have, we're still going to have that that long tail.",
            "Think that's why the work on sparse extractions is interesting and relevant.",
            "We didn't just say, Oh well, let's just increase our data set, but that is still a problem.",
            "And then to your other question.",
            "Well, OK, how well does this thing actually work and how well would work if we have two orders of magnitude more data?",
            "The thing that's hard about that is there isn't.",
            "Another iterative data set.",
            "How do you know?",
            "And you definitely don't know what's going to happen if you have two orders of magnitude more data.",
            "So the approach we've taken is to put our prototype up there on the web for people to play with.",
            "We've done systematic studies with small numbers of relations and with samples, and I alluded to some of those results, but the truth is.",
            "I give you a succinct answer to your question.",
            "I don't know.",
            "But I was thinking is that if you put 10 machines to do this thing for a month, you can build a database of fact based on the size of psych very easily.",
            "Why have you not been able to build it?",
            "And if you have, what are the results?",
            "I mean yeah, great question that let me pose a slightly different question.",
            "I think you're basically asking how does this compare with psych, right?",
            "And the answer is that already.",
            "This is much larger than psych in terms of the numbers of assertions.",
            "At the same time, it's also quite a bit noisier than side 'cause as you can imagine, right, there's all kinds of.",
            "Incorrect facts, I think that also the they've spent a huge amount of time worrying about the semantics of cycle.",
            "An inference over psych and what we're dealing with those textual strings that again are noisy and the semantics are not clear right.",
            "Contradictions happily coexist inside of text runner, so I would summarize this by saying this is much, much cheaper, much more scalable right we spend.",
            "A couple of years building text runner on a shoestring.",
            "Compared to Sykes, budget and it's higher recall but lower precision.",
            "And of course there's a challenge with Psych to actually get some interesting applications of any kind built on top of that, where as we've already at least demonstrated things like opine and so on that that work.",
            "And I think you'll see more applications coming out of our group.",
            "But that's really the the tradeoff here.",
            "Right now.",
            "This is precision, recall, and cost, and we were higher.",
            "Recall more scalable at much lower cost.",
            "But lower precision.",
            "So you mentioned earlier talk the idea of applying this to the web and you know integrating it with search engines to augment the search experience.",
            "I really like that idea, but from what I've heard I don't work for any of the major search engines, but the common wisdom is the user is sort of too lazy to do anything really beyond type in two or three words and their query.",
            "Do you have any thoughts about how you might once you have all this good data how you might present it to the user in a way to make the user actually really makes use of it?",
            "You know the HCI or user interface issues around making good use of this data.",
            "So that's a great question and I have a couple of ideas.",
            "Is not something I spend a huge amount of time thinking about, but I will say one is you do have to wonder about a statement of the form the user and I mean I know that people very much say that right?",
            "But I mean we're talking about billions of people, so there's probably classes of users with different behaviors, so some might be more motivated to do some work than others.",
            "1 #2, as I was trying to say that there are a number of things that.",
            "Happened behind the scenes OK. For example, you could use these extraction models as features in a ranking function, right?",
            "So it doesn't change user behavior, but to me actually the most exciting thing is supporting an incredibly lazy user by giving them high quality information that they can't get today.",
            "So again, let's take this opinion mining or review mining example right now to get the information I want about hotels takes me literally an hour.",
            "If you can do this kind of information Fusion information synthesis.",
            "On top of extractions it could take a couple of minutes to figure out.",
            "Can I find a quiet hotel in Vancouver by extracting information from the reviews so I actually think that the most exciting use of this is extraction is to allow you to ask questions and to answer questions.",
            "The right now require huge amount of manual labor so let's help the lazy OUSD users.",
            "The best computer scientists right?",
            "This is a lazy computer scientist.",
            "More questions.",
            "Well, let's thank the speaker."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now love.",
                    "label": 0
                },
                {
                    "sent": "We're very pleased to welcome Professor or an engineer from the University of Washington who is the founder and director of the university's touring center.",
                    "label": 1
                },
                {
                    "sent": "His Atripla fellow and the founder of several successful startups including Hamlet to buy or not to buy.",
                    "label": 0
                },
                {
                    "sent": "That's the question.",
                    "label": 0
                },
                {
                    "sent": "And he's interested in solving fundamental problems in the study of intelligence web search, machine reading and data mining.",
                    "label": 0
                },
                {
                    "sent": "So thanks for inference accepting an impression.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you, it's a pleasure to be here.",
                    "label": 0
                },
                {
                    "sent": "Hamlet is now called Farecast.",
                    "label": 0
                },
                {
                    "sent": "I don't know if that's an improvement, but I won't talk about that.",
                    "label": 0
                },
                {
                    "sent": "Let me start here.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Can anybody tell me what this is?",
                    "label": 0
                },
                {
                    "sent": "Thank you, it's a Rorschach test.",
                    "label": 0
                },
                {
                    "sent": "What I've done is I've given.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As a Rorschach test for computer scientists.",
                    "label": 0
                },
                {
                    "sent": "So if you look at this picture, you have to ask yourself what's that is that a?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Depiction of Moores law.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Storage cap",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "City number of web pages.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Facebook users?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Etc.",
                    "label": 0
                },
                {
                    "sent": "We live in a in an exponential world and the Turing Center is meant to investigate a set of questions around search natural language.",
                    "label": 0
                },
                {
                    "sent": "In that context, I'll just say a couple of words about the Turing Center and then I'll get to the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The topic of my talk.",
                    "label": 0
                },
                {
                    "sent": "So we have 3 research foci.",
                    "label": 0
                },
                {
                    "sent": "The first one is to scale machine translation to the large number of languages on the order of 7000 languages spoken on Earth.",
                    "label": 0
                },
                {
                    "sent": "That means 49 million language pairs standards, statistical machine translation technology that doesn't scale to this number of language pairs.",
                    "label": 0
                },
                {
                    "sent": "The reason is that they rely on aligned corpora, which are manually generated and you can't have this many align corporate scale.",
                    "label": 0
                },
                {
                    "sent": "What we've done is looked at an easier problem.",
                    "label": 0
                },
                {
                    "sent": "Lexical translation mapping words from one language to another, with the idea that we could do that for a much larger set of languages.",
                    "label": 0
                },
                {
                    "sent": "And so we've built something we called the translation graph.",
                    "label": 1
                },
                {
                    "sent": "Right now it has on the order of 2.5 million words as nodes and edges indicate translation between one word and another in a particular sense, and the question that we ask in that project is the inference.",
                    "label": 0
                },
                {
                    "sent": "Question is if you start in a word, say in Vietnamese, and you can translate it to French and from French.",
                    "label": 0
                },
                {
                    "sent": "Let's say you can go to Catalan.",
                    "label": 0
                },
                {
                    "sent": "Well, is that translation path is that truth preserving?",
                    "label": 0
                },
                {
                    "sent": "In other words, what's the probability that if you go from Vietnamese all the way to Catalan, you end up with the same word that you started with?",
                    "label": 0
                },
                {
                    "sent": "So interesting problem, let me just show you a quick search application built on top of this lexical translation.",
                    "label": 0
                },
                {
                    "sent": "One of the things we realized is that image search experience is not nearly as good if you speak a relatively small language like Turkish or Hebrew.",
                    "label": 0
                },
                {
                    "sent": "Or Vietnamese compared to our experience in English, simply the size of the webs there are smaller, so actually makes sense to take a query, let's say in Turkish and translate it to a different language before you do your image search.",
                    "label": 0
                },
                {
                    "sent": "So we built this engine called pan images that uses this translation technology to do exactly that and let me just give you a sense of how it works.",
                    "label": 0
                },
                {
                    "sent": "I'll do a query in English so you can click translate.",
                    "label": 0
                },
                {
                    "sent": "And what it does is it uses this graph and inference over that graph and computes a set of translations to different languages and you can see that the notion of breakfast across different cultures.",
                    "label": 0
                },
                {
                    "sent": "So a Japanese breakfast looks quite different than typical American ones, so this is query sent in parallel to Flickr and to Google or trying to be fair and you can see the images there.",
                    "label": 0
                },
                {
                    "sent": "So that's a fun.",
                    "label": 0
                },
                {
                    "sent": "A fun project at the touring center and you can play with this offline.",
                    "label": 0
                },
                {
                    "sent": "It set pan images.org.",
                    "label": 0
                },
                {
                    "sent": "A second project that has more of an AI flavor is we're asking, how do you accumulate massive amounts of knowledge from from the web age old AI problem?",
                    "label": 1
                },
                {
                    "sent": "How do you build an AI system with an enormous amount of knowledge?",
                    "label": 0
                },
                {
                    "sent": "And our hypothesis is well, if you can extract it from the web, that would be one way to build it, presumably more efficient than say what's happening in Psych where you have knowledge engineers hand entering that knowledge over what's been there more than 20 years.",
                    "label": 0
                },
                {
                    "sent": "What I want to talk about today though, is our third effort, which is to attempt to investigate and sketch out a new paradigm for.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Search so the outline of my talk.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about this new paradigm.",
                    "label": 1
                },
                {
                    "sent": "What I mean by that I'll talk about our brand if you will, of information extraction, which we call open information extraction and and then I'll talk about the various ways we try to do inference over that extracted content.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'll conclude.",
                    "label": 0
                },
                {
                    "sent": "So the question that we start with is what is web search going to look like far into the future?",
                    "label": 0
                },
                {
                    "sent": "OK, further than I think most of us usually think so I picked 2020 could be 20 thirty 2018, but let's think far ahead.",
                    "label": 0
                },
                {
                    "sent": "What's it going to look like?",
                    "label": 0
                },
                {
                    "sent": "Are we still going to be typing keywords into the search box?",
                    "label": 0
                },
                {
                    "sent": "Maybe?",
                    "label": 0
                },
                {
                    "sent": "Is it going to be a social type of thing is going to be running on top of Facebook is it going to be something like Mahalo or Cha Cha?",
                    "label": 0
                },
                {
                    "sent": "That's our Yahoo answers.",
                    "label": 0
                },
                {
                    "sent": "That's human powered.",
                    "label": 0
                },
                {
                    "sent": "Or that social?",
                    "label": 0
                },
                {
                    "sent": "Is it going to be the semantic web, right?",
                    "label": 0
                },
                {
                    "sent": "Tim Berners Lee's vision.",
                    "label": 0
                },
                {
                    "sent": "Are we going to get there?",
                    "label": 0
                },
                {
                    "sent": "By 2020, what about the technology exponentials?",
                    "label": 0
                },
                {
                    "sent": "In my little Rawr shack test?",
                    "label": 0
                },
                {
                    "sent": "OK, how do they change the equation?",
                    "label": 0
                },
                {
                    "sent": "How do we build search systems that use exponentially more powerful disks and machines?",
                    "label": 0
                },
                {
                    "sent": "So these are good questions and I think the best answer is to borrow Alan Kay's line.",
                    "label": 0
                },
                {
                    "sent": "The best way to predict the future is to invent it, and I think that's what most of us here, if not all of us are trying to do.",
                    "label": 1
                },
                {
                    "sent": "And again, I'm going to focus on our.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our way of inventing this future so our hypothesis hypothesis is that instead of merely retreiving and indexing web pages, we can read them with some level of understanding.",
                    "label": 1
                },
                {
                    "sent": "So what I mean by machine reading is some combination of information extraction, and I'll be clear about that term in a second, combined with some tractable inference.",
                    "label": 1
                },
                {
                    "sent": "OK, so we're not talking about reading Shakespeare and understanding the nuances of poetry.",
                    "label": 0
                },
                {
                    "sent": "But if machine sees the sentence.",
                    "label": 0
                },
                {
                    "sent": "Paris is the capital of France.",
                    "label": 0
                },
                {
                    "sent": "Why shouldn't it be able to realize?",
                    "label": 0
                },
                {
                    "sent": "OK, that is a particular very well defined meaning about.",
                    "label": 0
                },
                {
                    "sent": "Paris and France and the relationship between them.",
                    "label": 0
                },
                {
                    "sent": "So in general information extraction of a sentence is extracting the basic facts.",
                    "label": 1
                },
                {
                    "sent": "The basic relationships in that sentence, who did what, to whom and so.",
                    "label": 0
                },
                {
                    "sent": "What you end up with some tuple like alone, Halevi is going to be the speaker at University of Washington and then a lot of the information you find is not explicitly stated in the text.",
                    "label": 0
                },
                {
                    "sent": "So then you, if you ask the question well, will alone visit Seattle.",
                    "label": 0
                },
                {
                    "sent": "Often we're sitting waiting with bated breath, is he coming or not?",
                    "label": 0
                },
                {
                    "sent": "Presumably you could infer that from from this tuple if he's going to be speaking at UW, presumably he's going to be visiting Seattle and even some of the subtleties already starting to creep in here, right?",
                    "label": 0
                },
                {
                    "sent": "UW?",
                    "label": 0
                },
                {
                    "sent": "Maybe that's.",
                    "label": 0
                },
                {
                    "sent": "University of Wisconsin.",
                    "label": 0
                },
                {
                    "sent": "In which case he's not visiting Seattle, etc, etc.",
                    "label": 0
                },
                {
                    "sent": "So there's a lot of technical difficulties in doing this kind of processing.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's suppose you could do this kind of processing.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "What is this by you above and beyond what we have today and is really actually a tremendous number of applications of this sort of technology.",
                    "label": 0
                },
                {
                    "sent": "Let me focus on what I like to call information Fusion pulling together information from lots of different sources, lots of different textual sources to show you the potential if you can do this kind of extraction inference.",
                    "label": 0
                },
                {
                    "sent": "So imagine you asked the question what kills bacteria if you go to Google.",
                    "label": 0
                },
                {
                    "sent": "You'll get an answer.",
                    "label": 0
                },
                {
                    "sent": "You go to Wikipedia, you'll get antibiotics, kill bacteria.",
                    "label": 0
                },
                {
                    "sent": "OK, that's great, but there's actually hundreds if not thousands of things that kill bacteria, and different people talk about them in different contexts.",
                    "label": 0
                },
                {
                    "sent": "And I'll show you a demo shortly of the kind of answer that we're able to get drawn from all over the web for this kind of question.",
                    "label": 0
                },
                {
                    "sent": "Here's another one.",
                    "label": 0
                },
                {
                    "sent": "What West Coast nanotechnology companies are hiring today?",
                    "label": 1
                },
                {
                    "sent": "Again, I'm not aware of a single page on the web that has the answer to that, but if you go to a variety of pages, right, different companies, different company directory's, then you go to these companies, jobs, pages and so on.",
                    "label": 0
                },
                {
                    "sent": "You can pull together and synthesize a good answer to that question, right?",
                    "label": 0
                },
                {
                    "sent": "That's not something you get from a search engine today, but to something you could get from an information extraction engine.",
                    "label": 0
                },
                {
                    "sent": "Even more topical, right?",
                    "label": 0
                },
                {
                    "sent": "Let's say we want to compare Obama's buzz to Hillary's.",
                    "label": 0
                },
                {
                    "sent": "And let's say we want to do that specifically for pages that were generated after his recent victory in Maine.",
                    "label": 0
                },
                {
                    "sent": "How do we do that?",
                    "label": 0
                },
                {
                    "sent": "That's not something a search engine gives you, right?",
                    "label": 0
                },
                {
                    "sent": "We want to be able to go to a relevant set of pages, extract what people are saying him about him, and about her, and put that information side by side.",
                    "label": 0
                },
                {
                    "sent": "And then the last example that has maybe more of an information integration flavor.",
                    "label": 0
                },
                {
                    "sent": "Let's say I want a quiet, inexpensive four Star hotel in Vancouver, and maybe I'm going to get quiet out of trip adviser.",
                    "label": 0
                },
                {
                    "sent": "You actually have to dig pretty deep to figure out which hotels are quiet and which ones aren't.",
                    "label": 0
                },
                {
                    "sent": "If to read a lot of reviews and let's say inexpensive, I might get from a site like Farecast.",
                    "label": 0
                },
                {
                    "sent": "We also have an hotel engine in a ranking of inexpensiveness four star.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'll get from the hotel website, but I need to go to all these different places and put the pieces together.",
                    "label": 0
                },
                {
                    "sent": "So if you're an informed traveler.",
                    "label": 0
                },
                {
                    "sent": "Can take you an hour to do to find the right hotel in a place where as a system like this could do most of the work automatically for you, reduce that to to maybe a minute or.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me just show you one application that we did build on top of this kind of extraction and this is the part of the dissertation work of Anna Maria Popescu.",
                    "label": 0
                },
                {
                    "sent": "She built an opinion minor that we called opine and the idea was to apply information extraction to product reviews right there.",
                    "label": 0
                },
                {
                    "sent": "Highly informative there textual.",
                    "label": 0
                },
                {
                    "sent": "There's lots of them.",
                    "label": 0
                },
                {
                    "sent": "They are highly varied.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "And what we wanted to do is summarize the reviews without any prior knowledge of the product category.",
                    "label": 1
                },
                {
                    "sent": "So people have built one specifically for printers or specifically for hotels we wanted to build a general one with the idea of applying to arbiter reviews on the web.",
                    "label": 0
                },
                {
                    "sent": "And again, there's a paper on this and we got nice precision.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And recall results.",
                    "label": 0
                },
                {
                    "sent": "Let me just give you a snapshot then I apologize for the threadbare interface.",
                    "label": 0
                },
                {
                    "sent": "Here is just a way of displaying the information.",
                    "label": 0
                },
                {
                    "sent": "It's not really an interface, but what you see there in black on the left are the various attributes that automatically extracted and you see that some of the names make a lot of sense, like staffon location.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, don't these are all automatically generated and then you see the opinion phrases it extracted, so you could tell at a glance this is off of 17 reviews of the.",
                    "label": 0
                },
                {
                    "sent": "Particular hotel in New York and you could see at a glance that people felt pretty good about the staff.",
                    "label": 0
                },
                {
                    "sent": "Seven people said excellent.",
                    "label": 0
                },
                {
                    "sent": "Somebody said poor, that's probably somebody from the competition and somebody had a particularly bad experience.",
                    "label": 0
                },
                {
                    "sent": "So you can really tell at a glance.",
                    "label": 0
                },
                {
                    "sent": "The staff is great.",
                    "label": 0
                },
                {
                    "sent": "The location is great, etc etc.",
                    "label": 0
                },
                {
                    "sent": "If you're interested in drilling down more.",
                    "label": 0
                },
                {
                    "sent": "So let's say I'm interested in whether the rooms are clean.",
                    "label": 0
                },
                {
                    "sent": "I can actually click on the extraction and it opens the sentence.",
                    "label": 0
                },
                {
                    "sent": "Is that the extraction was taken from?",
                    "label": 0
                },
                {
                    "sent": "I can click on read more and actually go to the to the review so it gives me a way to slice and dice the information that allows me to drill down to the actual text, but also gives me this very handy summary.",
                    "label": 0
                },
                {
                    "sent": "The one other thing that was nice about this system it not only did this kind of extraction, but then it took the different hotels and place them on a graph.",
                    "label": 0
                },
                {
                    "sent": "So imagine a graph where the nodes are the different product, so then those are different hotels and then the edges are these different attributes like cleanliness and they're ordered by strength.",
                    "label": 0
                },
                {
                    "sent": "So basically if I look in a hotel and I see well there are two reviews that say that the rooms are clean and I must, you know, very fussy.",
                    "label": 0
                },
                {
                    "sent": "So I say, well I want a hotel that's cleaner than that that's rated as cleaner.",
                    "label": 0
                },
                {
                    "sent": "Well, I can click in this user.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Office and it will take me to another hotel where it realizes that Spotlessly clean is actually stronger than clean and fairly fancy mechanics that used to do that.",
                    "label": 0
                },
                {
                    "sent": "So again what I'm trying to show here is that on top of this kind of extraction, with some inference layered in about say what's stronger, spotlessly clean or clean on top of that you can really build fairly sophisticated applications that are more powerful than the kind we see today today in Google.",
                    "label": 0
                },
                {
                    "sent": "You type in a product name and it will happily collect for you a bunch of reviews, but it's up to you to you to sift through them and figure out the different attributes and how H1 compares to H2.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sign.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's so.",
                    "label": 0
                },
                {
                    "sent": "Hopefully I've persuaded you that if we could do this well, particularly web scale, life will be good.",
                    "label": 0
                },
                {
                    "sent": "Turns out the reading the web is a tough problem.",
                    "label": 1
                },
                {
                    "sent": "First of all, traditional information extraction has been applied to very narrow, very precise information needs.",
                    "label": 0
                },
                {
                    "sent": "The classical example is finding the location and the time from a corpus of CMU seminar announcements.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a very homogeneous corpus, very clear and narrow request.",
                    "label": 0
                },
                {
                    "sent": "And this is not going to scale to the web, so the 1st Order of business for us has been to scale information extraction to the web.",
                    "label": 0
                },
                {
                    "sent": "It's hard because no parser achieves high accuracy on the full web corpus.",
                    "label": 1
                },
                {
                    "sent": "It's hard because typically extraction relies on named entity taggers, right saying this is a location.",
                    "label": 0
                },
                {
                    "sent": "This is a person.",
                    "label": 0
                },
                {
                    "sent": "This is an organization.",
                    "label": 0
                },
                {
                    "sent": "You can't assume that when you're dealing with arbitrary web text because the number of types that we're dealing with the different types of entities is arbitrary and unbounded.",
                    "label": 0
                },
                {
                    "sent": "We don't even know in advance.",
                    "label": 0
                },
                {
                    "sent": "What types of objects, what types of entities we're going to be dealing with, and Lastly, you can't do supervised learning in such a massive space.",
                    "label": 0
                },
                {
                    "sent": "What are the concepts that you'd label?",
                    "label": 0
                },
                {
                    "sent": "How many examples do you need right?",
                    "label": 0
                },
                {
                    "sent": "It's just happening on a massive scale.",
                    "label": 0
                },
                {
                    "sent": "You might say, OK, you can do supervised learning.",
                    "label": 0
                },
                {
                    "sent": "What about semi supervised learning where we only need a small amount of data, hand labeled data, and then we leverage the unlabeled data that's available on the web.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, it's true that in semi supervised learning you only need a small number of hand labeled examples, but that's a small number per concept, right?",
                    "label": 0
                },
                {
                    "sent": "And the issue is that that place is an inherent limit on the number of concepts.",
                    "label": 1
                },
                {
                    "sent": "Think about it.",
                    "label": 0
                },
                {
                    "sent": "If I have a if I need let's say 20 and labeled examples per concept, and I have a million concepts all of a sudden I'm dealing with 20 million hand labeled examples and that starts to be too big.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, Semi supervised learning assumes that the concept is specified ahead of time.",
                    "label": 0
                },
                {
                    "sent": "And again, because of the long tail, because of the diversity on the web, we don't know the concepts ahead of time.",
                    "label": 0
                },
                {
                    "sent": "We don't know what the relations of interest are when we start, so we can't really do semisupervised learning either.",
                    "label": 0
                },
                {
                    "sent": "So the approach that we've taken is what we call self supervised learning.",
                    "label": 0
                },
                {
                    "sent": "The idea is that the learner discovers concepts on the fly and the learner automatically labels examples on its own, and I'll show you a little bit of how that's done and I'll refer you to to papers for more.",
                    "label": 1
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The details.",
                    "label": 0
                },
                {
                    "sent": "So really, the kind of extraction that we need to do that we call open information extraction open IE is quite different than the traditional kind.",
                    "label": 0
                },
                {
                    "sent": "So here I put them side by side.",
                    "label": 0
                },
                {
                    "sent": "Traditionally you have as input a corpus and some hand labeled data in open A.",
                    "label": 1
                },
                {
                    "sent": "All we can afford is the corpus in.",
                    "label": 1
                },
                {
                    "sent": "Traditionally the relations of interest are specified in in advance opening we discover them automatically.",
                    "label": 0
                },
                {
                    "sent": "In traditional, either complexities, order the number of documents times the number of relations, right?",
                    "label": 0
                },
                {
                    "sent": "We make multiple passes over the corpus, one for each relation.",
                    "label": 1
                },
                {
                    "sent": "In open a it's not a function of the relations is constant in the number of relations, and again that's key because we could we have thousands or hundreds of thousands of relations that we're dealing with.",
                    "label": 0
                },
                {
                    "sent": "And then Lastly the machinery for text analysis.",
                    "label": 0
                },
                {
                    "sent": "In traditionally, is pretty heavy, right?",
                    "label": 0
                },
                {
                    "sent": "There's a parser.",
                    "label": 0
                },
                {
                    "sent": "There's a named entity tagger.",
                    "label": 1
                },
                {
                    "sent": "All we use an opening is a noun phrase chunker, which is fairly robust.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so how does the extractor work?",
                    "label": 0
                },
                {
                    "sent": "And this is the work of Michel Bank Owen.",
                    "label": 0
                },
                {
                    "sent": "A recent paper.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is we start with a very simple model of relationships in English.",
                    "label": 1
                },
                {
                    "sent": "If you take a step back and think about it, we don't have to have a relation specific extractor to a first order.",
                    "label": 1
                },
                {
                    "sent": "This is oversimplified, but to 1st order you can say verbs tend to capture relationships in English, right?",
                    "label": 0
                },
                {
                    "sent": "So Thomas Edison invented the light bulb, invented is meant to capture the relationship and we have a subject and an object so we can start with a very simple model and label ourselves a whole bunch of.",
                    "label": 0
                },
                {
                    "sent": "Negative and positive examples of extraction and then we can bootstrap from that.",
                    "label": 1
                },
                {
                    "sent": "A much more fine grained model and that's that's what we've done and the the more general model is encoded as a conditional random field.",
                    "label": 0
                },
                {
                    "sent": "And then the next step is you decompose each sentence you come across into a noun.",
                    "label": 0
                },
                {
                    "sent": "Phrases and verb phrase into chunks, and then you go through those and identify exactly the entities and the relation expression using the conditional random field.",
                    "label": 0
                },
                {
                    "sent": "So the key to this is that this process, while it's language specific.",
                    "label": 0
                },
                {
                    "sent": "OK, we did this for English.",
                    "label": 0
                },
                {
                    "sent": "We haven't tried this for other languages.",
                    "label": 0
                },
                {
                    "sent": "We believe that you could try it for other languages, but we haven't.",
                    "label": 0
                },
                {
                    "sent": "The key here is that it's a relation independent extractor, which is very different than.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Previous work in this area so, So what does this look like for text runner?",
                    "label": 0
                },
                {
                    "sent": "Well, we tend to extract triples representing binary relations.",
                    "label": 0
                },
                {
                    "sent": "We've actually extended it recently to Enoree relations, but let me stay with binaries, so we're going to have argument one the relation, and then argument too.",
                    "label": 0
                },
                {
                    "sent": "So given a sentence like Internet powerhouse, eBay was originally founded by Pierre Omidyar.",
                    "label": 1
                },
                {
                    "sent": "What's going to happen is it's going to figure out?",
                    "label": 0
                },
                {
                    "sent": "OK, eBay is 1 entity here.",
                    "label": 0
                },
                {
                    "sent": "The relation is founded by pure, mature is the other argument.",
                    "label": 0
                },
                {
                    "sent": "And that's the triple that we're going to extract from the sentence.",
                    "label": 0
                },
                {
                    "sent": "And again, you can immediately think of lots of examples that are much trickier, right?",
                    "label": 0
                },
                {
                    "sent": "50 word sentence is.",
                    "label": 0
                },
                {
                    "sent": "And most of these 50 word sentence is we're just going to leave behind.",
                    "label": 0
                },
                {
                    "sent": "Either will take a simple piece of them and extract from that.",
                    "label": 0
                },
                {
                    "sent": "Or we might say, you know what we can't work on such a complicated sentence.",
                    "label": 0
                },
                {
                    "sent": "That's OK.",
                    "label": 0
                },
                {
                    "sent": "I've got 100 billion other sentences to work with, so that's one of the nice things about working with.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The web corpus.",
                    "label": 0
                },
                {
                    "sent": "There's still a ton of challenges here, and this is some of the work that we've done, and again, I'll just hint at some of them.",
                    "label": 0
                },
                {
                    "sent": "You have to drop nonessential information so it was originally founded by goes to founded by.",
                    "label": 1
                },
                {
                    "sent": "If you don't drop this kind of information, you're going to have a ton of relations that you think are different, but really all mean the same thing, so it's important to do some compression here.",
                    "label": 0
                },
                {
                    "sent": "As you're doing compression and dropping information, it's still important to retain key distinctions, right?",
                    "label": 0
                },
                {
                    "sent": "So you can't just say I'm going to look at the verbs, right?",
                    "label": 1
                },
                {
                    "sent": "eBay, founded by Pierre, is not the same as eBay founder Pierre, Right?",
                    "label": 0
                },
                {
                    "sent": "The word by there is actually critical, so so we need to know that, by the way, a bunch of relationships are not actually captured by verb, so George Bush, president of the United States, right?",
                    "label": 0
                },
                {
                    "sent": "There's a relationship there.",
                    "label": 0
                },
                {
                    "sent": "It's not a verb, and we also have to think a lot about problems of synonymy and aliasing.",
                    "label": 0
                },
                {
                    "sent": "So we need to realize that Albert Einstein.",
                    "label": 0
                },
                {
                    "sent": "Is the same as Einstein, but not the same as Einstein brothers was a leading bagel manufacturer on the on the web.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's how we put the pieces together.",
                    "label": 0
                },
                {
                    "sent": "Text Runner, which is our open a system I'm going to demo to in a second.",
                    "label": 0
                },
                {
                    "sent": "It has a self supervised learner that learns an extractor, then that extractor makes a single Passover.",
                    "label": 1
                },
                {
                    "sent": "The corpus just goes sentence by sentence over over webpages.",
                    "label": 1
                },
                {
                    "sent": "Extracts as much as it can from each sentence and then it indexes it.",
                    "label": 0
                },
                {
                    "sent": "We put it into Lucene and now we have something that.",
                    "label": 1
                },
                {
                    "sent": "Can answer queries at close.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Close to interactive speeds, so let me show you.",
                    "label": 0
                },
                {
                    "sent": "Hopefully my demo.",
                    "label": 0
                },
                {
                    "sent": "Will work here over the Internet and what I'm showing you here.",
                    "label": 0
                },
                {
                    "sent": "Is iaccessible for my homepage or just type in text runner into your favorite search engine?",
                    "label": 0
                },
                {
                    "sent": "You would find this.",
                    "label": 0
                },
                {
                    "sent": "So this is publicly available.",
                    "label": 0
                },
                {
                    "sent": "You can play with.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Obviously I'm going to show you a good example.",
                    "label": 0
                },
                {
                    "sent": "You know it.",
                    "label": 0
                },
                {
                    "sent": "It's it's.",
                    "label": 0
                },
                {
                    "sent": "Performance is variable, but to give you a sense of the potential of this kind of thing and this is run over about 120 million web pages of varying quality.",
                    "label": 0
                },
                {
                    "sent": "We have Wikipedia in there.",
                    "label": 0
                },
                {
                    "sent": "We've got some pretty random pages, so again, this is just a proof of concept, so let's ask it what kills bacteria as a simple question processor that basically tells it OK, we want kills as the relationship and bacteria is the object not telling me what you find an we see a variety.",
                    "label": 0
                },
                {
                    "sent": "Of answers here and here, it says 175 more, so I'm going to click on that.",
                    "label": 0
                },
                {
                    "sent": "And the.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And open it up and you see the various things that found that killed bacteria from the.",
                    "label": 0
                },
                {
                    "sent": "From from the obvious ones like antibiotics and if you look below we see chlorine heat amoxicillin pasteurization.",
                    "label": 0
                },
                {
                    "sent": "They start to get more obscure like garlic, alcohol, honey and if you're not sure that you believe in it, you can click on the number here and see the different sentences that it came from or let me do it here so it's more visible, right?",
                    "label": 0
                },
                {
                    "sent": "So you see different senses you can click through and actually go to the web pages and then another nice thing is you see there's a lot of compression going here underneath the scenes so.",
                    "label": 0
                },
                {
                    "sent": "All these different ways of referring to antibiotics.",
                    "label": 0
                },
                {
                    "sent": "We're all compressed to realize is really just saying antibiotics, so you're not swamped with a bunch of different answers that really mean.",
                    "label": 0
                },
                {
                    "sent": "The same thing.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So that gives you a flavor of how tannic text runner works, and.",
                    "label": 0
                },
                {
                    "sent": "Usually I open it up, but since everybody has their laptops here you can you can play with it yourselves and again this this prototype is not particularly optimized.",
                    "label": 0
                },
                {
                    "sent": "It takes, you know 10 to 30 seconds to give you an answer.",
                    "label": 0
                },
                {
                    "sent": "We could easily make that be a lot faster with just some elbow grease.",
                    "label": 0
                },
                {
                    "sent": "There's nothing inherently slow about the processing at.",
                    "label": 0
                },
                {
                    "sent": "At query time, right?",
                    "label": 0
                },
                {
                    "sent": "The hard work happened at compile time.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The crawler, so to give you a more quantitative sense, we took a sample of 9 million web pages that contained about 11.3 million triples and using samples we assessed OK. How many of these are actually any good and we found that of the 11.3 million there about 9.3 million with a well informed relation.",
                    "label": 1
                },
                {
                    "sent": "And of those there are 7.8 million that had both well formed relations and well informed entities.",
                    "label": 0
                },
                {
                    "sent": "There were meaningful.",
                    "label": 0
                },
                {
                    "sent": "And then what we find that of these there were six point 8 million that contained.",
                    "label": 0
                },
                {
                    "sent": "Abstract observations things like fruit contain vitamins.",
                    "label": 0
                },
                {
                    "sent": "Those might be good for ontology building various activities like that and then they were on the order of a million.",
                    "label": 1
                },
                {
                    "sent": "There were concrete facts like Oppenheimer taught at Berkeley and you see that the precision levels that we're achieving in here are actually pretty high, right?",
                    "label": 0
                },
                {
                    "sent": "So if you type in Oppenheimer to text runner, you're going to get a mixture of abstract facts and concrete facts, but with fairly high accuracy.",
                    "label": 0
                },
                {
                    "sent": "We tend to sort them by frequency and so the ones that are more often repeated more likely to be correct filter.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Up to the top.",
                    "label": 0
                },
                {
                    "sent": "OK, so so that's extraction and now what I want to talk about is OK if you have this kind of extraction, what can you do on top of that?",
                    "label": 0
                },
                {
                    "sent": "What kind of inference you can do, because again initially we got into this and we solve these billions and billions of senses.",
                    "label": 0
                },
                {
                    "sent": "We thought great, anything you could ever possibly want to know is in there.",
                    "label": 0
                },
                {
                    "sent": "But in fact, what we've realized overtime is a lot of the information that you want is actually implicit in some form, and so I'll talk about three kinds of inferences that.",
                    "label": 0
                },
                {
                    "sent": "We and other people have done over extraction and those are entity and predicate resolution, and this is the old problem, right of deduping has lots of different names, But the basic idea is to do this kind of compression and realize the two different sentences are actually talking about the same object.",
                    "label": 1
                },
                {
                    "sent": "And then I'll talk about how we assess the probability of correctness of a sentence.",
                    "label": 1
                },
                {
                    "sent": "That's pretty important.",
                    "label": 0
                },
                {
                    "sent": "And Lastly, I'll talk about our most recent work.",
                    "label": 0
                },
                {
                    "sent": "How do you put 2 + 2 to get 4?",
                    "label": 0
                },
                {
                    "sent": "How do you compose different facts you see on the web to draw a conclusion that's not explicitly stated at all?",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's start with entity resolution.",
                    "label": 0
                },
                {
                    "sent": "Again, this is part of the dissertation of Alex Yates.",
                    "label": 0
                },
                {
                    "sent": "Our basic idea is that we can determine the two objects are likely to be the same one based on the relationships we find the text runner is found and this is related to earlier work by Pentel and Lynn.",
                    "label": 0
                },
                {
                    "sent": "There's some technical differences, but it's not particular important right now, so the basic idea is if I have a bunch of these triples about X, and I know that X was born in 1941, is a citizen of the US is a friend of Joe.",
                    "label": 1
                },
                {
                    "sent": "And I have another entity, let's call it M, where some of these facts are true, but M is also a friend of Marys.",
                    "label": 0
                },
                {
                    "sent": "Basically, the observation is that the probability that X&M are the same is some function of the number of shared relations, right?",
                    "label": 0
                },
                {
                    "sent": "And we can make this a lot more sophisticated.",
                    "label": 0
                },
                {
                    "sent": "We can look at functional relationships, right?",
                    "label": 0
                },
                {
                    "sent": "So if X&M have the same spouse, then that further increases the likelihood that they are the same person.",
                    "label": 0
                },
                {
                    "sent": "And so on and so on.",
                    "label": 0
                },
                {
                    "sent": "So basic relational model.",
                    "label": 0
                },
                {
                    "sent": "Nice thing about this again is this is completely generic.",
                    "label": 0
                },
                {
                    "sent": "We don't need hand labeled data, we don't need specificity to a particular domain.",
                    "label": 0
                },
                {
                    "sent": "This is for arbitrary entities in the web corpus, and in fact we have.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Resolve are on our entire corpus.",
                    "label": 0
                },
                {
                    "sent": "Another elegant thing here is you can do the dual of this and apply to relations.",
                    "label": 0
                },
                {
                    "sent": "So here's a schematic example.",
                    "label": 0
                },
                {
                    "sent": "I have two relations are in are primed and if they both apply between one and 2, two and four etc etc.",
                    "label": 0
                },
                {
                    "sent": "I'm going to conclude that the probability that the two relations are the same is again a direct function of the number of shared argument pairs, and in fact there's a natural mutual recursion here between these two parts of the algorithm, right as you discover.",
                    "label": 0
                },
                {
                    "sent": "The more entities are the same.",
                    "label": 0
                },
                {
                    "sent": "You can then use that to discover the more relationships are likely to be the same and you can go backwards and forwards, and so we have here.",
                    "label": 0
                },
                {
                    "sent": "This unsupervised probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "The algorithm here is actually order N log N, so it's not entirely linear, but still easy to run on millions and 10s of millions of docs.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so next one probability of correctness.",
                    "label": 1
                },
                {
                    "sent": "So how likely is it that an extraction that we get is is correct and that's kind of a subtle question?",
                    "label": 1
                },
                {
                    "sent": "Even what does correct mean this correct mean true?",
                    "label": 0
                },
                {
                    "sent": "Or does it mean a true truth preserving transformation on the original sentence, right?",
                    "label": 0
                },
                {
                    "sent": "So if I see the sentence you know, JFK was killed by Elvis?",
                    "label": 0
                },
                {
                    "sent": "What exactly am I supposed to extract from that sentence, right?",
                    "label": 0
                },
                {
                    "sent": "And we've mainly focused on an truth preserving notion of correctness, right?",
                    "label": 0
                },
                {
                    "sent": "We're going to be trying to do an accurate reflection of what's in the text, and there are a lot of factors to consider if you do this analysis.",
                    "label": 1
                },
                {
                    "sent": "There's the authoritativeness of the source.",
                    "label": 0
                },
                {
                    "sent": "There's your confidence in the extraction method.",
                    "label": 1
                },
                {
                    "sent": "What I'm going to show you now is how we utilized the redundancy in the web corpus we ask.",
                    "label": 0
                },
                {
                    "sent": "OK, what's the number of independent extractions that you've gotten, and how is likelihood of correctness of function of of the?",
                    "label": 1
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Parameter.",
                    "label": 0
                },
                {
                    "sent": "And this idea of counting extractions is actually not a new idea.",
                    "label": 0
                },
                {
                    "sent": "If you start with lexico syntactic patterns, which is an old idea, goes back to Marty Herst back in 90 two.",
                    "label": 0
                },
                {
                    "sent": "The idea is you have a sentence.",
                    "label": 0
                },
                {
                    "sent": "Cities such as Seattle and Boston and that contains a clue right that Seattle and Boston are cities, right?",
                    "label": 1
                },
                {
                    "sent": "Such as their means that we have some class in Seattle and Boston are members of the class.",
                    "label": 0
                },
                {
                    "sent": "Well, those kinds of patterns.",
                    "label": 0
                },
                {
                    "sent": "Our hints as to the meaning of the sentence they suggest extractions.",
                    "label": 0
                },
                {
                    "sent": "Now Peter turning back in 2002, an ACL paper developed the PMI.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm.",
                    "label": 0
                },
                {
                    "sent": "The basic idea there was that he could take pointwise mutual information which we can just approximate as Co occurrence frequencies of different strings.",
                    "label": 0
                },
                {
                    "sent": "His idea was you can estimate those from the number of results you get from a search engine OK and then based on the number of results you can compute.",
                    "label": 0
                },
                {
                    "sent": "Confidence of membership of the class.",
                    "label": 0
                },
                {
                    "sent": "So basically what this means is we're going to go to our favorite search engine and type in a phrase.",
                    "label": 0
                },
                {
                    "sent": "For example, cities such as Seattle and we look at the number of results that comes back in.",
                    "label": 0
                },
                {
                    "sent": "If that number is higher than it is for some other string.",
                    "label": 0
                },
                {
                    "sent": "I don't know cities such as avocado, then we're going to be more confident that Seattle is a city than that avocado is a city.",
                    "label": 0
                },
                {
                    "sent": "And if you play these kinds of games and lots of people, have you find that these Co occurrence frequencies are actually very telling.",
                    "label": 0
                },
                {
                    "sent": "About what's what among.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Strength, so the next step is we actually formalize this, and this gets very complicated to hurry, but you can actually ask the following precise question if you have some extraction X and it occurs K times in a set of N distinct sentences, and each of these sentence has this kind of lexico syntactic extraction pattern, so each of them is basically suggesting that X is a member of some class, what actually the probability that X belongs to the class, and again by class I mean something like cities or can be.",
                    "label": 1
                },
                {
                    "sent": "Strings that belong to relation like mayor of a city.",
                    "label": 0
                },
                {
                    "sent": "And by the way, one really important thing here is that we only count in this, and that's what I'm trying to highlight there.",
                    "label": 0
                },
                {
                    "sent": "This thing sentences right if you have.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of copying on the web and we're aware of that obviously, and so you don't want to be counting the same sentence is just because it's been copied lots of times.",
                    "label": 0
                },
                {
                    "sent": "But if we.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Count distinct sentence is.",
                    "label": 0
                },
                {
                    "sent": "It turns out that we can develop a commentarial model.",
                    "label": 0
                },
                {
                    "sent": "We called the urns model, 'cause it's a balls and urns type of model.",
                    "label": 0
                },
                {
                    "sent": "I won't go through into the formal details, but I'll just show you the the punchline.",
                    "label": 0
                },
                {
                    "sent": "Here we can actually have a close form expression 4 if X appears K times in an draws of balls from the urn.",
                    "label": 0
                },
                {
                    "sent": "In other words in it appears K times as an extraction from out of an extractions that we conclude that the probability is this.",
                    "label": 0
                },
                {
                    "sent": "Nasty looking expression.",
                    "label": 0
                },
                {
                    "sent": "I haven't even defined the notation here.",
                    "label": 0
                },
                {
                    "sent": "I'm not expecting you to read this, I just want you to be impressed by our our equations and I'll refer you to the paper for for the details.",
                    "label": 0
                },
                {
                    "sent": "If you do look at it for a second though, you see that it does behave in sort of reasonable ways.",
                    "label": 0
                },
                {
                    "sent": "You see that the odds here are the probability do increase exponentially with K right?",
                    "label": 1
                },
                {
                    "sent": "In other words, the more times.",
                    "label": 0
                },
                {
                    "sent": "We see Seattle being appearing in a sentence.",
                    "label": 0
                },
                {
                    "sent": "Cities such as Seattle, we become exponentially more confident in the fact that Seattle is a city.",
                    "label": 1
                },
                {
                    "sent": "The odds also decrease exponentially with the number of extractions.",
                    "label": 0
                },
                {
                    "sent": "So if I saw Seattle being extracted as a city five times, the question is 5 times out of 10 sentences or five times out of 100 million sentences, right?",
                    "label": 0
                },
                {
                    "sent": "That's a very different probability.",
                    "label": 0
                },
                {
                    "sent": "So this expression is actually an interplay between these two exponentials, and it turns out to probably give.",
                    "label": 0
                },
                {
                    "sent": "The right probability now, why do we care about the fancy math here?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, if we compare the performance in practice to previous approaches so so this graph is showing the urns model compared to attorneys pointwise mutual information based model an compared to noisy or which is another model that was previously used in extraction work and you see this is for a sample for relations.",
                    "label": 0
                },
                {
                    "sent": "He ran on the.",
                    "label": 0
                },
                {
                    "sent": "Y axis you have deviation from ideal log likelihood, so this is a log scale and lower here is actually better, right?",
                    "label": 1
                },
                {
                    "sent": "How far we from from the right answer and you see that in dark black we have the urns model very close to the right answer and compared to the other ones we're getting an order of 1500% improvement or more.",
                    "label": 0
                },
                {
                    "sent": "So this is much, much, much better than than previous work, and again no hand labeled data completely domain independent.",
                    "label": 0
                },
                {
                    "sent": "In other words, methods that scale to trying to tackle the full web.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now it turns out that this urns model works great when you have lots of information.",
                    "label": 0
                },
                {
                    "sent": "When you have this kind of redundancy.",
                    "label": 0
                },
                {
                    "sent": "So if we want to know whether we believe the Michael Bloomberg is the mayor of New York, that kind of thing tends to be correct.",
                    "label": 1
                },
                {
                    "sent": "And if you play with text runner, you'll see that the statements that it extracts.",
                    "label": 0
                },
                {
                    "sent": "At the top of its results are often correct, but when things get sparse, right, well familiar with this load distribution we have on the order of 50% of the extractions, there only occur once, and then what do you do with that?",
                    "label": 0
                },
                {
                    "sent": "And you find that in those there's a mixture of statements that are rather obscure, but correct like Dave Shavers, the mayor of some small town called Pickerington, and also statements that are blatantly false like Ronald McDonald, is the mayor of Mcdonaldland where.",
                    "label": 0
                },
                {
                    "sent": "We all know it's.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry.",
                    "label": 0
                },
                {
                    "sent": "Thank you, I appreciate that.",
                    "label": 0
                },
                {
                    "sent": "So maybe not everybody knows that, but it's actually running.",
                    "label": 0
                },
                {
                    "sent": "This is not the mirror McDonald, so there's don't say you didn't learn anything from my talk, right?",
                    "label": 0
                },
                {
                    "sent": "So so how do we tease these apart, right?",
                    "label": 0
                },
                {
                    "sent": "How do we tease the?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The good ones apart from the bad ones and in more recent work by dug down and stuff show markers.",
                    "label": 0
                },
                {
                    "sent": "What they said is instead of just looking at these extraction patterns, these lexico syntactic patterns, why don't we, when we're considering an entity look at all the context that it occurs in so affectively?",
                    "label": 0
                },
                {
                    "sent": "If we're wondering, is Shaver Amayreh?",
                    "label": 0
                },
                {
                    "sent": "Why don't we ask the question?",
                    "label": 0
                },
                {
                    "sent": "Does he behave like a mayor and what does it mean?",
                    "label": 1
                },
                {
                    "sent": "Does he behave like a Mary means if we take all the sentence is that Shaver appears in.",
                    "label": 0
                },
                {
                    "sent": "Do we see statements like he was elected and you know he blocked the motion and he attended the City Council meeting etc etc.",
                    "label": 1
                },
                {
                    "sent": "Right so if we see him in the presence of the right kinds of strings then we're going to be more and more likely to believe that he is the mayor and the same for printing 10 if we see streets oven mayor of Pinkerton?",
                    "label": 0
                },
                {
                    "sent": "Going to more likely believe it's a city.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This is not an explicit computation.",
                    "label": 1
                },
                {
                    "sent": "What we do is we build a language model and in fact the language model is captured by a hierarchical Markov model.",
                    "label": 1
                },
                {
                    "sent": "It's built once per corpus, and what that model does is it really takes the entire corpus and projects it into a 20 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "A lower dimensional space, an what happens then is each string is a point in this lower dimensional space, and what you can do is you can measure distances between these strings and these distances are really referring to there.",
                    "label": 0
                },
                {
                    "sent": "Behavior in context to the statistical properties of their behavior in context.",
                    "label": 0
                },
                {
                    "sent": "Again, I'm glossing over a lot of technical detail here to give you the flavor, so the idea is if I'm trying to figure out if Pinkerton is a city I'm going to map it into this space and I'm going to ask what's the proximity between Pickerington and other cities that earns the urns model tells us are known cities like Seattle and Boston and so on, so using a model like this I can tell the Pickerington is relatively likely to be a city, certainly compared to other strings.",
                    "label": 0
                },
                {
                    "sent": "OK, so again more of the details in the paper, but the point is by using language models over the corpus we can handle the more Sparks cases as well and that's.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's very important.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me turn to the last example of inference and this is compositional inference, and this is work very much work in progress, so I'll just give you 1 slide on this.",
                    "label": 0
                },
                {
                    "sent": "Again, our idea here is to get an implicit information and what we're looking for is very short inference chains.",
                    "label": 0
                },
                {
                    "sent": "OK, we're not going to prove Fermat's last theorem using the simple mechanism.",
                    "label": 0
                },
                {
                    "sent": "I'm bout to show you, but we would like to get some basic facts that aren't in the corpus.",
                    "label": 0
                },
                {
                    "sent": "So let's say text Runner notes that Turing was born in London.",
                    "label": 0
                },
                {
                    "sent": "And let's say we know from word Net could get this from text renders well, easier to get out of London.",
                    "label": 0
                },
                {
                    "sent": "London is a part of England.",
                    "label": 0
                },
                {
                    "sent": "And Furthermore, let's say we have a rule that says born in is transitive through part of what that basically means is we can conclude the Turing is born in England OK even though the corpus might not say this and in fact in the 120 million web pages that are in the text runner.",
                    "label": 0
                },
                {
                    "sent": "Corpus it says that he was born in London, but doesn't say that is born in England, right?",
                    "label": 1
                },
                {
                    "sent": "So if you just did a simple question answering thing and ask it.",
                    "label": 0
                },
                {
                    "sent": "Hey was during born in England, it will say no.",
                    "label": 0
                },
                {
                    "sent": "He was born in London.",
                    "label": 0
                },
                {
                    "sent": "So that's not the behavior that we want out of these systems.",
                    "label": 0
                },
                {
                    "sent": "So what the mechanism that we built does is, it instantiates a Markov logic network on the fly based on the extraction.",
                    "label": 0
                },
                {
                    "sent": "Then it knows about based on rules learned from the corpus.",
                    "label": 0
                },
                {
                    "sent": "What I'm about to show you actually the rules were coded by hand, the automatic learning of rules is actually future work, but we get an inference simple system that hopefully I can show you here.",
                    "label": 0
                },
                {
                    "sent": "If I can find my mouse.",
                    "label": 0
                },
                {
                    "sent": "And again, so you can ask it a question like was Alan Turing born in England and the first thing it says?",
                    "label": 0
                },
                {
                    "sent": "Well, I don't have any direct evidence for that zero piece of evidence in the corpus, but it instantiates it's MLN and it comes to conclusion well, with some probability, I do believe that he was born in England, and if I click on the number I can see the again this chain of reasoning that led to that conclusion, and it's doing some probabilistic computation over that.",
                    "label": 0
                },
                {
                    "sent": "And again, this could be optimized further, so we're seeing here really over hundreds of millions of assertions, some very simple inference to get beyond what's explicitly stated in the text.",
                    "label": 0
                },
                {
                    "sent": "And again, everything here the probability values that are in green and the different modes of reasoning everything can be refined.",
                    "label": 0
                },
                {
                    "sent": "I just urge Steph to put something together just so I could show you something brand new and this is I would refer you to the paper on this, except we haven't written it yet, so.",
                    "label": 0
                },
                {
                    "sent": "I can't quite do that.",
                    "label": 0
                },
                {
                    "sent": "Whoops",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Here's excuse me the family tree.",
                    "label": 1
                },
                {
                    "sent": "The different systems that I talked about here are in green.",
                    "label": 0
                },
                {
                    "sent": "Some systems that we built that are fun that I didn't talk about are in red.",
                    "label": 0
                },
                {
                    "sent": "We were influenced to start this node all project by three systems that I should mention.",
                    "label": 0
                },
                {
                    "sent": "One is Tom Mitchell's project at CMU, the web project, which again very much shares this motivation of trying to get knowledge from text or a lens terminology.",
                    "label": 0
                },
                {
                    "sent": "It crossing the structure chasm.",
                    "label": 0
                },
                {
                    "sent": "Very important challenge for for this kind of work also by attorneys PMI our algorithm and then also by we had one of the early question answering systems from the web system back in 2001 was called Mulder.",
                    "label": 0
                },
                {
                    "sent": "You would ask it a question.",
                    "label": 0
                },
                {
                    "sent": "It would go through the search engines and get you your answer and this was a one off system and we realized instead of doing this kind of question answering thing one at a time, why don't we just go and apply the kind of extraction machinery we can too.",
                    "label": 0
                },
                {
                    "sent": "Every sentence on the web, and something that in fact we're very excited about, is exactly that.",
                    "label": 0
                },
                {
                    "sent": "Scaling text runner from the 120 million pages to the full web corpus, and we believe that if you do that, as you often see with these things with two orders of magnitude more data, the quality of what you get will go up substantially.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I want to acknowledge the team a number of people there dug down his name is in red.",
                    "label": 0
                },
                {
                    "sent": "Aside from his great contributions.",
                    "label": 0
                },
                {
                    "sent": "He's also on the job market today, so if people are interested they should contact him directly.",
                    "label": 0
                },
                {
                    "sent": "He's a great guy.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And let me just I want to leave time for questions.",
                    "label": 0
                },
                {
                    "sent": "So let me just end with again the high level conclusion.",
                    "label": 0
                },
                {
                    "sent": "So what I'm asking you to think about is search systems that operate over a much more semantic space than today's search system.",
                    "label": 1
                },
                {
                    "sent": "So instead of keywords and documents, we can have these kinds of extractions instead of TF IDF fan page rank.",
                    "label": 0
                },
                {
                    "sent": "We can have these relational models like the one I showed you that resolve are was using and instead of web pages and hyperlinks we can think of.",
                    "label": 0
                },
                {
                    "sent": "Entities and the relationships between them, and I guess I shouldn't even say instead of right?",
                    "label": 0
                },
                {
                    "sent": "I mean, these two spaces can coexist side-by-side, right?",
                    "label": 0
                },
                {
                    "sent": "You could ask, could we use extractions relational models to help build a better ranking function, right?",
                    "label": 0
                },
                {
                    "sent": "Could we use them to do better spam detection?",
                    "label": 0
                },
                {
                    "sent": "Could we use them alongside?",
                    "label": 0
                },
                {
                    "sent": "Imagine some model where somebody showed me a startup that's trying to do this recently where you have the pages, but the different entities are highlighted on the side and you can click the same way you can click.",
                    "label": 0
                },
                {
                    "sent": "Show me similar pages you can click on the entity and say show me more pages about this entity, right?",
                    "label": 0
                },
                {
                    "sent": "So these are not opposing but complementary, and what I hope I've persuaded use that this is not a pipe dream or even a long term super ambitious vision like Tim Berners Lee's vision of the Semantic Web.",
                    "label": 0
                },
                {
                    "sent": "This is something that we have prototypes of today, and other people are building even even more advancements as we speak, so this notion of reading the web.",
                    "label": 1
                },
                {
                    "sent": "Suggests a new search paradigm.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So one question.",
                    "label": 0
                },
                {
                    "sent": "So did these techniques are interesting and can give quite impressive results.",
                    "label": 0
                },
                {
                    "sent": "But let's say this is still corpus level of understanding of the triples usually, so you have the statistics page before roughly one triple per page or even.",
                    "label": 0
                },
                {
                    "sent": "A pervert picture or even less so.",
                    "label": 0
                },
                {
                    "sent": "Oh, how do you imagine that to the future?",
                    "label": 0
                },
                {
                    "sent": "In on the going down to document level understanding.",
                    "label": 0
                },
                {
                    "sent": "So where you would get?",
                    "label": 0
                },
                {
                    "sent": "I don't know 10s of triples or tense effects per page which would be reliable, contextualized, and everything else which which needs to be done basically to deal with this information.",
                    "label": 0
                },
                {
                    "sent": "So factually we get on the order of 1.5 extractions per sentence on average.",
                    "label": 0
                },
                {
                    "sent": "So in fact I think we've already.",
                    "label": 0
                },
                {
                    "sent": "Achieve part of what you're saying, which is we are very much working at the sentence level and getting extractions.",
                    "label": 0
                },
                {
                    "sent": "However, you're right that there's a lot more work needs to be done, so you said contextualize.",
                    "label": 0
                },
                {
                    "sent": "Then you said reliable and those are very good points.",
                    "label": 0
                },
                {
                    "sent": "We ignore the context and we do that at our Pearl, so if you have a web page entitled you know common misconceptions about the topic will happily dive into that an extract the misconceptions and believe them as if they were true, so.",
                    "label": 0
                },
                {
                    "sent": "And there are all kinds of even context within the sentence.",
                    "label": 0
                },
                {
                    "sent": "If you have a sentence that says, I believe that London is the capital of the United States, or I don't believe that London is the capital of United States, we're likely to get that.",
                    "label": 0
                },
                {
                    "sent": "London is the capital United States, so there's a lot of issues there now.",
                    "label": 0
                },
                {
                    "sent": "The massive redundancy really helps here.",
                    "label": 0
                },
                {
                    "sent": "OK, so the that London is the capital United States in those kinds of weird sentences is swamped by other sentence.",
                    "label": 0
                },
                {
                    "sent": "Is that say that what the capital actually is, right?",
                    "label": 0
                },
                {
                    "sent": "So that's part of it.",
                    "label": 0
                },
                {
                    "sent": "But there is a lot more work to be done.",
                    "label": 0
                },
                {
                    "sent": "On UN better extraction and better inference from my point of view.",
                    "label": 0
                },
                {
                    "sent": "As a professor, that's good news, because you know smart students are walking into my office and we have lots to work on.",
                    "label": 0
                },
                {
                    "sent": "I'm not shipping a product.",
                    "label": 0
                },
                {
                    "sent": "Yeah, urns model does very well in there like 15 times better than the previous model.",
                    "label": 0
                },
                {
                    "sent": "Then for the sparse data you mentioned that language models to the rescue.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                },
                {
                    "sent": "Would you expect like if you use the language models, could you further improve this orange model?",
                    "label": 0
                },
                {
                    "sent": "So we've done some measurements on that.",
                    "label": 0
                },
                {
                    "sent": "Again, I didn't have time to get into it 'cause it gets rather involved, but there are in the ACL paper.",
                    "label": 0
                },
                {
                    "sent": "Basically, we've found that on sparse extractions it increases the precision at a fixed level of recall, so precision at the top 20 to be precise.",
                    "label": 0
                },
                {
                    "sent": "By 90% so it really improves things a lot on sparse extractions.",
                    "label": 0
                },
                {
                    "sent": "So, so you mentioned that in the case of facts that are incorrect, you rely on the fact that there are a large number of sentences, so it's probably more correct ones than incorrect ones.",
                    "label": 0
                },
                {
                    "sent": "So in terms of what if you have facts that are correct at different points in time, so you mentioned Bloomberg as the mayor, right?",
                    "label": 0
                },
                {
                    "sent": "So maybe the day after a new mayor is elected, there's still more sentences that talk about Bloomberg as a mayor?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a very good point.",
                    "label": 0
                },
                {
                    "sent": "Along a number of dimensions, the first one is that this notion of redundancy.",
                    "label": 0
                },
                {
                    "sent": "It's a very powerful tool, but it's a heuristic.",
                    "label": 0
                },
                {
                    "sent": "OK, it's often wrong, so if there's a conspiracy theory that's very popular.",
                    "label": 0
                },
                {
                    "sent": "Again, text Runner will will happily believe that, right?",
                    "label": 0
                },
                {
                    "sent": "So first of all, it can easily be wrong, and again, there's countervailing forces, like if we look at authoritativeness of a source and so on.",
                    "label": 0
                },
                {
                    "sent": "But the point that you brought up in particular temporally scoped facts and so.",
                    "label": 0
                },
                {
                    "sent": "Text Runner is not very good with that, but that's not a function of the paradigm.",
                    "label": 0
                },
                {
                    "sent": "OK, we just need to invest the time to temporally scope the facts, and that's nontrivial because often the temporal scoping is not in that sentence, so we really need to extend extraction to go beyond single sentence to put in like you were saying.",
                    "label": 0
                },
                {
                    "sent": "Context an, for example, to associate.",
                    "label": 0
                },
                {
                    "sent": "You can think of this as metadata with every sentence.",
                    "label": 0
                },
                {
                    "sent": "Is it always true?",
                    "label": 0
                },
                {
                    "sent": "Is a true over a particular time interval, and so on and so on, and so that's very much.",
                    "label": 0
                },
                {
                    "sent": "A problem for future research that we haven't solved.",
                    "label": 0
                },
                {
                    "sent": "Hi, you made a really interesting comment near the end of your talk where you said if we had a couple more orders of magnitude of data then you'd see that the performance would be just hugely more and you also had a graph a little bit before that really showed that you have a lot of common things that are repeated very often in this huge tail.",
                    "label": 0
                },
                {
                    "sent": "So if you went up a couple orders of magnitude than your data is much more sparse.",
                    "label": 0
                },
                {
                    "sent": "The question I have is do you have a sense or have you actually measured?",
                    "label": 0
                },
                {
                    "sent": "What's the critical mass of information you have to be able to answer certain kinds of questions and.",
                    "label": 0
                },
                {
                    "sent": "How do you know that?",
                    "label": 0
                },
                {
                    "sent": "How much do you really need and what really is going to happen if you have a couple orders of magnitude more data?",
                    "label": 0
                },
                {
                    "sent": "So those are great great questions, so let me see if I can take him first of all, of course, you're right that however much more data we have, we're still going to have that that long tail.",
                    "label": 0
                },
                {
                    "sent": "Think that's why the work on sparse extractions is interesting and relevant.",
                    "label": 0
                },
                {
                    "sent": "We didn't just say, Oh well, let's just increase our data set, but that is still a problem.",
                    "label": 0
                },
                {
                    "sent": "And then to your other question.",
                    "label": 0
                },
                {
                    "sent": "Well, OK, how well does this thing actually work and how well would work if we have two orders of magnitude more data?",
                    "label": 0
                },
                {
                    "sent": "The thing that's hard about that is there isn't.",
                    "label": 0
                },
                {
                    "sent": "Another iterative data set.",
                    "label": 0
                },
                {
                    "sent": "How do you know?",
                    "label": 0
                },
                {
                    "sent": "And you definitely don't know what's going to happen if you have two orders of magnitude more data.",
                    "label": 0
                },
                {
                    "sent": "So the approach we've taken is to put our prototype up there on the web for people to play with.",
                    "label": 0
                },
                {
                    "sent": "We've done systematic studies with small numbers of relations and with samples, and I alluded to some of those results, but the truth is.",
                    "label": 0
                },
                {
                    "sent": "I give you a succinct answer to your question.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "But I was thinking is that if you put 10 machines to do this thing for a month, you can build a database of fact based on the size of psych very easily.",
                    "label": 0
                },
                {
                    "sent": "Why have you not been able to build it?",
                    "label": 0
                },
                {
                    "sent": "And if you have, what are the results?",
                    "label": 0
                },
                {
                    "sent": "I mean yeah, great question that let me pose a slightly different question.",
                    "label": 0
                },
                {
                    "sent": "I think you're basically asking how does this compare with psych, right?",
                    "label": 0
                },
                {
                    "sent": "And the answer is that already.",
                    "label": 0
                },
                {
                    "sent": "This is much larger than psych in terms of the numbers of assertions.",
                    "label": 0
                },
                {
                    "sent": "At the same time, it's also quite a bit noisier than side 'cause as you can imagine, right, there's all kinds of.",
                    "label": 0
                },
                {
                    "sent": "Incorrect facts, I think that also the they've spent a huge amount of time worrying about the semantics of cycle.",
                    "label": 0
                },
                {
                    "sent": "An inference over psych and what we're dealing with those textual strings that again are noisy and the semantics are not clear right.",
                    "label": 0
                },
                {
                    "sent": "Contradictions happily coexist inside of text runner, so I would summarize this by saying this is much, much cheaper, much more scalable right we spend.",
                    "label": 0
                },
                {
                    "sent": "A couple of years building text runner on a shoestring.",
                    "label": 0
                },
                {
                    "sent": "Compared to Sykes, budget and it's higher recall but lower precision.",
                    "label": 0
                },
                {
                    "sent": "And of course there's a challenge with Psych to actually get some interesting applications of any kind built on top of that, where as we've already at least demonstrated things like opine and so on that that work.",
                    "label": 0
                },
                {
                    "sent": "And I think you'll see more applications coming out of our group.",
                    "label": 0
                },
                {
                    "sent": "But that's really the the tradeoff here.",
                    "label": 0
                },
                {
                    "sent": "Right now.",
                    "label": 0
                },
                {
                    "sent": "This is precision, recall, and cost, and we were higher.",
                    "label": 0
                },
                {
                    "sent": "Recall more scalable at much lower cost.",
                    "label": 0
                },
                {
                    "sent": "But lower precision.",
                    "label": 0
                },
                {
                    "sent": "So you mentioned earlier talk the idea of applying this to the web and you know integrating it with search engines to augment the search experience.",
                    "label": 0
                },
                {
                    "sent": "I really like that idea, but from what I've heard I don't work for any of the major search engines, but the common wisdom is the user is sort of too lazy to do anything really beyond type in two or three words and their query.",
                    "label": 0
                },
                {
                    "sent": "Do you have any thoughts about how you might once you have all this good data how you might present it to the user in a way to make the user actually really makes use of it?",
                    "label": 0
                },
                {
                    "sent": "You know the HCI or user interface issues around making good use of this data.",
                    "label": 0
                },
                {
                    "sent": "So that's a great question and I have a couple of ideas.",
                    "label": 0
                },
                {
                    "sent": "Is not something I spend a huge amount of time thinking about, but I will say one is you do have to wonder about a statement of the form the user and I mean I know that people very much say that right?",
                    "label": 0
                },
                {
                    "sent": "But I mean we're talking about billions of people, so there's probably classes of users with different behaviors, so some might be more motivated to do some work than others.",
                    "label": 0
                },
                {
                    "sent": "1 #2, as I was trying to say that there are a number of things that.",
                    "label": 0
                },
                {
                    "sent": "Happened behind the scenes OK. For example, you could use these extraction models as features in a ranking function, right?",
                    "label": 0
                },
                {
                    "sent": "So it doesn't change user behavior, but to me actually the most exciting thing is supporting an incredibly lazy user by giving them high quality information that they can't get today.",
                    "label": 0
                },
                {
                    "sent": "So again, let's take this opinion mining or review mining example right now to get the information I want about hotels takes me literally an hour.",
                    "label": 0
                },
                {
                    "sent": "If you can do this kind of information Fusion information synthesis.",
                    "label": 0
                },
                {
                    "sent": "On top of extractions it could take a couple of minutes to figure out.",
                    "label": 0
                },
                {
                    "sent": "Can I find a quiet hotel in Vancouver by extracting information from the reviews so I actually think that the most exciting use of this is extraction is to allow you to ask questions and to answer questions.",
                    "label": 0
                },
                {
                    "sent": "The right now require huge amount of manual labor so let's help the lazy OUSD users.",
                    "label": 0
                },
                {
                    "sent": "The best computer scientists right?",
                    "label": 0
                },
                {
                    "sent": "This is a lazy computer scientist.",
                    "label": 0
                },
                {
                    "sent": "More questions.",
                    "label": 0
                },
                {
                    "sent": "Well, let's thank the speaker.",
                    "label": 0
                }
            ]
        }
    }
}