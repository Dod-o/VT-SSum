{
    "id": "36t4jfqxnsvf7ham4kyc5o7jryl23isa",
    "title": "Kernel Choice and Classifiability for RKHS Embeddings of Probability Distributions",
    "info": {
        "author": [
            "Bharath K. Sriperumbudur, Department of Electrical and Computer Engineering, UC San Diego"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/nips09_sriperumbudur_kcc/",
    "segmentation": [
        [
            "Good morning everybody.",
            "As the title of the talk says."
        ],
        [
            "About our coaches.",
            "Embedding of probability measures.",
            "Traditionally, kernel methods have been established as a powerful method of constructing nonlinear algorithms from linear algorithms by embedding points into reproducing kernel space.",
            "So let us extend this idea by embedding probability measures into reproducing connected, but space it can be shown that this provides a very nice way of dealing with the higher order statistics of these random variables are random variables and.",
            "Even if you choose dark, it just to be rich enough, then the probability measure can be completely characterized by these embeddings.",
            "So the idea in this talk is to define a distance measure between these probabilities by computing the distance between the embeddings."
        ],
        [
            "Shown here, so why do you need this notion of distance between poverty measures?",
            "So let us consider this application, which is a classic example problem where we are given these random samples, two sets of random samples drawn from 2 distributions P&Q and the question is you need to distinguish whether these samples are drawn from the same distribution or not.",
            "So let us define some distance measure gamma between two probability measures P&Q and I can pose this hypothesis test equality in this fashion.",
            "And now the problem is based on these samples that have done from P&QI estimate, what is the distance between the empirical estimates of P&Q and the test is to say if the distance is less than some threshold, we sapientia the same.",
            "If not, P&Q are not the same.",
            "So this is 1 one application where the distance between powered measures come into picture.",
            "So there are a bunch of applications where this notion of distance between quality measures play a role."
        ],
        [
            "As I mentioned in the in the case of testing, but for the independence or the conditional independence in the case of goodness of fit test.",
            "Even in the case of density estimation, where you're given samples drawn from some distribution, someone known distribution and you estimate the density based on these samples.",
            "So you need to know how good is the quality for estimate, so you have to come.",
            "You have to have some notion of distance between these provisions.",
            "So the distance between partners also has played role, including Central limit theorems in information theory and so on.",
            "So here I mentioned some of the examples of popular examples of these distance measures.",
            "All of these are like special instances of something called the divergents."
        ],
        [
            "So there is another class of distance measures called integral probability matrix which is defined here.",
            "So as you can see, this is basically the maximum difference between the means computed with respect to two distributions, P&Q and.",
            "The maximum is taken over a function class F. It can be shown that if you are properly choose this function class as I've shown here 3 examples, you get the popular distance measures, like a total variation distance and versus time distance and this distance measures have been very well studied in probability theory and statistics.",
            "So recently in this function, classes chosen to be a unit ball in."
        ],
        [
            "Not get just an if that is the case, it has been shown to great and others that you can get a very nice close form expression for this distance.",
            "Measure an.",
            "And as I mentioned in the first slide, that you can exactly compute what is inviting that is associated with the probability measure P. So here the probability measure P is being embedded azzameen element in this dark ages, these amendment of the kernel computed with respect to this property measure, and it's an element in this dark ages."
        ],
        [
            "As I told before, if you choose Dark Ages to be rich enough then you will be able to.",
            "Perfectly characterize the probability measure P. So, as I mentioned before, you have a different function classes and each of these function classes induce a metric on the space of quality measures."
        ],
        [
            "Now, what is advantage of choosing the function class have to be unit ball in our cages.",
            "It can be shown that this choice allows you to compute the distance measure in a very efficient way and empirical estimate improvements to matter of this distance measure, called the maximum mean discrepancy.",
            "From now on I'll I'll refer to as MMD.",
            "The empirical estimate of MMD.",
            "It can be shown to be consistent estimator of MMD and the important point is the last one that if you have probability measures which are defined on structured domains like graphs and strings, then you can use MMD to compute the distance between these property measures.",
            "OK, so.",
            "So far I have mentioned as well for how defined a distance measure, called MMD on the space of quality measures.",
            "So we need this metric.",
            "This this.",
            "This distance actually to be a metric as shown in the two sample problem that you need to distinguish between two probability measures.",
            "So which means if the distance is zero, you need to have that it implies the measures of the same."
        ],
        [
            "It's not you, it's not.",
            "It's not always true that MMD is a metric, as it can be seen here.",
            "So if I choose my kernel to be a dot product kernel, then MMD is actually there to distance between the means associated with two distributions, P&Q, and therefore, if MMD is equal to 0, it does not imply the measures of the same.",
            "So we define a kernel is characteristic if the embedding imbalance being same implies the measures of the same.",
            "So the natural question to ask is, under what conditions is a kernel characteristic?",
            "So this has been unanswered in these papers that I mentioned below, and in each of these works the characterization comes up with certain kind of a challenge and therefore in this work where presented.",
            "A new characterization of these characteristic kernels, but I'm not going to cover this in this talk.",
            "Becausw, though this characterization, it's a nice little characterization, but it being little technical."
        ],
        [
            "So I don't want to cover it in this talk, so please visit the poster for the details about this characterization.",
            "So what have you done so far so far?",
            "I have introduced the notion of distance between probability measures and our differing this notion of characteristic kernel being the one which induces, which makes an empty to be metric.",
            "And it has been shown based on these characterizations that you have a lot of lot of kernels which are characteristic now.",
            "Which one do I choose in practice?",
            "So that is, that is, one of the questions that we answered in the paper in this work and the last one is how are these characteristic kernels and how they play an important role in binary classification.",
            "So these are the two things that will be covering this topic.",
            "Now to the choice of characteristic kernels.",
            "So, as I mentioned before, the popular colors like the Gaussian Laplacian spine kernels, these have been shown to be characteristic.",
            "Now, let us consider the setting of the kernel being the Russian Colonel.",
            "Here I've shown the kernel.",
            "Which is which is dependent on the bandit parameter Sigma.",
            "So now, based on the definition of MMD, which is gamma K as shown here.",
            "Based on the definition of MMD, MMD is now a function of the bandwidth parameter.",
            "So now, how do you choose the bandwidth parameter?",
            "So this is the classic problem in kernel methods.",
            "How do you choose kernel parameter?",
            "So since since MMD depends is a function of the bandwidth parameter, what you have is a family of metrics rather than a single number that defines the distance between two probability measures.",
            "An which of them should we choose?",
            "And it's also important that the bandit parameter is important in applications like the two sample test.",
            "Because let's say I choose the bandwidth parameter, which is arbitrarily small or arbitrarily large.",
            "You can show that MMD goes to zero irrespective of whether the distributions is.",
            "Distributions are same or not.",
            "So.",
            "So you need to be careful, prudent in choosing the bandwidth parameter.",
            "So to solve this problem, we define new distance measure, which is which is shown here.",
            "By taking the supremum of all the supremum over the bandwidth parameter of all the MD's.",
            "So what is the idea behind this?",
            "Suppose P&Q are different, so this this proposition.",
            "It allows you to choose abandoned parameter which is sensitive to the maximal difference between the maximal difference between the distributions."
        ],
        [
            "This notion can be generalized.",
            "Which I call it as a generalized MMD.",
            "So here, whatever done here, I've taken the supremum over a class of kernels.",
            "Answer broom supremely.",
            "Water class of kernels and this class of kernels are shown some examples so cagey represents a class of Gaussian kernels, so exactly reduces to the case that I've shown in the previous slide.",
            "K can be the class of radial basis functions or other popular choices would be the class of linear kernels, the class of convex combination of kernels and so on."
        ],
        [
            "So this slide shows how do you compute this distance measure.",
            "So by by invoking the reproducing, reproducing property of the kernel.",
            "So you can compute.",
            "You can give an expression like this for the generalized MMD in applications where the distance measure P&Q are known only through samples drawn IID from these distributions.",
            "P&Q, we can compute the empirical measures which are shown here as PM in QM, and I can compute the empirical."
        ],
        [
            "LA estimator of generalized MD.",
            "OK so so far I have introduced a new metric called the Generalized MMD and then have proposed and then I have shown like how you can estimate it based on the finite samples drawn from these two distributions.",
            "Now we have two questions.",
            "Is the first question.",
            "When is this generalized MMD metric?",
            "So if you need to use this in applications like to sample tests, you better need you better have generalized anxiety metric.",
            "So the answer is if any kernel in the kernel class script K if it is characteristic.",
            "Dan Gomez, symmetric so generous MMD symmetric, if any kernel K is correct."
        ],
        [
            "Stick.",
            "The next question.",
            "For fix it K which is the case with the general, which is the case with MMD.",
            "It has been shown that empirical estimator of MMD is a consistent estimator.",
            "And here you have the rate of convergence.",
            "Now we can ask the same question what happens in the case of generalized MD?",
            "Do we have an almost sure convergence to generalized empirical estimator?",
            "Does it converge almost surely to the generalized MMD?",
            "And what is the rate of convergence?",
            "So this is, this is particularly useful when you're designing tests.",
            "You you basically need to get a good estimate of generalized MD by using as few samples as possible.",
            "So this is the statistical consistent result so.",
            "This result, this is these are my general result which depends on your kernel class, script, care.",
            "So here the terms Umm of K and UFC is mentioned here, so this is called the ramaker chaos complexity.",
            "So this can be seen as the 2nd order version of the Rademacher complexity.",
            "And if so, you will get.",
            "Consistency guarantees if this this term behaves well and."
        ],
        [
            "It can be shown that if K services sub graph class then what we get is the generalized MMD essentially behaves something like.",
            "MD in the case of statistical consistency results, so you get the similar rate of convergence and then you also get almost sure convergence.",
            "So what are the what are the classes that satisfy which obviously subgraph classes so unpopular kernel classes like the Gaussian kernels, the class of RBF kernels, the linear combination of kernels and convex combination of kernels?",
            "So all these classes of kernels which are popularly used in machine learning.",
            "So all these classes, RV subclasses.",
            "So using any of any kernel class gives you the statistical consistency result.",
            "OK."
        ],
        [
            "So, so we have answered the question of any generalized India metric and we have shown the statistical consistency result of generalized MMD.",
            "Now we need to see whether the proposition that we made about generalized empty.",
            "Is it true in the sense?",
            "Does it really solve the problem which is associated with MMD?",
            "So let us revisit the two sample problem where I just recap it.",
            "So we were given these samples."
        ],
        [
            "Done ID from 2 distributions P&Q and we need to test whether P&Q are same or not.",
            "So here.",
            "We re pose the the hypothesis testing problem in this way, so we can use generalized MMD as a test statistic.",
            "And we can use MMD as a test statistic.",
            "And we need to compare how these two test perform.",
            "So we say a test is good for user defined type 1 error type, 2 error is less.",
            "So this is experimental setup where you have I choose cute aggression distribution as shown here and then I can construct the probability measure P by perturbing this distribution Q.",
            "So here have shown."
        ],
        [
            "No party measure P for two different perturbation parameters, so here the automation is done in this way and the perturbation is controlled by this parameter new.",
            "So what is idea?",
            "If you if you put on distribution Q so that if P is very nicely, then to distinguish between P&Q you need a valid parameter that is small enough.",
            "So you need a parameter.",
            "You need the bandwidth parameter that is actually adapted to these distributions rather than some heuristically set parameter.",
            "So here we chose the kernel to be Gaussian an.",
            "So we use generalized MMD.",
            "Based down, we compute the generalized MMD based on the samples run from these distributions, and then we use MMD based on the samples drawn from these distributions for various values of the bandwidth parameter.",
            "Now we have this result which is obtained using generalized MMD.",
            "What does it show you?"
        ],
        [
            "So we fix the Type 1 error for, so here that's axis represents the perturbation parameter for every perturbation parameter.",
            "We fix the Type 1 error to be 5% and then we compute what is a Type 2 error given by the test and the Type 2 error here is zero, which means it always distinguishes between two different distributions.",
            "Now this is the result that is associated with MMD.",
            "Here."
        ],
        [
            "Next, accesses the logarithm of the bandwidth parameter.",
            "This plot shows the Type 1 error and this shows the Type 2 error.",
            "So the Type 1 error is fixed to 5% and this is the plot of the Type 2 error.",
            "So what we see in the Type 2 error is let's choose.",
            "Oh, if we, if we increase automation parameter so you need.",
            "Smaller and smaller bandwidths to distinguish between two different distributions, as you can actually see here.",
            "And if you choose abandoned parameter that is.",
            "That is somewhat large.",
            "Large is again dependent on what depends on the distributions.",
            "So if we choose abandoned parameter that is large, you can have arbitrary large type 2 errors.",
            "So these two plots show that the basic idea in proposing the generalized memory actually is good, and it works.",
            "So for more results please visit my."
        ],
        [
            "The poster.",
            "OK, so so far we have introduced this notion of characteristic kernels and then have given away how to choose the the kernel parameter in the character kernels, especially in a 2 sample problem.",
            "Now I relate how the characteristic kernels as search are very important in binary classification.",
            "So this is based on the intuition that if you have two distributions which are easily distinguishable in some sense, then it should be easily classifiable.",
            "So this can be proven genetically in the context of MMD.",
            "So this is our first result which relates MMD.",
            "The person window classifier."
        ],
        [
            "So this is the result which says so if P&Q are the class conditional distributions associated.",
            "With two classes plus one and minus one, and if I compute the MMD between these two class conditional distributions then what I have is this distance is actually the negative of the Bose risk that is associated with the person window classifier.",
            "So if you restrict your classifier to live within a unit ball in the dark Ages, then the risk that is associated with such a classifier with the Bayes risk associated with such a classifier is actually negative of MMD computed between two class conditional distributions P&Q.",
            "Now here you can see why the characteristic property of the colonies important.",
            "Suppose let's say K is not characteristic, which means for two distributions distinguish different distributions for P not equal to Q you can have."
        ],
        [
            "This distance measure being zero, which means the risk is maximum even when P&Q are different.",
            "So you need to have the characteristic property to achieve maximum risk and the maximum risk is achieved only when P is equal to Q.",
            "In the case if K is characteristic.",
            "Now we have other result which relates MMD to support Vector machine.",
            "So let FCM be the solution to hard margin support."
        ],
        [
            "Permission classifier then you can show.",
            "But if the current is characteristic you have this kind of a bound an.",
            "So the right answer delay the right, left right upper bound is is basically the margin that is associated with the hard margin SVM classifier.",
            "So it says the margin is bounded by half of MMD.",
            "That is computed between.",
            "Between the empirical distributions based on samples.",
            "Alright, so suppose if it is, suppose if this distance is small in the sense.",
            "If it's, it's not easy to distinguish between these two distributions, which means the margin of the same classifier is not big, so the two distributions are not distinguishable in the sense of MMD.",
            "So the right hand side of the expression is.",
            "So you have a risk that is associated with some classifier and you compute and you basically compute the Bayes risk which is compared over the class of all measurable functions.",
            "And the return of the expression used Bayes risk that is associated with a classifier that is contained in smaller class.",
            "So usually in the regularization based algorithms you always restrict your instead of dealing with the class of all measurable functions, you deal with the smaller class.",
            "And here we say that the function class H achieves the base risk.",
            "If this expression is satisfied under some conditions, it can be shown that the characteristic property of K is necessary for this condition to be satisfied.",
            "And if the constant functions are included in this or cages, you can show that the the the kernel being characteristic is sufficient for this condition to be satisfied, so.",
            "So this shows that the characteristic kernels play a role not only in distinguishing two different distributions, but also in the class of binary classification."
        ],
        [
            "To summarize.",
            "Introduced this notion of characteristic kernel, which in which introduced which induces a metric on the space of probability measures which is called as MMD and.",
            "I've shown how to choose characters kernels in practice by introducing this notion of generalized MMD, and are shown that this generalized MD performs better compared to MMD in the case of a 2 sample test.",
            "An I'm motivated why characters kernels are important in binary classification.",
            "Thank you.",
            "OK, no, there's lunch.",
            "But I have any questions."
        ],
        [
            "So it's just some very nice work whether there is a point here that choosing the correct I mean for the two sample problem, we know that basically you."
        ],
        [
            "Do not answer it with final samples.",
            "And the question is, how do you choose your measures in such a way that the distance that you can achieve is relevant to your problem?",
            "So like 2 distributions could be very different and still the samples will not be able to detect it there different.",
            "The question is what is the notion of difference that is relevant to your problem more than what is the notion of difference that will give you a non zero distance between them.",
            "So I I somehow I feel that this emphasis on characterization property is is kind of a little bit misleading because the real thing is what the distances between the distributions are relevant to your application.",
            "I mean finally you have to compute distance between distributions and so so so you need you need to have a way of measuring the distance between distributions.",
            "Right?",
            "Maybe maybe we should take it offline."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good morning everybody.",
                    "label": 0
                },
                {
                    "sent": "As the title of the talk says.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About our coaches.",
                    "label": 0
                },
                {
                    "sent": "Embedding of probability measures.",
                    "label": 0
                },
                {
                    "sent": "Traditionally, kernel methods have been established as a powerful method of constructing nonlinear algorithms from linear algorithms by embedding points into reproducing kernel space.",
                    "label": 0
                },
                {
                    "sent": "So let us extend this idea by embedding probability measures into reproducing connected, but space it can be shown that this provides a very nice way of dealing with the higher order statistics of these random variables are random variables and.",
                    "label": 0
                },
                {
                    "sent": "Even if you choose dark, it just to be rich enough, then the probability measure can be completely characterized by these embeddings.",
                    "label": 0
                },
                {
                    "sent": "So the idea in this talk is to define a distance measure between these probabilities by computing the distance between the embeddings.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Shown here, so why do you need this notion of distance between poverty measures?",
                    "label": 0
                },
                {
                    "sent": "So let us consider this application, which is a classic example problem where we are given these random samples, two sets of random samples drawn from 2 distributions P&Q and the question is you need to distinguish whether these samples are drawn from the same distribution or not.",
                    "label": 0
                },
                {
                    "sent": "So let us define some distance measure gamma between two probability measures P&Q and I can pose this hypothesis test equality in this fashion.",
                    "label": 0
                },
                {
                    "sent": "And now the problem is based on these samples that have done from P&QI estimate, what is the distance between the empirical estimates of P&Q and the test is to say if the distance is less than some threshold, we sapientia the same.",
                    "label": 0
                },
                {
                    "sent": "If not, P&Q are not the same.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 one application where the distance between powered measures come into picture.",
                    "label": 0
                },
                {
                    "sent": "So there are a bunch of applications where this notion of distance between quality measures play a role.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "As I mentioned in the in the case of testing, but for the independence or the conditional independence in the case of goodness of fit test.",
                    "label": 1
                },
                {
                    "sent": "Even in the case of density estimation, where you're given samples drawn from some distribution, someone known distribution and you estimate the density based on these samples.",
                    "label": 0
                },
                {
                    "sent": "So you need to know how good is the quality for estimate, so you have to come.",
                    "label": 0
                },
                {
                    "sent": "You have to have some notion of distance between these provisions.",
                    "label": 1
                },
                {
                    "sent": "So the distance between partners also has played role, including Central limit theorems in information theory and so on.",
                    "label": 1
                },
                {
                    "sent": "So here I mentioned some of the examples of popular examples of these distance measures.",
                    "label": 0
                },
                {
                    "sent": "All of these are like special instances of something called the divergents.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So there is another class of distance measures called integral probability matrix which is defined here.",
                    "label": 1
                },
                {
                    "sent": "So as you can see, this is basically the maximum difference between the means computed with respect to two distributions, P&Q and.",
                    "label": 0
                },
                {
                    "sent": "The maximum is taken over a function class F. It can be shown that if you are properly choose this function class as I've shown here 3 examples, you get the popular distance measures, like a total variation distance and versus time distance and this distance measures have been very well studied in probability theory and statistics.",
                    "label": 1
                },
                {
                    "sent": "So recently in this function, classes chosen to be a unit ball in.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not get just an if that is the case, it has been shown to great and others that you can get a very nice close form expression for this distance.",
                    "label": 0
                },
                {
                    "sent": "Measure an.",
                    "label": 0
                },
                {
                    "sent": "And as I mentioned in the first slide, that you can exactly compute what is inviting that is associated with the probability measure P. So here the probability measure P is being embedded azzameen element in this dark ages, these amendment of the kernel computed with respect to this property measure, and it's an element in this dark ages.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As I told before, if you choose Dark Ages to be rich enough then you will be able to.",
                    "label": 0
                },
                {
                    "sent": "Perfectly characterize the probability measure P. So, as I mentioned before, you have a different function classes and each of these function classes induce a metric on the space of quality measures.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, what is advantage of choosing the function class have to be unit ball in our cages.",
                    "label": 0
                },
                {
                    "sent": "It can be shown that this choice allows you to compute the distance measure in a very efficient way and empirical estimate improvements to matter of this distance measure, called the maximum mean discrepancy.",
                    "label": 0
                },
                {
                    "sent": "From now on I'll I'll refer to as MMD.",
                    "label": 0
                },
                {
                    "sent": "The empirical estimate of MMD.",
                    "label": 0
                },
                {
                    "sent": "It can be shown to be consistent estimator of MMD and the important point is the last one that if you have probability measures which are defined on structured domains like graphs and strings, then you can use MMD to compute the distance between these property measures.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "So far I have mentioned as well for how defined a distance measure, called MMD on the space of quality measures.",
                    "label": 0
                },
                {
                    "sent": "So we need this metric.",
                    "label": 0
                },
                {
                    "sent": "This this.",
                    "label": 0
                },
                {
                    "sent": "This distance actually to be a metric as shown in the two sample problem that you need to distinguish between two probability measures.",
                    "label": 0
                },
                {
                    "sent": "So which means if the distance is zero, you need to have that it implies the measures of the same.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's not you, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not always true that MMD is a metric, as it can be seen here.",
                    "label": 0
                },
                {
                    "sent": "So if I choose my kernel to be a dot product kernel, then MMD is actually there to distance between the means associated with two distributions, P&Q, and therefore, if MMD is equal to 0, it does not imply the measures of the same.",
                    "label": 0
                },
                {
                    "sent": "So we define a kernel is characteristic if the embedding imbalance being same implies the measures of the same.",
                    "label": 0
                },
                {
                    "sent": "So the natural question to ask is, under what conditions is a kernel characteristic?",
                    "label": 0
                },
                {
                    "sent": "So this has been unanswered in these papers that I mentioned below, and in each of these works the characterization comes up with certain kind of a challenge and therefore in this work where presented.",
                    "label": 0
                },
                {
                    "sent": "A new characterization of these characteristic kernels, but I'm not going to cover this in this talk.",
                    "label": 1
                },
                {
                    "sent": "Becausw, though this characterization, it's a nice little characterization, but it being little technical.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I don't want to cover it in this talk, so please visit the poster for the details about this characterization.",
                    "label": 0
                },
                {
                    "sent": "So what have you done so far so far?",
                    "label": 0
                },
                {
                    "sent": "I have introduced the notion of distance between probability measures and our differing this notion of characteristic kernel being the one which induces, which makes an empty to be metric.",
                    "label": 0
                },
                {
                    "sent": "And it has been shown based on these characterizations that you have a lot of lot of kernels which are characteristic now.",
                    "label": 0
                },
                {
                    "sent": "Which one do I choose in practice?",
                    "label": 1
                },
                {
                    "sent": "So that is, that is, one of the questions that we answered in the paper in this work and the last one is how are these characteristic kernels and how they play an important role in binary classification.",
                    "label": 0
                },
                {
                    "sent": "So these are the two things that will be covering this topic.",
                    "label": 1
                },
                {
                    "sent": "Now to the choice of characteristic kernels.",
                    "label": 0
                },
                {
                    "sent": "So, as I mentioned before, the popular colors like the Gaussian Laplacian spine kernels, these have been shown to be characteristic.",
                    "label": 0
                },
                {
                    "sent": "Now, let us consider the setting of the kernel being the Russian Colonel.",
                    "label": 0
                },
                {
                    "sent": "Here I've shown the kernel.",
                    "label": 0
                },
                {
                    "sent": "Which is which is dependent on the bandit parameter Sigma.",
                    "label": 0
                },
                {
                    "sent": "So now, based on the definition of MMD, which is gamma K as shown here.",
                    "label": 0
                },
                {
                    "sent": "Based on the definition of MMD, MMD is now a function of the bandwidth parameter.",
                    "label": 0
                },
                {
                    "sent": "So now, how do you choose the bandwidth parameter?",
                    "label": 0
                },
                {
                    "sent": "So this is the classic problem in kernel methods.",
                    "label": 0
                },
                {
                    "sent": "How do you choose kernel parameter?",
                    "label": 0
                },
                {
                    "sent": "So since since MMD depends is a function of the bandwidth parameter, what you have is a family of metrics rather than a single number that defines the distance between two probability measures.",
                    "label": 1
                },
                {
                    "sent": "An which of them should we choose?",
                    "label": 0
                },
                {
                    "sent": "And it's also important that the bandit parameter is important in applications like the two sample test.",
                    "label": 0
                },
                {
                    "sent": "Because let's say I choose the bandwidth parameter, which is arbitrarily small or arbitrarily large.",
                    "label": 0
                },
                {
                    "sent": "You can show that MMD goes to zero irrespective of whether the distributions is.",
                    "label": 0
                },
                {
                    "sent": "Distributions are same or not.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So you need to be careful, prudent in choosing the bandwidth parameter.",
                    "label": 0
                },
                {
                    "sent": "So to solve this problem, we define new distance measure, which is which is shown here.",
                    "label": 0
                },
                {
                    "sent": "By taking the supremum of all the supremum over the bandwidth parameter of all the MD's.",
                    "label": 0
                },
                {
                    "sent": "So what is the idea behind this?",
                    "label": 0
                },
                {
                    "sent": "Suppose P&Q are different, so this this proposition.",
                    "label": 0
                },
                {
                    "sent": "It allows you to choose abandoned parameter which is sensitive to the maximal difference between the maximal difference between the distributions.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This notion can be generalized.",
                    "label": 0
                },
                {
                    "sent": "Which I call it as a generalized MMD.",
                    "label": 0
                },
                {
                    "sent": "So here, whatever done here, I've taken the supremum over a class of kernels.",
                    "label": 0
                },
                {
                    "sent": "Answer broom supremely.",
                    "label": 0
                },
                {
                    "sent": "Water class of kernels and this class of kernels are shown some examples so cagey represents a class of Gaussian kernels, so exactly reduces to the case that I've shown in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "K can be the class of radial basis functions or other popular choices would be the class of linear kernels, the class of convex combination of kernels and so on.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this slide shows how do you compute this distance measure.",
                    "label": 0
                },
                {
                    "sent": "So by by invoking the reproducing, reproducing property of the kernel.",
                    "label": 0
                },
                {
                    "sent": "So you can compute.",
                    "label": 0
                },
                {
                    "sent": "You can give an expression like this for the generalized MMD in applications where the distance measure P&Q are known only through samples drawn IID from these distributions.",
                    "label": 0
                },
                {
                    "sent": "P&Q, we can compute the empirical measures which are shown here as PM in QM, and I can compute the empirical.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "LA estimator of generalized MD.",
                    "label": 0
                },
                {
                    "sent": "OK so so far I have introduced a new metric called the Generalized MMD and then have proposed and then I have shown like how you can estimate it based on the finite samples drawn from these two distributions.",
                    "label": 0
                },
                {
                    "sent": "Now we have two questions.",
                    "label": 0
                },
                {
                    "sent": "Is the first question.",
                    "label": 0
                },
                {
                    "sent": "When is this generalized MMD metric?",
                    "label": 0
                },
                {
                    "sent": "So if you need to use this in applications like to sample tests, you better need you better have generalized anxiety metric.",
                    "label": 0
                },
                {
                    "sent": "So the answer is if any kernel in the kernel class script K if it is characteristic.",
                    "label": 0
                },
                {
                    "sent": "Dan Gomez, symmetric so generous MMD symmetric, if any kernel K is correct.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Stick.",
                    "label": 0
                },
                {
                    "sent": "The next question.",
                    "label": 0
                },
                {
                    "sent": "For fix it K which is the case with the general, which is the case with MMD.",
                    "label": 0
                },
                {
                    "sent": "It has been shown that empirical estimator of MMD is a consistent estimator.",
                    "label": 0
                },
                {
                    "sent": "And here you have the rate of convergence.",
                    "label": 0
                },
                {
                    "sent": "Now we can ask the same question what happens in the case of generalized MD?",
                    "label": 0
                },
                {
                    "sent": "Do we have an almost sure convergence to generalized empirical estimator?",
                    "label": 0
                },
                {
                    "sent": "Does it converge almost surely to the generalized MMD?",
                    "label": 0
                },
                {
                    "sent": "And what is the rate of convergence?",
                    "label": 1
                },
                {
                    "sent": "So this is, this is particularly useful when you're designing tests.",
                    "label": 0
                },
                {
                    "sent": "You you basically need to get a good estimate of generalized MD by using as few samples as possible.",
                    "label": 0
                },
                {
                    "sent": "So this is the statistical consistent result so.",
                    "label": 0
                },
                {
                    "sent": "This result, this is these are my general result which depends on your kernel class, script, care.",
                    "label": 0
                },
                {
                    "sent": "So here the terms Umm of K and UFC is mentioned here, so this is called the ramaker chaos complexity.",
                    "label": 0
                },
                {
                    "sent": "So this can be seen as the 2nd order version of the Rademacher complexity.",
                    "label": 0
                },
                {
                    "sent": "And if so, you will get.",
                    "label": 0
                },
                {
                    "sent": "Consistency guarantees if this this term behaves well and.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It can be shown that if K services sub graph class then what we get is the generalized MMD essentially behaves something like.",
                    "label": 1
                },
                {
                    "sent": "MD in the case of statistical consistency results, so you get the similar rate of convergence and then you also get almost sure convergence.",
                    "label": 0
                },
                {
                    "sent": "So what are the what are the classes that satisfy which obviously subgraph classes so unpopular kernel classes like the Gaussian kernels, the class of RBF kernels, the linear combination of kernels and convex combination of kernels?",
                    "label": 0
                },
                {
                    "sent": "So all these classes of kernels which are popularly used in machine learning.",
                    "label": 0
                },
                {
                    "sent": "So all these classes, RV subclasses.",
                    "label": 0
                },
                {
                    "sent": "So using any of any kernel class gives you the statistical consistency result.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so we have answered the question of any generalized India metric and we have shown the statistical consistency result of generalized MMD.",
                    "label": 1
                },
                {
                    "sent": "Now we need to see whether the proposition that we made about generalized empty.",
                    "label": 0
                },
                {
                    "sent": "Is it true in the sense?",
                    "label": 0
                },
                {
                    "sent": "Does it really solve the problem which is associated with MMD?",
                    "label": 0
                },
                {
                    "sent": "So let us revisit the two sample problem where I just recap it.",
                    "label": 0
                },
                {
                    "sent": "So we were given these samples.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Done ID from 2 distributions P&Q and we need to test whether P&Q are same or not.",
                    "label": 0
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "We re pose the the hypothesis testing problem in this way, so we can use generalized MMD as a test statistic.",
                    "label": 0
                },
                {
                    "sent": "And we can use MMD as a test statistic.",
                    "label": 0
                },
                {
                    "sent": "And we need to compare how these two test perform.",
                    "label": 0
                },
                {
                    "sent": "So we say a test is good for user defined type 1 error type, 2 error is less.",
                    "label": 0
                },
                {
                    "sent": "So this is experimental setup where you have I choose cute aggression distribution as shown here and then I can construct the probability measure P by perturbing this distribution Q.",
                    "label": 0
                },
                {
                    "sent": "So here have shown.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No party measure P for two different perturbation parameters, so here the automation is done in this way and the perturbation is controlled by this parameter new.",
                    "label": 0
                },
                {
                    "sent": "So what is idea?",
                    "label": 0
                },
                {
                    "sent": "If you if you put on distribution Q so that if P is very nicely, then to distinguish between P&Q you need a valid parameter that is small enough.",
                    "label": 0
                },
                {
                    "sent": "So you need a parameter.",
                    "label": 0
                },
                {
                    "sent": "You need the bandwidth parameter that is actually adapted to these distributions rather than some heuristically set parameter.",
                    "label": 0
                },
                {
                    "sent": "So here we chose the kernel to be Gaussian an.",
                    "label": 0
                },
                {
                    "sent": "So we use generalized MMD.",
                    "label": 0
                },
                {
                    "sent": "Based down, we compute the generalized MMD based on the samples run from these distributions, and then we use MMD based on the samples drawn from these distributions for various values of the bandwidth parameter.",
                    "label": 0
                },
                {
                    "sent": "Now we have this result which is obtained using generalized MMD.",
                    "label": 0
                },
                {
                    "sent": "What does it show you?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we fix the Type 1 error for, so here that's axis represents the perturbation parameter for every perturbation parameter.",
                    "label": 0
                },
                {
                    "sent": "We fix the Type 1 error to be 5% and then we compute what is a Type 2 error given by the test and the Type 2 error here is zero, which means it always distinguishes between two different distributions.",
                    "label": 0
                },
                {
                    "sent": "Now this is the result that is associated with MMD.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Next, accesses the logarithm of the bandwidth parameter.",
                    "label": 0
                },
                {
                    "sent": "This plot shows the Type 1 error and this shows the Type 2 error.",
                    "label": 0
                },
                {
                    "sent": "So the Type 1 error is fixed to 5% and this is the plot of the Type 2 error.",
                    "label": 0
                },
                {
                    "sent": "So what we see in the Type 2 error is let's choose.",
                    "label": 0
                },
                {
                    "sent": "Oh, if we, if we increase automation parameter so you need.",
                    "label": 0
                },
                {
                    "sent": "Smaller and smaller bandwidths to distinguish between two different distributions, as you can actually see here.",
                    "label": 0
                },
                {
                    "sent": "And if you choose abandoned parameter that is.",
                    "label": 0
                },
                {
                    "sent": "That is somewhat large.",
                    "label": 0
                },
                {
                    "sent": "Large is again dependent on what depends on the distributions.",
                    "label": 0
                },
                {
                    "sent": "So if we choose abandoned parameter that is large, you can have arbitrary large type 2 errors.",
                    "label": 0
                },
                {
                    "sent": "So these two plots show that the basic idea in proposing the generalized memory actually is good, and it works.",
                    "label": 0
                },
                {
                    "sent": "So for more results please visit my.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The poster.",
                    "label": 0
                },
                {
                    "sent": "OK, so so far we have introduced this notion of characteristic kernels and then have given away how to choose the the kernel parameter in the character kernels, especially in a 2 sample problem.",
                    "label": 1
                },
                {
                    "sent": "Now I relate how the characteristic kernels as search are very important in binary classification.",
                    "label": 1
                },
                {
                    "sent": "So this is based on the intuition that if you have two distributions which are easily distinguishable in some sense, then it should be easily classifiable.",
                    "label": 0
                },
                {
                    "sent": "So this can be proven genetically in the context of MMD.",
                    "label": 0
                },
                {
                    "sent": "So this is our first result which relates MMD.",
                    "label": 0
                },
                {
                    "sent": "The person window classifier.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is the result which says so if P&Q are the class conditional distributions associated.",
                    "label": 0
                },
                {
                    "sent": "With two classes plus one and minus one, and if I compute the MMD between these two class conditional distributions then what I have is this distance is actually the negative of the Bose risk that is associated with the person window classifier.",
                    "label": 1
                },
                {
                    "sent": "So if you restrict your classifier to live within a unit ball in the dark Ages, then the risk that is associated with such a classifier with the Bayes risk associated with such a classifier is actually negative of MMD computed between two class conditional distributions P&Q.",
                    "label": 0
                },
                {
                    "sent": "Now here you can see why the characteristic property of the colonies important.",
                    "label": 0
                },
                {
                    "sent": "Suppose let's say K is not characteristic, which means for two distributions distinguish different distributions for P not equal to Q you can have.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This distance measure being zero, which means the risk is maximum even when P&Q are different.",
                    "label": 0
                },
                {
                    "sent": "So you need to have the characteristic property to achieve maximum risk and the maximum risk is achieved only when P is equal to Q.",
                    "label": 0
                },
                {
                    "sent": "In the case if K is characteristic.",
                    "label": 1
                },
                {
                    "sent": "Now we have other result which relates MMD to support Vector machine.",
                    "label": 1
                },
                {
                    "sent": "So let FCM be the solution to hard margin support.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Permission classifier then you can show.",
                    "label": 0
                },
                {
                    "sent": "But if the current is characteristic you have this kind of a bound an.",
                    "label": 0
                },
                {
                    "sent": "So the right answer delay the right, left right upper bound is is basically the margin that is associated with the hard margin SVM classifier.",
                    "label": 0
                },
                {
                    "sent": "So it says the margin is bounded by half of MMD.",
                    "label": 0
                },
                {
                    "sent": "That is computed between.",
                    "label": 0
                },
                {
                    "sent": "Between the empirical distributions based on samples.",
                    "label": 0
                },
                {
                    "sent": "Alright, so suppose if it is, suppose if this distance is small in the sense.",
                    "label": 0
                },
                {
                    "sent": "If it's, it's not easy to distinguish between these two distributions, which means the margin of the same classifier is not big, so the two distributions are not distinguishable in the sense of MMD.",
                    "label": 0
                },
                {
                    "sent": "So the right hand side of the expression is.",
                    "label": 0
                },
                {
                    "sent": "So you have a risk that is associated with some classifier and you compute and you basically compute the Bayes risk which is compared over the class of all measurable functions.",
                    "label": 1
                },
                {
                    "sent": "And the return of the expression used Bayes risk that is associated with a classifier that is contained in smaller class.",
                    "label": 0
                },
                {
                    "sent": "So usually in the regularization based algorithms you always restrict your instead of dealing with the class of all measurable functions, you deal with the smaller class.",
                    "label": 0
                },
                {
                    "sent": "And here we say that the function class H achieves the base risk.",
                    "label": 0
                },
                {
                    "sent": "If this expression is satisfied under some conditions, it can be shown that the characteristic property of K is necessary for this condition to be satisfied.",
                    "label": 0
                },
                {
                    "sent": "And if the constant functions are included in this or cages, you can show that the the the kernel being characteristic is sufficient for this condition to be satisfied, so.",
                    "label": 0
                },
                {
                    "sent": "So this shows that the characteristic kernels play a role not only in distinguishing two different distributions, but also in the class of binary classification.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To summarize.",
                    "label": 0
                },
                {
                    "sent": "Introduced this notion of characteristic kernel, which in which introduced which induces a metric on the space of probability measures which is called as MMD and.",
                    "label": 0
                },
                {
                    "sent": "I've shown how to choose characters kernels in practice by introducing this notion of generalized MMD, and are shown that this generalized MD performs better compared to MMD in the case of a 2 sample test.",
                    "label": 1
                },
                {
                    "sent": "An I'm motivated why characters kernels are important in binary classification.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "OK, no, there's lunch.",
                    "label": 0
                },
                {
                    "sent": "But I have any questions.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's just some very nice work whether there is a point here that choosing the correct I mean for the two sample problem, we know that basically you.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do not answer it with final samples.",
                    "label": 0
                },
                {
                    "sent": "And the question is, how do you choose your measures in such a way that the distance that you can achieve is relevant to your problem?",
                    "label": 0
                },
                {
                    "sent": "So like 2 distributions could be very different and still the samples will not be able to detect it there different.",
                    "label": 0
                },
                {
                    "sent": "The question is what is the notion of difference that is relevant to your problem more than what is the notion of difference that will give you a non zero distance between them.",
                    "label": 0
                },
                {
                    "sent": "So I I somehow I feel that this emphasis on characterization property is is kind of a little bit misleading because the real thing is what the distances between the distributions are relevant to your application.",
                    "label": 0
                },
                {
                    "sent": "I mean finally you have to compute distance between distributions and so so so you need you need to have a way of measuring the distance between distributions.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Maybe maybe we should take it offline.",
                    "label": 0
                }
            ]
        }
    }
}