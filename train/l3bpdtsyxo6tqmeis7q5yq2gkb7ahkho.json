{
    "id": "l3bpdtsyxo6tqmeis7q5yq2gkb7ahkho",
    "title": "Learning Vector Fields with Spectral Filtering",
    "info": {
        "author": [
            "Lorenzo Rosasco, Massachusetts Institute of Technology, MIT"
        ],
        "published": "Jan. 19, 2010",
        "recorded": "December 2009",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/nipsworkshops09_rosasco_lvfsf/",
    "segmentation": [
        [
            "Yeah, So what I'm going to talk about is very much complementary to everything we've seen today, so I care about stuff that.",
            "Pretty much nobody in the other talk looked at, and so it can give a view of.",
            "I'm not the only one, so you can give a view of some other concerns that might have while solving these kind of problems.",
            "So I really going to briefly through basically the same object that everybody used today, but like re phrasing it more like functional analytical framework.",
            "And then I'm just basically describe just the background, so I'm not going to see much on how do you actually choose the covariance function is going to be an object which is given, and I'm going to say once you have it kind of stuff you can do.",
            "And I'm going to try to make it brief after last another day, and I'd rather actually go and discuss a couple of thoughts that I have looking at other people talking while I start to work on this."
        ],
        [
            "Stuff.",
            "So very briefly, so.",
            "Let me just briefly recall this color case because I'll try to do a bit go on in analogy to this color case.",
            "I put very little assumption, but there are a lot in the kind of stuff I usually do, so just suppose that you want to function with the variables But in one dimension and you do it from samples.",
            "The sample for me will be always somewhat IID samples from some underlying problems distribution.",
            "There will be sampling and noise everybody every all these uncertainties will be probabilistic.",
            "So what I care about is doing a kind of kernel model where I basically take some kind of generalized linear model where my basis function, But this is.",
            "Is not to create computers is really a kernel, so it's a function of two variables and then I just point it at some point in space and when I take a linear combination of this, potentially an infinite dimensional.",
            "I just want this to be very very powerful.",
            "So we'll see how to handle that."
        ],
        [
            "So what is it more precisely?",
            "Well, for me, a kernel is the thing that he's behind a space of function.",
            "Huber space of functions have inner product of norms.",
            "And the kernel is a function of two variables and they test a couple of properties.",
            "If I fix X for any X, it belongs to the space, so it's a function for every X is a function in this space, and I've got this reproducing property so I can evaluate function at points.",
            "This will not play a big role in what I will say today is some kind of computational property that play role once we go down and try to do algorithmics.",
            "So, given a choice of this kernel, the typical idea is in regularization.",
            "The simplest one is to stick on regularization that basically says take a linear fit for Soria Square loss, kind of fit or any other loss function, among, say, most likely convex lock function in F and then it turns out that for many many different choice of K. This norm here can be thought as enforces and kind of simplicity, smoothness, control.",
            "The complexity of the function.",
            "So this is just setting up the notation, but I guess this."
        ],
        [
            "What everybody knows.",
            "So in machine learning, people typically choose the.",
            "There are a bunch of kernels in the books and then there are basically a couple of ways to build kernels, hudock and repair the three, but I'm just going to talk about two.",
            "The first one is it's rather than using the kernel you actually going to do a split some kind of feature map so you map your data using struct measurements out of your data.",
            "The other one, which is somewhat useful will be useful to compare to the multi output cases that.",
            "Rather than defining then fixing a kernel and then see the norm which is defined by the kernel, we're actually going to define some kind of norm.",
            "In some space of function and will see that this can be uniquely used by a kernel.",
            "OK, so this is well known for having splines theory when one look at a certain smoothness functional like that's the interval interval or the first derivative, the square integral of the first derivative, and this induces certain norm and then one can trace back the kernel underline that.",
            "So this point of view was quite a bit used in machine learning through design kernel.",
            "And we see that something very similar happened in."
        ],
        [
            "The output.",
            "So here the idea is that the function the multi output case just the function is not even possible values.",
            "So for me is really a vector field or function.",
            "Well, multiple output literally, so I will make a bit of a difference, then emphasize it a couple of times.",
            "In what, for example, and represented each task, each coordinate or task or output, I will use all these words.",
            "Assignment will have different possible input points and output points OK, especially the input points might be different.",
            "The classical case what was talking about it would call this case Etero topic.",
            "Well, as if the X don't depends on T. They're the same for every task.",
            "Every coordinate, it would be easier topic and I think this is somewhat different.",
            "Will come back to this, but I believe it's not very healthy thing to always consider it to be the same.",
            "So in this case, again I want something like this.",
            "Now this function is a vector valued function, so these coefficients here rather than being real numbers, are going to be vectors and we have to clarify a bit what is a kernel.",
            "It's a bit more complicated."
        ],
        [
            "Object.",
            "It's not too different, so I still have function that now our function in with many values and a kernel for each two input is going to return, not a real number, but a matrix.",
            "So here I don't have R, but it has the space of T by T matrices, so this is the way you view a reproducing kernel in the vector valued case and the two properties, the properties that KX is in this space and the reproducing property have to change a bit.",
            "And essentially I'm going to take a vector C in RT, and for any C, so this is a matrix debiti this is a vector.",
            "I get a function for every X and this belongs to X.",
            "And same here.",
            "So this is a function and it makes sense to take this inner product and I have this reproducing property.",
            "OK so I just put it there because it's nothing new, nothing fancy, but it's somewhat the natural generalization of this color case to the vector case.",
            "Once we have this.",
            "Model we can just go down and write down again.",
            "Something like the kind of regularization where for the last term we can just take the sum of the error.",
            "So there is no coupling here at all.",
            "It will be something like I guess.",
            "Gaussian noise model for each component independently to the any other, so I can put some coupling term, but we usually.",
            "Nobody does it in the kind of framework in the kind of regularization framework.",
            "Thinking about the question is really OK. What is this norm?",
            "What are example of this normal?",
            "We have classical example.",
            "What is the Gaussian kernel which is the polynomial kernel?",
            "What is the symbol of Colonel in this multivariate case?",
            "And it's kind of tricky.",
            "I mean there are not many off the shelf kernel, but you open a book and there is a kernel.",
            "In some sense the Gaussian kernels kernel that is often used.",
            "As a default or the linear kernel in this case is unclear, what would be the default, at least to me and it surely, if you find it default it doesn't work as well as the default choice in this color case.",
            "So I will not be too concerned."
        ],
        [
            "To discuss.",
            "Which kernels, but let me just briefly review what you can do kind of off the shelf.",
            "So one thing is so I defined this kernel as going from Ardito Ardito R to the space of matrices.",
            "It clearly sometimes it's nice to describe it this way.",
            "He's going to say basically, I'm going to just shift POV.",
            "I'm going to say that this is the space of input and is the indices of the coordinate.",
            "From one to T and I'm going to specify a real valued kernel, but when I specify both the input and the task.",
            "And I just stay cold.",
            "These kind of couples that compare the couples.",
            "OK, so when I do this, I don't factorize anything here.",
            "You know there might be some weird interleaving between input and output.",
            "And so this is friendlier in a sense that the one before we have to do with matrices.",
            "This kind of stuff here.",
            "OK, but it's it's just in the finite dimensional case, I think they're completely equivalent, at least in when you have finite number of coordinates, and that's what we're going to look at.",
            "The other general class of kernels with I think it is an immediate connection with this stuff.",
            "I think in for example, enhance talk and leisurely do statistics is to take the Colonel who have the following form.",
            "I take scholar Colonel.",
            "And then I take positive semidefinite matrices and I think your combination of this stuff.",
            "So here I'm making some kind of assumption because I factorize the effect of the input and the output OK, will see that this class of kernels are this more general one.",
            "Will have some different properties, for example from the computational point of view.",
            "So.",
            "Any questions so far?",
            "I'm basically going a bit fast, but yeah.",
            "Single pass.",
            "Yeah, so yeah.",
            "So this can be the say the.",
            "One activity so I can say when I look at this point for the first task and when I look at another point for another task.",
            "Yeah, so just if you want the simplest.",
            "So we just want to see if the simplest thing to go from this one to this one is to take a kernel on the axis and a kernel on the team.",
            "Just multiply them so again it would be a factorization tree factorization.",
            "Yep, so.",
            "I assume that people saw the repetition Colonel in this color case.",
            "It's got similar.",
            "There are a couple of differences and one can be more or less friendly depending on.",
            "But again, so I won't be concerned too much how you choose this, but I would just want to show a couple of examples where you don't learn anything.",
            "You try to fix it.",
            "In some sense, you don't try to learn AR or KR.",
            "Differently from what we've seen today, but I just tried to fix it according to the prior information.",
            "Ideally when I start to do this stuff, I wanted to know if we have analogous of a Gaussian kernel.",
            "So a family of kernels kind of powerful with simple parameters that I can kind of choose by cross validation and would give me something OK, and at least by now I don't haven't found such."
        ],
        [
            "Nice thing.",
            "So.",
            "I somewhat mentioned that people that engineered, so I'm going to be designing kernel rather than learning kernel and somewhat this little result that was, I think for the linear case was done by a massive Ponteland michelli program knows better than I do, and it's very easy to generalize to the general nonlinear case.",
            "Says the following.",
            "So I already said that if I give you I can give you this kernel and there will be some norm in the corresponding reproducing kernel based base.",
            "So if you define a kernel then of course you have the norm.",
            "OK, how about going the other way around?",
            "Can we define the norm and what is the induced kernel?",
            "And these results basically says that suppose that what you have is that you have different components, so you have these different outputs.",
            "But you believe that they are.",
            "They can be different, but they are all in the same space.",
            "OK, they're all in the same space of color function, and then you basically want to just define some notion of correlation between each task, explicitly by taking inner product and waking them with some weight.",
            "Coefficients ajji.",
            "OK. Then basically it turns out that if I do this with some way, if I do this and I put some weights here, I can collect that into a matrix inverted, and this gives me a kernel.",
            "And the other way around.",
            "OK.",
            "So it is not a complicated computation.",
            "The message here is that you can design regularizers of this form where you couple the different tasks with some weights in front of them.",
            "Then immediately you can go back easily.",
            "To the Colonel.",
            "If you have a kernel of this formula, always inducing regularizers of this form.",
            "OK. Makes sense.",
            "So this fji are the component of the of the function, of course.",
            "OK, so I'll give you just a few examples and."
        ],
        [
            "The first one is really some kind of mixed effect model.",
            "So it looks like this when I write it as a kernel and it looks like this when I write it as panelization regularizer.",
            "He basically says something like I want each of the component to be close to some kind of mean function.",
            "Across the different tasks, and then I want each component to be simple.",
            "So it's really I can think of each component as like same common effect plus a perturbation.",
            "The common effect here is basically the mean effect among the component.",
            "And this is this kernel here.",
            "Basically, when you look at it this way basically says.",
            "If you set Omega so this is the matrix of all ones full of ones, and this is the diagonal matrix with wine on the diagonal.",
            "This basically says.",
            "By choosing Omega from zero to one, I go from really considering all the tasks to be exactly the same.",
            "2 going to consider each task completely independent.",
            "So by moving this Omega is somewhat you have just one parameter to amount is a tropically.",
            "Explore the correlation among the tasks.",
            "So I think again, this is basically subcase."
        ],
        [
            "So two other example are suppose that I give you clusters.",
            "OK, I say that this component stays in this cluster.",
            "These other components in this other cluster and immediately you can given a matrix M which is debiti and specify this.",
            "I can tell you what is the kernel or I can view this as this penalization here that basically says the following is similar to the one before by cluster by cluster.",
            "Basically says each component should be close to the center of its cluster.",
            "And the center of the cluster should be a simple function.",
            "So C is the number of clusters and L is the number of components in each cluster.",
            "And again, this is just different way of writing things, but really, what I'm saying is the components lives in cluster and they don't know them a priority.",
            "They want to write this down."
        ],
        [
            "The last one is.",
            "So what the case where I give you an adjacency matrix among the tasks.",
            "So I give you matrix.",
            "Basically this statue related to this one.",
            "This tax rate to this other one, and you know in a few cases there are experts maybe.",
            "A friend of mine works in mortality series of different countries and Spain is more similar to Italy than some other country and so I can enforce some what correlation.",
            "This way.",
            "I design it by hand, expert driven and then once I have this matrix, MI can write this.",
            "This degree is just the sum of each row.",
            "This is the sum of each of these matrix.",
            "I get this something like a lot blasian and again I can write down both the kernel and the corresponding.",
            "Regularizer.",
            "So again, here is a matter of what's your favorite point of view?",
            "Can stem from the kernel?",
            "You can start from the regularizer and you can go from one to the other.",
            "Universe yeah.",
            "Precision.",
            "Here.",
            "Yeah, that's uh.",
            "So when you go yeah, when you go from the weight.",
            "So if you have a matrix of weights here, you have to take the pseudoinverse to go there.",
            "And the other way around.",
            "So it's exactly what you're saying.",
            "Anyway, so these are things that people design using this penalty."
        ],
        [
            "So what what we thought about is we've been working a bit on the following.",
            "Suppose that I give you this kernel.",
            "You designed it.",
            "OK, now you have to typically to solve this linear system.",
            "This is a kernel matrix and let me just explain the notation, so I put all my data is stuck them in blocks in this big matrix and now this Y is going to be all the possible output values stack one after the other and see the same thing is like the coefficients for each task.",
            "So this matrix is big is T task times entry points if the number of points is different times same and each of these guys T * N T and feel just the number of points for task.",
            "So computing this.",
            "It's very expensive and I guess.",
            "Similar to the comments that Marissa was doing, one can try to say, OK, let's try to start to do approximations.",
            "The price of in, particularly in machine learning, especially recently.",
            "This parameter Lambda is perceived as parameterized to be chosen with some kind of cross validation procedure, or something else.",
            "So when you do something like this, the price of this is not the price of doing it.",
            "Once the price of doing it as many times.",
            "As those needed to find these regularization parameter sometimes is called the regularization path.",
            "And here you have if you have different values and if you just do it naively by inverting.",
            "All the time you get in this.",
            "Kind of complexity, which is huge.",
            "This is really pessimistic.",
            "You can do it even in this case you can do it for just by doing SVD, except the bigger constant.",
            "But it might be better and you compute it just once.",
            "So our idea is that we basically worked quite a bit on connection with delivering inverse problems, where basically they would have to solve big linear systems.",
            "And this is basically solving a big linear system.",
            "By regularising it by doing a preconditioning.",
            "Augmented the diagonal.",
            "So our idea is OK, let's let's ask the Mark and the people after the mark in the last.",
            "100 years and let's see what I've done.",
            "If they have a better way of doing it or comperable way of doing it.",
            "And the idea here is that we view this.",
            "So if you put Lambda equal to 0, this problem can be unstable.",
            "So we want to regularize and the question is, can we regularize in some different way with respect to this one?"
        ],
        [
            "So.",
            "If I take the eigendecomposition of this big kernel matrix, suppose that somebody gives me the SVD.",
            "So you hear Eigenvector and Simard eigenvalues.",
            "I can just rewrite Tikhonov this way.",
            "OK, in components.",
            "OK with everybody.",
            "So if I do this, I see that the effect of tickle me some of the filtering of this component is well known in every community, yes, and ideas that I can view this as a filtering and the question is why these filters can I order under interesting filters.",
            "Is I say it's a filter because you can imagine this to be something like frequencies.",
            "So each of these are basically eigenvector.",
            "I ordered them according to the values of the corresponding eigenvalues, and I say that big eigenvalues are slow frequency essentially and small eigenvalues are high frequency.",
            "Or if you want principle big principle component and small ones and I just basically want to do some shrinkage across these bases which is given by the data."
        ],
        [
            "And these executive picture.",
            "So these are larger in values.",
            "This is more like in values and I want to do something like this a low pass filter and say whatever is here.",
            "I keep it whatever is there and throw it away.",
            "OK, so up to now is just a different way to describe kind of regularization."
        ],
        [
            "And this is just a filter for Tikhonov, so even if I'm acting on a matrix I can just consider the function on the eigenvalues."
        ],
        [
            "So there are a bunch of things that people have been doing is basically.",
            "A bunch of different kind of algorithms.",
            "In particular, I'm going to show this iterative algorithm that I've been around for awhile and I'm just going to describe the simplest one, though it's not the one that we actually use, but it's very easy to Excel."
        ],
        [
            "Name?",
            "And this simple 11 can describe it in two different ways.",
            "So let me first let's first look at this OK.",
            "So this you can recognize you're doing essentially something like the gradient descent of the least square error.",
            "Without any constraint.",
            "So you don't say that the norm is to be in a ball.",
            "You don't make any other.",
            "You don't project anywhere, you just so it's an unconstrained problem.",
            "You solve the square by gradient descent, so if you just let it go.",
            "You are fitting the data and that's it, OK?",
            "So.",
            "The intuition is that if you somewhat stop somewhere in between just by early stopping.",
            "You are getting a better solution.",
            "So let me suggest."
        ],
        [
            "So this is just a simple picture.",
            "The idea is that you start from.",
            "This is just this color case, but it will work in two or three dimensions the same way you just start from the flat.",
            "Solution just the zero solution if you let it go."
        ],
        [
            "This is it good to go."
        ],
        [
            "Much you start to overfit."
        ],
        [
            "And somewhere in between you got a better solution and this is very well known."
        ],
        [
            "In English of English problems.",
            "And the idea is that you can view this as a filter.",
            "OK, why well, you can do it in many ways.",
            "So one way you can just do this and just write it by induction.",
            "What you see, what the Alpha parameter azati step is and we turn out that will be described by this function.",
            "OK, this is slightly nicer way of doing it.",
            "That's somewhat surprising because at first sight the gradient decent.",
            "Has nothing to do with it.",
            "Take the Matrix A and do a power low expansion like you would do with just a real number.",
            "So for meters you can do the same thing.",
            "And it turns out that this filter here simply corresponds to take a power expansion of the universe, but then truncated.",
            "So I'm not describing the inverse, but I'm truncating it, so I throw away some information that is not useful, so that's where the regularization is coming from, and it turns out that.",
            "I don't have to compute the eigenvalues to do is.",
            "I don't have to do any kind of SVD, I can just do this iteration and I'm done.",
            "So it turns out that this is.",
            "In some sense, equivalent because it's doing a low pass filter.",
            "Similarly to taken off, but the way you implement it is different.",
            "You don't have to take any eigenvalue and you want to invert any matrix and you just do this thing and you see here what I do is that I have a matrix and vectors and I multiply 1 with the other.",
            "You also see that at the step size here, so I have two parameters.",
            "In fact, the number of iterations and the step size the same size.",
            "One can fix a priority, basically because if I want this series to converge.",
            "Similar to Discover Case E to a has to be smaller than one in this case in Norm.",
            "So this is our alternative or actually variation of this is actually the code of this algorithm, liberation.",
            "We have in mind is one line longer in code and basically shows that with a fixed step size you can achieve the same solution but with the square root of the number of iterations, so it's much faster.",
            "And we got this called.",
            "So this thing here was called Delta boosting in statistics and reinvented maybe 10 years ago, and it was done in the 70s.",
            "In the verse problem: fiber.",
            "It's just gradient descent.",
            "These are accelerated version is called the new methods and it's just some kind of accelerated gradient descent in the numerics."
        ],
        [
            "Literature.",
            "So.",
            "So this is one point I want to make.",
            "So by using things that are not taken often without the default, I write this down and I just invert the big matrix and I got this cube complexity.",
            "So here there are a couple of nice things.",
            "OK, so first of all, let's look at the complexity first.",
            "Each iteration just cost me.",
            "T&T Square is a matrix vector multiplication, that's it.",
            "Plus, whenever I run an iteration, I'm looking at the different regularization parameter because now the regulation parameter is no Lambda.",
            "Only relation parameter is the number of iterations.",
            "So I go.",
            "I have a consistent increase in complexity.",
            "As soon as I do this because I have a natural warm restart kind of procedure.",
            "And then the rest is.",
            "So some what it gives us from from from.",
            "Frequently is kind of perspective.",
            "This is not just doing empirical risk with the constraint on the potty space, or rather it's doing it, but in a slightly different way.",
            "So the way you enforce a prior notice, meaning it's somewhat strange.",
            "It's very tightly coupled with the way you solve the algorithm.",
            "At least, this is my point of view, so in some sense the inference principles, the probabilistic modeling principle, and the computational principle are really blended together, and they're not too subsequent step.",
            "So if you do an iteration here, it has an effect on the way you actually do the inference, so that was somewhat interesting.",
            "It was an interesting aspect to me."
        ],
        [
            "So it turns out that I will show you experiment, so if.",
            "If the kernel has this specific form.",
            "So just one kernel times one matrix.",
            "In fact, I can simply diagonalize this matrix, an project.",
            "All the data there, and then the complexity is really ridiculous is basically you can solve this if this is the case where at the same number of inputs for each task.",
            "So if you have the same inputs for each task and the kernel is of this form, well basically in that case I can just diagonalize a rotate everything according to A and it turns out is associated.",
            "The complexity of the multi task is the same as the complexity of 1 task.",
            "Essential.",
            "And I think this is somewhat.",
            "It probably is somewhat known, but it's not.",
            "I didn't see very much in the.",
            "Yeah, so so you are learning transfer.",
            "And then.",
            "Yeah, I was talking to address before.",
            "I don't think that people doing regularization observed.",
            "This thing, but coming it's it's there.",
            "Boston.",
            "Yeah, you can do the numbers right if you have.",
            "It depends how many data you have and how many tasks you have.",
            "If you have a billion test point.",
            "This video is going to kill you if you have 50 test point.",
            "And as we do a 50 by 50 matrix, you can still do it, especially if it's semidefinite.",
            "Yeah, depends can be.",
            "You can be a good thing or not."
        ],
        [
            "OK.",
            "So we just apply this to a simple case and we took this.",
            "So this is just a simulation data where we take this kind of velocity field and we convolve with a Gaussian because then it decays fast enough and we know that we can be decomposed in.",
            "In a field with a part which is curl free and apart with this divergent free."
        ],
        [
            "So it has two components, but with no curves.",
            "And what we know they vergence and we finally are a couple of kernels.",
            "They are not of the form KA.",
            "They really somewhat couple the different tasks in no funny way.",
            "And one of them is blind to divergent.",
            "It cannot reconstruct divergent that one is blind to.",
            "Curl.",
            "So we essentially taking an average of these two.",
            "You can actually try to reconstruct the good approximation of a field which is.",
            "Curl for your day veterans free and you can basically.",
            "Extract these two parts.",
            "We just looking from the book.",
            "This is one of the few I found in the neutral RBF functions for multi output functions, so I hope to find more in the literature of people that try to estimate vector valued functions, but in fact so far I didn't and these were some couple of nice ones.",
            "I think in fact that's kind of easy because it's again something like this color kernel and probably.",
            "Well, I don't think it's too bad, but.",
            "I just give it for granted."
        ],
        [
            "OK, and we applied it.",
            "You can estimate the true field estimated field and then you have the two parts, one which is curved three in the one which is divergent three and what I.",
            "It is very noisy, so by comparing independent an this kind of joint estimation, you don't have much of an improving.",
            "You have a bit of improvement, but when you can."
        ],
        [
            "At time of doing this new method, which is the accelerated gradient or that equal regularization, and you put both the cross validation and the the choice of the parameter into the game, then it's huge.",
            "It's very big.",
            "There, the computation time and here you can see the computation time as the number of training example increases.",
            "OK."
        ],
        [
            "So the other thing I want to say very quickly is that so the classic kernel, for example that I showed before, seems a bit too simple to me.",
            "But then I think that they are fixed.",
            "They don't depend on the data, so I can go and try to prove some kind of frequentist results and going to large deviation concentration of measure results which are basically based on the idea that when you have the kernel matrix is some kind of approximation of integral operator.",
            "So if you have, if you let the number of points goes to Infinity, you have somewhat consistent limit.",
            "An basis, some kind of generalization of random matrix results?",
            "That's one possible way so you can have like a concentration of the empirical matrix in a sense of qualitatively to its expected parts, or something like the empirical covariance matrix and expected covariance matrix.",
            "Even if the dimension and then by."
        ],
        [
            "Using this essentially you have rates of convergence.",
            "Which are somewhat optimal.",
            "So you can say that the function you retrieve is essentially up to.",
            "See in terms of learning rates, so in terms of sample complexity for, you can say how many sample I need to actually estimate it within this precision with a certain probability, and that's that's optimal.",
            "Yep.",
            "Sorry, this is just the best possible function which depends on the probability and this is the probability or the probability.",
            "So this this is this.",
            "This is essentially the risk.",
            "There is the least square risk.",
            "Averaged over the possible of the probability measure.",
            "I just wanted to.",
            "The meaning of this is just to show that with this specific class of kernels which are fixed and independent to the data, it's easy to prove this kind of result in some sense.",
            "You only have to care about the regularization part, but the rest is is not there."
        ],
        [
            "So.",
            "I'm pretty much done with the first part.",
            "So if you have any questions on this.",
            "To summarize, the idea here was to show in some sense we already have some other theory in this color case, and we wanted to see how it translate and what we could benefit with respect to what was already around going to the vector case and the main messages.",
            "I think computationally can be more interesting to use iterative methods, remembering that the iteration is doing something, so don't use it to solve a linear system.",
            "Many times whenever you change the requisition parameter, this is something that people would typically do in machine learning.",
            "That was.",
            "Potentially nice message and the other thing is that if the kernel is fixed pretty much older results in this color case just go through.",
            "And of course our results are a bit dumb because they don't exploit.",
            "They don't really look specifically at the features of this vector valued case.",
            "For example, the effect of the number of.",
            "Tasks OK, so this is just a preliminary result.",
            "There is a lot to do in this setting.",
            "Separation of your kernels versus exactly absolutely absolutely so.",
            "Essentially that result basically says if you have a fixed kernel and is bounded.",
            "OK, you can relax this, but that's easier one then everything is fine.",
            "And you have these nice results.",
            "You have a certain rate.",
            "Now you can expect that if.",
            "Separation will have an effect number of.",
            "Task will have an effect and we don't have it OK, and as far as I know.",
            "Is not there, nobody did it.",
            "It was just a preliminary thing."
        ],
        [
            "So let me tell you about my experience.",
            "So my original motivation to do this was not just to multitask or vector fields, but I was curious about multi class OK.",
            "So.",
            "At first I look at this stuff and I notice this difference an it's obvious.",
            "I mean, after today I even have a different name.",
            "I called this multi task and this vector fields, but is exactly the third topic is a topic that didn't have names.",
            "So now I have this name.",
            "Probably better use it, but essentially what I want.",
            "OK I just want to think a second about the following situation.",
            "You have two tasks.",
            "In the in the first case you have points here that you don't have here.",
            "And the two curves are very similar in the other case, the points are the same, but the noise might be different, for example.",
            "So I'm assuming that they're very similar in the picture is somewhat simplified, and the message of this is that.",
            "You see this function.",
            "I just have three points here.",
            "I don't have anything here, so if I can use somewhat correlation among the task, of course I'm going to prove the whole thing a lot, because the reconstruction of this task done here will be ridiculously bad.",
            "OK, so even if in some sense my modeling assumption among the tasks is not too smart.",
            "Still, it's better than 0, maybe, but it's kind of likely OK, whereas if I'm here.",
            "Well, I gotta be a bit smarter than that, because this I call this a boring strength effect.",
            "It's not that strong and that's why in some sense.",
            "I know this is important to me and.",
            "It's not cute.",
            "I don't know this is something I would like to know what other people think about."
        ],
        [
            "Anne.",
            "You know, after so much, but we're chatting today.",
            "I think that maybe one can.",
            "I was looking at the low dimensional application that are done in just autistic son.",
            "I was thinking that kind of very high dimensional situation or one is dealing with in learning anything that maybe one think of the following kind of regime to look at.",
            "The first one.",
            "OK, this is like classical, you have lots of data and then you have some dimension and some outputs.",
            "OK, it seems to be the classical thing.",
            "Central limit theorem kind of regime.",
            "Forget about the sampling.",
            "Have enough variable, I just estimate they try to be.",
            "Modeling.",
            "This is somewhat high dimensional inference regime.",
            "You have many more variables than points higher because it's the nature of the problem.",
            "It was because your ignorance reflecting the fact that you take lots of measurements.",
            "I take a billion wavelets coefficients out of an image or I have micro arrays.",
            "We chart at billions numbers for a few.",
            "But that maybe once you think a bit more about this situation, how many samples you have with respect to the template into the template to the coordinates.",
            "Maybe you know if you're in two days into coordinates here, or if you like, I think what this you think about you have a million coordinates output.",
            "That's the kind of different regime and the modeling assumption you make might not be the same, especially."
        ],
        [
            "Having this in mind.",
            "This kind of difference.",
            "So I personally have not seen.",
            "Improvement.",
            "In problems like this in dimension bigger than 3 four.",
            "That's why today is largely we're discussing this.",
            "This is very interesting to me.",
            "I would be curious to see a fair comparison with like a model selection done in high dimensional output case.",
            "Exactly in this situation you mean.",
            "At this point, but suppose that you can actually enforce it.",
            "OK, So what is the kernels I showed you before?",
            "Basically said that for some reason you can either say that these two curves are like average curve plus perturbation, so I'm not as nice as they did I can shift them around, but maybe here is not exactly like this.",
            "It might be a bit up and down, but it's not flat because it's similar to this one, so it's a prior assumption.",
            "I don't learn it, it's prior, so the kernels I show our prior based.",
            "It might be like spatially close the two things in this patient, absolutely.",
            "He was doing earlier than that, so that's why.",
            "Absolutely, you might have.",
            "But I think Seton Hall addition and then you could learn it and assumed about overlap correlation existed across the space station.",
            "There, be she optimal degree of overlap, that the purely I wish it were it is in I so yeah this is not just an extreme header.",
            "Obviously too.",
            "So yeah.",
            "I agree, I agree.",
            "I just think it's it's.",
            "It's maybe something to think about.",
            "Usually it's just a big list of problems with multiple output.",
            "Yeah sure it's true, but maybe they're not exactly the same and maybe they think you can see being effective in one case or not in some other case, this is definitely my XP."
        ],
        [
            "Particularly, I just want to mention briefly this.",
            "So when is one class classification multiclass classification?",
            "At least in my neighborhood, everybody tend to refer to this paper that came out a few years ago and basically after 15 years of people generalizing SVM's, an related thing just sit and do a simple one versus cold one versus whole scheme.",
            "So you just take one class against all the others.",
            "Next class against all the others and in the end you take the maximum of your classification.",
            "For each class individually OK. And this paper basically sit there, they don't have many particularly theoretical claims, but they basically do all the experience that people did before using their own code and basically show that if you use the one versus Oh well, meaning that you do the nice cross validation for each task, you get better result or comperable result.",
            "So in some sense, to me this was like.",
            "Annoying because you're like really.",
            "I mean if I tell you that you have cats and dogs and you have tables and seats, you cannot do better than that.",
            "I mean, you forget about that and you do better than that.",
            "Is that possible?",
            "But still it doesn't.",
            "You know it was a good benchmark, so let's go and try it.",
            "That's what I thought."
        ],
        [
            "And so I thought maybe we could use the multi output kind of stuff.",
            "So how well the simplest idea is to say well just put a coding vector to each class so.",
            "This is this is the way I would do it from a functional perspective.",
            "So I say the class one is this coding 1 -- 1 -- 1 + 2 is minus one one.",
            "I keep on going like that.",
            "Then in some sense I just do least squares on these coding vectors.",
            "And then so it was interesting.",
            "It turns out this is our.",
            "Vector field.",
            "And now OK this yeah that's right.",
            "And this is just the independent case.",
            "So this is I wrote it as a vector problem, but it's really an independent problem.",
            "Suppose they do this and then in the end I checked.",
            "Just take the maximum of this.",
            "So this is a glass exactly one versus all sonar in the language that we discussed today.",
            "One versus all is least square estimation with a block diagonal Colonel.",
            "And again, I'm going to say, can I learn this stuff?",
            "Can I do something better than this?",
            "So the good news in some sense that even if at first sight this might be weird to do this coding decoding doesn't matter a lot.",
            "And when you do this, when you take this FJ, this FJ are proportional the one that you estimate are proportional to the conditional probabilities, so that in fact makes a lot of sense and one can prove based consistency and rates of this kind of."
        ],
        [
            "From a large deviation point of view.",
            "The weird thing is that the state of the art doesn't put any correlation among classes.",
            "And now for us this is really for me is really this means that the kernel is block diagonal OK and the question is can we do better than that?",
            "But now the problem is that.",
            "I learn about the literature in cracking, for example, and I can try to do OK. Let's try to do some kind of correlation among the tasks, but this is going to be."
        ],
        [
            "Completely meaningless because I invented them.",
            "But there is no distance here.",
            "So the question in this case is, I think even harder than the vector field case because."
        ],
        [
            "How do I get this correlation among the tasks without enforcing, you know, too much information that is going to overfit the data.",
            "And.",
            "So in some sense my view OK, we try to do this stuff.",
            "We tried to put it to construct cases where we have 3 little bowl and on a triangle and we put a couple together.",
            "So we tried all different kind of tricks and we tried to enforce prior information on that and what we saw at least in our experiments that you improve the conditional probabilities.",
            "So you look at the connection with the look better when you take this Mac."
        ],
        [
            "And you do the classification.",
            "Kill it.",
            "So if you just look at the rates.",
            "That's it.",
            "Then again, it will be very crazy.",
            "This is the end of the story, really.",
            "One versus all is the best thing you can do.",
            "And.",
            "Well, yeah, I guess that's pretty much what I wanted to say.",
            "So in my view there are really some what hierarchy of difficulties of problems.",
            "The multitask problem is in some sense the easier one.",
            "And the vector fields in a multiclass are harder and harder.",
            "And yeah, that's pretty much what I want to say.",
            "Question.",
            "Send Brenda what's the?"
        ],
        [
            "That right there, when were you in the case where we had basically the same?",
            "Covariance breach we had that picture of, you know, three points here 340 yeah, I was usually.",
            "I was using one of these kernels here.",
            "The one that I showed at the beginning like this say."
        ],
        [
            "And I actually I I knew this.",
            "They designed it.",
            "I was using still this model where I have this simple kernel by design that.",
            "And I was asking myself how many points I need for each class before I see some improvement.",
            "And you know, it's after few few points per class.",
            "Everything worked perfectly.",
            "Sorry I missed it, but that was for the classification.",
            "Yeah, so I think that this was the problem we all had in Gaussian process is when we were modeling all these things nicely.",
            "Stuff like that you don't be support vector machines doing one versus when you're doing maximum Alpha, you're making the nice predictions of these conditional probability and then you have to find an excuse as to why.",
            "That's useful, yeah.",
            "Yeah, but again, I was thinking about this.",
            "I haven't done it yet, so I'm going to do it.",
            "So there are these huge image datasets where you have 100 + 200 class.",
            "And I don't know.",
            "I still don't understand how is it possible that it by telling you their animals and furniture or something and taking features that will reflect that it doesn't help.",
            "So to me.",
            "Yeah yeah, yeah.",
            "Not as far as I know, not as far as results that I've seen compared with benchmarks that I can be reproducible.",
            "They may also play a different game, which is.",
            "You if you can't get the belief distinction between, I don't know.",
            "Yeah yeah, yeah, yeah.",
            "Yeah.",
            "Yeah.",
            "Now, again, I've not.",
            "I look at some results in the hard cold vision perspective.",
            "But it might be different.",
            "Exclusive are they associate discount.",
            "In this case I was just going to I just considered.",
            "I just consider the exclusive exclusive case just because then I can prove everything from a theoretical point of using the other problems then.",
            "He might be.",
            "Yeah, yeah.",
            "Yeah, I it seems like classifications within forever regression.",
            "Also multiclass seems to be next thing and one versus solid best, that's why.",
            "I think this is there so I don't know if you have having something at lunch about.",
            "We have this pre workshop in July is topical was talking.",
            "We can turn this into the panel."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, So what I'm going to talk about is very much complementary to everything we've seen today, so I care about stuff that.",
                    "label": 0
                },
                {
                    "sent": "Pretty much nobody in the other talk looked at, and so it can give a view of.",
                    "label": 0
                },
                {
                    "sent": "I'm not the only one, so you can give a view of some other concerns that might have while solving these kind of problems.",
                    "label": 0
                },
                {
                    "sent": "So I really going to briefly through basically the same object that everybody used today, but like re phrasing it more like functional analytical framework.",
                    "label": 0
                },
                {
                    "sent": "And then I'm just basically describe just the background, so I'm not going to see much on how do you actually choose the covariance function is going to be an object which is given, and I'm going to say once you have it kind of stuff you can do.",
                    "label": 0
                },
                {
                    "sent": "And I'm going to try to make it brief after last another day, and I'd rather actually go and discuss a couple of thoughts that I have looking at other people talking while I start to work on this.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Stuff.",
                    "label": 0
                },
                {
                    "sent": "So very briefly, so.",
                    "label": 0
                },
                {
                    "sent": "Let me just briefly recall this color case because I'll try to do a bit go on in analogy to this color case.",
                    "label": 0
                },
                {
                    "sent": "I put very little assumption, but there are a lot in the kind of stuff I usually do, so just suppose that you want to function with the variables But in one dimension and you do it from samples.",
                    "label": 0
                },
                {
                    "sent": "The sample for me will be always somewhat IID samples from some underlying problems distribution.",
                    "label": 0
                },
                {
                    "sent": "There will be sampling and noise everybody every all these uncertainties will be probabilistic.",
                    "label": 0
                },
                {
                    "sent": "So what I care about is doing a kind of kernel model where I basically take some kind of generalized linear model where my basis function, But this is.",
                    "label": 0
                },
                {
                    "sent": "Is not to create computers is really a kernel, so it's a function of two variables and then I just point it at some point in space and when I take a linear combination of this, potentially an infinite dimensional.",
                    "label": 0
                },
                {
                    "sent": "I just want this to be very very powerful.",
                    "label": 0
                },
                {
                    "sent": "So we'll see how to handle that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what is it more precisely?",
                    "label": 0
                },
                {
                    "sent": "Well, for me, a kernel is the thing that he's behind a space of function.",
                    "label": 1
                },
                {
                    "sent": "Huber space of functions have inner product of norms.",
                    "label": 1
                },
                {
                    "sent": "And the kernel is a function of two variables and they test a couple of properties.",
                    "label": 0
                },
                {
                    "sent": "If I fix X for any X, it belongs to the space, so it's a function for every X is a function in this space, and I've got this reproducing property so I can evaluate function at points.",
                    "label": 0
                },
                {
                    "sent": "This will not play a big role in what I will say today is some kind of computational property that play role once we go down and try to do algorithmics.",
                    "label": 0
                },
                {
                    "sent": "So, given a choice of this kernel, the typical idea is in regularization.",
                    "label": 0
                },
                {
                    "sent": "The simplest one is to stick on regularization that basically says take a linear fit for Soria Square loss, kind of fit or any other loss function, among, say, most likely convex lock function in F and then it turns out that for many many different choice of K. This norm here can be thought as enforces and kind of simplicity, smoothness, control.",
                    "label": 0
                },
                {
                    "sent": "The complexity of the function.",
                    "label": 0
                },
                {
                    "sent": "So this is just setting up the notation, but I guess this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What everybody knows.",
                    "label": 0
                },
                {
                    "sent": "So in machine learning, people typically choose the.",
                    "label": 0
                },
                {
                    "sent": "There are a bunch of kernels in the books and then there are basically a couple of ways to build kernels, hudock and repair the three, but I'm just going to talk about two.",
                    "label": 0
                },
                {
                    "sent": "The first one is it's rather than using the kernel you actually going to do a split some kind of feature map so you map your data using struct measurements out of your data.",
                    "label": 0
                },
                {
                    "sent": "The other one, which is somewhat useful will be useful to compare to the multi output cases that.",
                    "label": 0
                },
                {
                    "sent": "Rather than defining then fixing a kernel and then see the norm which is defined by the kernel, we're actually going to define some kind of norm.",
                    "label": 0
                },
                {
                    "sent": "In some space of function and will see that this can be uniquely used by a kernel.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is well known for having splines theory when one look at a certain smoothness functional like that's the interval interval or the first derivative, the square integral of the first derivative, and this induces certain norm and then one can trace back the kernel underline that.",
                    "label": 0
                },
                {
                    "sent": "So this point of view was quite a bit used in machine learning through design kernel.",
                    "label": 0
                },
                {
                    "sent": "And we see that something very similar happened in.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The output.",
                    "label": 0
                },
                {
                    "sent": "So here the idea is that the function the multi output case just the function is not even possible values.",
                    "label": 0
                },
                {
                    "sent": "So for me is really a vector field or function.",
                    "label": 0
                },
                {
                    "sent": "Well, multiple output literally, so I will make a bit of a difference, then emphasize it a couple of times.",
                    "label": 0
                },
                {
                    "sent": "In what, for example, and represented each task, each coordinate or task or output, I will use all these words.",
                    "label": 0
                },
                {
                    "sent": "Assignment will have different possible input points and output points OK, especially the input points might be different.",
                    "label": 0
                },
                {
                    "sent": "The classical case what was talking about it would call this case Etero topic.",
                    "label": 0
                },
                {
                    "sent": "Well, as if the X don't depends on T. They're the same for every task.",
                    "label": 0
                },
                {
                    "sent": "Every coordinate, it would be easier topic and I think this is somewhat different.",
                    "label": 0
                },
                {
                    "sent": "Will come back to this, but I believe it's not very healthy thing to always consider it to be the same.",
                    "label": 0
                },
                {
                    "sent": "So in this case, again I want something like this.",
                    "label": 0
                },
                {
                    "sent": "Now this function is a vector valued function, so these coefficients here rather than being real numbers, are going to be vectors and we have to clarify a bit what is a kernel.",
                    "label": 0
                },
                {
                    "sent": "It's a bit more complicated.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Object.",
                    "label": 0
                },
                {
                    "sent": "It's not too different, so I still have function that now our function in with many values and a kernel for each two input is going to return, not a real number, but a matrix.",
                    "label": 0
                },
                {
                    "sent": "So here I don't have R, but it has the space of T by T matrices, so this is the way you view a reproducing kernel in the vector valued case and the two properties, the properties that KX is in this space and the reproducing property have to change a bit.",
                    "label": 1
                },
                {
                    "sent": "And essentially I'm going to take a vector C in RT, and for any C, so this is a matrix debiti this is a vector.",
                    "label": 0
                },
                {
                    "sent": "I get a function for every X and this belongs to X.",
                    "label": 0
                },
                {
                    "sent": "And same here.",
                    "label": 1
                },
                {
                    "sent": "So this is a function and it makes sense to take this inner product and I have this reproducing property.",
                    "label": 0
                },
                {
                    "sent": "OK so I just put it there because it's nothing new, nothing fancy, but it's somewhat the natural generalization of this color case to the vector case.",
                    "label": 0
                },
                {
                    "sent": "Once we have this.",
                    "label": 0
                },
                {
                    "sent": "Model we can just go down and write down again.",
                    "label": 1
                },
                {
                    "sent": "Something like the kind of regularization where for the last term we can just take the sum of the error.",
                    "label": 0
                },
                {
                    "sent": "So there is no coupling here at all.",
                    "label": 1
                },
                {
                    "sent": "It will be something like I guess.",
                    "label": 0
                },
                {
                    "sent": "Gaussian noise model for each component independently to the any other, so I can put some coupling term, but we usually.",
                    "label": 0
                },
                {
                    "sent": "Nobody does it in the kind of framework in the kind of regularization framework.",
                    "label": 0
                },
                {
                    "sent": "Thinking about the question is really OK. What is this norm?",
                    "label": 0
                },
                {
                    "sent": "What are example of this normal?",
                    "label": 0
                },
                {
                    "sent": "We have classical example.",
                    "label": 1
                },
                {
                    "sent": "What is the Gaussian kernel which is the polynomial kernel?",
                    "label": 0
                },
                {
                    "sent": "What is the symbol of Colonel in this multivariate case?",
                    "label": 0
                },
                {
                    "sent": "And it's kind of tricky.",
                    "label": 0
                },
                {
                    "sent": "I mean there are not many off the shelf kernel, but you open a book and there is a kernel.",
                    "label": 0
                },
                {
                    "sent": "In some sense the Gaussian kernels kernel that is often used.",
                    "label": 0
                },
                {
                    "sent": "As a default or the linear kernel in this case is unclear, what would be the default, at least to me and it surely, if you find it default it doesn't work as well as the default choice in this color case.",
                    "label": 0
                },
                {
                    "sent": "So I will not be too concerned.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To discuss.",
                    "label": 0
                },
                {
                    "sent": "Which kernels, but let me just briefly review what you can do kind of off the shelf.",
                    "label": 0
                },
                {
                    "sent": "So one thing is so I defined this kernel as going from Ardito Ardito R to the space of matrices.",
                    "label": 0
                },
                {
                    "sent": "It clearly sometimes it's nice to describe it this way.",
                    "label": 0
                },
                {
                    "sent": "He's going to say basically, I'm going to just shift POV.",
                    "label": 0
                },
                {
                    "sent": "I'm going to say that this is the space of input and is the indices of the coordinate.",
                    "label": 0
                },
                {
                    "sent": "From one to T and I'm going to specify a real valued kernel, but when I specify both the input and the task.",
                    "label": 0
                },
                {
                    "sent": "And I just stay cold.",
                    "label": 0
                },
                {
                    "sent": "These kind of couples that compare the couples.",
                    "label": 0
                },
                {
                    "sent": "OK, so when I do this, I don't factorize anything here.",
                    "label": 0
                },
                {
                    "sent": "You know there might be some weird interleaving between input and output.",
                    "label": 0
                },
                {
                    "sent": "And so this is friendlier in a sense that the one before we have to do with matrices.",
                    "label": 0
                },
                {
                    "sent": "This kind of stuff here.",
                    "label": 0
                },
                {
                    "sent": "OK, but it's it's just in the finite dimensional case, I think they're completely equivalent, at least in when you have finite number of coordinates, and that's what we're going to look at.",
                    "label": 0
                },
                {
                    "sent": "The other general class of kernels with I think it is an immediate connection with this stuff.",
                    "label": 1
                },
                {
                    "sent": "I think in for example, enhance talk and leisurely do statistics is to take the Colonel who have the following form.",
                    "label": 0
                },
                {
                    "sent": "I take scholar Colonel.",
                    "label": 0
                },
                {
                    "sent": "And then I take positive semidefinite matrices and I think your combination of this stuff.",
                    "label": 0
                },
                {
                    "sent": "So here I'm making some kind of assumption because I factorize the effect of the input and the output OK, will see that this class of kernels are this more general one.",
                    "label": 0
                },
                {
                    "sent": "Will have some different properties, for example from the computational point of view.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Any questions so far?",
                    "label": 0
                },
                {
                    "sent": "I'm basically going a bit fast, but yeah.",
                    "label": 0
                },
                {
                    "sent": "Single pass.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so yeah.",
                    "label": 0
                },
                {
                    "sent": "So this can be the say the.",
                    "label": 0
                },
                {
                    "sent": "One activity so I can say when I look at this point for the first task and when I look at another point for another task.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so just if you want the simplest.",
                    "label": 0
                },
                {
                    "sent": "So we just want to see if the simplest thing to go from this one to this one is to take a kernel on the axis and a kernel on the team.",
                    "label": 0
                },
                {
                    "sent": "Just multiply them so again it would be a factorization tree factorization.",
                    "label": 0
                },
                {
                    "sent": "Yep, so.",
                    "label": 0
                },
                {
                    "sent": "I assume that people saw the repetition Colonel in this color case.",
                    "label": 0
                },
                {
                    "sent": "It's got similar.",
                    "label": 0
                },
                {
                    "sent": "There are a couple of differences and one can be more or less friendly depending on.",
                    "label": 0
                },
                {
                    "sent": "But again, so I won't be concerned too much how you choose this, but I would just want to show a couple of examples where you don't learn anything.",
                    "label": 0
                },
                {
                    "sent": "You try to fix it.",
                    "label": 0
                },
                {
                    "sent": "In some sense, you don't try to learn AR or KR.",
                    "label": 0
                },
                {
                    "sent": "Differently from what we've seen today, but I just tried to fix it according to the prior information.",
                    "label": 0
                },
                {
                    "sent": "Ideally when I start to do this stuff, I wanted to know if we have analogous of a Gaussian kernel.",
                    "label": 0
                },
                {
                    "sent": "So a family of kernels kind of powerful with simple parameters that I can kind of choose by cross validation and would give me something OK, and at least by now I don't haven't found such.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nice thing.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I somewhat mentioned that people that engineered, so I'm going to be designing kernel rather than learning kernel and somewhat this little result that was, I think for the linear case was done by a massive Ponteland michelli program knows better than I do, and it's very easy to generalize to the general nonlinear case.",
                    "label": 0
                },
                {
                    "sent": "Says the following.",
                    "label": 0
                },
                {
                    "sent": "So I already said that if I give you I can give you this kernel and there will be some norm in the corresponding reproducing kernel based base.",
                    "label": 0
                },
                {
                    "sent": "So if you define a kernel then of course you have the norm.",
                    "label": 0
                },
                {
                    "sent": "OK, how about going the other way around?",
                    "label": 0
                },
                {
                    "sent": "Can we define the norm and what is the induced kernel?",
                    "label": 0
                },
                {
                    "sent": "And these results basically says that suppose that what you have is that you have different components, so you have these different outputs.",
                    "label": 0
                },
                {
                    "sent": "But you believe that they are.",
                    "label": 0
                },
                {
                    "sent": "They can be different, but they are all in the same space.",
                    "label": 0
                },
                {
                    "sent": "OK, they're all in the same space of color function, and then you basically want to just define some notion of correlation between each task, explicitly by taking inner product and waking them with some weight.",
                    "label": 0
                },
                {
                    "sent": "Coefficients ajji.",
                    "label": 0
                },
                {
                    "sent": "OK. Then basically it turns out that if I do this with some way, if I do this and I put some weights here, I can collect that into a matrix inverted, and this gives me a kernel.",
                    "label": 0
                },
                {
                    "sent": "And the other way around.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So it is not a complicated computation.",
                    "label": 0
                },
                {
                    "sent": "The message here is that you can design regularizers of this form where you couple the different tasks with some weights in front of them.",
                    "label": 0
                },
                {
                    "sent": "Then immediately you can go back easily.",
                    "label": 0
                },
                {
                    "sent": "To the Colonel.",
                    "label": 0
                },
                {
                    "sent": "If you have a kernel of this formula, always inducing regularizers of this form.",
                    "label": 0
                },
                {
                    "sent": "OK. Makes sense.",
                    "label": 0
                },
                {
                    "sent": "So this fji are the component of the of the function, of course.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'll give you just a few examples and.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first one is really some kind of mixed effect model.",
                    "label": 1
                },
                {
                    "sent": "So it looks like this when I write it as a kernel and it looks like this when I write it as panelization regularizer.",
                    "label": 1
                },
                {
                    "sent": "He basically says something like I want each of the component to be close to some kind of mean function.",
                    "label": 1
                },
                {
                    "sent": "Across the different tasks, and then I want each component to be simple.",
                    "label": 0
                },
                {
                    "sent": "So it's really I can think of each component as like same common effect plus a perturbation.",
                    "label": 0
                },
                {
                    "sent": "The common effect here is basically the mean effect among the component.",
                    "label": 1
                },
                {
                    "sent": "And this is this kernel here.",
                    "label": 0
                },
                {
                    "sent": "Basically, when you look at it this way basically says.",
                    "label": 1
                },
                {
                    "sent": "If you set Omega so this is the matrix of all ones full of ones, and this is the diagonal matrix with wine on the diagonal.",
                    "label": 0
                },
                {
                    "sent": "This basically says.",
                    "label": 0
                },
                {
                    "sent": "By choosing Omega from zero to one, I go from really considering all the tasks to be exactly the same.",
                    "label": 0
                },
                {
                    "sent": "2 going to consider each task completely independent.",
                    "label": 0
                },
                {
                    "sent": "So by moving this Omega is somewhat you have just one parameter to amount is a tropically.",
                    "label": 0
                },
                {
                    "sent": "Explore the correlation among the tasks.",
                    "label": 0
                },
                {
                    "sent": "So I think again, this is basically subcase.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So two other example are suppose that I give you clusters.",
                    "label": 0
                },
                {
                    "sent": "OK, I say that this component stays in this cluster.",
                    "label": 0
                },
                {
                    "sent": "These other components in this other cluster and immediately you can given a matrix M which is debiti and specify this.",
                    "label": 1
                },
                {
                    "sent": "I can tell you what is the kernel or I can view this as this penalization here that basically says the following is similar to the one before by cluster by cluster.",
                    "label": 1
                },
                {
                    "sent": "Basically says each component should be close to the center of its cluster.",
                    "label": 1
                },
                {
                    "sent": "And the center of the cluster should be a simple function.",
                    "label": 1
                },
                {
                    "sent": "So C is the number of clusters and L is the number of components in each cluster.",
                    "label": 0
                },
                {
                    "sent": "And again, this is just different way of writing things, but really, what I'm saying is the components lives in cluster and they don't know them a priority.",
                    "label": 0
                },
                {
                    "sent": "They want to write this down.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The last one is.",
                    "label": 0
                },
                {
                    "sent": "So what the case where I give you an adjacency matrix among the tasks.",
                    "label": 1
                },
                {
                    "sent": "So I give you matrix.",
                    "label": 0
                },
                {
                    "sent": "Basically this statue related to this one.",
                    "label": 1
                },
                {
                    "sent": "This tax rate to this other one, and you know in a few cases there are experts maybe.",
                    "label": 0
                },
                {
                    "sent": "A friend of mine works in mortality series of different countries and Spain is more similar to Italy than some other country and so I can enforce some what correlation.",
                    "label": 0
                },
                {
                    "sent": "This way.",
                    "label": 0
                },
                {
                    "sent": "I design it by hand, expert driven and then once I have this matrix, MI can write this.",
                    "label": 0
                },
                {
                    "sent": "This degree is just the sum of each row.",
                    "label": 0
                },
                {
                    "sent": "This is the sum of each of these matrix.",
                    "label": 0
                },
                {
                    "sent": "I get this something like a lot blasian and again I can write down both the kernel and the corresponding.",
                    "label": 0
                },
                {
                    "sent": "Regularizer.",
                    "label": 0
                },
                {
                    "sent": "So again, here is a matter of what's your favorite point of view?",
                    "label": 0
                },
                {
                    "sent": "Can stem from the kernel?",
                    "label": 0
                },
                {
                    "sent": "You can start from the regularizer and you can go from one to the other.",
                    "label": 0
                },
                {
                    "sent": "Universe yeah.",
                    "label": 0
                },
                {
                    "sent": "Precision.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's uh.",
                    "label": 0
                },
                {
                    "sent": "So when you go yeah, when you go from the weight.",
                    "label": 0
                },
                {
                    "sent": "So if you have a matrix of weights here, you have to take the pseudoinverse to go there.",
                    "label": 0
                },
                {
                    "sent": "And the other way around.",
                    "label": 0
                },
                {
                    "sent": "So it's exactly what you're saying.",
                    "label": 0
                },
                {
                    "sent": "Anyway, so these are things that people design using this penalty.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what what we thought about is we've been working a bit on the following.",
                    "label": 0
                },
                {
                    "sent": "Suppose that I give you this kernel.",
                    "label": 0
                },
                {
                    "sent": "You designed it.",
                    "label": 0
                },
                {
                    "sent": "OK, now you have to typically to solve this linear system.",
                    "label": 0
                },
                {
                    "sent": "This is a kernel matrix and let me just explain the notation, so I put all my data is stuck them in blocks in this big matrix and now this Y is going to be all the possible output values stack one after the other and see the same thing is like the coefficients for each task.",
                    "label": 0
                },
                {
                    "sent": "So this matrix is big is T task times entry points if the number of points is different times same and each of these guys T * N T and feel just the number of points for task.",
                    "label": 0
                },
                {
                    "sent": "So computing this.",
                    "label": 0
                },
                {
                    "sent": "It's very expensive and I guess.",
                    "label": 0
                },
                {
                    "sent": "Similar to the comments that Marissa was doing, one can try to say, OK, let's try to start to do approximations.",
                    "label": 0
                },
                {
                    "sent": "The price of in, particularly in machine learning, especially recently.",
                    "label": 0
                },
                {
                    "sent": "This parameter Lambda is perceived as parameterized to be chosen with some kind of cross validation procedure, or something else.",
                    "label": 0
                },
                {
                    "sent": "So when you do something like this, the price of this is not the price of doing it.",
                    "label": 0
                },
                {
                    "sent": "Once the price of doing it as many times.",
                    "label": 0
                },
                {
                    "sent": "As those needed to find these regularization parameter sometimes is called the regularization path.",
                    "label": 0
                },
                {
                    "sent": "And here you have if you have different values and if you just do it naively by inverting.",
                    "label": 0
                },
                {
                    "sent": "All the time you get in this.",
                    "label": 0
                },
                {
                    "sent": "Kind of complexity, which is huge.",
                    "label": 0
                },
                {
                    "sent": "This is really pessimistic.",
                    "label": 0
                },
                {
                    "sent": "You can do it even in this case you can do it for just by doing SVD, except the bigger constant.",
                    "label": 0
                },
                {
                    "sent": "But it might be better and you compute it just once.",
                    "label": 0
                },
                {
                    "sent": "So our idea is that we basically worked quite a bit on connection with delivering inverse problems, where basically they would have to solve big linear systems.",
                    "label": 0
                },
                {
                    "sent": "And this is basically solving a big linear system.",
                    "label": 0
                },
                {
                    "sent": "By regularising it by doing a preconditioning.",
                    "label": 0
                },
                {
                    "sent": "Augmented the diagonal.",
                    "label": 0
                },
                {
                    "sent": "So our idea is OK, let's let's ask the Mark and the people after the mark in the last.",
                    "label": 0
                },
                {
                    "sent": "100 years and let's see what I've done.",
                    "label": 0
                },
                {
                    "sent": "If they have a better way of doing it or comperable way of doing it.",
                    "label": 0
                },
                {
                    "sent": "And the idea here is that we view this.",
                    "label": 0
                },
                {
                    "sent": "So if you put Lambda equal to 0, this problem can be unstable.",
                    "label": 0
                },
                {
                    "sent": "So we want to regularize and the question is, can we regularize in some different way with respect to this one?",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If I take the eigendecomposition of this big kernel matrix, suppose that somebody gives me the SVD.",
                    "label": 0
                },
                {
                    "sent": "So you hear Eigenvector and Simard eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "I can just rewrite Tikhonov this way.",
                    "label": 0
                },
                {
                    "sent": "OK, in components.",
                    "label": 0
                },
                {
                    "sent": "OK with everybody.",
                    "label": 0
                },
                {
                    "sent": "So if I do this, I see that the effect of tickle me some of the filtering of this component is well known in every community, yes, and ideas that I can view this as a filtering and the question is why these filters can I order under interesting filters.",
                    "label": 0
                },
                {
                    "sent": "Is I say it's a filter because you can imagine this to be something like frequencies.",
                    "label": 0
                },
                {
                    "sent": "So each of these are basically eigenvector.",
                    "label": 0
                },
                {
                    "sent": "I ordered them according to the values of the corresponding eigenvalues, and I say that big eigenvalues are slow frequency essentially and small eigenvalues are high frequency.",
                    "label": 0
                },
                {
                    "sent": "Or if you want principle big principle component and small ones and I just basically want to do some shrinkage across these bases which is given by the data.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And these executive picture.",
                    "label": 0
                },
                {
                    "sent": "So these are larger in values.",
                    "label": 0
                },
                {
                    "sent": "This is more like in values and I want to do something like this a low pass filter and say whatever is here.",
                    "label": 1
                },
                {
                    "sent": "I keep it whatever is there and throw it away.",
                    "label": 0
                },
                {
                    "sent": "OK, so up to now is just a different way to describe kind of regularization.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this is just a filter for Tikhonov, so even if I'm acting on a matrix I can just consider the function on the eigenvalues.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are a bunch of things that people have been doing is basically.",
                    "label": 0
                },
                {
                    "sent": "A bunch of different kind of algorithms.",
                    "label": 0
                },
                {
                    "sent": "In particular, I'm going to show this iterative algorithm that I've been around for awhile and I'm just going to describe the simplest one, though it's not the one that we actually use, but it's very easy to Excel.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Name?",
                    "label": 0
                },
                {
                    "sent": "And this simple 11 can describe it in two different ways.",
                    "label": 0
                },
                {
                    "sent": "So let me first let's first look at this OK.",
                    "label": 0
                },
                {
                    "sent": "So this you can recognize you're doing essentially something like the gradient descent of the least square error.",
                    "label": 1
                },
                {
                    "sent": "Without any constraint.",
                    "label": 0
                },
                {
                    "sent": "So you don't say that the norm is to be in a ball.",
                    "label": 0
                },
                {
                    "sent": "You don't make any other.",
                    "label": 0
                },
                {
                    "sent": "You don't project anywhere, you just so it's an unconstrained problem.",
                    "label": 1
                },
                {
                    "sent": "You solve the square by gradient descent, so if you just let it go.",
                    "label": 0
                },
                {
                    "sent": "You are fitting the data and that's it, OK?",
                    "label": 1
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The intuition is that if you somewhat stop somewhere in between just by early stopping.",
                    "label": 0
                },
                {
                    "sent": "You are getting a better solution.",
                    "label": 0
                },
                {
                    "sent": "So let me suggest.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is just a simple picture.",
                    "label": 0
                },
                {
                    "sent": "The idea is that you start from.",
                    "label": 0
                },
                {
                    "sent": "This is just this color case, but it will work in two or three dimensions the same way you just start from the flat.",
                    "label": 0
                },
                {
                    "sent": "Solution just the zero solution if you let it go.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is it good to go.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Much you start to overfit.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And somewhere in between you got a better solution and this is very well known.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In English of English problems.",
                    "label": 0
                },
                {
                    "sent": "And the idea is that you can view this as a filter.",
                    "label": 0
                },
                {
                    "sent": "OK, why well, you can do it in many ways.",
                    "label": 0
                },
                {
                    "sent": "So one way you can just do this and just write it by induction.",
                    "label": 0
                },
                {
                    "sent": "What you see, what the Alpha parameter azati step is and we turn out that will be described by this function.",
                    "label": 0
                },
                {
                    "sent": "OK, this is slightly nicer way of doing it.",
                    "label": 0
                },
                {
                    "sent": "That's somewhat surprising because at first sight the gradient decent.",
                    "label": 0
                },
                {
                    "sent": "Has nothing to do with it.",
                    "label": 0
                },
                {
                    "sent": "Take the Matrix A and do a power low expansion like you would do with just a real number.",
                    "label": 0
                },
                {
                    "sent": "So for meters you can do the same thing.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that this filter here simply corresponds to take a power expansion of the universe, but then truncated.",
                    "label": 1
                },
                {
                    "sent": "So I'm not describing the inverse, but I'm truncating it, so I throw away some information that is not useful, so that's where the regularization is coming from, and it turns out that.",
                    "label": 0
                },
                {
                    "sent": "I don't have to compute the eigenvalues to do is.",
                    "label": 0
                },
                {
                    "sent": "I don't have to do any kind of SVD, I can just do this iteration and I'm done.",
                    "label": 0
                },
                {
                    "sent": "So it turns out that this is.",
                    "label": 0
                },
                {
                    "sent": "In some sense, equivalent because it's doing a low pass filter.",
                    "label": 1
                },
                {
                    "sent": "Similarly to taken off, but the way you implement it is different.",
                    "label": 0
                },
                {
                    "sent": "You don't have to take any eigenvalue and you want to invert any matrix and you just do this thing and you see here what I do is that I have a matrix and vectors and I multiply 1 with the other.",
                    "label": 0
                },
                {
                    "sent": "You also see that at the step size here, so I have two parameters.",
                    "label": 0
                },
                {
                    "sent": "In fact, the number of iterations and the step size the same size.",
                    "label": 1
                },
                {
                    "sent": "One can fix a priority, basically because if I want this series to converge.",
                    "label": 0
                },
                {
                    "sent": "Similar to Discover Case E to a has to be smaller than one in this case in Norm.",
                    "label": 0
                },
                {
                    "sent": "So this is our alternative or actually variation of this is actually the code of this algorithm, liberation.",
                    "label": 0
                },
                {
                    "sent": "We have in mind is one line longer in code and basically shows that with a fixed step size you can achieve the same solution but with the square root of the number of iterations, so it's much faster.",
                    "label": 0
                },
                {
                    "sent": "And we got this called.",
                    "label": 1
                },
                {
                    "sent": "So this thing here was called Delta boosting in statistics and reinvented maybe 10 years ago, and it was done in the 70s.",
                    "label": 0
                },
                {
                    "sent": "In the verse problem: fiber.",
                    "label": 1
                },
                {
                    "sent": "It's just gradient descent.",
                    "label": 0
                },
                {
                    "sent": "These are accelerated version is called the new methods and it's just some kind of accelerated gradient descent in the numerics.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Literature.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So this is one point I want to make.",
                    "label": 0
                },
                {
                    "sent": "So by using things that are not taken often without the default, I write this down and I just invert the big matrix and I got this cube complexity.",
                    "label": 0
                },
                {
                    "sent": "So here there are a couple of nice things.",
                    "label": 0
                },
                {
                    "sent": "OK, so first of all, let's look at the complexity first.",
                    "label": 0
                },
                {
                    "sent": "Each iteration just cost me.",
                    "label": 0
                },
                {
                    "sent": "T&T Square is a matrix vector multiplication, that's it.",
                    "label": 0
                },
                {
                    "sent": "Plus, whenever I run an iteration, I'm looking at the different regularization parameter because now the regulation parameter is no Lambda.",
                    "label": 0
                },
                {
                    "sent": "Only relation parameter is the number of iterations.",
                    "label": 0
                },
                {
                    "sent": "So I go.",
                    "label": 0
                },
                {
                    "sent": "I have a consistent increase in complexity.",
                    "label": 0
                },
                {
                    "sent": "As soon as I do this because I have a natural warm restart kind of procedure.",
                    "label": 0
                },
                {
                    "sent": "And then the rest is.",
                    "label": 0
                },
                {
                    "sent": "So some what it gives us from from from.",
                    "label": 0
                },
                {
                    "sent": "Frequently is kind of perspective.",
                    "label": 0
                },
                {
                    "sent": "This is not just doing empirical risk with the constraint on the potty space, or rather it's doing it, but in a slightly different way.",
                    "label": 0
                },
                {
                    "sent": "So the way you enforce a prior notice, meaning it's somewhat strange.",
                    "label": 0
                },
                {
                    "sent": "It's very tightly coupled with the way you solve the algorithm.",
                    "label": 0
                },
                {
                    "sent": "At least, this is my point of view, so in some sense the inference principles, the probabilistic modeling principle, and the computational principle are really blended together, and they're not too subsequent step.",
                    "label": 0
                },
                {
                    "sent": "So if you do an iteration here, it has an effect on the way you actually do the inference, so that was somewhat interesting.",
                    "label": 0
                },
                {
                    "sent": "It was an interesting aspect to me.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So it turns out that I will show you experiment, so if.",
                    "label": 0
                },
                {
                    "sent": "If the kernel has this specific form.",
                    "label": 0
                },
                {
                    "sent": "So just one kernel times one matrix.",
                    "label": 0
                },
                {
                    "sent": "In fact, I can simply diagonalize this matrix, an project.",
                    "label": 0
                },
                {
                    "sent": "All the data there, and then the complexity is really ridiculous is basically you can solve this if this is the case where at the same number of inputs for each task.",
                    "label": 0
                },
                {
                    "sent": "So if you have the same inputs for each task and the kernel is of this form, well basically in that case I can just diagonalize a rotate everything according to A and it turns out is associated.",
                    "label": 1
                },
                {
                    "sent": "The complexity of the multi task is the same as the complexity of 1 task.",
                    "label": 0
                },
                {
                    "sent": "Essential.",
                    "label": 0
                },
                {
                    "sent": "And I think this is somewhat.",
                    "label": 0
                },
                {
                    "sent": "It probably is somewhat known, but it's not.",
                    "label": 0
                },
                {
                    "sent": "I didn't see very much in the.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so you are learning transfer.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I was talking to address before.",
                    "label": 0
                },
                {
                    "sent": "I don't think that people doing regularization observed.",
                    "label": 0
                },
                {
                    "sent": "This thing, but coming it's it's there.",
                    "label": 0
                },
                {
                    "sent": "Boston.",
                    "label": 0
                },
                {
                    "sent": "Yeah, you can do the numbers right if you have.",
                    "label": 0
                },
                {
                    "sent": "It depends how many data you have and how many tasks you have.",
                    "label": 0
                },
                {
                    "sent": "If you have a billion test point.",
                    "label": 0
                },
                {
                    "sent": "This video is going to kill you if you have 50 test point.",
                    "label": 0
                },
                {
                    "sent": "And as we do a 50 by 50 matrix, you can still do it, especially if it's semidefinite.",
                    "label": 0
                },
                {
                    "sent": "Yeah, depends can be.",
                    "label": 0
                },
                {
                    "sent": "You can be a good thing or not.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we just apply this to a simple case and we took this.",
                    "label": 0
                },
                {
                    "sent": "So this is just a simulation data where we take this kind of velocity field and we convolve with a Gaussian because then it decays fast enough and we know that we can be decomposed in.",
                    "label": 0
                },
                {
                    "sent": "In a field with a part which is curl free and apart with this divergent free.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it has two components, but with no curves.",
                    "label": 0
                },
                {
                    "sent": "And what we know they vergence and we finally are a couple of kernels.",
                    "label": 0
                },
                {
                    "sent": "They are not of the form KA.",
                    "label": 0
                },
                {
                    "sent": "They really somewhat couple the different tasks in no funny way.",
                    "label": 0
                },
                {
                    "sent": "And one of them is blind to divergent.",
                    "label": 0
                },
                {
                    "sent": "It cannot reconstruct divergent that one is blind to.",
                    "label": 0
                },
                {
                    "sent": "Curl.",
                    "label": 0
                },
                {
                    "sent": "So we essentially taking an average of these two.",
                    "label": 0
                },
                {
                    "sent": "You can actually try to reconstruct the good approximation of a field which is.",
                    "label": 0
                },
                {
                    "sent": "Curl for your day veterans free and you can basically.",
                    "label": 0
                },
                {
                    "sent": "Extract these two parts.",
                    "label": 0
                },
                {
                    "sent": "We just looking from the book.",
                    "label": 0
                },
                {
                    "sent": "This is one of the few I found in the neutral RBF functions for multi output functions, so I hope to find more in the literature of people that try to estimate vector valued functions, but in fact so far I didn't and these were some couple of nice ones.",
                    "label": 0
                },
                {
                    "sent": "I think in fact that's kind of easy because it's again something like this color kernel and probably.",
                    "label": 0
                },
                {
                    "sent": "Well, I don't think it's too bad, but.",
                    "label": 0
                },
                {
                    "sent": "I just give it for granted.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and we applied it.",
                    "label": 0
                },
                {
                    "sent": "You can estimate the true field estimated field and then you have the two parts, one which is curved three in the one which is divergent three and what I.",
                    "label": 0
                },
                {
                    "sent": "It is very noisy, so by comparing independent an this kind of joint estimation, you don't have much of an improving.",
                    "label": 0
                },
                {
                    "sent": "You have a bit of improvement, but when you can.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At time of doing this new method, which is the accelerated gradient or that equal regularization, and you put both the cross validation and the the choice of the parameter into the game, then it's huge.",
                    "label": 0
                },
                {
                    "sent": "It's very big.",
                    "label": 0
                },
                {
                    "sent": "There, the computation time and here you can see the computation time as the number of training example increases.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the other thing I want to say very quickly is that so the classic kernel, for example that I showed before, seems a bit too simple to me.",
                    "label": 0
                },
                {
                    "sent": "But then I think that they are fixed.",
                    "label": 0
                },
                {
                    "sent": "They don't depend on the data, so I can go and try to prove some kind of frequentist results and going to large deviation concentration of measure results which are basically based on the idea that when you have the kernel matrix is some kind of approximation of integral operator.",
                    "label": 0
                },
                {
                    "sent": "So if you have, if you let the number of points goes to Infinity, you have somewhat consistent limit.",
                    "label": 0
                },
                {
                    "sent": "An basis, some kind of generalization of random matrix results?",
                    "label": 0
                },
                {
                    "sent": "That's one possible way so you can have like a concentration of the empirical matrix in a sense of qualitatively to its expected parts, or something like the empirical covariance matrix and expected covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Even if the dimension and then by.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Using this essentially you have rates of convergence.",
                    "label": 0
                },
                {
                    "sent": "Which are somewhat optimal.",
                    "label": 0
                },
                {
                    "sent": "So you can say that the function you retrieve is essentially up to.",
                    "label": 0
                },
                {
                    "sent": "See in terms of learning rates, so in terms of sample complexity for, you can say how many sample I need to actually estimate it within this precision with a certain probability, and that's that's optimal.",
                    "label": 0
                },
                {
                    "sent": "Yep.",
                    "label": 0
                },
                {
                    "sent": "Sorry, this is just the best possible function which depends on the probability and this is the probability or the probability.",
                    "label": 0
                },
                {
                    "sent": "So this this is this.",
                    "label": 0
                },
                {
                    "sent": "This is essentially the risk.",
                    "label": 0
                },
                {
                    "sent": "There is the least square risk.",
                    "label": 0
                },
                {
                    "sent": "Averaged over the possible of the probability measure.",
                    "label": 0
                },
                {
                    "sent": "I just wanted to.",
                    "label": 0
                },
                {
                    "sent": "The meaning of this is just to show that with this specific class of kernels which are fixed and independent to the data, it's easy to prove this kind of result in some sense.",
                    "label": 0
                },
                {
                    "sent": "You only have to care about the regularization part, but the rest is is not there.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'm pretty much done with the first part.",
                    "label": 0
                },
                {
                    "sent": "So if you have any questions on this.",
                    "label": 0
                },
                {
                    "sent": "To summarize, the idea here was to show in some sense we already have some other theory in this color case, and we wanted to see how it translate and what we could benefit with respect to what was already around going to the vector case and the main messages.",
                    "label": 0
                },
                {
                    "sent": "I think computationally can be more interesting to use iterative methods, remembering that the iteration is doing something, so don't use it to solve a linear system.",
                    "label": 0
                },
                {
                    "sent": "Many times whenever you change the requisition parameter, this is something that people would typically do in machine learning.",
                    "label": 0
                },
                {
                    "sent": "That was.",
                    "label": 0
                },
                {
                    "sent": "Potentially nice message and the other thing is that if the kernel is fixed pretty much older results in this color case just go through.",
                    "label": 0
                },
                {
                    "sent": "And of course our results are a bit dumb because they don't exploit.",
                    "label": 0
                },
                {
                    "sent": "They don't really look specifically at the features of this vector valued case.",
                    "label": 0
                },
                {
                    "sent": "For example, the effect of the number of.",
                    "label": 0
                },
                {
                    "sent": "Tasks OK, so this is just a preliminary result.",
                    "label": 0
                },
                {
                    "sent": "There is a lot to do in this setting.",
                    "label": 0
                },
                {
                    "sent": "Separation of your kernels versus exactly absolutely absolutely so.",
                    "label": 0
                },
                {
                    "sent": "Essentially that result basically says if you have a fixed kernel and is bounded.",
                    "label": 0
                },
                {
                    "sent": "OK, you can relax this, but that's easier one then everything is fine.",
                    "label": 0
                },
                {
                    "sent": "And you have these nice results.",
                    "label": 0
                },
                {
                    "sent": "You have a certain rate.",
                    "label": 0
                },
                {
                    "sent": "Now you can expect that if.",
                    "label": 0
                },
                {
                    "sent": "Separation will have an effect number of.",
                    "label": 0
                },
                {
                    "sent": "Task will have an effect and we don't have it OK, and as far as I know.",
                    "label": 0
                },
                {
                    "sent": "Is not there, nobody did it.",
                    "label": 0
                },
                {
                    "sent": "It was just a preliminary thing.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me tell you about my experience.",
                    "label": 0
                },
                {
                    "sent": "So my original motivation to do this was not just to multitask or vector fields, but I was curious about multi class OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "At first I look at this stuff and I notice this difference an it's obvious.",
                    "label": 0
                },
                {
                    "sent": "I mean, after today I even have a different name.",
                    "label": 0
                },
                {
                    "sent": "I called this multi task and this vector fields, but is exactly the third topic is a topic that didn't have names.",
                    "label": 1
                },
                {
                    "sent": "So now I have this name.",
                    "label": 0
                },
                {
                    "sent": "Probably better use it, but essentially what I want.",
                    "label": 0
                },
                {
                    "sent": "OK I just want to think a second about the following situation.",
                    "label": 0
                },
                {
                    "sent": "You have two tasks.",
                    "label": 0
                },
                {
                    "sent": "In the in the first case you have points here that you don't have here.",
                    "label": 1
                },
                {
                    "sent": "And the two curves are very similar in the other case, the points are the same, but the noise might be different, for example.",
                    "label": 1
                },
                {
                    "sent": "So I'm assuming that they're very similar in the picture is somewhat simplified, and the message of this is that.",
                    "label": 0
                },
                {
                    "sent": "You see this function.",
                    "label": 0
                },
                {
                    "sent": "I just have three points here.",
                    "label": 1
                },
                {
                    "sent": "I don't have anything here, so if I can use somewhat correlation among the task, of course I'm going to prove the whole thing a lot, because the reconstruction of this task done here will be ridiculously bad.",
                    "label": 0
                },
                {
                    "sent": "OK, so even if in some sense my modeling assumption among the tasks is not too smart.",
                    "label": 0
                },
                {
                    "sent": "Still, it's better than 0, maybe, but it's kind of likely OK, whereas if I'm here.",
                    "label": 0
                },
                {
                    "sent": "Well, I gotta be a bit smarter than that, because this I call this a boring strength effect.",
                    "label": 0
                },
                {
                    "sent": "It's not that strong and that's why in some sense.",
                    "label": 0
                },
                {
                    "sent": "I know this is important to me and.",
                    "label": 0
                },
                {
                    "sent": "It's not cute.",
                    "label": 0
                },
                {
                    "sent": "I don't know this is something I would like to know what other people think about.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "You know, after so much, but we're chatting today.",
                    "label": 0
                },
                {
                    "sent": "I think that maybe one can.",
                    "label": 0
                },
                {
                    "sent": "I was looking at the low dimensional application that are done in just autistic son.",
                    "label": 0
                },
                {
                    "sent": "I was thinking that kind of very high dimensional situation or one is dealing with in learning anything that maybe one think of the following kind of regime to look at.",
                    "label": 0
                },
                {
                    "sent": "The first one.",
                    "label": 0
                },
                {
                    "sent": "OK, this is like classical, you have lots of data and then you have some dimension and some outputs.",
                    "label": 0
                },
                {
                    "sent": "OK, it seems to be the classical thing.",
                    "label": 0
                },
                {
                    "sent": "Central limit theorem kind of regime.",
                    "label": 0
                },
                {
                    "sent": "Forget about the sampling.",
                    "label": 0
                },
                {
                    "sent": "Have enough variable, I just estimate they try to be.",
                    "label": 0
                },
                {
                    "sent": "Modeling.",
                    "label": 0
                },
                {
                    "sent": "This is somewhat high dimensional inference regime.",
                    "label": 1
                },
                {
                    "sent": "You have many more variables than points higher because it's the nature of the problem.",
                    "label": 0
                },
                {
                    "sent": "It was because your ignorance reflecting the fact that you take lots of measurements.",
                    "label": 0
                },
                {
                    "sent": "I take a billion wavelets coefficients out of an image or I have micro arrays.",
                    "label": 0
                },
                {
                    "sent": "We chart at billions numbers for a few.",
                    "label": 0
                },
                {
                    "sent": "But that maybe once you think a bit more about this situation, how many samples you have with respect to the template into the template to the coordinates.",
                    "label": 0
                },
                {
                    "sent": "Maybe you know if you're in two days into coordinates here, or if you like, I think what this you think about you have a million coordinates output.",
                    "label": 0
                },
                {
                    "sent": "That's the kind of different regime and the modeling assumption you make might not be the same, especially.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Having this in mind.",
                    "label": 0
                },
                {
                    "sent": "This kind of difference.",
                    "label": 0
                },
                {
                    "sent": "So I personally have not seen.",
                    "label": 0
                },
                {
                    "sent": "Improvement.",
                    "label": 0
                },
                {
                    "sent": "In problems like this in dimension bigger than 3 four.",
                    "label": 0
                },
                {
                    "sent": "That's why today is largely we're discussing this.",
                    "label": 0
                },
                {
                    "sent": "This is very interesting to me.",
                    "label": 0
                },
                {
                    "sent": "I would be curious to see a fair comparison with like a model selection done in high dimensional output case.",
                    "label": 0
                },
                {
                    "sent": "Exactly in this situation you mean.",
                    "label": 0
                },
                {
                    "sent": "At this point, but suppose that you can actually enforce it.",
                    "label": 0
                },
                {
                    "sent": "OK, So what is the kernels I showed you before?",
                    "label": 0
                },
                {
                    "sent": "Basically said that for some reason you can either say that these two curves are like average curve plus perturbation, so I'm not as nice as they did I can shift them around, but maybe here is not exactly like this.",
                    "label": 0
                },
                {
                    "sent": "It might be a bit up and down, but it's not flat because it's similar to this one, so it's a prior assumption.",
                    "label": 0
                },
                {
                    "sent": "I don't learn it, it's prior, so the kernels I show our prior based.",
                    "label": 0
                },
                {
                    "sent": "It might be like spatially close the two things in this patient, absolutely.",
                    "label": 0
                },
                {
                    "sent": "He was doing earlier than that, so that's why.",
                    "label": 0
                },
                {
                    "sent": "Absolutely, you might have.",
                    "label": 0
                },
                {
                    "sent": "But I think Seton Hall addition and then you could learn it and assumed about overlap correlation existed across the space station.",
                    "label": 0
                },
                {
                    "sent": "There, be she optimal degree of overlap, that the purely I wish it were it is in I so yeah this is not just an extreme header.",
                    "label": 0
                },
                {
                    "sent": "Obviously too.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                },
                {
                    "sent": "I agree, I agree.",
                    "label": 0
                },
                {
                    "sent": "I just think it's it's.",
                    "label": 0
                },
                {
                    "sent": "It's maybe something to think about.",
                    "label": 0
                },
                {
                    "sent": "Usually it's just a big list of problems with multiple output.",
                    "label": 0
                },
                {
                    "sent": "Yeah sure it's true, but maybe they're not exactly the same and maybe they think you can see being effective in one case or not in some other case, this is definitely my XP.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Particularly, I just want to mention briefly this.",
                    "label": 0
                },
                {
                    "sent": "So when is one class classification multiclass classification?",
                    "label": 0
                },
                {
                    "sent": "At least in my neighborhood, everybody tend to refer to this paper that came out a few years ago and basically after 15 years of people generalizing SVM's, an related thing just sit and do a simple one versus cold one versus whole scheme.",
                    "label": 0
                },
                {
                    "sent": "So you just take one class against all the others.",
                    "label": 0
                },
                {
                    "sent": "Next class against all the others and in the end you take the maximum of your classification.",
                    "label": 0
                },
                {
                    "sent": "For each class individually OK. And this paper basically sit there, they don't have many particularly theoretical claims, but they basically do all the experience that people did before using their own code and basically show that if you use the one versus Oh well, meaning that you do the nice cross validation for each task, you get better result or comperable result.",
                    "label": 0
                },
                {
                    "sent": "So in some sense, to me this was like.",
                    "label": 0
                },
                {
                    "sent": "Annoying because you're like really.",
                    "label": 0
                },
                {
                    "sent": "I mean if I tell you that you have cats and dogs and you have tables and seats, you cannot do better than that.",
                    "label": 0
                },
                {
                    "sent": "I mean, you forget about that and you do better than that.",
                    "label": 0
                },
                {
                    "sent": "Is that possible?",
                    "label": 0
                },
                {
                    "sent": "But still it doesn't.",
                    "label": 0
                },
                {
                    "sent": "You know it was a good benchmark, so let's go and try it.",
                    "label": 0
                },
                {
                    "sent": "That's what I thought.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so I thought maybe we could use the multi output kind of stuff.",
                    "label": 0
                },
                {
                    "sent": "So how well the simplest idea is to say well just put a coding vector to each class so.",
                    "label": 0
                },
                {
                    "sent": "This is this is the way I would do it from a functional perspective.",
                    "label": 0
                },
                {
                    "sent": "So I say the class one is this coding 1 -- 1 -- 1 + 2 is minus one one.",
                    "label": 1
                },
                {
                    "sent": "I keep on going like that.",
                    "label": 0
                },
                {
                    "sent": "Then in some sense I just do least squares on these coding vectors.",
                    "label": 0
                },
                {
                    "sent": "And then so it was interesting.",
                    "label": 0
                },
                {
                    "sent": "It turns out this is our.",
                    "label": 0
                },
                {
                    "sent": "Vector field.",
                    "label": 0
                },
                {
                    "sent": "And now OK this yeah that's right.",
                    "label": 0
                },
                {
                    "sent": "And this is just the independent case.",
                    "label": 0
                },
                {
                    "sent": "So this is I wrote it as a vector problem, but it's really an independent problem.",
                    "label": 0
                },
                {
                    "sent": "Suppose they do this and then in the end I checked.",
                    "label": 0
                },
                {
                    "sent": "Just take the maximum of this.",
                    "label": 0
                },
                {
                    "sent": "So this is a glass exactly one versus all sonar in the language that we discussed today.",
                    "label": 1
                },
                {
                    "sent": "One versus all is least square estimation with a block diagonal Colonel.",
                    "label": 0
                },
                {
                    "sent": "And again, I'm going to say, can I learn this stuff?",
                    "label": 0
                },
                {
                    "sent": "Can I do something better than this?",
                    "label": 0
                },
                {
                    "sent": "So the good news in some sense that even if at first sight this might be weird to do this coding decoding doesn't matter a lot.",
                    "label": 0
                },
                {
                    "sent": "And when you do this, when you take this FJ, this FJ are proportional the one that you estimate are proportional to the conditional probabilities, so that in fact makes a lot of sense and one can prove based consistency and rates of this kind of.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "From a large deviation point of view.",
                    "label": 0
                },
                {
                    "sent": "The weird thing is that the state of the art doesn't put any correlation among classes.",
                    "label": 1
                },
                {
                    "sent": "And now for us this is really for me is really this means that the kernel is block diagonal OK and the question is can we do better than that?",
                    "label": 0
                },
                {
                    "sent": "But now the problem is that.",
                    "label": 0
                },
                {
                    "sent": "I learn about the literature in cracking, for example, and I can try to do OK. Let's try to do some kind of correlation among the tasks, but this is going to be.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Completely meaningless because I invented them.",
                    "label": 0
                },
                {
                    "sent": "But there is no distance here.",
                    "label": 0
                },
                {
                    "sent": "So the question in this case is, I think even harder than the vector field case because.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "How do I get this correlation among the tasks without enforcing, you know, too much information that is going to overfit the data.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So in some sense my view OK, we try to do this stuff.",
                    "label": 0
                },
                {
                    "sent": "We tried to put it to construct cases where we have 3 little bowl and on a triangle and we put a couple together.",
                    "label": 0
                },
                {
                    "sent": "So we tried all different kind of tricks and we tried to enforce prior information on that and what we saw at least in our experiments that you improve the conditional probabilities.",
                    "label": 0
                },
                {
                    "sent": "So you look at the connection with the look better when you take this Mac.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you do the classification.",
                    "label": 0
                },
                {
                    "sent": "Kill it.",
                    "label": 0
                },
                {
                    "sent": "So if you just look at the rates.",
                    "label": 0
                },
                {
                    "sent": "That's it.",
                    "label": 0
                },
                {
                    "sent": "Then again, it will be very crazy.",
                    "label": 0
                },
                {
                    "sent": "This is the end of the story, really.",
                    "label": 0
                },
                {
                    "sent": "One versus all is the best thing you can do.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah, I guess that's pretty much what I wanted to say.",
                    "label": 0
                },
                {
                    "sent": "So in my view there are really some what hierarchy of difficulties of problems.",
                    "label": 0
                },
                {
                    "sent": "The multitask problem is in some sense the easier one.",
                    "label": 0
                },
                {
                    "sent": "And the vector fields in a multiclass are harder and harder.",
                    "label": 0
                },
                {
                    "sent": "And yeah, that's pretty much what I want to say.",
                    "label": 0
                },
                {
                    "sent": "Question.",
                    "label": 0
                },
                {
                    "sent": "Send Brenda what's the?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That right there, when were you in the case where we had basically the same?",
                    "label": 0
                },
                {
                    "sent": "Covariance breach we had that picture of, you know, three points here 340 yeah, I was usually.",
                    "label": 0
                },
                {
                    "sent": "I was using one of these kernels here.",
                    "label": 0
                },
                {
                    "sent": "The one that I showed at the beginning like this say.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I actually I I knew this.",
                    "label": 0
                },
                {
                    "sent": "They designed it.",
                    "label": 0
                },
                {
                    "sent": "I was using still this model where I have this simple kernel by design that.",
                    "label": 0
                },
                {
                    "sent": "And I was asking myself how many points I need for each class before I see some improvement.",
                    "label": 0
                },
                {
                    "sent": "And you know, it's after few few points per class.",
                    "label": 0
                },
                {
                    "sent": "Everything worked perfectly.",
                    "label": 0
                },
                {
                    "sent": "Sorry I missed it, but that was for the classification.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I think that this was the problem we all had in Gaussian process is when we were modeling all these things nicely.",
                    "label": 0
                },
                {
                    "sent": "Stuff like that you don't be support vector machines doing one versus when you're doing maximum Alpha, you're making the nice predictions of these conditional probability and then you have to find an excuse as to why.",
                    "label": 0
                },
                {
                    "sent": "That's useful, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but again, I was thinking about this.",
                    "label": 0
                },
                {
                    "sent": "I haven't done it yet, so I'm going to do it.",
                    "label": 0
                },
                {
                    "sent": "So there are these huge image datasets where you have 100 + 200 class.",
                    "label": 0
                },
                {
                    "sent": "And I don't know.",
                    "label": 0
                },
                {
                    "sent": "I still don't understand how is it possible that it by telling you their animals and furniture or something and taking features that will reflect that it doesn't help.",
                    "label": 0
                },
                {
                    "sent": "So to me.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Not as far as I know, not as far as results that I've seen compared with benchmarks that I can be reproducible.",
                    "label": 0
                },
                {
                    "sent": "They may also play a different game, which is.",
                    "label": 0
                },
                {
                    "sent": "You if you can't get the belief distinction between, I don't know.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Now, again, I've not.",
                    "label": 0
                },
                {
                    "sent": "I look at some results in the hard cold vision perspective.",
                    "label": 0
                },
                {
                    "sent": "But it might be different.",
                    "label": 0
                },
                {
                    "sent": "Exclusive are they associate discount.",
                    "label": 0
                },
                {
                    "sent": "In this case I was just going to I just considered.",
                    "label": 0
                },
                {
                    "sent": "I just consider the exclusive exclusive case just because then I can prove everything from a theoretical point of using the other problems then.",
                    "label": 0
                },
                {
                    "sent": "He might be.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I it seems like classifications within forever regression.",
                    "label": 0
                },
                {
                    "sent": "Also multiclass seems to be next thing and one versus solid best, that's why.",
                    "label": 0
                },
                {
                    "sent": "I think this is there so I don't know if you have having something at lunch about.",
                    "label": 0
                },
                {
                    "sent": "We have this pre workshop in July is topical was talking.",
                    "label": 0
                },
                {
                    "sent": "We can turn this into the panel.",
                    "label": 0
                }
            ]
        }
    }
}