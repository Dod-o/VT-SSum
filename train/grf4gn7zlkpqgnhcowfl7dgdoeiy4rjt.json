{
    "id": "grf4gn7zlkpqgnhcowfl7dgdoeiy4rjt",
    "title": "Efficient Projections onto the L1-Ball for Learning in High Dimensions",
    "info": {
        "author": [
            "Yoram Singer, The Hebrew University of Jerusalem"
        ],
        "published": "Aug. 29, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_singer_ep/",
    "segmentation": [
        [
            "This is joint work with John Duchi, Shalev Shwartz and two short Chandram.",
            "And let me start with kind of a gentle introduction, and I apologize for those who are experts in compressed sensing.",
            "It might."
        ],
        [
            "You know, be boring, but still I think that you know a couple of slides on motivation would be nice.",
            "So classical problem, machine learning is a problem of feature selection combined with learning.",
            "So here is an example from the Reuters data set.",
            "This is a document from the Reuters data set and.",
            "The task in a text categorization or filtering or filtering the goal is to classify the document as belonging to one of K classes, but to do so since you know the document is over dictionary or very large dictionary."
        ],
        [
            "Typically people would like to extract some tokens.",
            "Some keywords that are going to be useful in the classification and then associate's a label with from a predefined label set."
        ],
        [
            "And then once."
        ],
        [
            "The tokens are identified.",
            "Assign weights to the different tokens as different words might have different importance, so this is kind of a two stage classical approach.",
            "First feature selection, then sort of weight estimation, weight learning."
        ],
        [
            "Penalized learning.",
            "In alternative approaches to impose, take the entire space into our vocabulary, not don't select any word and impose L1 domain constraint."
        ],
        [
            "So what do I mean by this?",
            "And now I'll be more formal.",
            "So let's assume that we have an empirical loss.",
            "We assume that we have."
        ],
        [
            "Loss function with a parameter vector so the parameter vector can be the weight for each feature is the weight for each feature and we impose an L1 domain constraint.",
            "We force the weight vector to be in an L1.",
            "Bull and Z can be thought of a regularization parameter.",
            "This is the radius of the ball."
        ],
        [
            "Now, why L1?",
            "So let me go."
        ],
        [
            "Sort of a proof by picture.",
            "Suppose we have this L1 bowl and we want the solution to be inside this L1 bowl, or at least on the surface of it.",
            "And we have a convex function, so I'm going to deal.",
            "We're going to assume that L is convex.",
            "In many cases it's going to be strongly convey."
        ],
        [
            "So if we have the convex function, we can draw it's level curves right?",
            "And we can start expanding this.",
            "So this is the minimum of the function, the unconstrained menu."
        ],
        [
            "Until he hits the L1 Dimondale one bowl.",
            "And when it hits the alone bullets, it's likely to hit the corner where in high dimension a corner.",
            "If you look in this case, it's a point where many of the coefficients many of the weights or are likely to be 0, and this is, you know there are two citations here, but there's actually much more work on this, but you're also going to see from the algorithmic side that we're going to get sparsity.",
            "We're going to get many zero coefficients just because of the algorithm.",
            "No.",
            "How are we going to work with this?",
            "So this work is actually the L1 sequel to Pegasus?",
            "If you sort of know what Pegasus is, which is a very simple stochastic optimizer for SVM, so this is the L1 version of it, where we replace the L2 regularization with L1 domain constraints so."
        ],
        [
            "The tool is very very basic and ancient.",
            "Actually it's stochastic gradient, but with domain."
        ],
        [
            "Trains or so."
        ],
        [
            "Picture Lee or sorry, first of all, formally we assume that we have basically iterate where on iteration T we take a stochastic estimate of the ground.",
            "So if we're if L is empirical losses.",
            "This combined from multiple examples, we take a subset of the sample as get an estimate of the gradient, making an unconstrained gradient step, and then project back onto the L1 bowl where the projection is the Indy Euclidean sense.",
            "And this is a classical technique called also.",
            "Stochastic gradient projection."
        ],
        [
            "Again, let me just illustrate.",
            "So suppose we have this estimate WT and we want to stay to do stochastic variant dissent while projecting it into the."
        ],
        [
            "So we pick a direction which is the ground and then we project OK and then this gives us WT plus one we pick another."
        ],
        [
            "Direction, which is great in based and project with.",
            "This gives us W2.",
            "So this entire talk is focused on this part.",
            "Basically getting back to the ball.",
            "OK so I'm not going to talk about bounds.",
            "I'm not going to talk about rates of convey."
        ],
        [
            "I'm going to focus solely on efficient algorithms for cleaning projections onto the L1 bowl, but the emphasis will be eventually in high dimension, so I'll have to go through actually low dimension in order to to build.",
            "Basically the tools in order to show how we deal with.",
            "How do we deal with ID?"
        ],
        [
            "High dimensional space?",
            "OK, so let me start with a very very basic projection where all the examples due to the lack of imagination clicks.",
            "My imagination are going to be in R2, but you need to realize that it is affecting only effective only when we have many dementia."
        ],
        [
            "So suppose we have a point, so this can be the result of an unconstrained gradient stepping, which you end up outside of all, and we want to perform the L1 projection, so the L."
        ],
        [
            "Projection amounts to.",
            "Going basically deducting Theta in each cordon."
        ],
        [
            "Meaning that in this case, when the ball is far enough from when the point is far enough from the L1 ball, we're going to simply go down and go left data."
        ],
        [
            "This is going to be our our projected point or mathematically in and I mentioned this is going to be V1 through VN.",
            "We deduct Theta from each coordinate and this is also assuming that the vector is in the positive or a tent."
        ],
        [
            "What happens though, if the point that we need to project is kind of close to one of the axis or in Ida mentions there are many small coefficients we still want."
        ],
        [
            "Growth data in each direction.",
            "So we were supposed to go through to the sort of from language theory tells us that we need to go Theta in need in each."
        ],
        [
            "Each axis, but we can.",
            "We cannot actually sorry also tells us we cannot go below zero.",
            "OK, So what happens is that we."
        ],
        [
            "Not going and then we clip and then we end up doing going Theta only in the X direction.",
            "So mathematically it amounts to and again in N dimensions there is V1 through VN.",
            "Trying to deduct data, but if we hit zero we stop at 0.",
            "So this is actually the basic L1 projection.",
            "Very simple, although I didn't describe what data."
        ],
        [
            "What happens if the point is in general orientation, not necessarily in the positive or tent, so we can deal with?",
            "You know, with this general orientation just by reduction, we're going to take the mirror image of the point, perform the projection operation, get the end result, and then do the inverse of the of the mirroring.",
            "In mathematic Lee, it's not that."
        ],
        [
            "Complicated, it just says, OK, let's take the sign.",
            "Remember it perform the projection of the absolute value of the vector and restore this sign.",
            "OK, so this is actually the entire L1 projection, but there is 1 missing ingredient which is Theta."
        ],
        [
            "So let me give you yet another sort of view which is more still cartoonish but more algebraic.",
            "Your medical Jabra go metric.",
            "Suppose now for the sake of the example we have 7 dimensions OK, and we have a vector in.",
            "General orientation, so if someone tells me what state."
        ],
        [
            "Is I can simply say OK, here's a threshold, Theta everything above the threshold is going."
        ],
        [
            "Whoops, I missed it.",
            "Everything above the threshold is going to.",
            "I'm going to deduct data everything below the threshold is going to be 0."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is joint work with John Duchi, Shalev Shwartz and two short Chandram.",
                    "label": 0
                },
                {
                    "sent": "And let me start with kind of a gentle introduction, and I apologize for those who are experts in compressed sensing.",
                    "label": 0
                },
                {
                    "sent": "It might.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You know, be boring, but still I think that you know a couple of slides on motivation would be nice.",
                    "label": 0
                },
                {
                    "sent": "So classical problem, machine learning is a problem of feature selection combined with learning.",
                    "label": 1
                },
                {
                    "sent": "So here is an example from the Reuters data set.",
                    "label": 0
                },
                {
                    "sent": "This is a document from the Reuters data set and.",
                    "label": 0
                },
                {
                    "sent": "The task in a text categorization or filtering or filtering the goal is to classify the document as belonging to one of K classes, but to do so since you know the document is over dictionary or very large dictionary.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Typically people would like to extract some tokens.",
                    "label": 0
                },
                {
                    "sent": "Some keywords that are going to be useful in the classification and then associate's a label with from a predefined label set.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then once.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The tokens are identified.",
                    "label": 0
                },
                {
                    "sent": "Assign weights to the different tokens as different words might have different importance, so this is kind of a two stage classical approach.",
                    "label": 1
                },
                {
                    "sent": "First feature selection, then sort of weight estimation, weight learning.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Penalized learning.",
                    "label": 0
                },
                {
                    "sent": "In alternative approaches to impose, take the entire space into our vocabulary, not don't select any word and impose L1 domain constraint.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what do I mean by this?",
                    "label": 0
                },
                {
                    "sent": "And now I'll be more formal.",
                    "label": 0
                },
                {
                    "sent": "So let's assume that we have an empirical loss.",
                    "label": 0
                },
                {
                    "sent": "We assume that we have.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Loss function with a parameter vector so the parameter vector can be the weight for each feature is the weight for each feature and we impose an L1 domain constraint.",
                    "label": 0
                },
                {
                    "sent": "We force the weight vector to be in an L1.",
                    "label": 0
                },
                {
                    "sent": "Bull and Z can be thought of a regularization parameter.",
                    "label": 0
                },
                {
                    "sent": "This is the radius of the ball.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, why L1?",
                    "label": 0
                },
                {
                    "sent": "So let me go.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sort of a proof by picture.",
                    "label": 0
                },
                {
                    "sent": "Suppose we have this L1 bowl and we want the solution to be inside this L1 bowl, or at least on the surface of it.",
                    "label": 0
                },
                {
                    "sent": "And we have a convex function, so I'm going to deal.",
                    "label": 0
                },
                {
                    "sent": "We're going to assume that L is convex.",
                    "label": 0
                },
                {
                    "sent": "In many cases it's going to be strongly convey.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if we have the convex function, we can draw it's level curves right?",
                    "label": 0
                },
                {
                    "sent": "And we can start expanding this.",
                    "label": 0
                },
                {
                    "sent": "So this is the minimum of the function, the unconstrained menu.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Until he hits the L1 Dimondale one bowl.",
                    "label": 0
                },
                {
                    "sent": "And when it hits the alone bullets, it's likely to hit the corner where in high dimension a corner.",
                    "label": 0
                },
                {
                    "sent": "If you look in this case, it's a point where many of the coefficients many of the weights or are likely to be 0, and this is, you know there are two citations here, but there's actually much more work on this, but you're also going to see from the algorithmic side that we're going to get sparsity.",
                    "label": 0
                },
                {
                    "sent": "We're going to get many zero coefficients just because of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "How are we going to work with this?",
                    "label": 0
                },
                {
                    "sent": "So this work is actually the L1 sequel to Pegasus?",
                    "label": 0
                },
                {
                    "sent": "If you sort of know what Pegasus is, which is a very simple stochastic optimizer for SVM, so this is the L1 version of it, where we replace the L2 regularization with L1 domain constraints so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The tool is very very basic and ancient.",
                    "label": 0
                },
                {
                    "sent": "Actually it's stochastic gradient, but with domain.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Trains or so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Picture Lee or sorry, first of all, formally we assume that we have basically iterate where on iteration T we take a stochastic estimate of the ground.",
                    "label": 0
                },
                {
                    "sent": "So if we're if L is empirical losses.",
                    "label": 0
                },
                {
                    "sent": "This combined from multiple examples, we take a subset of the sample as get an estimate of the gradient, making an unconstrained gradient step, and then project back onto the L1 bowl where the projection is the Indy Euclidean sense.",
                    "label": 0
                },
                {
                    "sent": "And this is a classical technique called also.",
                    "label": 0
                },
                {
                    "sent": "Stochastic gradient projection.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, let me just illustrate.",
                    "label": 0
                },
                {
                    "sent": "So suppose we have this estimate WT and we want to stay to do stochastic variant dissent while projecting it into the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we pick a direction which is the ground and then we project OK and then this gives us WT plus one we pick another.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Direction, which is great in based and project with.",
                    "label": 0
                },
                {
                    "sent": "This gives us W2.",
                    "label": 0
                },
                {
                    "sent": "So this entire talk is focused on this part.",
                    "label": 0
                },
                {
                    "sent": "Basically getting back to the ball.",
                    "label": 0
                },
                {
                    "sent": "OK so I'm not going to talk about bounds.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to talk about rates of convey.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm going to focus solely on efficient algorithms for cleaning projections onto the L1 bowl, but the emphasis will be eventually in high dimension, so I'll have to go through actually low dimension in order to to build.",
                    "label": 1
                },
                {
                    "sent": "Basically the tools in order to show how we deal with.",
                    "label": 0
                },
                {
                    "sent": "How do we deal with ID?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "High dimensional space?",
                    "label": 0
                },
                {
                    "sent": "OK, so let me start with a very very basic projection where all the examples due to the lack of imagination clicks.",
                    "label": 0
                },
                {
                    "sent": "My imagination are going to be in R2, but you need to realize that it is affecting only effective only when we have many dementia.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So suppose we have a point, so this can be the result of an unconstrained gradient stepping, which you end up outside of all, and we want to perform the L1 projection, so the L.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Projection amounts to.",
                    "label": 0
                },
                {
                    "sent": "Going basically deducting Theta in each cordon.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Meaning that in this case, when the ball is far enough from when the point is far enough from the L1 ball, we're going to simply go down and go left data.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is going to be our our projected point or mathematically in and I mentioned this is going to be V1 through VN.",
                    "label": 0
                },
                {
                    "sent": "We deduct Theta from each coordinate and this is also assuming that the vector is in the positive or a tent.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What happens though, if the point that we need to project is kind of close to one of the axis or in Ida mentions there are many small coefficients we still want.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Growth data in each direction.",
                    "label": 0
                },
                {
                    "sent": "So we were supposed to go through to the sort of from language theory tells us that we need to go Theta in need in each.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each axis, but we can.",
                    "label": 0
                },
                {
                    "sent": "We cannot actually sorry also tells us we cannot go below zero.",
                    "label": 0
                },
                {
                    "sent": "OK, So what happens is that we.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not going and then we clip and then we end up doing going Theta only in the X direction.",
                    "label": 0
                },
                {
                    "sent": "So mathematically it amounts to and again in N dimensions there is V1 through VN.",
                    "label": 0
                },
                {
                    "sent": "Trying to deduct data, but if we hit zero we stop at 0.",
                    "label": 0
                },
                {
                    "sent": "So this is actually the basic L1 projection.",
                    "label": 0
                },
                {
                    "sent": "Very simple, although I didn't describe what data.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What happens if the point is in general orientation, not necessarily in the positive or tent, so we can deal with?",
                    "label": 0
                },
                {
                    "sent": "You know, with this general orientation just by reduction, we're going to take the mirror image of the point, perform the projection operation, get the end result, and then do the inverse of the of the mirroring.",
                    "label": 0
                },
                {
                    "sent": "In mathematic Lee, it's not that.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Complicated, it just says, OK, let's take the sign.",
                    "label": 0
                },
                {
                    "sent": "Remember it perform the projection of the absolute value of the vector and restore this sign.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is actually the entire L1 projection, but there is 1 missing ingredient which is Theta.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So let me give you yet another sort of view which is more still cartoonish but more algebraic.",
                    "label": 0
                },
                {
                    "sent": "Your medical Jabra go metric.",
                    "label": 0
                },
                {
                    "sent": "Suppose now for the sake of the example we have 7 dimensions OK, and we have a vector in.",
                    "label": 0
                },
                {
                    "sent": "General orientation, so if someone tells me what state.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is I can simply say OK, here's a threshold, Theta everything above the threshold is going.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Whoops, I missed it.",
                    "label": 0
                },
                {
                    "sent": "Everything above the threshold is going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to deduct data everything below the threshold is going to be 0.",
                    "label": 0
                }
            ]
        }
    }
}