{
    "id": "okvwj4oi6ut5wwkiwxe4ibmlz72zpdtf",
    "title": "Structured Prediction for Natural Language Processing",
    "info": {
        "author": [
            "Noah Smith, Language Technologies Institute, Carnegie Mellon University"
        ],
        "published": "Sept. 17, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml09_smith_spn/",
    "segmentation": [
        [
            "I people may still trickle in, but this is going to take.",
            "I think most of the time we've got, so I want to go ahead and start close to the starting time.",
            "So thanks for coming.",
            "I'm Noah Smith from Carnegie Mellon and this is a tutorial about structured prediction for natural language processing.",
            "I should say upfront that this is a condensed version of a course that I teach at Carnegie Mellon called Language and Statistics 2.",
            "There's at least one veteran in the audience.",
            "And there's quite a lot of material to talk about, so I've been somewhat selective, but hopefully this will give you a nice picture of the connection between machine learning and linguistic analysis in natural language processing."
        ],
        [
            "So the relationship between natural language processing and machine learning goes back a very long time.",
            "Some of the earliest people to think about computational models of language.",
            "We're thinking we're using statistical statistical methods for analyzing text, and some of the first people to start thinking about applying statistics to real world problems.",
            "Looked at text as one source of examples, and so if you want to sum things up today, you might say that natural language processing really loves machine learning, because it gives us elegant, well formed.",
            "Solutions to solve the kinds of problems we need to solve, and Meanwhile machine learning loves to tout NLP as one of it.",
            "One of the great application areas.",
            "So every year there are papers at ICM, Ellen Nips about natural language processing.",
            "It's become one of the really hot topics, but I would put forward that we are starting to see some signs of a strain between the two.",
            "So here are some of the agreement."
        ],
        [
            "Is from from the natural language processing side when when natural language processing people like me go to machine learning conferences, we often get a little bit frustrated.",
            "Machine learning algorithms don't always scale well to all the data we've got for language processing.",
            "Sometimes scalability is just not not the focus in machine learning.",
            "A second problem is that the models that are often put forward in machine learning are very simplistic and make very strange to a computational linguists point of view independence assumptions.",
            "So machine learning is to stop assuming things and finally natural language processing does not for the most part.",
            "I hopefully will convince you of this by the end of the tutorial.",
            "Reduce to a problem of classification or a collection of classification problems.",
            "We need structured classification or structured prediction, so the fixation maybe of machine learning on classification and regression doesn't sit well with natural language process."
        ],
        [
            "Types now.",
            "Meanwhile, I also consider myself a machine learning person, so when I go to NLP conferences, here are some of the grievances that I might put forward so.",
            "One unfortunate thing about natural language is that the data never give you everything you really want to model.",
            "The data are almost always going to be incomplete, so that means supervised learning kind of breaks down.",
            "So why can't you just tell me what you want?",
            "Also, the evaluation criteria in natural language processing, the final loss function that you're going to use to evaluate how well your technique works is always changing.",
            "People don't even agree in natural language processing on how we should evaluate our systems.",
            "And finally, I don't know people who are trained in machine learning may often be puzzled to hear all the talk about this thing called linguistics.",
            "At natural language processing conferences, what is this linguistics anyway?",
            "And why do we need it?",
            "And why are you always talking about it?",
            "And why is it called computational linguistics anyway?",
            "So hopefully I'll clear up a little bit of this.",
            "So my claim is that the marriage."
        ],
        [
            "We not only processing machine, learning can survive, but both parties need to learn to understand each other, and I think that's happening as we see more and more overlap between the two communities.",
            "And this tutorial is meant to be marriage counseling for natural language processing and machine learning.",
            "So, given that that actually I should ask how many people here when you were never supposed to ask a crowd to tell you something negative about themselves, you have to ask them to tell you something positive.",
            "So who here would count themselves an expert in natural language processing?",
            "Excellent, and who would count themselves an expert in machine learning and maybe even like structured prediction in particular.",
            "OK, great so hopefully apart from the one or two people who raise their hands both times, hopefully everyone will learn something.",
            "I won't be offended if you leave now."
        ],
        [
            "OK, so this is.",
            "This is sort of a framework outline of the talk.",
            "I'm going to start out by talking about some linguistic analysis problems that are examples of structured prediction, and I'm not going to talk about how we solve them.",
            "I'm just going to try and state what the problems are and why they are important, so that's going to be the part where we get closest to linguistics.",
            "The second, the second part after the first break, I'll talk about decoding or specific algorithmic tools that are frequently used to make structured predictions, and there I'm not going to tell you how we get the models.",
            "I'm just going to talk about the prediction problems.",
            "And then the third part, sorry, let's see.",
            "did I break this down properly?",
            "Actually, no, that's not.",
            "After the break, the 1st two big bullets are before the 1st break.",
            "The the second section of the of the tutorial will be about supervised learning.",
            "When you've got complete data, a case that we do see in NLP but not as often as we'd like, and then the third part of the talk, which will be the shortest after the second break, will cover the incomplete data case or unsupervised learning.",
            "What I'm not going to do is tell you what the best techniques are to use for doing that."
        ],
        [
            "Processing I'm not going to show you any experimental comparisons at all.",
            "You can see the literature for those.",
            "I'm not going to go into too much detail on specific datasets.",
            "How to find the particular parsing data set that you want to use to test out your idea, or how annotation conventions work, or linguistic theory.",
            "There's plenty of that in the references, but I am going to hopefully give you some insight as to why people care about those things, and I'm not going to talk too much about implementation.",
            "I'm going to try and keep this at a fairly high level of abstractions, so you're probably not going to be able to leave this tutorial.",
            "Able to go implement a state of the art natural language processing system.",
            "Unfortunately, I can't teach you that in 2 1/2 hours."
        ],
        [
            "OK, so hopefully at the end you'll be more excited about natural language processing.",
            "An inspired to go read more and consider working on problems in NLP and finding new ones.",
            "Hopefully you'll understand structured prediction better, at least the the applications view of it, and maybe have a broader view of what structure prediction can encompass.",
            "An hopefully you'll start to raise your expectations about what structured ML should be able to do.",
            "OK, So what is structured prediction?"
        ],
        [
            "There is no conventional definition.",
            "I think Hilda may, in his thesis proposed a couple of a couple of formal statements.",
            "Basically, I'm going to say you know it when you see it.",
            "Basically, the idea is that your output spaces somehow combinatorially very large and highly dependent on the input space.",
            "So as your input gets larger, your output can get larger and as your input gets larger linearly, your output space can get bigger exponentially, and there's this possible thing we can quibble about whether the loss function has to decompose into parts, and I'm going to say that it might or might not.",
            "Sorry whether the loss function is allowed to decompose into parts, and I'm going to say it is allowed to, because often LP they do.",
            "Well, we'll come back to this point a little later."
        ],
        [
            "OK, so now we get into the meet.",
            "I'm going to start talking about representations of problems in natural language processing."
        ],
        [
            "Don't think about bags of words.",
            "If you're going to do natural language processing, one of the first things you have to accept is that natural language is more interesting than just a histogram.",
            "So language or a document or piece of text has some implicit structure.",
            "To put it another way, the words in the document are not IID, and so you know enough to put it one way this might.",
            "This might modify some of my linguist friends.",
            "I could say that linguistic linguistics offers many different theories about the relationships among pieces of text, and many different models for explaining how those words are not IID, and we're going to see some hints of some of those.",
            "So I'm stealing this."
        ],
        [
            "My colleague Dan Klein at Berkeley, although he uses it for a different purpose, this is what you see in a document, and in fact as soon as you start."
        ],
        [
            "Thing to do?",
            "Anything interesting with text like automated understanding or translation or summarization or basically any kind of deeper processing for an application.",
            "You realize that you're going to have to model something deeper that you actually can't see underneath, and that can be about as big as you want, and it might even extend below the."
        ],
        [
            "Bottom of the slide.",
            "OK, I'm going to stick with the convention that script X is the set of possible inputs.",
            "Most frequently this is going to be Sigma star for some vocabulary Sigma an Y is going to be a set of possible outputs and I'm going to show you a bunch of different examples of why it could be for different problems."
        ],
        [
            "So this is the first one if you.",
            "If you're still puzzling over why, I suggested that we shouldn't think about bags of words.",
            "Chinese should convince you that bags of words are sort of inadequate.",
            "If you want to language processing.",
            "So there may be people in the room who can read the second line here, which is in Chinese it doesn't.",
            "With Chinese is not written conventionally with white space.",
            "If you ask a Chinese person or a Chinese speaking person to find boundaries between the words.",
            "I asked a student of mine to do this, and he did I don't know if he lied to me or not.",
            "But this is what he tells me.",
            "This is where he tells me the word boundaries are.",
            "People are laughing.",
            "So maybe he.",
            "Maybe he completely played a trick on New York.",
            "It seems reasonable.",
            "OK, so so if you're going to do NLP with Chinese, you need to figure out where the word boundaries are.",
            "So this is called word segmentation.",
            "And so we can imagine that what it what it requires is taking a."
        ],
        [
            "Sequence of characters and mapping it to a sequence of words, and this is mostly trivial if you want to work with English, write English gives us very strong clues.",
            "The conventions of English writing give us very strong clues as to where the word boundaries are.",
            "There might be some little things you'd want to do.",
            "Tokenize off punctuation or something like that, but for the most part this is not really a problem we deal with in English, although there is this problem called sentence segmentation that nobody talks about anymore but 10 or 15 years ago it was.",
            "It was an important problem.",
            "Look at a piece of text and find the sentence boundaries.",
            "You think periods full stops are good sign, but they're not a perfect sign.",
            "We use it after Mr and there are places where sentence is end without a period.",
            "OK, so so now we've gotten a little bit past words."
        ],
        [
            "There's still a problem even once you found all the words in a document and identified what counts as a word.",
            "There are a lot of them an awful lot of them, and they tend to follow their sort of the classic case of zipfian distributions.",
            "So one of the first things that most NLP systems try to do is."
        ],
        [
            "Words more manageable.",
            "So what I have here is I have histogram of the words in a collection of really boring financial reports from 2001 to 2005.",
            "I think it's about 10,000 documents and So what I've done is I've just counted the number of times each word occurs and then sorted based on the frequency.",
            "So this is the most frequent word and it occurs 8 * 10 to 6 times and you can see they fall off and those are the first 10 words and if we look at the first 100 words we."
        ],
        [
            "With this beautiful long tail distribution, and if we look at the first 1000 words."
        ],
        [
            "It keeps going and if we look at the next slide it takes awhile to load because.",
            "Why does it take awhile to load?",
            "It's taking awhile to load because it's a really big graphic 'cause there are an awful lot of words in this document and this is sort of a device to."
        ],
        [
            "Just need to slow down."
        ],
        [
            "There we go, right?",
            "There are more than 120,000 different unique words in this collection of documents, most of which occur once.",
            "Right, so you know if you wanted to build, say, a probability distribution over those words, or a collection of distributions over those words, you'd be really hard pressed because this is just very high dimensional so."
        ],
        [
            "What we'd like to do is project vocabulary into something simpler, and so people have a lot of ways of doing this.",
            "This isn't really deeper, interesting, but just to give some examples, we have a sentence at the top.",
            "On the surface, you might tokenize and split off some of the.",
            "Some of the punctuation, maybe down case another thing people do, sometimes not so much in NLP, is to split off certain common endings of words.",
            "So, for example, important might map to import, which actually seems wrong.",
            "If we were trying to understand this sentence, these important and import have two very different meanings, even if angle and angle without an E seem like that like that might be plausible.",
            "Another thing people do is they limit eyes and reduce words to their simplest forms.",
            "We had an example.",
            "Cats becomes cat.",
            "This is mostly solved in short Perl scripts.",
            "This isn't really difficult."
        ],
        [
            "Problem people don't write papers about these things anymore, but they are instances of structured prediction.",
            "It's just that we make the prediction using rules using very simple methods."
        ],
        [
            "But there are other ways of reducing the vocabulary, and some of them are are much more challenging.",
            "So the next thing to talk about is a sort of extreme form of tokenization where we map we map words into word classes or parts of speech.",
            "You've probably seen this problem before.",
            "It's kind of a Canonical natural language processing problem, so here I've taken each word in the sentence and assigned it a blue part of speech.",
            "So, determiners, nouns, prepositions, these are.",
            "These are the parts of speech as as decided by somebody.",
            "I think these go back to Aristotle or something in the beginning, but you know you can.",
            "You can kind of agree on a convention that you may be able to decide on a way to map words to their parts of speech.",
            "And this is this is done almost, you know."
        ],
        [
            "Personally, now in NLP and you might think you know why is this what's challenging here the the problem, of course, is that many words are ambiguous, and so you could independently classify each word on its own based on based on what you've seen before.",
            "And you can get close to 90% accuracy on this problem if you've got enough data in terms of the fraction of words you correctly tag.",
            "But context actually is really important.",
            "So for example, the word leaves can be a plural noun or singular verb, but it's probably a verb if it's followed by noun.",
            "Similarly, the word bear is community can be a noun or verb, but if it has a determiner in front of it, you can bet it's probably a noun.",
            "So these sort of local interactions among the words tend to allow us to do a lot better if we can model them.",
            "So the best performance on part of speech tagging.",
            "I think these days, is around 97%, and these are based on structured models where we make joint decisions about all the words at the same."
        ],
        [
            "OK, so this is.",
            "This is sort of the simplest example where the current NLP paradigm took off and continues to be the dominant way of doing things.",
            "So the general framework here is that you get some people who understand part of speech tagging or whatever your linguistic structure is.",
            "You get them to label some text.",
            "You probably pay them a little bit of money to do that, and then you go perform supervised learning.",
            "And that could be some kind of structured prediction model, as I would advocate here, but you should.",
            "You should be a little bit wary of this.",
            "These part of speech, you know if you find the data set that gives you parts of speech, you should remember that this didn't come from God, right?",
            "This came from somebody who had some insight about what parts of speech were useful for that language, and then he paid some guys to sit down and write down what the parts of speech were in some text.",
            "This is this is a choice somebody somebody had some thoughts and you might you know if you knew the language you might disagree with them.",
            "Personally I disagree with some of the conventions that were chosen in the current standard datasets for part of speech.",
            "Further, even once you've laid down what the conventions are, people who are annotating don't always agree with each other, so annotation is actually a major effort that usually requires you know bringing in lots of different people, making sure they agree, going through various phases to measure Inter annotator agreement, building up a case that we've got is."
        ],
        [
            "Really real OK?",
            "Let's come back to words again.",
            "So for a long time in LP focused mainly mainly on English, but of course not everyone speaks English and we'd like to do natural language processing for other languages.",
            "So here's a sentence in Korean and the graph at the bottom is a little lattice or finite state network that shows you the different possible analysis of this Korean team speak Korean.",
            "No, OK, I'm safe.",
            "Each of these, so each of these words, many of the words in that in that sequence can be analyzed more than one way.",
            "So I think if you multiply this out, there's something like 40 different analysis of the sentence, depending on how you break the words up and how you tag them.",
            "So this is sort of showing you different ways of segmenting and tagging the morphemes in this sentence."
        ],
        [
            "OK, so this may be a surprise if you only speak English, but morphological disambiguation is really kind of crucial for NLP in other languages, so again, this is just a sequence to sequence mapping problem.",
            "You get a sequence of surface words or characters maybe, and you want to want to break them into morphemes and possibly tag them as well.",
            "This is one of my favorite examples.",
            "Does anyone know?"
        ],
        [
            "Language this is.",
            "This is a word in Turkish.",
            "It's very long and it means something like behaving as if you were among those whom we could not civilize.",
            "It's a single word.",
            "And if you wanted to, you know you could say this is kind of weird.",
            "This is like some weird fringe effect, but there are more than 60 million people in the world who speak this language.",
            "And if we wanted to provide intelligent software, they could manage their language for them.",
            "We would have to break that word into its parts and figure out what at least some of the parts mean, probably.",
            "Right?",
            "What makes things even more nasty with Turkish is that often the vowels change throughout the sequence to match each other.",
            "There's this thing called vowel harmony, so the problem gets even hairier.",
            "It's like it's almost as if they've conspired to make their language hard.",
            "OK."
        ],
        [
            "So OK, so I'm going to move up a little bit from words.",
            "A lot of people have built their careers on just handling the words in different languages, but we're going to keep going.",
            "So one thing that's often useful is to find specific kinds of substrings in text.",
            "So I'm going to call these chunks.",
            "So, for example, if we go back to our cat cat sentence, this sentence has two noun phrases in it in two prepositional phrases, which I've marked, or two based on phrases I should say, and so we can.",
            "We can use structured prediction to find these important segments or interesting segments or chunks using what?",
            "Well, the representation that's usually used is to turn this into a sequence labeling problem where you label the beginning and interior and exterior points for each chunk."
        ],
        [
            "So this is just another string transduction problem.",
            "You have a sentence coming in and what you're going to send out, or these ayobi labels possibly."
        ],
        [
            "Augmented with chunks.",
            "So this means that this is the beginning of a noun phrase the interior, and now we're starting a prepositional phrase, the interior, the interior, and now we're outside.",
            "And so the encoding is really not that important, it's just there."
        ],
        [
            "Widely used, it's worth seeing, so this is this kind of this technique is used for base noun phrase chunking, also for other kinds of shallow parsing.",
            "For sort of other types of phrases, and most commonly named entity recognition."
        ],
        [
            "I think that's what I want to show, so here's a sentence.",
            "From the news.",
            "Not too long ago and."
        ],
        [
            "Of the named entities an We might label those same entities, so the first one is a place, the second one is a person and the third one.",
            "What is RoboCop?",
            "I guess RoboCop is a fantastical person.",
            "I would call that miscellaneous but depends on your convention.",
            "Maybe RoboCop is a person.",
            "OK, so if you take this idea of finding interesting substrings too."
        ],
        [
            "To its extreme.",
            "An imagine that we want to find all the interesting substrings, and we believe that they have a hierarchical structure.",
            "Then you get the parsing problem.",
            "OK, and this is this is a problem that has dominated and continues to dominate and LP it's one of the main areas.",
            "If you go to a computational linguistics conference, there will be tons of sessions on parsing and this is sort of parsing.",
            "In a nutshell, the idea is to find the compositional phrases from the whole sentence.",
            "The S up at the top all the way down to the words.",
            "And if you're training computer science, you've probably seen something like this in a compilers class.",
            "Here we're just doing it for natural language.",
            "OK, so this S means sentence because it's a sentence.",
            "NP means noun phrase, just like we saw before.",
            "But notice that noun phrases can be recursive.",
            "You can embed one non phrase in another.",
            "You can embed.",
            "Sentence is another sentence is so this this kind of structure is deeply recursive and it should be very clear to you that if I'm permitting all kinds of words to play all kinds of roles, then there may be many, many ways to parse a sentence in practice."
        ],
        [
            "So the problem is to find the compositional phrases from the whole sentence all the way down to the words and often this is done using some kind of context free grammar, not always."
        ],
        [
            "So this is maybe a slight tangent, but when I teach undergraduates NLP, particularly the ones who have taken programming language classes, programming language classes, they often think Oh well, you know I can use.",
            "I can use standard tools for building compilers, and I can parse natural language by by parsing it with Lex and yacc, and so forth.",
            "And of course this completely fails and This is why the problem is that natural language is full of ambiguity.",
            "There's ambiguity at every level.",
            "Much of it is uninteresting.",
            "It's just a failing of our models to be able to distinguish what can happen and what can happen.",
            "But because because our models are not perfect, and because natural language really is ambiguous even to people who people who can understand it.",
            "This is why natural language processing.",
            "This is why I'm giving the talk here at I smell another programming Language Languages conference, so this is this is my favorite example of an ambiguous sentence.",
            "This is a headline a little hope given brain damaged woman and so."
        ],
        [
            "The the the head of the story was actually about Terry Schivo, and so it was really the sober story about, you know, the doctor told her family that was bad news.",
            "She wasn't likely to recover, but if you have a sick mind."
        ],
        [
            "Then you may be able to.",
            "Come up with a different reading of the same sentence.",
            "Little hope, IE.",
            "For us given brain damaged woman that was chosen to be on the campaign.",
            "I'm not getting any laughs from this crowd."
        ],
        [
            "OK, this is this is the really sick and twisted reading.",
            "Little Hope was given a brain damaged woman.",
            "Little hope given brain damaged woman.",
            "It was a very strange Christmas."
        ],
        [
            "And then there's there's this last one which I didn't see myself, but it incredibly twisted individual pointed out, if only little hope had used her gift for good rather than evil, we gave little hope of brain.",
            "We give her brain, and she went in damage to woman.",
            "Little help given brain damaged woman, past tense verb."
        ],
        [
            "OK so I should you know I should point out this is a newspaper headline.",
            "This is an actual headline from the New York Times, about six years ago.",
            "Headlines are a little bit more easy to create these examples from because they're so telegraphic and lots of words are left out.",
            "But trust me, the problem.",
            "The problem continues to persist even in longer in longer sentences, in ways that you would never process because you're not sick and twisted.",
            "But computer program that can find every possible parts of something can easily be LED astray."
        ],
        [
            "OK, so there's another convention for parsing that has caught on quite a lot recently that I think deserves mention called dependency parsing, and it's really not fundamentally different.",
            "You can show that you can move back and forth between dependency and compositional phrase structures.",
            "They're not identical, but they capture a lot of the same ideas.",
            "The idea is that every word has a parent to which it serves as an argument.",
            "So if you if you look at this and kind of squint and look at the word is, which is sort of the central.",
            "Bit of a sentence that's equating two things.",
            "The two arguments of is are over here, angle and over here clue angle is a clue that summarizes the sentence fairly well.",
            "Those are the most important pieces of the sentence, and then everything else is sort of modifying or serving as an argument to something else.",
            "And so dependencies syntax focuses on these relationships.",
            "You might label the arcs, you can do a lot more fancy things, but this is sort of the bare bones version."
        ],
        [
            "So the idea is that your structure that you need to predict is a mapping from each word to its set of children.",
            "And there's there's sort of another version of dependency parsing."
        ],
        [
            "Goes a step farther, which is kind of interesting because it was only recently pointed out.",
            "If if you limit yourself to trees where the edges all lay on one side of the of the words and don't cross each other, that's called.",
            "Those are called projective trees.",
            "I can give you a more formal definition, but that's sort of the simplest way to say."
        ],
        [
            "At the graph, the graph is restricted to be on one side and it's completely planar.",
            "The edges never cross, and this actually works very well for English.",
            "You can explain most of English this way, but.",
            "Uh."
        ],
        [
            "Occasionally we really want a crossing arc, so this is an example of a non projective Trea talk is scheduled on cats ears today.",
            "Perfectly good English.",
            "Nobody's going to Jack because this isn't a linguistics talk.",
            "On Cats, ears is attached.",
            "That phrase is attached through on to talk, causing a crossing link or non projective arc.",
            "So non productivity seems to be much more important in some languages than others.",
            "Choosing not to be not to use it, not to model it is perfectly OK, but you might sacrifice some accuracy in doing so.",
            "OK."
        ],
        [
            "So what we've been doing, sort of progressively as I've shown more and more of these problems, is we started at the top and talked about words which you can mostly see.",
            "Now you should be skeptical about.",
            "Of course you can't completely see them, but mostly, and we just moved down.",
            "We've been moving down to deeper and deeper structure, so now I'm going to go a little."
        ],
        [
            "A bit lower.",
            "I'm going to show a picture.",
            "My version of what's called the Linguistic pipeline and what I want you to be aware of is that as we go deeper and deeper and try to analyze more and more of the structure in language, we're getting, we're getting farther and farther away from the particulars of the language itself and more into physical and cultural and non linguistic things.",
            "That language is really about."
        ],
        [
            "At.",
            "OK, so this is.",
            "This is sort of a breakdown, maybe of the field of linguistics, plus plus a little extra.",
            "So up at the top, just imagine the iceberg superimposed.",
            "I didn't know how to do that in Latex, but imagine the iceberg superimposed over top of this up at the top we have the the acoustics that the phonetics which are the physical properties of speech, the sound waves that are moving from my mouth into your ear, or the orthography.",
            "The units of writing that I put down a page or in a slide that you can read and then we start getting to things that are harder to see and we have to reason about more carefully.",
            "Like the units of sound, the structure of words, the structure of sentences, morphology and syntax, or what I've been talking about mostly so far, and then we get down into this thing called meaning.",
            "What does?",
            "What does a sequence of words actually mean?",
            "What is it referring to in the world?",
            "What is it?",
            "What is its literal meaning, or what are the acts of communication which are intended?",
            "When I say can you pass the salt?",
            "I'm not asking you literally a question about whether you're capable of passing the salt, right?",
            "I want you to do something, and I'm conveying it to you in a polite way that you're not meant to interpret literally.",
            "And further, if we go down, there's discourse, which is, I say something.",
            "You say something, or in this case you sit there and stare at me awkwardly, because this is a lecture scenario.",
            "There you can parse discourse when it's just one person speaking as well as we move down, we're getting farther and farther from the things that are easy to observe.",
            "OK, so I'm going to go a little bit farther and you're going to start to see everything come apart at the seams."
        ],
        [
            "So the problem of meaning would be to convert a sentence into some kind of Canonical meaning representation language.",
            "So if I get Robin swam across the River and delivered the message, I might parse this into two sort of predicates likes women delivered.",
            "There were two events, one in which somebody was swimming who was swimming.",
            "Oh, Robin was swimming, she's the agent, and she did.",
            "She swim, swim through.",
            "She swam through the River.",
            "That's the medium, and I'm sort of making this up as I go.",
            "The agent is a technical term, but you can sort of assign these semantic roles to the arguments of each predicate.",
            "You could convert this into some kind of 1st order logic.",
            "I took a stab at it.",
            "You might disagree with exactly how to represent this in first order logic.",
            "There are a lot of things in natural language that you can represent this way, but you know nobody quite agrees and nobody feels like this.",
            "These representations are quite everything we need to represent the meaning of."
        ],
        [
            "Of language, so I'm going to say for script why there's no consensus yet.",
            "Script X is a sentence coming in finding the semantic structure.",
            "We don't know.",
            "There are a lot of things on the table.",
            "Semantic roles are one that's getting a lot of attention this year.",
            "It was part of the Donald Connell shared task at the Natural Language Learning Conference.",
            "1st Order logic expressions have always been around.",
            "They were there was a little bit of a resurgence there in a few recent papers out of MIT.",
            "And then there are other things that people have proposed for very specific problems so that.",
            "Often what you'll have is if you have a particular problem you want to solve.",
            "People will design a problem specific meaning language.",
            "So for example, I had a student who is interested in automating understanding of cooking recipes, so it took us about 3 months and we sat down and we designed a a meaning language for cooking recipes, which is a fairly low dimensional space.",
            "You don't have to talk about everything in the world, only a few things in the kitchen, and we came up with some primitives and we were able to do it without a whole lot of work.",
            "But it took three months and then we had to annotate data and people don't always agree about what a recipe means and and so on.",
            "So you can see why this is not getting quite as much attention.",
            "It's hard."
        ],
        [
            "Maybe a little less hard is figuring out which real world entities are being mentioned in text and where.",
            "So here's a here's a piece of text from the news recently.",
            "You can guess who it's about.",
            "Might be a little bit more helpful."
        ],
        [
            "To understand it, if I highlight the things that are being referred to and color them the same if they're referring to the same entity.",
            "So there are about four entities being mentioned here, I think.",
            "Who is it about?",
            "You springport, that's right, it's about Sotomayor.",
            "I.",
            "Right, so the key here is to figure out which this is called.",
            "Part of this is called the coreference problem, but it's also sort of.",
            "You also have to detect all of the referring expressions, right?",
            "And so you can see this is sort of a chunking task, but then there's this higher level structure where you link you link things denoted here by color.",
            "OK, so detecting entities and deciding which ones."
        ],
        [
            "Co.",
            "Refer and then depending on what type of text you have, you might then want to link those Co referring expressions back into some kind of ontology or representation of the world.",
            "So where this often comes up for examples in biomedical literature, suppose you have an ontology of all the genes known in the human genome, or some of them.",
            "Even.",
            "You might want to look at scientific articles and link back the references to those genes, no matter how they look in the text back into the ontology.",
            "OK, so we're going to run out of time if I keep going, but."
        ],
        [
            "There's a whole another dimension to this that's worth bringing up, because there's another sort of data which people have been kind of fascinated by since since maybe 20 years ago, which are parallel, or comparable corpora.",
            "Mostly parallel corpora, so parallel corpus is a collection of texts in two or more languages, which are translations of each other.",
            "These these come about as a natural byproduct of the working of international organizations like the UN, and certain news agencies.",
            "Also the European Parliament, and they're really nice because they show you.",
            "How the same thing is expressed in two different languages?",
            "So one idea that's been kind of floating around for a long time is that you can.",
            "You can build better models of language if you can kind of triangulate.",
            "If you know how the same idea is expressed in two languages, that gives you more insight into what the idea might be and how it might look.",
            "So there's a ton of work on things like aligning the sentence as well.",
            "That was kind of easy.",
            "We kind of solve that about 10 years ago, but aligning the words between these two sentences.",
            "The words that mean the same thing or phrases or aligning their parse trees or finding finding the underlying bilingual dictionary.",
            "Or or and so on and so on.",
            "And the biggest application of all this, of course, is translation.",
            "Being able to take one of the sides as input and produce the other's output.",
            "So you can think of that itself as a structured prediction prob."
        ],
        [
            "OK, so that was a very gross smattering of some of the problems people are thinking about in natural language processing and some that they thought about awhile back and sort of stopped thinking about a lot of these things.",
            "Not everyone agrees that all of these problems are important.",
            "So for example, you probably can't.",
            "You're probably going to have a hard time publishing a part of speech, tagging paper about English nowadays, because it's mostly solved and it's not clear that you know that any advance is really going to make a difference for any real application.",
            "People who focus more on applications like machine translation or summarization are often sceptical about any new kind of linguistic analysis and may ask how is this going to actually help my application does.",
            "Does this particular step to serve its own intrinsic evaluation, so one one that's gotten a lot of attention is word alignment.",
            "You've got two sides of a parallel text you can.",
            "There are many algorithms for trying to figure out which words on each side of the text line up with which other words that mean the same thing and then.",
            "People started evaluating that on its own, and so people who work on machine translation kind of scratched their heads and said, well, your word alignment got better.",
            "But did my MC system get better?",
            "Right, so this is this kind of debate always is going on."
        ],
        [
            "Should we even evaluate parsing on its own anymore?",
            "Is a question that's kind of a controversial question, but it's one that comes up OK, so.",
            "Um?",
            "So right everything in natural language processing comes with controversy.",
            "I here's.",
            "Here's a suggestion that there are some problems that haven't really been viewed as structured prediction directly, but deserve to maybe so.",
            "One is to find the entire predicate argument structure of a sentence, a sort of rough approximation to the semantics along the lines of the 1st order models.",
            "I was suggesting earlier for the whole sentence that hasn't really been done.",
            "Tasks that require generation of text where text is the output answering a question translating.",
            "We talked about that.",
            "Or you get it, you get some text in and you want to simplify it.",
            "Or maybe just clean it up, fix its grammar, fix it spelling.",
            "People haven't really used structured prediction models for these things, yes.",
            "That wasn't generation, was it?",
            "Creative content logical representation assembly.",
            "Sure, but it wasn't a.",
            "You're right, what they were mapping to was was sort of a first order logic that they used to transform into SQL queries.",
            "Right, which they could then pose to the database, so that's fair.",
            "It hasn't been done.",
            "It hasn't been done.",
            "Open domain though.",
            "That was a very restricted domain.",
            "Yeah, so I should clarify.",
            "Oh cool, great looking forward.",
            "Is not a solved problem, would you agree?",
            "OK, so another another problem that people have been trying to work on in natural language processing but haven't really used structured prediction for yet is learning about linguistic types.",
            "So we often talk about tokens words in text.",
            "For example in the sentence, Barack Obama likes to talk about Barack Obama.",
            "There are two instances of the token Obama, both of which instantiate the type Obama getting a little bit metaphysical here.",
            "But the question is, are there things to say about the word Obama on its own?",
            "Yeah, absolutely.",
            "Refers to a person.",
            "It's a name, it's Kenyon.",
            "There are lots of things we can say about words at the type level, and so we often call these things.",
            "You know, these collections of knowledge.",
            "Lexicons are ontologies, and people would like to build those from data as well.",
            "And you can view that as a type of structured prediction, but nobody really has.",
            "Maybe maybe they have started, but I would say that we haven't really figured out how to think about that discourse.",
            "Structured dialogue, structure, learning world knowledge from text.",
            "These are all problems people are thinking about, but structured prediction.",
            "Isn't really that old are using right now for them, but it might be.",
            "I'm going to skip that.",
            "Take a quick pause.",
            "Any questions at this point.",
            "This is the quietest group."
        ],
        [
            "Of 30 people I've ever spoken to.",
            "OK, so I'm going to talk about decoding."
        ],
        [
            "OK, here's my notation.",
            "I'm going to expand it a little bit.",
            "Script X is going to be a set of possible inputs.",
            "Often it's Sigma star like before, and I'm going to use big bold X to mean a random variable over the inputs, and I'm going to use script.",
            "Why again, to be the set of possible outputs?",
            "And I'm going to use big bold.",
            "Why to be the random variable over the outputs and H is going to be a prediction function that Maps X is to wise.",
            "A decoder is simply an implementation of X.",
            "Sorry of H that takes an accent."
        ],
        [
            "And returns you why?",
            "And the reason it's called decoding this may be puzzling, so I'm going to give us a very quick history lesson.",
            "So nowadays in NLP we like to talk about structured prediction.",
            "It's it's, you know, the hot metaphor that we love to use to describe everything we do.",
            "At least I am in this talk.",
            "But before that, maybe 10 years ago, the thing everybody used was source channel models.",
            "Everything was seen as a source channel model and it's not a bad metaphor.",
            "The idea is that you want to model jointly this.",
            "These two random variables X&Y.",
            "X happens to be your input.",
            "Why happens to be your output and so the way you're going to do that is you're going to model first the distribution of Y, and then given why you're going to generate X.",
            "So it's a little generative model, and we haven't really done anything fancy here.",
            "It's just the just the chain rule, so there's the source model generated by the channel model generates X given Y, and then our job is to decode so.",
            "We want to find the X that Max that is most likely given.",
            "Sorry we want to find the why that is most likely given X and we're going to call that H of X and so we use Bayes rule.",
            "We ignored the denominator because the marginal is constant and this is our decoder and so decoding usually refers to this sort of balancing of the source model and the channel model and this metaphor you can get.",
            "You can get so much out of it.",
            "The classic example that.",
            "Kind of kicked off its use in NLP I think apart from apart from speech recognition where it's widespread was this great story told about 60 years ago by Warren Weaver about translation.",
            "So this was back when the goal of the goal of all of natural language processing in the United States, of course, was to spy on the Russians.",
            "And so the story was that they're speaking Russian.",
            "That's what you hear.",
            "That's the observable, that's the X.",
            "But in fact, those pesky Russians they're actually thinking in English, of course, because that's what everybody thinks in, of course.",
            "So what happened is that they thought this thought in Russia.",
            "This devious thought.",
            "Of course, in Russian, and then it passed through their broken mechanism.",
            "I'm sorry getting myself confused.",
            "They have this devious thought in English, because that's the language of thought.",
            "And then that pass through this garbled filtering came out as Russian.",
            "And so our job is to decode it and recover with the original message was as it as it existed before it got it got garbled.",
            "So why, of course, is the English the true?",
            "The true language underneath?",
            "OK, so I'm going to keep the term decode."
        ],
        [
            "Because people in NLP like it and it's sort of abstracts away.",
            "The problem of decoding from what it's often called inference, but that's a very loaded term in machine learning, so I'm not going to call it in France.",
            "I'm just going to decode.",
            "So generally what what you want to do.",
            "You know this this equation at the top sums up a lot of different cases of decoding, but generally it involves finding the Y that minimizes some expected risk, or some expected loss given some distribution over the true value of Y.",
            "So I'm going to use Ly XY to be.",
            "The loss.",
            "When you choose the cost that you pay, when you choose label, lowercase Y and the true value is that third argument given input X, so we're going to say that but true value is distributed according to some distribution, which we may or may not know, and we want to find the why that is least offensive under that distribution.",
            "And if we take the most common case where the loss is 01, meaning you lose one point if you get it wrong and you don't lose any points if you get it right 01 loss, then what this gives you back is just find the most likely why Unix.",
            "OK, that assumes that we have a probability model.",
            "Otherwise, given X is, which we don't always."
        ],
        [
            "There's a more.",
            "There's a more general framework that encompasses almost everything that people do in an LP, which is to use linear models.",
            "So I'm going to.",
            "I'm going to use G to denote a feature vector function that takes an X&Y and Maps them into some Euclidean space.",
            "Alright, this is probably familiar.",
            "You have to remember here that this is a mapping not just to the input.",
            "It's a mapping of both.",
            "Everything works better if you write it out this way, so you get an input and output and you map them into this this Euclidean space and then you take an inner product with a weight vector.",
            "Of the same dimensionality, of course, and then decoding involves finding the Y whose embedding paired with the given X as input has the highest score.",
            "Inner product score with W. Much of what we don't much of what we want to do can be mapped to exactly this problem.",
            "OK, so."
        ],
        [
            "So the simplest, the simplest kind of structure prediction is going to involve the specification of the input and output spaces.",
            "The feature vector representation G. We're going to need a decoding algorithm that can can find the argmax over Y, and then we're going to need a method for learning the parameters W and that last part will come after the break.",
            "OK, so.",
            "How do we get this to work there's?"
        ],
        [
            "Really no way to say, you know you.",
            "It's almost impossible to say to give some kind of generic recipe for building a decoder, and So what usually happens is that people build on work that's already been done, and use algorithms that already exists and show how they actually solve particular decoding problems.",
            "So what we what often is useful is if we can show if we can.",
            "If we can build our model such that G factors into local parts of the.",
            "The output structure so far.",
            "So so right?",
            "So another.",
            "Another way to put this is to say that different parts of Y are conditionally independent of each other given X or given local local arrangements of why.",
            "So the most extreme version of this.",
            "Imagine that we saw many problems where the input is a sequence sequence and the output is a sequence.",
            "Imagine that we're trying to model us do structured prediction over over sequences.",
            "So why is this sequence Y one through YN?",
            "Could do is.",
            "We could assume that this overall global function G breaks down into local functions, which depends just on a single Y at a time at a single position.",
            "And if we were to do that, it should be fairly clear that you can build a decoder that solves this problem very fast by making a decision about each Y intern irrespective of the others, they're completely independent.",
            "This choice of feature feature embedding by factoring things locali by choosing features that factor locally gives us a non structured classifier.",
            "As soon as we want feature functions that can look at two or more wise at a time, we've got a structured prediction problem and there's an interdependency.",
            "OK, so."
        ],
        [
            "Are these combinatorially optimization problems when they have a certain kind of optimal substructure can often be solved efficiently with dynamic programming?",
            "Natural language processing people love dynamic programming.",
            "OK, it is.",
            "It is the most commonly used to LAN.",
            "It solves a huge number of problems.",
            "Unfortunately, it also limits us, so at a high level, this is how I would describe it.",
            "If you can make an assumption like this that your G vector breaks down into a sum over local parts of Y.",
            "Then you can use dynamic programming.",
            "If the parts are local enough.",
            "OK, so the Viterbi algorithm is a great example.",
            "I'm going to try to make this a little more concrete."
        ],
        [
            "So our input is X and I'm going to augment X with this extra stop symbol at the end, just to make life a little easier.",
            "An R output is going to be Y and I'm going to augment it with the start symbol on the stock symbol at the beginning and end respectively, and so with with a hidden Markov model we can describe the joint distribution over X&Y using this product, which factors into parts at each time step, you first predict the Y, then you predict the X that it generates.",
            "So why is the state sequence an axis?",
            "The observation sequence and you can you can write down.",
            "It's relatively easy to derive a set of recursive equations that will solve the decoding problem.",
            "For an HMM, this is.",
            "This is one way of writing the Viterbi algorithm.",
            "You start out by scoring being in the start state at Timestep Zero with value one, and then you recursively find the score of being in each state at each timestep given given."
        ],
        [
            "The possibilities before, so equivalently, you can think of this as finding the best path through a graph where you weight each edge according to the transition and emission probability that are appropriate at that point.",
            "OK, so finding the best path through this what's often called the trellis from the green to the red State.",
            "That's the Viterbi algorithm.",
            "It's almost identical to Dijkstra's algorithm, depending on your implementation."
        ],
        [
            "So what's what's kind of interesting is that you can view this, of course, as a case of factored structured prediction where everything factors into these local parts.",
            "So all I've done is I've kind of changed notation and mapped the peas, the transition and emission probabilities in the HMM into feature functions which basically check there's probably 1 feature for each transition type and one feature for each emission type, and probably one for the stopping event and you're good to go.",
            "You've gotta Locali factored model and.",
            "You're summing over the parts, which are the different wise at each position, and you only have to look at the previous way."
        ],
        [
            "At a time and so we can actually generalize, Viterbi very easily.",
            "And here we're multiplying in the local.",
            "The local term for for our global score."
        ],
        [
            "OK, So what you should take away from this is that having specified which which algorithm I'm going to use for decoding, that sort of implies which features were local and which features were not local.",
            "So generalized Iturbi as I described it would actually permit me to augment my model with feature with many kinds of features beyond the transitions and emissions I could look at the words I could look at their spelling.",
            "I could look at their shape.",
            "Do they have capitals?",
            "Do they include numbers?",
            "Are they punctuation?",
            "Anything I want I can also do anything I want with the Jason output symbols.",
            "Are they the same or are they different?",
            "Not just what particularly are they?",
            "So that's kind of useful, but there are still some things that can't do.",
            "It can't look at three consecutive output symbols.",
            "For that I would need a different algorithm.",
            "It can't decide whether two instances of the same word that are separated by several words are going to be the same.",
            "That might be important if we were doing named entity recognition, right?",
            "If I see Washington at one point in the sentence and I want to label it a person, then I see it again.",
            "Having labeled it a person before might lead me to do the same thing later.",
            "That's the classic case of a non local feature for the verb with am counting the number of times you've seen, why?",
            "If you're doing part of speech tagging, you might prefer only to see one verb in a sentence.",
            "The Viterbi algorithm doesn't give you a straightforward way of keeping track of that.",
            "OK there."
        ],
        [
            "A lot of other problems that reduce the dynamic programming sequence alignment.",
            "Edit distance.",
            "You've seen many of these before many kinds of parsing, including standard phrase structure parsing and dependencies, but also other formalisms that linguists have studied over the years, and because so because so many problems hinge on dynamic programming.",
            "There's been a lot of research lately on generic dynamic programming methods, so generalized."
        ],
        [
            "Viewing dynamic programming as a generalization of logic programming and doing it with some earrings and coming up with semiring independent solvers.",
            "Connecting this to search on hypergraphs.",
            "Generalizing a star search on hypergraphs and coming up with approximate algorithms that handled directly the non non factoring features.",
            "All of these things are getting a lot of attention so the bottom line is that to a first approximation it's often helpful to try and think of your decoding algorithm as you're designing it as dynamic programming and see if there's a way to make it work.",
            "But that of course imposes certain restrictions on features."
        ],
        [
            "So I'm going to point out a couple of other interesting cases where you can accomplish decoding using other graph algorithms.",
            "Everything is in dynamic programming.",
            "We're coming to realize that in NLP fairly late, so one example."
        ],
        [
            "Alyssa is weighted bipartite matching, so suppose that my input is two different.",
            "Sentence is X&X prime and what I want as output is a matching of the words in the two sentences.",
            "So an example of this might be one sentence is English, the other is Chinese and I want to find a matching between the words.",
            "I want to figure out which words in English correspond to which in Chinese finding the best matching is is solvable if."
        ],
        [
            "The features on the two sequences and the matching breakdown into pairs.",
            "In the matching.",
            "In other words, every feature has to be associated with only one edge at a time.",
            "It can't know whether other edges are being included or not.",
            "And this can be solved in polytime with something called the Hungarian algorithm.",
            "This was pointed out by Malamed, and generalizations of this using network flow and other techniques on graphs have been have been used for generalizations of this word alignment problem."
        ],
        [
            "Another nice example is spanning trees.",
            "So if our input is the sentence X and our goal is to do non projective dependency parsing, then then you can show under certain assumptions about the features that finding the zero arborescence that has the highest score solves the decoding problem.",
            "So as an arborescence, is it directed spanning tree?",
            "We want one with X zero that special start symbol as the root.",
            "And so if again, if the features breakdown by edges and only look at a word in its parent and don't know anything about the other edges that are present or not present in the tree, we can solve this with something called the Tulou Edmonds algorithm, which is from the 60s.",
            "And there's a special case, a special trick that lets you solve this in quadratic time.",
            "People only realized this for for the dependency parsing problem in 2005, even though we've been working on dependency parsing for a few decades before that.",
            "This connection was kind of liberating and allowed us to start thinking about dependency parsing for a lot of other languages.",
            "Were non productivity is important."
        ],
        [
            "So you know to give some examples of the kinds of things that work when we want to use the spanning tree algorithm.",
            "Anything that looks at parents or children, their word classes, their lemmas, those are all fine contexts of either word, as long as you're not looking at the tree, how far apart the words are, you can include that.",
            "But when you can't look at is who are my siblings, what other words are attached to my parent or who is my grandparent?",
            "What are the what are the?",
            "What are my grandchildren?",
            "You can't look.",
            "These are called 2nd order features.",
            "You can't count the number of children you have.",
            "So if you're a verb that tends to take a subject and an object but not an indirect object, there's no way to enforce that.",
            "You only take one object on your right.",
            "And phrases and lots of other things that you the entire."
        ],
        [
            "If you're if you're sub of your subtree, you can't look at.",
            "You can only look at one child at a time.",
            "OK, so.",
            "Unfortunately, not everything reduces to a well known dynamic programming algorithm or a graph of weighted graph problem.",
            "There are a lot of other techniques that are being explored, so integer linear programming has kind of taken off as a formalism that lets you solve many structured prediction problems.",
            "And with good modern ILP solvers, this is often reasonably fast.",
            "Re ranking has been around for awhile.",
            "The idea here is that you use a simpler model.",
            "GTK best list of solutions from that model and then re rank them with the.",
            "Richer model that has has the ability to look at larger features, stacking belief propagation, MCMC methods, search all these things from AI and machine learning can be used to decode.",
            "I think in general people in natural language processing are averse to having to use these generic techniques.",
            "They'd rather find something that is particularly efficient for the problem they'd like to solve.",
            "I mean, wouldn't we operate often these particular learning algorithms are particular decoding methods are linked very closely to particular learning algorithms.",
            "But in principle it need."
        ],
        [
            "Opisso so right now a couple of things that are getting a lot of attention are coarse to fine decoding, where again you use some kind of simpler model to reduce your search space and iteratively branch out until you until you get to a complete Y. Decoding with multiple structures at once, which is sometimes called joint inference.",
            "Suppose that I want to parse a sentence in a language like Hebrew, which has both morphology and and has hierarchical syntax.",
            "We'd like to.",
            "We'd like to disambiguate both the morphology and syntax at the same time, and also people are starting to think about generic tools that let you build decoders using using these techniques like dynamic programming, integer linear programming."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I people may still trickle in, but this is going to take.",
                    "label": 0
                },
                {
                    "sent": "I think most of the time we've got, so I want to go ahead and start close to the starting time.",
                    "label": 0
                },
                {
                    "sent": "So thanks for coming.",
                    "label": 0
                },
                {
                    "sent": "I'm Noah Smith from Carnegie Mellon and this is a tutorial about structured prediction for natural language processing.",
                    "label": 1
                },
                {
                    "sent": "I should say upfront that this is a condensed version of a course that I teach at Carnegie Mellon called Language and Statistics 2.",
                    "label": 0
                },
                {
                    "sent": "There's at least one veteran in the audience.",
                    "label": 0
                },
                {
                    "sent": "And there's quite a lot of material to talk about, so I've been somewhat selective, but hopefully this will give you a nice picture of the connection between machine learning and linguistic analysis in natural language processing.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the relationship between natural language processing and machine learning goes back a very long time.",
                    "label": 1
                },
                {
                    "sent": "Some of the earliest people to think about computational models of language.",
                    "label": 0
                },
                {
                    "sent": "We're thinking we're using statistical statistical methods for analyzing text, and some of the first people to start thinking about applying statistics to real world problems.",
                    "label": 0
                },
                {
                    "sent": "Looked at text as one source of examples, and so if you want to sum things up today, you might say that natural language processing really loves machine learning, because it gives us elegant, well formed.",
                    "label": 0
                },
                {
                    "sent": "Solutions to solve the kinds of problems we need to solve, and Meanwhile machine learning loves to tout NLP as one of it.",
                    "label": 0
                },
                {
                    "sent": "One of the great application areas.",
                    "label": 0
                },
                {
                    "sent": "So every year there are papers at ICM, Ellen Nips about natural language processing.",
                    "label": 0
                },
                {
                    "sent": "It's become one of the really hot topics, but I would put forward that we are starting to see some signs of a strain between the two.",
                    "label": 0
                },
                {
                    "sent": "So here are some of the agreement.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is from from the natural language processing side when when natural language processing people like me go to machine learning conferences, we often get a little bit frustrated.",
                    "label": 0
                },
                {
                    "sent": "Machine learning algorithms don't always scale well to all the data we've got for language processing.",
                    "label": 1
                },
                {
                    "sent": "Sometimes scalability is just not not the focus in machine learning.",
                    "label": 0
                },
                {
                    "sent": "A second problem is that the models that are often put forward in machine learning are very simplistic and make very strange to a computational linguists point of view independence assumptions.",
                    "label": 0
                },
                {
                    "sent": "So machine learning is to stop assuming things and finally natural language processing does not for the most part.",
                    "label": 1
                },
                {
                    "sent": "I hopefully will convince you of this by the end of the tutorial.",
                    "label": 0
                },
                {
                    "sent": "Reduce to a problem of classification or a collection of classification problems.",
                    "label": 0
                },
                {
                    "sent": "We need structured classification or structured prediction, so the fixation maybe of machine learning on classification and regression doesn't sit well with natural language process.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Types now.",
                    "label": 0
                },
                {
                    "sent": "Meanwhile, I also consider myself a machine learning person, so when I go to NLP conferences, here are some of the grievances that I might put forward so.",
                    "label": 0
                },
                {
                    "sent": "One unfortunate thing about natural language is that the data never give you everything you really want to model.",
                    "label": 0
                },
                {
                    "sent": "The data are almost always going to be incomplete, so that means supervised learning kind of breaks down.",
                    "label": 0
                },
                {
                    "sent": "So why can't you just tell me what you want?",
                    "label": 1
                },
                {
                    "sent": "Also, the evaluation criteria in natural language processing, the final loss function that you're going to use to evaluate how well your technique works is always changing.",
                    "label": 0
                },
                {
                    "sent": "People don't even agree in natural language processing on how we should evaluate our systems.",
                    "label": 0
                },
                {
                    "sent": "And finally, I don't know people who are trained in machine learning may often be puzzled to hear all the talk about this thing called linguistics.",
                    "label": 0
                },
                {
                    "sent": "At natural language processing conferences, what is this linguistics anyway?",
                    "label": 0
                },
                {
                    "sent": "And why do we need it?",
                    "label": 1
                },
                {
                    "sent": "And why are you always talking about it?",
                    "label": 0
                },
                {
                    "sent": "And why is it called computational linguistics anyway?",
                    "label": 0
                },
                {
                    "sent": "So hopefully I'll clear up a little bit of this.",
                    "label": 0
                },
                {
                    "sent": "So my claim is that the marriage.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We not only processing machine, learning can survive, but both parties need to learn to understand each other, and I think that's happening as we see more and more overlap between the two communities.",
                    "label": 1
                },
                {
                    "sent": "And this tutorial is meant to be marriage counseling for natural language processing and machine learning.",
                    "label": 1
                },
                {
                    "sent": "So, given that that actually I should ask how many people here when you were never supposed to ask a crowd to tell you something negative about themselves, you have to ask them to tell you something positive.",
                    "label": 0
                },
                {
                    "sent": "So who here would count themselves an expert in natural language processing?",
                    "label": 0
                },
                {
                    "sent": "Excellent, and who would count themselves an expert in machine learning and maybe even like structured prediction in particular.",
                    "label": 0
                },
                {
                    "sent": "OK, great so hopefully apart from the one or two people who raise their hands both times, hopefully everyone will learn something.",
                    "label": 0
                },
                {
                    "sent": "I won't be offended if you leave now.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "This is sort of a framework outline of the talk.",
                    "label": 0
                },
                {
                    "sent": "I'm going to start out by talking about some linguistic analysis problems that are examples of structured prediction, and I'm not going to talk about how we solve them.",
                    "label": 1
                },
                {
                    "sent": "I'm just going to try and state what the problems are and why they are important, so that's going to be the part where we get closest to linguistics.",
                    "label": 0
                },
                {
                    "sent": "The second, the second part after the first break, I'll talk about decoding or specific algorithmic tools that are frequently used to make structured predictions, and there I'm not going to tell you how we get the models.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to talk about the prediction problems.",
                    "label": 0
                },
                {
                    "sent": "And then the third part, sorry, let's see.",
                    "label": 0
                },
                {
                    "sent": "did I break this down properly?",
                    "label": 0
                },
                {
                    "sent": "Actually, no, that's not.",
                    "label": 0
                },
                {
                    "sent": "After the break, the 1st two big bullets are before the 1st break.",
                    "label": 0
                },
                {
                    "sent": "The the second section of the of the tutorial will be about supervised learning.",
                    "label": 0
                },
                {
                    "sent": "When you've got complete data, a case that we do see in NLP but not as often as we'd like, and then the third part of the talk, which will be the shortest after the second break, will cover the incomplete data case or unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "What I'm not going to do is tell you what the best techniques are to use for doing that.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Processing I'm not going to show you any experimental comparisons at all.",
                    "label": 1
                },
                {
                    "sent": "You can see the literature for those.",
                    "label": 1
                },
                {
                    "sent": "I'm not going to go into too much detail on specific datasets.",
                    "label": 1
                },
                {
                    "sent": "How to find the particular parsing data set that you want to use to test out your idea, or how annotation conventions work, or linguistic theory.",
                    "label": 0
                },
                {
                    "sent": "There's plenty of that in the references, but I am going to hopefully give you some insight as to why people care about those things, and I'm not going to talk too much about implementation.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try and keep this at a fairly high level of abstractions, so you're probably not going to be able to leave this tutorial.",
                    "label": 0
                },
                {
                    "sent": "Able to go implement a state of the art natural language processing system.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, I can't teach you that in 2 1/2 hours.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so hopefully at the end you'll be more excited about natural language processing.",
                    "label": 0
                },
                {
                    "sent": "An inspired to go read more and consider working on problems in NLP and finding new ones.",
                    "label": 0
                },
                {
                    "sent": "Hopefully you'll understand structured prediction better, at least the the applications view of it, and maybe have a broader view of what structure prediction can encompass.",
                    "label": 1
                },
                {
                    "sent": "An hopefully you'll start to raise your expectations about what structured ML should be able to do.",
                    "label": 1
                },
                {
                    "sent": "OK, So what is structured prediction?",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is no conventional definition.",
                    "label": 1
                },
                {
                    "sent": "I think Hilda may, in his thesis proposed a couple of a couple of formal statements.",
                    "label": 0
                },
                {
                    "sent": "Basically, I'm going to say you know it when you see it.",
                    "label": 1
                },
                {
                    "sent": "Basically, the idea is that your output spaces somehow combinatorially very large and highly dependent on the input space.",
                    "label": 0
                },
                {
                    "sent": "So as your input gets larger, your output can get larger and as your input gets larger linearly, your output space can get bigger exponentially, and there's this possible thing we can quibble about whether the loss function has to decompose into parts, and I'm going to say that it might or might not.",
                    "label": 0
                },
                {
                    "sent": "Sorry whether the loss function is allowed to decompose into parts, and I'm going to say it is allowed to, because often LP they do.",
                    "label": 0
                },
                {
                    "sent": "Well, we'll come back to this point a little later.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we get into the meet.",
                    "label": 0
                },
                {
                    "sent": "I'm going to start talking about representations of problems in natural language processing.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Don't think about bags of words.",
                    "label": 1
                },
                {
                    "sent": "If you're going to do natural language processing, one of the first things you have to accept is that natural language is more interesting than just a histogram.",
                    "label": 0
                },
                {
                    "sent": "So language or a document or piece of text has some implicit structure.",
                    "label": 1
                },
                {
                    "sent": "To put it another way, the words in the document are not IID, and so you know enough to put it one way this might.",
                    "label": 0
                },
                {
                    "sent": "This might modify some of my linguist friends.",
                    "label": 0
                },
                {
                    "sent": "I could say that linguistic linguistics offers many different theories about the relationships among pieces of text, and many different models for explaining how those words are not IID, and we're going to see some hints of some of those.",
                    "label": 1
                },
                {
                    "sent": "So I'm stealing this.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "My colleague Dan Klein at Berkeley, although he uses it for a different purpose, this is what you see in a document, and in fact as soon as you start.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thing to do?",
                    "label": 0
                },
                {
                    "sent": "Anything interesting with text like automated understanding or translation or summarization or basically any kind of deeper processing for an application.",
                    "label": 0
                },
                {
                    "sent": "You realize that you're going to have to model something deeper that you actually can't see underneath, and that can be about as big as you want, and it might even extend below the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Bottom of the slide.",
                    "label": 0
                },
                {
                    "sent": "OK, I'm going to stick with the convention that script X is the set of possible inputs.",
                    "label": 1
                },
                {
                    "sent": "Most frequently this is going to be Sigma star for some vocabulary Sigma an Y is going to be a set of possible outputs and I'm going to show you a bunch of different examples of why it could be for different problems.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the first one if you.",
                    "label": 0
                },
                {
                    "sent": "If you're still puzzling over why, I suggested that we shouldn't think about bags of words.",
                    "label": 0
                },
                {
                    "sent": "Chinese should convince you that bags of words are sort of inadequate.",
                    "label": 0
                },
                {
                    "sent": "If you want to language processing.",
                    "label": 0
                },
                {
                    "sent": "So there may be people in the room who can read the second line here, which is in Chinese it doesn't.",
                    "label": 0
                },
                {
                    "sent": "With Chinese is not written conventionally with white space.",
                    "label": 0
                },
                {
                    "sent": "If you ask a Chinese person or a Chinese speaking person to find boundaries between the words.",
                    "label": 0
                },
                {
                    "sent": "I asked a student of mine to do this, and he did I don't know if he lied to me or not.",
                    "label": 0
                },
                {
                    "sent": "But this is what he tells me.",
                    "label": 0
                },
                {
                    "sent": "This is where he tells me the word boundaries are.",
                    "label": 0
                },
                {
                    "sent": "People are laughing.",
                    "label": 0
                },
                {
                    "sent": "So maybe he.",
                    "label": 0
                },
                {
                    "sent": "Maybe he completely played a trick on New York.",
                    "label": 0
                },
                {
                    "sent": "It seems reasonable.",
                    "label": 0
                },
                {
                    "sent": "OK, so so if you're going to do NLP with Chinese, you need to figure out where the word boundaries are.",
                    "label": 0
                },
                {
                    "sent": "So this is called word segmentation.",
                    "label": 0
                },
                {
                    "sent": "And so we can imagine that what it what it requires is taking a.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sequence of characters and mapping it to a sequence of words, and this is mostly trivial if you want to work with English, write English gives us very strong clues.",
                    "label": 0
                },
                {
                    "sent": "The conventions of English writing give us very strong clues as to where the word boundaries are.",
                    "label": 0
                },
                {
                    "sent": "There might be some little things you'd want to do.",
                    "label": 0
                },
                {
                    "sent": "Tokenize off punctuation or something like that, but for the most part this is not really a problem we deal with in English, although there is this problem called sentence segmentation that nobody talks about anymore but 10 or 15 years ago it was.",
                    "label": 0
                },
                {
                    "sent": "It was an important problem.",
                    "label": 0
                },
                {
                    "sent": "Look at a piece of text and find the sentence boundaries.",
                    "label": 0
                },
                {
                    "sent": "You think periods full stops are good sign, but they're not a perfect sign.",
                    "label": 0
                },
                {
                    "sent": "We use it after Mr and there are places where sentence is end without a period.",
                    "label": 0
                },
                {
                    "sent": "OK, so so now we've gotten a little bit past words.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's still a problem even once you found all the words in a document and identified what counts as a word.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of them an awful lot of them, and they tend to follow their sort of the classic case of zipfian distributions.",
                    "label": 1
                },
                {
                    "sent": "So one of the first things that most NLP systems try to do is.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Words more manageable.",
                    "label": 0
                },
                {
                    "sent": "So what I have here is I have histogram of the words in a collection of really boring financial reports from 2001 to 2005.",
                    "label": 0
                },
                {
                    "sent": "I think it's about 10,000 documents and So what I've done is I've just counted the number of times each word occurs and then sorted based on the frequency.",
                    "label": 0
                },
                {
                    "sent": "So this is the most frequent word and it occurs 8 * 10 to 6 times and you can see they fall off and those are the first 10 words and if we look at the first 100 words we.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With this beautiful long tail distribution, and if we look at the first 1000 words.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It keeps going and if we look at the next slide it takes awhile to load because.",
                    "label": 0
                },
                {
                    "sent": "Why does it take awhile to load?",
                    "label": 0
                },
                {
                    "sent": "It's taking awhile to load because it's a really big graphic 'cause there are an awful lot of words in this document and this is sort of a device to.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just need to slow down.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There we go, right?",
                    "label": 0
                },
                {
                    "sent": "There are more than 120,000 different unique words in this collection of documents, most of which occur once.",
                    "label": 1
                },
                {
                    "sent": "Right, so you know if you wanted to build, say, a probability distribution over those words, or a collection of distributions over those words, you'd be really hard pressed because this is just very high dimensional so.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we'd like to do is project vocabulary into something simpler, and so people have a lot of ways of doing this.",
                    "label": 0
                },
                {
                    "sent": "This isn't really deeper, interesting, but just to give some examples, we have a sentence at the top.",
                    "label": 0
                },
                {
                    "sent": "On the surface, you might tokenize and split off some of the.",
                    "label": 0
                },
                {
                    "sent": "Some of the punctuation, maybe down case another thing people do, sometimes not so much in NLP, is to split off certain common endings of words.",
                    "label": 0
                },
                {
                    "sent": "So, for example, important might map to import, which actually seems wrong.",
                    "label": 0
                },
                {
                    "sent": "If we were trying to understand this sentence, these important and import have two very different meanings, even if angle and angle without an E seem like that like that might be plausible.",
                    "label": 0
                },
                {
                    "sent": "Another thing people do is they limit eyes and reduce words to their simplest forms.",
                    "label": 0
                },
                {
                    "sent": "We had an example.",
                    "label": 0
                },
                {
                    "sent": "Cats becomes cat.",
                    "label": 0
                },
                {
                    "sent": "This is mostly solved in short Perl scripts.",
                    "label": 0
                },
                {
                    "sent": "This isn't really difficult.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem people don't write papers about these things anymore, but they are instances of structured prediction.",
                    "label": 0
                },
                {
                    "sent": "It's just that we make the prediction using rules using very simple methods.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there are other ways of reducing the vocabulary, and some of them are are much more challenging.",
                    "label": 0
                },
                {
                    "sent": "So the next thing to talk about is a sort of extreme form of tokenization where we map we map words into word classes or parts of speech.",
                    "label": 0
                },
                {
                    "sent": "You've probably seen this problem before.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a Canonical natural language processing problem, so here I've taken each word in the sentence and assigned it a blue part of speech.",
                    "label": 0
                },
                {
                    "sent": "So, determiners, nouns, prepositions, these are.",
                    "label": 0
                },
                {
                    "sent": "These are the parts of speech as as decided by somebody.",
                    "label": 0
                },
                {
                    "sent": "I think these go back to Aristotle or something in the beginning, but you know you can.",
                    "label": 0
                },
                {
                    "sent": "You can kind of agree on a convention that you may be able to decide on a way to map words to their parts of speech.",
                    "label": 0
                },
                {
                    "sent": "And this is this is done almost, you know.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Personally, now in NLP and you might think you know why is this what's challenging here the the problem, of course, is that many words are ambiguous, and so you could independently classify each word on its own based on based on what you've seen before.",
                    "label": 0
                },
                {
                    "sent": "And you can get close to 90% accuracy on this problem if you've got enough data in terms of the fraction of words you correctly tag.",
                    "label": 0
                },
                {
                    "sent": "But context actually is really important.",
                    "label": 0
                },
                {
                    "sent": "So for example, the word leaves can be a plural noun or singular verb, but it's probably a verb if it's followed by noun.",
                    "label": 1
                },
                {
                    "sent": "Similarly, the word bear is community can be a noun or verb, but if it has a determiner in front of it, you can bet it's probably a noun.",
                    "label": 0
                },
                {
                    "sent": "So these sort of local interactions among the words tend to allow us to do a lot better if we can model them.",
                    "label": 0
                },
                {
                    "sent": "So the best performance on part of speech tagging.",
                    "label": 0
                },
                {
                    "sent": "I think these days, is around 97%, and these are based on structured models where we make joint decisions about all the words at the same.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "This is sort of the simplest example where the current NLP paradigm took off and continues to be the dominant way of doing things.",
                    "label": 1
                },
                {
                    "sent": "So the general framework here is that you get some people who understand part of speech tagging or whatever your linguistic structure is.",
                    "label": 0
                },
                {
                    "sent": "You get them to label some text.",
                    "label": 1
                },
                {
                    "sent": "You probably pay them a little bit of money to do that, and then you go perform supervised learning.",
                    "label": 0
                },
                {
                    "sent": "And that could be some kind of structured prediction model, as I would advocate here, but you should.",
                    "label": 0
                },
                {
                    "sent": "You should be a little bit wary of this.",
                    "label": 0
                },
                {
                    "sent": "These part of speech, you know if you find the data set that gives you parts of speech, you should remember that this didn't come from God, right?",
                    "label": 0
                },
                {
                    "sent": "This came from somebody who had some insight about what parts of speech were useful for that language, and then he paid some guys to sit down and write down what the parts of speech were in some text.",
                    "label": 0
                },
                {
                    "sent": "This is this is a choice somebody somebody had some thoughts and you might you know if you knew the language you might disagree with them.",
                    "label": 0
                },
                {
                    "sent": "Personally I disagree with some of the conventions that were chosen in the current standard datasets for part of speech.",
                    "label": 0
                },
                {
                    "sent": "Further, even once you've laid down what the conventions are, people who are annotating don't always agree with each other, so annotation is actually a major effort that usually requires you know bringing in lots of different people, making sure they agree, going through various phases to measure Inter annotator agreement, building up a case that we've got is.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Really real OK?",
                    "label": 0
                },
                {
                    "sent": "Let's come back to words again.",
                    "label": 0
                },
                {
                    "sent": "So for a long time in LP focused mainly mainly on English, but of course not everyone speaks English and we'd like to do natural language processing for other languages.",
                    "label": 0
                },
                {
                    "sent": "So here's a sentence in Korean and the graph at the bottom is a little lattice or finite state network that shows you the different possible analysis of this Korean team speak Korean.",
                    "label": 0
                },
                {
                    "sent": "No, OK, I'm safe.",
                    "label": 0
                },
                {
                    "sent": "Each of these, so each of these words, many of the words in that in that sequence can be analyzed more than one way.",
                    "label": 0
                },
                {
                    "sent": "So I think if you multiply this out, there's something like 40 different analysis of the sentence, depending on how you break the words up and how you tag them.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of showing you different ways of segmenting and tagging the morphemes in this sentence.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this may be a surprise if you only speak English, but morphological disambiguation is really kind of crucial for NLP in other languages, so again, this is just a sequence to sequence mapping problem.",
                    "label": 1
                },
                {
                    "sent": "You get a sequence of surface words or characters maybe, and you want to want to break them into morphemes and possibly tag them as well.",
                    "label": 0
                },
                {
                    "sent": "This is one of my favorite examples.",
                    "label": 0
                },
                {
                    "sent": "Does anyone know?",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Language this is.",
                    "label": 0
                },
                {
                    "sent": "This is a word in Turkish.",
                    "label": 0
                },
                {
                    "sent": "It's very long and it means something like behaving as if you were among those whom we could not civilize.",
                    "label": 1
                },
                {
                    "sent": "It's a single word.",
                    "label": 0
                },
                {
                    "sent": "And if you wanted to, you know you could say this is kind of weird.",
                    "label": 1
                },
                {
                    "sent": "This is like some weird fringe effect, but there are more than 60 million people in the world who speak this language.",
                    "label": 0
                },
                {
                    "sent": "And if we wanted to provide intelligent software, they could manage their language for them.",
                    "label": 0
                },
                {
                    "sent": "We would have to break that word into its parts and figure out what at least some of the parts mean, probably.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "What makes things even more nasty with Turkish is that often the vowels change throughout the sequence to match each other.",
                    "label": 0
                },
                {
                    "sent": "There's this thing called vowel harmony, so the problem gets even hairier.",
                    "label": 0
                },
                {
                    "sent": "It's like it's almost as if they've conspired to make their language hard.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So OK, so I'm going to move up a little bit from words.",
                    "label": 0
                },
                {
                    "sent": "A lot of people have built their careers on just handling the words in different languages, but we're going to keep going.",
                    "label": 0
                },
                {
                    "sent": "So one thing that's often useful is to find specific kinds of substrings in text.",
                    "label": 1
                },
                {
                    "sent": "So I'm going to call these chunks.",
                    "label": 0
                },
                {
                    "sent": "So, for example, if we go back to our cat cat sentence, this sentence has two noun phrases in it in two prepositional phrases, which I've marked, or two based on phrases I should say, and so we can.",
                    "label": 0
                },
                {
                    "sent": "We can use structured prediction to find these important segments or interesting segments or chunks using what?",
                    "label": 0
                },
                {
                    "sent": "Well, the representation that's usually used is to turn this into a sequence labeling problem where you label the beginning and interior and exterior points for each chunk.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is just another string transduction problem.",
                    "label": 0
                },
                {
                    "sent": "You have a sentence coming in and what you're going to send out, or these ayobi labels possibly.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Augmented with chunks.",
                    "label": 0
                },
                {
                    "sent": "So this means that this is the beginning of a noun phrase the interior, and now we're starting a prepositional phrase, the interior, the interior, and now we're outside.",
                    "label": 0
                },
                {
                    "sent": "And so the encoding is really not that important, it's just there.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Widely used, it's worth seeing, so this is this kind of this technique is used for base noun phrase chunking, also for other kinds of shallow parsing.",
                    "label": 0
                },
                {
                    "sent": "For sort of other types of phrases, and most commonly named entity recognition.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think that's what I want to show, so here's a sentence.",
                    "label": 0
                },
                {
                    "sent": "From the news.",
                    "label": 0
                },
                {
                    "sent": "Not too long ago and.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the named entities an We might label those same entities, so the first one is a place, the second one is a person and the third one.",
                    "label": 0
                },
                {
                    "sent": "What is RoboCop?",
                    "label": 0
                },
                {
                    "sent": "I guess RoboCop is a fantastical person.",
                    "label": 0
                },
                {
                    "sent": "I would call that miscellaneous but depends on your convention.",
                    "label": 0
                },
                {
                    "sent": "Maybe RoboCop is a person.",
                    "label": 0
                },
                {
                    "sent": "OK, so if you take this idea of finding interesting substrings too.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To its extreme.",
                    "label": 0
                },
                {
                    "sent": "An imagine that we want to find all the interesting substrings, and we believe that they have a hierarchical structure.",
                    "label": 0
                },
                {
                    "sent": "Then you get the parsing problem.",
                    "label": 1
                },
                {
                    "sent": "OK, and this is this is a problem that has dominated and continues to dominate and LP it's one of the main areas.",
                    "label": 0
                },
                {
                    "sent": "If you go to a computational linguistics conference, there will be tons of sessions on parsing and this is sort of parsing.",
                    "label": 0
                },
                {
                    "sent": "In a nutshell, the idea is to find the compositional phrases from the whole sentence.",
                    "label": 1
                },
                {
                    "sent": "The S up at the top all the way down to the words.",
                    "label": 0
                },
                {
                    "sent": "And if you're training computer science, you've probably seen something like this in a compilers class.",
                    "label": 0
                },
                {
                    "sent": "Here we're just doing it for natural language.",
                    "label": 0
                },
                {
                    "sent": "OK, so this S means sentence because it's a sentence.",
                    "label": 0
                },
                {
                    "sent": "NP means noun phrase, just like we saw before.",
                    "label": 0
                },
                {
                    "sent": "But notice that noun phrases can be recursive.",
                    "label": 0
                },
                {
                    "sent": "You can embed one non phrase in another.",
                    "label": 0
                },
                {
                    "sent": "You can embed.",
                    "label": 0
                },
                {
                    "sent": "Sentence is another sentence is so this this kind of structure is deeply recursive and it should be very clear to you that if I'm permitting all kinds of words to play all kinds of roles, then there may be many, many ways to parse a sentence in practice.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the problem is to find the compositional phrases from the whole sentence all the way down to the words and often this is done using some kind of context free grammar, not always.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is maybe a slight tangent, but when I teach undergraduates NLP, particularly the ones who have taken programming language classes, programming language classes, they often think Oh well, you know I can use.",
                    "label": 1
                },
                {
                    "sent": "I can use standard tools for building compilers, and I can parse natural language by by parsing it with Lex and yacc, and so forth.",
                    "label": 1
                },
                {
                    "sent": "And of course this completely fails and This is why the problem is that natural language is full of ambiguity.",
                    "label": 0
                },
                {
                    "sent": "There's ambiguity at every level.",
                    "label": 0
                },
                {
                    "sent": "Much of it is uninteresting.",
                    "label": 0
                },
                {
                    "sent": "It's just a failing of our models to be able to distinguish what can happen and what can happen.",
                    "label": 1
                },
                {
                    "sent": "But because because our models are not perfect, and because natural language really is ambiguous even to people who people who can understand it.",
                    "label": 0
                },
                {
                    "sent": "This is why natural language processing.",
                    "label": 0
                },
                {
                    "sent": "This is why I'm giving the talk here at I smell another programming Language Languages conference, so this is this is my favorite example of an ambiguous sentence.",
                    "label": 1
                },
                {
                    "sent": "This is a headline a little hope given brain damaged woman and so.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The the the head of the story was actually about Terry Schivo, and so it was really the sober story about, you know, the doctor told her family that was bad news.",
                    "label": 0
                },
                {
                    "sent": "She wasn't likely to recover, but if you have a sick mind.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then you may be able to.",
                    "label": 0
                },
                {
                    "sent": "Come up with a different reading of the same sentence.",
                    "label": 0
                },
                {
                    "sent": "Little hope, IE.",
                    "label": 0
                },
                {
                    "sent": "For us given brain damaged woman that was chosen to be on the campaign.",
                    "label": 0
                },
                {
                    "sent": "I'm not getting any laughs from this crowd.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, this is this is the really sick and twisted reading.",
                    "label": 0
                },
                {
                    "sent": "Little Hope was given a brain damaged woman.",
                    "label": 0
                },
                {
                    "sent": "Little hope given brain damaged woman.",
                    "label": 0
                },
                {
                    "sent": "It was a very strange Christmas.",
                    "label": 1
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then there's there's this last one which I didn't see myself, but it incredibly twisted individual pointed out, if only little hope had used her gift for good rather than evil, we gave little hope of brain.",
                    "label": 1
                },
                {
                    "sent": "We give her brain, and she went in damage to woman.",
                    "label": 0
                },
                {
                    "sent": "Little help given brain damaged woman, past tense verb.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so I should you know I should point out this is a newspaper headline.",
                    "label": 0
                },
                {
                    "sent": "This is an actual headline from the New York Times, about six years ago.",
                    "label": 0
                },
                {
                    "sent": "Headlines are a little bit more easy to create these examples from because they're so telegraphic and lots of words are left out.",
                    "label": 0
                },
                {
                    "sent": "But trust me, the problem.",
                    "label": 0
                },
                {
                    "sent": "The problem continues to persist even in longer in longer sentences, in ways that you would never process because you're not sick and twisted.",
                    "label": 0
                },
                {
                    "sent": "But computer program that can find every possible parts of something can easily be LED astray.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so there's another convention for parsing that has caught on quite a lot recently that I think deserves mention called dependency parsing, and it's really not fundamentally different.",
                    "label": 0
                },
                {
                    "sent": "You can show that you can move back and forth between dependency and compositional phrase structures.",
                    "label": 0
                },
                {
                    "sent": "They're not identical, but they capture a lot of the same ideas.",
                    "label": 0
                },
                {
                    "sent": "The idea is that every word has a parent to which it serves as an argument.",
                    "label": 0
                },
                {
                    "sent": "So if you if you look at this and kind of squint and look at the word is, which is sort of the central.",
                    "label": 0
                },
                {
                    "sent": "Bit of a sentence that's equating two things.",
                    "label": 0
                },
                {
                    "sent": "The two arguments of is are over here, angle and over here clue angle is a clue that summarizes the sentence fairly well.",
                    "label": 0
                },
                {
                    "sent": "Those are the most important pieces of the sentence, and then everything else is sort of modifying or serving as an argument to something else.",
                    "label": 0
                },
                {
                    "sent": "And so dependencies syntax focuses on these relationships.",
                    "label": 1
                },
                {
                    "sent": "You might label the arcs, you can do a lot more fancy things, but this is sort of the bare bones version.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the idea is that your structure that you need to predict is a mapping from each word to its set of children.",
                    "label": 0
                },
                {
                    "sent": "And there's there's sort of another version of dependency parsing.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Goes a step farther, which is kind of interesting because it was only recently pointed out.",
                    "label": 1
                },
                {
                    "sent": "If if you limit yourself to trees where the edges all lay on one side of the of the words and don't cross each other, that's called.",
                    "label": 0
                },
                {
                    "sent": "Those are called projective trees.",
                    "label": 1
                },
                {
                    "sent": "I can give you a more formal definition, but that's sort of the simplest way to say.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the graph, the graph is restricted to be on one side and it's completely planar.",
                    "label": 0
                },
                {
                    "sent": "The edges never cross, and this actually works very well for English.",
                    "label": 0
                },
                {
                    "sent": "You can explain most of English this way, but.",
                    "label": 0
                },
                {
                    "sent": "Uh.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Occasionally we really want a crossing arc, so this is an example of a non projective Trea talk is scheduled on cats ears today.",
                    "label": 1
                },
                {
                    "sent": "Perfectly good English.",
                    "label": 0
                },
                {
                    "sent": "Nobody's going to Jack because this isn't a linguistics talk.",
                    "label": 0
                },
                {
                    "sent": "On Cats, ears is attached.",
                    "label": 0
                },
                {
                    "sent": "That phrase is attached through on to talk, causing a crossing link or non projective arc.",
                    "label": 0
                },
                {
                    "sent": "So non productivity seems to be much more important in some languages than others.",
                    "label": 1
                },
                {
                    "sent": "Choosing not to be not to use it, not to model it is perfectly OK, but you might sacrifice some accuracy in doing so.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we've been doing, sort of progressively as I've shown more and more of these problems, is we started at the top and talked about words which you can mostly see.",
                    "label": 0
                },
                {
                    "sent": "Now you should be skeptical about.",
                    "label": 0
                },
                {
                    "sent": "Of course you can't completely see them, but mostly, and we just moved down.",
                    "label": 0
                },
                {
                    "sent": "We've been moving down to deeper and deeper structure, so now I'm going to go a little.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A bit lower.",
                    "label": 0
                },
                {
                    "sent": "I'm going to show a picture.",
                    "label": 0
                },
                {
                    "sent": "My version of what's called the Linguistic pipeline and what I want you to be aware of is that as we go deeper and deeper and try to analyze more and more of the structure in language, we're getting, we're getting farther and farther away from the particulars of the language itself and more into physical and cultural and non linguistic things.",
                    "label": 0
                },
                {
                    "sent": "That language is really about.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is.",
                    "label": 0
                },
                {
                    "sent": "This is sort of a breakdown, maybe of the field of linguistics, plus plus a little extra.",
                    "label": 0
                },
                {
                    "sent": "So up at the top, just imagine the iceberg superimposed.",
                    "label": 0
                },
                {
                    "sent": "I didn't know how to do that in Latex, but imagine the iceberg superimposed over top of this up at the top we have the the acoustics that the phonetics which are the physical properties of speech, the sound waves that are moving from my mouth into your ear, or the orthography.",
                    "label": 0
                },
                {
                    "sent": "The units of writing that I put down a page or in a slide that you can read and then we start getting to things that are harder to see and we have to reason about more carefully.",
                    "label": 0
                },
                {
                    "sent": "Like the units of sound, the structure of words, the structure of sentences, morphology and syntax, or what I've been talking about mostly so far, and then we get down into this thing called meaning.",
                    "label": 1
                },
                {
                    "sent": "What does?",
                    "label": 1
                },
                {
                    "sent": "What does a sequence of words actually mean?",
                    "label": 0
                },
                {
                    "sent": "What is it referring to in the world?",
                    "label": 0
                },
                {
                    "sent": "What is it?",
                    "label": 1
                },
                {
                    "sent": "What is its literal meaning, or what are the acts of communication which are intended?",
                    "label": 0
                },
                {
                    "sent": "When I say can you pass the salt?",
                    "label": 0
                },
                {
                    "sent": "I'm not asking you literally a question about whether you're capable of passing the salt, right?",
                    "label": 0
                },
                {
                    "sent": "I want you to do something, and I'm conveying it to you in a polite way that you're not meant to interpret literally.",
                    "label": 0
                },
                {
                    "sent": "And further, if we go down, there's discourse, which is, I say something.",
                    "label": 0
                },
                {
                    "sent": "You say something, or in this case you sit there and stare at me awkwardly, because this is a lecture scenario.",
                    "label": 0
                },
                {
                    "sent": "There you can parse discourse when it's just one person speaking as well as we move down, we're getting farther and farther from the things that are easy to observe.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to go a little bit farther and you're going to start to see everything come apart at the seams.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem of meaning would be to convert a sentence into some kind of Canonical meaning representation language.",
                    "label": 1
                },
                {
                    "sent": "So if I get Robin swam across the River and delivered the message, I might parse this into two sort of predicates likes women delivered.",
                    "label": 1
                },
                {
                    "sent": "There were two events, one in which somebody was swimming who was swimming.",
                    "label": 0
                },
                {
                    "sent": "Oh, Robin was swimming, she's the agent, and she did.",
                    "label": 0
                },
                {
                    "sent": "She swim, swim through.",
                    "label": 0
                },
                {
                    "sent": "She swam through the River.",
                    "label": 0
                },
                {
                    "sent": "That's the medium, and I'm sort of making this up as I go.",
                    "label": 0
                },
                {
                    "sent": "The agent is a technical term, but you can sort of assign these semantic roles to the arguments of each predicate.",
                    "label": 0
                },
                {
                    "sent": "You could convert this into some kind of 1st order logic.",
                    "label": 0
                },
                {
                    "sent": "I took a stab at it.",
                    "label": 0
                },
                {
                    "sent": "You might disagree with exactly how to represent this in first order logic.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of things in natural language that you can represent this way, but you know nobody quite agrees and nobody feels like this.",
                    "label": 0
                },
                {
                    "sent": "These representations are quite everything we need to represent the meaning of.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of language, so I'm going to say for script why there's no consensus yet.",
                    "label": 1
                },
                {
                    "sent": "Script X is a sentence coming in finding the semantic structure.",
                    "label": 0
                },
                {
                    "sent": "We don't know.",
                    "label": 1
                },
                {
                    "sent": "There are a lot of things on the table.",
                    "label": 0
                },
                {
                    "sent": "Semantic roles are one that's getting a lot of attention this year.",
                    "label": 0
                },
                {
                    "sent": "It was part of the Donald Connell shared task at the Natural Language Learning Conference.",
                    "label": 0
                },
                {
                    "sent": "1st Order logic expressions have always been around.",
                    "label": 1
                },
                {
                    "sent": "They were there was a little bit of a resurgence there in a few recent papers out of MIT.",
                    "label": 0
                },
                {
                    "sent": "And then there are other things that people have proposed for very specific problems so that.",
                    "label": 1
                },
                {
                    "sent": "Often what you'll have is if you have a particular problem you want to solve.",
                    "label": 0
                },
                {
                    "sent": "People will design a problem specific meaning language.",
                    "label": 0
                },
                {
                    "sent": "So for example, I had a student who is interested in automating understanding of cooking recipes, so it took us about 3 months and we sat down and we designed a a meaning language for cooking recipes, which is a fairly low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "You don't have to talk about everything in the world, only a few things in the kitchen, and we came up with some primitives and we were able to do it without a whole lot of work.",
                    "label": 0
                },
                {
                    "sent": "But it took three months and then we had to annotate data and people don't always agree about what a recipe means and and so on.",
                    "label": 0
                },
                {
                    "sent": "So you can see why this is not getting quite as much attention.",
                    "label": 0
                },
                {
                    "sent": "It's hard.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Maybe a little less hard is figuring out which real world entities are being mentioned in text and where.",
                    "label": 1
                },
                {
                    "sent": "So here's a here's a piece of text from the news recently.",
                    "label": 0
                },
                {
                    "sent": "You can guess who it's about.",
                    "label": 1
                },
                {
                    "sent": "Might be a little bit more helpful.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To understand it, if I highlight the things that are being referred to and color them the same if they're referring to the same entity.",
                    "label": 0
                },
                {
                    "sent": "So there are about four entities being mentioned here, I think.",
                    "label": 0
                },
                {
                    "sent": "Who is it about?",
                    "label": 0
                },
                {
                    "sent": "You springport, that's right, it's about Sotomayor.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "Right, so the key here is to figure out which this is called.",
                    "label": 0
                },
                {
                    "sent": "Part of this is called the coreference problem, but it's also sort of.",
                    "label": 0
                },
                {
                    "sent": "You also have to detect all of the referring expressions, right?",
                    "label": 0
                },
                {
                    "sent": "And so you can see this is sort of a chunking task, but then there's this higher level structure where you link you link things denoted here by color.",
                    "label": 0
                },
                {
                    "sent": "OK, so detecting entities and deciding which ones.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Co.",
                    "label": 0
                },
                {
                    "sent": "Refer and then depending on what type of text you have, you might then want to link those Co referring expressions back into some kind of ontology or representation of the world.",
                    "label": 1
                },
                {
                    "sent": "So where this often comes up for examples in biomedical literature, suppose you have an ontology of all the genes known in the human genome, or some of them.",
                    "label": 0
                },
                {
                    "sent": "Even.",
                    "label": 0
                },
                {
                    "sent": "You might want to look at scientific articles and link back the references to those genes, no matter how they look in the text back into the ontology.",
                    "label": 0
                },
                {
                    "sent": "OK, so we're going to run out of time if I keep going, but.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a whole another dimension to this that's worth bringing up, because there's another sort of data which people have been kind of fascinated by since since maybe 20 years ago, which are parallel, or comparable corpora.",
                    "label": 1
                },
                {
                    "sent": "Mostly parallel corpora, so parallel corpus is a collection of texts in two or more languages, which are translations of each other.",
                    "label": 0
                },
                {
                    "sent": "These these come about as a natural byproduct of the working of international organizations like the UN, and certain news agencies.",
                    "label": 0
                },
                {
                    "sent": "Also the European Parliament, and they're really nice because they show you.",
                    "label": 0
                },
                {
                    "sent": "How the same thing is expressed in two different languages?",
                    "label": 0
                },
                {
                    "sent": "So one idea that's been kind of floating around for a long time is that you can.",
                    "label": 0
                },
                {
                    "sent": "You can build better models of language if you can kind of triangulate.",
                    "label": 0
                },
                {
                    "sent": "If you know how the same idea is expressed in two languages, that gives you more insight into what the idea might be and how it might look.",
                    "label": 0
                },
                {
                    "sent": "So there's a ton of work on things like aligning the sentence as well.",
                    "label": 0
                },
                {
                    "sent": "That was kind of easy.",
                    "label": 0
                },
                {
                    "sent": "We kind of solve that about 10 years ago, but aligning the words between these two sentences.",
                    "label": 1
                },
                {
                    "sent": "The words that mean the same thing or phrases or aligning their parse trees or finding finding the underlying bilingual dictionary.",
                    "label": 0
                },
                {
                    "sent": "Or or and so on and so on.",
                    "label": 0
                },
                {
                    "sent": "And the biggest application of all this, of course, is translation.",
                    "label": 0
                },
                {
                    "sent": "Being able to take one of the sides as input and produce the other's output.",
                    "label": 0
                },
                {
                    "sent": "So you can think of that itself as a structured prediction prob.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that was a very gross smattering of some of the problems people are thinking about in natural language processing and some that they thought about awhile back and sort of stopped thinking about a lot of these things.",
                    "label": 0
                },
                {
                    "sent": "Not everyone agrees that all of these problems are important.",
                    "label": 1
                },
                {
                    "sent": "So for example, you probably can't.",
                    "label": 0
                },
                {
                    "sent": "You're probably going to have a hard time publishing a part of speech, tagging paper about English nowadays, because it's mostly solved and it's not clear that you know that any advance is really going to make a difference for any real application.",
                    "label": 0
                },
                {
                    "sent": "People who focus more on applications like machine translation or summarization are often sceptical about any new kind of linguistic analysis and may ask how is this going to actually help my application does.",
                    "label": 0
                },
                {
                    "sent": "Does this particular step to serve its own intrinsic evaluation, so one one that's gotten a lot of attention is word alignment.",
                    "label": 0
                },
                {
                    "sent": "You've got two sides of a parallel text you can.",
                    "label": 0
                },
                {
                    "sent": "There are many algorithms for trying to figure out which words on each side of the text line up with which other words that mean the same thing and then.",
                    "label": 0
                },
                {
                    "sent": "People started evaluating that on its own, and so people who work on machine translation kind of scratched their heads and said, well, your word alignment got better.",
                    "label": 1
                },
                {
                    "sent": "But did my MC system get better?",
                    "label": 0
                },
                {
                    "sent": "Right, so this is this kind of debate always is going on.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Should we even evaluate parsing on its own anymore?",
                    "label": 0
                },
                {
                    "sent": "Is a question that's kind of a controversial question, but it's one that comes up OK, so.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So right everything in natural language processing comes with controversy.",
                    "label": 0
                },
                {
                    "sent": "I here's.",
                    "label": 0
                },
                {
                    "sent": "Here's a suggestion that there are some problems that haven't really been viewed as structured prediction directly, but deserve to maybe so.",
                    "label": 0
                },
                {
                    "sent": "One is to find the entire predicate argument structure of a sentence, a sort of rough approximation to the semantics along the lines of the 1st order models.",
                    "label": 1
                },
                {
                    "sent": "I was suggesting earlier for the whole sentence that hasn't really been done.",
                    "label": 1
                },
                {
                    "sent": "Tasks that require generation of text where text is the output answering a question translating.",
                    "label": 0
                },
                {
                    "sent": "We talked about that.",
                    "label": 0
                },
                {
                    "sent": "Or you get it, you get some text in and you want to simplify it.",
                    "label": 0
                },
                {
                    "sent": "Or maybe just clean it up, fix its grammar, fix it spelling.",
                    "label": 0
                },
                {
                    "sent": "People haven't really used structured prediction models for these things, yes.",
                    "label": 0
                },
                {
                    "sent": "That wasn't generation, was it?",
                    "label": 0
                },
                {
                    "sent": "Creative content logical representation assembly.",
                    "label": 0
                },
                {
                    "sent": "Sure, but it wasn't a.",
                    "label": 0
                },
                {
                    "sent": "You're right, what they were mapping to was was sort of a first order logic that they used to transform into SQL queries.",
                    "label": 0
                },
                {
                    "sent": "Right, which they could then pose to the database, so that's fair.",
                    "label": 0
                },
                {
                    "sent": "It hasn't been done.",
                    "label": 0
                },
                {
                    "sent": "It hasn't been done.",
                    "label": 0
                },
                {
                    "sent": "Open domain though.",
                    "label": 0
                },
                {
                    "sent": "That was a very restricted domain.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so I should clarify.",
                    "label": 0
                },
                {
                    "sent": "Oh cool, great looking forward.",
                    "label": 0
                },
                {
                    "sent": "Is not a solved problem, would you agree?",
                    "label": 0
                },
                {
                    "sent": "OK, so another another problem that people have been trying to work on in natural language processing but haven't really used structured prediction for yet is learning about linguistic types.",
                    "label": 0
                },
                {
                    "sent": "So we often talk about tokens words in text.",
                    "label": 0
                },
                {
                    "sent": "For example in the sentence, Barack Obama likes to talk about Barack Obama.",
                    "label": 0
                },
                {
                    "sent": "There are two instances of the token Obama, both of which instantiate the type Obama getting a little bit metaphysical here.",
                    "label": 0
                },
                {
                    "sent": "But the question is, are there things to say about the word Obama on its own?",
                    "label": 0
                },
                {
                    "sent": "Yeah, absolutely.",
                    "label": 0
                },
                {
                    "sent": "Refers to a person.",
                    "label": 0
                },
                {
                    "sent": "It's a name, it's Kenyon.",
                    "label": 0
                },
                {
                    "sent": "There are lots of things we can say about words at the type level, and so we often call these things.",
                    "label": 0
                },
                {
                    "sent": "You know, these collections of knowledge.",
                    "label": 1
                },
                {
                    "sent": "Lexicons are ontologies, and people would like to build those from data as well.",
                    "label": 0
                },
                {
                    "sent": "And you can view that as a type of structured prediction, but nobody really has.",
                    "label": 0
                },
                {
                    "sent": "Maybe maybe they have started, but I would say that we haven't really figured out how to think about that discourse.",
                    "label": 0
                },
                {
                    "sent": "Structured dialogue, structure, learning world knowledge from text.",
                    "label": 1
                },
                {
                    "sent": "These are all problems people are thinking about, but structured prediction.",
                    "label": 0
                },
                {
                    "sent": "Isn't really that old are using right now for them, but it might be.",
                    "label": 0
                },
                {
                    "sent": "I'm going to skip that.",
                    "label": 0
                },
                {
                    "sent": "Take a quick pause.",
                    "label": 0
                },
                {
                    "sent": "Any questions at this point.",
                    "label": 0
                },
                {
                    "sent": "This is the quietest group.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of 30 people I've ever spoken to.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to talk about decoding.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, here's my notation.",
                    "label": 0
                },
                {
                    "sent": "I'm going to expand it a little bit.",
                    "label": 0
                },
                {
                    "sent": "Script X is going to be a set of possible inputs.",
                    "label": 1
                },
                {
                    "sent": "Often it's Sigma star like before, and I'm going to use big bold X to mean a random variable over the inputs, and I'm going to use script.",
                    "label": 0
                },
                {
                    "sent": "Why again, to be the set of possible outputs?",
                    "label": 1
                },
                {
                    "sent": "And I'm going to use big bold.",
                    "label": 1
                },
                {
                    "sent": "Why to be the random variable over the outputs and H is going to be a prediction function that Maps X is to wise.",
                    "label": 0
                },
                {
                    "sent": "A decoder is simply an implementation of X.",
                    "label": 1
                },
                {
                    "sent": "Sorry of H that takes an accent.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And returns you why?",
                    "label": 0
                },
                {
                    "sent": "And the reason it's called decoding this may be puzzling, so I'm going to give us a very quick history lesson.",
                    "label": 0
                },
                {
                    "sent": "So nowadays in NLP we like to talk about structured prediction.",
                    "label": 0
                },
                {
                    "sent": "It's it's, you know, the hot metaphor that we love to use to describe everything we do.",
                    "label": 0
                },
                {
                    "sent": "At least I am in this talk.",
                    "label": 0
                },
                {
                    "sent": "But before that, maybe 10 years ago, the thing everybody used was source channel models.",
                    "label": 1
                },
                {
                    "sent": "Everything was seen as a source channel model and it's not a bad metaphor.",
                    "label": 0
                },
                {
                    "sent": "The idea is that you want to model jointly this.",
                    "label": 0
                },
                {
                    "sent": "These two random variables X&Y.",
                    "label": 0
                },
                {
                    "sent": "X happens to be your input.",
                    "label": 0
                },
                {
                    "sent": "Why happens to be your output and so the way you're going to do that is you're going to model first the distribution of Y, and then given why you're going to generate X.",
                    "label": 0
                },
                {
                    "sent": "So it's a little generative model, and we haven't really done anything fancy here.",
                    "label": 0
                },
                {
                    "sent": "It's just the just the chain rule, so there's the source model generated by the channel model generates X given Y, and then our job is to decode so.",
                    "label": 0
                },
                {
                    "sent": "We want to find the X that Max that is most likely given.",
                    "label": 0
                },
                {
                    "sent": "Sorry we want to find the why that is most likely given X and we're going to call that H of X and so we use Bayes rule.",
                    "label": 0
                },
                {
                    "sent": "We ignored the denominator because the marginal is constant and this is our decoder and so decoding usually refers to this sort of balancing of the source model and the channel model and this metaphor you can get.",
                    "label": 0
                },
                {
                    "sent": "You can get so much out of it.",
                    "label": 0
                },
                {
                    "sent": "The classic example that.",
                    "label": 0
                },
                {
                    "sent": "Kind of kicked off its use in NLP I think apart from apart from speech recognition where it's widespread was this great story told about 60 years ago by Warren Weaver about translation.",
                    "label": 0
                },
                {
                    "sent": "So this was back when the goal of the goal of all of natural language processing in the United States, of course, was to spy on the Russians.",
                    "label": 0
                },
                {
                    "sent": "And so the story was that they're speaking Russian.",
                    "label": 0
                },
                {
                    "sent": "That's what you hear.",
                    "label": 0
                },
                {
                    "sent": "That's the observable, that's the X.",
                    "label": 0
                },
                {
                    "sent": "But in fact, those pesky Russians they're actually thinking in English, of course, because that's what everybody thinks in, of course.",
                    "label": 0
                },
                {
                    "sent": "So what happened is that they thought this thought in Russia.",
                    "label": 0
                },
                {
                    "sent": "This devious thought.",
                    "label": 0
                },
                {
                    "sent": "Of course, in Russian, and then it passed through their broken mechanism.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry getting myself confused.",
                    "label": 0
                },
                {
                    "sent": "They have this devious thought in English, because that's the language of thought.",
                    "label": 0
                },
                {
                    "sent": "And then that pass through this garbled filtering came out as Russian.",
                    "label": 0
                },
                {
                    "sent": "And so our job is to decode it and recover with the original message was as it as it existed before it got it got garbled.",
                    "label": 0
                },
                {
                    "sent": "So why, of course, is the English the true?",
                    "label": 0
                },
                {
                    "sent": "The true language underneath?",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm going to keep the term decode.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because people in NLP like it and it's sort of abstracts away.",
                    "label": 0
                },
                {
                    "sent": "The problem of decoding from what it's often called inference, but that's a very loaded term in machine learning, so I'm not going to call it in France.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to decode.",
                    "label": 0
                },
                {
                    "sent": "So generally what what you want to do.",
                    "label": 0
                },
                {
                    "sent": "You know this this equation at the top sums up a lot of different cases of decoding, but generally it involves finding the Y that minimizes some expected risk, or some expected loss given some distribution over the true value of Y.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to use Ly XY to be.",
                    "label": 0
                },
                {
                    "sent": "The loss.",
                    "label": 0
                },
                {
                    "sent": "When you choose the cost that you pay, when you choose label, lowercase Y and the true value is that third argument given input X, so we're going to say that but true value is distributed according to some distribution, which we may or may not know, and we want to find the why that is least offensive under that distribution.",
                    "label": 0
                },
                {
                    "sent": "And if we take the most common case where the loss is 01, meaning you lose one point if you get it wrong and you don't lose any points if you get it right 01 loss, then what this gives you back is just find the most likely why Unix.",
                    "label": 0
                },
                {
                    "sent": "OK, that assumes that we have a probability model.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, given X is, which we don't always.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There's a more.",
                    "label": 0
                },
                {
                    "sent": "There's a more general framework that encompasses almost everything that people do in an LP, which is to use linear models.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to.",
                    "label": 0
                },
                {
                    "sent": "I'm going to use G to denote a feature vector function that takes an X&Y and Maps them into some Euclidean space.",
                    "label": 1
                },
                {
                    "sent": "Alright, this is probably familiar.",
                    "label": 0
                },
                {
                    "sent": "You have to remember here that this is a mapping not just to the input.",
                    "label": 0
                },
                {
                    "sent": "It's a mapping of both.",
                    "label": 0
                },
                {
                    "sent": "Everything works better if you write it out this way, so you get an input and output and you map them into this this Euclidean space and then you take an inner product with a weight vector.",
                    "label": 0
                },
                {
                    "sent": "Of the same dimensionality, of course, and then decoding involves finding the Y whose embedding paired with the given X as input has the highest score.",
                    "label": 0
                },
                {
                    "sent": "Inner product score with W. Much of what we don't much of what we want to do can be mapped to exactly this problem.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the simplest, the simplest kind of structure prediction is going to involve the specification of the input and output spaces.",
                    "label": 0
                },
                {
                    "sent": "The feature vector representation G. We're going to need a decoding algorithm that can can find the argmax over Y, and then we're going to need a method for learning the parameters W and that last part will come after the break.",
                    "label": 1
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "How do we get this to work there's?",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Really no way to say, you know you.",
                    "label": 0
                },
                {
                    "sent": "It's almost impossible to say to give some kind of generic recipe for building a decoder, and So what usually happens is that people build on work that's already been done, and use algorithms that already exists and show how they actually solve particular decoding problems.",
                    "label": 0
                },
                {
                    "sent": "So what we what often is useful is if we can show if we can.",
                    "label": 0
                },
                {
                    "sent": "If we can build our model such that G factors into local parts of the.",
                    "label": 1
                },
                {
                    "sent": "The output structure so far.",
                    "label": 0
                },
                {
                    "sent": "So so right?",
                    "label": 0
                },
                {
                    "sent": "So another.",
                    "label": 0
                },
                {
                    "sent": "Another way to put this is to say that different parts of Y are conditionally independent of each other given X or given local local arrangements of why.",
                    "label": 1
                },
                {
                    "sent": "So the most extreme version of this.",
                    "label": 1
                },
                {
                    "sent": "Imagine that we saw many problems where the input is a sequence sequence and the output is a sequence.",
                    "label": 0
                },
                {
                    "sent": "Imagine that we're trying to model us do structured prediction over over sequences.",
                    "label": 0
                },
                {
                    "sent": "So why is this sequence Y one through YN?",
                    "label": 0
                },
                {
                    "sent": "Could do is.",
                    "label": 0
                },
                {
                    "sent": "We could assume that this overall global function G breaks down into local functions, which depends just on a single Y at a time at a single position.",
                    "label": 0
                },
                {
                    "sent": "And if we were to do that, it should be fairly clear that you can build a decoder that solves this problem very fast by making a decision about each Y intern irrespective of the others, they're completely independent.",
                    "label": 0
                },
                {
                    "sent": "This choice of feature feature embedding by factoring things locali by choosing features that factor locally gives us a non structured classifier.",
                    "label": 0
                },
                {
                    "sent": "As soon as we want feature functions that can look at two or more wise at a time, we've got a structured prediction problem and there's an interdependency.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are these combinatorially optimization problems when they have a certain kind of optimal substructure can often be solved efficiently with dynamic programming?",
                    "label": 1
                },
                {
                    "sent": "Natural language processing people love dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "OK, it is.",
                    "label": 0
                },
                {
                    "sent": "It is the most commonly used to LAN.",
                    "label": 0
                },
                {
                    "sent": "It solves a huge number of problems.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, it also limits us, so at a high level, this is how I would describe it.",
                    "label": 0
                },
                {
                    "sent": "If you can make an assumption like this that your G vector breaks down into a sum over local parts of Y.",
                    "label": 1
                },
                {
                    "sent": "Then you can use dynamic programming.",
                    "label": 1
                },
                {
                    "sent": "If the parts are local enough.",
                    "label": 0
                },
                {
                    "sent": "OK, so the Viterbi algorithm is a great example.",
                    "label": 0
                },
                {
                    "sent": "I'm going to try to make this a little more concrete.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our input is X and I'm going to augment X with this extra stop symbol at the end, just to make life a little easier.",
                    "label": 0
                },
                {
                    "sent": "An R output is going to be Y and I'm going to augment it with the start symbol on the stock symbol at the beginning and end respectively, and so with with a hidden Markov model we can describe the joint distribution over X&Y using this product, which factors into parts at each time step, you first predict the Y, then you predict the X that it generates.",
                    "label": 0
                },
                {
                    "sent": "So why is the state sequence an axis?",
                    "label": 0
                },
                {
                    "sent": "The observation sequence and you can you can write down.",
                    "label": 0
                },
                {
                    "sent": "It's relatively easy to derive a set of recursive equations that will solve the decoding problem.",
                    "label": 0
                },
                {
                    "sent": "For an HMM, this is.",
                    "label": 0
                },
                {
                    "sent": "This is one way of writing the Viterbi algorithm.",
                    "label": 0
                },
                {
                    "sent": "You start out by scoring being in the start state at Timestep Zero with value one, and then you recursively find the score of being in each state at each timestep given given.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The possibilities before, so equivalently, you can think of this as finding the best path through a graph where you weight each edge according to the transition and emission probability that are appropriate at that point.",
                    "label": 0
                },
                {
                    "sent": "OK, so finding the best path through this what's often called the trellis from the green to the red State.",
                    "label": 0
                },
                {
                    "sent": "That's the Viterbi algorithm.",
                    "label": 0
                },
                {
                    "sent": "It's almost identical to Dijkstra's algorithm, depending on your implementation.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what's what's kind of interesting is that you can view this, of course, as a case of factored structured prediction where everything factors into these local parts.",
                    "label": 0
                },
                {
                    "sent": "So all I've done is I've kind of changed notation and mapped the peas, the transition and emission probabilities in the HMM into feature functions which basically check there's probably 1 feature for each transition type and one feature for each emission type, and probably one for the stopping event and you're good to go.",
                    "label": 0
                },
                {
                    "sent": "You've gotta Locali factored model and.",
                    "label": 0
                },
                {
                    "sent": "You're summing over the parts, which are the different wise at each position, and you only have to look at the previous way.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At a time and so we can actually generalize, Viterbi very easily.",
                    "label": 0
                },
                {
                    "sent": "And here we're multiplying in the local.",
                    "label": 0
                },
                {
                    "sent": "The local term for for our global score.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what you should take away from this is that having specified which which algorithm I'm going to use for decoding, that sort of implies which features were local and which features were not local.",
                    "label": 0
                },
                {
                    "sent": "So generalized Iturbi as I described it would actually permit me to augment my model with feature with many kinds of features beyond the transitions and emissions I could look at the words I could look at their spelling.",
                    "label": 0
                },
                {
                    "sent": "I could look at their shape.",
                    "label": 0
                },
                {
                    "sent": "Do they have capitals?",
                    "label": 0
                },
                {
                    "sent": "Do they include numbers?",
                    "label": 0
                },
                {
                    "sent": "Are they punctuation?",
                    "label": 0
                },
                {
                    "sent": "Anything I want I can also do anything I want with the Jason output symbols.",
                    "label": 0
                },
                {
                    "sent": "Are they the same or are they different?",
                    "label": 0
                },
                {
                    "sent": "Not just what particularly are they?",
                    "label": 0
                },
                {
                    "sent": "So that's kind of useful, but there are still some things that can't do.",
                    "label": 1
                },
                {
                    "sent": "It can't look at three consecutive output symbols.",
                    "label": 1
                },
                {
                    "sent": "For that I would need a different algorithm.",
                    "label": 0
                },
                {
                    "sent": "It can't decide whether two instances of the same word that are separated by several words are going to be the same.",
                    "label": 1
                },
                {
                    "sent": "That might be important if we were doing named entity recognition, right?",
                    "label": 0
                },
                {
                    "sent": "If I see Washington at one point in the sentence and I want to label it a person, then I see it again.",
                    "label": 0
                },
                {
                    "sent": "Having labeled it a person before might lead me to do the same thing later.",
                    "label": 0
                },
                {
                    "sent": "That's the classic case of a non local feature for the verb with am counting the number of times you've seen, why?",
                    "label": 0
                },
                {
                    "sent": "If you're doing part of speech tagging, you might prefer only to see one verb in a sentence.",
                    "label": 0
                },
                {
                    "sent": "The Viterbi algorithm doesn't give you a straightforward way of keeping track of that.",
                    "label": 0
                },
                {
                    "sent": "OK there.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A lot of other problems that reduce the dynamic programming sequence alignment.",
                    "label": 0
                },
                {
                    "sent": "Edit distance.",
                    "label": 0
                },
                {
                    "sent": "You've seen many of these before many kinds of parsing, including standard phrase structure parsing and dependencies, but also other formalisms that linguists have studied over the years, and because so because so many problems hinge on dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "There's been a lot of research lately on generic dynamic programming methods, so generalized.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Viewing dynamic programming as a generalization of logic programming and doing it with some earrings and coming up with semiring independent solvers.",
                    "label": 1
                },
                {
                    "sent": "Connecting this to search on hypergraphs.",
                    "label": 0
                },
                {
                    "sent": "Generalizing a star search on hypergraphs and coming up with approximate algorithms that handled directly the non non factoring features.",
                    "label": 1
                },
                {
                    "sent": "All of these things are getting a lot of attention so the bottom line is that to a first approximation it's often helpful to try and think of your decoding algorithm as you're designing it as dynamic programming and see if there's a way to make it work.",
                    "label": 0
                },
                {
                    "sent": "But that of course imposes certain restrictions on features.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'm going to point out a couple of other interesting cases where you can accomplish decoding using other graph algorithms.",
                    "label": 1
                },
                {
                    "sent": "Everything is in dynamic programming.",
                    "label": 0
                },
                {
                    "sent": "We're coming to realize that in NLP fairly late, so one example.",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alyssa is weighted bipartite matching, so suppose that my input is two different.",
                    "label": 1
                },
                {
                    "sent": "Sentence is X&X prime and what I want as output is a matching of the words in the two sentences.",
                    "label": 1
                },
                {
                    "sent": "So an example of this might be one sentence is English, the other is Chinese and I want to find a matching between the words.",
                    "label": 0
                },
                {
                    "sent": "I want to figure out which words in English correspond to which in Chinese finding the best matching is is solvable if.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The features on the two sequences and the matching breakdown into pairs.",
                    "label": 0
                },
                {
                    "sent": "In the matching.",
                    "label": 0
                },
                {
                    "sent": "In other words, every feature has to be associated with only one edge at a time.",
                    "label": 0
                },
                {
                    "sent": "It can't know whether other edges are being included or not.",
                    "label": 0
                },
                {
                    "sent": "And this can be solved in polytime with something called the Hungarian algorithm.",
                    "label": 0
                },
                {
                    "sent": "This was pointed out by Malamed, and generalizations of this using network flow and other techniques on graphs have been have been used for generalizations of this word alignment problem.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another nice example is spanning trees.",
                    "label": 0
                },
                {
                    "sent": "So if our input is the sentence X and our goal is to do non projective dependency parsing, then then you can show under certain assumptions about the features that finding the zero arborescence that has the highest score solves the decoding problem.",
                    "label": 0
                },
                {
                    "sent": "So as an arborescence, is it directed spanning tree?",
                    "label": 1
                },
                {
                    "sent": "We want one with X zero that special start symbol as the root.",
                    "label": 1
                },
                {
                    "sent": "And so if again, if the features breakdown by edges and only look at a word in its parent and don't know anything about the other edges that are present or not present in the tree, we can solve this with something called the Tulou Edmonds algorithm, which is from the 60s.",
                    "label": 0
                },
                {
                    "sent": "And there's a special case, a special trick that lets you solve this in quadratic time.",
                    "label": 0
                },
                {
                    "sent": "People only realized this for for the dependency parsing problem in 2005, even though we've been working on dependency parsing for a few decades before that.",
                    "label": 0
                },
                {
                    "sent": "This connection was kind of liberating and allowed us to start thinking about dependency parsing for a lot of other languages.",
                    "label": 0
                },
                {
                    "sent": "Were non productivity is important.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you know to give some examples of the kinds of things that work when we want to use the spanning tree algorithm.",
                    "label": 1
                },
                {
                    "sent": "Anything that looks at parents or children, their word classes, their lemmas, those are all fine contexts of either word, as long as you're not looking at the tree, how far apart the words are, you can include that.",
                    "label": 0
                },
                {
                    "sent": "But when you can't look at is who are my siblings, what other words are attached to my parent or who is my grandparent?",
                    "label": 0
                },
                {
                    "sent": "What are the what are the?",
                    "label": 0
                },
                {
                    "sent": "What are my grandchildren?",
                    "label": 0
                },
                {
                    "sent": "You can't look.",
                    "label": 0
                },
                {
                    "sent": "These are called 2nd order features.",
                    "label": 0
                },
                {
                    "sent": "You can't count the number of children you have.",
                    "label": 0
                },
                {
                    "sent": "So if you're a verb that tends to take a subject and an object but not an indirect object, there's no way to enforce that.",
                    "label": 0
                },
                {
                    "sent": "You only take one object on your right.",
                    "label": 0
                },
                {
                    "sent": "And phrases and lots of other things that you the entire.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If you're if you're sub of your subtree, you can't look at.",
                    "label": 0
                },
                {
                    "sent": "You can only look at one child at a time.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, not everything reduces to a well known dynamic programming algorithm or a graph of weighted graph problem.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of other techniques that are being explored, so integer linear programming has kind of taken off as a formalism that lets you solve many structured prediction problems.",
                    "label": 1
                },
                {
                    "sent": "And with good modern ILP solvers, this is often reasonably fast.",
                    "label": 0
                },
                {
                    "sent": "Re ranking has been around for awhile.",
                    "label": 0
                },
                {
                    "sent": "The idea here is that you use a simpler model.",
                    "label": 1
                },
                {
                    "sent": "GTK best list of solutions from that model and then re rank them with the.",
                    "label": 0
                },
                {
                    "sent": "Richer model that has has the ability to look at larger features, stacking belief propagation, MCMC methods, search all these things from AI and machine learning can be used to decode.",
                    "label": 0
                },
                {
                    "sent": "I think in general people in natural language processing are averse to having to use these generic techniques.",
                    "label": 0
                },
                {
                    "sent": "They'd rather find something that is particularly efficient for the problem they'd like to solve.",
                    "label": 0
                },
                {
                    "sent": "I mean, wouldn't we operate often these particular learning algorithms are particular decoding methods are linked very closely to particular learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "But in principle it need.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Opisso so right now a couple of things that are getting a lot of attention are coarse to fine decoding, where again you use some kind of simpler model to reduce your search space and iteratively branch out until you until you get to a complete Y. Decoding with multiple structures at once, which is sometimes called joint inference.",
                    "label": 1
                },
                {
                    "sent": "Suppose that I want to parse a sentence in a language like Hebrew, which has both morphology and and has hierarchical syntax.",
                    "label": 0
                },
                {
                    "sent": "We'd like to.",
                    "label": 1
                },
                {
                    "sent": "We'd like to disambiguate both the morphology and syntax at the same time, and also people are starting to think about generic tools that let you build decoders using using these techniques like dynamic programming, integer linear programming.",
                    "label": 0
                }
            ]
        }
    }
}