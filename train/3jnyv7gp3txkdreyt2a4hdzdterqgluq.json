{
    "id": "3jnyv7gp3txkdreyt2a4hdzdterqgluq",
    "title": "Metric Regression Forests for Human Pose Estimation",
    "info": {
        "author": [
            "Gerard Pons-Moll, Faculty of Electrical Engineering and Computer Science, Leibniz University of Hannover"
        ],
        "published": "April 3, 2014",
        "recorded": "September 2013",
        "category": [
            "Top->Computer Science->Computer Vision",
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/bmvc2013_pons_moll_regression/",
    "segmentation": [
        [
            "Yeah, so my name is Alphonse and this is some of the work that we did while I was at Microsoft last summer and this is joint work with the people you see down here."
        ],
        [
            "So the problem we want to solve is basically that of 3D human pose dimension.",
            "So what we want is to estimate the joint angles and jump positions given a depth image.",
            "So in this talk."
        ],
        [
            "I'm going to assume that we have a Kinect depth camera as an input an we perform single frame poses summation, so this means no tracking, but at every frame estimate the joint positions and joint angles."
        ],
        [
            "So notable work on this topic is the work by a Jamie Shotton at CPR 2011 in which they pose.",
            "The problem is classification problem.",
            "So for every depth pixel in the image they have to determine to which body part label it belongs.",
            "So basically you have like your model and you define a set of body parts and once you have performed this classification task, you assemble all these labels into a skeleton."
        ],
        [
            "In further work, they went from a classification problem to a regression problem.",
            "So instead of determining for every death pixel to which body part label it has like, they have to regress to a joint location, though not joint location like allocation on the base base mesh model.",
            "So basically, instead of like inferring body part labels, you're inferring the location on the base based model.",
            "Um?"
        ],
        [
            "So basically how it works is they train a random forest."
        ],
        [
            "So that at every depth pixel it regresses to a single location on the base model, and this creates a set of correspondences."
        ],
        [
            "The set of correspondences you have.",
            "You can set up an energy minimization in which you have to find the post parameters which are the joint angles such that the base mesh model fits the data that you have."
        ],
        [
            "So basically you set up this minimization and you optimize the parameters to fit the model into the data.",
            "OK, this is a paper that was called the Vitruvian Manifold, and this was printed CPR 2002."
        ],
        [
            "Off so basically how it works.",
            "So how these correspondence are inferred is for every depth pixel you traverse, the random forest and you apply a set of split functions."
        ],
        [
            "So you go from from top to bottom."
        ],
        [
            "At the bottom of the leaves of the forest, you're storing a single correspondence from this depth pixel to the base mesh model.",
            "So by traversing the forest for every single depth pixel."
        ],
        [
            "You can obtain a set of correspondences."
        ],
        [
            "And you do this for every single depth pixel."
        ],
        [
            "So how they trained the forest is using the following procedure like they take they take the training data.",
            "The training data are depth pixels with body part labels.",
            "So these body part labels that I showed you with the classification objective.",
            "And the goal is to decrease the entropy of this of this body part histogram classification.",
            "And So what you will do is at every node of the tree you have to select from a batch of split functions you have to select the split function that decreases the insurgente the most and how you decrease in certainties by decreasing the entropy of this body part histograms.",
            "So very low entropy means like you have all the probability in one of the classes.",
            "This low entropy and very high entropy is like all the classes are almost.",
            "Equally problem, so you would expect that the histograms look like this at the at the top of the tree like.",
            "I'm like almost uniform.",
            "So a lot of entropy here.",
            "And as you traverse the tree down like you decrease the entropy.",
            "This is how you train the tree or how they train the tree."
        ],
        [
            "But they were performing a regression problem, not the classification.",
            "So what they did is to use this train tree with this part objective and like push all the data.",
            "All the data are all the depth pixels in your training data."
        ],
        [
            "You push all the data and you collect this."
        ],
        [
            "Distributions at the leaf nodes.",
            "And at the leaf nodes, so you would obtain like a distribution of points on this base mesh model, which are the correspondences.",
            "And they base."
        ],
        [
            "We do mean shift to find.",
            "The mode of this distribution, and like this you have like at every leaf you have one mode, so you traverse the forest.",
            "You can get one correspondence on the base mesh model.",
            "Is that clear?",
            "Anne."
        ],
        [
            "So OK, So what are the current limitations of this approach?",
            "Is that?",
            "Well, first of all, like how do you find the parts and how many parts that you use so these parts are like manually drawn on the model, so you have to manually design this."
        ],
        [
            "Body parts so there's a question of how do you define this?"
        ],
        [
            "Also, using a classification objective as we have seen for a regression problem seems it doesn't seem like the right thing to do, right?"
        ],
        [
            "And also they need to define like this base mesh model in which the occlusion distances between points resemble the Jurassic distances they have to define this.",
            "That's why they use this model in this post because it resembles the Jurassic distance when the poses like this."
        ],
        [
            "Also, how do you extend this method to other models like?",
            "Imagine that you're not inferring correspondence between between depth pixels and human model, but you have a hand model or a car model Aurora face.",
            "So how does this approach of body part classification extent to this?"
        ],
        [
            "So we believe that because.",
            "Previous work is not using the proper training objective to train the forest.",
            "We can do better and this is what we want to show with this graph.",
            "So I want you to forget about the red curve and just focus on the other two curves.",
            "So here what we show is the percentage of since correctly estimated and by correctly estimated we mean that all the joints are within a certain threshold distance that we define on the horizontal axis.",
            "So for example, let's see if I can find this.",
            "Well anyways.",
            "You see the 0.1 on the horizontal axis.",
            "This means 10 centimeters and we go.",
            "We go in the graph and if we hit this point of 35%, it means that 35% of the scenes are with all the joints within 10 centimeters.",
            "OK, that's what it means.",
            "This graph.",
            "And so here we see with the blue curve we see the performance that they got in this CPR.",
            "2012 Taylor ET al.",
            "And in the green curve I show the performance we would get with perfect responses.",
            "So given the ground truth labels we set up the minimization with the perfect responses and we draw this because, like this, we have an idea of how much better we can get if we improve the correspondences.",
            "As you can see, there's a gap here, so if we can improve responses in principle, we can improve the accuracy of the position.",
            "Right, this is our hypothesis."
        ],
        [
            "So before getting into the into the algorithm, let me introduce some notation very fast.",
            "Um?",
            "So here you prime.",
            "This is no you first is the continuous space of points on this base mesh model.",
            "This is the manifold defined by the surface model and you prime is the discretisation of this of this space.",
            "Then UI is a point in this space and UI prime.",
            "A continuous point in this space and UI prime is one of these discrete points on this space, right?",
            "And then DU is just the distance metric that we define that I will explain later.",
            "And finally B Big B is the number of vertices in this space OK?"
        ],
        [
            "OK, so the question now is.",
            "What if instead of minimizing the decreasing the entropy of these body part labels, we can decrease the entropy of directly the distributions?",
            "Of correspondences on this base mesh model.",
            "Right, so we would need a way to estimate the continuous distribution of the training data of points on the base model.",
            "If we could do this, then we could decrease the continuous entropy and this is what we're after so.",
            "Basically, at every node right you have instead of this histogram of body parts you have like continuous distribution of correspondences on the base model.",
            "Like a very high entropy distribution is like uniform Anna.",
            "Very low entropy is like one that has all the mass localized in a location in this space model."
        ],
        [
            "OK, so the problem is that we have to compute this integral and we don't have an expression of the probability density when one we only have training data, right?",
            "So when you don't have an expression for the probability."
        ],
        [
            "You have to do is that every point you have to approximate this probability in some way.",
            "So one popular ways to use kernel density estimation.",
            "So at every location you estimate the probability as a some linear combination of kernel functions and the kernel functions are centered at the data points.",
            "The data points that you have."
        ],
        [
            "So basically these kernel functions are a function.",
            "We use, at least it's an exponential function of a distance metric between.",
            "The location where you're evaluating and the points in the training data.",
            "The distance metric we use.",
            "This is like a part of your design, but it should be proportional to the correlations you want, right?",
            "So if you want these two points to be highly correlated, they should be like at a small distance, and if you want this point at this point to be non correlated, there should be a big distance and for this purpose the Jurassic distances on the base mesh model work well for our purpose.",
            "We definitely want that this point at this point are uncorrelated, because this is a big mistake when we optimize.",
            "If we confuse these correspondences.",
            "Yeah, so here what I show here in this.",
            "In this, in this colored mesh is like from this this white point the distance.",
            "To all the rest of the mesh.",
            "So like blue color means small distance.",
            "Anna red color means a bigger distance.",
            "That's why the feed are further further away and they are colored in red."
        ],
        [
            "OK, so now the question how do we compute this entropy?"
        ],
        [
            "Am so basically because we're integrating over this P of you log P of you.",
            "This is the same as the expectation with respect to this distribution of the logarithm."
        ],
        [
            "So basically, to compute this we do Monte Carlo sampling.",
            "This means you loop over your training data points and at every data point you compute this logarithm of the sum of kernel functions.",
            "This is what you would have to do, right and."
        ],
        [
            "Now the question is OK, can you do this to train the forest like in a typical training data set that the typical training datasets that we were using it was consisted of 20,000 images and they roughly contain 1500 pixels foreground pixels.",
            "This boils down to having 30 million training examples.",
            "Now if you have to do this."
        ],
        [
            "Computation for every split function at every node of the tree for all the data.",
            "This complexity is N squared and it would never finish.",
            "It would be like way too expensive so the native way of doing this.",
            "It's something that you cannot afford, like it would never finish."
        ],
        [
            "So basically what we do is we exploit the fact that our space is confined and what we do is to discretize the space in a set of vertex locations.",
            "And basically what we do is to compute the kernel contributions of every point to every other point, and we store this in a matrix matrix of kernel distances.",
            "Right, so in this matrix will have for every single data point the contribution kernel contribution to any other.",
            "Every other data point."
        ],
        [
            "OK, so now at a given node in the tree, we have like a certain empirical distribution and the question is how do we go from this empirical distribution to the continuous and approximation of the continuous distribution?"
        ],
        [
            "So what we do is we generate a histogram of vertex histogram right?",
            "So we like vertex system and then we multiply every kernel that with these kernels that I showed you that we stored and we moved by every kernel by the bin count of the histogram.",
            "So you can imagine there's a kernel at every single point and you have to push up or down the kernels according to the histogram count and like this."
        ],
        [
            "In doing this, you obtain like continuous distribution."
        ],
        [
            "So basically we are approximating this distribution with this formula, which is this.",
            "Peas are the histogram counts multiplied by the kernels that are precomputed."
        ],
        [
            "OK, so now we have to compute the entropy.",
            "So basically.",
            "We have to sum over every single discrete location that we have.",
            "This expression we have for the probability logarithm of the probability and the good thing about."
        ],
        [
            "Doing this is that now the complexity is N plus be multiplied by N because N is to build the histogram.",
            "The vertex histogram and then B is the number of vertice is.",
            "Multiplied by the number of neighbors that this is a choice that you like.",
            "If the kernels fall off very fast, then you would know you don't need to.",
            "Use all the neighbors, just the close ones.",
            "So we went from N squared to something that is much more because the number of vertices is much less than the training data."
        ],
        [
            "OK, coming to the results now.",
            "This is very similar to what I showed you before.",
            "Now I want to show you the percentage of correct correspondences.",
            "It's the same thing like we count the number of correspondence that are within a certain threshold distance from the ground truth.",
            "And the metric we use here is the genetic distance on the base mesh model.",
            "What I show here on the top.",
            "OK so basically in blue what we show is when we do this perform this.",
            "Regression using the bot based objective and in red is when we use the new training objectives are continuous continuous thing and as we expected, because we're doing, we're doing the proper thing, we are obtaining a benefit and we obtain much better correspondences, which is what you can see here.",
            "That the red line is above.",
            "This is with the tree of depth 20."
        ],
        [
            "OK, now what do we do with this correspondence is we want the correspondence is a better one?",
            "Would think that it would translate to better post, right?",
            "So?"
        ],
        [
            "Who is like what I explained before we refer to responses like this and we."
        ],
        [
            "Optimize the post parameters.",
            "Using these new correspondences, we use an energy function that is the same that was used at the Victory Manifold Paper."
        ],
        [
            "OK, so this is the post result.",
            "So now it's the joint accuracy.",
            "The number of scenes with all the joints correctly estimated.",
            "And again because we have better correspondences it translates to better posts, which is good, which is what we were after basically."
        ],
        [
            "We also tried to perform iterative closest point.",
            "This means, like you estimate the model and then your re estimate the model parameters.",
            "You re estimate the correspondences and you iterate this a couple of Times Now competent several times until it converges.",
            "An interesting thing here is that so in the dashed lines.",
            "You see, this is the graph we had with a single iteration, so now I see P and then the continuous lines is after the convergence with PCP.",
            "And the interesting thing is that when we start with better correspondence is it ends with better results even after doing AICP.",
            "So because we started with better correspondences even after doing AICP continues to perform better.",
            "So this is.",
            "Nothing that is encouraging."
        ],
        [
            "I'm.",
            "So coming back to the first graph, we saw that the blue thing is what they obtained with the trivial Manifold paper.",
            "The green thing was with perfect responses, and the yellow thing or the yellow graph, the yellow orange graph is what we get with our improvements.",
            "So this means that it's a significant step towards, you know, having perfect responses.",
            "So we believe this is a significant step forward.",
            "Um?"
        ],
        [
            "OK, so coming to the conclusions already.",
            "We showed that the regression forests can be trained efficiently using like a proper training objective, that these results into better Model 2 image correspondences now.",
            "This is a metric embedding in which they had to define the base mesh model like this.",
            "This is no longer needed because we can compute the genetic distances in any pose.",
            "We only need like a connectivity graph of the mesh.",
            "This will allow us to apply this to any other regression task and you have in which you have targets that lie in a metric space.",
            "So as long as you can define the targets and the distance between these targets, you can apply this method.",
            "Yeah, so this is."
        ],
        [
            "I would like to conclude here and if you have any questions I'll be happy to answer."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, so my name is Alphonse and this is some of the work that we did while I was at Microsoft last summer and this is joint work with the people you see down here.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the problem we want to solve is basically that of 3D human pose dimension.",
                    "label": 1
                },
                {
                    "sent": "So what we want is to estimate the joint angles and jump positions given a depth image.",
                    "label": 0
                },
                {
                    "sent": "So in this talk.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to assume that we have a Kinect depth camera as an input an we perform single frame poses summation, so this means no tracking, but at every frame estimate the joint positions and joint angles.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So notable work on this topic is the work by a Jamie Shotton at CPR 2011 in which they pose.",
                    "label": 0
                },
                {
                    "sent": "The problem is classification problem.",
                    "label": 0
                },
                {
                    "sent": "So for every depth pixel in the image they have to determine to which body part label it belongs.",
                    "label": 0
                },
                {
                    "sent": "So basically you have like your model and you define a set of body parts and once you have performed this classification task, you assemble all these labels into a skeleton.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In further work, they went from a classification problem to a regression problem.",
                    "label": 0
                },
                {
                    "sent": "So instead of determining for every death pixel to which body part label it has like, they have to regress to a joint location, though not joint location like allocation on the base base mesh model.",
                    "label": 0
                },
                {
                    "sent": "So basically, instead of like inferring body part labels, you're inferring the location on the base based model.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically how it works is they train a random forest.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that at every depth pixel it regresses to a single location on the base model, and this creates a set of correspondences.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The set of correspondences you have.",
                    "label": 0
                },
                {
                    "sent": "You can set up an energy minimization in which you have to find the post parameters which are the joint angles such that the base mesh model fits the data that you have.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically you set up this minimization and you optimize the parameters to fit the model into the data.",
                    "label": 0
                },
                {
                    "sent": "OK, this is a paper that was called the Vitruvian Manifold, and this was printed CPR 2002.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Off so basically how it works.",
                    "label": 0
                },
                {
                    "sent": "So how these correspondence are inferred is for every depth pixel you traverse, the random forest and you apply a set of split functions.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you go from from top to bottom.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the bottom of the leaves of the forest, you're storing a single correspondence from this depth pixel to the base mesh model.",
                    "label": 0
                },
                {
                    "sent": "So by traversing the forest for every single depth pixel.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can obtain a set of correspondences.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And you do this for every single depth pixel.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how they trained the forest is using the following procedure like they take they take the training data.",
                    "label": 0
                },
                {
                    "sent": "The training data are depth pixels with body part labels.",
                    "label": 0
                },
                {
                    "sent": "So these body part labels that I showed you with the classification objective.",
                    "label": 1
                },
                {
                    "sent": "And the goal is to decrease the entropy of this of this body part histogram classification.",
                    "label": 0
                },
                {
                    "sent": "And So what you will do is at every node of the tree you have to select from a batch of split functions you have to select the split function that decreases the insurgente the most and how you decrease in certainties by decreasing the entropy of this body part histograms.",
                    "label": 0
                },
                {
                    "sent": "So very low entropy means like you have all the probability in one of the classes.",
                    "label": 0
                },
                {
                    "sent": "This low entropy and very high entropy is like all the classes are almost.",
                    "label": 0
                },
                {
                    "sent": "Equally problem, so you would expect that the histograms look like this at the at the top of the tree like.",
                    "label": 0
                },
                {
                    "sent": "I'm like almost uniform.",
                    "label": 0
                },
                {
                    "sent": "So a lot of entropy here.",
                    "label": 0
                },
                {
                    "sent": "And as you traverse the tree down like you decrease the entropy.",
                    "label": 0
                },
                {
                    "sent": "This is how you train the tree or how they train the tree.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But they were performing a regression problem, not the classification.",
                    "label": 0
                },
                {
                    "sent": "So what they did is to use this train tree with this part objective and like push all the data.",
                    "label": 0
                },
                {
                    "sent": "All the data are all the depth pixels in your training data.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You push all the data and you collect this.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Distributions at the leaf nodes.",
                    "label": 0
                },
                {
                    "sent": "And at the leaf nodes, so you would obtain like a distribution of points on this base mesh model, which are the correspondences.",
                    "label": 0
                },
                {
                    "sent": "And they base.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We do mean shift to find.",
                    "label": 1
                },
                {
                    "sent": "The mode of this distribution, and like this you have like at every leaf you have one mode, so you traverse the forest.",
                    "label": 0
                },
                {
                    "sent": "You can get one correspondence on the base mesh model.",
                    "label": 0
                },
                {
                    "sent": "Is that clear?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So OK, So what are the current limitations of this approach?",
                    "label": 1
                },
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "Well, first of all, like how do you find the parts and how many parts that you use so these parts are like manually drawn on the model, so you have to manually design this.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Body parts so there's a question of how do you define this?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, using a classification objective as we have seen for a regression problem seems it doesn't seem like the right thing to do, right?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And also they need to define like this base mesh model in which the occlusion distances between points resemble the Jurassic distances they have to define this.",
                    "label": 0
                },
                {
                    "sent": "That's why they use this model in this post because it resembles the Jurassic distance when the poses like this.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Also, how do you extend this method to other models like?",
                    "label": 1
                },
                {
                    "sent": "Imagine that you're not inferring correspondence between between depth pixels and human model, but you have a hand model or a car model Aurora face.",
                    "label": 0
                },
                {
                    "sent": "So how does this approach of body part classification extent to this?",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we believe that because.",
                    "label": 0
                },
                {
                    "sent": "Previous work is not using the proper training objective to train the forest.",
                    "label": 0
                },
                {
                    "sent": "We can do better and this is what we want to show with this graph.",
                    "label": 1
                },
                {
                    "sent": "So I want you to forget about the red curve and just focus on the other two curves.",
                    "label": 0
                },
                {
                    "sent": "So here what we show is the percentage of since correctly estimated and by correctly estimated we mean that all the joints are within a certain threshold distance that we define on the horizontal axis.",
                    "label": 0
                },
                {
                    "sent": "So for example, let's see if I can find this.",
                    "label": 0
                },
                {
                    "sent": "Well anyways.",
                    "label": 0
                },
                {
                    "sent": "You see the 0.1 on the horizontal axis.",
                    "label": 0
                },
                {
                    "sent": "This means 10 centimeters and we go.",
                    "label": 0
                },
                {
                    "sent": "We go in the graph and if we hit this point of 35%, it means that 35% of the scenes are with all the joints within 10 centimeters.",
                    "label": 0
                },
                {
                    "sent": "OK, that's what it means.",
                    "label": 0
                },
                {
                    "sent": "This graph.",
                    "label": 0
                },
                {
                    "sent": "And so here we see with the blue curve we see the performance that they got in this CPR.",
                    "label": 0
                },
                {
                    "sent": "2012 Taylor ET al.",
                    "label": 0
                },
                {
                    "sent": "And in the green curve I show the performance we would get with perfect responses.",
                    "label": 0
                },
                {
                    "sent": "So given the ground truth labels we set up the minimization with the perfect responses and we draw this because, like this, we have an idea of how much better we can get if we improve the correspondences.",
                    "label": 1
                },
                {
                    "sent": "As you can see, there's a gap here, so if we can improve responses in principle, we can improve the accuracy of the position.",
                    "label": 0
                },
                {
                    "sent": "Right, this is our hypothesis.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So before getting into the into the algorithm, let me introduce some notation very fast.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So here you prime.",
                    "label": 0
                },
                {
                    "sent": "This is no you first is the continuous space of points on this base mesh model.",
                    "label": 1
                },
                {
                    "sent": "This is the manifold defined by the surface model and you prime is the discretisation of this of this space.",
                    "label": 1
                },
                {
                    "sent": "Then UI is a point in this space and UI prime.",
                    "label": 1
                },
                {
                    "sent": "A continuous point in this space and UI prime is one of these discrete points on this space, right?",
                    "label": 0
                },
                {
                    "sent": "And then DU is just the distance metric that we define that I will explain later.",
                    "label": 1
                },
                {
                    "sent": "And finally B Big B is the number of vertices in this space OK?",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the question now is.",
                    "label": 0
                },
                {
                    "sent": "What if instead of minimizing the decreasing the entropy of these body part labels, we can decrease the entropy of directly the distributions?",
                    "label": 0
                },
                {
                    "sent": "Of correspondences on this base mesh model.",
                    "label": 0
                },
                {
                    "sent": "Right, so we would need a way to estimate the continuous distribution of the training data of points on the base model.",
                    "label": 0
                },
                {
                    "sent": "If we could do this, then we could decrease the continuous entropy and this is what we're after so.",
                    "label": 0
                },
                {
                    "sent": "Basically, at every node right you have instead of this histogram of body parts you have like continuous distribution of correspondences on the base model.",
                    "label": 0
                },
                {
                    "sent": "Like a very high entropy distribution is like uniform Anna.",
                    "label": 0
                },
                {
                    "sent": "Very low entropy is like one that has all the mass localized in a location in this space model.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so the problem is that we have to compute this integral and we don't have an expression of the probability density when one we only have training data, right?",
                    "label": 0
                },
                {
                    "sent": "So when you don't have an expression for the probability.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You have to do is that every point you have to approximate this probability in some way.",
                    "label": 0
                },
                {
                    "sent": "So one popular ways to use kernel density estimation.",
                    "label": 1
                },
                {
                    "sent": "So at every location you estimate the probability as a some linear combination of kernel functions and the kernel functions are centered at the data points.",
                    "label": 0
                },
                {
                    "sent": "The data points that you have.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically these kernel functions are a function.",
                    "label": 0
                },
                {
                    "sent": "We use, at least it's an exponential function of a distance metric between.",
                    "label": 0
                },
                {
                    "sent": "The location where you're evaluating and the points in the training data.",
                    "label": 0
                },
                {
                    "sent": "The distance metric we use.",
                    "label": 0
                },
                {
                    "sent": "This is like a part of your design, but it should be proportional to the correlations you want, right?",
                    "label": 0
                },
                {
                    "sent": "So if you want these two points to be highly correlated, they should be like at a small distance, and if you want this point at this point to be non correlated, there should be a big distance and for this purpose the Jurassic distances on the base mesh model work well for our purpose.",
                    "label": 0
                },
                {
                    "sent": "We definitely want that this point at this point are uncorrelated, because this is a big mistake when we optimize.",
                    "label": 0
                },
                {
                    "sent": "If we confuse these correspondences.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so here what I show here in this.",
                    "label": 0
                },
                {
                    "sent": "In this, in this colored mesh is like from this this white point the distance.",
                    "label": 0
                },
                {
                    "sent": "To all the rest of the mesh.",
                    "label": 0
                },
                {
                    "sent": "So like blue color means small distance.",
                    "label": 0
                },
                {
                    "sent": "Anna red color means a bigger distance.",
                    "label": 0
                },
                {
                    "sent": "That's why the feed are further further away and they are colored in red.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now the question how do we compute this entropy?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Am so basically because we're integrating over this P of you log P of you.",
                    "label": 0
                },
                {
                    "sent": "This is the same as the expectation with respect to this distribution of the logarithm.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So basically, to compute this we do Monte Carlo sampling.",
                    "label": 1
                },
                {
                    "sent": "This means you loop over your training data points and at every data point you compute this logarithm of the sum of kernel functions.",
                    "label": 1
                },
                {
                    "sent": "This is what you would have to do, right and.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now the question is OK, can you do this to train the forest like in a typical training data set that the typical training datasets that we were using it was consisted of 20,000 images and they roughly contain 1500 pixels foreground pixels.",
                    "label": 0
                },
                {
                    "sent": "This boils down to having 30 million training examples.",
                    "label": 1
                },
                {
                    "sent": "Now if you have to do this.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Computation for every split function at every node of the tree for all the data.",
                    "label": 0
                },
                {
                    "sent": "This complexity is N squared and it would never finish.",
                    "label": 1
                },
                {
                    "sent": "It would be like way too expensive so the native way of doing this.",
                    "label": 0
                },
                {
                    "sent": "It's something that you cannot afford, like it would never finish.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So basically what we do is we exploit the fact that our space is confined and what we do is to discretize the space in a set of vertex locations.",
                    "label": 1
                },
                {
                    "sent": "And basically what we do is to compute the kernel contributions of every point to every other point, and we store this in a matrix matrix of kernel distances.",
                    "label": 0
                },
                {
                    "sent": "Right, so in this matrix will have for every single data point the contribution kernel contribution to any other.",
                    "label": 0
                },
                {
                    "sent": "Every other data point.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now at a given node in the tree, we have like a certain empirical distribution and the question is how do we go from this empirical distribution to the continuous and approximation of the continuous distribution?",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we do is we generate a histogram of vertex histogram right?",
                    "label": 0
                },
                {
                    "sent": "So we like vertex system and then we multiply every kernel that with these kernels that I showed you that we stored and we moved by every kernel by the bin count of the histogram.",
                    "label": 0
                },
                {
                    "sent": "So you can imagine there's a kernel at every single point and you have to push up or down the kernels according to the histogram count and like this.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In doing this, you obtain like continuous distribution.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So basically we are approximating this distribution with this formula, which is this.",
                    "label": 0
                },
                {
                    "sent": "Peas are the histogram counts multiplied by the kernels that are precomputed.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now we have to compute the entropy.",
                    "label": 0
                },
                {
                    "sent": "So basically.",
                    "label": 0
                },
                {
                    "sent": "We have to sum over every single discrete location that we have.",
                    "label": 0
                },
                {
                    "sent": "This expression we have for the probability logarithm of the probability and the good thing about.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doing this is that now the complexity is N plus be multiplied by N because N is to build the histogram.",
                    "label": 0
                },
                {
                    "sent": "The vertex histogram and then B is the number of vertice is.",
                    "label": 0
                },
                {
                    "sent": "Multiplied by the number of neighbors that this is a choice that you like.",
                    "label": 0
                },
                {
                    "sent": "If the kernels fall off very fast, then you would know you don't need to.",
                    "label": 0
                },
                {
                    "sent": "Use all the neighbors, just the close ones.",
                    "label": 0
                },
                {
                    "sent": "So we went from N squared to something that is much more because the number of vertices is much less than the training data.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, coming to the results now.",
                    "label": 0
                },
                {
                    "sent": "This is very similar to what I showed you before.",
                    "label": 0
                },
                {
                    "sent": "Now I want to show you the percentage of correct correspondences.",
                    "label": 0
                },
                {
                    "sent": "It's the same thing like we count the number of correspondence that are within a certain threshold distance from the ground truth.",
                    "label": 0
                },
                {
                    "sent": "And the metric we use here is the genetic distance on the base mesh model.",
                    "label": 0
                },
                {
                    "sent": "What I show here on the top.",
                    "label": 0
                },
                {
                    "sent": "OK so basically in blue what we show is when we do this perform this.",
                    "label": 0
                },
                {
                    "sent": "Regression using the bot based objective and in red is when we use the new training objectives are continuous continuous thing and as we expected, because we're doing, we're doing the proper thing, we are obtaining a benefit and we obtain much better correspondences, which is what you can see here.",
                    "label": 0
                },
                {
                    "sent": "That the red line is above.",
                    "label": 0
                },
                {
                    "sent": "This is with the tree of depth 20.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now what do we do with this correspondence is we want the correspondence is a better one?",
                    "label": 0
                },
                {
                    "sent": "Would think that it would translate to better post, right?",
                    "label": 0
                },
                {
                    "sent": "So?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who is like what I explained before we refer to responses like this and we.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Optimize the post parameters.",
                    "label": 0
                },
                {
                    "sent": "Using these new correspondences, we use an energy function that is the same that was used at the Victory Manifold Paper.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is the post result.",
                    "label": 0
                },
                {
                    "sent": "So now it's the joint accuracy.",
                    "label": 0
                },
                {
                    "sent": "The number of scenes with all the joints correctly estimated.",
                    "label": 0
                },
                {
                    "sent": "And again because we have better correspondences it translates to better posts, which is good, which is what we were after basically.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also tried to perform iterative closest point.",
                    "label": 1
                },
                {
                    "sent": "This means, like you estimate the model and then your re estimate the model parameters.",
                    "label": 0
                },
                {
                    "sent": "You re estimate the correspondences and you iterate this a couple of Times Now competent several times until it converges.",
                    "label": 0
                },
                {
                    "sent": "An interesting thing here is that so in the dashed lines.",
                    "label": 0
                },
                {
                    "sent": "You see, this is the graph we had with a single iteration, so now I see P and then the continuous lines is after the convergence with PCP.",
                    "label": 0
                },
                {
                    "sent": "And the interesting thing is that when we start with better correspondence is it ends with better results even after doing AICP.",
                    "label": 1
                },
                {
                    "sent": "So because we started with better correspondences even after doing AICP continues to perform better.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "Nothing that is encouraging.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm.",
                    "label": 0
                },
                {
                    "sent": "So coming back to the first graph, we saw that the blue thing is what they obtained with the trivial Manifold paper.",
                    "label": 0
                },
                {
                    "sent": "The green thing was with perfect responses, and the yellow thing or the yellow graph, the yellow orange graph is what we get with our improvements.",
                    "label": 0
                },
                {
                    "sent": "So this means that it's a significant step towards, you know, having perfect responses.",
                    "label": 0
                },
                {
                    "sent": "So we believe this is a significant step forward.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so coming to the conclusions already.",
                    "label": 0
                },
                {
                    "sent": "We showed that the regression forests can be trained efficiently using like a proper training objective, that these results into better Model 2 image correspondences now.",
                    "label": 1
                },
                {
                    "sent": "This is a metric embedding in which they had to define the base mesh model like this.",
                    "label": 1
                },
                {
                    "sent": "This is no longer needed because we can compute the genetic distances in any pose.",
                    "label": 1
                },
                {
                    "sent": "We only need like a connectivity graph of the mesh.",
                    "label": 0
                },
                {
                    "sent": "This will allow us to apply this to any other regression task and you have in which you have targets that lie in a metric space.",
                    "label": 0
                },
                {
                    "sent": "So as long as you can define the targets and the distance between these targets, you can apply this method.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so this is.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I would like to conclude here and if you have any questions I'll be happy to answer.",
                    "label": 0
                }
            ]
        }
    }
}