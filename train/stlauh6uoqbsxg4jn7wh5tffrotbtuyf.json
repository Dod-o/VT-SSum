{
    "id": "stlauh6uoqbsxg4jn7wh5tffrotbtuyf",
    "title": "Ontology Driven Extraction of Research Processes",
    "info": {
        "author": [
            "Vayianos Pertsas, Athens University of Economics and Business"
        ],
        "published": "Nov. 22, 2018",
        "recorded": "October 2018",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2018_pertsas_ontology_extraction_research/",
    "segmentation": [
        [
            "Hello everyone, I began as purchase and today will be presenting the work that along with panels custom Douglas and even other jobless is entitled ontology driven extraction of research processes."
        ],
        [
            "Let's consider for a moment the following questions say that during our research we need to find information, for example regarding which papers are just a given problem or how was a given problem solved or who has worked on a particular topic and what do we know him about about him or about their work or even more concretely, has a particular experiment be conducted which steps were taken with which order, what results?",
            "Answering these."
        ],
        [
            "And today it relies heavily on the use of search engines.",
            "We use our favorite search engine in order to find relative articles, but then we have to read the article by ourselves in order to figure out what kind of research activities are described.",
            "Which methodology is followed, what results have been produced with resources have been used, etc.",
            "Then we might also find other relevant information from other digital resources in the web and combine all of the above in order to gain knowledge and continue.",
            "So based."
        ],
        [
            "The tools that are available to us today are either large digital research infrastructures that aiming to gather resources, tools and services, or search engines like Google Scholar or Semantic Scholar.",
            "That however, others the the problem by exploiting mainly mostly the articles meta data or traversing through authorization graphs.",
            "Hence it is question how well these offered services fit.",
            "The actual research lifecycle and address information needs of scholars well."
        ],
        [
            "Hold it to address this challenge.",
            "We the plan for today, is to briefly describe how we can establish a conceptual model that provides a flexible framework for modeling research processes.",
            "And then we'll show how, based on this conceptual model, we can create knowledge bases that are actually exploding.",
            "The available digital resources in order to gather, combine an inferno knowledge that supports colors in their work.",
            "Now, the conceptual framework that we created."
        ],
        [
            "We call it scholarly ontology and it is designed so that it can be that I will be able to answer questions of the form.",
            "Who does what, why, where, and how, and to do so, the ontology revolves around the central notion of activity, which can be any research process As for example a biological experiment or an archaeological excavation or a medical survey.",
            "And we approach this notion of activity through various perspectives that highlight each time relevant parts of these methods accordingly.",
            "So for example, we have the procedure perspective with methods such as with entities such as a method covering the how part.",
            "Or the agency perspective with entities such as actors or goals that answer The Who or the Y or the research perspective with entities such as account, an item which can be any research article, for example that answer the what the questions."
        ],
        [
            "Now our task for today will be to focus mostly on the active the entity of the activity and we will do so will try to extract the textual chunks representing those activities from the actual text of the research articles.",
            "Then after having identified those activities we want to interrelate them based on their sequential order as it is as it is right by the textual descriptions from the text, but it won't stop there after having interrelate those activities we want to associate them with contextual information that we gather.",
            "From other resources or from the articles metadata and publish, everything's linked data at the end."
        ],
        [
            "Now to do so, the first step and information extraction task we created a train, an unlabeled training set from 50,000 research articles, and we use these training set in order to create word embeddings of 100 dimensions.",
            "Both embeddings of 25 dimensions.",
            "By replacing the word with its relevant post tag and dependencies, buildings of 25 dimensions using the word web algorithm from our experiments, we observed that there is a.",
            "We had a bit our embeddings created in a domain specific.",
            "Corpora had a better performance than the only that the other train models that you can find in the in other sources that are trained with generic purpose corpora such as Wikipedia are news feeds.",
            "Further on, we created a labeled training set consisting."
        ],
        [
            "Deriving from 50 research articles with a broad discipline coverage that comprised approximately about 2700 entities and about 1000 relations.",
            "Now we wanted to expose our classifiers in a variety of writing styles.",
            "Hence, we created three discipline specific test sets with 15 research articles, each from domains of digital humanities, bioinformatics, and medicine comprising approximately about 600 entities per set and 200 relations in digital humanities.",
            "500 delicious bioinformatics and 600 elation in medicine test.",
            "In addition to that, in order to assure that this that our results given the size of our tests were statistically significant, we performed approximate randomization tests using the buffer only correction in order to lower the threshold of the P value.",
            "Depending on the number of the systems that we are comparing.",
            "Now someone would think OK, activity extraction.",
            "This sounds great."
        ],
        [
            "Much like a classical named entity recognition problem, why don't we just use typical named entity recognition methodology?",
            "Train it on this training set and see how it performs.",
            "Well, we did that.",
            "We use the classical name data recognition method based on a sliding window classifier with support vector machines and window size of 15 tokens, typically named entity recognition tasks and features like word embeddings, post embeddings, and binary features representing the surface form of tokens.",
            "But as you can see here that the results that we got.",
            "Are far from satisfying, so this made us think why?",
            "What are the characteristics of our task that differentiated from classical name letter recognition tasks?"
        ],
        [
            "Well, it turns out that the research process is the entities that we are dealing.",
            "Here are entities of high semantic complexity.",
            "This means that instead of being able to represent them with proper names or one or two words at as is the case with usual named entities such as persons, locations, organizations.",
            "Here we're dealing with actual textual descriptions of variable length, sometimes far larger than usual nerve problems, where the actual difference of tokens inside the textual chunks is practically relevant, and most of that some of the times the entity can cover almost the entire sentence.",
            "On top of that, based on the syntactic analysis that we perform based on our data sets where we found out that the syntactic structure of these extracted entities can be generalized into some lexical syntactic part patterns, and in a decent chunk boundaries almost match specific dependency subtrees inside the dependency tree of a sentence, and apart from the token, the order of appearance of the tokens inside the sentence, which is linear, there is another sequence.",
            "This based on the syntactic dependency of war.",
            "One word to another which cannot be linear doesn't necessarily follow the order of appearance, which could also influence our results and then provide some good indeed."
        ],
        [
            "Nation, so with these insights we preceded in our future the creation of our feature space that comprises, as I said earlier of embeddings word post independency and binary features, where we use the 57 features for 100 coding of post tags or 71 features of 100 independence attacks.",
            "When we're using random forest method for classification and 14 features of binary features for surface representation that examine, for example, whether the talking is capitalized, digit email or number etc.",
            "And then smart features that examine the participation of the token inside a special lexical syntactic patterns that we have identified now as."
        ],
        [
            "Example of these MoD features consider this sentence.",
            "Parsing the sentence sewer syntactic parser such as Spacey that we use."
        ],
        [
            "In our work, use dependency tree of this kind Now you."
        ],
        [
            "Sing some more specific lexical syntactic patterns we can isolate As for example, look for the subtree of the dependency tree that has a root.",
            "A verb in the past sense in the past tense with the subject of first person singular or plural.",
            "We can isolate this specific textual chunks that provide good indicators for the existence of activities inside the sentence."
        ],
        [
            "Now, the methods that we created are all based on the token based classification using sliding window classifiers.",
            "The Windows side that we chose after appropriate testing with the validation set.",
            "Proved to be 30 tokens around the token of the examination.",
            "For that we use logistic regression, support vector machines and random forest for classification and starting from the basics.",
            "The first method that we present consists only of word and post embeddings plus the 14 binary features for surface representation using logistic regression support vector machines."
        ],
        [
            "For the second methodology, we added the Dependencia Beddings and the 10 smart features.",
            "Again, all of the rest of the features were kept the same.",
            "In the third method we use random forests and because of that, instead of using, embeddings will use the one hotel encodings of post independency tags be keeping all the rest of the features the same.",
            "Well, in the."
        ],
        [
            "4th Method we dropped completely the word embeddings.",
            "We just used post independence embeddings, keeping the rest of their features the same, but this time instead of taking into account only one token we took into account the syntactic sequence of the tokens in a 3 Level 3 level syntactic sequence.",
            "Meaning for each token we took into account it's syntactic Father an in syntactic Gran Father.",
            "Again, we use logistic regression support vector machines.",
            "Now as."
        ],
        [
            "Another the methodology we used, we implemented the pipeline architecture where in the ocean behind that is to split the original problem into two separate smaller subtasks and so that each classifier having to deal with with an easier problem would perform a Dick.",
            "Oddly enough, so that the contact information of the results would achieve better performance.",
            "Having said that, the first classifier in a pipeline performs settings classification and tries to identify, for each sentence whether there is an activity inside or not.",
            "Now, for the sentences that."
        ],
        [
            "The first classifier says that there is an activity.",
            "We have a second classifier performing token classification in order to detect the boundaries of the activity inside the sentence."
        ],
        [
            "Now for evaluation."
        ],
        [
            "And we performed two experiments, one token based and one entity based.",
            "The baseline that we described earlier is in the first language, and as you can see, all of our methods with pipeline performing the best as well, the same results we got in the entity based evaluation.",
            "But as you can see from the results of all of our methods being better than the baseline, the initial hypothesis was correct.",
            "The fact meaning the fact that.",
            "The size of the window classifier an the dependencies syntactic dependencies, or the participation in special syntactic features or the the order based on the syntactic sequence of tokens are all factors that should be considered when someone deals with the extraction of entities of this kind of complex."
        ],
        [
            "Sting now moving on to sequence relay."
        ],
        [
            "Extraction, let's say that we have this sample text here and we have somehow identified."
        ],
        [
            "All the entities, all of the activities inside the text.",
            "Our task here would be for each chunk bounded by any of two activities inside the text to check whether the whether the relation follow."
        ],
        [
            "Act two act.",
            "One meaning that the activity of the last boundary follows the activity in the in the beginning of the chunk without any other activities interfering inside.",
            "Whether this relation holds or not.",
            "We do that for every chunk.",
            "So the methodology that we have implemented the first 2 use the average of post independency embeddings per dimension for all of the tokens inside the chunk plus five binary features for structural structure attributes of the text that examined.",
            "For example, whether there are other activities inside the chunk, whether we have separate sentences or neighboring sentences of whether the activities are in different paragraphs etc.",
            "6 binary features for dependence of lexical sequential indicators, meaning tokens As for example.",
            "First Secondly, following subsequently etc that are syntactically dependent with any of the tokens inside of our bounding entities.",
            "And for that we use logistic regression and support vector machines.",
            "In the third methodology we use."
        ],
        [
            "Random forests, and because of that, instead of taking the average, we took the sum of post independency.",
            "One hot encoding's per dimension, keeping the list of the features the same."
        ],
        [
            "Evaluation"
        ],
        [
            "Again, all of our methods showed better performance with a simple rule based baseline that we created for demonstration purposes here.",
            "And as you can see, random Forest performs the best the best of the three, reaching performances up to 0.8, nine F1 score in all renal test set."
        ],
        [
            "Now, having either related or entities, we proceed with your eye creation and for the entities with proper name, meaning those that we extract from the articles metadata, such as persons, organizations, articles, topics, etc.",
            "the UI is created based on the namespace that we provide the entity type according to the semantics of scholarly ontology and the Entity ID.",
            "As for the entities."
        ],
        [
            "That extracted from text, such as the activities that we are dealing here.",
            "the UI is based on the namespace, the entity type of the source, meaning the article ID and boundary offsets showing the where the actual textual chunk is inside the text.",
            "Of course we support our DFS and older models.",
            "We also provide or sit integration so that for each author we can retrieve through the."
        ],
        [
            "Table API, his ORCID, ID, and any extra information such as bio or project that he has participated and having said that, as a recapitulation, what we try to show here is that combining the goods of both worlds, if you say if you say machine learning extraction with powerful feature engineering or one hand and ontologies an linked data technologies on the other allows us starting from sentences like that to be able to.",
            "Identify."
        ],
        [
            "Actual chunks of interest.",
            "Associate them with the particular entities of our hodology.",
            "Into"
        ],
        [
            "Relate them based on their sequential order associated with other contextual information that we gather from articles made the data."
        ],
        [
            "Or other deposit repository, such as receipt.",
            "And link to them into."
        ],
        [
            "Existing knowledge that we have already derived from the same article or other articles in order to create such knowledge graphs now having created such a knowledge graph, it's easy for us."
        ],
        [
            "To identify and semantic paths of interest, such As for example, retrieve all of the articles.",
            "All of the activities based on the same author or for example."
        ],
        [
            "Will retrieve the activities from that are exist in the same article and doing so allows for the questions that we saw in the beginning of the presentation to be answered automatically."
        ],
        [
            "Thank you."
        ],
        [
            "I love this computer.",
            "Is the definition of authority basic?"
        ],
        [
            "About your possible entries, I think we all agree that would be great because certain point we can, you know, except medical technology, that is a quote ever.",
            "But then it seems that you focus just on active.",
            "Yes, do you plan now digging into activity for extracting the best and in so it's just a second part?",
            "And how did you end it with the extraction?",
            "Because it was not clear to me what was the advantage in this case of using this ontology?",
            "OK, thank you for the question first of all.",
            "Yes, of course.",
            "The the idea in the plan is to be able to populate all of the entities of the ontology or at least the core entities of the ontology.",
            "But populating each of these entities is different problem by itself, so of course we are working on that.",
            "It's a work in progress, but I wasn't able to fit everything in just one paper, so there are practical reasons for that.",
            "And the the second, how the ontology is inferring it to the creation of into the extraction.",
            "If I got the question correct right?",
            "And semantic technologies so.",
            "Note of course in their representation this item is that OK. First of all, what we're trying to do here is.",
            "Infuse if I may, if I'm allowed with the term, the semantics of the ontology that describe that describes an activity as something that is is done already in the past, and it's done by by someone who, in our case here in scholarship, is the author of the paper.",
            "So we're trying to encode these kind of features as features into classifier, and that's how it lead us to create these smart features that I showed you that actually identify those potential textual chunks.",
            "And we infuse those features into inside the classifier and this allowed us to achieve this higher performance.",
            "OK, thank you.",
            "Thank you.",
            "Identify some of these tasks by hand.",
            "Yes.",
            "And so the question is that I don't have a computer by my.",
            "How do you derive the follows relationship?",
            "Because typically in an experiment everything is not a pipeline is like a very complex graph, right?",
            "So it's not clear to me that even if an activity is described as later in the text that that.",
            "Always happy.",
            "Yes, we do.",
            "The the idea is to associate activities with this follows relation only when it is proved by the textual description.",
            "That was how they are human annotators annotated the tasks.",
            "So because we observe that there are a lot of times where we have two sequential activities that one appears after the other inside the text.",
            "But it doesn't mean necessarily that leads to activities follow each other because it doesn't.",
            "It's not derived from the textual description.",
            "So this is.",
            "This is why we use the machine learning methodology in order to do that.",
            "And actually our rule based pipeline use such as that we used heuristics just as the one that you mentioned, that for example, if the activities are just one follows after the other and they are all inside the paragraph, then assign them automatically the follows relation.",
            "But this did not achieve the good performance at the end.",
            "Well, if they are implied, but they're not mentioned in text, then it's a different kind of task.",
            "Actually, because we're dealing here with the activities that are already identified OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello everyone, I began as purchase and today will be presenting the work that along with panels custom Douglas and even other jobless is entitled ontology driven extraction of research processes.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's consider for a moment the following questions say that during our research we need to find information, for example regarding which papers are just a given problem or how was a given problem solved or who has worked on a particular topic and what do we know him about about him or about their work or even more concretely, has a particular experiment be conducted which steps were taken with which order, what results?",
                    "label": 0
                },
                {
                    "sent": "Answering these.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And today it relies heavily on the use of search engines.",
                    "label": 0
                },
                {
                    "sent": "We use our favorite search engine in order to find relative articles, but then we have to read the article by ourselves in order to figure out what kind of research activities are described.",
                    "label": 0
                },
                {
                    "sent": "Which methodology is followed, what results have been produced with resources have been used, etc.",
                    "label": 1
                },
                {
                    "sent": "Then we might also find other relevant information from other digital resources in the web and combine all of the above in order to gain knowledge and continue.",
                    "label": 0
                },
                {
                    "sent": "So based.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The tools that are available to us today are either large digital research infrastructures that aiming to gather resources, tools and services, or search engines like Google Scholar or Semantic Scholar.",
                    "label": 1
                },
                {
                    "sent": "That however, others the the problem by exploiting mainly mostly the articles meta data or traversing through authorization graphs.",
                    "label": 1
                },
                {
                    "sent": "Hence it is question how well these offered services fit.",
                    "label": 1
                },
                {
                    "sent": "The actual research lifecycle and address information needs of scholars well.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hold it to address this challenge.",
                    "label": 0
                },
                {
                    "sent": "We the plan for today, is to briefly describe how we can establish a conceptual model that provides a flexible framework for modeling research processes.",
                    "label": 1
                },
                {
                    "sent": "And then we'll show how, based on this conceptual model, we can create knowledge bases that are actually exploding.",
                    "label": 0
                },
                {
                    "sent": "The available digital resources in order to gather, combine an inferno knowledge that supports colors in their work.",
                    "label": 1
                },
                {
                    "sent": "Now, the conceptual framework that we created.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We call it scholarly ontology and it is designed so that it can be that I will be able to answer questions of the form.",
                    "label": 1
                },
                {
                    "sent": "Who does what, why, where, and how, and to do so, the ontology revolves around the central notion of activity, which can be any research process As for example a biological experiment or an archaeological excavation or a medical survey.",
                    "label": 0
                },
                {
                    "sent": "And we approach this notion of activity through various perspectives that highlight each time relevant parts of these methods accordingly.",
                    "label": 0
                },
                {
                    "sent": "So for example, we have the procedure perspective with methods such as with entities such as a method covering the how part.",
                    "label": 0
                },
                {
                    "sent": "Or the agency perspective with entities such as actors or goals that answer The Who or the Y or the research perspective with entities such as account, an item which can be any research article, for example that answer the what the questions.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now our task for today will be to focus mostly on the active the entity of the activity and we will do so will try to extract the textual chunks representing those activities from the actual text of the research articles.",
                    "label": 0
                },
                {
                    "sent": "Then after having identified those activities we want to interrelate them based on their sequential order as it is as it is right by the textual descriptions from the text, but it won't stop there after having interrelate those activities we want to associate them with contextual information that we gather.",
                    "label": 0
                },
                {
                    "sent": "From other resources or from the articles metadata and publish, everything's linked data at the end.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now to do so, the first step and information extraction task we created a train, an unlabeled training set from 50,000 research articles, and we use these training set in order to create word embeddings of 100 dimensions.",
                    "label": 1
                },
                {
                    "sent": "Both embeddings of 25 dimensions.",
                    "label": 0
                },
                {
                    "sent": "By replacing the word with its relevant post tag and dependencies, buildings of 25 dimensions using the word web algorithm from our experiments, we observed that there is a.",
                    "label": 0
                },
                {
                    "sent": "We had a bit our embeddings created in a domain specific.",
                    "label": 0
                },
                {
                    "sent": "Corpora had a better performance than the only that the other train models that you can find in the in other sources that are trained with generic purpose corpora such as Wikipedia are news feeds.",
                    "label": 0
                },
                {
                    "sent": "Further on, we created a labeled training set consisting.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Deriving from 50 research articles with a broad discipline coverage that comprised approximately about 2700 entities and about 1000 relations.",
                    "label": 1
                },
                {
                    "sent": "Now we wanted to expose our classifiers in a variety of writing styles.",
                    "label": 0
                },
                {
                    "sent": "Hence, we created three discipline specific test sets with 15 research articles, each from domains of digital humanities, bioinformatics, and medicine comprising approximately about 600 entities per set and 200 relations in digital humanities.",
                    "label": 1
                },
                {
                    "sent": "500 delicious bioinformatics and 600 elation in medicine test.",
                    "label": 0
                },
                {
                    "sent": "In addition to that, in order to assure that this that our results given the size of our tests were statistically significant, we performed approximate randomization tests using the buffer only correction in order to lower the threshold of the P value.",
                    "label": 0
                },
                {
                    "sent": "Depending on the number of the systems that we are comparing.",
                    "label": 0
                },
                {
                    "sent": "Now someone would think OK, activity extraction.",
                    "label": 0
                },
                {
                    "sent": "This sounds great.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Much like a classical named entity recognition problem, why don't we just use typical named entity recognition methodology?",
                    "label": 0
                },
                {
                    "sent": "Train it on this training set and see how it performs.",
                    "label": 0
                },
                {
                    "sent": "Well, we did that.",
                    "label": 0
                },
                {
                    "sent": "We use the classical name data recognition method based on a sliding window classifier with support vector machines and window size of 15 tokens, typically named entity recognition tasks and features like word embeddings, post embeddings, and binary features representing the surface form of tokens.",
                    "label": 0
                },
                {
                    "sent": "But as you can see here that the results that we got.",
                    "label": 0
                },
                {
                    "sent": "Are far from satisfying, so this made us think why?",
                    "label": 0
                },
                {
                    "sent": "What are the characteristics of our task that differentiated from classical name letter recognition tasks?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, it turns out that the research process is the entities that we are dealing.",
                    "label": 0
                },
                {
                    "sent": "Here are entities of high semantic complexity.",
                    "label": 1
                },
                {
                    "sent": "This means that instead of being able to represent them with proper names or one or two words at as is the case with usual named entities such as persons, locations, organizations.",
                    "label": 1
                },
                {
                    "sent": "Here we're dealing with actual textual descriptions of variable length, sometimes far larger than usual nerve problems, where the actual difference of tokens inside the textual chunks is practically relevant, and most of that some of the times the entity can cover almost the entire sentence.",
                    "label": 0
                },
                {
                    "sent": "On top of that, based on the syntactic analysis that we perform based on our data sets where we found out that the syntactic structure of these extracted entities can be generalized into some lexical syntactic part patterns, and in a decent chunk boundaries almost match specific dependency subtrees inside the dependency tree of a sentence, and apart from the token, the order of appearance of the tokens inside the sentence, which is linear, there is another sequence.",
                    "label": 1
                },
                {
                    "sent": "This based on the syntactic dependency of war.",
                    "label": 0
                },
                {
                    "sent": "One word to another which cannot be linear doesn't necessarily follow the order of appearance, which could also influence our results and then provide some good indeed.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nation, so with these insights we preceded in our future the creation of our feature space that comprises, as I said earlier of embeddings word post independency and binary features, where we use the 57 features for 100 coding of post tags or 71 features of 100 independence attacks.",
                    "label": 1
                },
                {
                    "sent": "When we're using random forest method for classification and 14 features of binary features for surface representation that examine, for example, whether the talking is capitalized, digit email or number etc.",
                    "label": 1
                },
                {
                    "sent": "And then smart features that examine the participation of the token inside a special lexical syntactic patterns that we have identified now as.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Example of these MoD features consider this sentence.",
                    "label": 0
                },
                {
                    "sent": "Parsing the sentence sewer syntactic parser such as Spacey that we use.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In our work, use dependency tree of this kind Now you.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sing some more specific lexical syntactic patterns we can isolate As for example, look for the subtree of the dependency tree that has a root.",
                    "label": 0
                },
                {
                    "sent": "A verb in the past sense in the past tense with the subject of first person singular or plural.",
                    "label": 0
                },
                {
                    "sent": "We can isolate this specific textual chunks that provide good indicators for the existence of activities inside the sentence.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, the methods that we created are all based on the token based classification using sliding window classifiers.",
                    "label": 1
                },
                {
                    "sent": "The Windows side that we chose after appropriate testing with the validation set.",
                    "label": 1
                },
                {
                    "sent": "Proved to be 30 tokens around the token of the examination.",
                    "label": 0
                },
                {
                    "sent": "For that we use logistic regression, support vector machines and random forest for classification and starting from the basics.",
                    "label": 1
                },
                {
                    "sent": "The first method that we present consists only of word and post embeddings plus the 14 binary features for surface representation using logistic regression support vector machines.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For the second methodology, we added the Dependencia Beddings and the 10 smart features.",
                    "label": 1
                },
                {
                    "sent": "Again, all of the rest of the features were kept the same.",
                    "label": 1
                },
                {
                    "sent": "In the third method we use random forests and because of that, instead of using, embeddings will use the one hotel encodings of post independency tags be keeping all the rest of the features the same.",
                    "label": 0
                },
                {
                    "sent": "Well, in the.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "4th Method we dropped completely the word embeddings.",
                    "label": 0
                },
                {
                    "sent": "We just used post independence embeddings, keeping the rest of their features the same, but this time instead of taking into account only one token we took into account the syntactic sequence of the tokens in a 3 Level 3 level syntactic sequence.",
                    "label": 0
                },
                {
                    "sent": "Meaning for each token we took into account it's syntactic Father an in syntactic Gran Father.",
                    "label": 0
                },
                {
                    "sent": "Again, we use logistic regression support vector machines.",
                    "label": 0
                },
                {
                    "sent": "Now as.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another the methodology we used, we implemented the pipeline architecture where in the ocean behind that is to split the original problem into two separate smaller subtasks and so that each classifier having to deal with with an easier problem would perform a Dick.",
                    "label": 1
                },
                {
                    "sent": "Oddly enough, so that the contact information of the results would achieve better performance.",
                    "label": 0
                },
                {
                    "sent": "Having said that, the first classifier in a pipeline performs settings classification and tries to identify, for each sentence whether there is an activity inside or not.",
                    "label": 0
                },
                {
                    "sent": "Now, for the sentences that.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first classifier says that there is an activity.",
                    "label": 0
                },
                {
                    "sent": "We have a second classifier performing token classification in order to detect the boundaries of the activity inside the sentence.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now for evaluation.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we performed two experiments, one token based and one entity based.",
                    "label": 0
                },
                {
                    "sent": "The baseline that we described earlier is in the first language, and as you can see, all of our methods with pipeline performing the best as well, the same results we got in the entity based evaluation.",
                    "label": 0
                },
                {
                    "sent": "But as you can see from the results of all of our methods being better than the baseline, the initial hypothesis was correct.",
                    "label": 0
                },
                {
                    "sent": "The fact meaning the fact that.",
                    "label": 0
                },
                {
                    "sent": "The size of the window classifier an the dependencies syntactic dependencies, or the participation in special syntactic features or the the order based on the syntactic sequence of tokens are all factors that should be considered when someone deals with the extraction of entities of this kind of complex.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sting now moving on to sequence relay.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Extraction, let's say that we have this sample text here and we have somehow identified.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All the entities, all of the activities inside the text.",
                    "label": 0
                },
                {
                    "sent": "Our task here would be for each chunk bounded by any of two activities inside the text to check whether the whether the relation follow.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Act two act.",
                    "label": 0
                },
                {
                    "sent": "One meaning that the activity of the last boundary follows the activity in the in the beginning of the chunk without any other activities interfering inside.",
                    "label": 0
                },
                {
                    "sent": "Whether this relation holds or not.",
                    "label": 0
                },
                {
                    "sent": "We do that for every chunk.",
                    "label": 0
                },
                {
                    "sent": "So the methodology that we have implemented the first 2 use the average of post independency embeddings per dimension for all of the tokens inside the chunk plus five binary features for structural structure attributes of the text that examined.",
                    "label": 0
                },
                {
                    "sent": "For example, whether there are other activities inside the chunk, whether we have separate sentences or neighboring sentences of whether the activities are in different paragraphs etc.",
                    "label": 0
                },
                {
                    "sent": "6 binary features for dependence of lexical sequential indicators, meaning tokens As for example.",
                    "label": 0
                },
                {
                    "sent": "First Secondly, following subsequently etc that are syntactically dependent with any of the tokens inside of our bounding entities.",
                    "label": 0
                },
                {
                    "sent": "And for that we use logistic regression and support vector machines.",
                    "label": 0
                },
                {
                    "sent": "In the third methodology we use.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Random forests, and because of that, instead of taking the average, we took the sum of post independency.",
                    "label": 0
                },
                {
                    "sent": "One hot encoding's per dimension, keeping the list of the features the same.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Evaluation",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Again, all of our methods showed better performance with a simple rule based baseline that we created for demonstration purposes here.",
                    "label": 0
                },
                {
                    "sent": "And as you can see, random Forest performs the best the best of the three, reaching performances up to 0.8, nine F1 score in all renal test set.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now, having either related or entities, we proceed with your eye creation and for the entities with proper name, meaning those that we extract from the articles metadata, such as persons, organizations, articles, topics, etc.",
                    "label": 1
                },
                {
                    "sent": "the UI is created based on the namespace that we provide the entity type according to the semantics of scholarly ontology and the Entity ID.",
                    "label": 0
                },
                {
                    "sent": "As for the entities.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That extracted from text, such as the activities that we are dealing here.",
                    "label": 0
                },
                {
                    "sent": "the UI is based on the namespace, the entity type of the source, meaning the article ID and boundary offsets showing the where the actual textual chunk is inside the text.",
                    "label": 0
                },
                {
                    "sent": "Of course we support our DFS and older models.",
                    "label": 0
                },
                {
                    "sent": "We also provide or sit integration so that for each author we can retrieve through the.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Table API, his ORCID, ID, and any extra information such as bio or project that he has participated and having said that, as a recapitulation, what we try to show here is that combining the goods of both worlds, if you say if you say machine learning extraction with powerful feature engineering or one hand and ontologies an linked data technologies on the other allows us starting from sentences like that to be able to.",
                    "label": 0
                },
                {
                    "sent": "Identify.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actual chunks of interest.",
                    "label": 0
                },
                {
                    "sent": "Associate them with the particular entities of our hodology.",
                    "label": 0
                },
                {
                    "sent": "Into",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Relate them based on their sequential order associated with other contextual information that we gather from articles made the data.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Or other deposit repository, such as receipt.",
                    "label": 0
                },
                {
                    "sent": "And link to them into.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Existing knowledge that we have already derived from the same article or other articles in order to create such knowledge graphs now having created such a knowledge graph, it's easy for us.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To identify and semantic paths of interest, such As for example, retrieve all of the articles.",
                    "label": 0
                },
                {
                    "sent": "All of the activities based on the same author or for example.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will retrieve the activities from that are exist in the same article and doing so allows for the questions that we saw in the beginning of the presentation to be answered automatically.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I love this computer.",
                    "label": 0
                },
                {
                    "sent": "Is the definition of authority basic?",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About your possible entries, I think we all agree that would be great because certain point we can, you know, except medical technology, that is a quote ever.",
                    "label": 0
                },
                {
                    "sent": "But then it seems that you focus just on active.",
                    "label": 0
                },
                {
                    "sent": "Yes, do you plan now digging into activity for extracting the best and in so it's just a second part?",
                    "label": 0
                },
                {
                    "sent": "And how did you end it with the extraction?",
                    "label": 0
                },
                {
                    "sent": "Because it was not clear to me what was the advantage in this case of using this ontology?",
                    "label": 0
                },
                {
                    "sent": "OK, thank you for the question first of all.",
                    "label": 0
                },
                {
                    "sent": "Yes, of course.",
                    "label": 0
                },
                {
                    "sent": "The the idea in the plan is to be able to populate all of the entities of the ontology or at least the core entities of the ontology.",
                    "label": 0
                },
                {
                    "sent": "But populating each of these entities is different problem by itself, so of course we are working on that.",
                    "label": 0
                },
                {
                    "sent": "It's a work in progress, but I wasn't able to fit everything in just one paper, so there are practical reasons for that.",
                    "label": 0
                },
                {
                    "sent": "And the the second, how the ontology is inferring it to the creation of into the extraction.",
                    "label": 0
                },
                {
                    "sent": "If I got the question correct right?",
                    "label": 0
                },
                {
                    "sent": "And semantic technologies so.",
                    "label": 0
                },
                {
                    "sent": "Note of course in their representation this item is that OK. First of all, what we're trying to do here is.",
                    "label": 0
                },
                {
                    "sent": "Infuse if I may, if I'm allowed with the term, the semantics of the ontology that describe that describes an activity as something that is is done already in the past, and it's done by by someone who, in our case here in scholarship, is the author of the paper.",
                    "label": 0
                },
                {
                    "sent": "So we're trying to encode these kind of features as features into classifier, and that's how it lead us to create these smart features that I showed you that actually identify those potential textual chunks.",
                    "label": 0
                },
                {
                    "sent": "And we infuse those features into inside the classifier and this allowed us to achieve this higher performance.",
                    "label": 0
                },
                {
                    "sent": "OK, thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Identify some of these tasks by hand.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "And so the question is that I don't have a computer by my.",
                    "label": 0
                },
                {
                    "sent": "How do you derive the follows relationship?",
                    "label": 0
                },
                {
                    "sent": "Because typically in an experiment everything is not a pipeline is like a very complex graph, right?",
                    "label": 0
                },
                {
                    "sent": "So it's not clear to me that even if an activity is described as later in the text that that.",
                    "label": 0
                },
                {
                    "sent": "Always happy.",
                    "label": 0
                },
                {
                    "sent": "Yes, we do.",
                    "label": 0
                },
                {
                    "sent": "The the idea is to associate activities with this follows relation only when it is proved by the textual description.",
                    "label": 0
                },
                {
                    "sent": "That was how they are human annotators annotated the tasks.",
                    "label": 0
                },
                {
                    "sent": "So because we observe that there are a lot of times where we have two sequential activities that one appears after the other inside the text.",
                    "label": 0
                },
                {
                    "sent": "But it doesn't mean necessarily that leads to activities follow each other because it doesn't.",
                    "label": 0
                },
                {
                    "sent": "It's not derived from the textual description.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                },
                {
                    "sent": "This is why we use the machine learning methodology in order to do that.",
                    "label": 0
                },
                {
                    "sent": "And actually our rule based pipeline use such as that we used heuristics just as the one that you mentioned, that for example, if the activities are just one follows after the other and they are all inside the paragraph, then assign them automatically the follows relation.",
                    "label": 0
                },
                {
                    "sent": "But this did not achieve the good performance at the end.",
                    "label": 0
                },
                {
                    "sent": "Well, if they are implied, but they're not mentioned in text, then it's a different kind of task.",
                    "label": 0
                },
                {
                    "sent": "Actually, because we're dealing here with the activities that are already identified OK.",
                    "label": 0
                }
            ]
        }
    }
}