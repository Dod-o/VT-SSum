{
    "id": "fbwid3ls6mfuwgt4izxf6doeg7chryth",
    "title": "Visual Classification with Multi-Task Joint Sparse Representation",
    "info": {
        "author": [
            "Xiao-Tong Yuan, National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences"
        ],
        "published": "July 19, 2010",
        "recorded": "June 2010",
        "category": [
            "Top->Computer Science->Data Visualisation"
        ]
    },
    "url": "http://videolectures.net/cvpr2010_yuan_vcmt/",
    "segmentation": [
        [
            "First talk is going to be about.",
            "A visual classification with multitask joint, sparse representation, and the speaker is chat only one."
        ],
        [
            "Thanks for the introduction.",
            "Good afternoon everyone.",
            "In this presentation I'm going to introduce model.",
            "Sorry, I'm going to introduce multitask join us boss representation method for visual feature classification."
        ],
        [
            "We know that feature plays a fundamental role in the success of many visual classification problems.",
            "Some objects are distinctive in color, somewhat distinctive in texture, and somewhat distinctive in shape.",
            "But in most cases it is a combination of these features that make the objects distinct to each other.",
            "This leads to the recent popularity or feature combination method in computer vision.",
            "The most popular methods exist in our training based methods, for example, MCL and some SVM example methods.",
            "These methods typically trained classifiers for each individual features and then combines the output of features in a separate or.",
            "You know, you know, unify the framework.",
            "Different from the training based methods.",
            "In this work, we propose to cost feature combination to a multi task joined sparse representation problem."
        ],
        [
            "The technical motivations of this work is include the advances in sparse representation for recognition and the reason advances in multi task sparse learning.",
            "The basic idea of sparse representation for recognition is to cast the recognition problem to a linear regression problem utilizing some mathematical tools from compressed sensing or one minimization.",
            "This leads to very robust solutions for.",
            "Recognition problems, for example, face recognition, another repeating with respect of sparse representation is that the model is generally training free.",
            "Another technical motivation over Messenger is a multi task sparse learning.",
            "In the practice of machine learning and computer vision, when the task is to be learned.",
            "Share some common factors.",
            "It is.",
            "Very useful to transfer some knowledge from 1 task to the other.",
            "When more task learning is applied to sparse learning, the model typically output joining sparse solutions."
        ],
        [
            "Hey, here's a quick review of some related work.",
            "The most popular kernel feature combination method is multiple kernel learning, which can be seen to linearly combine some kernel matrix so that the combined one can years improve the performance.",
            "More recently, some SVM in sample.",
            "Methods has also been proposed to process to handle the multiple kernel based visual classification problems.",
            "You methodology or method is most related to the multitask joined Covide selection model presented by opposite ski in 09.",
            "This model can be seen as a combination model of group lasso and the multi task lasso.",
            "It can achieve the block level sparsity.",
            "Also, our method can be seen as an extension of the widely applied this possible presentation model from single feature to multiple kernel feature setting."
        ],
        [
            "OK, here the introduction of our method or method, namely MTJ SRC, is a multi task joined sparse representation and classification method.",
            "The basic idea is to utilize each feature to form a linear representation task.",
            "Then we formulate our method as Mark task joint cover, right selection model and.",
            "Then jointly select some representative images from a few classes to reconstruct the test images.",
            "To further improve the performance.",
            "We we then put we can boost the individual features using linear programming, boosting to improve the overall recognition performance.",
            "Then to handle the multiple kernel combination, we further extend our method to its kind of use."
        ],
        [
            "OK, here is the.",
            "Key component of this method adjoined sparse representation mechanism.",
            "Consider we are given a set of images with J different classes.",
            "Each sample can be represented by K different types of features.",
            "Also, given a test image, we can extract the same key.",
            "Different types of features.",
            "For each feature, we form a linear representation task.",
            "The objective.",
            "Is 2 output block levels past coefficients WJ?",
            "Here's the WJ.",
            "Is coefficients associated with JS class from all the P Class K features.",
            "The philosophy behind this theory is very simple since the feature.",
            "The key feature or extracted from the same test image.",
            "They should be reconstructed from a few common.",
            "Classes of.",
            "Dictionary images."
        ],
        [
            "OK, now we have a supervised K task linear representation problem.",
            "The general formulation is the mark task least square regression with airopeek mixed norm regularization.",
            "Here the first term.",
            "Is.",
            "Quadratic error term and a second term is mixed norm regularization term.",
            "The parameter P controls the sparsity of the output and parameter Q.",
            "Controls the weights of the different tasks.",
            "OK, when P takes value from interval 01, sparse output may be expected to.",
            "Output butter the objective may be nonconvex.",
            "When P is greater than one.",
            "Although the solution may be less sparse, but the objective is convex.",
            "We in this work we are focused on the latter case since some of the shelf convex optimization methods can be applied in our model."
        ],
        [
            "Here we show that different setting of peculiar leads to different mixed norms.",
            "For example, when P equals one Q = 2, the.",
            "Normally, is Jonas Party inducing and this is what used in our task.",
            "Join is correct selection model.",
            "When P = 1, curiously Infinity.",
            "This is the term is the regulation term is also joint sparsity inducing, and this is what used in multi task lasso.",
            "When P is 1 curious one, the regularization term discovered and model based on 2K independent as our tasks.",
            "And a four P = 2 groups.",
            "Two this is.",
            "The model is just a key independent regression tasks."
        ],
        [
            "In this work we use P equals one Q = 2."
        ],
        [
            "Since the objective is convex and compensate for optimization, we use accelerated proximal gradient method.",
            "The APG method iterates between the generalized gradient mapping step and the aggregation step.",
            "Strong convergence can be guaranteed by AP method."
        ],
        [
            "After the optimal reconstruction coefficients have been.",
            "Obtained, the decision is done by.",
            "Is ruled over the.",
            "If you were off the reconstruction error.",
            "Class that achieves the lowest reconstruction error error.",
            "Here the serial key is the confidence of each feature.",
            "This confidence can be learned via linear programming, boosting on validation set.",
            "Or we can just set the weight to be equal weights."
        ],
        [
            "OK, here is the algorithm in details.",
            "The first 2 in the main loop, the first 2 step is the gradient.",
            "Generalize grid into making stuff, and the last two lines are the aggregation step.",
            "Here.",
            "The good news is that.",
            "The algorithm is characterized by the inner product of features.",
            "This motivates us to extend our model to screen reviews.",
            "OK here wait."
        ],
        [
            "Give the curvy extensions of our method.",
            "First, we consider the smart task join as possible presentation in reducing kernel Hilbert space.",
            "For each feature K, the inner product of the high dimensional features user characterized by the by.",
            "Convert function GK.",
            "OK, we can write out our model in the arcade address.",
            "As we mentioned in the previous slide, the APG method is characterized by inner product or features.",
            "In other words, only the training kernel matrix and the test kernel vectors are involved in the optimization."
        ],
        [
            "Here's the detailed algorithm of money.",
            "Just empty SRC in arc address."
        ],
        [
            "OK, another simple way to make use of the existing kernel matrices is to just take the columns of each kernel matrix as feature vectors.",
            "We can straightforwardly apply our model in this feature space.",
            "Without any.",
            "Modification."
        ],
        [
            "OK here I experiments.",
            "We compare our method with three feature combination methods.",
            "The first one is the nearest subspace method and the second one is the sparse representation method and we also compare our method with some representative feature combination methods in literatures.",
            "Here, the combination for the for the first two kinds of methods combination is done by fusing the outputs of the.",
            "Example nearest subspace classifiers for individual features."
        ],
        [
            "For datasets, we use three datasets for experiments.",
            "The for the Oxford flower datasets.",
            "We use the kernels and the experimental protocols provided by the data set authors and for care Tech 11.",
            "We use the kernel kinds of kernels in Vollmer, 2007."
        ],
        [
            "Here are the results for the Oxford Flower 17.",
            "This table lists feature combination method by all the competing algorithms, our method, the results over method is last two columns.",
            "It can be seen that our method, although do not involve any training, still achieve the state of the art results.",
            "We also provided the singer results on single features for single feature them appear reduced to SVM and.",
            "Of course, each boost and every post all reduced two SVM and our method is used to convert extension of the sparse representation method.",
            "From this table we can see that our method consistently outperform SVM for individual single features.",
            "Here are."
        ],
        [
            "Representation coefficients for the.",
            "Adapt their test image from this figure we can see the joint sparsity of our method.",
            "OK."
        ],
        [
            "Here are the results for Oxford Flower 102.",
            "Again, we can see that our method achieves state of the art results on this data set.",
            "And for single features, our method either perform comparably to SVM or slightly better than SVM."
        ],
        [
            "And the lots on Caltech 101 we use the experiment protocol of 15 trend and 15 tests.",
            "The.",
            "Again, we can see that our method achieves the state of the other results on this data set.",
            "OK, on single features our method is.",
            "Also perform quite competitive to SVM classifiers."
        ],
        [
            "OK, to summarize, the big secret observation of this method is that.",
            "Hearthstone is sparse.",
            "Representation is effective to combine complimentary visual features an for single feature.",
            "We also find out that kind of extensions of our method performed quite comparative to SVM.",
            "Finally, our messages free of model training and can be very flexible in the practice of computer vision."
        ],
        [
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "First talk is going to be about.",
                    "label": 0
                },
                {
                    "sent": "A visual classification with multitask joint, sparse representation, and the speaker is chat only one.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks for the introduction.",
                    "label": 0
                },
                {
                    "sent": "Good afternoon everyone.",
                    "label": 0
                },
                {
                    "sent": "In this presentation I'm going to introduce model.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I'm going to introduce multitask join us boss representation method for visual feature classification.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We know that feature plays a fundamental role in the success of many visual classification problems.",
                    "label": 0
                },
                {
                    "sent": "Some objects are distinctive in color, somewhat distinctive in texture, and somewhat distinctive in shape.",
                    "label": 0
                },
                {
                    "sent": "But in most cases it is a combination of these features that make the objects distinct to each other.",
                    "label": 0
                },
                {
                    "sent": "This leads to the recent popularity or feature combination method in computer vision.",
                    "label": 0
                },
                {
                    "sent": "The most popular methods exist in our training based methods, for example, MCL and some SVM example methods.",
                    "label": 0
                },
                {
                    "sent": "These methods typically trained classifiers for each individual features and then combines the output of features in a separate or.",
                    "label": 0
                },
                {
                    "sent": "You know, you know, unify the framework.",
                    "label": 0
                },
                {
                    "sent": "Different from the training based methods.",
                    "label": 0
                },
                {
                    "sent": "In this work, we propose to cost feature combination to a multi task joined sparse representation problem.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The technical motivations of this work is include the advances in sparse representation for recognition and the reason advances in multi task sparse learning.",
                    "label": 1
                },
                {
                    "sent": "The basic idea of sparse representation for recognition is to cast the recognition problem to a linear regression problem utilizing some mathematical tools from compressed sensing or one minimization.",
                    "label": 0
                },
                {
                    "sent": "This leads to very robust solutions for.",
                    "label": 0
                },
                {
                    "sent": "Recognition problems, for example, face recognition, another repeating with respect of sparse representation is that the model is generally training free.",
                    "label": 0
                },
                {
                    "sent": "Another technical motivation over Messenger is a multi task sparse learning.",
                    "label": 0
                },
                {
                    "sent": "In the practice of machine learning and computer vision, when the task is to be learned.",
                    "label": 0
                },
                {
                    "sent": "Share some common factors.",
                    "label": 0
                },
                {
                    "sent": "It is.",
                    "label": 0
                },
                {
                    "sent": "Very useful to transfer some knowledge from 1 task to the other.",
                    "label": 0
                },
                {
                    "sent": "When more task learning is applied to sparse learning, the model typically output joining sparse solutions.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hey, here's a quick review of some related work.",
                    "label": 0
                },
                {
                    "sent": "The most popular kernel feature combination method is multiple kernel learning, which can be seen to linearly combine some kernel matrix so that the combined one can years improve the performance.",
                    "label": 0
                },
                {
                    "sent": "More recently, some SVM in sample.",
                    "label": 0
                },
                {
                    "sent": "Methods has also been proposed to process to handle the multiple kernel based visual classification problems.",
                    "label": 0
                },
                {
                    "sent": "You methodology or method is most related to the multitask joined Covide selection model presented by opposite ski in 09.",
                    "label": 0
                },
                {
                    "sent": "This model can be seen as a combination model of group lasso and the multi task lasso.",
                    "label": 0
                },
                {
                    "sent": "It can achieve the block level sparsity.",
                    "label": 0
                },
                {
                    "sent": "Also, our method can be seen as an extension of the widely applied this possible presentation model from single feature to multiple kernel feature setting.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here the introduction of our method or method, namely MTJ SRC, is a multi task joined sparse representation and classification method.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is to utilize each feature to form a linear representation task.",
                    "label": 0
                },
                {
                    "sent": "Then we formulate our method as Mark task joint cover, right selection model and.",
                    "label": 0
                },
                {
                    "sent": "Then jointly select some representative images from a few classes to reconstruct the test images.",
                    "label": 0
                },
                {
                    "sent": "To further improve the performance.",
                    "label": 0
                },
                {
                    "sent": "We we then put we can boost the individual features using linear programming, boosting to improve the overall recognition performance.",
                    "label": 0
                },
                {
                    "sent": "Then to handle the multiple kernel combination, we further extend our method to its kind of use.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, here is the.",
                    "label": 0
                },
                {
                    "sent": "Key component of this method adjoined sparse representation mechanism.",
                    "label": 0
                },
                {
                    "sent": "Consider we are given a set of images with J different classes.",
                    "label": 0
                },
                {
                    "sent": "Each sample can be represented by K different types of features.",
                    "label": 0
                },
                {
                    "sent": "Also, given a test image, we can extract the same key.",
                    "label": 0
                },
                {
                    "sent": "Different types of features.",
                    "label": 0
                },
                {
                    "sent": "For each feature, we form a linear representation task.",
                    "label": 1
                },
                {
                    "sent": "The objective.",
                    "label": 0
                },
                {
                    "sent": "Is 2 output block levels past coefficients WJ?",
                    "label": 0
                },
                {
                    "sent": "Here's the WJ.",
                    "label": 0
                },
                {
                    "sent": "Is coefficients associated with JS class from all the P Class K features.",
                    "label": 0
                },
                {
                    "sent": "The philosophy behind this theory is very simple since the feature.",
                    "label": 0
                },
                {
                    "sent": "The key feature or extracted from the same test image.",
                    "label": 0
                },
                {
                    "sent": "They should be reconstructed from a few common.",
                    "label": 0
                },
                {
                    "sent": "Classes of.",
                    "label": 0
                },
                {
                    "sent": "Dictionary images.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, now we have a supervised K task linear representation problem.",
                    "label": 0
                },
                {
                    "sent": "The general formulation is the mark task least square regression with airopeek mixed norm regularization.",
                    "label": 0
                },
                {
                    "sent": "Here the first term.",
                    "label": 0
                },
                {
                    "sent": "Is.",
                    "label": 0
                },
                {
                    "sent": "Quadratic error term and a second term is mixed norm regularization term.",
                    "label": 0
                },
                {
                    "sent": "The parameter P controls the sparsity of the output and parameter Q.",
                    "label": 0
                },
                {
                    "sent": "Controls the weights of the different tasks.",
                    "label": 0
                },
                {
                    "sent": "OK, when P takes value from interval 01, sparse output may be expected to.",
                    "label": 0
                },
                {
                    "sent": "Output butter the objective may be nonconvex.",
                    "label": 0
                },
                {
                    "sent": "When P is greater than one.",
                    "label": 0
                },
                {
                    "sent": "Although the solution may be less sparse, but the objective is convex.",
                    "label": 0
                },
                {
                    "sent": "We in this work we are focused on the latter case since some of the shelf convex optimization methods can be applied in our model.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here we show that different setting of peculiar leads to different mixed norms.",
                    "label": 0
                },
                {
                    "sent": "For example, when P equals one Q = 2, the.",
                    "label": 0
                },
                {
                    "sent": "Normally, is Jonas Party inducing and this is what used in our task.",
                    "label": 0
                },
                {
                    "sent": "Join is correct selection model.",
                    "label": 0
                },
                {
                    "sent": "When P = 1, curiously Infinity.",
                    "label": 0
                },
                {
                    "sent": "This is the term is the regulation term is also joint sparsity inducing, and this is what used in multi task lasso.",
                    "label": 0
                },
                {
                    "sent": "When P is 1 curious one, the regularization term discovered and model based on 2K independent as our tasks.",
                    "label": 0
                },
                {
                    "sent": "And a four P = 2 groups.",
                    "label": 0
                },
                {
                    "sent": "Two this is.",
                    "label": 0
                },
                {
                    "sent": "The model is just a key independent regression tasks.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this work we use P equals one Q = 2.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Since the objective is convex and compensate for optimization, we use accelerated proximal gradient method.",
                    "label": 0
                },
                {
                    "sent": "The APG method iterates between the generalized gradient mapping step and the aggregation step.",
                    "label": 0
                },
                {
                    "sent": "Strong convergence can be guaranteed by AP method.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "After the optimal reconstruction coefficients have been.",
                    "label": 0
                },
                {
                    "sent": "Obtained, the decision is done by.",
                    "label": 0
                },
                {
                    "sent": "Is ruled over the.",
                    "label": 0
                },
                {
                    "sent": "If you were off the reconstruction error.",
                    "label": 0
                },
                {
                    "sent": "Class that achieves the lowest reconstruction error error.",
                    "label": 0
                },
                {
                    "sent": "Here the serial key is the confidence of each feature.",
                    "label": 0
                },
                {
                    "sent": "This confidence can be learned via linear programming, boosting on validation set.",
                    "label": 0
                },
                {
                    "sent": "Or we can just set the weight to be equal weights.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, here is the algorithm in details.",
                    "label": 0
                },
                {
                    "sent": "The first 2 in the main loop, the first 2 step is the gradient.",
                    "label": 0
                },
                {
                    "sent": "Generalize grid into making stuff, and the last two lines are the aggregation step.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "The good news is that.",
                    "label": 0
                },
                {
                    "sent": "The algorithm is characterized by the inner product of features.",
                    "label": 1
                },
                {
                    "sent": "This motivates us to extend our model to screen reviews.",
                    "label": 0
                },
                {
                    "sent": "OK here wait.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Give the curvy extensions of our method.",
                    "label": 0
                },
                {
                    "sent": "First, we consider the smart task join as possible presentation in reducing kernel Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "For each feature K, the inner product of the high dimensional features user characterized by the by.",
                    "label": 0
                },
                {
                    "sent": "Convert function GK.",
                    "label": 0
                },
                {
                    "sent": "OK, we can write out our model in the arcade address.",
                    "label": 0
                },
                {
                    "sent": "As we mentioned in the previous slide, the APG method is characterized by inner product or features.",
                    "label": 0
                },
                {
                    "sent": "In other words, only the training kernel matrix and the test kernel vectors are involved in the optimization.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's the detailed algorithm of money.",
                    "label": 0
                },
                {
                    "sent": "Just empty SRC in arc address.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, another simple way to make use of the existing kernel matrices is to just take the columns of each kernel matrix as feature vectors.",
                    "label": 0
                },
                {
                    "sent": "We can straightforwardly apply our model in this feature space.",
                    "label": 0
                },
                {
                    "sent": "Without any.",
                    "label": 0
                },
                {
                    "sent": "Modification.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK here I experiments.",
                    "label": 0
                },
                {
                    "sent": "We compare our method with three feature combination methods.",
                    "label": 0
                },
                {
                    "sent": "The first one is the nearest subspace method and the second one is the sparse representation method and we also compare our method with some representative feature combination methods in literatures.",
                    "label": 0
                },
                {
                    "sent": "Here, the combination for the for the first two kinds of methods combination is done by fusing the outputs of the.",
                    "label": 0
                },
                {
                    "sent": "Example nearest subspace classifiers for individual features.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For datasets, we use three datasets for experiments.",
                    "label": 0
                },
                {
                    "sent": "The for the Oxford flower datasets.",
                    "label": 0
                },
                {
                    "sent": "We use the kernels and the experimental protocols provided by the data set authors and for care Tech 11.",
                    "label": 0
                },
                {
                    "sent": "We use the kernel kinds of kernels in Vollmer, 2007.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here are the results for the Oxford Flower 17.",
                    "label": 0
                },
                {
                    "sent": "This table lists feature combination method by all the competing algorithms, our method, the results over method is last two columns.",
                    "label": 0
                },
                {
                    "sent": "It can be seen that our method, although do not involve any training, still achieve the state of the art results.",
                    "label": 0
                },
                {
                    "sent": "We also provided the singer results on single features for single feature them appear reduced to SVM and.",
                    "label": 1
                },
                {
                    "sent": "Of course, each boost and every post all reduced two SVM and our method is used to convert extension of the sparse representation method.",
                    "label": 0
                },
                {
                    "sent": "From this table we can see that our method consistently outperform SVM for individual single features.",
                    "label": 0
                },
                {
                    "sent": "Here are.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Representation coefficients for the.",
                    "label": 0
                },
                {
                    "sent": "Adapt their test image from this figure we can see the joint sparsity of our method.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are the results for Oxford Flower 102.",
                    "label": 0
                },
                {
                    "sent": "Again, we can see that our method achieves state of the art results on this data set.",
                    "label": 0
                },
                {
                    "sent": "And for single features, our method either perform comparably to SVM or slightly better than SVM.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the lots on Caltech 101 we use the experiment protocol of 15 trend and 15 tests.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Again, we can see that our method achieves the state of the other results on this data set.",
                    "label": 0
                },
                {
                    "sent": "OK, on single features our method is.",
                    "label": 0
                },
                {
                    "sent": "Also perform quite competitive to SVM classifiers.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, to summarize, the big secret observation of this method is that.",
                    "label": 0
                },
                {
                    "sent": "Hearthstone is sparse.",
                    "label": 0
                },
                {
                    "sent": "Representation is effective to combine complimentary visual features an for single feature.",
                    "label": 0
                },
                {
                    "sent": "We also find out that kind of extensions of our method performed quite comparative to SVM.",
                    "label": 0
                },
                {
                    "sent": "Finally, our messages free of model training and can be very flexible in the practice of computer vision.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}