{
    "id": "ukpmduihggyu3v73h6avdesz32k3l6ua",
    "title": "Low-Complexity Single-Image Super-Resolution based on Nonnegative Neighbor Embedding",
    "info": {
        "author": [
            "Marco Bevilacqua, INRIA"
        ],
        "published": "Oct. 9, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/bmvc2012_bevilacqua_neighbor_embedding/",
    "segmentation": [
        [
            "I will present an algorithm for single image super resolution and whose aim is to reach a low complexity target."
        ],
        [
            "So the outline of representation is this one.",
            "I will say what super resolution single image super resolution is.",
            "What are the methods used in the literature and in particular the neighbor embedding super resolution methods?",
            "Then I will speak about the proposed algorithm that will see it as three key points and then results and conclusions."
        ],
        [
            "So this single image super resolution is the problem of generating announced upscaling given a loss of resolution input image.",
            "And the world and asked stands for the fact that we want to reach better results than classical interpolation methods.",
            "And our target gain is to design a low complexity.",
            "But of course efficient.",
            "Algorithm of this kind.",
            "See."
        ],
        [
            "English image resolution methods can be broadly classified into these two categories.",
            "We have inverse problem methods that sees that see the Super resolution problem as an inverse problem.",
            "So we have the original Paris solution image and we have several degradations like warping, downscaling and blurring that we want to reverse.",
            "Anne II family is about machine learning methods and in this family we can find the example based super resolution where the word example stands for patches basically so we have a generally a Dictionary of correspondences of low resolution, high resolution patches and we use this correspondence is in order to infer the high resolution details.",
            "An we can.",
            "The identified 2 main families among these example made methods and we have a direct local learning method that basically they try to infer the function between the that goes from the low resolution to the high resolution patches and nearest neighbor estimation methods.",
            "Nearest neighbor Super Solution basically as."
        ],
        [
            "These four steps.",
            "So in the first step we create the dictionary and the dictionary can be created either by using external training images and by sampling from them from them.",
            "The patches I load a solution, either solution patches or we can exploit somehow the self similarities of the input image itself.",
            "Then once we have the Dictionary of the patches, we go.",
            "To the proper super resolution algorithm and for each input Patch we have three steps.",
            "We have generally nearest neighbor search.",
            "So for each low resolution Patch we find the K. In general, global solution, best matching patches in the dictionary.",
            "And then we compute somehow the ways of the past combinations and then in the first step we apply these ways to the corresponding other solution candidates in order to generate the output patches.",
            "And in general, in most of the algorithms.",
            "Or procedure is carried out in a feature space.",
            "An we can identify 2 main approaches.",
            "The one pass approach where we directly go to the desired magnification factor or multi pass algorithms that generate generate the output.",
            "the Super resolved image gradually by going through intermediate scale factors.",
            "An indication that is this matter the based on entity which."
        ],
        [
            "It's for local leaner embedding, embedding and and it is a method for money for learning and data dimensionality reduction.",
            "And here the third step by step about the weight computation is performed.",
            "By solving this riskware problem where we try to basically to approximate approximate the input Patch XD.",
            "We did the neighbors an so in this case we we speak about neighbor embedding an.",
            "Basically we assume that the patches lie on the manifold and what we do by minimizing this approximation error is to approximate the manifold locally within hyperplane and we have a constraint.",
            "Which is a sum up to one constraint and the fact that the weights must sum up to 1 means that they are basically invariant to rotations and translation, and that means that we want to learn the intrinsic geometry of the manifold.",
            "And so since we apply the same way to head to the high resolution patches, we assume that the two manifolds low resolution loaded in high resolution manifolds, they share the same geometry.",
            "And As for the features, this algorithm, the literature use great uses gradient features for the low resolution patches."
        ],
        [
            "So if we want to design an algorithm of this kind, we have three points we can play with.",
            "We have the features as said, the methods to the method to compute the weights and finally the dictionary whether to use an external or internal dictionary.",
            "So for the feature."
        ],
        [
            "An official representation means that a Patch is represented by vector of features computed on on pixels, and the role of the features is double.",
            "First is to firstly to catch the most important information or that the lower solution candidates to be good predictors for the high resolution reconstructions and Secondly is to enforce this.",
            "Hypothesis is the manifold similarity, and we have various possibilities.",
            "So to use luminance values so centered luminance values or gradient features."
        ],
        [
            "We performed an analysis of their features here.",
            "In the plots they plot we can see.",
            "3 feature representation considered.",
            "So the blue one is the 1st order gradient descent.",
            "But the red lines are bout centered luminance values, so luminance values with mean removal and the green curve is the concatenation of the two and a paper.",
            "States that disturb feature representations of the green curve.",
            "Is the best one, but actually in this paper the analysis is not performed by considering the evolution of K, the number of neighbors, but just by fixing K. And as we see from the plots actually.",
            "The the trend varies a lot whether we consider.",
            "Small now number of neighbors.",
            "So large number of neighbors an if we consider the plot of a role we can we can see see the the centered luminance values.",
            "So the red line is the best performing.",
            "But we observe also a fo in all cases that is even dramatic in case of feature too.",
            "And so we want to use these features so the center luminance values.",
            "Even because they are the most locals features, but we of course want to avoid this hole in the performance.",
            "To explain."
        ],
        [
            "In the fall we can make these two observations first.",
            "If we consider the neighborhood Mattix, which is generally di times K, where D is the dimension of the feature vectors.",
            "We observed that in our case at the rank of these metrics is with high probability the highest possible.",
            "That means the minimum between the minus one and K D -- 1, because we have centered feature attempted.",
            "Vectors.",
            "And the second observation is that then 4K equal to the the summer to one riskware problem that we described before is assimilable to a square linear system because we have.",
            "K. Or the equations.",
            "the D -- 1 linearly independent equation of the approximation error approximation error system an plus.",
            "One of the given by the constraint in K equal to D unknowns, which are the entries of the weight vector an so.",
            "Experimentally, we have.",
            "We observed that here we have the critical points in the performance and that means that the fall is because of overfitting problem."
        ],
        [
            "So the idea is to replace this equality constraint with the an inequality constraint.",
            "In particular non negative inequality constraint in order to relax this least square problem and so the least square problem becomes the one in the equation two with the same approximation error that we want to minimize but with a non negative constraint.",
            "And as we can see here from this plot."
        ],
        [
            "We have here we have the distribution of the weights and as we can see that in the case of this summer, too well some optimally square program, we have larger weights in amplitude in proximity of the fall.",
            "That means that when the.",
            "When the dimension is.",
            "Equals today the dimension of the problem, then the weights computed are perfectly fitted on the low resolution patches, but then when applied to the high resolution reconstructions, they lead to better constructions and for the non negative ways.",
            "Instead we observe.",
            "That the way the decade in amplitude with K that's good and the good Nestle is non negative embedding is confirmed also in the second plot where we have the distance between the actual low resolution weights that we can compute and the idea always that we would compute if we had the ground truth vectors.",
            "Now the third key point."
        ],
        [
            "Which is the dictionary.",
            "As said, we have two possibilities.",
            "First one is to use external training images and.",
            "Sampling and sample value from them.",
            "The patches second possibilities to learn the dispatch.",
            "Correspondence is in a pyramid of recursively scaled images starting from the low resolution input image and this is what is done in there well famous algorithm of glasner.",
            "And as we can see from the table, in our case, since we we want to use a one pass procedure.",
            "And we can extract from this pyramid of recursively scale images.",
            "We can extract patches only if the distance between the two layers is exactly the magnification factor, and so in our case we experience that the number of patches that we can extract from the pyramid is not sufficient.",
            "So therefore we use an external dictionary.",
            "So just do some."
        ],
        [
            "Nice three key points where the feature representations used and we decided to use central luminous features for both kinds of patches.",
            "Then it the first the weight computation method, then we will use this new nonnegative embedding and the third one was the choice of the dictionary, which is an external dictionary.",
            "And as we can see that the curves that we obtain.",
            "Are much better than the previous ones with the Eli based algorithm.",
            "Now the experiment section."
        ],
        [
            "We consider this to this for algorithms.",
            "The first one is the energy based algorithm of charger.",
            "Second one is the pyramid algorithm of glass metal.",
            "Then we have algorithm of time based on based on a kernel Ridge regression, an hour algorithm.",
            "As we can see we are different features used different methods for the pet reconstruction an different procedures.",
            "Single step or multi pass?",
            "An as you can see from the."
        ],
        [
            "Table.",
            "Our algorithm is the best performing for a scale factor of two and is slightly worse than glasner on in full scale factors of three and four.",
            "This is for the PS in our performance whereas."
        ],
        [
            "For the time performance this is a rough estimation of the compositional load is just a measure of intelligence of the running time.",
            "And as we can see, our ad is presents it sensibly reduced computational time, and this is thanks to two main things, either thanks to the one pass procedure.",
            "Or thanks to the shorted feature vectors, because we are never algorithm used this center features which is 1 value per pixel, whereas in other algorithms they use two or four values per pixel."
        ],
        [
            "We also Additionally we also try to combine the approach of Tanga bought the regression method but with our futures.",
            "And as we can see the results are even better.",
            "So better PS in our values and also computational time compara bulto our first algorithm.",
            "These are some."
        ],
        [
            "Visual results, so this is an image we want to magnify by a factor of 3."
        ],
        [
            "This is the becoming interpolation."
        ],
        [
            "Method of chunk."
        ],
        [
            "Glasner tongue."
        ],
        [
            "Hours."
        ],
        [
            "For another one in."
        ],
        [
            "Boot image again, magnified it by you 3."
        ],
        [
            "The big."
        ],
        [
            "Interpolation."
        ],
        [
            "Method of charging daily."
        ],
        [
            "Is the neighbor embedding.",
            "Glasner tongue."
        ],
        [
            "An hours and in both cases the time is much less in our case.",
            "So."
        ],
        [
            "To conclude, our method presents.",
            "Better results than other one pass algorithms.",
            "The two we presented challenging time.",
            "The results are compareable to the multipass algorithm of glasner.",
            "But presents of course much lower computational time for the future work.",
            "We want to further study the regression method as we already obtained promising results, and also possibly to design other strategies to select the neighbors."
        ],
        [
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I will present an algorithm for single image super resolution and whose aim is to reach a low complexity target.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the outline of representation is this one.",
                    "label": 0
                },
                {
                    "sent": "I will say what super resolution single image super resolution is.",
                    "label": 0
                },
                {
                    "sent": "What are the methods used in the literature and in particular the neighbor embedding super resolution methods?",
                    "label": 1
                },
                {
                    "sent": "Then I will speak about the proposed algorithm that will see it as three key points and then results and conclusions.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this single image super resolution is the problem of generating announced upscaling given a loss of resolution input image.",
                    "label": 1
                },
                {
                    "sent": "And the world and asked stands for the fact that we want to reach better results than classical interpolation methods.",
                    "label": 0
                },
                {
                    "sent": "And our target gain is to design a low complexity.",
                    "label": 1
                },
                {
                    "sent": "But of course efficient.",
                    "label": 0
                },
                {
                    "sent": "Algorithm of this kind.",
                    "label": 0
                },
                {
                    "sent": "See.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "English image resolution methods can be broadly classified into these two categories.",
                    "label": 0
                },
                {
                    "sent": "We have inverse problem methods that sees that see the Super resolution problem as an inverse problem.",
                    "label": 1
                },
                {
                    "sent": "So we have the original Paris solution image and we have several degradations like warping, downscaling and blurring that we want to reverse.",
                    "label": 1
                },
                {
                    "sent": "Anne II family is about machine learning methods and in this family we can find the example based super resolution where the word example stands for patches basically so we have a generally a Dictionary of correspondences of low resolution, high resolution patches and we use this correspondence is in order to infer the high resolution details.",
                    "label": 0
                },
                {
                    "sent": "An we can.",
                    "label": 1
                },
                {
                    "sent": "The identified 2 main families among these example made methods and we have a direct local learning method that basically they try to infer the function between the that goes from the low resolution to the high resolution patches and nearest neighbor estimation methods.",
                    "label": 0
                },
                {
                    "sent": "Nearest neighbor Super Solution basically as.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These four steps.",
                    "label": 0
                },
                {
                    "sent": "So in the first step we create the dictionary and the dictionary can be created either by using external training images and by sampling from them from them.",
                    "label": 0
                },
                {
                    "sent": "The patches I load a solution, either solution patches or we can exploit somehow the self similarities of the input image itself.",
                    "label": 0
                },
                {
                    "sent": "Then once we have the Dictionary of the patches, we go.",
                    "label": 0
                },
                {
                    "sent": "To the proper super resolution algorithm and for each input Patch we have three steps.",
                    "label": 0
                },
                {
                    "sent": "We have generally nearest neighbor search.",
                    "label": 1
                },
                {
                    "sent": "So for each low resolution Patch we find the K. In general, global solution, best matching patches in the dictionary.",
                    "label": 0
                },
                {
                    "sent": "And then we compute somehow the ways of the past combinations and then in the first step we apply these ways to the corresponding other solution candidates in order to generate the output patches.",
                    "label": 1
                },
                {
                    "sent": "And in general, in most of the algorithms.",
                    "label": 0
                },
                {
                    "sent": "Or procedure is carried out in a feature space.",
                    "label": 1
                },
                {
                    "sent": "An we can identify 2 main approaches.",
                    "label": 0
                },
                {
                    "sent": "The one pass approach where we directly go to the desired magnification factor or multi pass algorithms that generate generate the output.",
                    "label": 0
                },
                {
                    "sent": "the Super resolved image gradually by going through intermediate scale factors.",
                    "label": 0
                },
                {
                    "sent": "An indication that is this matter the based on entity which.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It's for local leaner embedding, embedding and and it is a method for money for learning and data dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "And here the third step by step about the weight computation is performed.",
                    "label": 1
                },
                {
                    "sent": "By solving this riskware problem where we try to basically to approximate approximate the input Patch XD.",
                    "label": 0
                },
                {
                    "sent": "We did the neighbors an so in this case we we speak about neighbor embedding an.",
                    "label": 0
                },
                {
                    "sent": "Basically we assume that the patches lie on the manifold and what we do by minimizing this approximation error is to approximate the manifold locally within hyperplane and we have a constraint.",
                    "label": 0
                },
                {
                    "sent": "Which is a sum up to one constraint and the fact that the weights must sum up to 1 means that they are basically invariant to rotations and translation, and that means that we want to learn the intrinsic geometry of the manifold.",
                    "label": 0
                },
                {
                    "sent": "And so since we apply the same way to head to the high resolution patches, we assume that the two manifolds low resolution loaded in high resolution manifolds, they share the same geometry.",
                    "label": 0
                },
                {
                    "sent": "And As for the features, this algorithm, the literature use great uses gradient features for the low resolution patches.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we want to design an algorithm of this kind, we have three points we can play with.",
                    "label": 0
                },
                {
                    "sent": "We have the features as said, the methods to the method to compute the weights and finally the dictionary whether to use an external or internal dictionary.",
                    "label": 1
                },
                {
                    "sent": "So for the feature.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An official representation means that a Patch is represented by vector of features computed on on pixels, and the role of the features is double.",
                    "label": 1
                },
                {
                    "sent": "First is to firstly to catch the most important information or that the lower solution candidates to be good predictors for the high resolution reconstructions and Secondly is to enforce this.",
                    "label": 0
                },
                {
                    "sent": "Hypothesis is the manifold similarity, and we have various possibilities.",
                    "label": 1
                },
                {
                    "sent": "So to use luminance values so centered luminance values or gradient features.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We performed an analysis of their features here.",
                    "label": 1
                },
                {
                    "sent": "In the plots they plot we can see.",
                    "label": 0
                },
                {
                    "sent": "3 feature representation considered.",
                    "label": 0
                },
                {
                    "sent": "So the blue one is the 1st order gradient descent.",
                    "label": 1
                },
                {
                    "sent": "But the red lines are bout centered luminance values, so luminance values with mean removal and the green curve is the concatenation of the two and a paper.",
                    "label": 1
                },
                {
                    "sent": "States that disturb feature representations of the green curve.",
                    "label": 0
                },
                {
                    "sent": "Is the best one, but actually in this paper the analysis is not performed by considering the evolution of K, the number of neighbors, but just by fixing K. And as we see from the plots actually.",
                    "label": 0
                },
                {
                    "sent": "The the trend varies a lot whether we consider.",
                    "label": 1
                },
                {
                    "sent": "Small now number of neighbors.",
                    "label": 0
                },
                {
                    "sent": "So large number of neighbors an if we consider the plot of a role we can we can see see the the centered luminance values.",
                    "label": 0
                },
                {
                    "sent": "So the red line is the best performing.",
                    "label": 0
                },
                {
                    "sent": "But we observe also a fo in all cases that is even dramatic in case of feature too.",
                    "label": 1
                },
                {
                    "sent": "And so we want to use these features so the center luminance values.",
                    "label": 0
                },
                {
                    "sent": "Even because they are the most locals features, but we of course want to avoid this hole in the performance.",
                    "label": 0
                },
                {
                    "sent": "To explain.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the fall we can make these two observations first.",
                    "label": 1
                },
                {
                    "sent": "If we consider the neighborhood Mattix, which is generally di times K, where D is the dimension of the feature vectors.",
                    "label": 0
                },
                {
                    "sent": "We observed that in our case at the rank of these metrics is with high probability the highest possible.",
                    "label": 0
                },
                {
                    "sent": "That means the minimum between the minus one and K D -- 1, because we have centered feature attempted.",
                    "label": 0
                },
                {
                    "sent": "Vectors.",
                    "label": 0
                },
                {
                    "sent": "And the second observation is that then 4K equal to the the summer to one riskware problem that we described before is assimilable to a square linear system because we have.",
                    "label": 1
                },
                {
                    "sent": "K. Or the equations.",
                    "label": 0
                },
                {
                    "sent": "the D -- 1 linearly independent equation of the approximation error approximation error system an plus.",
                    "label": 0
                },
                {
                    "sent": "One of the given by the constraint in K equal to D unknowns, which are the entries of the weight vector an so.",
                    "label": 0
                },
                {
                    "sent": "Experimentally, we have.",
                    "label": 0
                },
                {
                    "sent": "We observed that here we have the critical points in the performance and that means that the fall is because of overfitting problem.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the idea is to replace this equality constraint with the an inequality constraint.",
                    "label": 1
                },
                {
                    "sent": "In particular non negative inequality constraint in order to relax this least square problem and so the least square problem becomes the one in the equation two with the same approximation error that we want to minimize but with a non negative constraint.",
                    "label": 0
                },
                {
                    "sent": "And as we can see here from this plot.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We have here we have the distribution of the weights and as we can see that in the case of this summer, too well some optimally square program, we have larger weights in amplitude in proximity of the fall.",
                    "label": 1
                },
                {
                    "sent": "That means that when the.",
                    "label": 0
                },
                {
                    "sent": "When the dimension is.",
                    "label": 0
                },
                {
                    "sent": "Equals today the dimension of the problem, then the weights computed are perfectly fitted on the low resolution patches, but then when applied to the high resolution reconstructions, they lead to better constructions and for the non negative ways.",
                    "label": 0
                },
                {
                    "sent": "Instead we observe.",
                    "label": 0
                },
                {
                    "sent": "That the way the decade in amplitude with K that's good and the good Nestle is non negative embedding is confirmed also in the second plot where we have the distance between the actual low resolution weights that we can compute and the idea always that we would compute if we had the ground truth vectors.",
                    "label": 0
                },
                {
                    "sent": "Now the third key point.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Which is the dictionary.",
                    "label": 0
                },
                {
                    "sent": "As said, we have two possibilities.",
                    "label": 1
                },
                {
                    "sent": "First one is to use external training images and.",
                    "label": 0
                },
                {
                    "sent": "Sampling and sample value from them.",
                    "label": 1
                },
                {
                    "sent": "The patches second possibilities to learn the dispatch.",
                    "label": 0
                },
                {
                    "sent": "Correspondence is in a pyramid of recursively scaled images starting from the low resolution input image and this is what is done in there well famous algorithm of glasner.",
                    "label": 1
                },
                {
                    "sent": "And as we can see from the table, in our case, since we we want to use a one pass procedure.",
                    "label": 0
                },
                {
                    "sent": "And we can extract from this pyramid of recursively scale images.",
                    "label": 1
                },
                {
                    "sent": "We can extract patches only if the distance between the two layers is exactly the magnification factor, and so in our case we experience that the number of patches that we can extract from the pyramid is not sufficient.",
                    "label": 0
                },
                {
                    "sent": "So therefore we use an external dictionary.",
                    "label": 0
                },
                {
                    "sent": "So just do some.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nice three key points where the feature representations used and we decided to use central luminous features for both kinds of patches.",
                    "label": 0
                },
                {
                    "sent": "Then it the first the weight computation method, then we will use this new nonnegative embedding and the third one was the choice of the dictionary, which is an external dictionary.",
                    "label": 1
                },
                {
                    "sent": "And as we can see that the curves that we obtain.",
                    "label": 0
                },
                {
                    "sent": "Are much better than the previous ones with the Eli based algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now the experiment section.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We consider this to this for algorithms.",
                    "label": 0
                },
                {
                    "sent": "The first one is the energy based algorithm of charger.",
                    "label": 0
                },
                {
                    "sent": "Second one is the pyramid algorithm of glass metal.",
                    "label": 0
                },
                {
                    "sent": "Then we have algorithm of time based on based on a kernel Ridge regression, an hour algorithm.",
                    "label": 1
                },
                {
                    "sent": "As we can see we are different features used different methods for the pet reconstruction an different procedures.",
                    "label": 0
                },
                {
                    "sent": "Single step or multi pass?",
                    "label": 0
                },
                {
                    "sent": "An as you can see from the.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Table.",
                    "label": 0
                },
                {
                    "sent": "Our algorithm is the best performing for a scale factor of two and is slightly worse than glasner on in full scale factors of three and four.",
                    "label": 0
                },
                {
                    "sent": "This is for the PS in our performance whereas.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the time performance this is a rough estimation of the compositional load is just a measure of intelligence of the running time.",
                    "label": 0
                },
                {
                    "sent": "And as we can see, our ad is presents it sensibly reduced computational time, and this is thanks to two main things, either thanks to the one pass procedure.",
                    "label": 0
                },
                {
                    "sent": "Or thanks to the shorted feature vectors, because we are never algorithm used this center features which is 1 value per pixel, whereas in other algorithms they use two or four values per pixel.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also Additionally we also try to combine the approach of Tanga bought the regression method but with our futures.",
                    "label": 0
                },
                {
                    "sent": "And as we can see the results are even better.",
                    "label": 0
                },
                {
                    "sent": "So better PS in our values and also computational time compara bulto our first algorithm.",
                    "label": 0
                },
                {
                    "sent": "These are some.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Visual results, so this is an image we want to magnify by a factor of 3.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is the becoming interpolation.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Method of chunk.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Glasner tongue.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hours.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For another one in.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Boot image again, magnified it by you 3.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The big.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interpolation.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Method of charging daily.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the neighbor embedding.",
                    "label": 0
                },
                {
                    "sent": "Glasner tongue.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An hours and in both cases the time is much less in our case.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To conclude, our method presents.",
                    "label": 0
                },
                {
                    "sent": "Better results than other one pass algorithms.",
                    "label": 1
                },
                {
                    "sent": "The two we presented challenging time.",
                    "label": 0
                },
                {
                    "sent": "The results are compareable to the multipass algorithm of glasner.",
                    "label": 1
                },
                {
                    "sent": "But presents of course much lower computational time for the future work.",
                    "label": 0
                },
                {
                    "sent": "We want to further study the regression method as we already obtained promising results, and also possibly to design other strategies to select the neighbors.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}