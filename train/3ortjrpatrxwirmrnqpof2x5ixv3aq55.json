{
    "id": "3ortjrpatrxwirmrnqpof2x5ixv3aq55",
    "title": "On the Chance Accuracies of Large Collections of Classifiers",
    "info": {
        "author": [
            "Mark Palatucci, Robotics Institute, School of Computer Science, Carnegie Mellon University"
        ],
        "published": "Aug. 29, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_palatucci_oca/",
    "segmentation": [
        [
            "So there are a lot of problems in modern machine learning that deal with very high dimensionality.",
            "We've seen a few of them already today and.",
            "Often in these problems you have many, many more dimensions than you have features in some case."
        ],
        [
            "Orders of magnitude.",
            "So a few examples.",
            "These would be a gene study with a microarray.",
            "So you might be interested in.",
            "Given that I have a micro array of gene expression data, is this patient going to develop cancer or not?",
            "An in these tasks?",
            "Usually you might have order of two 3000 features, sometimes more an on the order of maybe about 100 training examples."
        ],
        [
            "A more extreme case would be cognitive state classification using functional magnetic resonance imaging.",
            "So here.",
            "You might have some MRI image of a person's neural activation and you want to ask what is the person actually thinking about an in these tasks.",
            "You can have hundreds and hundreds of thousands of features.",
            "An usually may have only less than 100 examples."
        ],
        [
            "So in order to build a classifier, usually have to go through some sort of dimensionality reduction step and here are common ways that you can do this.",
            "Usually in an fMRI study, there's only a very small number of features that actually contain information about the tasks, maybe on the order of 10%, sometimes 20 most relevant so.",
            "There are many different feature selection methods.",
            "One of those common is embedded methods.",
            "So you could use L1 regularize logistic regression.",
            "Similar to all the lasso type type things we've been hearing about support vector decomposition machine is a combination of a simultaneous dimensionality reduction along with an SVM.",
            "There are these things called filter methods, which are basically preprocessing steps.",
            "And there you might just say average over a temporal dimension, maybe over regions of space could also run PCA or ICA.",
            "And another common thing is to use multiple hypothesis testing.",
            "Try to determine which are the most active voxels and usually use at Test or something like that.",
            "There are these wrapper methods.",
            "Here it wraps around some sort of induction, other induction algorithm, and people are probably familiar with forward or backward stepwise selection.",
            "Or you might just choose the highest discriminating voxels on some sort of validation step, and this is what I want to talk about a little bit more detail.",
            "This is very common for a number of algorithms.",
            "Sure, everybody's probably used it."
        ],
        [
            "At some point so.",
            "Just a quick review.",
            "Discriminative feature selection might have some training set examples you have a little validation set and then some test an you train a classifier using on each individual feature.",
            "And then you would evaluate those classifiers on some set of validation examples.",
            "And then you're going to choose, say like the N best performing features or all the features that may be performed better than a certain percentage accuracy.",
            "So a natural question is, well, how do we choose a good value of N?",
            "How do we choose a good accuracy?"
        ],
        [
            "For this, an often I've seen a lot of papers where people may just pick that value like arbitrarily, so they might say, OK, let's choose N = 500, or let's use everything better than 70% accuracy.",
            "What we're going to talk about in more detail later is that this is actually something that's very dangerous to do when you have a problem that's such a huge dimensionality and have so few features.",
            "Another thing you might do is to use a statistical hypothesis test.",
            "But there your question is alright.",
            "Well, how do you choose a good Alpha value?",
            "And there's not really a good Alpha value that works for problems with various dimensions.",
            "And then if you've done any hypothesis testing, usually know that if you run multiple tests rather concurrently, you have to usually correct that Alpha value to adjust for the multiple tests, and that's usually you do something called the Bonferroni correction, or maybe the false discovery rate there.",
            "Another thing you could do is just have a second validation step, but that's pretty computationally expensive an it's also problematic 'cause we have so little data to deal with that we don't want to have a whole other Val."
        ],
        [
            "Nation steps so.",
            "Let's consider a simple experiment.",
            "This is an fMRI classification task and what we want to do is we want to get a sense of how many features are going to be irrelevant, right?",
            "So here there's 80,000 original features to two class.",
            "Task and we're going to train Gaussian naive Bayes on each feature.",
            "I'm going to valuate it on a set of 40 examples, and this is basically a histogram.",
            "Of those 80,000 features and their respective accuracies, so 50% accuracy would be right about here, about little more than 20 an you see that you get, you know several 1000 features that get below 50% accuracy, and then you get a number that perform better.",
            "The mean is slightly better than 50%.",
            "An you know natural question would be what's a good cut off to use an also.",
            "What if all the features in this task were irrelevant?",
            "What would this histogram actually look like?",
            "And you know, more importantly, is how well would we expect you know that some kind of noisy feature could do."
        ],
        [
            "Just by chance, so we can ask this question formally and we can say given M classifiers that each produce labels randomly.",
            "On an example.",
            "So what is the expected accuracy of the best one?"
        ],
        [
            "So.",
            "We can answer this question actually using order statistics an we're going to do this.",
            "We're going to model the number of errors made by the classifier is just a binomial random variable and then we can actually order the samples from smallest to largest.",
            "OK, and we're going to call the Earth smallest value.",
            "Actually, the Earth order statistic here noted with this notation here.",
            "Anne.",
            "If you've never used order statistics before, they're pretty neat because.",
            "This are statistic.",
            "Is actually a random variable itself, and it has its own distribution and it has moments, so it has a mean, and it has a variance."
        ],
        [
            "So.",
            "What we can do then is we can say, alright, well if I draw a bunch of variables from a binomial an I say alright.",
            "Well, the classifier with the smallest number of errors.",
            "The best best accuracy that's going to be the first order statistic, just them in.",
            "And then I can say, alright, well, what's the expected minimum number of errors, OK?",
            "So.",
            "If the original distribution were continuous value in these orders, statistics are actually pretty well understood.",
            "And it's pretty easy to get.",
            "The distribution and also the moments but.",
            "If you're dealing with discrete variables, it's actually a lot harder and took us a long time to figure it out.",
            "We worked with some order statistics professors an."
        ],
        [
            "We were able to get.",
            "The theorem out of this an the derivation of this proof is in the paper, but essentially our theorem basically says the highest chance accuracy of a collection of random features is.",
            "It's going to be given by this expression.",
            "Where this component of it is actually computing that moment of the 1st order statistic and then this is a sum.",
            "Over this incomplete beta function that we're evaluating OK, Ann.",
            "It's a little hard to understand why the incomplete data is there, but if you read the paper it will make a lot more sense.",
            "So to use this expression, it's pretty easy you just.",
            "And specifies the number of examples in your problem.",
            "Number of classifiers that you have and then the probability of a single classifier making an error on an example is given by Pierre, so this doesn't depend on the number of classes.",
            "It's not limited to two.",
            "You could this just changes based on the number of classes in your problem so.",
            "In some sense, this theorem kind of defines we call as a natural significance threshold, 'cause it's basically just a function of the number of examples in the number of classifiers in the problem.",
            "Anne."
        ],
        [
            "Let's do a quick.",
            "For example, to give you some intuition about how this theorem works, so consider that you have a football pool at your office Ann.",
            "Each person you have 200 participants and you're going to bet on the outcome of 20 football games.",
            "And the question would be or how well would we expect the winner to do?",
            "If no one really knows anything about football and you just choose the outcome of the game at random according to say, like a Bernoulli trial.",
            "So I don't know if anyone has any guesses.",
            "But in this case.",
            "The accuracy of the winner is going to be 80%.",
            "So someone gets 80% accuracy.",
            "You think oh this guy knows something about football.",
            "And here the math is telling us that that's not true and you're actually going to see someone that gets that accuracy, even though you'd expect that you need a million participants before you get one.",
            "That would get a perfect labeling.",
            "So.",
            "The takeaway point here is that.",
            "The chance of obtaining a very good labeling can actually be very high, even if the chance of obtaining a perfect labeling is very low."
        ],
        [
            "So.",
            "If we look at a plot of that function of that theorem right here.",
            "In our case, we're going to consider a two class problem where are probably making mistake is .5.",
            "Alright, we can see that you'd expect the individual classifier to get 50% accuracy.",
            "OK, but.",
            "You see that there's a huge gap here where with a very small number of examples, very small number of choices that you're making as few as long as you have a few classifiers, you're going to actually see one that has a very, very high accuracy, or you on expectation OK, and then as you increase the number of choices that you have to make the decisions that you have to make, then this gap actually shrinks towards 50%, which is what you'd expect."
        ],
        [
            "So.",
            "Can we use this theorem to provide a more principled threshold for doing discriminative feature?"
        ],
        [
            "Election.",
            "And yes, we can, but.",
            "We have to consider the assumptions that the theorem makes.",
            "So first of all, the theorem assumes that.",
            "All the features are independent.",
            "And that their noisy, and that they're just producing labels at random.",
            "We also assume that the probability of making an error can be modeled querying this Bernoulli trial OK, but in practice obviously you're going to have some features, even a very sparse problem.",
            "You have some features that are relevant.",
            "And then often you're going to have features that are correlated, so both of those.",
            "Both of these factors will actually cause you to have a lower expected accuracy, but the theorem gives us this useful upper bound on the expected accuracy of the relevant features an we can show how to relax that later on to make."
        ],
        [
            "More useful.",
            "So.",
            "Get some more intuition.",
            "Consider that we have a classification problem.",
            "Let's say we have 80,000 features an.",
            "We're going to train and evaluate each feature on a set of 40 examples.",
            "There's two classes, so.",
            "If a feature is irrelevant, we'd expect that the accuracy of a single feature is going to be 50%.",
            "It's going to get roughly 20 out of the out of those 40 examples, correct?",
            "If all the features are irrelevant, then we expect.",
            "The best one to get 83 percent 33 correct.",
            "And if our violations that were even smaller, we'd see that this is actually really high that we see some feature that we could get 94% accuracy.",
            "So there's this gap between the expected accuracy of an individual feature and the best one.",
            "And this gap is a function of the number of examples in the number of features.",
            "So as you increase the number of classifiers, right, that gap grows.",
            "Similarly, if you make the number of examples that you.",
            "Sort of examples.",
            "You're producing labels for gets smaller than that gap grows."
        ],
        [
            "So we wanted to find this gap formally an so once again we're going to model our errors according to binomial.",
            "Where we model the smallest number of errors once again in the order statistic, the 1st order statistic.",
            "And then we're going to find this what we call the multiplicity gap.",
            "Which is essentially the difference between the expected.",
            "Accuracy over single classifier and then the accuracy of the best random one.",
            "OK, and that once again as a function of examples in classifier and the number of."
        ],
        [
            "Classifiers, yeah, so.",
            "Let's take a look in experiment.",
            "Once again, we're going to talk about our MRI here and the classification task is is the person viewing a picture or are they reading a sentence?",
            "There's 80,000 features in this task.",
            "Two classes, 40 examples.",
            "We're going to train using a Gaussian naive Bayes classifier.",
            "Leave one out cross validation and our training procedure is basically we're going to train on.",
            "19 In the examples we're going to evaluate on this 20 validation examples.",
            "We're going to choose features that made no more than certain number of errors.",
            "And then we're going to take those features that pass the threshold, and we're going to train on the 19 + 20.",
            "So all 39 tests the held out example, and then we're going to beat that for every single example.",
            "And then what we're going to do is we're actually going to try many different error thresholds."
        ],
        [
            "To get some intuition about what that looks like so.",
            "The X axis in this graph is.",
            "The number of errors that we allow in the validation set, so we'll choose a feature if it makes less than these number of errors.",
            "So with 20 validation, 50% accuracy would be 10%.",
            "Ann this accuracy over here are the Y axis is the accuracy on the resulting test data on the test set.",
            "So we're going to plot these two extremes.",
            "This is the expected value of.",
            "The expected error on a single individual classifier.",
            "It's 50% here.",
            "OK, and then we're going to plot our expected value of the best guy.",
            "Alright, and our conjecture is that.",
            "The peak the best test accuracy is going to result when you choose a threshold that falls in this multiplicity gap.",
            "OK, and here you get the best test accuracy.",
            "If you had chosen.",
            "Five errors as your threshold."
        ],
        [
            "So.",
            "Here's that same curve, but for a few other experiments.",
            "OK, Anne.",
            "You see the peak.",
            "In each one."
        ],
        [
            "So we did this an.",
            "Empirically, we found that that peak did in fact fall in the multiplicity gap for all of our experiments in the paper.",
            "We did this for 14 and so the question would be alright.",
            "Well, you know how do I choose a good threshold?",
            "Anne."
        ],
        [
            "And we've.",
            "We show that we have these two extremes.",
            "Anne.",
            "If you expect the peak to kind of fall randomly in there, then what you can do with the threshold you can pick so that you're not too far away would be actually the midpoint."
        ],
        [
            "So.",
            "This is where we use this user theorem.",
            "We get a heuristic and we call the multiplicity gap midpoint method.",
            "An essentially all you do is you take the mean.",
            "Are the midpoint of the two extremes.",
            "Alright, so this you essentially calculate using the theorem that we presented before.",
            "This is just the random accuracy.",
            "I'm an individual guy.",
            "This gives you a threshold and."
        ],
        [
            "You use that so.",
            "I'm going to test this on the task we just talked about, but we actually going to test it on 13 different subjects individually.",
            "And then the results were going to show next are going to be the average result together, and then we tested on a microarray data set.",
            "So here the task was determined whether or not the patient is going to get cancer based on their gene expression levels of 2000 features in 60 examples.",
            "And then we use the same experimental setup that we did before with Gaussian naive Bayes and leave."
        ],
        [
            "Cross validation.",
            "So.",
            "If we look at this curve, we look at.",
            "This is the cancer and this is the fMRI.",
            "We have five different methods.",
            "The dark blue here is the result that we get if we just didn't do any feature selection at all.",
            "OK.",
            "This slightly lighter blue is the result is that if you use that.",
            "The actual threshold that the theorem gives you directly, which is about which we already talked about, is very conservative.",
            "This green value is kind of a state of the art multiple hypothesis test correction, which is the false discovery rate, and we used a standard significance level there.",
            "In the Orange, is this multiplicity gap midpoint relaxation?",
            "And the yellow if we compare against is we call like the Oracle threshold, meaning that if an Oracle told us the threshold to use in the validation set, that would lead to the highest test accuracy.",
            "This is the test accuracy that you would get and you see that the midpoint actually comes very close to that.",
            "And then if we look at the plots, we often find that peak does actually fall close to the midpoint."
        ],
        [
            "So.",
            "The takeaways here is that we developed this chance accuracy theorem that gives you intuitive number an.",
            "We claim that it defines this kind of natural significance threshold.",
            "That's a function of the number of examples of number of features in the problem, and then we've used that to provide this feature selection heuristic that we've shown empirically actually works really well on these high dimensional sparse datasets.",
            "An you can compute it actually in just one line of Matlab, so that incomplete beta function, Matlab and Mathematica, and a lot of numerical computing environments have that.",
            "I have really fast implementations, so you can write one line of Matlab to compute that value, and if you're interested in the proof or some some more results, just check out the paper."
        ],
        [
            "And anyway, I'd like to thank.",
            "Our statistician who helped us duration an also.",
            "Tech Foundation, Yahoo and NSF for their financial support.",
            "Thank you.",
            "Excuse me feature or not.",
            "Yeah so.",
            "Ideally right, you could come up with some principled method where you could measure.",
            "Maybe like the peak correlation or something like that.",
            "So the midpoint is a way to adjust for that right?",
            "This midpoint method that we talked about, and that's why it's a heuristic, right?",
            "So the theorem itself makes these assumptions which don't entirely hold, meaning that that threshold is going to be too conservative if the features are correlated.",
            "If they're independent and we tested this, actually the expected accuracy that you would get goes down, right?",
            "So this heuristic?",
            "Tries to balance that off.",
            "Tries to balance that tradeoff and adjust for the fact that those assumptions don't hold.",
            "But I think in an ideal world, and I've thought about ways to try to do this, is to kind of maybe measure like the highest correlation between features and then use that.",
            "To May automatically choose, choose that threshold, but in practice we actually found that the midpoint worked really well."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there are a lot of problems in modern machine learning that deal with very high dimensionality.",
                    "label": 0
                },
                {
                    "sent": "We've seen a few of them already today and.",
                    "label": 0
                },
                {
                    "sent": "Often in these problems you have many, many more dimensions than you have features in some case.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Orders of magnitude.",
                    "label": 0
                },
                {
                    "sent": "So a few examples.",
                    "label": 0
                },
                {
                    "sent": "These would be a gene study with a microarray.",
                    "label": 1
                },
                {
                    "sent": "So you might be interested in.",
                    "label": 0
                },
                {
                    "sent": "Given that I have a micro array of gene expression data, is this patient going to develop cancer or not?",
                    "label": 1
                },
                {
                    "sent": "An in these tasks?",
                    "label": 0
                },
                {
                    "sent": "Usually you might have order of two 3000 features, sometimes more an on the order of maybe about 100 training examples.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A more extreme case would be cognitive state classification using functional magnetic resonance imaging.",
                    "label": 1
                },
                {
                    "sent": "So here.",
                    "label": 0
                },
                {
                    "sent": "You might have some MRI image of a person's neural activation and you want to ask what is the person actually thinking about an in these tasks.",
                    "label": 1
                },
                {
                    "sent": "You can have hundreds and hundreds of thousands of features.",
                    "label": 0
                },
                {
                    "sent": "An usually may have only less than 100 examples.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in order to build a classifier, usually have to go through some sort of dimensionality reduction step and here are common ways that you can do this.",
                    "label": 0
                },
                {
                    "sent": "Usually in an fMRI study, there's only a very small number of features that actually contain information about the tasks, maybe on the order of 10%, sometimes 20 most relevant so.",
                    "label": 0
                },
                {
                    "sent": "There are many different feature selection methods.",
                    "label": 1
                },
                {
                    "sent": "One of those common is embedded methods.",
                    "label": 1
                },
                {
                    "sent": "So you could use L1 regularize logistic regression.",
                    "label": 1
                },
                {
                    "sent": "Similar to all the lasso type type things we've been hearing about support vector decomposition machine is a combination of a simultaneous dimensionality reduction along with an SVM.",
                    "label": 0
                },
                {
                    "sent": "There are these things called filter methods, which are basically preprocessing steps.",
                    "label": 0
                },
                {
                    "sent": "And there you might just say average over a temporal dimension, maybe over regions of space could also run PCA or ICA.",
                    "label": 1
                },
                {
                    "sent": "And another common thing is to use multiple hypothesis testing.",
                    "label": 1
                },
                {
                    "sent": "Try to determine which are the most active voxels and usually use at Test or something like that.",
                    "label": 0
                },
                {
                    "sent": "There are these wrapper methods.",
                    "label": 0
                },
                {
                    "sent": "Here it wraps around some sort of induction, other induction algorithm, and people are probably familiar with forward or backward stepwise selection.",
                    "label": 1
                },
                {
                    "sent": "Or you might just choose the highest discriminating voxels on some sort of validation step, and this is what I want to talk about a little bit more detail.",
                    "label": 0
                },
                {
                    "sent": "This is very common for a number of algorithms.",
                    "label": 0
                },
                {
                    "sent": "Sure, everybody's probably used it.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At some point so.",
                    "label": 0
                },
                {
                    "sent": "Just a quick review.",
                    "label": 0
                },
                {
                    "sent": "Discriminative feature selection might have some training set examples you have a little validation set and then some test an you train a classifier using on each individual feature.",
                    "label": 1
                },
                {
                    "sent": "And then you would evaluate those classifiers on some set of validation examples.",
                    "label": 0
                },
                {
                    "sent": "And then you're going to choose, say like the N best performing features or all the features that may be performed better than a certain percentage accuracy.",
                    "label": 0
                },
                {
                    "sent": "So a natural question is, well, how do we choose a good value of N?",
                    "label": 0
                },
                {
                    "sent": "How do we choose a good accuracy?",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For this, an often I've seen a lot of papers where people may just pick that value like arbitrarily, so they might say, OK, let's choose N = 500, or let's use everything better than 70% accuracy.",
                    "label": 0
                },
                {
                    "sent": "What we're going to talk about in more detail later is that this is actually something that's very dangerous to do when you have a problem that's such a huge dimensionality and have so few features.",
                    "label": 0
                },
                {
                    "sent": "Another thing you might do is to use a statistical hypothesis test.",
                    "label": 1
                },
                {
                    "sent": "But there your question is alright.",
                    "label": 0
                },
                {
                    "sent": "Well, how do you choose a good Alpha value?",
                    "label": 1
                },
                {
                    "sent": "And there's not really a good Alpha value that works for problems with various dimensions.",
                    "label": 0
                },
                {
                    "sent": "And then if you've done any hypothesis testing, usually know that if you run multiple tests rather concurrently, you have to usually correct that Alpha value to adjust for the multiple tests, and that's usually you do something called the Bonferroni correction, or maybe the false discovery rate there.",
                    "label": 0
                },
                {
                    "sent": "Another thing you could do is just have a second validation step, but that's pretty computationally expensive an it's also problematic 'cause we have so little data to deal with that we don't want to have a whole other Val.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nation steps so.",
                    "label": 0
                },
                {
                    "sent": "Let's consider a simple experiment.",
                    "label": 1
                },
                {
                    "sent": "This is an fMRI classification task and what we want to do is we want to get a sense of how many features are going to be irrelevant, right?",
                    "label": 0
                },
                {
                    "sent": "So here there's 80,000 original features to two class.",
                    "label": 0
                },
                {
                    "sent": "Task and we're going to train Gaussian naive Bayes on each feature.",
                    "label": 1
                },
                {
                    "sent": "I'm going to valuate it on a set of 40 examples, and this is basically a histogram.",
                    "label": 0
                },
                {
                    "sent": "Of those 80,000 features and their respective accuracies, so 50% accuracy would be right about here, about little more than 20 an you see that you get, you know several 1000 features that get below 50% accuracy, and then you get a number that perform better.",
                    "label": 1
                },
                {
                    "sent": "The mean is slightly better than 50%.",
                    "label": 1
                },
                {
                    "sent": "An you know natural question would be what's a good cut off to use an also.",
                    "label": 0
                },
                {
                    "sent": "What if all the features in this task were irrelevant?",
                    "label": 1
                },
                {
                    "sent": "What would this histogram actually look like?",
                    "label": 0
                },
                {
                    "sent": "And you know, more importantly, is how well would we expect you know that some kind of noisy feature could do.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just by chance, so we can ask this question formally and we can say given M classifiers that each produce labels randomly.",
                    "label": 1
                },
                {
                    "sent": "On an example.",
                    "label": 0
                },
                {
                    "sent": "So what is the expected accuracy of the best one?",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "We can answer this question actually using order statistics an we're going to do this.",
                    "label": 0
                },
                {
                    "sent": "We're going to model the number of errors made by the classifier is just a binomial random variable and then we can actually order the samples from smallest to largest.",
                    "label": 1
                },
                {
                    "sent": "OK, and we're going to call the Earth smallest value.",
                    "label": 0
                },
                {
                    "sent": "Actually, the Earth order statistic here noted with this notation here.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "If you've never used order statistics before, they're pretty neat because.",
                    "label": 0
                },
                {
                    "sent": "This are statistic.",
                    "label": 0
                },
                {
                    "sent": "Is actually a random variable itself, and it has its own distribution and it has moments, so it has a mean, and it has a variance.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What we can do then is we can say, alright, well if I draw a bunch of variables from a binomial an I say alright.",
                    "label": 0
                },
                {
                    "sent": "Well, the classifier with the smallest number of errors.",
                    "label": 1
                },
                {
                    "sent": "The best best accuracy that's going to be the first order statistic, just them in.",
                    "label": 1
                },
                {
                    "sent": "And then I can say, alright, well, what's the expected minimum number of errors, OK?",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If the original distribution were continuous value in these orders, statistics are actually pretty well understood.",
                    "label": 0
                },
                {
                    "sent": "And it's pretty easy to get.",
                    "label": 0
                },
                {
                    "sent": "The distribution and also the moments but.",
                    "label": 0
                },
                {
                    "sent": "If you're dealing with discrete variables, it's actually a lot harder and took us a long time to figure it out.",
                    "label": 0
                },
                {
                    "sent": "We worked with some order statistics professors an.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We were able to get.",
                    "label": 0
                },
                {
                    "sent": "The theorem out of this an the derivation of this proof is in the paper, but essentially our theorem basically says the highest chance accuracy of a collection of random features is.",
                    "label": 0
                },
                {
                    "sent": "It's going to be given by this expression.",
                    "label": 0
                },
                {
                    "sent": "Where this component of it is actually computing that moment of the 1st order statistic and then this is a sum.",
                    "label": 0
                },
                {
                    "sent": "Over this incomplete beta function that we're evaluating OK, Ann.",
                    "label": 0
                },
                {
                    "sent": "It's a little hard to understand why the incomplete data is there, but if you read the paper it will make a lot more sense.",
                    "label": 0
                },
                {
                    "sent": "So to use this expression, it's pretty easy you just.",
                    "label": 0
                },
                {
                    "sent": "And specifies the number of examples in your problem.",
                    "label": 0
                },
                {
                    "sent": "Number of classifiers that you have and then the probability of a single classifier making an error on an example is given by Pierre, so this doesn't depend on the number of classes.",
                    "label": 1
                },
                {
                    "sent": "It's not limited to two.",
                    "label": 0
                },
                {
                    "sent": "You could this just changes based on the number of classes in your problem so.",
                    "label": 0
                },
                {
                    "sent": "In some sense, this theorem kind of defines we call as a natural significance threshold, 'cause it's basically just a function of the number of examples in the number of classifiers in the problem.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let's do a quick.",
                    "label": 0
                },
                {
                    "sent": "For example, to give you some intuition about how this theorem works, so consider that you have a football pool at your office Ann.",
                    "label": 0
                },
                {
                    "sent": "Each person you have 200 participants and you're going to bet on the outcome of 20 football games.",
                    "label": 1
                },
                {
                    "sent": "And the question would be or how well would we expect the winner to do?",
                    "label": 1
                },
                {
                    "sent": "If no one really knows anything about football and you just choose the outcome of the game at random according to say, like a Bernoulli trial.",
                    "label": 0
                },
                {
                    "sent": "So I don't know if anyone has any guesses.",
                    "label": 1
                },
                {
                    "sent": "But in this case.",
                    "label": 0
                },
                {
                    "sent": "The accuracy of the winner is going to be 80%.",
                    "label": 0
                },
                {
                    "sent": "So someone gets 80% accuracy.",
                    "label": 0
                },
                {
                    "sent": "You think oh this guy knows something about football.",
                    "label": 1
                },
                {
                    "sent": "And here the math is telling us that that's not true and you're actually going to see someone that gets that accuracy, even though you'd expect that you need a million participants before you get one.",
                    "label": 0
                },
                {
                    "sent": "That would get a perfect labeling.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The takeaway point here is that.",
                    "label": 0
                },
                {
                    "sent": "The chance of obtaining a very good labeling can actually be very high, even if the chance of obtaining a perfect labeling is very low.",
                    "label": 1
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If we look at a plot of that function of that theorem right here.",
                    "label": 0
                },
                {
                    "sent": "In our case, we're going to consider a two class problem where are probably making mistake is .5.",
                    "label": 0
                },
                {
                    "sent": "Alright, we can see that you'd expect the individual classifier to get 50% accuracy.",
                    "label": 0
                },
                {
                    "sent": "OK, but.",
                    "label": 0
                },
                {
                    "sent": "You see that there's a huge gap here where with a very small number of examples, very small number of choices that you're making as few as long as you have a few classifiers, you're going to actually see one that has a very, very high accuracy, or you on expectation OK, and then as you increase the number of choices that you have to make the decisions that you have to make, then this gap actually shrinks towards 50%, which is what you'd expect.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Can we use this theorem to provide a more principled threshold for doing discriminative feature?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Election.",
                    "label": 0
                },
                {
                    "sent": "And yes, we can, but.",
                    "label": 0
                },
                {
                    "sent": "We have to consider the assumptions that the theorem makes.",
                    "label": 0
                },
                {
                    "sent": "So first of all, the theorem assumes that.",
                    "label": 0
                },
                {
                    "sent": "All the features are independent.",
                    "label": 1
                },
                {
                    "sent": "And that their noisy, and that they're just producing labels at random.",
                    "label": 0
                },
                {
                    "sent": "We also assume that the probability of making an error can be modeled querying this Bernoulli trial OK, but in practice obviously you're going to have some features, even a very sparse problem.",
                    "label": 0
                },
                {
                    "sent": "You have some features that are relevant.",
                    "label": 0
                },
                {
                    "sent": "And then often you're going to have features that are correlated, so both of those.",
                    "label": 0
                },
                {
                    "sent": "Both of these factors will actually cause you to have a lower expected accuracy, but the theorem gives us this useful upper bound on the expected accuracy of the relevant features an we can show how to relax that later on to make.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More useful.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Get some more intuition.",
                    "label": 0
                },
                {
                    "sent": "Consider that we have a classification problem.",
                    "label": 0
                },
                {
                    "sent": "Let's say we have 80,000 features an.",
                    "label": 0
                },
                {
                    "sent": "We're going to train and evaluate each feature on a set of 40 examples.",
                    "label": 1
                },
                {
                    "sent": "There's two classes, so.",
                    "label": 0
                },
                {
                    "sent": "If a feature is irrelevant, we'd expect that the accuracy of a single feature is going to be 50%.",
                    "label": 1
                },
                {
                    "sent": "It's going to get roughly 20 out of the out of those 40 examples, correct?",
                    "label": 1
                },
                {
                    "sent": "If all the features are irrelevant, then we expect.",
                    "label": 0
                },
                {
                    "sent": "The best one to get 83 percent 33 correct.",
                    "label": 1
                },
                {
                    "sent": "And if our violations that were even smaller, we'd see that this is actually really high that we see some feature that we could get 94% accuracy.",
                    "label": 0
                },
                {
                    "sent": "So there's this gap between the expected accuracy of an individual feature and the best one.",
                    "label": 1
                },
                {
                    "sent": "And this gap is a function of the number of examples in the number of features.",
                    "label": 0
                },
                {
                    "sent": "So as you increase the number of classifiers, right, that gap grows.",
                    "label": 0
                },
                {
                    "sent": "Similarly, if you make the number of examples that you.",
                    "label": 0
                },
                {
                    "sent": "Sort of examples.",
                    "label": 0
                },
                {
                    "sent": "You're producing labels for gets smaller than that gap grows.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we wanted to find this gap formally an so once again we're going to model our errors according to binomial.",
                    "label": 0
                },
                {
                    "sent": "Where we model the smallest number of errors once again in the order statistic, the 1st order statistic.",
                    "label": 1
                },
                {
                    "sent": "And then we're going to find this what we call the multiplicity gap.",
                    "label": 0
                },
                {
                    "sent": "Which is essentially the difference between the expected.",
                    "label": 1
                },
                {
                    "sent": "Accuracy over single classifier and then the accuracy of the best random one.",
                    "label": 0
                },
                {
                    "sent": "OK, and that once again as a function of examples in classifier and the number of.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Classifiers, yeah, so.",
                    "label": 0
                },
                {
                    "sent": "Let's take a look in experiment.",
                    "label": 0
                },
                {
                    "sent": "Once again, we're going to talk about our MRI here and the classification task is is the person viewing a picture or are they reading a sentence?",
                    "label": 1
                },
                {
                    "sent": "There's 80,000 features in this task.",
                    "label": 0
                },
                {
                    "sent": "Two classes, 40 examples.",
                    "label": 1
                },
                {
                    "sent": "We're going to train using a Gaussian naive Bayes classifier.",
                    "label": 0
                },
                {
                    "sent": "Leave one out cross validation and our training procedure is basically we're going to train on.",
                    "label": 0
                },
                {
                    "sent": "19 In the examples we're going to evaluate on this 20 validation examples.",
                    "label": 1
                },
                {
                    "sent": "We're going to choose features that made no more than certain number of errors.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to take those features that pass the threshold, and we're going to train on the 19 + 20.",
                    "label": 0
                },
                {
                    "sent": "So all 39 tests the held out example, and then we're going to beat that for every single example.",
                    "label": 0
                },
                {
                    "sent": "And then what we're going to do is we're actually going to try many different error thresholds.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To get some intuition about what that looks like so.",
                    "label": 0
                },
                {
                    "sent": "The X axis in this graph is.",
                    "label": 0
                },
                {
                    "sent": "The number of errors that we allow in the validation set, so we'll choose a feature if it makes less than these number of errors.",
                    "label": 0
                },
                {
                    "sent": "So with 20 validation, 50% accuracy would be 10%.",
                    "label": 0
                },
                {
                    "sent": "Ann this accuracy over here are the Y axis is the accuracy on the resulting test data on the test set.",
                    "label": 0
                },
                {
                    "sent": "So we're going to plot these two extremes.",
                    "label": 0
                },
                {
                    "sent": "This is the expected value of.",
                    "label": 0
                },
                {
                    "sent": "The expected error on a single individual classifier.",
                    "label": 0
                },
                {
                    "sent": "It's 50% here.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we're going to plot our expected value of the best guy.",
                    "label": 0
                },
                {
                    "sent": "Alright, and our conjecture is that.",
                    "label": 0
                },
                {
                    "sent": "The peak the best test accuracy is going to result when you choose a threshold that falls in this multiplicity gap.",
                    "label": 1
                },
                {
                    "sent": "OK, and here you get the best test accuracy.",
                    "label": 0
                },
                {
                    "sent": "If you had chosen.",
                    "label": 0
                },
                {
                    "sent": "Five errors as your threshold.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Here's that same curve, but for a few other experiments.",
                    "label": 0
                },
                {
                    "sent": "OK, Anne.",
                    "label": 0
                },
                {
                    "sent": "You see the peak.",
                    "label": 0
                },
                {
                    "sent": "In each one.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we did this an.",
                    "label": 0
                },
                {
                    "sent": "Empirically, we found that that peak did in fact fall in the multiplicity gap for all of our experiments in the paper.",
                    "label": 1
                },
                {
                    "sent": "We did this for 14 and so the question would be alright.",
                    "label": 1
                },
                {
                    "sent": "Well, you know how do I choose a good threshold?",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we've.",
                    "label": 0
                },
                {
                    "sent": "We show that we have these two extremes.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "If you expect the peak to kind of fall randomly in there, then what you can do with the threshold you can pick so that you're not too far away would be actually the midpoint.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "This is where we use this user theorem.",
                    "label": 0
                },
                {
                    "sent": "We get a heuristic and we call the multiplicity gap midpoint method.",
                    "label": 1
                },
                {
                    "sent": "An essentially all you do is you take the mean.",
                    "label": 0
                },
                {
                    "sent": "Are the midpoint of the two extremes.",
                    "label": 1
                },
                {
                    "sent": "Alright, so this you essentially calculate using the theorem that we presented before.",
                    "label": 0
                },
                {
                    "sent": "This is just the random accuracy.",
                    "label": 0
                },
                {
                    "sent": "I'm an individual guy.",
                    "label": 0
                },
                {
                    "sent": "This gives you a threshold and.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You use that so.",
                    "label": 0
                },
                {
                    "sent": "I'm going to test this on the task we just talked about, but we actually going to test it on 13 different subjects individually.",
                    "label": 0
                },
                {
                    "sent": "And then the results were going to show next are going to be the average result together, and then we tested on a microarray data set.",
                    "label": 0
                },
                {
                    "sent": "So here the task was determined whether or not the patient is going to get cancer based on their gene expression levels of 2000 features in 60 examples.",
                    "label": 0
                },
                {
                    "sent": "And then we use the same experimental setup that we did before with Gaussian naive Bayes and leave.",
                    "label": 1
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cross validation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "If we look at this curve, we look at.",
                    "label": 0
                },
                {
                    "sent": "This is the cancer and this is the fMRI.",
                    "label": 0
                },
                {
                    "sent": "We have five different methods.",
                    "label": 0
                },
                {
                    "sent": "The dark blue here is the result that we get if we just didn't do any feature selection at all.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "This slightly lighter blue is the result is that if you use that.",
                    "label": 0
                },
                {
                    "sent": "The actual threshold that the theorem gives you directly, which is about which we already talked about, is very conservative.",
                    "label": 0
                },
                {
                    "sent": "This green value is kind of a state of the art multiple hypothesis test correction, which is the false discovery rate, and we used a standard significance level there.",
                    "label": 0
                },
                {
                    "sent": "In the Orange, is this multiplicity gap midpoint relaxation?",
                    "label": 0
                },
                {
                    "sent": "And the yellow if we compare against is we call like the Oracle threshold, meaning that if an Oracle told us the threshold to use in the validation set, that would lead to the highest test accuracy.",
                    "label": 0
                },
                {
                    "sent": "This is the test accuracy that you would get and you see that the midpoint actually comes very close to that.",
                    "label": 0
                },
                {
                    "sent": "And then if we look at the plots, we often find that peak does actually fall close to the midpoint.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "The takeaways here is that we developed this chance accuracy theorem that gives you intuitive number an.",
                    "label": 1
                },
                {
                    "sent": "We claim that it defines this kind of natural significance threshold.",
                    "label": 1
                },
                {
                    "sent": "That's a function of the number of examples of number of features in the problem, and then we've used that to provide this feature selection heuristic that we've shown empirically actually works really well on these high dimensional sparse datasets.",
                    "label": 0
                },
                {
                    "sent": "An you can compute it actually in just one line of Matlab, so that incomplete beta function, Matlab and Mathematica, and a lot of numerical computing environments have that.",
                    "label": 0
                },
                {
                    "sent": "I have really fast implementations, so you can write one line of Matlab to compute that value, and if you're interested in the proof or some some more results, just check out the paper.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And anyway, I'd like to thank.",
                    "label": 0
                },
                {
                    "sent": "Our statistician who helped us duration an also.",
                    "label": 0
                },
                {
                    "sent": "Tech Foundation, Yahoo and NSF for their financial support.",
                    "label": 1
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Excuse me feature or not.",
                    "label": 0
                },
                {
                    "sent": "Yeah so.",
                    "label": 0
                },
                {
                    "sent": "Ideally right, you could come up with some principled method where you could measure.",
                    "label": 0
                },
                {
                    "sent": "Maybe like the peak correlation or something like that.",
                    "label": 0
                },
                {
                    "sent": "So the midpoint is a way to adjust for that right?",
                    "label": 0
                },
                {
                    "sent": "This midpoint method that we talked about, and that's why it's a heuristic, right?",
                    "label": 0
                },
                {
                    "sent": "So the theorem itself makes these assumptions which don't entirely hold, meaning that that threshold is going to be too conservative if the features are correlated.",
                    "label": 0
                },
                {
                    "sent": "If they're independent and we tested this, actually the expected accuracy that you would get goes down, right?",
                    "label": 0
                },
                {
                    "sent": "So this heuristic?",
                    "label": 0
                },
                {
                    "sent": "Tries to balance that off.",
                    "label": 0
                },
                {
                    "sent": "Tries to balance that tradeoff and adjust for the fact that those assumptions don't hold.",
                    "label": 0
                },
                {
                    "sent": "But I think in an ideal world, and I've thought about ways to try to do this, is to kind of maybe measure like the highest correlation between features and then use that.",
                    "label": 0
                },
                {
                    "sent": "To May automatically choose, choose that threshold, but in practice we actually found that the midpoint worked really well.",
                    "label": 0
                }
            ]
        }
    }
}