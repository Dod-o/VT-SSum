{
    "id": "wdydqzus244rvoratafufpjauqwpgmxz",
    "title": "Finite horizon exploration for path integral control problems",
    "info": {
        "author": [
            "Bert Kappen, Department of Medical Physics and Biophysics, Radboud University Nijmegen"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "December 2006",
        "category": [
            "Top->Computer Science->Optimization Methods",
            "Top->Mathematics->Operations Research"
        ]
    },
    "url": "http://videolectures.net/otee06_kappen_fhepi/",
    "segmentation": [
        [
            "Finance horizon exploration for passenger control problems and this this body of work is really.",
            "Not so much central in the theory of exploration, exploitation, research, but more in the area of control of mainly stochastic and only or stochastic systems.",
            "And it is.",
            "It is the message that I want to get across in the talk is to contrast this finite horizon control problems to control exploration, exploitation that is commonly done in reinforcement learning.",
            "And so it would be good for me to know.",
            "Many of you are familiar with reinforcement learning.",
            "Yeah, OK, so will your personal stocks right?",
            "OK."
        ],
        [
            "So.",
            "Where is the best place to stand?",
            "Maybe here?",
            "So control just to remind me to remind you is is a delayed reward problem.",
            "It's about how to act now to optimize future rewards.",
            "So despite it wants to get to the money and he can either walk around the Lake or go over the bridge and these are two possible trajectories that can take with different costs and it is clear that if the there's no noise, it's the shortest passes over the bridge.",
            "If there is noise, it may be better to walk around the Lake and so you see that optimal solutions depend on the noise in quiet and on.",
            "Trivial way, and so if you set this up as a full stochastic problem then you will find out that finding such a planning so finding such optimal trajectory is an intractable problem.",
            "And the tractable approaches for control basically are two classes.",
            "One is the LQ control, which is linear.",
            "Quadratic control is linear costs and quadratic, quadratic or sorry and linear dynamics, which is it gives unimodal solutions kind of model these kind of cases.",
            "Or you can have deterministic controls if you ignore the noise.",
            "You can also get unimodal solutions.",
            "So there we have a problem.",
            "So on top of all this complexity, which is really fast, there is a problem of exploration.",
            "So this is all assuming that I know the environment and I can compute.",
            "I have to compute these optimal trajectories given this environment, but on top of that we have the exploration problem.",
            "Now the types of control problems that you can typically consider is finite horizon controls, like in this case where you have a path cost, an end cost, and maybe the things are time dependent.",
            "So the dynamics maybe of the animal itself may be time dependent.",
            "Environment may be time dependent, so it's a full time dependent setting, or you can consider another body of work is infinite Horizon control and then.",
            "The most famous.",
            "Example is there is reinforcement learning where if a discounted reward and you have basically only a path cost, no end cost and the optimal control there is just a mapping from states to actions and there's no explicit time dependence in the optimal control."
        ],
        [
            "So now if you think about exploration in the context of reinforcement learning, then it's typically something like like like this that life is infinite.",
            "So you have an infinitely long time too.",
            "For this discounting episode, and but also on top of that, there's an infinitely many life, so if you repeated game settings, so you're doing something here and you move to another state, but that new States the whole process is starting again as new, and so you have this sort of nested exploration process.",
            "And the the end of this so that you can basically think of it as a learning phase and a planning phase and the end of the learning phase is to learn the best policy and this is typically done by some mix of exploration and exploitation.",
            "Sort of famous examples are actor, critic networks and Q learning and also some other proposals.",
            "And you end up basically visiting all states of the system and building and hope and sort of sort of.",
            "Or not, but by saying that you were which I want to say that you may at some point decide that some parts of the state space are actually not words to explore.",
            "Using some heuris tics that often are derived from the values of the Q learning parameters, Q earnings estimates that you have intermediate.",
            "That you drive intermediately?",
            "So here devaluation is as synthetic as I've tried to indicate, so you're trying to get sort of the best value function.",
            "Optimal value functions of all states of the system.",
            "So this is the kind of thing that you're trying to trying to optimize, and good exploration is to make this period of learning as short as possible."
        ],
        [
            "Now clearly you can also have a fixed horizon and do the same thing.",
            "Then you have a sort of receding horizon and do this same kind of a setting.",
            "So also fixed horizon receding horizon problem.",
            "You can have this kind of."
        ],
        [
            "Approach but is clearly different.",
            "If there is only if you have a finance.",
            "Verizon is only one life then at T0 you don't know anything and you just have to learn the environment and then as time goes on you have to do exploit more and explore less because the time you live is finite and there.",
            "Here the problem is very different because here you aim to maximize the reward or minimize the cost from say a given initial state that you are at.",
            "And then you hope to get so it is a very tight coupling of exploration and exploitation.",
            "And here you also will see that here the exploration actually at some later time will be of a shorter horizon than in early in early in time, because early in time you you have time to explore more and it is valuable to explore more.",
            "Then that is at a later time, so we'll get to these intuitions how?"
        ],
        [
            "Get it so the outline of the talk is too.",
            "Today I will tell you very little thing, a little bit.",
            "A brief introduction of the path integral control for Finite Horizon control problems and compare how this compares to reinforcement learning.",
            "In finer receding horizon task and then look a little bit at exploration."
        ],
        [
            "So I don't have time to go into the details of these control issues, But basically it is a continuous time continuous space loops.",
            "So here's some for exploitation.",
            "So it's basically continuous time continuous space.",
            "Can we delete this from the video over there?",
            "OK.",
            "So continuous time, continuous space, there's nonlinear dynamics, but the crucial point is, is that the control X in the same dimensions as the noise and so apart from that is quite general, and I have no time to go into detail.",
            "Why does this, but these kind of problems you can treat in the following sense that if you now look at the control cost, which is then the expected cost over all the noisy trajectory's of some given control path that you take.",
            "It consists of a past term which is an integral overtime of some arbitrary time dependent cost term and a quadratic term in the control parameter.",
            "This control parameter here and some end costs.",
            "Then the objective of the control is to find that path that minimizes this expected cost.",
            "So this is a standard.",
            "The quadratic is needed to get actually to this pass in the core formulation, so that's a special case.",
            "There's a special assumption that we need to make.",
            "So it has to be quadratic linear in the control linear additive in control, and arbitrary in the rest of the states.",
            "So it's an easy, very simplified control on an arbitrary system if you wish."
        ],
        [
            "So the standard notion that people define is across the optimal cost to go is to say that at some intermediate time T you have already solved the problem of optimal control up to the end time TF.",
            "You have found this solution and then this is a quantity which at the end time will give this ends cost and this will is well known, satisfies the stochastic Hamilton Jacobi Bellman equation, which is of this form and the cause of the quadratic nature.",
            "An linear we can.",
            "Do this explicit minimalization and get a nonlinear partial differential equation which we don't have to solve backwards in time.",
            "With this end condition to solve for the optimal control, so it is solved backwards in time from future to the current time.",
            "And for this particular case, because of this linear quadratic structure, you can actually solve this equation explicitly, and that's the the solution is a passive goal.",
            "So the solution is given here.",
            "It's minus the log of an integral over a kernel, which is a diffusion kernel times the exponent of the end cost divided by new, where new is the noise.",
            "And then the optimal control is just minus the gradient of that of that of that scalar quantity that you compute.",
            "Right, so the big thing is here.",
            "The big messages here that you don't need to have any Bellman recursions for this control problem.",
            "You have to solve this this passenger door."
        ],
        [
            "And why so?",
            "So what is this diffusion?",
            "It is the probability to be at state Y at time TF, given that you start at state X at time T, and it is given by this focus block equation, which is then solved with this boundary condition.",
            "So you are truly at State X at time T. And this is just an ordinary diffusion process which is very similar to the diffusion process that we started off with the control variable.",
            "Except that the control now is absent, it's put to 0.",
            "And there is a fine.",
            "There is this term which is funny, which is not probability conserving.",
            "It takes out probability with a rate via Delta T over over new.",
            "So it is you think of it as a particle trajectories particles are disappearing at some finite rate."
        ],
        [
            "And you can write this.",
            "Then this diffusion processes the path integral where there is an action which is basically the cost of a certain trajectory and you have to sum of all possible trajectory starting at location X, and that if you can evaluate that, that gives you the cost the optimal cost to go from that current time and current location that you are.",
            "So it has a little bit idea of free energy.",
            "You see there's a log of a sum of a large number of terms, whereas is strictly interpretation of an energy.",
            "A new which is the noise, is the temperature and you can see you get all kind of funny things which I don't have time to go into.",
            "But like spontaneous symmetry breaking as a function of noise which is typical for these free energy kind of systems.",
            "And also we can use the standard approximation methods of applause approximation or Monte Carlo sample to efficiently compute this quantity."
        ],
        [
            "So here we have an example.",
            "It's a double slit.",
            "I have to speed up a bit I think.",
            "If you want to get from T0 to TS2, the optimal control at T0 is.",
            "This blue line is.",
            "This is Jay.",
            "You can compute it here.",
            "Close for me actually with simple example here.",
            "See two trajectories that move on this."
        ],
        [
            "Control.",
            "Then so you can also sample it and you see some sample trajectories and most of them get killed because of this infinite potential that you have.",
            "So you have to think about efficient sampling, so this is inefficient sampling comparing with the exact solution an here is you can do some important sampling and get quite efficient as sampling.",
            "So this is."
        ],
        [
            "So we must rate how you would compute with this so.",
            "How does this compare to reinforcement learning now reinforcement learning there is here considered as one dimensional example.",
            "Here I have a potential in one dimension and I do a reinforcement learning.",
            "It depends on the parameter gamma.",
            "If I have a small gamma, not so right now, the far from one, the optimal value function you see it is green line.",
            "If I have a large gamma I get here this line at the interpretation is that gamma is sort of proportional to the horizon time.",
            "If I look for short horizon, I was the optimal trajectory is to go in this direction an if I have a long horizon it pays to cross this mountain of high cost and to go to the other side.",
            "So this is Stan."
        ],
        [
            "Reinforcement learning and you can get the same thing with this power central control, where I now here.",
            "Still the solution that is computed by by the Laplace approximation.",
            "And here are the Laplace path.",
            "Here is a time in.",
            "Here is a space and here you see these different LastPass going into these different valleys and the Laplace approximation is here.",
            "This blue line which has here these two bumps here showing that for short Horizon time it's a good idea to go back to the right if you start there.",
            "And for large Verizon time it is good to be transferred to Sensata.",
            "Go over this, it is mountain so."
        ],
        [
            "You get very similar type of solutions.",
            "And but the big difference is that for reinforcement learning you have recursive equation of Q in terms of itself, and depending on the value function that you that you have the reward.",
            "Sorry that you have it at each location and whereas for this path integral control you have an explicit solution.",
            "So the solution of J at different X are in fact not coupled.",
            "Whereas in the reinforcement learning they are sort of coupled and so.",
            "But in either case of course you need to know the environment."
        ],
        [
            "So how can we deal with this in an exploration setting?",
            "How much time do I have?",
            "10 OK.",
            "So suppose that we have a 1 dimensional problem, and here's X.",
            "And here's the time he is.",
            "The rise in time, and I want to.",
            "I'm here at this location and I maybe I know this part of the environment.",
            "I've explored that already and this part of the environment is unknown.",
            "I can look at.",
            "I can certainly compute the optimal the optimal solution, the optimal cost to go in the known environment which is given by this path integral term is maybe this trajectory.",
            "Maybe the optimal trajectory in debt?",
            "Case and I have to compare whether I want to exploit depends on what my belief is of the kind of rewards that I can expect in this unknown territory.",
            "So suppose that I believe in a better state why, with the cost VY here at some location, then the cost to go there.",
            "I have to move in sometime T to T prime to make this movement, and then I stay there and I can compute the cost to do that and it is you can optimize for this intermediate time.",
            "And basically expression is is here and it is proportional to the distance to that new point and it is proportional to the time 2.",
            "Time to the to the horizon and proportional to the cost of that obtained there.",
            "So it is advantageous to explore this new area if this new cost.",
            "If this cost is actually less than the cost to exploit here.",
            "So this set is equal.",
            "I can solve for the distance and it basically says that the distance is proportional to the Verizon time and proportional to this term, which is basically saying it's a difference of the cost of I get.",
            "If I stay in the known environment and of course that I get if I go to the unknown environment.",
            "So this is you could call the optimism, it gets bigger the more optimistic I am about getting nice rewards in this unknown territory, and T is the time to go.",
            "So the exploration of horizon the the distance that you need to explore is proportional to the time to go.",
            "In other words, if you're close to death, it shrinks.",
            "Is proportional to your optimism."
        ],
        [
            "So in picture you get something like this.",
            "So if you live from here to here, then in the immediate lifetime you have a large exploration region, and if you get close to death you get shorter.",
            "Smaller or?"
        ],
        [
            "Tyson so here's a here's a simple example to show how that works in a very very simple example, but it's quite difficult to get to compute this exactly in larger systems, so this is the case.",
            "Here's a 1 dimensional X, and here's time, and there's a number of slits which are infinite potentials which you cannot go through, and then at location a, an minus aard openings.",
            "In here an at location a there is a cost fi which is 0.",
            "And I know that.",
            "So I know this part of the world and I don't know this part of the world in particular.",
            "I don't know this value whether it's plus or minus one.",
            "So what are my options?"
        ],
        [
            "I can either move to the exploit and I make some cost of moving here and then I will stay here forever and now the cost is zero of that state and I have no movement cost nor any potential cost.",
            "So the total cost is just this movement R. So this is one option that they can do."
        ],
        [
            "The other option is that I.",
            "Move here and then if I encountered there.",
            "If I come there, I measure the value of this of this Phi, and if it turns out to be minus one, I'm very lucky.",
            "It's a low value, so we will stay there.",
            "But it can also turn out to be one, and in that case I have still two options.",
            "It may be advantageous, actually 4 + 1, two then go back to this one because it has a lower cost 5 zero.",
            "But I have a path cost to actually move back there, or it can be better to have a short horizon actually to stay here, not to move at all anymore.",
            "In total the cost of explore depends on the value of five.",
            "Develop counter there and it has his first pass cost R. Plus a minimum of these two trajectories, which is which is given given here."
        ],
        [
            "So whether to explore to exploit then is the trade off these two quantities.",
            "One is to exploit which cost R or the expectation value of the Explorer, which depends on what probability I assume over these over this optimism is how big I think there is a nugget there in this unknown territory.",
            "And so if I set this inequality I will find this relation which basically says.",
            "That's where the probability of 5 -- 1, that is the probability of finding something which is better than I currently have.",
            "So this is the optimism.",
            "Basically it is bounded by this quantity.",
            "So if you plot that it gives you this face diagram basically which says here's the horizon time, and here's the probability to that you assume a priori that we have something valuable at minus one, and you see that if the IT is, you will explore.",
            "If you're in this region and you explore it when this reason, which basically means.",
            "Yes, your exploration will increase with Horizon time and it will increase with your optimism, which is which is in that direction, right?",
            "So this is an illustration of this of this horizon exploration horizon that I mentioned.",
            "Now it is very."
        ],
        [
            "Difficult to actually generalize this to in general, to fit it in the Bellman equations and occasionally solve that.",
            "And I've not been able to do that, and I'm very much struggling with that at the moment, but in the context of.",
            "We're receiving horizon, so if you have not one life which have a fixed horizon which is receding, like very much like to reinforcement learning problem there, you can actually use this.",
            "Have a very nice way to do the do the exploration.",
            "It's the following way we sample along Trace according to the forward diffusion that I introduced before.",
            "And so such that the number of times that that is large compared to the horizon time.",
            "And if at iteration we I we are in a state XC we can compute this quantity.",
            "That is the we want to compute this quantity, which is basically the finite discrete space discrete time estimate of the path integral.",
            "It's a sum of the current iteration step until sometime in the future of the of the cost that I'm going to see in that encounter in that in that in those locations now this is something to the future, but we can easily set up a recurrent.",
            "Algorithm that does that through the past.",
            "So in other words, when we come at a location XI, we initialize PSI as one and at a number of other locations that we visited in the past.",
            "We update them for a number of time steps with fixed deley which is proportional to the Verizon time that we're trying to learn.",
            "In other words, we're learning."
        ],
        [
            "We're learning one.",
            "We have one exploration which is random and from the one exploration we can get statistics which allow us to compute the optimal cost to go for a number of horizon times at the same time.",
            "So and here you see that solution.",
            "So here we start at X0 with 8000 iteration doing this diffusion process and then we been these points and at these points we can compute.",
            "This previous formula and you see here the cost to go from this one trace for this problem that I showed before of this one dimensional fixed receding horizon problem where you have 4 short horizon.",
            "You didn't compute this this blue line for larger eyes and you compute this line which which was just a very brief simulation, but it shows clearly the difference of here you want to if you stay here you want to go to the right.",
            "If you hear you may have this idea too.",
            "Optimally go go through the left, so it illustrates that, in contrast to reinforcement learning, where you have to explore for a fixed horizon length.",
            "And then if you have a solution and you want to have a solution for another horizon, you have to start from scratch.",
            "Here you can.",
            "You can sort of the computation of finding the the exploring the environment and doing the computation are very much dissociated, and you can more flexible compute different quantities from the same environment.",
            "Yeah."
        ],
        [
            "So in other words, the diffusion process service to process of purposes is it explores an area of a certain size just by having a random process, and it allows the computation of past contributions needed for the control, and it's 2 for one."
        ],
        [
            "In essence.",
            "So in summary.",
            "For the exploration, there is the single fixed horizon problem and there is the repeated receding or infinite horizon problem and are quite quite different in the repeated problems.",
            "The relation between exploration and exploitation is rather weak.",
            "You can explore using some methods and then you measure the performance using the expected discounted reward.",
            "For instance, in the path integral control, exploration and exploitation are truly independent.",
            "And single exploration traces you can compute optimal control for different horizon times simultaneously.",
            "So this is sort of the.",
            "This is sort of the contrast with the with the reinforcement learning the other problem of the single fixed horizon problem.",
            "There is a true dilemma and the exploration is there at the cost of exploitation, which is quite different from this setting.",
            "You want to measure the performance starting at a given X and not sort of averaged over the whole space.",
            "And it requires an estimate of the cost to go in both the explored and unexplored domains, as I gave you in the example.",
            "So."
        ],
        [
            "This was my contribution to the workshop.",
            "If you want to read more about this type of work, there is here some papers that that are on this work and also there's a poster this morning in some other session today.",
            "I think we've got time for a couple of quick questions.",
            "Yeah.",
            "Computers.",
            "Case of probabilistic perhaps?",
            "Relocation reality because usually agents don't know what is their lifetime.",
            "I'm also human fat purpose.",
            "I mean only expectation towards how far you seem like they will be or how far you think you have resources would be like.",
            "Guidance for choosing different strategies.",
            "If you forgot the probability of surviving each particular point, could you make a strategy based on that person?",
            "Well?",
            "Typically if you have a more, if you have the principle answer is that anytime that you add any uncertainty, you add complexity to the thing that you need to compute right?",
            "So it gets more complex but but.",
            "In on the.",
            "In robotics, for instance, if you want to do it is if you have an example.",
            "Typical example to use this kind of stuff is to say I have a robot is moving here and it can observe all the robots moving in the environment and that it can be modeled by changing environment and then you have to optimize some costs and collision costs, maybe up to some horizon.",
            "And the question of how to choose that Horizon time is partly solved by the fact that if you make a model.",
            "Of the future trajectories of these other objects by some opponent modeling.",
            "In a sense, you know these are robots, and they may be malicious.",
            "They maybe want to hit you, or they may be very, very gentle and actually want to avoid you.",
            "But in any case, whatever they do, it is very likely that as further you get into the future, the more uncertain their trajectories become.",
            "So this potential V is getting fatter and fatter, and so that by itself is already taken care.",
            "Sort of getting a smooth over eyes.",
            "And so my answer would be actually that if you put the horizon.",
            "Large enough, then, the uncertainty modeling that you do in your opponents will by itself certain natural horizon in that way.",
            "Yeah.",
            "There's no other questions.",
            "I think we should go into the coffee break.",
            "Anyone wants to ask her any questions?",
            "He's got a poster session posted at 5:15.",
            "OK. Coffee is just outside, so I think 15."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Finance horizon exploration for passenger control problems and this this body of work is really.",
                    "label": 1
                },
                {
                    "sent": "Not so much central in the theory of exploration, exploitation, research, but more in the area of control of mainly stochastic and only or stochastic systems.",
                    "label": 0
                },
                {
                    "sent": "And it is.",
                    "label": 0
                },
                {
                    "sent": "It is the message that I want to get across in the talk is to contrast this finite horizon control problems to control exploration, exploitation that is commonly done in reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "And so it would be good for me to know.",
                    "label": 0
                },
                {
                    "sent": "Many of you are familiar with reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "Yeah, OK, so will your personal stocks right?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Where is the best place to stand?",
                    "label": 0
                },
                {
                    "sent": "Maybe here?",
                    "label": 0
                },
                {
                    "sent": "So control just to remind me to remind you is is a delayed reward problem.",
                    "label": 0
                },
                {
                    "sent": "It's about how to act now to optimize future rewards.",
                    "label": 1
                },
                {
                    "sent": "So despite it wants to get to the money and he can either walk around the Lake or go over the bridge and these are two possible trajectories that can take with different costs and it is clear that if the there's no noise, it's the shortest passes over the bridge.",
                    "label": 0
                },
                {
                    "sent": "If there is noise, it may be better to walk around the Lake and so you see that optimal solutions depend on the noise in quiet and on.",
                    "label": 0
                },
                {
                    "sent": "Trivial way, and so if you set this up as a full stochastic problem then you will find out that finding such a planning so finding such optimal trajectory is an intractable problem.",
                    "label": 0
                },
                {
                    "sent": "And the tractable approaches for control basically are two classes.",
                    "label": 0
                },
                {
                    "sent": "One is the LQ control, which is linear.",
                    "label": 0
                },
                {
                    "sent": "Quadratic control is linear costs and quadratic, quadratic or sorry and linear dynamics, which is it gives unimodal solutions kind of model these kind of cases.",
                    "label": 0
                },
                {
                    "sent": "Or you can have deterministic controls if you ignore the noise.",
                    "label": 0
                },
                {
                    "sent": "You can also get unimodal solutions.",
                    "label": 0
                },
                {
                    "sent": "So there we have a problem.",
                    "label": 0
                },
                {
                    "sent": "So on top of all this complexity, which is really fast, there is a problem of exploration.",
                    "label": 0
                },
                {
                    "sent": "So this is all assuming that I know the environment and I can compute.",
                    "label": 0
                },
                {
                    "sent": "I have to compute these optimal trajectories given this environment, but on top of that we have the exploration problem.",
                    "label": 1
                },
                {
                    "sent": "Now the types of control problems that you can typically consider is finite horizon controls, like in this case where you have a path cost, an end cost, and maybe the things are time dependent.",
                    "label": 0
                },
                {
                    "sent": "So the dynamics maybe of the animal itself may be time dependent.",
                    "label": 0
                },
                {
                    "sent": "Environment may be time dependent, so it's a full time dependent setting, or you can consider another body of work is infinite Horizon control and then.",
                    "label": 0
                },
                {
                    "sent": "The most famous.",
                    "label": 0
                },
                {
                    "sent": "Example is there is reinforcement learning where if a discounted reward and you have basically only a path cost, no end cost and the optimal control there is just a mapping from states to actions and there's no explicit time dependence in the optimal control.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now if you think about exploration in the context of reinforcement learning, then it's typically something like like like this that life is infinite.",
                    "label": 0
                },
                {
                    "sent": "So you have an infinitely long time too.",
                    "label": 0
                },
                {
                    "sent": "For this discounting episode, and but also on top of that, there's an infinitely many life, so if you repeated game settings, so you're doing something here and you move to another state, but that new States the whole process is starting again as new, and so you have this sort of nested exploration process.",
                    "label": 0
                },
                {
                    "sent": "And the the end of this so that you can basically think of it as a learning phase and a planning phase and the end of the learning phase is to learn the best policy and this is typically done by some mix of exploration and exploitation.",
                    "label": 1
                },
                {
                    "sent": "Sort of famous examples are actor, critic networks and Q learning and also some other proposals.",
                    "label": 0
                },
                {
                    "sent": "And you end up basically visiting all states of the system and building and hope and sort of sort of.",
                    "label": 0
                },
                {
                    "sent": "Or not, but by saying that you were which I want to say that you may at some point decide that some parts of the state space are actually not words to explore.",
                    "label": 0
                },
                {
                    "sent": "Using some heuris tics that often are derived from the values of the Q learning parameters, Q earnings estimates that you have intermediate.",
                    "label": 0
                },
                {
                    "sent": "That you drive intermediately?",
                    "label": 0
                },
                {
                    "sent": "So here devaluation is as synthetic as I've tried to indicate, so you're trying to get sort of the best value function.",
                    "label": 1
                },
                {
                    "sent": "Optimal value functions of all states of the system.",
                    "label": 0
                },
                {
                    "sent": "So this is the kind of thing that you're trying to trying to optimize, and good exploration is to make this period of learning as short as possible.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now clearly you can also have a fixed horizon and do the same thing.",
                    "label": 0
                },
                {
                    "sent": "Then you have a sort of receding horizon and do this same kind of a setting.",
                    "label": 0
                },
                {
                    "sent": "So also fixed horizon receding horizon problem.",
                    "label": 1
                },
                {
                    "sent": "You can have this kind of.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approach but is clearly different.",
                    "label": 0
                },
                {
                    "sent": "If there is only if you have a finance.",
                    "label": 1
                },
                {
                    "sent": "Verizon is only one life then at T0 you don't know anything and you just have to learn the environment and then as time goes on you have to do exploit more and explore less because the time you live is finite and there.",
                    "label": 0
                },
                {
                    "sent": "Here the problem is very different because here you aim to maximize the reward or minimize the cost from say a given initial state that you are at.",
                    "label": 1
                },
                {
                    "sent": "And then you hope to get so it is a very tight coupling of exploration and exploitation.",
                    "label": 0
                },
                {
                    "sent": "And here you also will see that here the exploration actually at some later time will be of a shorter horizon than in early in early in time, because early in time you you have time to explore more and it is valuable to explore more.",
                    "label": 0
                },
                {
                    "sent": "Then that is at a later time, so we'll get to these intuitions how?",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Get it so the outline of the talk is too.",
                    "label": 0
                },
                {
                    "sent": "Today I will tell you very little thing, a little bit.",
                    "label": 0
                },
                {
                    "sent": "A brief introduction of the path integral control for Finite Horizon control problems and compare how this compares to reinforcement learning.",
                    "label": 1
                },
                {
                    "sent": "In finer receding horizon task and then look a little bit at exploration.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I don't have time to go into the details of these control issues, But basically it is a continuous time continuous space loops.",
                    "label": 0
                },
                {
                    "sent": "So here's some for exploitation.",
                    "label": 0
                },
                {
                    "sent": "So it's basically continuous time continuous space.",
                    "label": 0
                },
                {
                    "sent": "Can we delete this from the video over there?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So continuous time, continuous space, there's nonlinear dynamics, but the crucial point is, is that the control X in the same dimensions as the noise and so apart from that is quite general, and I have no time to go into detail.",
                    "label": 0
                },
                {
                    "sent": "Why does this, but these kind of problems you can treat in the following sense that if you now look at the control cost, which is then the expected cost over all the noisy trajectory's of some given control path that you take.",
                    "label": 0
                },
                {
                    "sent": "It consists of a past term which is an integral overtime of some arbitrary time dependent cost term and a quadratic term in the control parameter.",
                    "label": 0
                },
                {
                    "sent": "This control parameter here and some end costs.",
                    "label": 0
                },
                {
                    "sent": "Then the objective of the control is to find that path that minimizes this expected cost.",
                    "label": 0
                },
                {
                    "sent": "So this is a standard.",
                    "label": 0
                },
                {
                    "sent": "The quadratic is needed to get actually to this pass in the core formulation, so that's a special case.",
                    "label": 0
                },
                {
                    "sent": "There's a special assumption that we need to make.",
                    "label": 0
                },
                {
                    "sent": "So it has to be quadratic linear in the control linear additive in control, and arbitrary in the rest of the states.",
                    "label": 0
                },
                {
                    "sent": "So it's an easy, very simplified control on an arbitrary system if you wish.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the standard notion that people define is across the optimal cost to go is to say that at some intermediate time T you have already solved the problem of optimal control up to the end time TF.",
                    "label": 1
                },
                {
                    "sent": "You have found this solution and then this is a quantity which at the end time will give this ends cost and this will is well known, satisfies the stochastic Hamilton Jacobi Bellman equation, which is of this form and the cause of the quadratic nature.",
                    "label": 0
                },
                {
                    "sent": "An linear we can.",
                    "label": 0
                },
                {
                    "sent": "Do this explicit minimalization and get a nonlinear partial differential equation which we don't have to solve backwards in time.",
                    "label": 0
                },
                {
                    "sent": "With this end condition to solve for the optimal control, so it is solved backwards in time from future to the current time.",
                    "label": 0
                },
                {
                    "sent": "And for this particular case, because of this linear quadratic structure, you can actually solve this equation explicitly, and that's the the solution is a passive goal.",
                    "label": 0
                },
                {
                    "sent": "So the solution is given here.",
                    "label": 0
                },
                {
                    "sent": "It's minus the log of an integral over a kernel, which is a diffusion kernel times the exponent of the end cost divided by new, where new is the noise.",
                    "label": 0
                },
                {
                    "sent": "And then the optimal control is just minus the gradient of that of that of that scalar quantity that you compute.",
                    "label": 0
                },
                {
                    "sent": "Right, so the big thing is here.",
                    "label": 0
                },
                {
                    "sent": "The big messages here that you don't need to have any Bellman recursions for this control problem.",
                    "label": 0
                },
                {
                    "sent": "You have to solve this this passenger door.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And why so?",
                    "label": 0
                },
                {
                    "sent": "So what is this diffusion?",
                    "label": 0
                },
                {
                    "sent": "It is the probability to be at state Y at time TF, given that you start at state X at time T, and it is given by this focus block equation, which is then solved with this boundary condition.",
                    "label": 0
                },
                {
                    "sent": "So you are truly at State X at time T. And this is just an ordinary diffusion process which is very similar to the diffusion process that we started off with the control variable.",
                    "label": 0
                },
                {
                    "sent": "Except that the control now is absent, it's put to 0.",
                    "label": 0
                },
                {
                    "sent": "And there is a fine.",
                    "label": 0
                },
                {
                    "sent": "There is this term which is funny, which is not probability conserving.",
                    "label": 0
                },
                {
                    "sent": "It takes out probability with a rate via Delta T over over new.",
                    "label": 0
                },
                {
                    "sent": "So it is you think of it as a particle trajectories particles are disappearing at some finite rate.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And you can write this.",
                    "label": 0
                },
                {
                    "sent": "Then this diffusion processes the path integral where there is an action which is basically the cost of a certain trajectory and you have to sum of all possible trajectory starting at location X, and that if you can evaluate that, that gives you the cost the optimal cost to go from that current time and current location that you are.",
                    "label": 0
                },
                {
                    "sent": "So it has a little bit idea of free energy.",
                    "label": 0
                },
                {
                    "sent": "You see there's a log of a sum of a large number of terms, whereas is strictly interpretation of an energy.",
                    "label": 1
                },
                {
                    "sent": "A new which is the noise, is the temperature and you can see you get all kind of funny things which I don't have time to go into.",
                    "label": 0
                },
                {
                    "sent": "But like spontaneous symmetry breaking as a function of noise which is typical for these free energy kind of systems.",
                    "label": 1
                },
                {
                    "sent": "And also we can use the standard approximation methods of applause approximation or Monte Carlo sample to efficiently compute this quantity.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we have an example.",
                    "label": 0
                },
                {
                    "sent": "It's a double slit.",
                    "label": 0
                },
                {
                    "sent": "I have to speed up a bit I think.",
                    "label": 0
                },
                {
                    "sent": "If you want to get from T0 to TS2, the optimal control at T0 is.",
                    "label": 0
                },
                {
                    "sent": "This blue line is.",
                    "label": 0
                },
                {
                    "sent": "This is Jay.",
                    "label": 0
                },
                {
                    "sent": "You can compute it here.",
                    "label": 0
                },
                {
                    "sent": "Close for me actually with simple example here.",
                    "label": 0
                },
                {
                    "sent": "See two trajectories that move on this.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Control.",
                    "label": 0
                },
                {
                    "sent": "Then so you can also sample it and you see some sample trajectories and most of them get killed because of this infinite potential that you have.",
                    "label": 0
                },
                {
                    "sent": "So you have to think about efficient sampling, so this is inefficient sampling comparing with the exact solution an here is you can do some important sampling and get quite efficient as sampling.",
                    "label": 0
                },
                {
                    "sent": "So this is.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we must rate how you would compute with this so.",
                    "label": 0
                },
                {
                    "sent": "How does this compare to reinforcement learning now reinforcement learning there is here considered as one dimensional example.",
                    "label": 0
                },
                {
                    "sent": "Here I have a potential in one dimension and I do a reinforcement learning.",
                    "label": 0
                },
                {
                    "sent": "It depends on the parameter gamma.",
                    "label": 0
                },
                {
                    "sent": "If I have a small gamma, not so right now, the far from one, the optimal value function you see it is green line.",
                    "label": 0
                },
                {
                    "sent": "If I have a large gamma I get here this line at the interpretation is that gamma is sort of proportional to the horizon time.",
                    "label": 0
                },
                {
                    "sent": "If I look for short horizon, I was the optimal trajectory is to go in this direction an if I have a long horizon it pays to cross this mountain of high cost and to go to the other side.",
                    "label": 0
                },
                {
                    "sent": "So this is Stan.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Reinforcement learning and you can get the same thing with this power central control, where I now here.",
                    "label": 0
                },
                {
                    "sent": "Still the solution that is computed by by the Laplace approximation.",
                    "label": 0
                },
                {
                    "sent": "And here are the Laplace path.",
                    "label": 0
                },
                {
                    "sent": "Here is a time in.",
                    "label": 0
                },
                {
                    "sent": "Here is a space and here you see these different LastPass going into these different valleys and the Laplace approximation is here.",
                    "label": 0
                },
                {
                    "sent": "This blue line which has here these two bumps here showing that for short Horizon time it's a good idea to go back to the right if you start there.",
                    "label": 0
                },
                {
                    "sent": "And for large Verizon time it is good to be transferred to Sensata.",
                    "label": 0
                },
                {
                    "sent": "Go over this, it is mountain so.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You get very similar type of solutions.",
                    "label": 0
                },
                {
                    "sent": "And but the big difference is that for reinforcement learning you have recursive equation of Q in terms of itself, and depending on the value function that you that you have the reward.",
                    "label": 0
                },
                {
                    "sent": "Sorry that you have it at each location and whereas for this path integral control you have an explicit solution.",
                    "label": 0
                },
                {
                    "sent": "So the solution of J at different X are in fact not coupled.",
                    "label": 1
                },
                {
                    "sent": "Whereas in the reinforcement learning they are sort of coupled and so.",
                    "label": 0
                },
                {
                    "sent": "But in either case of course you need to know the environment.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how can we deal with this in an exploration setting?",
                    "label": 0
                },
                {
                    "sent": "How much time do I have?",
                    "label": 0
                },
                {
                    "sent": "10 OK.",
                    "label": 0
                },
                {
                    "sent": "So suppose that we have a 1 dimensional problem, and here's X.",
                    "label": 0
                },
                {
                    "sent": "And here's the time he is.",
                    "label": 0
                },
                {
                    "sent": "The rise in time, and I want to.",
                    "label": 0
                },
                {
                    "sent": "I'm here at this location and I maybe I know this part of the environment.",
                    "label": 0
                },
                {
                    "sent": "I've explored that already and this part of the environment is unknown.",
                    "label": 0
                },
                {
                    "sent": "I can look at.",
                    "label": 0
                },
                {
                    "sent": "I can certainly compute the optimal the optimal solution, the optimal cost to go in the known environment which is given by this path integral term is maybe this trajectory.",
                    "label": 0
                },
                {
                    "sent": "Maybe the optimal trajectory in debt?",
                    "label": 0
                },
                {
                    "sent": "Case and I have to compare whether I want to exploit depends on what my belief is of the kind of rewards that I can expect in this unknown territory.",
                    "label": 0
                },
                {
                    "sent": "So suppose that I believe in a better state why, with the cost VY here at some location, then the cost to go there.",
                    "label": 0
                },
                {
                    "sent": "I have to move in sometime T to T prime to make this movement, and then I stay there and I can compute the cost to do that and it is you can optimize for this intermediate time.",
                    "label": 1
                },
                {
                    "sent": "And basically expression is is here and it is proportional to the distance to that new point and it is proportional to the time 2.",
                    "label": 0
                },
                {
                    "sent": "Time to the to the horizon and proportional to the cost of that obtained there.",
                    "label": 0
                },
                {
                    "sent": "So it is advantageous to explore this new area if this new cost.",
                    "label": 0
                },
                {
                    "sent": "If this cost is actually less than the cost to exploit here.",
                    "label": 0
                },
                {
                    "sent": "So this set is equal.",
                    "label": 0
                },
                {
                    "sent": "I can solve for the distance and it basically says that the distance is proportional to the Verizon time and proportional to this term, which is basically saying it's a difference of the cost of I get.",
                    "label": 0
                },
                {
                    "sent": "If I stay in the known environment and of course that I get if I go to the unknown environment.",
                    "label": 0
                },
                {
                    "sent": "So this is you could call the optimism, it gets bigger the more optimistic I am about getting nice rewards in this unknown territory, and T is the time to go.",
                    "label": 0
                },
                {
                    "sent": "So the exploration of horizon the the distance that you need to explore is proportional to the time to go.",
                    "label": 1
                },
                {
                    "sent": "In other words, if you're close to death, it shrinks.",
                    "label": 0
                },
                {
                    "sent": "Is proportional to your optimism.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in picture you get something like this.",
                    "label": 0
                },
                {
                    "sent": "So if you live from here to here, then in the immediate lifetime you have a large exploration region, and if you get close to death you get shorter.",
                    "label": 0
                },
                {
                    "sent": "Smaller or?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tyson so here's a here's a simple example to show how that works in a very very simple example, but it's quite difficult to get to compute this exactly in larger systems, so this is the case.",
                    "label": 0
                },
                {
                    "sent": "Here's a 1 dimensional X, and here's time, and there's a number of slits which are infinite potentials which you cannot go through, and then at location a, an minus aard openings.",
                    "label": 0
                },
                {
                    "sent": "In here an at location a there is a cost fi which is 0.",
                    "label": 0
                },
                {
                    "sent": "And I know that.",
                    "label": 0
                },
                {
                    "sent": "So I know this part of the world and I don't know this part of the world in particular.",
                    "label": 0
                },
                {
                    "sent": "I don't know this value whether it's plus or minus one.",
                    "label": 0
                },
                {
                    "sent": "So what are my options?",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I can either move to the exploit and I make some cost of moving here and then I will stay here forever and now the cost is zero of that state and I have no movement cost nor any potential cost.",
                    "label": 0
                },
                {
                    "sent": "So the total cost is just this movement R. So this is one option that they can do.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The other option is that I.",
                    "label": 0
                },
                {
                    "sent": "Move here and then if I encountered there.",
                    "label": 0
                },
                {
                    "sent": "If I come there, I measure the value of this of this Phi, and if it turns out to be minus one, I'm very lucky.",
                    "label": 0
                },
                {
                    "sent": "It's a low value, so we will stay there.",
                    "label": 0
                },
                {
                    "sent": "But it can also turn out to be one, and in that case I have still two options.",
                    "label": 0
                },
                {
                    "sent": "It may be advantageous, actually 4 + 1, two then go back to this one because it has a lower cost 5 zero.",
                    "label": 0
                },
                {
                    "sent": "But I have a path cost to actually move back there, or it can be better to have a short horizon actually to stay here, not to move at all anymore.",
                    "label": 0
                },
                {
                    "sent": "In total the cost of explore depends on the value of five.",
                    "label": 0
                },
                {
                    "sent": "Develop counter there and it has his first pass cost R. Plus a minimum of these two trajectories, which is which is given given here.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So whether to explore to exploit then is the trade off these two quantities.",
                    "label": 0
                },
                {
                    "sent": "One is to exploit which cost R or the expectation value of the Explorer, which depends on what probability I assume over these over this optimism is how big I think there is a nugget there in this unknown territory.",
                    "label": 0
                },
                {
                    "sent": "And so if I set this inequality I will find this relation which basically says.",
                    "label": 0
                },
                {
                    "sent": "That's where the probability of 5 -- 1, that is the probability of finding something which is better than I currently have.",
                    "label": 0
                },
                {
                    "sent": "So this is the optimism.",
                    "label": 0
                },
                {
                    "sent": "Basically it is bounded by this quantity.",
                    "label": 0
                },
                {
                    "sent": "So if you plot that it gives you this face diagram basically which says here's the horizon time, and here's the probability to that you assume a priori that we have something valuable at minus one, and you see that if the IT is, you will explore.",
                    "label": 0
                },
                {
                    "sent": "If you're in this region and you explore it when this reason, which basically means.",
                    "label": 0
                },
                {
                    "sent": "Yes, your exploration will increase with Horizon time and it will increase with your optimism, which is which is in that direction, right?",
                    "label": 0
                },
                {
                    "sent": "So this is an illustration of this of this horizon exploration horizon that I mentioned.",
                    "label": 0
                },
                {
                    "sent": "Now it is very.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Difficult to actually generalize this to in general, to fit it in the Bellman equations and occasionally solve that.",
                    "label": 0
                },
                {
                    "sent": "And I've not been able to do that, and I'm very much struggling with that at the moment, but in the context of.",
                    "label": 0
                },
                {
                    "sent": "We're receiving horizon, so if you have not one life which have a fixed horizon which is receding, like very much like to reinforcement learning problem there, you can actually use this.",
                    "label": 0
                },
                {
                    "sent": "Have a very nice way to do the do the exploration.",
                    "label": 0
                },
                {
                    "sent": "It's the following way we sample along Trace according to the forward diffusion that I introduced before.",
                    "label": 1
                },
                {
                    "sent": "And so such that the number of times that that is large compared to the horizon time.",
                    "label": 1
                },
                {
                    "sent": "And if at iteration we I we are in a state XC we can compute this quantity.",
                    "label": 0
                },
                {
                    "sent": "That is the we want to compute this quantity, which is basically the finite discrete space discrete time estimate of the path integral.",
                    "label": 0
                },
                {
                    "sent": "It's a sum of the current iteration step until sometime in the future of the of the cost that I'm going to see in that encounter in that in that in those locations now this is something to the future, but we can easily set up a recurrent.",
                    "label": 1
                },
                {
                    "sent": "Algorithm that does that through the past.",
                    "label": 0
                },
                {
                    "sent": "So in other words, when we come at a location XI, we initialize PSI as one and at a number of other locations that we visited in the past.",
                    "label": 0
                },
                {
                    "sent": "We update them for a number of time steps with fixed deley which is proportional to the Verizon time that we're trying to learn.",
                    "label": 0
                },
                {
                    "sent": "In other words, we're learning.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're learning one.",
                    "label": 0
                },
                {
                    "sent": "We have one exploration which is random and from the one exploration we can get statistics which allow us to compute the optimal cost to go for a number of horizon times at the same time.",
                    "label": 0
                },
                {
                    "sent": "So and here you see that solution.",
                    "label": 0
                },
                {
                    "sent": "So here we start at X0 with 8000 iteration doing this diffusion process and then we been these points and at these points we can compute.",
                    "label": 0
                },
                {
                    "sent": "This previous formula and you see here the cost to go from this one trace for this problem that I showed before of this one dimensional fixed receding horizon problem where you have 4 short horizon.",
                    "label": 0
                },
                {
                    "sent": "You didn't compute this this blue line for larger eyes and you compute this line which which was just a very brief simulation, but it shows clearly the difference of here you want to if you stay here you want to go to the right.",
                    "label": 0
                },
                {
                    "sent": "If you hear you may have this idea too.",
                    "label": 0
                },
                {
                    "sent": "Optimally go go through the left, so it illustrates that, in contrast to reinforcement learning, where you have to explore for a fixed horizon length.",
                    "label": 0
                },
                {
                    "sent": "And then if you have a solution and you want to have a solution for another horizon, you have to start from scratch.",
                    "label": 0
                },
                {
                    "sent": "Here you can.",
                    "label": 0
                },
                {
                    "sent": "You can sort of the computation of finding the the exploring the environment and doing the computation are very much dissociated, and you can more flexible compute different quantities from the same environment.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in other words, the diffusion process service to process of purposes is it explores an area of a certain size just by having a random process, and it allows the computation of past contributions needed for the control, and it's 2 for one.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In essence.",
                    "label": 0
                },
                {
                    "sent": "So in summary.",
                    "label": 0
                },
                {
                    "sent": "For the exploration, there is the single fixed horizon problem and there is the repeated receding or infinite horizon problem and are quite quite different in the repeated problems.",
                    "label": 1
                },
                {
                    "sent": "The relation between exploration and exploitation is rather weak.",
                    "label": 1
                },
                {
                    "sent": "You can explore using some methods and then you measure the performance using the expected discounted reward.",
                    "label": 0
                },
                {
                    "sent": "For instance, in the path integral control, exploration and exploitation are truly independent.",
                    "label": 0
                },
                {
                    "sent": "And single exploration traces you can compute optimal control for different horizon times simultaneously.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of the.",
                    "label": 0
                },
                {
                    "sent": "This is sort of the contrast with the with the reinforcement learning the other problem of the single fixed horizon problem.",
                    "label": 1
                },
                {
                    "sent": "There is a true dilemma and the exploration is there at the cost of exploitation, which is quite different from this setting.",
                    "label": 0
                },
                {
                    "sent": "You want to measure the performance starting at a given X and not sort of averaged over the whole space.",
                    "label": 0
                },
                {
                    "sent": "And it requires an estimate of the cost to go in both the explored and unexplored domains, as I gave you in the example.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This was my contribution to the workshop.",
                    "label": 0
                },
                {
                    "sent": "If you want to read more about this type of work, there is here some papers that that are on this work and also there's a poster this morning in some other session today.",
                    "label": 0
                },
                {
                    "sent": "I think we've got time for a couple of quick questions.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Computers.",
                    "label": 0
                },
                {
                    "sent": "Case of probabilistic perhaps?",
                    "label": 0
                },
                {
                    "sent": "Relocation reality because usually agents don't know what is their lifetime.",
                    "label": 0
                },
                {
                    "sent": "I'm also human fat purpose.",
                    "label": 0
                },
                {
                    "sent": "I mean only expectation towards how far you seem like they will be or how far you think you have resources would be like.",
                    "label": 0
                },
                {
                    "sent": "Guidance for choosing different strategies.",
                    "label": 0
                },
                {
                    "sent": "If you forgot the probability of surviving each particular point, could you make a strategy based on that person?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "Typically if you have a more, if you have the principle answer is that anytime that you add any uncertainty, you add complexity to the thing that you need to compute right?",
                    "label": 0
                },
                {
                    "sent": "So it gets more complex but but.",
                    "label": 0
                },
                {
                    "sent": "In on the.",
                    "label": 0
                },
                {
                    "sent": "In robotics, for instance, if you want to do it is if you have an example.",
                    "label": 0
                },
                {
                    "sent": "Typical example to use this kind of stuff is to say I have a robot is moving here and it can observe all the robots moving in the environment and that it can be modeled by changing environment and then you have to optimize some costs and collision costs, maybe up to some horizon.",
                    "label": 0
                },
                {
                    "sent": "And the question of how to choose that Horizon time is partly solved by the fact that if you make a model.",
                    "label": 0
                },
                {
                    "sent": "Of the future trajectories of these other objects by some opponent modeling.",
                    "label": 0
                },
                {
                    "sent": "In a sense, you know these are robots, and they may be malicious.",
                    "label": 0
                },
                {
                    "sent": "They maybe want to hit you, or they may be very, very gentle and actually want to avoid you.",
                    "label": 0
                },
                {
                    "sent": "But in any case, whatever they do, it is very likely that as further you get into the future, the more uncertain their trajectories become.",
                    "label": 0
                },
                {
                    "sent": "So this potential V is getting fatter and fatter, and so that by itself is already taken care.",
                    "label": 0
                },
                {
                    "sent": "Sort of getting a smooth over eyes.",
                    "label": 0
                },
                {
                    "sent": "And so my answer would be actually that if you put the horizon.",
                    "label": 0
                },
                {
                    "sent": "Large enough, then, the uncertainty modeling that you do in your opponents will by itself certain natural horizon in that way.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "There's no other questions.",
                    "label": 0
                },
                {
                    "sent": "I think we should go into the coffee break.",
                    "label": 0
                },
                {
                    "sent": "Anyone wants to ask her any questions?",
                    "label": 0
                },
                {
                    "sent": "He's got a poster session posted at 5:15.",
                    "label": 0
                },
                {
                    "sent": "OK. Coffee is just outside, so I think 15.",
                    "label": 0
                }
            ]
        }
    }
}