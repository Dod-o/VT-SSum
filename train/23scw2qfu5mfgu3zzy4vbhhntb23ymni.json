{
    "id": "23scw2qfu5mfgu3zzy4vbhhntb23ymni",
    "title": "Fitting a Graph to Vector Data",
    "info": {
        "author": [
            "Samuel I. Daitch, Department of Computer Science, Yale University"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Mathematics->Graph Theory"
        ]
    },
    "url": "http://videolectures.net/icml09_daitch_fgv/",
    "segmentation": [
        [
            "Hello so I'm going to talk about constructing a graph on a collection of vectors.",
            "This is I'm Sam Daitch and this is joint work with John Kellner and Dan Spielman.",
            "So what?"
        ],
        [
            "Is our goal, so we have a collection of N vectors in D dimensional space and would like to do is using those vectors as vertices construct an undirected weighted graph?",
            "They'll be useful for solving various machine learning problems because there are definitely lots of machine learning algorithms and data graph based, but not all data sets come with graphs naturally included in them, so would like to be able to draw graphs on the sets of data that don't already have them.",
            "We'd like the graphs to be.",
            "Efficiently computable, and we like the graph to be sparse, so that we can use so that we can do computations on the on the graph sufficiently.",
            "In addition to these practical concerns would like there to be a sound theoretical motivation behind the graphs, and we also would like we'd like for the graphs to have sort of nice combinatorial properties.",
            "You know, that sort of will talk more in detail about what types of properties that these might be, but we want the graph to sort of look nice in a combinatorial way.",
            "So.",
            "What I'm going to do?",
            "I'll start by."
        ],
        [
            "Um?",
            "Start by talking about some standard ways that are used that have been used to construct graphs on set of data.",
            "Then the onset of vectors, and then I'll go ahead and start to describe how we construct our graphs in the motivations behind that.",
            "I'll just, I'll just, I'll discuss some interesting combinatorial properties that are graphs have, and then I'll describe some experiments that we did using our graphs and also justify the fact that we can.",
            "We can compute our graph sufficiently.",
            "So."
        ],
        [
            "Alright, So what are some standard ways that have that have been used to construct graphs on this on a set of vectors?",
            "So the basic underlying idea behind any of these constructions is that closer two vectors are together together and you could kleidion space.",
            "That means that there they have a stronger connection to each other.",
            "So with that in mind, when we're picking edges in the graph, common things to do our connect each vertex to its K nearest neighbors, or to connect each vertex.",
            "To all other vertices that are within a certain threshold distance and then once we make their choice of edges, we might either leave all the edges unweighted or we could wait the edges proportionately to how how far apart the two vertices are.",
            "So the most common way to do that is this exponential.",
            "Waiting here where, where the where the weight on an edge is decreases exponentially as the square of the distance between the two vertices increases.",
            "So now let's let me."
        ],
        [
            "Start discussing how we went about constructing graphs.",
            "So the first thing we want to think about is if we have a graph, how would you use it to do a regression problem?",
            "So suppose we have a function that's defined on all the vectors in the graph and we know the value on every vector, every vertex except for one.",
            "How would we?",
            "How would we get?",
            "How would we estimate the value on that missing vector based on the values on the other actors and the other vertices so so?",
            "Natural thing to do using the graph would be to take an average of the of the value of the function and all the other all the other neighboring vertices and so particularly have weighted graph.",
            "So we'll take a weighted average where the weights WIJR the edge weights to the neighboring vertices, and then we scale by D, which is the sum of all the all the weights coming out of vertex size.",
            "That's the weighted degree of vertex I.",
            "So this is going to be.",
            "This is how this is how we would like to say that we can estimate sort of using a leave one out type of regression.",
            "How we could estimate missing missing value on one of the one of the vertices.",
            "So what are some?",
            "What are fun?",
            "What's the function on the vertices that we would like to be able to?",
            "That we would like to be able to do this regression nicely on well, well, we have a vector associated with each of each vertex the the D coordinates of the of the vectors are themselves can be thought of as functions on on the on the vertices, and So what we'd like to say is."
        ],
        [
            "We can sort of we can estimate the values of the coordinates on any vertex by taking weighted averages of the values of those same coordinates on the neighboring on the neighboring vertices.",
            "So in other words, for the case coordinate of vector, I would like to say that we can estimate that by taking the same weighted average of the case coordinate on all the other vectors that correspond to the neighboring vertices in the graph.",
            "And so basically what we're proposing is just to say that we're going to choose the graph that minimizes the squares of of the minimizes the squares of the error."
        ],
        [
            "All these estimates of all the coordinates of all the vectors.",
            "Right, so we sum over all vertices and all all coordinates, all vectors.",
            "Now coordinates the squared error of of of the estimates that we that we get using this regression technique, and that's basically it.",
            "We're going to choose the graph that minimizes that objective function, so.",
            "That's not quite what we end up doing.",
            "It's very close.",
            "There's a few problems with this exact formulation one."
        ],
        [
            "Problem is that this objective function is not convex, so it's going to be difficult to compute and perhaps a more sort of even fundamental problem if we look at the graphs that we get by trying to optimize this function, they don't really look like what we would want them to look like.",
            "So if you take for an example, consider."
        ],
        [
            "Five points spaced equidistantly around a circle.",
            "What graph do we get by minimizing this objective function?",
            "Well, not surprisingly we get."
        ],
        [
            "Cycle around the convex Hull of the of the five vertices, so that's nothing wrong with that.",
            "But what if we now?"
        ],
        [
            "Add another vertex in the middle of the other vertices.",
            "So basically the inside Vertex wants to learn its value equally from all of the vertices on the on the HO, but the vertices that are on the outside are quite happy the way they are and don't really want to be connected to that vertex in the middle just to make things worse, so ends up happening is we want to."
        ],
        [
            "We connect the inside vertex to all the various is on the outside, but the smaller we make those weights the better.",
            "The better this optimization function is going to be, and so you know we end up having you know infinitely high ratios between between the different edge weights, which is certainly not going to be a useful graph, and this is how this happens in general is not just one example, so we need to come up with a slight modification of this optimization function so that we prevent these problems from happening."
        ],
        [
            "So what we do is we just wait each of the square of the of the squared errors by the degree of the vertex.",
            "So basically what that means is on vertices with higher degrees, we have a stronger requirement that that you be able to estimate the coordinates on the on those vertices well from the neighboring coordinates, and so once we do that, there's a few advantages.",
            "First of all, it's important to note that we can write."
        ],
        [
            "This objective function as the square of the Frobenius norm of LX, where L is the graph Laplacian and X is a matrix where each row is where the I throw is vector.",
            "I basically and so we can see that this is a quadratic function on the weights because each entry in the graph Laplacian is a linear function of weights and X is not depending on the weights at all.",
            "So now we certainly have a quadratic function of the weights, which is, which is certainly much better easy.",
            "Better to be much easier to compute than what we had before.",
            "We do have a new issue that comes up that we didn't have with the old formulation, which is that now if we scale down all the weights."
        ],
        [
            "The graph uniformly that's going to make this objective function go down so we don't.",
            "We don't want the empty graph to be the optimal graph.",
            "So what we do is we optimize this.",
            "This function subject first of all first.",
            "First we do is subject to saying all the degrees, all the all the way to degrees must be at least one.",
            "That's what we call a hard graph.",
            "And another thing we do is we we optimize subject to this.",
            "This constraint, which basically says we allow some of the some of the weighted, agrees to be slightly smaller than one as long as not too many of them are too.",
            "Too much smaller than one, so it's slightly more lenient, but it's basically the same idea, so let's see what these graphs look like."
        ],
        [
            "So here's this.",
            "A random set of two dimensional vectors, and we've drawn the graphs so that the thicker edges represent higher weighted, higher weighted edges, and so we see it's a pretty nice looking graph, right so?",
            "You know there's not too many edges, it's planar, and we're going to.",
            "We're going to.",
            "We actually prove that all that for every set of two dimensional.",
            "Of vertices into vectors into dimensions, you'll get a planar graph which is interesting.",
            "Look at another."
        ],
        [
            "Sample so if we have two sort of clusters of vectors that are that are far apart from each other, so we get thicker at think we get heavier edge weights on within the clusters then then between the two clusters which makes sense.",
            "And it's worth noting that we do.",
            "Still we do still have some edges between the two clusters, and in fact we have found that all we found that are both hard graphs and soft grass will always be connected, although this is something that we frustrated and haven't been able to prove, we're very convinced it's true, but.",
            "It's eluded formal proof thus far.",
            "Um?",
            "OK, so let me mention a few other things about this graph."
        ],
        [
            "Objective function so first of all, if you're familiar with Croatian, solves local linear embedding algorithm, then this might look familiar because as part of their part of their algorithm, what they basically do is construct a graph that optimizes this same objective function that we reject that we're optimizing with a few important differences.",
            "First of all, we allow arbitrary edges to be in the graph, and there only allowing each vertex to be connected to some.",
            "You know some vertices in this neighborhood, and another, probably more fundamental differences that.",
            "There the weights they're choosing are asymmetric, meaning that the the degree to which vertex I learn from vertex J is not going to be the same as the amount that vertex Jalen from vertex I and we we are acquiring symmetry.",
            "Another connection is that our is that our graph objective function can actually be thought of as a generalization of the K means objective function on clusters on clusterings, meaning that if you take a clustering and look at its value, you look at this K means value.",
            "That's going to be the same as the value of our graph objective function when you have a complete graph drawn on each of the K clusters.",
            "So if you're so, if you are too.",
            "If you were to minimize our objective function subject to the constraint that you want to have a union of K, you know of K disjoint complete graphs, and then that would be equivalent to optimizing for K means.",
            "Alright, so let's talk about some properties that are graphs."
        ],
        [
            "So first of all, it's important to clarify that I can't really necessarily refer to the hard graft for these soft graph of a set of vectors, because there are certain situations where there's where the graphs are not unique and there's more than one graph that optimizes this objective function.",
            "It only only seems to happen in very highly symmetric situations where like the vectors exhibit A high degree of precise symmetry, and we conjecture that if you for any for any set of for any distribution vectors, if you even.",
            "For perturb all of the all of the coordinates very slightly randomly, then you'll always have unique hard hard graphs or unique soft graphs.",
            "OK, another property that."
        ],
        [
            "We can prove that all that graph will always be sparse in the sense if you're constructing graph on N vectors in D dimensions, are graphs will have at most N * D + 1 edges.",
            "So another way of thinking that is the average degree will be at most 2 * D + 1.",
            "And if you look at this tape."
        ],
        [
            "Well, it shows the degrees.",
            "Actually got when constructing graphs on various datasets and you can see that certainly the average user always less than twice the dimension, and sometimes they're even less than one time dimension in the in the sonar case you can see that the average degree."
        ],
        [
            "Significantly less than the dimension, which is interesting.",
            "Um?"
        ],
        [
            "OK, another interesting property of our graphs is that if you're doing, if you're considering vectors in two dimensions, the grass will always be playing, or at least in the case where there's none where there's not a unique solution, there will exist a hard graph or South graph that is planar.",
            "Now this is something that is sort of weird 'cause it doesn't really.",
            "It's not obvious what this has to do with the original sort of regression problem that we're trying to optimize for, but it's sort of interesting that it comes out.",
            "You know that.",
            "That somehow you know the right graph, even for that sort of seemingly unrelated problem.",
            "You know the graph turned out to be playing or sort of cool, and we even have a higher dimension."
        ],
        [
            "Generalization of this weird that in higher dimensions we can say that if if we associate with every complete graph every complete sub graph a simplex, then our graph is a simplicial complex."
        ],
        [
            "OK, so let me talk about the experiments that we did using our graphs.",
            "So we we did some classification regression problems the way we did regression problems.",
            "So we you know we have a.",
            "We have a set of vertices where we know that we know the value is another set where we don't and we guess the values on the unknowns.",
            "Using this regression function from juga Romanian Leopardi where we basically minimize this Laplacian quadratic quadratic expression.",
            "And I should point out that this is sort of a generalization of the regression formula that I was describing at the beginning of the talk.",
            "If you, if you limited to the case where there's where you're learning onto one test 1 test, you know there's one unknown value and all the rest are known, then this, then what we had before is what you get here, and so we do.",
            "Regression with classification is the same, except instead of choosing the optimal.",
            "Optimal.",
            "Optimal vector instead of variables being the values of the function.",
            "The variables are indicator variables, so let's see what we."
        ],
        [
            "I'm sorry so.",
            "So what did we did?",
            "We do so and I should point out that since none of the none of this paper really machine learning people, so we're starting to get into sort of unknown territory when we when we were describing when we were trying to when we were trying to sort of.",
            "Choose what experiments to do, so we're certainly open to other suggestions for what experiments would be interesting to try.",
            "But what we did is we did 10 fold cross validation using the technique that I described in the previous slide.",
            "Our graphs don't have any parameters that we didn't have to train any, and then we compared them to K nearest neighbors.",
            "Graph Santa threshold graphs with exponential weights where we chose those parameters by start doing a search over various grid parameters, and we repeated anyway.",
            "Repeated all these experiments 100 times."
        ],
        [
            "And.",
            "What we get is you can see that our soft graphs and the hard graphs also just didn't have room for that here.",
            "Do you know pretty well compared to the other types of graphs?",
            "I mean, we usually do better, and even in the cases where we don't, we still compete pretty well.",
            "If you if we compare to the performance of support vector machine algorithm."
        ],
        [
            "And then it's not quite as.",
            "Not quite as strong, but still we do.",
            "We do fairly comparably with them.",
            "OK."
        ],
        [
            "Let me show the regression results.",
            "So here again, we seem to actually do quite well compared to the K nearest neighbor and threshold graphs in the regression tests.",
            "We also"
        ],
        [
            "We did some clustering experiments where we basically did sort of in Jordan Weiss type of clustering algorithm.",
            "Except instead of using exponential weighted graphs we used our graphs and.",
            "So we clustered on, you know, for making K clusters we clustered on the we clustered on the K highest eigenvectors of.",
            "Of the posse and and we.",
            "Sorry and so we compare this to.",
            "We compare this result to the engine wise in the K means algorithms as they are implemented in spider and what we we actually observed is that.",
            "You can actually mean in Jordan twice.",
            "They they project they clustered on the K largest eigenvectors.",
            "When they were doing when they were doing K clustering, we found that actually that's actually not necessarily the best number of eigenvectors to choose.",
            "We sort of.",
            "We actually by trying to choose the number of eigenvectors, sort of."
        ],
        [
            "Captures all the data which may be more than the more than just number of clusters.",
            "Then we found that we actually did better."
        ],
        [
            "And let me just mention briefly about how we computed the graph.",
            "So I mentioned before that heart that hard graphs are.",
            "Hard graphs arkanar the solution to a quadratic program.",
            "The soft graphs can be reduced to optimizing an ugly squares problem, and in each case, while the number of variables, this large one for each edge, since we know the solutions are going to be sparse.",
            "Um?",
            "As we know the solutions are going to be sparse.",
            "We don't need to solve over all of the edges of all the edge weights.",
            "At one time we can so we can start dissolution over a subset of the edges and then we're able to look at the gradients of the objective function on the edges that we didn't yet include.",
            "And then add those in and recompute until we actually get the optimum.",
            "Get the optimal graph globally, so we do, even though we're computing, we doing local computation to step through.",
            "In the end we get, we are able to compute the optimum graph.",
            "Overall the overall."
        ],
        [
            "Edges.",
            "And so here's the computation times for various data."
        ],
        [
            "Sets and let me just go to open questions.",
            "So what some questions that we that are worth asking are there?",
            "Are there other similar types of natural graphs that one could one could construct on sets of vectors?",
            "Are there other ways to modify our construction so that would I mean, right now we don't have any parameters in our in our construction, so maybe there would be other ways to modify graph that would parameterise them.",
            "Or we could make him make have more options for how to construct the graphs we don't yet know how to prove the graphs are connected.",
            "Although we really think that they are, and we also think that their unique under perturbation would like to be able to prove that we think that we think that there are approximate solutions for these graphs in arbitrarily high dimensions that are sparse.",
            "And we also would like to, but we don't know how to prove that, and we'd also like to be able to say if we have a graph, how to guess the value for a for a vector that isn't yet included in the graph.",
            "So I guess that's all, say thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello so I'm going to talk about constructing a graph on a collection of vectors.",
                    "label": 0
                },
                {
                    "sent": "This is I'm Sam Daitch and this is joint work with John Kellner and Dan Spielman.",
                    "label": 0
                },
                {
                    "sent": "So what?",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is our goal, so we have a collection of N vectors in D dimensional space and would like to do is using those vectors as vertices construct an undirected weighted graph?",
                    "label": 1
                },
                {
                    "sent": "They'll be useful for solving various machine learning problems because there are definitely lots of machine learning algorithms and data graph based, but not all data sets come with graphs naturally included in them, so would like to be able to draw graphs on the sets of data that don't already have them.",
                    "label": 0
                },
                {
                    "sent": "We'd like the graphs to be.",
                    "label": 1
                },
                {
                    "sent": "Efficiently computable, and we like the graph to be sparse, so that we can use so that we can do computations on the on the graph sufficiently.",
                    "label": 0
                },
                {
                    "sent": "In addition to these practical concerns would like there to be a sound theoretical motivation behind the graphs, and we also would like we'd like for the graphs to have sort of nice combinatorial properties.",
                    "label": 0
                },
                {
                    "sent": "You know, that sort of will talk more in detail about what types of properties that these might be, but we want the graph to sort of look nice in a combinatorial way.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What I'm going to do?",
                    "label": 0
                },
                {
                    "sent": "I'll start by.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Start by talking about some standard ways that are used that have been used to construct graphs on set of data.",
                    "label": 0
                },
                {
                    "sent": "Then the onset of vectors, and then I'll go ahead and start to describe how we construct our graphs in the motivations behind that.",
                    "label": 0
                },
                {
                    "sent": "I'll just, I'll just, I'll discuss some interesting combinatorial properties that are graphs have, and then I'll describe some experiments that we did using our graphs and also justify the fact that we can.",
                    "label": 1
                },
                {
                    "sent": "We can compute our graph sufficiently.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Alright, So what are some standard ways that have that have been used to construct graphs on this on a set of vectors?",
                    "label": 0
                },
                {
                    "sent": "So the basic underlying idea behind any of these constructions is that closer two vectors are together together and you could kleidion space.",
                    "label": 0
                },
                {
                    "sent": "That means that there they have a stronger connection to each other.",
                    "label": 0
                },
                {
                    "sent": "So with that in mind, when we're picking edges in the graph, common things to do our connect each vertex to its K nearest neighbors, or to connect each vertex.",
                    "label": 1
                },
                {
                    "sent": "To all other vertices that are within a certain threshold distance and then once we make their choice of edges, we might either leave all the edges unweighted or we could wait the edges proportionately to how how far apart the two vertices are.",
                    "label": 0
                },
                {
                    "sent": "So the most common way to do that is this exponential.",
                    "label": 0
                },
                {
                    "sent": "Waiting here where, where the where the weight on an edge is decreases exponentially as the square of the distance between the two vertices increases.",
                    "label": 0
                },
                {
                    "sent": "So now let's let me.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Start discussing how we went about constructing graphs.",
                    "label": 0
                },
                {
                    "sent": "So the first thing we want to think about is if we have a graph, how would you use it to do a regression problem?",
                    "label": 0
                },
                {
                    "sent": "So suppose we have a function that's defined on all the vectors in the graph and we know the value on every vector, every vertex except for one.",
                    "label": 0
                },
                {
                    "sent": "How would we?",
                    "label": 0
                },
                {
                    "sent": "How would we get?",
                    "label": 0
                },
                {
                    "sent": "How would we estimate the value on that missing vector based on the values on the other actors and the other vertices so so?",
                    "label": 0
                },
                {
                    "sent": "Natural thing to do using the graph would be to take an average of the of the value of the function and all the other all the other neighboring vertices and so particularly have weighted graph.",
                    "label": 0
                },
                {
                    "sent": "So we'll take a weighted average where the weights WIJR the edge weights to the neighboring vertices, and then we scale by D, which is the sum of all the all the weights coming out of vertex size.",
                    "label": 1
                },
                {
                    "sent": "That's the weighted degree of vertex I.",
                    "label": 0
                },
                {
                    "sent": "So this is going to be.",
                    "label": 0
                },
                {
                    "sent": "This is how this is how we would like to say that we can estimate sort of using a leave one out type of regression.",
                    "label": 0
                },
                {
                    "sent": "How we could estimate missing missing value on one of the one of the vertices.",
                    "label": 0
                },
                {
                    "sent": "So what are some?",
                    "label": 0
                },
                {
                    "sent": "What are fun?",
                    "label": 0
                },
                {
                    "sent": "What's the function on the vertices that we would like to be able to?",
                    "label": 0
                },
                {
                    "sent": "That we would like to be able to do this regression nicely on well, well, we have a vector associated with each of each vertex the the D coordinates of the of the vectors are themselves can be thought of as functions on on the on the vertices, and So what we'd like to say is.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can sort of we can estimate the values of the coordinates on any vertex by taking weighted averages of the values of those same coordinates on the neighboring on the neighboring vertices.",
                    "label": 0
                },
                {
                    "sent": "So in other words, for the case coordinate of vector, I would like to say that we can estimate that by taking the same weighted average of the case coordinate on all the other vectors that correspond to the neighboring vertices in the graph.",
                    "label": 0
                },
                {
                    "sent": "And so basically what we're proposing is just to say that we're going to choose the graph that minimizes the squares of of the minimizes the squares of the error.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All these estimates of all the coordinates of all the vectors.",
                    "label": 0
                },
                {
                    "sent": "Right, so we sum over all vertices and all all coordinates, all vectors.",
                    "label": 0
                },
                {
                    "sent": "Now coordinates the squared error of of of the estimates that we that we get using this regression technique, and that's basically it.",
                    "label": 0
                },
                {
                    "sent": "We're going to choose the graph that minimizes that objective function, so.",
                    "label": 0
                },
                {
                    "sent": "That's not quite what we end up doing.",
                    "label": 0
                },
                {
                    "sent": "It's very close.",
                    "label": 0
                },
                {
                    "sent": "There's a few problems with this exact formulation one.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem is that this objective function is not convex, so it's going to be difficult to compute and perhaps a more sort of even fundamental problem if we look at the graphs that we get by trying to optimize this function, they don't really look like what we would want them to look like.",
                    "label": 0
                },
                {
                    "sent": "So if you take for an example, consider.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Five points spaced equidistantly around a circle.",
                    "label": 0
                },
                {
                    "sent": "What graph do we get by minimizing this objective function?",
                    "label": 0
                },
                {
                    "sent": "Well, not surprisingly we get.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cycle around the convex Hull of the of the five vertices, so that's nothing wrong with that.",
                    "label": 0
                },
                {
                    "sent": "But what if we now?",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Add another vertex in the middle of the other vertices.",
                    "label": 0
                },
                {
                    "sent": "So basically the inside Vertex wants to learn its value equally from all of the vertices on the on the HO, but the vertices that are on the outside are quite happy the way they are and don't really want to be connected to that vertex in the middle just to make things worse, so ends up happening is we want to.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We connect the inside vertex to all the various is on the outside, but the smaller we make those weights the better.",
                    "label": 0
                },
                {
                    "sent": "The better this optimization function is going to be, and so you know we end up having you know infinitely high ratios between between the different edge weights, which is certainly not going to be a useful graph, and this is how this happens in general is not just one example, so we need to come up with a slight modification of this optimization function so that we prevent these problems from happening.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we do is we just wait each of the square of the of the squared errors by the degree of the vertex.",
                    "label": 0
                },
                {
                    "sent": "So basically what that means is on vertices with higher degrees, we have a stronger requirement that that you be able to estimate the coordinates on the on those vertices well from the neighboring coordinates, and so once we do that, there's a few advantages.",
                    "label": 0
                },
                {
                    "sent": "First of all, it's important to note that we can write.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This objective function as the square of the Frobenius norm of LX, where L is the graph Laplacian and X is a matrix where each row is where the I throw is vector.",
                    "label": 0
                },
                {
                    "sent": "I basically and so we can see that this is a quadratic function on the weights because each entry in the graph Laplacian is a linear function of weights and X is not depending on the weights at all.",
                    "label": 0
                },
                {
                    "sent": "So now we certainly have a quadratic function of the weights, which is, which is certainly much better easy.",
                    "label": 0
                },
                {
                    "sent": "Better to be much easier to compute than what we had before.",
                    "label": 0
                },
                {
                    "sent": "We do have a new issue that comes up that we didn't have with the old formulation, which is that now if we scale down all the weights.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The graph uniformly that's going to make this objective function go down so we don't.",
                    "label": 0
                },
                {
                    "sent": "We don't want the empty graph to be the optimal graph.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we optimize this.",
                    "label": 0
                },
                {
                    "sent": "This function subject first of all first.",
                    "label": 0
                },
                {
                    "sent": "First we do is subject to saying all the degrees, all the all the way to degrees must be at least one.",
                    "label": 0
                },
                {
                    "sent": "That's what we call a hard graph.",
                    "label": 0
                },
                {
                    "sent": "And another thing we do is we we optimize subject to this.",
                    "label": 0
                },
                {
                    "sent": "This constraint, which basically says we allow some of the some of the weighted, agrees to be slightly smaller than one as long as not too many of them are too.",
                    "label": 0
                },
                {
                    "sent": "Too much smaller than one, so it's slightly more lenient, but it's basically the same idea, so let's see what these graphs look like.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's this.",
                    "label": 0
                },
                {
                    "sent": "A random set of two dimensional vectors, and we've drawn the graphs so that the thicker edges represent higher weighted, higher weighted edges, and so we see it's a pretty nice looking graph, right so?",
                    "label": 0
                },
                {
                    "sent": "You know there's not too many edges, it's planar, and we're going to.",
                    "label": 0
                },
                {
                    "sent": "We're going to.",
                    "label": 0
                },
                {
                    "sent": "We actually prove that all that for every set of two dimensional.",
                    "label": 0
                },
                {
                    "sent": "Of vertices into vectors into dimensions, you'll get a planar graph which is interesting.",
                    "label": 0
                },
                {
                    "sent": "Look at another.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sample so if we have two sort of clusters of vectors that are that are far apart from each other, so we get thicker at think we get heavier edge weights on within the clusters then then between the two clusters which makes sense.",
                    "label": 0
                },
                {
                    "sent": "And it's worth noting that we do.",
                    "label": 0
                },
                {
                    "sent": "Still we do still have some edges between the two clusters, and in fact we have found that all we found that are both hard graphs and soft grass will always be connected, although this is something that we frustrated and haven't been able to prove, we're very convinced it's true, but.",
                    "label": 0
                },
                {
                    "sent": "It's eluded formal proof thus far.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so let me mention a few other things about this graph.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Objective function so first of all, if you're familiar with Croatian, solves local linear embedding algorithm, then this might look familiar because as part of their part of their algorithm, what they basically do is construct a graph that optimizes this same objective function that we reject that we're optimizing with a few important differences.",
                    "label": 0
                },
                {
                    "sent": "First of all, we allow arbitrary edges to be in the graph, and there only allowing each vertex to be connected to some.",
                    "label": 0
                },
                {
                    "sent": "You know some vertices in this neighborhood, and another, probably more fundamental differences that.",
                    "label": 0
                },
                {
                    "sent": "There the weights they're choosing are asymmetric, meaning that the the degree to which vertex I learn from vertex J is not going to be the same as the amount that vertex Jalen from vertex I and we we are acquiring symmetry.",
                    "label": 0
                },
                {
                    "sent": "Another connection is that our is that our graph objective function can actually be thought of as a generalization of the K means objective function on clusters on clusterings, meaning that if you take a clustering and look at its value, you look at this K means value.",
                    "label": 0
                },
                {
                    "sent": "That's going to be the same as the value of our graph objective function when you have a complete graph drawn on each of the K clusters.",
                    "label": 1
                },
                {
                    "sent": "So if you're so, if you are too.",
                    "label": 0
                },
                {
                    "sent": "If you were to minimize our objective function subject to the constraint that you want to have a union of K, you know of K disjoint complete graphs, and then that would be equivalent to optimizing for K means.",
                    "label": 1
                },
                {
                    "sent": "Alright, so let's talk about some properties that are graphs.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So first of all, it's important to clarify that I can't really necessarily refer to the hard graft for these soft graph of a set of vectors, because there are certain situations where there's where the graphs are not unique and there's more than one graph that optimizes this objective function.",
                    "label": 0
                },
                {
                    "sent": "It only only seems to happen in very highly symmetric situations where like the vectors exhibit A high degree of precise symmetry, and we conjecture that if you for any for any set of for any distribution vectors, if you even.",
                    "label": 0
                },
                {
                    "sent": "For perturb all of the all of the coordinates very slightly randomly, then you'll always have unique hard hard graphs or unique soft graphs.",
                    "label": 0
                },
                {
                    "sent": "OK, another property that.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can prove that all that graph will always be sparse in the sense if you're constructing graph on N vectors in D dimensions, are graphs will have at most N * D + 1 edges.",
                    "label": 1
                },
                {
                    "sent": "So another way of thinking that is the average degree will be at most 2 * D + 1.",
                    "label": 0
                },
                {
                    "sent": "And if you look at this tape.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, it shows the degrees.",
                    "label": 0
                },
                {
                    "sent": "Actually got when constructing graphs on various datasets and you can see that certainly the average user always less than twice the dimension, and sometimes they're even less than one time dimension in the in the sonar case you can see that the average degree.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Significantly less than the dimension, which is interesting.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, another interesting property of our graphs is that if you're doing, if you're considering vectors in two dimensions, the grass will always be playing, or at least in the case where there's none where there's not a unique solution, there will exist a hard graph or South graph that is planar.",
                    "label": 1
                },
                {
                    "sent": "Now this is something that is sort of weird 'cause it doesn't really.",
                    "label": 0
                },
                {
                    "sent": "It's not obvious what this has to do with the original sort of regression problem that we're trying to optimize for, but it's sort of interesting that it comes out.",
                    "label": 0
                },
                {
                    "sent": "You know that.",
                    "label": 0
                },
                {
                    "sent": "That somehow you know the right graph, even for that sort of seemingly unrelated problem.",
                    "label": 0
                },
                {
                    "sent": "You know the graph turned out to be playing or sort of cool, and we even have a higher dimension.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Generalization of this weird that in higher dimensions we can say that if if we associate with every complete graph every complete sub graph a simplex, then our graph is a simplicial complex.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so let me talk about the experiments that we did using our graphs.",
                    "label": 0
                },
                {
                    "sent": "So we we did some classification regression problems the way we did regression problems.",
                    "label": 0
                },
                {
                    "sent": "So we you know we have a.",
                    "label": 0
                },
                {
                    "sent": "We have a set of vertices where we know that we know the value is another set where we don't and we guess the values on the unknowns.",
                    "label": 0
                },
                {
                    "sent": "Using this regression function from juga Romanian Leopardi where we basically minimize this Laplacian quadratic quadratic expression.",
                    "label": 0
                },
                {
                    "sent": "And I should point out that this is sort of a generalization of the regression formula that I was describing at the beginning of the talk.",
                    "label": 0
                },
                {
                    "sent": "If you, if you limited to the case where there's where you're learning onto one test 1 test, you know there's one unknown value and all the rest are known, then this, then what we had before is what you get here, and so we do.",
                    "label": 0
                },
                {
                    "sent": "Regression with classification is the same, except instead of choosing the optimal.",
                    "label": 0
                },
                {
                    "sent": "Optimal.",
                    "label": 0
                },
                {
                    "sent": "Optimal vector instead of variables being the values of the function.",
                    "label": 0
                },
                {
                    "sent": "The variables are indicator variables, so let's see what we.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm sorry so.",
                    "label": 0
                },
                {
                    "sent": "So what did we did?",
                    "label": 0
                },
                {
                    "sent": "We do so and I should point out that since none of the none of this paper really machine learning people, so we're starting to get into sort of unknown territory when we when we were describing when we were trying to when we were trying to sort of.",
                    "label": 0
                },
                {
                    "sent": "Choose what experiments to do, so we're certainly open to other suggestions for what experiments would be interesting to try.",
                    "label": 0
                },
                {
                    "sent": "But what we did is we did 10 fold cross validation using the technique that I described in the previous slide.",
                    "label": 1
                },
                {
                    "sent": "Our graphs don't have any parameters that we didn't have to train any, and then we compared them to K nearest neighbors.",
                    "label": 0
                },
                {
                    "sent": "Graph Santa threshold graphs with exponential weights where we chose those parameters by start doing a search over various grid parameters, and we repeated anyway.",
                    "label": 0
                },
                {
                    "sent": "Repeated all these experiments 100 times.",
                    "label": 1
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "What we get is you can see that our soft graphs and the hard graphs also just didn't have room for that here.",
                    "label": 0
                },
                {
                    "sent": "Do you know pretty well compared to the other types of graphs?",
                    "label": 0
                },
                {
                    "sent": "I mean, we usually do better, and even in the cases where we don't, we still compete pretty well.",
                    "label": 0
                },
                {
                    "sent": "If you if we compare to the performance of support vector machine algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then it's not quite as.",
                    "label": 0
                },
                {
                    "sent": "Not quite as strong, but still we do.",
                    "label": 0
                },
                {
                    "sent": "We do fairly comparably with them.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let me show the regression results.",
                    "label": 0
                },
                {
                    "sent": "So here again, we seem to actually do quite well compared to the K nearest neighbor and threshold graphs in the regression tests.",
                    "label": 0
                },
                {
                    "sent": "We also",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We did some clustering experiments where we basically did sort of in Jordan Weiss type of clustering algorithm.",
                    "label": 0
                },
                {
                    "sent": "Except instead of using exponential weighted graphs we used our graphs and.",
                    "label": 0
                },
                {
                    "sent": "So we clustered on, you know, for making K clusters we clustered on the we clustered on the K highest eigenvectors of.",
                    "label": 0
                },
                {
                    "sent": "Of the posse and and we.",
                    "label": 0
                },
                {
                    "sent": "Sorry and so we compare this to.",
                    "label": 0
                },
                {
                    "sent": "We compare this result to the engine wise in the K means algorithms as they are implemented in spider and what we we actually observed is that.",
                    "label": 0
                },
                {
                    "sent": "You can actually mean in Jordan twice.",
                    "label": 0
                },
                {
                    "sent": "They they project they clustered on the K largest eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "When they were doing when they were doing K clustering, we found that actually that's actually not necessarily the best number of eigenvectors to choose.",
                    "label": 0
                },
                {
                    "sent": "We sort of.",
                    "label": 0
                },
                {
                    "sent": "We actually by trying to choose the number of eigenvectors, sort of.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Captures all the data which may be more than the more than just number of clusters.",
                    "label": 0
                },
                {
                    "sent": "Then we found that we actually did better.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And let me just mention briefly about how we computed the graph.",
                    "label": 0
                },
                {
                    "sent": "So I mentioned before that heart that hard graphs are.",
                    "label": 0
                },
                {
                    "sent": "Hard graphs arkanar the solution to a quadratic program.",
                    "label": 1
                },
                {
                    "sent": "The soft graphs can be reduced to optimizing an ugly squares problem, and in each case, while the number of variables, this large one for each edge, since we know the solutions are going to be sparse.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "As we know the solutions are going to be sparse.",
                    "label": 0
                },
                {
                    "sent": "We don't need to solve over all of the edges of all the edge weights.",
                    "label": 0
                },
                {
                    "sent": "At one time we can so we can start dissolution over a subset of the edges and then we're able to look at the gradients of the objective function on the edges that we didn't yet include.",
                    "label": 0
                },
                {
                    "sent": "And then add those in and recompute until we actually get the optimum.",
                    "label": 0
                },
                {
                    "sent": "Get the optimal graph globally, so we do, even though we're computing, we doing local computation to step through.",
                    "label": 0
                },
                {
                    "sent": "In the end we get, we are able to compute the optimum graph.",
                    "label": 0
                },
                {
                    "sent": "Overall the overall.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Edges.",
                    "label": 0
                },
                {
                    "sent": "And so here's the computation times for various data.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sets and let me just go to open questions.",
                    "label": 0
                },
                {
                    "sent": "So what some questions that we that are worth asking are there?",
                    "label": 0
                },
                {
                    "sent": "Are there other similar types of natural graphs that one could one could construct on sets of vectors?",
                    "label": 0
                },
                {
                    "sent": "Are there other ways to modify our construction so that would I mean, right now we don't have any parameters in our in our construction, so maybe there would be other ways to modify graph that would parameterise them.",
                    "label": 1
                },
                {
                    "sent": "Or we could make him make have more options for how to construct the graphs we don't yet know how to prove the graphs are connected.",
                    "label": 0
                },
                {
                    "sent": "Although we really think that they are, and we also think that their unique under perturbation would like to be able to prove that we think that we think that there are approximate solutions for these graphs in arbitrarily high dimensions that are sparse.",
                    "label": 0
                },
                {
                    "sent": "And we also would like to, but we don't know how to prove that, and we'd also like to be able to say if we have a graph, how to guess the value for a for a vector that isn't yet included in the graph.",
                    "label": 0
                },
                {
                    "sent": "So I guess that's all, say thank you.",
                    "label": 0
                }
            ]
        }
    }
}