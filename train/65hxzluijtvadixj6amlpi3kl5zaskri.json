{
    "id": "65hxzluijtvadixj6amlpi3kl5zaskri",
    "title": "On Learning Distributions from their Samples",
    "info": {
        "author": [
            "Sudeep Kamath, Department of Electrical Engineering, Princeton University"
        ],
        "published": "Aug. 20, 2015",
        "recorded": "July 2015",
        "category": [
            "Top->Computer Science->Machine Learning->Active Learning",
            "Top->Computer Science->Machine Learning->Computational Learning Theory",
            "Top->Computer Science->Machine Learning->On-line Learning",
            "Top->Computer Science->Machine Learning->Reinforcement Learning",
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/colt2015_kamath_learning_distributions/",
    "segmentation": [
        [
            "Hello everyone, I'm I'm so deep.",
            "Comment from Princeton University.",
            "I'm a postdoc.",
            "This is joint work with Alon Andy Raj and Theater who is in the audience.",
            "This work is about learning distributions from samples, so let me begin, you have."
        ],
        [
            "Then samples drawn IID from an unknown distribution P distribution is over an alphabet of size K, so you know OK, but you don't know the distribution you want to estimate the distribution from the samples.",
            "To formalize this question, let us have a loss function L of PQ.",
            "This is a numerical penalty for producing an estimate Q when the true distribution is is P. OK, so given any such loss function which will the choice of loss function will depend on the application.",
            "We can define this quantity.",
            "The Mini Max risk associated with this loss function.",
            "OK, so this is what is?",
            "This is the expected loss between the true distribution and the estimate.",
            "Reproduce maximize overall distributions in the simplex and minimize overall estimators.",
            "So the very classical statistical quantity associated with any loss function L's OK here is the size of the alphabet.",
            "N is the number of samples.",
            "And we want to understand what is the minimax risk and what is the optimal estimator to be used.",
            "OK, one popular family of estimators is this family.",
            "The family of AD beta estimators.",
            "So here what you do is either probability assigned to any symbol is proportional to the number of times you see that symbol show up in the sequence plus some fixed constant beta, non negative constant.",
            "This is 1 simple family of estimators.",
            "Our three main results correspond to understanding the minimax risk and the optimal estimator.",
            "For.",
            "Three different notions of loss functions.",
            "The first one is the."
        ],
        [
            "One distance very important notion of distance in classification in machine learning and a folklore theorem tells us that to learn a distribution to within a distance of epsilon you need to draw number of samples which is of the order of K -- 1 over epsilon squared, where K is the size of the alphabet.",
            "OK, our theorem says the following.",
            "If you fix the size of the alphabet K then the minimax risk as a function of North has that first order term, it's square root 2 * K -- 1 / \u03c0, and so if you plug in N from the folklore theorem in there, you get the minimax risk.",
            "Roughly of the order Epsilon, so our theorem makes the folklore theorem precise and quantitative in this sense.",
            "And what is the estimator that achieves this first order behavior?",
            "It turns out it's simply the empirical estimator or the maximum likelihood estimate."
        ],
        [
            "Now the second distance or loss function we consider this chi squared distance between P&Q.",
            "So Q is the estimate and the penalty you suffer is the chi squared distance between your estimate and the true distribution.",
            "Here we show that if the alphabet size K is fixed the minimax risk.",
            "Behaves as follows.",
            "The 1st order term is is like K -- 1 / N. Alright, and what estimator achieves this performance?",
            "It's simply the Laplace estimator.",
            "The ad beta estimator with beta equals one, so this was Laplace's famous answer to the Sunrise problem.",
            "If you've seen the sunrise for 10 days in a row, what is your estimate for the sunrising tomorrow?",
            "Laplace suggestion we should use this estimate OK?"
        ],
        [
            "Third loss function we look at is a family of loss functions F divergance for any convex function F with F of 1 = 0.",
            "You can define this notion of distance between.",
            "OK, we show that.",
            "Understanding the minimax risk as the property of this convex function F is a highly challenging problem and I'll explain why when you come to my poster, we restrict the space of probability distributions.",
            "So imagine that the probabilities you know are bounded from below by some positive constant Alpha.",
            "If you know that to be true and F is some smooth function that say thrice differentiable and subexponential, then we show that the minimax risk as a function of this parameter Alpha which lower bounds the probabilities is.",
            "Is a very simple function of F. It's K -- 1 / 2 N times the second derivative of the function F of X at X = 1, and you notice that the 1st order term in fact does not actually depend on Alpha at all.",
            "And what estimator achieves this?",
            "This first order term?",
            "Any guesses?",
            "You're right, it turns out to be Ennead beta estimator for any beta positive.",
            "To know more, please come to my poster.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello everyone, I'm I'm so deep.",
                    "label": 0
                },
                {
                    "sent": "Comment from Princeton University.",
                    "label": 0
                },
                {
                    "sent": "I'm a postdoc.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with Alon Andy Raj and Theater who is in the audience.",
                    "label": 1
                },
                {
                    "sent": "This work is about learning distributions from samples, so let me begin, you have.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then samples drawn IID from an unknown distribution P distribution is over an alphabet of size K, so you know OK, but you don't know the distribution you want to estimate the distribution from the samples.",
                    "label": 0
                },
                {
                    "sent": "To formalize this question, let us have a loss function L of PQ.",
                    "label": 0
                },
                {
                    "sent": "This is a numerical penalty for producing an estimate Q when the true distribution is is P. OK, so given any such loss function which will the choice of loss function will depend on the application.",
                    "label": 0
                },
                {
                    "sent": "We can define this quantity.",
                    "label": 0
                },
                {
                    "sent": "The Mini Max risk associated with this loss function.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what is?",
                    "label": 0
                },
                {
                    "sent": "This is the expected loss between the true distribution and the estimate.",
                    "label": 0
                },
                {
                    "sent": "Reproduce maximize overall distributions in the simplex and minimize overall estimators.",
                    "label": 0
                },
                {
                    "sent": "So the very classical statistical quantity associated with any loss function L's OK here is the size of the alphabet.",
                    "label": 0
                },
                {
                    "sent": "N is the number of samples.",
                    "label": 0
                },
                {
                    "sent": "And we want to understand what is the minimax risk and what is the optimal estimator to be used.",
                    "label": 0
                },
                {
                    "sent": "OK, one popular family of estimators is this family.",
                    "label": 0
                },
                {
                    "sent": "The family of AD beta estimators.",
                    "label": 0
                },
                {
                    "sent": "So here what you do is either probability assigned to any symbol is proportional to the number of times you see that symbol show up in the sequence plus some fixed constant beta, non negative constant.",
                    "label": 0
                },
                {
                    "sent": "This is 1 simple family of estimators.",
                    "label": 1
                },
                {
                    "sent": "Our three main results correspond to understanding the minimax risk and the optimal estimator.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "Three different notions of loss functions.",
                    "label": 0
                },
                {
                    "sent": "The first one is the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One distance very important notion of distance in classification in machine learning and a folklore theorem tells us that to learn a distribution to within a distance of epsilon you need to draw number of samples which is of the order of K -- 1 over epsilon squared, where K is the size of the alphabet.",
                    "label": 0
                },
                {
                    "sent": "OK, our theorem says the following.",
                    "label": 0
                },
                {
                    "sent": "If you fix the size of the alphabet K then the minimax risk as a function of North has that first order term, it's square root 2 * K -- 1 / \u03c0, and so if you plug in N from the folklore theorem in there, you get the minimax risk.",
                    "label": 0
                },
                {
                    "sent": "Roughly of the order Epsilon, so our theorem makes the folklore theorem precise and quantitative in this sense.",
                    "label": 0
                },
                {
                    "sent": "And what is the estimator that achieves this first order behavior?",
                    "label": 0
                },
                {
                    "sent": "It turns out it's simply the empirical estimator or the maximum likelihood estimate.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now the second distance or loss function we consider this chi squared distance between P&Q.",
                    "label": 0
                },
                {
                    "sent": "So Q is the estimate and the penalty you suffer is the chi squared distance between your estimate and the true distribution.",
                    "label": 0
                },
                {
                    "sent": "Here we show that if the alphabet size K is fixed the minimax risk.",
                    "label": 0
                },
                {
                    "sent": "Behaves as follows.",
                    "label": 0
                },
                {
                    "sent": "The 1st order term is is like K -- 1 / N. Alright, and what estimator achieves this performance?",
                    "label": 0
                },
                {
                    "sent": "It's simply the Laplace estimator.",
                    "label": 0
                },
                {
                    "sent": "The ad beta estimator with beta equals one, so this was Laplace's famous answer to the Sunrise problem.",
                    "label": 0
                },
                {
                    "sent": "If you've seen the sunrise for 10 days in a row, what is your estimate for the sunrising tomorrow?",
                    "label": 0
                },
                {
                    "sent": "Laplace suggestion we should use this estimate OK?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Third loss function we look at is a family of loss functions F divergance for any convex function F with F of 1 = 0.",
                    "label": 0
                },
                {
                    "sent": "You can define this notion of distance between.",
                    "label": 0
                },
                {
                    "sent": "OK, we show that.",
                    "label": 0
                },
                {
                    "sent": "Understanding the minimax risk as the property of this convex function F is a highly challenging problem and I'll explain why when you come to my poster, we restrict the space of probability distributions.",
                    "label": 0
                },
                {
                    "sent": "So imagine that the probabilities you know are bounded from below by some positive constant Alpha.",
                    "label": 0
                },
                {
                    "sent": "If you know that to be true and F is some smooth function that say thrice differentiable and subexponential, then we show that the minimax risk as a function of this parameter Alpha which lower bounds the probabilities is.",
                    "label": 0
                },
                {
                    "sent": "Is a very simple function of F. It's K -- 1 / 2 N times the second derivative of the function F of X at X = 1, and you notice that the 1st order term in fact does not actually depend on Alpha at all.",
                    "label": 0
                },
                {
                    "sent": "And what estimator achieves this?",
                    "label": 0
                },
                {
                    "sent": "This first order term?",
                    "label": 0
                },
                {
                    "sent": "Any guesses?",
                    "label": 0
                },
                {
                    "sent": "You're right, it turns out to be Ennead beta estimator for any beta positive.",
                    "label": 0
                },
                {
                    "sent": "To know more, please come to my poster.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}