{
    "id": "lhdzgspbwjwwiwx7elxjzn3cpla44iqk",
    "title": "Quantification and Semi-supervised Classification Methods for Handling Changes in Class Distribution",
    "info": {
        "author": [
            "Gary M. Weiss, Fordham University"
        ],
        "published": "Sept. 14, 2009",
        "recorded": "July 2009",
        "category": [
            "Top->Computer Science->Data Mining->Applications"
        ]
    },
    "url": "http://videolectures.net/kdd09_weiss_qsscmhccd/",
    "segmentation": [
        [
            "OK, it's my pleasure to introduce our last speaker professional, Gary Wise from Fort Hands First Hand University.",
            "Thank you all for staying for the very last talk of the entire conference, so I appreciate you staying so I'm going to talk about some work I did with a student of mine, Jack Xu.",
            "So we look at what I think is a pretty important research problem.",
            "We induce a class."
        ],
        [
            "Fire and then the data distribution may change.",
            "In our particular scenario, we're looking at a very specific kind of concept drift.",
            "If I can use that term, although the concept doesn't really change with the only thing that changes is the class distribution.",
            "So in this scenario we don't even know that the class distribution changes because we just have unlabeled data.",
            "So if labeled data we induce a classifier, and now we have unlabeled data and we think that maybe something has changed so little bit more formally, P of Y changes P of Y given X doesn't change.",
            "And there are many scenarios where this is true, but if P of Y given X doesn't change where, why is the label X?",
            "Is the example that means that P of X must change.",
            "So for example, if you look at two hospitals, the prevalence of diseases may be very different.",
            "It's not that the P of Y given X is different, it's just that they have different populations.",
            "So this is what we want to look into.",
            "And again, the interesting thing here is we only have unlabeled data, which makes it a much more interesting.",
            "Are pro."
        ],
        [
            "But certainly a very practical one.",
            "So specifically want to look at how can we improve the classifier performance when the class is distribution may change but is unknown?",
            "And how can we exploit this unlabeled data?",
            "What we want to do is we want to outperform a naive method which basically ignores the problem.",
            "That's typically what we do.",
            "We induce a classifier, we never change it and we want to try and approach the performance of an Oracle which would actually have the labels for examples from the new distribution where we could then induce."
        ],
        [
            "Classifier from those from those new labels.",
            "So this basically shows the problem and what our goals are here.",
            "The original class distribution.",
            "This is a real data set.",
            "The adult data set is 5050 and we vary the new distribution if you will, which is on the X axis from 1 to 1% to 99% at 1% increments.",
            "So the naive method here shows what happens if we induce a classifier and we don't change it.",
            "You can see its performance is much worse than that of the Oracle.",
            "The Oracle has this you shape because at least for accuracy, highly skewed distribution is actually easier to perform well on.",
            "So if you look all the way to the right if it's 99 to one, you should be able to at least get 99% accuracy.",
            "So the Oracle is much better than the method we would actually approach the performance of this naive method."
        ],
        [
            "I'm sorry, the Oracle method, so we look at two basic approaches, quantification based approaches which will see do very well and semi supervised learning approaches which don't do very well.",
            "So the quantification approach is motivated by a KDD paper from three years ago by George Foreman where basically he says it's it's pretty easy to quantify where quantify is estimating the class distribution.",
            "It's much easier than the classification problem from an insurance company.",
            "It's hard to predict who will have an accident.",
            "But over my entire customer base, I can predict fairly fairly accurately how many accidents there will be.",
            "So we're going to use some quantification methods to calculate the change in class distribution.",
            "And then basically just the model, we just the model doing one of various things.",
            "The easiest thing would be to change the probability thresholds.",
            "This is something that's done very often in cost sensitive learning.",
            "So if we know the class distribution doubles for one class versus the other, we can just adjust the probability thresholds to compensate for this.",
            "The other type of methods are semi supervised learning methods.",
            "We're really going to try and label the data from the new distribution and then we learn from that."
        ],
        [
            "So I'll go over the specific methods and then show you the results.",
            "So the CD Oracle method is cheating.",
            "We know what the new class distribution is, so we just adjust for that shift in class distribution.",
            "You can view that is just looking at the labels which are covered with hidden the CD iterate method basically is very simple.",
            "We build a model on the original training data.",
            "We label the new distribution and from that we can estimate the change in class distribution.",
            "However, because the original model was trained on the different.",
            "Distribution, it's going to tend to underestimate whatever those changes are, so for that reason we tend to iterate so we find the change in class distribution, we adjust the original model when we do it again, CD iterate one says what happens after one iteration and."
        ],
        [
            "Our case, we do it up to three times.",
            "I'm cdac is based on adjusted count quantification method that George Foreman discussed in this paper here three years ago.",
            "I'm just going to throw up the formula and not trying to explain it but TPR and FPR the true positive and false positive rates on the original class distribution.",
            "PR Prime is the estimated class distribution.",
            "Again you could build a model, label things from the new distribution and PR star in this case is what they predict with the estimated class distribution is the adjusted one.",
            "So those are the class distribution estimation methods or the quantification methods.",
            "The other types of math."
        ],
        [
            "That's our the semi supervised learning method.",
            "The naive semi supervised learning method means we build a model.",
            "We labeled the unlabeled data from the potentially new distribution and we build a model from that.",
            "The problem with this and the reason why ultimately does poorly, is all of those label assignments are uncertain.",
            "We don't have the actual answers, so that introduces some uncertainty.",
            "The SSL self training method is similar to the naive method, but we use self training which is a known technique.",
            "From some of semi supervised learning where basically we look at the new labeled examples from the new distribution, the estimate the estimated labels, we find the ones that we think are most certain and we merge those into the original training set and we do that iteratively until we run out of examples or until we get to 4 iterations."
        ],
        [
            "Finally, the last method I'll talk about is the hybrid method.",
            "It's basically the self training method, but as we merge the examples into the original training set, we update with the class distribution is it will start going up, approaching the new distribution, but we still account for the differences.",
            "So the idea was it would have the benefits of both.",
            "It does this quantification adjustment, but it also uses the."
        ],
        [
            "Data now I'm going to briefly talk about the methodology and then just jump into the results.",
            "We use five relatively large UCI datasets.",
            "They need to be large because we want to vary the class distribution.",
            "So we do that.",
            "Via Undersampling, we create an original data, set a new original distribution and new distribution, and again we report our results using a balanced distribution for the original distribution, and then for the new distribution we varied between one and 99% one percent increments.",
            "We use WeChat.",
            "J48 for all our experiments, and we tracked the."
        ],
        [
            "The values for accuracy and F measure.",
            "So now I'm going to show you the results for accuracy for the adult data set.",
            "Then I'll show you the summary results.",
            "Overall 5 datasets, and then I'll do the same thing for F measure.",
            "So this is the naive result.",
            "Naive approach which I talked about we."
        ],
        [
            "We saw the results in an earlier slide.",
            "This is SSL naive.",
            "It actually doesn't do better in most cases.",
            "It's about the same.",
            "Some cases does better."
        ],
        [
            "Kisters worse, this is SSL self training again some cases does better, some cases does worse.",
            "None of the."
        ],
        [
            "This is really very good, but as soon as we get to the quantification methods we see all of a sudden this begins to look more like the Oracle, and in fact I don't plot the Oracle on this set of slides, but it's very close to the best of the methods I'll show you and it will be on the summary slide, so this is CD iterate one.",
            "It is much more."
        ],
        [
            "Other than those other methods for you, look at CD iterate two.",
            "It does even better, so more iterations tend to help again each iteration you tend to converge on.",
            "We think it's better because you tend to underestimate the change in class distribution due to."
        ],
        [
            "Original bias, and if we look at the hybrid approach where we would have like this to do best, it's the most sophisticated, but it does worse a little bit worse.",
            "Probably because when you're using any of the data from the new distribution, all those all of those labels are uncertain.",
            "It turns out the quantification methods do very well."
        ],
        [
            "So it's just not very necessary.",
            "Finally, we show this CDAC.",
            "This is the adjusted count method from George Foremans quantification paper.",
            "And if you look closely, actually does the best and we'll see in a minute overall."
        ],
        [
            "Datasets that tends to do the best, so these are the summary results for accuracy over the five datasets ordered from top to bottom.",
            "It's not too surprising the Oracle methods do the best CD Oracle is again when we know the actual class distribution of the new distribution and we adjust for it.",
            "But if you look at the other methods like CDAC which consistently does best and is really the winner of all the practical methods, we don't normally have oracles, we can see that that actually does very close to the CD Oracle and does very well.",
            "And overall, it's the best for each of the datasets.",
            "Again, each of these cells is averaged over 99 distributions, so it may not be its best in every single case, but overall you know it's a clear winner.",
            "We see this CD iterate methods do very well and more iterations help.",
            "We didn't go beyond three iterations.",
            "Would be interesting to do this in the future just to make sure that the results converge and we don't just getting start getting worse at some point.",
            "Hybrid method does OK, and as we saw before, the naive and semi supervised learning methods do poorly again."
        ],
        [
            "We're using the uncertain labels.",
            "If you look at F measure we see again the methods that did poorly again do poorly.",
            "Methods that did pretty well tend to do well.",
            "The only surprise here really is that the Oracle method and the CD AC, which had done very well for accuracy, actually don't do very well, and it's somewhat an artifact of the experimental set."
        ],
        [
            "But there is something we'll hear, so the reason?",
            "So again, I'll show you this summer results.",
            "It's really the same pattern.",
            "the CD iterate methods do the best again, the semi supervised learning methods in naive methods do the worst, but again the oracles are somewhere in the bottom, so the methods that did well for."
        ],
        [
            "Accuracy tend not to do well here, and really what we did is we only trained the initial distribution is always balanced and really want to vary that in the future and we will.",
            "The new distribution tends to be skewed very often.",
            "So what happens with the Oracle is we're looking at the actual labels but only from the new very skewed distribution.",
            "But F measure, like AUC, really cares about doing well on both classes.",
            "So if you're going to look at a distribution that's 99 to one, you may do very well for accuracy, But you're going to be poorly for AUC.",
            "And F measure so I mean there is something really to say here even if we change our experimental setup.",
            "If you have a distribution that becomes very much more unbalanced and you care about things like F measure AUC, you probably do care more about the original distribution.",
            "But"
        ],
        [
            "Kind of explains this surprising behavior.",
            "So in conclusion, we can substantially improve performance by not ignoring changes to the class distribution, and in particular we can exploit the unlabeled data, which is very good because she don't want to continue to having to get labeled data once you induce the classifier.",
            "And we can use it to fairly reliably estimate the class distribution of the new data.",
            "And once you do that, it's very easy to adjust the original classifier so these quantification based methods.",
            "You are really very good and it's fairly simple to actually apply them and improve and get."
        ],
        [
            "Pretty good results as far as future work.",
            "It's been pointed out that if we had a very good calibrated probability model, you wouldn't need to do anything, but in reality most models are not well calibrated, and even those that are aren't going to be perfectly calibrated.",
            "As I pointed out earlier, what's really happening here is P of X is changing.",
            "So certainly if we can reliably understand the changes in the probability distributions of the examples, we could wait them in the original training sets and wind up.",
            "With even more robust way of adjusting for these changes experimentally, I want to try and experiment with different original class distributions, and certainly there's other issues like dealing with issues in real time like data streams, which in this case wouldn't be very hard to do, so there's a few references here if you want to refer to them if you're interested and."
        ],
        [
            "I think I finished on time, so thank you and thank you for staying till the very end."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, it's my pleasure to introduce our last speaker professional, Gary Wise from Fort Hands First Hand University.",
                    "label": 0
                },
                {
                    "sent": "Thank you all for staying for the very last talk of the entire conference, so I appreciate you staying so I'm going to talk about some work I did with a student of mine, Jack Xu.",
                    "label": 0
                },
                {
                    "sent": "So we look at what I think is a pretty important research problem.",
                    "label": 0
                },
                {
                    "sent": "We induce a class.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Fire and then the data distribution may change.",
                    "label": 1
                },
                {
                    "sent": "In our particular scenario, we're looking at a very specific kind of concept drift.",
                    "label": 0
                },
                {
                    "sent": "If I can use that term, although the concept doesn't really change with the only thing that changes is the class distribution.",
                    "label": 0
                },
                {
                    "sent": "So in this scenario we don't even know that the class distribution changes because we just have unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "So if labeled data we induce a classifier, and now we have unlabeled data and we think that maybe something has changed so little bit more formally, P of Y changes P of Y given X doesn't change.",
                    "label": 0
                },
                {
                    "sent": "And there are many scenarios where this is true, but if P of Y given X doesn't change where, why is the label X?",
                    "label": 1
                },
                {
                    "sent": "Is the example that means that P of X must change.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you look at two hospitals, the prevalence of diseases may be very different.",
                    "label": 0
                },
                {
                    "sent": "It's not that the P of Y given X is different, it's just that they have different populations.",
                    "label": 0
                },
                {
                    "sent": "So this is what we want to look into.",
                    "label": 0
                },
                {
                    "sent": "And again, the interesting thing here is we only have unlabeled data, which makes it a much more interesting.",
                    "label": 0
                },
                {
                    "sent": "Are pro.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But certainly a very practical one.",
                    "label": 0
                },
                {
                    "sent": "So specifically want to look at how can we improve the classifier performance when the class is distribution may change but is unknown?",
                    "label": 1
                },
                {
                    "sent": "And how can we exploit this unlabeled data?",
                    "label": 0
                },
                {
                    "sent": "What we want to do is we want to outperform a naive method which basically ignores the problem.",
                    "label": 0
                },
                {
                    "sent": "That's typically what we do.",
                    "label": 0
                },
                {
                    "sent": "We induce a classifier, we never change it and we want to try and approach the performance of an Oracle which would actually have the labels for examples from the new distribution where we could then induce.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Classifier from those from those new labels.",
                    "label": 0
                },
                {
                    "sent": "So this basically shows the problem and what our goals are here.",
                    "label": 0
                },
                {
                    "sent": "The original class distribution.",
                    "label": 0
                },
                {
                    "sent": "This is a real data set.",
                    "label": 0
                },
                {
                    "sent": "The adult data set is 5050 and we vary the new distribution if you will, which is on the X axis from 1 to 1% to 99% at 1% increments.",
                    "label": 0
                },
                {
                    "sent": "So the naive method here shows what happens if we induce a classifier and we don't change it.",
                    "label": 0
                },
                {
                    "sent": "You can see its performance is much worse than that of the Oracle.",
                    "label": 0
                },
                {
                    "sent": "The Oracle has this you shape because at least for accuracy, highly skewed distribution is actually easier to perform well on.",
                    "label": 0
                },
                {
                    "sent": "So if you look all the way to the right if it's 99 to one, you should be able to at least get 99% accuracy.",
                    "label": 0
                },
                {
                    "sent": "So the Oracle is much better than the method we would actually approach the performance of this naive method.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I'm sorry, the Oracle method, so we look at two basic approaches, quantification based approaches which will see do very well and semi supervised learning approaches which don't do very well.",
                    "label": 0
                },
                {
                    "sent": "So the quantification approach is motivated by a KDD paper from three years ago by George Foreman where basically he says it's it's pretty easy to quantify where quantify is estimating the class distribution.",
                    "label": 0
                },
                {
                    "sent": "It's much easier than the classification problem from an insurance company.",
                    "label": 1
                },
                {
                    "sent": "It's hard to predict who will have an accident.",
                    "label": 0
                },
                {
                    "sent": "But over my entire customer base, I can predict fairly fairly accurately how many accidents there will be.",
                    "label": 1
                },
                {
                    "sent": "So we're going to use some quantification methods to calculate the change in class distribution.",
                    "label": 0
                },
                {
                    "sent": "And then basically just the model, we just the model doing one of various things.",
                    "label": 0
                },
                {
                    "sent": "The easiest thing would be to change the probability thresholds.",
                    "label": 0
                },
                {
                    "sent": "This is something that's done very often in cost sensitive learning.",
                    "label": 0
                },
                {
                    "sent": "So if we know the class distribution doubles for one class versus the other, we can just adjust the probability thresholds to compensate for this.",
                    "label": 1
                },
                {
                    "sent": "The other type of methods are semi supervised learning methods.",
                    "label": 0
                },
                {
                    "sent": "We're really going to try and label the data from the new distribution and then we learn from that.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I'll go over the specific methods and then show you the results.",
                    "label": 0
                },
                {
                    "sent": "So the CD Oracle method is cheating.",
                    "label": 0
                },
                {
                    "sent": "We know what the new class distribution is, so we just adjust for that shift in class distribution.",
                    "label": 0
                },
                {
                    "sent": "You can view that is just looking at the labels which are covered with hidden the CD iterate method basically is very simple.",
                    "label": 0
                },
                {
                    "sent": "We build a model on the original training data.",
                    "label": 1
                },
                {
                    "sent": "We label the new distribution and from that we can estimate the change in class distribution.",
                    "label": 1
                },
                {
                    "sent": "However, because the original model was trained on the different.",
                    "label": 0
                },
                {
                    "sent": "Distribution, it's going to tend to underestimate whatever those changes are, so for that reason we tend to iterate so we find the change in class distribution, we adjust the original model when we do it again, CD iterate one says what happens after one iteration and.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our case, we do it up to three times.",
                    "label": 0
                },
                {
                    "sent": "I'm cdac is based on adjusted count quantification method that George Foreman discussed in this paper here three years ago.",
                    "label": 1
                },
                {
                    "sent": "I'm just going to throw up the formula and not trying to explain it but TPR and FPR the true positive and false positive rates on the original class distribution.",
                    "label": 1
                },
                {
                    "sent": "PR Prime is the estimated class distribution.",
                    "label": 0
                },
                {
                    "sent": "Again you could build a model, label things from the new distribution and PR star in this case is what they predict with the estimated class distribution is the adjusted one.",
                    "label": 0
                },
                {
                    "sent": "So those are the class distribution estimation methods or the quantification methods.",
                    "label": 0
                },
                {
                    "sent": "The other types of math.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's our the semi supervised learning method.",
                    "label": 0
                },
                {
                    "sent": "The naive semi supervised learning method means we build a model.",
                    "label": 0
                },
                {
                    "sent": "We labeled the unlabeled data from the potentially new distribution and we build a model from that.",
                    "label": 1
                },
                {
                    "sent": "The problem with this and the reason why ultimately does poorly, is all of those label assignments are uncertain.",
                    "label": 0
                },
                {
                    "sent": "We don't have the actual answers, so that introduces some uncertainty.",
                    "label": 0
                },
                {
                    "sent": "The SSL self training method is similar to the naive method, but we use self training which is a known technique.",
                    "label": 1
                },
                {
                    "sent": "From some of semi supervised learning where basically we look at the new labeled examples from the new distribution, the estimate the estimated labels, we find the ones that we think are most certain and we merge those into the original training set and we do that iteratively until we run out of examples or until we get to 4 iterations.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finally, the last method I'll talk about is the hybrid method.",
                    "label": 0
                },
                {
                    "sent": "It's basically the self training method, but as we merge the examples into the original training set, we update with the class distribution is it will start going up, approaching the new distribution, but we still account for the differences.",
                    "label": 0
                },
                {
                    "sent": "So the idea was it would have the benefits of both.",
                    "label": 0
                },
                {
                    "sent": "It does this quantification adjustment, but it also uses the.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Data now I'm going to briefly talk about the methodology and then just jump into the results.",
                    "label": 0
                },
                {
                    "sent": "We use five relatively large UCI datasets.",
                    "label": 1
                },
                {
                    "sent": "They need to be large because we want to vary the class distribution.",
                    "label": 0
                },
                {
                    "sent": "So we do that.",
                    "label": 1
                },
                {
                    "sent": "Via Undersampling, we create an original data, set a new original distribution and new distribution, and again we report our results using a balanced distribution for the original distribution, and then for the new distribution we varied between one and 99% one percent increments.",
                    "label": 1
                },
                {
                    "sent": "We use WeChat.",
                    "label": 0
                },
                {
                    "sent": "J48 for all our experiments, and we tracked the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The values for accuracy and F measure.",
                    "label": 0
                },
                {
                    "sent": "So now I'm going to show you the results for accuracy for the adult data set.",
                    "label": 1
                },
                {
                    "sent": "Then I'll show you the summary results.",
                    "label": 0
                },
                {
                    "sent": "Overall 5 datasets, and then I'll do the same thing for F measure.",
                    "label": 0
                },
                {
                    "sent": "So this is the naive result.",
                    "label": 0
                },
                {
                    "sent": "Naive approach which I talked about we.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We saw the results in an earlier slide.",
                    "label": 0
                },
                {
                    "sent": "This is SSL naive.",
                    "label": 0
                },
                {
                    "sent": "It actually doesn't do better in most cases.",
                    "label": 0
                },
                {
                    "sent": "It's about the same.",
                    "label": 0
                },
                {
                    "sent": "Some cases does better.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kisters worse, this is SSL self training again some cases does better, some cases does worse.",
                    "label": 0
                },
                {
                    "sent": "None of the.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is really very good, but as soon as we get to the quantification methods we see all of a sudden this begins to look more like the Oracle, and in fact I don't plot the Oracle on this set of slides, but it's very close to the best of the methods I'll show you and it will be on the summary slide, so this is CD iterate one.",
                    "label": 0
                },
                {
                    "sent": "It is much more.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Other than those other methods for you, look at CD iterate two.",
                    "label": 0
                },
                {
                    "sent": "It does even better, so more iterations tend to help again each iteration you tend to converge on.",
                    "label": 0
                },
                {
                    "sent": "We think it's better because you tend to underestimate the change in class distribution due to.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Original bias, and if we look at the hybrid approach where we would have like this to do best, it's the most sophisticated, but it does worse a little bit worse.",
                    "label": 0
                },
                {
                    "sent": "Probably because when you're using any of the data from the new distribution, all those all of those labels are uncertain.",
                    "label": 0
                },
                {
                    "sent": "It turns out the quantification methods do very well.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's just not very necessary.",
                    "label": 0
                },
                {
                    "sent": "Finally, we show this CDAC.",
                    "label": 0
                },
                {
                    "sent": "This is the adjusted count method from George Foremans quantification paper.",
                    "label": 0
                },
                {
                    "sent": "And if you look closely, actually does the best and we'll see in a minute overall.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Datasets that tends to do the best, so these are the summary results for accuracy over the five datasets ordered from top to bottom.",
                    "label": 0
                },
                {
                    "sent": "It's not too surprising the Oracle methods do the best CD Oracle is again when we know the actual class distribution of the new distribution and we adjust for it.",
                    "label": 0
                },
                {
                    "sent": "But if you look at the other methods like CDAC which consistently does best and is really the winner of all the practical methods, we don't normally have oracles, we can see that that actually does very close to the CD Oracle and does very well.",
                    "label": 0
                },
                {
                    "sent": "And overall, it's the best for each of the datasets.",
                    "label": 0
                },
                {
                    "sent": "Again, each of these cells is averaged over 99 distributions, so it may not be its best in every single case, but overall you know it's a clear winner.",
                    "label": 0
                },
                {
                    "sent": "We see this CD iterate methods do very well and more iterations help.",
                    "label": 0
                },
                {
                    "sent": "We didn't go beyond three iterations.",
                    "label": 0
                },
                {
                    "sent": "Would be interesting to do this in the future just to make sure that the results converge and we don't just getting start getting worse at some point.",
                    "label": 0
                },
                {
                    "sent": "Hybrid method does OK, and as we saw before, the naive and semi supervised learning methods do poorly again.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're using the uncertain labels.",
                    "label": 0
                },
                {
                    "sent": "If you look at F measure we see again the methods that did poorly again do poorly.",
                    "label": 0
                },
                {
                    "sent": "Methods that did pretty well tend to do well.",
                    "label": 0
                },
                {
                    "sent": "The only surprise here really is that the Oracle method and the CD AC, which had done very well for accuracy, actually don't do very well, and it's somewhat an artifact of the experimental set.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there is something we'll hear, so the reason?",
                    "label": 0
                },
                {
                    "sent": "So again, I'll show you this summer results.",
                    "label": 0
                },
                {
                    "sent": "It's really the same pattern.",
                    "label": 0
                },
                {
                    "sent": "the CD iterate methods do the best again, the semi supervised learning methods in naive methods do the worst, but again the oracles are somewhere in the bottom, so the methods that did well for.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Accuracy tend not to do well here, and really what we did is we only trained the initial distribution is always balanced and really want to vary that in the future and we will.",
                    "label": 0
                },
                {
                    "sent": "The new distribution tends to be skewed very often.",
                    "label": 0
                },
                {
                    "sent": "So what happens with the Oracle is we're looking at the actual labels but only from the new very skewed distribution.",
                    "label": 0
                },
                {
                    "sent": "But F measure, like AUC, really cares about doing well on both classes.",
                    "label": 0
                },
                {
                    "sent": "So if you're going to look at a distribution that's 99 to one, you may do very well for accuracy, But you're going to be poorly for AUC.",
                    "label": 0
                },
                {
                    "sent": "And F measure so I mean there is something really to say here even if we change our experimental setup.",
                    "label": 0
                },
                {
                    "sent": "If you have a distribution that becomes very much more unbalanced and you care about things like F measure AUC, you probably do care more about the original distribution.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Kind of explains this surprising behavior.",
                    "label": 0
                },
                {
                    "sent": "So in conclusion, we can substantially improve performance by not ignoring changes to the class distribution, and in particular we can exploit the unlabeled data, which is very good because she don't want to continue to having to get labeled data once you induce the classifier.",
                    "label": 1
                },
                {
                    "sent": "And we can use it to fairly reliably estimate the class distribution of the new data.",
                    "label": 0
                },
                {
                    "sent": "And once you do that, it's very easy to adjust the original classifier so these quantification based methods.",
                    "label": 0
                },
                {
                    "sent": "You are really very good and it's fairly simple to actually apply them and improve and get.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pretty good results as far as future work.",
                    "label": 0
                },
                {
                    "sent": "It's been pointed out that if we had a very good calibrated probability model, you wouldn't need to do anything, but in reality most models are not well calibrated, and even those that are aren't going to be perfectly calibrated.",
                    "label": 0
                },
                {
                    "sent": "As I pointed out earlier, what's really happening here is P of X is changing.",
                    "label": 0
                },
                {
                    "sent": "So certainly if we can reliably understand the changes in the probability distributions of the examples, we could wait them in the original training sets and wind up.",
                    "label": 0
                },
                {
                    "sent": "With even more robust way of adjusting for these changes experimentally, I want to try and experiment with different original class distributions, and certainly there's other issues like dealing with issues in real time like data streams, which in this case wouldn't be very hard to do, so there's a few references here if you want to refer to them if you're interested and.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think I finished on time, so thank you and thank you for staying till the very end.",
                    "label": 0
                }
            ]
        }
    }
}