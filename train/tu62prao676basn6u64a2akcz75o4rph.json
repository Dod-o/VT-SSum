{
    "id": "tu62prao676basn6u64a2akcz75o4rph",
    "title": "An Empirical Study of Vocabulary Relatedness and Its application to Recommender Systems",
    "info": {
        "author": [
            "Gong Cheng, Nanjing University"
        ],
        "published": "Nov. 25, 2011",
        "recorded": "October 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Recommender Systems",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2011_cheng_study/",
    "segmentation": [
        [
            "Well, my name is coming from the University and in this presentation I will show you some results of our empirical study of vocabulary relatedness and we also apply this relatedness to the application of vocabulary recommendation.",
            "Well basically vocabulary ontology are interchangeable terms in this presentation, but we prefer vocabulary.",
            "Yeah.",
            "So well, the title of the session is about until the matching, but I have to say that this work does not touch any strictly speaking, any matching stuff.",
            "It's more about more general notion of related."
        ],
        [
            "Dennis but here you can still get some interesting factual knowledge of the vocabularies in the real world.",
            "So well look vocabulary matching or ontology matching is about to measure the similarity between terms within different vocabularies or ontologies."
        ],
        [
            "That's following this line of work.",
            "There is another line of work known as vocabulary distance, which is not aiming at measuring the similarity between terms, but to measure similarity between vocabularies.",
            "So for instance, to find which vocabularies are closer to each other, whether there's a cluster of vocabularies or something like that."
        ],
        [
            "But in our work.",
            "We tried to do something on the topic of what we call vocabulary lateinische, so it's more about similarity or distance.",
            "So relation is kind of more general notion than similarity, because two things might be similar, but see my yet to similar things might always be related, but two related things might not be always same enough.",
            "For example, dog and doghouse are related, but definitely dog is not similar to doghouse, something like that so.",
            "So for example, in this case we have two very simple vocabularies, one is about faculty and professor and the other is about PhD degree.",
            "So these two smaller vocabularies are not similar.",
            "I believe that if you run some ontology matching tools, two matches, 22 vocabularies, you might get no mappings at all.",
            "But these two variables are related.",
            "That's what we are interested in in this world."
        ],
        [
            "So our contribution is threefold.",
            "We actually ask, and the answer 3 questions first is how to measure this kind of relatedness between vocabularies and Secondly, how about the vocabulary lists in real world cases?",
            "How about what's the situation in the real world?",
            "And finally, well to apply this vocabulary lessons.",
            "Can we find any application so we have some measure?",
            "How well is the application?",
            "So the answer to the first question is we actually define six.",
            "Simple but effective.",
            "Related in this measures four vocabularies from 4 different aspects.",
            "And Secondly, we performed an empirical study of around 3000 vocabularies collected from the from like data.",
            "Just all the semantic web and also other forbidden RDF triples which have instantiates this vocabularies.",
            "So I could see that it's really relatively very large datasets that we have been using.",
            "And finally, we also find an application which is we called postselection vocabulary recommendation.",
            "So the basic idea is that we use vocabulary relatedness to help recommend vocabularies in some applications such as vocabulary search.",
            "So yeah."
        ],
        [
            "So the outline of this.",
            "Presentation in the next I will firstly briefly introduce the data set we use because our main focus is an empirical analysis.",
            "And then I will introduce the related measures we have developed and also a large number of statistical findings we have got from our analysis, then to discuss our application."
        ],
        [
            "And finally, some conclusions.",
            "So the datasets.",
            "Oh we are running a semantic web search engine called Falcone.",
            "Some of you might know that and so the data set we used in this work is crowd from February last year.",
            "To make this year and this data set consists of more than 15 million RDF documents coming from more than 5000 pay level domains and they collectively contain more than 4 billion RDF triples.",
            "So from this.",
            "I can see large datasets we have found.",
            "Nearly 3000 vocabularies.",
            "And they come from more than 200 different pay level domains and they in total contain.",
            "More than 300,000 classes and five and 50,000 properties.",
            "So based on these indicators, we could say that the data set is relatively large and also diverse."
        ],
        [
            "So here's just some more.",
            "Pictures some more figures for you to better know our data set.",
            "Well, here is the distribution of all the idea of documents we have collected over the pay level domain, so we could see a power law which we you might have seen it many times in some other talks.",
            "Well, so basically.",
            "That means many pay level domains act almost be level domains actually contribute one or only a few other documents, but we still have some.",
            "Have several very large pay level domains that they just serve a large number of our documents.",
            "That's why we have such a long tail in this power parallel distribution.",
            "And."
        ],
        [
            "About the vocabularies.",
            "So we have around 3000 vocabularies and the most of these vocabularies comefrom.organd.edu top level domains and only a few come from others such as the common dot, EU, UK, France some others."
        ],
        [
            "Yeah.",
            "OK, so it's a brief introduction of our data set.",
            "In the end about the."
        ],
        [
            "Vocabulary related in this.",
            "So we actually developed 6 numerical measures from 4 different aspects which are semantically related.",
            "News content, similarity expressively, closeness and distributional relatedness.",
            "So for semantic relatedness we developped 3 variants of measures.",
            "So in total we have six measures.",
            "But from for quite different aspects."
        ],
        [
            "So the first measure is what we call explicit semantic relatedness.",
            "The basic idea is the matter is quite simple actually so.",
            "We believe that true vocabularies are are related to each other.",
            "If there is some kind of explicit relation between them which are given by the vocabulary owners.",
            "So for example, the owner of Vocabulary One says that it imports some vocabulary true and vocabulary to maybe see also back to recovery 1.",
            "Something like that.",
            "So it's actually some other triples included in the meta description of vocabularies.",
            "So it course it's explicit semantic relatedness.",
            "So in fact we derive an undirected edge weighted graph from this connections.",
            "So in this graph, each node corresponds to a vocabulary and an edge connects two nodes.",
            "If there is any connection between these two vocabularies directly.",
            "So particularly if the connection is mutual, is is.",
            "Until on both directions.",
            "So the wait is is defined as one, and if it's just single direction, the way it is defined as two so.",
            "And finally, based on this graph, we can define the relatedness between any two vocabularies that they need to knows in this graph as the inverse of the weight of the shortest path between these two nodes in the graph.",
            "So that's the first match."
        ],
        [
            "Yeah.",
            "Similarly, the second measure is called implicit semantic relatedness becausw.",
            "But when we see implicit women that there is certain kind of connection between the two vocabularies but they're at the term level not not embodied in the vocabulary meta discussions.",
            "For example in this case.",
            "Well.",
            "Terms in different vocabularies are connected by triples, but.",
            "Such such connections might not be reflected by in the vocab resubmitted description.",
            "For example it might be possible that vocabulary to never says that it imports with three or never.",
            "See also in through something like that.",
            "So we call it implicit relatedness.",
            "And similarly we can derive an undirected edge weighted graph from such.",
            "Connections between terms to the relations between vocabularies.",
            "And the weightings are are given in a similar way, so I want to give to much details and also the related measures defined in a similar way."
        ],
        [
            "Well then.",
            "We can combine explicit relatedness and implicit relatedness into a single measure which required hybrid semantically related lists.",
            "So it's just the in.",
            "We also construct such amount on directly actuated graph and but this address and the weights are based on both explicit and implicit relatedness.",
            "Relations."
        ],
        [
            "So the marriage is defined in a similar way.",
            "So far we have three measures and also three graphs that characterize the relatedness between vocabularies.",
            "So let's see some indicators of these graphs to make a comparison, but definitely these three graphs have similar sets of nodes, because each node exactly corresponds to vocabulary.",
            "And for GE and GI, which are the explicit one in the implicit one, they have quite similar number of edges.",
            "But when we combine them, when we merge them, roughly speaking, when we merge them, we can get even more much more edges, which means.",
            "Um?",
            "Although these two graphs have very have similar numbers of edges, but these edges.",
            "In fact, connect quite different pairs of vocabularies.",
            "In other words, many, many term never connections are not involved in the vocabulary level meter descriptions, so that's one finding here.",
            "And another finding here is that.",
            "Although these two graphs have quite similar number of edges, GE seems more fragmented becausw it has more isolated nodes and the size of its largest connected components is relatively small.",
            "So G is more fragmented as a second file."
        ],
        [
            "And also you might have notice that we actually ignore these specific types of relations between terms of between vocabularies, so there is a connection.",
            "Then there is an edge we don't really care about.",
            "What's the connection is about is or imports or is she?",
            "Also its prior version.",
            "We don't care about that.",
            "Big cause.",
            "In our datasets, we find that.",
            "Most of such relations between vocabularies are just all imports.",
            "As you can see in this table.",
            "So yeah, we can still find some other relations such as.",
            "See also a prior version or or some others, but they are relative relatively, not just not that popular in real life cases.",
            "That's why we didn't develop any sophisticated."
        ],
        [
            "Weighting schemes.",
            "So the next measure is about content similarities.",
            "Yeah, more closer to the the topic of this session actually so.",
            "The idea here is that two vocabularies are also related to each other.",
            "If the art actually similar to each other, they have very similar attacks in their descriptions, so we.",
            "I don't want to go to the details actually, but basically the idea is that we just use some stream metric like called Ice App which was very alone stream metric for measuring the similarity between two strings, and so we use this measure to measure to calculate the similarity between the labels of the terms in different vocabularies and we aggregate all this similarity values into single similarity between two vocabularies.",
            "So that's what we have to."
        ],
        [
            "Long for this measure.",
            "And I'd like to highlight on this results of empirical analysis is that, you know, we we, we calculate similarity based on labels, so we need to know which properties refer to labels actually.",
            "Well.",
            "In order to find us more labels as possible, we we use our label and we use this in title and we also use their sub properties and subproperties and so on and in total we have found 86 of Labor like properties.",
            "But we also use local name because we have to.",
            "That is becausw in our data set, we find that.",
            "Well, we have like 60% of terms which have at least one label but still 1/3 of all terms have no neighbors, so that's why we also use local names in the.",
            "Similarity calculation and from another point of view, these terms are distributed.",
            "These terms that have labels are distributed in around only 60.",
            "Only 30% of vocabularies.",
            "That means.",
            "Around 2/3 of vocabularies they never provide labels for their terms."
        ],
        [
            "OK, so the next batter?",
            "We would like to describe this what we call it expressively?",
            "Closeness?",
            "So the idea is quite simple that.",
            "Two vocabularies are related to each other or they are similar to each other if.",
            "They have quite.",
            "Similar expressive tea.",
            "So the measure is also quite simple.",
            "We just so the the top left side is very simple vocabulary description from which we extract all the meta level terms that have been instantiating have been used in this description.",
            "For example, RDF domain an.",
            "In transitive property an inverse of something like that, so they comprise a set of meta level terms that are used in this vocabulary description and given any true vocabularies, we can just compute Jaccard similarity between these two sets.",
            "Two as a.",
            "As a kind of indication of the expressivity closeness."
        ],
        [
            "So the results we would like to report about this measure is that in our datasets we find more than 4000 midlevel terms that have ever been used in at least one vocabulary.",
            "Among them, more than 400 have been used in.",
            "More than one vocabulary, so it's.",
            "We could say that it's relatively large number because their midlevel terms, like RDF, is domain or inverse off or something like that.",
            "So there there are 400.",
            "And of course, the most popular ones are those from the language.",
            "Languages such as RDF server or something like that, but after we excluding this language constructs.",
            "Here are some top ranked wins.",
            "Such as DC descripcion and scores definition and term status or something like that.",
            "But basically they're not that popular.",
            "So for example, DC discussion only used by like 1.5% of vocabularies.",
            "From another point of view.",
            "In average, each vocabulary.",
            "Chooses around Tim middle level terms in its description.",
            "And for most of the coverage that is for more than 990% of all the vocabularies, they use less than 20 middle level terms, but.",
            "We also find hundreds of middle level terms in some large vocabulary, such as cycle.",
            "They use them for metamodeling stuff.",
            "So it's also quite unbalanced distribution."
        ],
        [
            "Last but not least as well, personally is my favorite measure.",
            "It's about distributional relatedness, so the basic idea is that two vocabularies are related to each other.",
            "If they are more often used together in certain contexts.",
            "So too.",
            "To measure such kind of thing, we follow some ideas from linguistic research to define the distribution of profile for each vocabulary, which is N dimensional vector.",
            "Well in means the total number of vocabulary we have in our data set.",
            "So each component here means it is actually a conditional probability which means given.",
            "Factor that's a vocabulary has been implemented in a certain context.",
            "For example another document.",
            "What is the probability that every other vocabulary will be also instantiated in this?",
            "Context.",
            "So we have a vector and we for any two given vocabularies we can just measure the cosine similarity between their distribution profiles and to estimate such probabilities we just performed a statistical analysis of our data set.",
            "And we use RTF document as the contexts.",
            "But we find problems.",
            "Because as we as I have presented previously that already have documents.",
            "We follow a power law and very unbalanced distribution over pay level domains.",
            "That means a large clever domain domain.",
            "May you know, affect the results, largely which we don't like that to happen.",
            "So which we choose instead we choose pay level domain as."
        ],
        [
            "The Contacts.",
            "Anyway, throughout our analysis we have found.",
            "That more than 60% of cabinets have ever been instantiation.",
            "By at least once in the data set.",
            "And the most popular ones, excluding the languages.",
            "Of course RDC Fauve and Geo.",
            "Something like that and particularly."
        ],
        [
            "Easy on the profile.",
            "Credit popular.",
            "You know what they said.",
            "Well about Co instantiation if it has been fine.",
            "Found for more than 9000 vocabularies pairs of vocabularies.",
            "And the top ranked top ranked the most popular Pals, excluding languages, are some DC pals in DC plus pho for both plus Geo.",
            "Something like that."
        ],
        [
            "And we also made the."
        ],
        [
            "Operation so I have to be quick.",
            "So yeah, we just use Spearman's rank correlation coefficient to look at the agreement between the different measures we have developed.",
            "So the only finding out share with you is that RSE is strongly positively correlated with Rd which means.",
            "If two vocabularies.",
            "If there is a any relation, an explicit relation between two vocabularies.",
            "So they tend to be instantiated together, so that's a message I would like to deliver based on this fact.",
            "And also with you some classroom based."
        ],
        [
            "Found this correlation, but yeah, it's not so important.",
            "Well, and the last is we apply this related measures to the problem of."
        ],
        [
            "Capital recommendations so basically imagine that we have a vocabulary search engine and for each result we would like to have such kind of related list related ontologies button.",
            "Be associated with this search result so that we can also retrieve some related results immediately, so we can exactly use vocabulary relatedness for achieving this.",
            "We can use a single measures or we can use."
        ],
        [
            "Emanations of mirrors.",
            "And we also.",
            "Besides using this related relatedness measures, we also use popularity as a re ranking strategy.",
            "Because we we want to recommend those popular vocabularies."
        ],
        [
            "And yeah, probably I would have not enough time."
        ],
        [
            "For showing the evaluation results but."
        ],
        [
            "The yeah the.",
            "Just some quick results so.",
            "Which actually collects collected some gold standards for the relatedness between vocabularies by using polling techniques and by inviting a human experts to give gold standards and finally the results is that RC is the best performing measure in our experiment, which means that at least our human experts, they prefer to define the relatedness between vocabularies only based mainly based on the.",
            "Content similarity that is, the similarity between the labels of terms.",
            "And Eve."
        ],
        [
            "And so, um.",
            "Although the content similarity has already performed very well, but it can still be improved by combined with some other measures we have developed."
        ],
        [
            "Yes, so not to do and the very last result is that we actually need to study a tradeoff between relating this and popularity because we use popularity as as a strategy for re ranking.",
            "But you know, as showing this in this figure that if we tend to recommend more popular vocabularies, we might lose some relatedness, because this kind of trade off and it seems that it's quite difficult to find the best tradeoff, so it's solely depend."
        ],
        [
            "Zone applications."
        ],
        [
            "Conclusions.",
            "Well, we have defined 6 measures of vocabulary relatedness from 4 different aspects and we performed an empirical analysis report.",
            "Some sets of findings and we apply those measures to a new problem and we have implemented this recommended service at our search engine Falcons Ontology search."
        ],
        [
            "So this last slide?",
            "Some takeaway for you vocabulary matter descriptiones are incomplete because many term level relations are not embodied in vocabulary level meter descriptiones second terms like labels.",
            "Finally, explicitly relating this vocabularies tend to be instantiated together.",
            "So if you like to populate your vocabulary just try to connect it to some well known ones that might help, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, my name is coming from the University and in this presentation I will show you some results of our empirical study of vocabulary relatedness and we also apply this relatedness to the application of vocabulary recommendation.",
                    "label": 1
                },
                {
                    "sent": "Well basically vocabulary ontology are interchangeable terms in this presentation, but we prefer vocabulary.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So well, the title of the session is about until the matching, but I have to say that this work does not touch any strictly speaking, any matching stuff.",
                    "label": 0
                },
                {
                    "sent": "It's more about more general notion of related.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dennis but here you can still get some interesting factual knowledge of the vocabularies in the real world.",
                    "label": 0
                },
                {
                    "sent": "So well look vocabulary matching or ontology matching is about to measure the similarity between terms within different vocabularies or ontologies.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's following this line of work.",
                    "label": 0
                },
                {
                    "sent": "There is another line of work known as vocabulary distance, which is not aiming at measuring the similarity between terms, but to measure similarity between vocabularies.",
                    "label": 0
                },
                {
                    "sent": "So for instance, to find which vocabularies are closer to each other, whether there's a cluster of vocabularies or something like that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But in our work.",
                    "label": 0
                },
                {
                    "sent": "We tried to do something on the topic of what we call vocabulary lateinische, so it's more about similarity or distance.",
                    "label": 0
                },
                {
                    "sent": "So relation is kind of more general notion than similarity, because two things might be similar, but see my yet to similar things might always be related, but two related things might not be always same enough.",
                    "label": 0
                },
                {
                    "sent": "For example, dog and doghouse are related, but definitely dog is not similar to doghouse, something like that so.",
                    "label": 0
                },
                {
                    "sent": "So for example, in this case we have two very simple vocabularies, one is about faculty and professor and the other is about PhD degree.",
                    "label": 0
                },
                {
                    "sent": "So these two smaller vocabularies are not similar.",
                    "label": 0
                },
                {
                    "sent": "I believe that if you run some ontology matching tools, two matches, 22 vocabularies, you might get no mappings at all.",
                    "label": 0
                },
                {
                    "sent": "But these two variables are related.",
                    "label": 0
                },
                {
                    "sent": "That's what we are interested in in this world.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So our contribution is threefold.",
                    "label": 0
                },
                {
                    "sent": "We actually ask, and the answer 3 questions first is how to measure this kind of relatedness between vocabularies and Secondly, how about the vocabulary lists in real world cases?",
                    "label": 1
                },
                {
                    "sent": "How about what's the situation in the real world?",
                    "label": 1
                },
                {
                    "sent": "And finally, well to apply this vocabulary lessons.",
                    "label": 1
                },
                {
                    "sent": "Can we find any application so we have some measure?",
                    "label": 0
                },
                {
                    "sent": "How well is the application?",
                    "label": 0
                },
                {
                    "sent": "So the answer to the first question is we actually define six.",
                    "label": 1
                },
                {
                    "sent": "Simple but effective.",
                    "label": 0
                },
                {
                    "sent": "Related in this measures four vocabularies from 4 different aspects.",
                    "label": 0
                },
                {
                    "sent": "And Secondly, we performed an empirical study of around 3000 vocabularies collected from the from like data.",
                    "label": 0
                },
                {
                    "sent": "Just all the semantic web and also other forbidden RDF triples which have instantiates this vocabularies.",
                    "label": 0
                },
                {
                    "sent": "So I could see that it's really relatively very large datasets that we have been using.",
                    "label": 1
                },
                {
                    "sent": "And finally, we also find an application which is we called postselection vocabulary recommendation.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea is that we use vocabulary relatedness to help recommend vocabularies in some applications such as vocabulary search.",
                    "label": 0
                },
                {
                    "sent": "So yeah.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the outline of this.",
                    "label": 0
                },
                {
                    "sent": "Presentation in the next I will firstly briefly introduce the data set we use because our main focus is an empirical analysis.",
                    "label": 1
                },
                {
                    "sent": "And then I will introduce the related measures we have developed and also a large number of statistical findings we have got from our analysis, then to discuss our application.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And finally, some conclusions.",
                    "label": 0
                },
                {
                    "sent": "So the datasets.",
                    "label": 0
                },
                {
                    "sent": "Oh we are running a semantic web search engine called Falcone.",
                    "label": 0
                },
                {
                    "sent": "Some of you might know that and so the data set we used in this work is crowd from February last year.",
                    "label": 1
                },
                {
                    "sent": "To make this year and this data set consists of more than 15 million RDF documents coming from more than 5000 pay level domains and they collectively contain more than 4 billion RDF triples.",
                    "label": 0
                },
                {
                    "sent": "So from this.",
                    "label": 0
                },
                {
                    "sent": "I can see large datasets we have found.",
                    "label": 0
                },
                {
                    "sent": "Nearly 3000 vocabularies.",
                    "label": 0
                },
                {
                    "sent": "And they come from more than 200 different pay level domains and they in total contain.",
                    "label": 0
                },
                {
                    "sent": "More than 300,000 classes and five and 50,000 properties.",
                    "label": 0
                },
                {
                    "sent": "So based on these indicators, we could say that the data set is relatively large and also diverse.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's just some more.",
                    "label": 0
                },
                {
                    "sent": "Pictures some more figures for you to better know our data set.",
                    "label": 1
                },
                {
                    "sent": "Well, here is the distribution of all the idea of documents we have collected over the pay level domain, so we could see a power law which we you might have seen it many times in some other talks.",
                    "label": 0
                },
                {
                    "sent": "Well, so basically.",
                    "label": 0
                },
                {
                    "sent": "That means many pay level domains act almost be level domains actually contribute one or only a few other documents, but we still have some.",
                    "label": 0
                },
                {
                    "sent": "Have several very large pay level domains that they just serve a large number of our documents.",
                    "label": 0
                },
                {
                    "sent": "That's why we have such a long tail in this power parallel distribution.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "About the vocabularies.",
                    "label": 0
                },
                {
                    "sent": "So we have around 3000 vocabularies and the most of these vocabularies comefrom.organd.edu top level domains and only a few come from others such as the common dot, EU, UK, France some others.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a brief introduction of our data set.",
                    "label": 1
                },
                {
                    "sent": "In the end about the.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Vocabulary related in this.",
                    "label": 0
                },
                {
                    "sent": "So we actually developed 6 numerical measures from 4 different aspects which are semantically related.",
                    "label": 1
                },
                {
                    "sent": "News content, similarity expressively, closeness and distributional relatedness.",
                    "label": 0
                },
                {
                    "sent": "So for semantic relatedness we developped 3 variants of measures.",
                    "label": 0
                },
                {
                    "sent": "So in total we have six measures.",
                    "label": 0
                },
                {
                    "sent": "But from for quite different aspects.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first measure is what we call explicit semantic relatedness.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is the matter is quite simple actually so.",
                    "label": 0
                },
                {
                    "sent": "We believe that true vocabularies are are related to each other.",
                    "label": 0
                },
                {
                    "sent": "If there is some kind of explicit relation between them which are given by the vocabulary owners.",
                    "label": 0
                },
                {
                    "sent": "So for example, the owner of Vocabulary One says that it imports some vocabulary true and vocabulary to maybe see also back to recovery 1.",
                    "label": 0
                },
                {
                    "sent": "Something like that.",
                    "label": 0
                },
                {
                    "sent": "So it's actually some other triples included in the meta description of vocabularies.",
                    "label": 0
                },
                {
                    "sent": "So it course it's explicit semantic relatedness.",
                    "label": 1
                },
                {
                    "sent": "So in fact we derive an undirected edge weighted graph from this connections.",
                    "label": 0
                },
                {
                    "sent": "So in this graph, each node corresponds to a vocabulary and an edge connects two nodes.",
                    "label": 0
                },
                {
                    "sent": "If there is any connection between these two vocabularies directly.",
                    "label": 0
                },
                {
                    "sent": "So particularly if the connection is mutual, is is.",
                    "label": 0
                },
                {
                    "sent": "Until on both directions.",
                    "label": 0
                },
                {
                    "sent": "So the wait is is defined as one, and if it's just single direction, the way it is defined as two so.",
                    "label": 0
                },
                {
                    "sent": "And finally, based on this graph, we can define the relatedness between any two vocabularies that they need to knows in this graph as the inverse of the weight of the shortest path between these two nodes in the graph.",
                    "label": 0
                },
                {
                    "sent": "So that's the first match.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Similarly, the second measure is called implicit semantic relatedness becausw.",
                    "label": 1
                },
                {
                    "sent": "But when we see implicit women that there is certain kind of connection between the two vocabularies but they're at the term level not not embodied in the vocabulary meta discussions.",
                    "label": 0
                },
                {
                    "sent": "For example in this case.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "Terms in different vocabularies are connected by triples, but.",
                    "label": 0
                },
                {
                    "sent": "Such such connections might not be reflected by in the vocab resubmitted description.",
                    "label": 0
                },
                {
                    "sent": "For example it might be possible that vocabulary to never says that it imports with three or never.",
                    "label": 0
                },
                {
                    "sent": "See also in through something like that.",
                    "label": 0
                },
                {
                    "sent": "So we call it implicit relatedness.",
                    "label": 0
                },
                {
                    "sent": "And similarly we can derive an undirected edge weighted graph from such.",
                    "label": 0
                },
                {
                    "sent": "Connections between terms to the relations between vocabularies.",
                    "label": 0
                },
                {
                    "sent": "And the weightings are are given in a similar way, so I want to give to much details and also the related measures defined in a similar way.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well then.",
                    "label": 0
                },
                {
                    "sent": "We can combine explicit relatedness and implicit relatedness into a single measure which required hybrid semantically related lists.",
                    "label": 0
                },
                {
                    "sent": "So it's just the in.",
                    "label": 0
                },
                {
                    "sent": "We also construct such amount on directly actuated graph and but this address and the weights are based on both explicit and implicit relatedness.",
                    "label": 0
                },
                {
                    "sent": "Relations.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the marriage is defined in a similar way.",
                    "label": 0
                },
                {
                    "sent": "So far we have three measures and also three graphs that characterize the relatedness between vocabularies.",
                    "label": 0
                },
                {
                    "sent": "So let's see some indicators of these graphs to make a comparison, but definitely these three graphs have similar sets of nodes, because each node exactly corresponds to vocabulary.",
                    "label": 0
                },
                {
                    "sent": "And for GE and GI, which are the explicit one in the implicit one, they have quite similar number of edges.",
                    "label": 0
                },
                {
                    "sent": "But when we combine them, when we merge them, roughly speaking, when we merge them, we can get even more much more edges, which means.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Although these two graphs have very have similar numbers of edges, but these edges.",
                    "label": 0
                },
                {
                    "sent": "In fact, connect quite different pairs of vocabularies.",
                    "label": 0
                },
                {
                    "sent": "In other words, many, many term never connections are not involved in the vocabulary level meter descriptions, so that's one finding here.",
                    "label": 0
                },
                {
                    "sent": "And another finding here is that.",
                    "label": 0
                },
                {
                    "sent": "Although these two graphs have quite similar number of edges, GE seems more fragmented becausw it has more isolated nodes and the size of its largest connected components is relatively small.",
                    "label": 0
                },
                {
                    "sent": "So G is more fragmented as a second file.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And also you might have notice that we actually ignore these specific types of relations between terms of between vocabularies, so there is a connection.",
                    "label": 0
                },
                {
                    "sent": "Then there is an edge we don't really care about.",
                    "label": 0
                },
                {
                    "sent": "What's the connection is about is or imports or is she?",
                    "label": 0
                },
                {
                    "sent": "Also its prior version.",
                    "label": 0
                },
                {
                    "sent": "We don't care about that.",
                    "label": 0
                },
                {
                    "sent": "Big cause.",
                    "label": 0
                },
                {
                    "sent": "In our datasets, we find that.",
                    "label": 0
                },
                {
                    "sent": "Most of such relations between vocabularies are just all imports.",
                    "label": 1
                },
                {
                    "sent": "As you can see in this table.",
                    "label": 0
                },
                {
                    "sent": "So yeah, we can still find some other relations such as.",
                    "label": 0
                },
                {
                    "sent": "See also a prior version or or some others, but they are relative relatively, not just not that popular in real life cases.",
                    "label": 0
                },
                {
                    "sent": "That's why we didn't develop any sophisticated.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Weighting schemes.",
                    "label": 0
                },
                {
                    "sent": "So the next measure is about content similarities.",
                    "label": 0
                },
                {
                    "sent": "Yeah, more closer to the the topic of this session actually so.",
                    "label": 0
                },
                {
                    "sent": "The idea here is that two vocabularies are also related to each other.",
                    "label": 0
                },
                {
                    "sent": "If the art actually similar to each other, they have very similar attacks in their descriptions, so we.",
                    "label": 0
                },
                {
                    "sent": "I don't want to go to the details actually, but basically the idea is that we just use some stream metric like called Ice App which was very alone stream metric for measuring the similarity between two strings, and so we use this measure to measure to calculate the similarity between the labels of the terms in different vocabularies and we aggregate all this similarity values into single similarity between two vocabularies.",
                    "label": 0
                },
                {
                    "sent": "So that's what we have to.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Long for this measure.",
                    "label": 0
                },
                {
                    "sent": "And I'd like to highlight on this results of empirical analysis is that, you know, we we, we calculate similarity based on labels, so we need to know which properties refer to labels actually.",
                    "label": 1
                },
                {
                    "sent": "Well.",
                    "label": 0
                },
                {
                    "sent": "In order to find us more labels as possible, we we use our label and we use this in title and we also use their sub properties and subproperties and so on and in total we have found 86 of Labor like properties.",
                    "label": 0
                },
                {
                    "sent": "But we also use local name because we have to.",
                    "label": 1
                },
                {
                    "sent": "That is becausw in our data set, we find that.",
                    "label": 0
                },
                {
                    "sent": "Well, we have like 60% of terms which have at least one label but still 1/3 of all terms have no neighbors, so that's why we also use local names in the.",
                    "label": 0
                },
                {
                    "sent": "Similarity calculation and from another point of view, these terms are distributed.",
                    "label": 0
                },
                {
                    "sent": "These terms that have labels are distributed in around only 60.",
                    "label": 0
                },
                {
                    "sent": "Only 30% of vocabularies.",
                    "label": 0
                },
                {
                    "sent": "That means.",
                    "label": 0
                },
                {
                    "sent": "Around 2/3 of vocabularies they never provide labels for their terms.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so the next batter?",
                    "label": 0
                },
                {
                    "sent": "We would like to describe this what we call it expressively?",
                    "label": 0
                },
                {
                    "sent": "Closeness?",
                    "label": 0
                },
                {
                    "sent": "So the idea is quite simple that.",
                    "label": 0
                },
                {
                    "sent": "Two vocabularies are related to each other or they are similar to each other if.",
                    "label": 0
                },
                {
                    "sent": "They have quite.",
                    "label": 0
                },
                {
                    "sent": "Similar expressive tea.",
                    "label": 0
                },
                {
                    "sent": "So the measure is also quite simple.",
                    "label": 0
                },
                {
                    "sent": "We just so the the top left side is very simple vocabulary description from which we extract all the meta level terms that have been instantiating have been used in this description.",
                    "label": 0
                },
                {
                    "sent": "For example, RDF domain an.",
                    "label": 0
                },
                {
                    "sent": "In transitive property an inverse of something like that, so they comprise a set of meta level terms that are used in this vocabulary description and given any true vocabularies, we can just compute Jaccard similarity between these two sets.",
                    "label": 0
                },
                {
                    "sent": "Two as a.",
                    "label": 0
                },
                {
                    "sent": "As a kind of indication of the expressivity closeness.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the results we would like to report about this measure is that in our datasets we find more than 4000 midlevel terms that have ever been used in at least one vocabulary.",
                    "label": 0
                },
                {
                    "sent": "Among them, more than 400 have been used in.",
                    "label": 0
                },
                {
                    "sent": "More than one vocabulary, so it's.",
                    "label": 0
                },
                {
                    "sent": "We could say that it's relatively large number because their midlevel terms, like RDF, is domain or inverse off or something like that.",
                    "label": 0
                },
                {
                    "sent": "So there there are 400.",
                    "label": 0
                },
                {
                    "sent": "And of course, the most popular ones are those from the language.",
                    "label": 1
                },
                {
                    "sent": "Languages such as RDF server or something like that, but after we excluding this language constructs.",
                    "label": 0
                },
                {
                    "sent": "Here are some top ranked wins.",
                    "label": 0
                },
                {
                    "sent": "Such as DC descripcion and scores definition and term status or something like that.",
                    "label": 0
                },
                {
                    "sent": "But basically they're not that popular.",
                    "label": 0
                },
                {
                    "sent": "So for example, DC discussion only used by like 1.5% of vocabularies.",
                    "label": 0
                },
                {
                    "sent": "From another point of view.",
                    "label": 0
                },
                {
                    "sent": "In average, each vocabulary.",
                    "label": 0
                },
                {
                    "sent": "Chooses around Tim middle level terms in its description.",
                    "label": 1
                },
                {
                    "sent": "And for most of the coverage that is for more than 990% of all the vocabularies, they use less than 20 middle level terms, but.",
                    "label": 0
                },
                {
                    "sent": "We also find hundreds of middle level terms in some large vocabulary, such as cycle.",
                    "label": 0
                },
                {
                    "sent": "They use them for metamodeling stuff.",
                    "label": 0
                },
                {
                    "sent": "So it's also quite unbalanced distribution.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Last but not least as well, personally is my favorite measure.",
                    "label": 0
                },
                {
                    "sent": "It's about distributional relatedness, so the basic idea is that two vocabularies are related to each other.",
                    "label": 0
                },
                {
                    "sent": "If they are more often used together in certain contexts.",
                    "label": 0
                },
                {
                    "sent": "So too.",
                    "label": 0
                },
                {
                    "sent": "To measure such kind of thing, we follow some ideas from linguistic research to define the distribution of profile for each vocabulary, which is N dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "Well in means the total number of vocabulary we have in our data set.",
                    "label": 0
                },
                {
                    "sent": "So each component here means it is actually a conditional probability which means given.",
                    "label": 0
                },
                {
                    "sent": "Factor that's a vocabulary has been implemented in a certain context.",
                    "label": 0
                },
                {
                    "sent": "For example another document.",
                    "label": 0
                },
                {
                    "sent": "What is the probability that every other vocabulary will be also instantiated in this?",
                    "label": 0
                },
                {
                    "sent": "Context.",
                    "label": 0
                },
                {
                    "sent": "So we have a vector and we for any two given vocabularies we can just measure the cosine similarity between their distribution profiles and to estimate such probabilities we just performed a statistical analysis of our data set.",
                    "label": 0
                },
                {
                    "sent": "And we use RTF document as the contexts.",
                    "label": 0
                },
                {
                    "sent": "But we find problems.",
                    "label": 0
                },
                {
                    "sent": "Because as we as I have presented previously that already have documents.",
                    "label": 0
                },
                {
                    "sent": "We follow a power law and very unbalanced distribution over pay level domains.",
                    "label": 0
                },
                {
                    "sent": "That means a large clever domain domain.",
                    "label": 0
                },
                {
                    "sent": "May you know, affect the results, largely which we don't like that to happen.",
                    "label": 0
                },
                {
                    "sent": "So which we choose instead we choose pay level domain as.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The Contacts.",
                    "label": 0
                },
                {
                    "sent": "Anyway, throughout our analysis we have found.",
                    "label": 0
                },
                {
                    "sent": "That more than 60% of cabinets have ever been instantiation.",
                    "label": 0
                },
                {
                    "sent": "By at least once in the data set.",
                    "label": 0
                },
                {
                    "sent": "And the most popular ones, excluding the languages.",
                    "label": 1
                },
                {
                    "sent": "Of course RDC Fauve and Geo.",
                    "label": 0
                },
                {
                    "sent": "Something like that and particularly.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Easy on the profile.",
                    "label": 0
                },
                {
                    "sent": "Credit popular.",
                    "label": 0
                },
                {
                    "sent": "You know what they said.",
                    "label": 0
                },
                {
                    "sent": "Well about Co instantiation if it has been fine.",
                    "label": 0
                },
                {
                    "sent": "Found for more than 9000 vocabularies pairs of vocabularies.",
                    "label": 1
                },
                {
                    "sent": "And the top ranked top ranked the most popular Pals, excluding languages, are some DC pals in DC plus pho for both plus Geo.",
                    "label": 0
                },
                {
                    "sent": "Something like that.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we also made the.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Operation so I have to be quick.",
                    "label": 0
                },
                {
                    "sent": "So yeah, we just use Spearman's rank correlation coefficient to look at the agreement between the different measures we have developed.",
                    "label": 1
                },
                {
                    "sent": "So the only finding out share with you is that RSE is strongly positively correlated with Rd which means.",
                    "label": 0
                },
                {
                    "sent": "If two vocabularies.",
                    "label": 0
                },
                {
                    "sent": "If there is a any relation, an explicit relation between two vocabularies.",
                    "label": 0
                },
                {
                    "sent": "So they tend to be instantiated together, so that's a message I would like to deliver based on this fact.",
                    "label": 0
                },
                {
                    "sent": "And also with you some classroom based.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Found this correlation, but yeah, it's not so important.",
                    "label": 0
                },
                {
                    "sent": "Well, and the last is we apply this related measures to the problem of.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Capital recommendations so basically imagine that we have a vocabulary search engine and for each result we would like to have such kind of related list related ontologies button.",
                    "label": 0
                },
                {
                    "sent": "Be associated with this search result so that we can also retrieve some related results immediately, so we can exactly use vocabulary relatedness for achieving this.",
                    "label": 0
                },
                {
                    "sent": "We can use a single measures or we can use.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Emanations of mirrors.",
                    "label": 0
                },
                {
                    "sent": "And we also.",
                    "label": 0
                },
                {
                    "sent": "Besides using this related relatedness measures, we also use popularity as a re ranking strategy.",
                    "label": 0
                },
                {
                    "sent": "Because we we want to recommend those popular vocabularies.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And yeah, probably I would have not enough time.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For showing the evaluation results but.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The yeah the.",
                    "label": 0
                },
                {
                    "sent": "Just some quick results so.",
                    "label": 0
                },
                {
                    "sent": "Which actually collects collected some gold standards for the relatedness between vocabularies by using polling techniques and by inviting a human experts to give gold standards and finally the results is that RC is the best performing measure in our experiment, which means that at least our human experts, they prefer to define the relatedness between vocabularies only based mainly based on the.",
                    "label": 0
                },
                {
                    "sent": "Content similarity that is, the similarity between the labels of terms.",
                    "label": 0
                },
                {
                    "sent": "And Eve.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so, um.",
                    "label": 0
                },
                {
                    "sent": "Although the content similarity has already performed very well, but it can still be improved by combined with some other measures we have developed.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, so not to do and the very last result is that we actually need to study a tradeoff between relating this and popularity because we use popularity as as a strategy for re ranking.",
                    "label": 0
                },
                {
                    "sent": "But you know, as showing this in this figure that if we tend to recommend more popular vocabularies, we might lose some relatedness, because this kind of trade off and it seems that it's quite difficult to find the best tradeoff, so it's solely depend.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Zone applications.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Conclusions.",
                    "label": 0
                },
                {
                    "sent": "Well, we have defined 6 measures of vocabulary relatedness from 4 different aspects and we performed an empirical analysis report.",
                    "label": 1
                },
                {
                    "sent": "Some sets of findings and we apply those measures to a new problem and we have implemented this recommended service at our search engine Falcons Ontology search.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this last slide?",
                    "label": 0
                },
                {
                    "sent": "Some takeaway for you vocabulary matter descriptiones are incomplete because many term level relations are not embodied in vocabulary level meter descriptiones second terms like labels.",
                    "label": 0
                },
                {
                    "sent": "Finally, explicitly relating this vocabularies tend to be instantiated together.",
                    "label": 0
                },
                {
                    "sent": "So if you like to populate your vocabulary just try to connect it to some well known ones that might help, thank you.",
                    "label": 0
                }
            ]
        }
    }
}