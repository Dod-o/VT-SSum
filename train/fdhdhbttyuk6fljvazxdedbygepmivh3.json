{
    "id": "fdhdhbttyuk6fljvazxdedbygepmivh3",
    "title": "Deep Learning via Semi-Supervised Embedding",
    "info": {
        "author": [
            "Frederic Ratle, University of Lausanne"
        ],
        "published": "Aug. 5, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Semi-supervised Learning"
        ]
    },
    "url": "http://videolectures.net/icml08_ratle_dls/",
    "segmentation": [
        [
            "So we're going to talk a little bit about deep learning, so I'm presenting this talk entitled Deep learning via semi supervised embedding, which is a joint work between Jason Weston, Hanukkah, Levar and myself."
        ],
        [
            "So what we present is a new way of deep semi supervised learning for neural networks.",
            "In our opinion it's really fast, easy and simple to implement a train compared to existing deep deep learning techniques.",
            "The experiments show that we can train networks up to 15 layers relatively easily with better result in shallow networks, that is, networks with less than four layers.",
            "And we present an application to natural language processing.",
            "Over 600 million examples, so I'll present this to you shortly.",
            "The idea is basically the train a neural net jointly with the labeled an unlabeled data.",
            "The unsupervised task here is to learn an embedding on any layer of the network, or potentially on all layers of the neural network."
        ],
        [
            "So for those, maybe you are less familiar with deep learning.",
            "It simply means systems or neural networks with many many layers.",
            "Which make them somehow more powerful.",
            "So here for example, you have a picture of Yoshua Bengio sitting.",
            "And you have a really low level representation like pixel based for instance.",
            "And from those local features you try to learn a really global features, such as the fact that there is a man sitting on the picture.",
            "And in between those two representations, well, you learn different levels of representation of increasing, increasing complexity.",
            "And this is quite useful for very hard tasks.",
            "The thing is that on such networks, standard backdrop doesn't really give good results.",
            "It's very hard to optimize."
        ],
        [
            "Many deep training methods have been proposed in the last one or two years.",
            "Among those, the deep belief Nets by Hinton and coworkers.",
            "And the different types of autoencoders by the groups of Yoshua Bengio and Ranzato, one of these, for instance, is, well we've taken this image from the paper of Banjo and Regula Hashel.",
            "And basically you lose.",
            "You use the unlabeled data to pre train the network layer by layer.",
            "So first you learn in an auto encoding of X.",
            "You encode and you decode and you try to minimize the loss function between the original input and the encoded and decoded input.",
            "So you basically learn to match these two.",
            "Once that's done, you.",
            "You discard this and you do this on the subsequent layer, and you do that type of training layer by layer up to the prediction.",
            "And once that's done, you you fine tune the network using a standard backdrop with the labeled data that you have.",
            "It works really well, but in our opinion it's a bit complicated and we tried to think of something simpler to do to train such networks."
        ],
        [
            "Now there are somehow two point of views that are conflicting here you have people doing deep learning research which we call deep researchers and people doing shallow learning research which we call shallow researchers.",
            "So basically, deep researchers believe that you have to learn different tasks in layers.",
            "Like if you want to learn what's written in a handwritten sentence, you have to 1st recognize the words.",
            "Then you have to do some tagging and then you have to understand the sentence and this can be encoded in different layers of the neural net.",
            "So this is a natural essential for multitask learning and a good thing is that the way that nonlinearity is is handled.",
            "And these types of network is computationally efficient compared to shallow methods that are usually like Ncube.",
            "On the other hand, shallow researchers usually find neural networks really complicated, which is true?",
            "And new deep methods are worse, which is also a bit true because it's hard to use if you're not very trained to use these things.",
            "And also they they're usually quite elegant, and it's quite easy to see what's going on inside.",
            "We agree with both point of views and we basically try to pick ideas from both frameworks to provide a somewhat clean framework for deep."
        ],
        [
            "Running now I'm going to describe a few algorithms that are used in shallow research, from which we're going to pick ideas to provide a new deep learning framework.",
            "Now, most embeddings algorithm are usually based on a loss function such as this one, that is a pairwise loss function that tries to preserve some structure between FXY&FXJ and the relationship and the input space between these data are encoded into the W metrics, the similarity metrics.",
            "So you have MDS Isomap who have who use this type of loss function.",
            "An the one we're focusing on Laplacian Eigen Maps.",
            "That basically minimize this loss function which encourages neighbors and input space to be neighbors in the embedding space.",
            "If you try to optimize this using an optimization algorithm rather than the eigen decomposition, you have to take into account these contract constraints which ensure that you have unit variance and zero mean, because otherwise we're going to collapse the embedding to a single point."
        ],
        [
            "Now there exists many ways to encode this type of mapping from input space to embedding space using a functional method such as a neural network.",
            "These are usually calling the CME's networks one well known methods among these are the Doctor Lim approach.",
            "Which is presented in the paper by Hatsell, Supra and Luca.",
            "Basically, the function that is optimized within the neural net using stochastic gradient descent is this one.",
            "So the upper part is basically basically Laplacian Eigen Maps.",
            "And this part is a supplementary term.",
            "In order to ensure that points that aren't neighbors are pulled apart if they are too close, like if I have if I have points that are deemed that are not the neighbors in the input space and that are mapped really closely, well, they'll be pulled apart.",
            "So this term avoids the, allows us to avoid the user of the balancing constraints because you.",
            "Because of this, you can't collapse.",
            "You're embedding to one single point, which is good, and also using a function such as a neural network rather than a point to point correspondence like in most embedding algorithms is also interesting because you can encode and this set of function prior knowledge that you have about the problem.",
            "Then you can control the capacity of those functions and importantly, you don't have an out of sample problem.",
            "That is, if you apply normal Laplacian Eigen Maps and then you have new points that you want to map while you're a bit annoyed because you have to recompute the eigenvectors or use some very complicated techniques.",
            "While here we have a nice function that we can apply to any other point."
        ],
        [
            "These ideas have been applied in the semi supervised learning context.",
            "Here we give an example.",
            "It's the Laplacian SVM presented a few years ago.",
            "What it does is simply adding to the SVM loss function a term based on manifold regularization.",
            "That is, you regularize the input as to force neighbors in input space to be neighbors in the embedded space.",
            "So you also try to minimize this term and the two terms are weighted by a Lambda factor.",
            "Other techniques that would fall into the.",
            "Mold of shallow semi supervision is what people usually call pre processing, such as using ISOMAP as input to an SVM or any other feature extraction technique that people use prior to a classification task or regression task."
        ],
        [
            "What we propose is here is to use these ideas in the neural network context.",
            "We call this deep embedding the use of a neural net allows us to, rather than applying the regularizer on the output layer, such as in the lap SVM, you can actually apply your regularizer at any layer of the neural net.",
            "Applied here, this looks quite similar to lap SVM.",
            "You can apply it also internally on any hidden layer of the architecture.",
            "Or you can also learn an auxiliary task, the embedding as a separate task, so you share the weights in the first layers of the architecture and then you learn a separate task, the embedding and you hope that it will help to improve your supervised learning task.",
            "So these three modes are expressed here in those three loss functions, Big L. So we have out regularization on the output internally or as an auxiliary task."
        ],
        [
            "And the algorithm is quite simple.",
            "All you have to do is at each iteration you pick some labeled example.",
            "You optimize the supervised loss function.",
            "Here we have used the hinge loss function.",
            "And then you optimize your unsupervised loss function so you pick some neighbors.",
            "You make a Step 2 to push them together, and you pick some random examples.",
            "Non neighbors.",
            "And you make a step on your unsupervised loss function to pull them apart.",
            "So it's quite simple and the use of stochastic gradient descent makes it really really fast."
        ],
        [
            "So we have first tested the approach on some classical datasets that are used widely in the semi supervised learning literature.",
            "So G50C text, USPS and NIST amnist with 106 hundred and 1000 labels.",
            "We have to keep things simple.",
            "We have put an equal weight to both loss function supervised and unsupervised.",
            "And in this first experiments we only consider two layer Nets, so we expect some kind of similar results to Laplacian SVM 'cause it's using a two layer net.",
            "It's basically the same idea."
        ],
        [
            "So here are the 1st results on those small scale datasets.",
            "We've also compared them to the transductive SVM method, which is another semi supervised learning method that's widely used.",
            "As it could be expected, we observed between a normal neural net and our method, which we coin and embed an N the same type of difference that you observe between the SVM and the lap SVM.",
            "So this is an encouraging result, but somehow it could be expected.",
            "And now the interesting thing is that the use of deep networks.",
            "Will allow us to handle much larger problems and much harder task than the lap SVM that doesn't really scale."
        ],
        [
            "So here we we.",
            "We performed some experiments on the MNIST datasets with the number of labeled examples that I specified.",
            "We compare those to some deep learning methods.",
            "We just actually picked the results from the corresponding papers.",
            "So we have the restricted Boltzmann machine.",
            "The sparse encoding machine in the deep belief Nets.",
            "And here we have a normal neural net and a convolutional neural net with six layers with regularization on the output on the on internal layer or as a separate auxiliary task.",
            "So what we observe is that we are actually quite competitive with the existing methods, even much better in some cases.",
            "An and it's actually quite faster, it's.",
            "How using on the MNIST data set it takes about an hour maybe on the full data set so it's.",
            "Quite interesting on that respect.",
            "And so in those two cases, it's a six layer neural network."
        ],
        [
            "And here we tried to compare two different modes of embedding.",
            "That is the embedding on the output layer, such as in the Laplacian SVM and the embedding on all the layers of the neural networks.",
            "Just to see what would happen and we compare this to a normal neural neural net.",
            "Here we have a varying number of layers from 2 to 15, with each 50 hidden units.",
            "As we can expect, the normal neural net obviously overfits really fast, while both methods with the embedding kind of continue decreasing, it sort of stabilizes around here, but still we don't.",
            "We don't observe it out when overfitting as we do with the neural network without the embedding regularization.",
            "And we observe that the embedding on all layers is looks like the best method on this task."
        ],
        [
            "Now The thing is that.",
            "Since now I I've mentioned that we're using pairwise relationship between examples, but legitimately one could ask, isn't training on pairs of examples slow?",
            "Well, the answer is no.",
            "Because you could use any sampling methods to perform KNN.",
            "Or you could use epsilon bars if you can.",
            "If your problem allows you to use Euclidean distance.",
            "But also very often the context of the problem allows you to avoid the precomputation of neighbors.",
            "For instance, in the video sequences, if you have two consecutive frames, while you can just label them as neighbors, and that's it.",
            "The same goes for audio, or for text very common hypothesis that is made is that words that are close usually share the same topic, so they can also be labeled as neighbors.",
            "So there are methods to actually speed the computation of the W matrix, which is obviously really hard to compute for problems such as amnesty or bigger."
        ],
        [
            "Then there is a.",
            "We did an application to the natural language processing problem of semantic role labeling and from those label predicting whether a sentence is from the English language or not.",
            "We compare this method to the ASIRT method, which is a method based on hand coded features.",
            "So a lot of prior knowledge.",
            "An and a simple convolutional neural net, and with our NR method with no prior knowledge.",
            "Basically how did we construct the neighbors?",
            "Well, it's just that.",
            "If we have two English sentences, we label them as neighbors, such as the cat sat on the mat and champion.",
            "Federer wins again.",
            "Actually, we don't know.",
            "We'll know in 2 hours maybe.",
            "And then we take one of those sentences and we distorted syntactically or semantically, so it's no longer a sentence from the English language.",
            "So here we swap Federer and began so champion began wins again.",
            "It's not an English structure, so we label it as a non neighbor pair.",
            "And using this embedding regularization on a CNN, we achieve an error rate of 1415%, which is actually better than the method that uses prior knowledge.",
            "So it's quite interesting and this has been done.",
            "The unlabeled data set is of a size of 631 million, so it's.",
            "So there is some scalability here."
        ],
        [
            "And that's it.",
            "In conclusion, well, people already used these ID's and the shallow learning literature.",
            "By using unlabeled data as in a pre processing fashion or such as in lap SVM to embed.",
            "Regularize the output with an embedding regularization.",
            "Our methods somehow generalizes these results because we we applied this regularization on any layer of the network.",
            "And So what can it bring?",
            "It's very easy to train very fast and scalable using stochastic gradient descent.",
            "And we can train.",
            "We've trained networks up to 15 layers, so it's quite interesting.",
            "And we hope it should help, given that our embedding matrix W is correlated to the supervised task.",
            "But if it's not, well, I don't think that you can do anything either, so.",
            "I think it's a.",
            "This is a reasonable hypothesis, so that's it, and thank you.",
            "Methods comes over linear methods and sometimes comes from the fact that you can have data points are close together actually get very different representations in the hidden layer, But then you have an embedding in every single layer.",
            "Your enforcement of the opposite that you enforcing the data points are close.",
            "We've always had to be closed.",
            "In some sense, you're taking away the power from the nonlinearity of your networks.",
            "Give any comments.",
            "Well yeah, it's true.",
            "I guess that for.",
            "EMS data set.",
            "It's I mean it's a data set that's pretty well.",
            "The groups are quite well defined, so maybe that wasn't the problem, but I think that you're right.",
            "In that case the internal mode of embedding shouldn't be the best mode.",
            "Maybe learning the embedding as an auxiliary tasks in the cases where we actually lose this contribution from the internal layers would be would probably give better results.",
            "So I think it's just the internal mode of embedding that with the provide the worst result in that case.",
            "Yeah.",
            "Questions.",
            "If the networks so I was wondering, in the customers VM I have this too.",
            "The objectives and you might talk to Mike both at the same time.",
            "In this layer networks, it seems that you're optimizing for this embedding thing, and then only the last step you're actually optimizing for the loss function.",
            "I was wondering if that is true.",
            "It isn't that a problem, the fact that negative embedding will make it look information like PCA isn't the same as an everyday, and you mean the unsupervised last function?",
            "Well, we can add it on any of the layers so.",
            "Information as labeled information.",
            "Oh"
        ],
        [
            "Actually, you have a supervised loss function and as in lap SVM you have a sum of those two terms."
        ],
        [
            "And when you take at each iteration, you take a gradient step on the supervised loss function before doing doing it on the unsupervised loss function.",
            "I understand I'm talking about the data, yeah?",
            "With the name of examples through using that loss.",
            "Or my questions to user not only the supervised flossing and final layer?",
            "Oh yeah, yeah, yeah.",
            "Why is the gates of prediction?",
            "After this information, when you're going up on the neighbors in front of these.",
            "If you're.",
            "Projecting on the lower dimensional space.",
            "And not using the new information.",
            "And so maybe you have 15 layers type.",
            "I'd make sure that in the layer #15 you actually have information available for the destination task.",
            "Only additional questions asked and my second question is being related to that is you said he used Lambda equal to 1 so you waited equally both terms.",
            "I know that in the best SVM they use a range of values like 4 -- 10 to the 6:00 to 5:50.",
            "Well you can, you can do it, it's just it's just another parameter to tune.",
            "You could fine tune it, but it seems to work just like that.",
            "So of course you can play with.",
            "There's many more parameters with.",
            "Which you can play, but I think that.",
            "I mean, since it's it's works without tuning it, then it's really it's an advantage.",
            "Of course then you can fine tune this Lambda factor and obtain eventually better results, but we just we just optimized the rest.",
            "Put in equal.",
            "But of course you can push the optimization further and play with the factor of course.",
            "It's a, it's just something else to tune so.",
            "It seems that it's quite.",
            "It's not so sensitive, actually.",
            "I think this parameter.",
            "I was wondering you chose, I think 50.",
            "I'm wondering if you ran the experiments.",
            "Pairing different numbers of yeah, they were optimized by cross validation.",
            "So I guess the thing it would be interesting right is.",
            "How does kind of like a deep network with say 2000 units compared to the shallow network with 2000 units?",
            "And if you have a neural network with.",
            "More hidden, sorry.",
            "It seems that maybe that.",
            "Go away two layer network, but I should perform better as well so well.",
            "I can't really answer directly that question, 'cause I don't know, but people tend to argue that if for the same number of hidden neurons you can learn more complex tasks if you go in depth rather than with so people.",
            "I guess.",
            "I'm hoping that someone you know has evidence to really like, say that too.",
            "Well, maybe there are some actual comparisons?",
            "I think so.",
            "But yeah, we just haven't.",
            "We didn't do that comparison.",
            "Network.",
            "The number of units please.",
            "Foundation.",
            "That was just it.",
            "Sorry.",
            "Yes.",
            "It's not the same as what you're doing, but.",
            "Pointer there also.",
            "Doing some supervised learning by generating tasks.",
            "Natural resources in communities.",
            "Fantastic result.",
            "I think, well yeah, maybe you can just show me later.",
            "I might have seen it, yeah?",
            "One question.",
            "The intuitive question when you learning these auxiliary embeddings is the is the is your intuition back there.",
            "They're helping the training too.",
            "Avoid very bad local minima.",
            "Or is the intuition more that there helping to prevent overfitting?",
            "Or is there?",
            "Are they trying to do both at the same time?",
            "It's something that is a little different.",
            "Overfitting is you're finding too good in minimum and almost want to be trapped in the local movements in case still perfect so.",
            "I don't know.",
            "In my opinion, it's just that if you don't put that regularizer you get like huge overfitting.",
            "So it's just a way in my opinion of somehow constraining that solution so it's.",
            "It's more like keeping the parameters in the sensible range, I think.",
            "OK, good luck."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we're going to talk a little bit about deep learning, so I'm presenting this talk entitled Deep learning via semi supervised embedding, which is a joint work between Jason Weston, Hanukkah, Levar and myself.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what we present is a new way of deep semi supervised learning for neural networks.",
                    "label": 1
                },
                {
                    "sent": "In our opinion it's really fast, easy and simple to implement a train compared to existing deep deep learning techniques.",
                    "label": 0
                },
                {
                    "sent": "The experiments show that we can train networks up to 15 layers relatively easily with better result in shallow networks, that is, networks with less than four layers.",
                    "label": 0
                },
                {
                    "sent": "And we present an application to natural language processing.",
                    "label": 1
                },
                {
                    "sent": "Over 600 million examples, so I'll present this to you shortly.",
                    "label": 0
                },
                {
                    "sent": "The idea is basically the train a neural net jointly with the labeled an unlabeled data.",
                    "label": 1
                },
                {
                    "sent": "The unsupervised task here is to learn an embedding on any layer of the network, or potentially on all layers of the neural network.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for those, maybe you are less familiar with deep learning.",
                    "label": 1
                },
                {
                    "sent": "It simply means systems or neural networks with many many layers.",
                    "label": 0
                },
                {
                    "sent": "Which make them somehow more powerful.",
                    "label": 0
                },
                {
                    "sent": "So here for example, you have a picture of Yoshua Bengio sitting.",
                    "label": 0
                },
                {
                    "sent": "And you have a really low level representation like pixel based for instance.",
                    "label": 0
                },
                {
                    "sent": "And from those local features you try to learn a really global features, such as the fact that there is a man sitting on the picture.",
                    "label": 0
                },
                {
                    "sent": "And in between those two representations, well, you learn different levels of representation of increasing, increasing complexity.",
                    "label": 0
                },
                {
                    "sent": "And this is quite useful for very hard tasks.",
                    "label": 0
                },
                {
                    "sent": "The thing is that on such networks, standard backdrop doesn't really give good results.",
                    "label": 0
                },
                {
                    "sent": "It's very hard to optimize.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Many deep training methods have been proposed in the last one or two years.",
                    "label": 1
                },
                {
                    "sent": "Among those, the deep belief Nets by Hinton and coworkers.",
                    "label": 0
                },
                {
                    "sent": "And the different types of autoencoders by the groups of Yoshua Bengio and Ranzato, one of these, for instance, is, well we've taken this image from the paper of Banjo and Regula Hashel.",
                    "label": 0
                },
                {
                    "sent": "And basically you lose.",
                    "label": 1
                },
                {
                    "sent": "You use the unlabeled data to pre train the network layer by layer.",
                    "label": 0
                },
                {
                    "sent": "So first you learn in an auto encoding of X.",
                    "label": 0
                },
                {
                    "sent": "You encode and you decode and you try to minimize the loss function between the original input and the encoded and decoded input.",
                    "label": 0
                },
                {
                    "sent": "So you basically learn to match these two.",
                    "label": 0
                },
                {
                    "sent": "Once that's done, you.",
                    "label": 0
                },
                {
                    "sent": "You discard this and you do this on the subsequent layer, and you do that type of training layer by layer up to the prediction.",
                    "label": 1
                },
                {
                    "sent": "And once that's done, you you fine tune the network using a standard backdrop with the labeled data that you have.",
                    "label": 0
                },
                {
                    "sent": "It works really well, but in our opinion it's a bit complicated and we tried to think of something simpler to do to train such networks.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now there are somehow two point of views that are conflicting here you have people doing deep learning research which we call deep researchers and people doing shallow learning research which we call shallow researchers.",
                    "label": 0
                },
                {
                    "sent": "So basically, deep researchers believe that you have to learn different tasks in layers.",
                    "label": 1
                },
                {
                    "sent": "Like if you want to learn what's written in a handwritten sentence, you have to 1st recognize the words.",
                    "label": 0
                },
                {
                    "sent": "Then you have to do some tagging and then you have to understand the sentence and this can be encoded in different layers of the neural net.",
                    "label": 1
                },
                {
                    "sent": "So this is a natural essential for multitask learning and a good thing is that the way that nonlinearity is is handled.",
                    "label": 1
                },
                {
                    "sent": "And these types of network is computationally efficient compared to shallow methods that are usually like Ncube.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, shallow researchers usually find neural networks really complicated, which is true?",
                    "label": 0
                },
                {
                    "sent": "And new deep methods are worse, which is also a bit true because it's hard to use if you're not very trained to use these things.",
                    "label": 0
                },
                {
                    "sent": "And also they they're usually quite elegant, and it's quite easy to see what's going on inside.",
                    "label": 0
                },
                {
                    "sent": "We agree with both point of views and we basically try to pick ideas from both frameworks to provide a somewhat clean framework for deep.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Running now I'm going to describe a few algorithms that are used in shallow research, from which we're going to pick ideas to provide a new deep learning framework.",
                    "label": 0
                },
                {
                    "sent": "Now, most embeddings algorithm are usually based on a loss function such as this one, that is a pairwise loss function that tries to preserve some structure between FXY&FXJ and the relationship and the input space between these data are encoded into the W metrics, the similarity metrics.",
                    "label": 0
                },
                {
                    "sent": "So you have MDS Isomap who have who use this type of loss function.",
                    "label": 0
                },
                {
                    "sent": "An the one we're focusing on Laplacian Eigen Maps.",
                    "label": 0
                },
                {
                    "sent": "That basically minimize this loss function which encourages neighbors and input space to be neighbors in the embedding space.",
                    "label": 0
                },
                {
                    "sent": "If you try to optimize this using an optimization algorithm rather than the eigen decomposition, you have to take into account these contract constraints which ensure that you have unit variance and zero mean, because otherwise we're going to collapse the embedding to a single point.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now there exists many ways to encode this type of mapping from input space to embedding space using a functional method such as a neural network.",
                    "label": 0
                },
                {
                    "sent": "These are usually calling the CME's networks one well known methods among these are the Doctor Lim approach.",
                    "label": 0
                },
                {
                    "sent": "Which is presented in the paper by Hatsell, Supra and Luca.",
                    "label": 0
                },
                {
                    "sent": "Basically, the function that is optimized within the neural net using stochastic gradient descent is this one.",
                    "label": 0
                },
                {
                    "sent": "So the upper part is basically basically Laplacian Eigen Maps.",
                    "label": 0
                },
                {
                    "sent": "And this part is a supplementary term.",
                    "label": 0
                },
                {
                    "sent": "In order to ensure that points that aren't neighbors are pulled apart if they are too close, like if I have if I have points that are deemed that are not the neighbors in the input space and that are mapped really closely, well, they'll be pulled apart.",
                    "label": 0
                },
                {
                    "sent": "So this term avoids the, allows us to avoid the user of the balancing constraints because you.",
                    "label": 0
                },
                {
                    "sent": "Because of this, you can't collapse.",
                    "label": 0
                },
                {
                    "sent": "You're embedding to one single point, which is good, and also using a function such as a neural network rather than a point to point correspondence like in most embedding algorithms is also interesting because you can encode and this set of function prior knowledge that you have about the problem.",
                    "label": 0
                },
                {
                    "sent": "Then you can control the capacity of those functions and importantly, you don't have an out of sample problem.",
                    "label": 0
                },
                {
                    "sent": "That is, if you apply normal Laplacian Eigen Maps and then you have new points that you want to map while you're a bit annoyed because you have to recompute the eigenvectors or use some very complicated techniques.",
                    "label": 0
                },
                {
                    "sent": "While here we have a nice function that we can apply to any other point.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "These ideas have been applied in the semi supervised learning context.",
                    "label": 0
                },
                {
                    "sent": "Here we give an example.",
                    "label": 0
                },
                {
                    "sent": "It's the Laplacian SVM presented a few years ago.",
                    "label": 0
                },
                {
                    "sent": "What it does is simply adding to the SVM loss function a term based on manifold regularization.",
                    "label": 0
                },
                {
                    "sent": "That is, you regularize the input as to force neighbors in input space to be neighbors in the embedded space.",
                    "label": 0
                },
                {
                    "sent": "So you also try to minimize this term and the two terms are weighted by a Lambda factor.",
                    "label": 0
                },
                {
                    "sent": "Other techniques that would fall into the.",
                    "label": 0
                },
                {
                    "sent": "Mold of shallow semi supervision is what people usually call pre processing, such as using ISOMAP as input to an SVM or any other feature extraction technique that people use prior to a classification task or regression task.",
                    "label": 1
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we propose is here is to use these ideas in the neural network context.",
                    "label": 0
                },
                {
                    "sent": "We call this deep embedding the use of a neural net allows us to, rather than applying the regularizer on the output layer, such as in the lap SVM, you can actually apply your regularizer at any layer of the neural net.",
                    "label": 0
                },
                {
                    "sent": "Applied here, this looks quite similar to lap SVM.",
                    "label": 0
                },
                {
                    "sent": "You can apply it also internally on any hidden layer of the architecture.",
                    "label": 0
                },
                {
                    "sent": "Or you can also learn an auxiliary task, the embedding as a separate task, so you share the weights in the first layers of the architecture and then you learn a separate task, the embedding and you hope that it will help to improve your supervised learning task.",
                    "label": 0
                },
                {
                    "sent": "So these three modes are expressed here in those three loss functions, Big L. So we have out regularization on the output internally or as an auxiliary task.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the algorithm is quite simple.",
                    "label": 0
                },
                {
                    "sent": "All you have to do is at each iteration you pick some labeled example.",
                    "label": 0
                },
                {
                    "sent": "You optimize the supervised loss function.",
                    "label": 0
                },
                {
                    "sent": "Here we have used the hinge loss function.",
                    "label": 0
                },
                {
                    "sent": "And then you optimize your unsupervised loss function so you pick some neighbors.",
                    "label": 0
                },
                {
                    "sent": "You make a Step 2 to push them together, and you pick some random examples.",
                    "label": 0
                },
                {
                    "sent": "Non neighbors.",
                    "label": 0
                },
                {
                    "sent": "And you make a step on your unsupervised loss function to pull them apart.",
                    "label": 0
                },
                {
                    "sent": "So it's quite simple and the use of stochastic gradient descent makes it really really fast.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we have first tested the approach on some classical datasets that are used widely in the semi supervised learning literature.",
                    "label": 0
                },
                {
                    "sent": "So G50C text, USPS and NIST amnist with 106 hundred and 1000 labels.",
                    "label": 0
                },
                {
                    "sent": "We have to keep things simple.",
                    "label": 0
                },
                {
                    "sent": "We have put an equal weight to both loss function supervised and unsupervised.",
                    "label": 1
                },
                {
                    "sent": "And in this first experiments we only consider two layer Nets, so we expect some kind of similar results to Laplacian SVM 'cause it's using a two layer net.",
                    "label": 0
                },
                {
                    "sent": "It's basically the same idea.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here are the 1st results on those small scale datasets.",
                    "label": 0
                },
                {
                    "sent": "We've also compared them to the transductive SVM method, which is another semi supervised learning method that's widely used.",
                    "label": 0
                },
                {
                    "sent": "As it could be expected, we observed between a normal neural net and our method, which we coin and embed an N the same type of difference that you observe between the SVM and the lap SVM.",
                    "label": 0
                },
                {
                    "sent": "So this is an encouraging result, but somehow it could be expected.",
                    "label": 0
                },
                {
                    "sent": "And now the interesting thing is that the use of deep networks.",
                    "label": 0
                },
                {
                    "sent": "Will allow us to handle much larger problems and much harder task than the lap SVM that doesn't really scale.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we we.",
                    "label": 0
                },
                {
                    "sent": "We performed some experiments on the MNIST datasets with the number of labeled examples that I specified.",
                    "label": 0
                },
                {
                    "sent": "We compare those to some deep learning methods.",
                    "label": 0
                },
                {
                    "sent": "We just actually picked the results from the corresponding papers.",
                    "label": 0
                },
                {
                    "sent": "So we have the restricted Boltzmann machine.",
                    "label": 0
                },
                {
                    "sent": "The sparse encoding machine in the deep belief Nets.",
                    "label": 0
                },
                {
                    "sent": "And here we have a normal neural net and a convolutional neural net with six layers with regularization on the output on the on internal layer or as a separate auxiliary task.",
                    "label": 0
                },
                {
                    "sent": "So what we observe is that we are actually quite competitive with the existing methods, even much better in some cases.",
                    "label": 0
                },
                {
                    "sent": "An and it's actually quite faster, it's.",
                    "label": 0
                },
                {
                    "sent": "How using on the MNIST data set it takes about an hour maybe on the full data set so it's.",
                    "label": 0
                },
                {
                    "sent": "Quite interesting on that respect.",
                    "label": 0
                },
                {
                    "sent": "And so in those two cases, it's a six layer neural network.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And here we tried to compare two different modes of embedding.",
                    "label": 0
                },
                {
                    "sent": "That is the embedding on the output layer, such as in the Laplacian SVM and the embedding on all the layers of the neural networks.",
                    "label": 1
                },
                {
                    "sent": "Just to see what would happen and we compare this to a normal neural neural net.",
                    "label": 0
                },
                {
                    "sent": "Here we have a varying number of layers from 2 to 15, with each 50 hidden units.",
                    "label": 0
                },
                {
                    "sent": "As we can expect, the normal neural net obviously overfits really fast, while both methods with the embedding kind of continue decreasing, it sort of stabilizes around here, but still we don't.",
                    "label": 0
                },
                {
                    "sent": "We don't observe it out when overfitting as we do with the neural network without the embedding regularization.",
                    "label": 0
                },
                {
                    "sent": "And we observe that the embedding on all layers is looks like the best method on this task.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now The thing is that.",
                    "label": 0
                },
                {
                    "sent": "Since now I I've mentioned that we're using pairwise relationship between examples, but legitimately one could ask, isn't training on pairs of examples slow?",
                    "label": 1
                },
                {
                    "sent": "Well, the answer is no.",
                    "label": 0
                },
                {
                    "sent": "Because you could use any sampling methods to perform KNN.",
                    "label": 1
                },
                {
                    "sent": "Or you could use epsilon bars if you can.",
                    "label": 0
                },
                {
                    "sent": "If your problem allows you to use Euclidean distance.",
                    "label": 0
                },
                {
                    "sent": "But also very often the context of the problem allows you to avoid the precomputation of neighbors.",
                    "label": 0
                },
                {
                    "sent": "For instance, in the video sequences, if you have two consecutive frames, while you can just label them as neighbors, and that's it.",
                    "label": 0
                },
                {
                    "sent": "The same goes for audio, or for text very common hypothesis that is made is that words that are close usually share the same topic, so they can also be labeled as neighbors.",
                    "label": 0
                },
                {
                    "sent": "So there are methods to actually speed the computation of the W matrix, which is obviously really hard to compute for problems such as amnesty or bigger.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then there is a.",
                    "label": 0
                },
                {
                    "sent": "We did an application to the natural language processing problem of semantic role labeling and from those label predicting whether a sentence is from the English language or not.",
                    "label": 0
                },
                {
                    "sent": "We compare this method to the ASIRT method, which is a method based on hand coded features.",
                    "label": 0
                },
                {
                    "sent": "So a lot of prior knowledge.",
                    "label": 1
                },
                {
                    "sent": "An and a simple convolutional neural net, and with our NR method with no prior knowledge.",
                    "label": 1
                },
                {
                    "sent": "Basically how did we construct the neighbors?",
                    "label": 0
                },
                {
                    "sent": "Well, it's just that.",
                    "label": 0
                },
                {
                    "sent": "If we have two English sentences, we label them as neighbors, such as the cat sat on the mat and champion.",
                    "label": 1
                },
                {
                    "sent": "Federer wins again.",
                    "label": 0
                },
                {
                    "sent": "Actually, we don't know.",
                    "label": 0
                },
                {
                    "sent": "We'll know in 2 hours maybe.",
                    "label": 0
                },
                {
                    "sent": "And then we take one of those sentences and we distorted syntactically or semantically, so it's no longer a sentence from the English language.",
                    "label": 1
                },
                {
                    "sent": "So here we swap Federer and began so champion began wins again.",
                    "label": 0
                },
                {
                    "sent": "It's not an English structure, so we label it as a non neighbor pair.",
                    "label": 0
                },
                {
                    "sent": "And using this embedding regularization on a CNN, we achieve an error rate of 1415%, which is actually better than the method that uses prior knowledge.",
                    "label": 0
                },
                {
                    "sent": "So it's quite interesting and this has been done.",
                    "label": 0
                },
                {
                    "sent": "The unlabeled data set is of a size of 631 million, so it's.",
                    "label": 0
                },
                {
                    "sent": "So there is some scalability here.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And that's it.",
                    "label": 0
                },
                {
                    "sent": "In conclusion, well, people already used these ID's and the shallow learning literature.",
                    "label": 0
                },
                {
                    "sent": "By using unlabeled data as in a pre processing fashion or such as in lap SVM to embed.",
                    "label": 0
                },
                {
                    "sent": "Regularize the output with an embedding regularization.",
                    "label": 0
                },
                {
                    "sent": "Our methods somehow generalizes these results because we we applied this regularization on any layer of the network.",
                    "label": 0
                },
                {
                    "sent": "And So what can it bring?",
                    "label": 0
                },
                {
                    "sent": "It's very easy to train very fast and scalable using stochastic gradient descent.",
                    "label": 0
                },
                {
                    "sent": "And we can train.",
                    "label": 0
                },
                {
                    "sent": "We've trained networks up to 15 layers, so it's quite interesting.",
                    "label": 0
                },
                {
                    "sent": "And we hope it should help, given that our embedding matrix W is correlated to the supervised task.",
                    "label": 1
                },
                {
                    "sent": "But if it's not, well, I don't think that you can do anything either, so.",
                    "label": 0
                },
                {
                    "sent": "I think it's a.",
                    "label": 0
                },
                {
                    "sent": "This is a reasonable hypothesis, so that's it, and thank you.",
                    "label": 0
                },
                {
                    "sent": "Methods comes over linear methods and sometimes comes from the fact that you can have data points are close together actually get very different representations in the hidden layer, But then you have an embedding in every single layer.",
                    "label": 0
                },
                {
                    "sent": "Your enforcement of the opposite that you enforcing the data points are close.",
                    "label": 0
                },
                {
                    "sent": "We've always had to be closed.",
                    "label": 0
                },
                {
                    "sent": "In some sense, you're taking away the power from the nonlinearity of your networks.",
                    "label": 0
                },
                {
                    "sent": "Give any comments.",
                    "label": 0
                },
                {
                    "sent": "Well yeah, it's true.",
                    "label": 0
                },
                {
                    "sent": "I guess that for.",
                    "label": 0
                },
                {
                    "sent": "EMS data set.",
                    "label": 0
                },
                {
                    "sent": "It's I mean it's a data set that's pretty well.",
                    "label": 0
                },
                {
                    "sent": "The groups are quite well defined, so maybe that wasn't the problem, but I think that you're right.",
                    "label": 0
                },
                {
                    "sent": "In that case the internal mode of embedding shouldn't be the best mode.",
                    "label": 0
                },
                {
                    "sent": "Maybe learning the embedding as an auxiliary tasks in the cases where we actually lose this contribution from the internal layers would be would probably give better results.",
                    "label": 0
                },
                {
                    "sent": "So I think it's just the internal mode of embedding that with the provide the worst result in that case.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                },
                {
                    "sent": "If the networks so I was wondering, in the customers VM I have this too.",
                    "label": 0
                },
                {
                    "sent": "The objectives and you might talk to Mike both at the same time.",
                    "label": 0
                },
                {
                    "sent": "In this layer networks, it seems that you're optimizing for this embedding thing, and then only the last step you're actually optimizing for the loss function.",
                    "label": 0
                },
                {
                    "sent": "I was wondering if that is true.",
                    "label": 0
                },
                {
                    "sent": "It isn't that a problem, the fact that negative embedding will make it look information like PCA isn't the same as an everyday, and you mean the unsupervised last function?",
                    "label": 0
                },
                {
                    "sent": "Well, we can add it on any of the layers so.",
                    "label": 0
                },
                {
                    "sent": "Information as labeled information.",
                    "label": 0
                },
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually, you have a supervised loss function and as in lap SVM you have a sum of those two terms.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And when you take at each iteration, you take a gradient step on the supervised loss function before doing doing it on the unsupervised loss function.",
                    "label": 0
                },
                {
                    "sent": "I understand I'm talking about the data, yeah?",
                    "label": 0
                },
                {
                    "sent": "With the name of examples through using that loss.",
                    "label": 0
                },
                {
                    "sent": "Or my questions to user not only the supervised flossing and final layer?",
                    "label": 1
                },
                {
                    "sent": "Oh yeah, yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Why is the gates of prediction?",
                    "label": 0
                },
                {
                    "sent": "After this information, when you're going up on the neighbors in front of these.",
                    "label": 0
                },
                {
                    "sent": "If you're.",
                    "label": 0
                },
                {
                    "sent": "Projecting on the lower dimensional space.",
                    "label": 0
                },
                {
                    "sent": "And not using the new information.",
                    "label": 0
                },
                {
                    "sent": "And so maybe you have 15 layers type.",
                    "label": 0
                },
                {
                    "sent": "I'd make sure that in the layer #15 you actually have information available for the destination task.",
                    "label": 0
                },
                {
                    "sent": "Only additional questions asked and my second question is being related to that is you said he used Lambda equal to 1 so you waited equally both terms.",
                    "label": 0
                },
                {
                    "sent": "I know that in the best SVM they use a range of values like 4 -- 10 to the 6:00 to 5:50.",
                    "label": 0
                },
                {
                    "sent": "Well you can, you can do it, it's just it's just another parameter to tune.",
                    "label": 0
                },
                {
                    "sent": "You could fine tune it, but it seems to work just like that.",
                    "label": 0
                },
                {
                    "sent": "So of course you can play with.",
                    "label": 0
                },
                {
                    "sent": "There's many more parameters with.",
                    "label": 0
                },
                {
                    "sent": "Which you can play, but I think that.",
                    "label": 0
                },
                {
                    "sent": "I mean, since it's it's works without tuning it, then it's really it's an advantage.",
                    "label": 0
                },
                {
                    "sent": "Of course then you can fine tune this Lambda factor and obtain eventually better results, but we just we just optimized the rest.",
                    "label": 0
                },
                {
                    "sent": "Put in equal.",
                    "label": 0
                },
                {
                    "sent": "But of course you can push the optimization further and play with the factor of course.",
                    "label": 0
                },
                {
                    "sent": "It's a, it's just something else to tune so.",
                    "label": 0
                },
                {
                    "sent": "It seems that it's quite.",
                    "label": 0
                },
                {
                    "sent": "It's not so sensitive, actually.",
                    "label": 0
                },
                {
                    "sent": "I think this parameter.",
                    "label": 0
                },
                {
                    "sent": "I was wondering you chose, I think 50.",
                    "label": 0
                },
                {
                    "sent": "I'm wondering if you ran the experiments.",
                    "label": 0
                },
                {
                    "sent": "Pairing different numbers of yeah, they were optimized by cross validation.",
                    "label": 0
                },
                {
                    "sent": "So I guess the thing it would be interesting right is.",
                    "label": 1
                },
                {
                    "sent": "How does kind of like a deep network with say 2000 units compared to the shallow network with 2000 units?",
                    "label": 0
                },
                {
                    "sent": "And if you have a neural network with.",
                    "label": 0
                },
                {
                    "sent": "More hidden, sorry.",
                    "label": 0
                },
                {
                    "sent": "It seems that maybe that.",
                    "label": 0
                },
                {
                    "sent": "Go away two layer network, but I should perform better as well so well.",
                    "label": 0
                },
                {
                    "sent": "I can't really answer directly that question, 'cause I don't know, but people tend to argue that if for the same number of hidden neurons you can learn more complex tasks if you go in depth rather than with so people.",
                    "label": 0
                },
                {
                    "sent": "I guess.",
                    "label": 0
                },
                {
                    "sent": "I'm hoping that someone you know has evidence to really like, say that too.",
                    "label": 0
                },
                {
                    "sent": "Well, maybe there are some actual comparisons?",
                    "label": 0
                },
                {
                    "sent": "I think so.",
                    "label": 0
                },
                {
                    "sent": "But yeah, we just haven't.",
                    "label": 0
                },
                {
                    "sent": "We didn't do that comparison.",
                    "label": 0
                },
                {
                    "sent": "Network.",
                    "label": 0
                },
                {
                    "sent": "The number of units please.",
                    "label": 0
                },
                {
                    "sent": "Foundation.",
                    "label": 0
                },
                {
                    "sent": "That was just it.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "It's not the same as what you're doing, but.",
                    "label": 0
                },
                {
                    "sent": "Pointer there also.",
                    "label": 0
                },
                {
                    "sent": "Doing some supervised learning by generating tasks.",
                    "label": 0
                },
                {
                    "sent": "Natural resources in communities.",
                    "label": 0
                },
                {
                    "sent": "Fantastic result.",
                    "label": 0
                },
                {
                    "sent": "I think, well yeah, maybe you can just show me later.",
                    "label": 0
                },
                {
                    "sent": "I might have seen it, yeah?",
                    "label": 0
                },
                {
                    "sent": "One question.",
                    "label": 0
                },
                {
                    "sent": "The intuitive question when you learning these auxiliary embeddings is the is the is your intuition back there.",
                    "label": 0
                },
                {
                    "sent": "They're helping the training too.",
                    "label": 0
                },
                {
                    "sent": "Avoid very bad local minima.",
                    "label": 0
                },
                {
                    "sent": "Or is the intuition more that there helping to prevent overfitting?",
                    "label": 0
                },
                {
                    "sent": "Or is there?",
                    "label": 0
                },
                {
                    "sent": "Are they trying to do both at the same time?",
                    "label": 1
                },
                {
                    "sent": "It's something that is a little different.",
                    "label": 0
                },
                {
                    "sent": "Overfitting is you're finding too good in minimum and almost want to be trapped in the local movements in case still perfect so.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "In my opinion, it's just that if you don't put that regularizer you get like huge overfitting.",
                    "label": 0
                },
                {
                    "sent": "So it's just a way in my opinion of somehow constraining that solution so it's.",
                    "label": 0
                },
                {
                    "sent": "It's more like keeping the parameters in the sensible range, I think.",
                    "label": 0
                },
                {
                    "sent": "OK, good luck.",
                    "label": 0
                }
            ]
        }
    }
}