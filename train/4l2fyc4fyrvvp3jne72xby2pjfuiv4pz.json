{
    "id": "4l2fyc4fyrvvp3jne72xby2pjfuiv4pz",
    "title": "KnowMore \u2013 Knowledge Base Augmentation with Structured Web Markup",
    "info": {
        "author": [
            "Ran Yu, GESIS - Leibniz Institute for the Social Sciences"
        ],
        "published": "Dec. 10, 2019",
        "recorded": "October 2019",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2019_yu_knowmore/",
    "segmentation": [
        [
            "So the talk would including four parts.",
            "I will explain briefly, explain the motivation of the work."
        ],
        [
            "Summarize of relative work and introduce the data source we use and our approach."
        ],
        [
            "So start from the knowledge base, the knowledge base in this talk refers to the RDF datasets.",
            "For instance, DB pedia as the most frequently used data set for by researchers.",
            "There are instances of real world entities in the database and the instances are identified by the UI as a subject of the triple and then there are properties and values saved as RDF triples to describe the entity."
        ],
        [
            "There are a lot of applications using knowledge base as background data set.",
            "For instance, there are semantic search, Google Answers and entity centric queries using knowledge graph in the back back end.",
            "For instance, if I search for Forrest Gump, they.",
            "A summary of the entity will be showed on the search page with a rich representation of the search result and there are product search and product recommendations and also question answering systems using knowledge graph and smarter systems such as Amazon.",
            "Alexa is also using knowledge Graph to answer to have conversation with users."
        ],
        [
            "The current state of the Knowledge Graph is that there are a lot of open, accessible datasets, for instance, as reported by linked open data.",
            "There are more than 1200 headsets and more than 100 billion statements by March 2019, and there are even more.",
            "There are also more knowledge graphs preserved by different companies that are accessible with the through the applications provided by the companies such as Google Knowledge Graph.",
            "So in summary, the Knowledge Graph has attract a lot of attention and it it's at fast growing scale and it covers diverse domains however."
        ],
        [
            "It still suffers from the challenge that the knowledge bases are very incomplete at this moment.",
            "In particular, there's a large percentage of less popular, also also called as long tail entities and properties that are under represented.",
            "So on the slides showing 2 examples, the one on the left side is the statistics about the entities of type person in Freebase.",
            "This is from a paper published in 2014.",
            "As you can see that among all day person entities there are 68% of the persons do not have a.",
            "Value of our profession and even for the properties such as place of birth which everyone supposed to have.",
            "There are 71% of entities.",
            "Doesn't have this problem.",
            "Doesn't have value for this property, even for the most frequent and annotates the top 100,000 frequent entities.",
            "Personalities certain percent of persons are missing place of birth.",
            "On the right side is a snippet taken from our experiment that has said where we randomly selected certain movies and circuit books from Wikipedia and then we get statistics about the top 15 popular properties.",
            "See how many of these entities have values for these 15 properties in DB pedia, Freebase and Vicki data the space you see between the top of the bar and 100% are the.",
            "Missing at the percentage of entities missing the respective property.",
            "And most of these are the gaps that we want to fill in with our approach.",
            "But one thing to note is that not every entity is supposed to have all these 15 properties populated.",
            "For instance, for the movie entities, there's a property we look at is a word which suppose which is not supposed to be 100% for every entity."
        ],
        [
            "But the good thing is that the.",
            "Link data are the the knowledge base is still just a very small fraction of the web and there are a lot of knowledge buried in the unstructured or semi structured web data."
        ],
        [
            "There are a lot of."
        ],
        [
            "Works has developed approaches for enrich or augment knowledge base using web data.",
            "For instance, there's a line of work using internal data from the knowledge basis, which is mainly influence on existing knowledge for new relations or new types.",
            "And there are a lot of work using external data from the web.",
            "And for instance, there are work using unstructured web documents, like the article Free tax article in Wikipedia or news articles or General Web page and product descriptions from Amazon for populating the knowledge base and our work using semistructured about data for instance using web tables or web markup as what we do in our work."
        ],
        [
            "Web markup are the structure entertain information embedded into web pages using standard such as microformats, RDF, A, or macro data.",
            "Right side of the of the slide is example of IMDb page of the movie Forrest Gump.",
            "If you look at the source code of the page you can see that there are some entity information embedded in the webpage using macro data standard.",
            "And for instance, you can see from the source code that Tom Hanks is value of the property name, which is.",
            "Fact about a person about entity of type person and this person is the actor of the movie Forrest Gump.",
            "So Web data Commons data set is Adair said that extracted from the web markup of web pages in the common crowd and for the update comes data set extracted from 2018 November version of of common crop.",
            "They found that 77 more than 37% of web pages have this kind of web markup which covers more than 29% of pay level domains.",
            "And constitute of more than 31 billions of RDF quotes, so."
        ],
        [
            "This medical Commons data set is of very large scale.",
            "It contains up-to-date information.",
            "It has information for long tail entities and properties, and it already used some kind of ontologies which are most the largest, most frequently usedontologyschema.org, and it has semantic annotations and relations and easier to pass the unstructured web content.",
            "However, there are a lot of disadvantage of this data set as well.",
            "For instance, there are a lot of common errors like wrong namespace or the website manager might use undefined types and properties just because this will color makes sense, but didn't really look up in the ontology definitions and.",
            "Most of the entity.",
            "Most of the instances things are unlinked.",
            "For instance the.",
            "Entity descriptions for the same entity are isolated with each other.",
            "For instance, in the Commons data set, there are more than 10,000 instances, for if you search for iPhone 6, but all these nodes are isolated, you don't know that there are reference entities and this also resulting in very high redundancy of the data.",
            "And there are a lot of semantically misused vocabularies."
        ],
        [
            "So it come too."
        ],
        [
            "Our approach of resolving these challenges we developed developed approach with three steps.",
            "We were first we clean the data to fix the issues such as common errors and then we run entity matching step to find the references for the entity that we want to augment and in the end we fuse all this matched endocr Efron sanity descriptiones to find the gnu and correct fax for this entity that we want to augment."
        ],
        [
            "So the first step into matching we use beyond 25 as a blocking step to run a fuzzy match for the name of the entity Descripcion in WDC data set and the query entity.",
            "And then we run a pairwise comparison which is supervised classification approach.",
            "We consider each property the properties as and as feature space requires that we think different properties have contribute different weights when you.",
            "Decide if there's 20 to descriptions are the same for the same entity or not.",
            "For instance, the author have higher, might have higher rate than publisher.",
            "When you decide if two books are the same book or not.",
            "And with this similarity vector, we run supervised classification to classify whether or not this entities are referenced."
        ],
        [
            "And after we get all the matched entity descriptiones we extract features from both from different dimensions.",
            "For instance, we consider the similarity.",
            "Now we consider the quality of the source and the other dimensions to extract features and apply another supervised classification to classify defects from all these matched and the descriptions to decide whether or not this is a correct factor.",
            "It's a false fact.",
            "At this step will remove all the wrong facts.",
            "For instance, Tom Cruise is not is not the actor of Forrest Gump and then we remove this fact from the pool and the next step we ensure the novelty of the fact that we found, which basically means the fact we extracted so should not be redundant among themselves and also not redundant with respect to the corresponding knowledge base.",
            "We want to augment."
        ],
        [
            "For this approach we have to run experiment on using Web data Commons 14 data set.",
            "We use we create ground truth on the 30 movie and books randomly selected from Wikipedia.",
            "And we augment DB pedia Freebase and Vicky data.",
            "The properties are as shown to 15 top populated properties and we manually mapped the different ontologies across copies to schema.org."
        ],
        [
            "Vocabularies so I briefly show some of the evaluation result.",
            "We use prison record F1 to evaluate whether or not the fact we selected are correct.",
            "So this is a evaluation of the correctness step and we see that F1 measure.",
            "Outperforms our baselines, which we have one baseline which focus on the precision and quality of the facts, and the source of the facts.",
            "And another approach.",
            "Focus on the novelty of the effects so our approach outperforms both baselines, and with especially the recall is much higher than the other approaches."
        ],
        [
            "We also evaluate the novelty of the fact that we recalled from the data set.",
            "And the novelty has we have over 90% novelty for the fact that we extracted for augmenting the KP.",
            "And we also able to recall over 90% of no effects from the matching from all the matched entity descriptions.",
            "And there's a slight tradeoff between art and recall.",
            "If we set a threshold into different values.",
            "But this treshold is 4.",
            "Far filter filtering out the duplicate fact based on the similarity metric, which does not influence the values of numbers values has type of numbers or data and.",
            "The date and time.",
            "That's why it's not has a.",
            "It doesn't have a very strong influence on the result."
        ],
        [
            "And we can always notice that we can get for an average of 30% of movies without drawing has been augmented with at least one jar jar's and there are four other properties.",
            "We also have increased coverage in the knowledge base after augmenting using web markup data.",
            "So there are more evaluation results in the paper and."
        ],
        [
            "We also evaluate the potential of this approach and we we notice that we can find a lot of new facts which properties has already been used by DB pedia.",
            "And we can also find new facts of longtail properties that are not used by DPR yet."
        ],
        [
            "And here is a brief example of a book that we try to populate and we see that we can recall a lot of new facts.",
            "From which hours do not exist in DB pedia?"
        ],
        [
            "And yeah, briefly, the conclusion is that we could find facts for augment machine readable knowledge basis from the web, and bad for some fact we need to consider to be more considerate on how to add it to the knowledge basis.",
            "For instance, if we want to add the different add information about different version of books in the same page, or we want to consider as different entities, or some facts are only temporarily correct if we want to.",
            "I haven't hit the database or not.",
            "Thanks."
        ],
        [
            "Are there any questions?",
            "There's just a microphone.",
            "OK, I guess you can't.",
            "Today there is.",
            "It's authentication people put false information on the web in text and I could imagine that if this technique is used a lot, the next step will do is try to embed semantic markup in there for the false information which could then provide a risk for this technique.",
            "Yeah, that's a good point.",
            "It's actually true that there are already a lot of false information in the Mac app data set, but we try to add this kind of quality control into the features to Yonder to filter out the facts from the not so authorized resource.",
            "But yeah, there are still some false information in the result.",
            "Yeah, and we need to continue working on that.",
            "Are there any other questions?",
            "So I have a question about and.",
            "Do you do any analysis of which facts you want to find out?",
            "So you are saying that there are some attributes that you wouldn't expect everything to have, and then there's presumably some attributes that are less interesting or less central to what, what, what, what it's about.",
            "Do you?",
            "Do you do any analysis of what the which ones you should be prioritizing, filling in which?",
            "Which facts are them at the important missing ones or not?",
            "Yeah, I guess which properties are?",
            "Facts are important?",
            "Would more or less depends on the applications.",
            "In our case we developed the approach for general purpose, so we assume that we could find the correct and the novel effects and effects.",
            "It's important for specific user.",
            "It would be more depend on the.",
            "A specific application that this knowledge base aim to be used for.",
            "Things have been interesting problem if you look at more specific domains.",
            "Yeah, absolutely.",
            "Yeah, that's another question.",
            "OK, let's thank you speaker again, thank you, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the talk would including four parts.",
                    "label": 0
                },
                {
                    "sent": "I will explain briefly, explain the motivation of the work.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Summarize of relative work and introduce the data source we use and our approach.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So start from the knowledge base, the knowledge base in this talk refers to the RDF datasets.",
                    "label": 1
                },
                {
                    "sent": "For instance, DB pedia as the most frequently used data set for by researchers.",
                    "label": 0
                },
                {
                    "sent": "There are instances of real world entities in the database and the instances are identified by the UI as a subject of the triple and then there are properties and values saved as RDF triples to describe the entity.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are a lot of applications using knowledge base as background data set.",
                    "label": 0
                },
                {
                    "sent": "For instance, there are semantic search, Google Answers and entity centric queries using knowledge graph in the back back end.",
                    "label": 0
                },
                {
                    "sent": "For instance, if I search for Forrest Gump, they.",
                    "label": 0
                },
                {
                    "sent": "A summary of the entity will be showed on the search page with a rich representation of the search result and there are product search and product recommendations and also question answering systems using knowledge graph and smarter systems such as Amazon.",
                    "label": 1
                },
                {
                    "sent": "Alexa is also using knowledge Graph to answer to have conversation with users.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The current state of the Knowledge Graph is that there are a lot of open, accessible datasets, for instance, as reported by linked open data.",
                    "label": 0
                },
                {
                    "sent": "There are more than 1200 headsets and more than 100 billion statements by March 2019, and there are even more.",
                    "label": 1
                },
                {
                    "sent": "There are also more knowledge graphs preserved by different companies that are accessible with the through the applications provided by the companies such as Google Knowledge Graph.",
                    "label": 0
                },
                {
                    "sent": "So in summary, the Knowledge Graph has attract a lot of attention and it it's at fast growing scale and it covers diverse domains however.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It still suffers from the challenge that the knowledge bases are very incomplete at this moment.",
                    "label": 0
                },
                {
                    "sent": "In particular, there's a large percentage of less popular, also also called as long tail entities and properties that are under represented.",
                    "label": 1
                },
                {
                    "sent": "So on the slides showing 2 examples, the one on the left side is the statistics about the entities of type person in Freebase.",
                    "label": 0
                },
                {
                    "sent": "This is from a paper published in 2014.",
                    "label": 0
                },
                {
                    "sent": "As you can see that among all day person entities there are 68% of the persons do not have a.",
                    "label": 0
                },
                {
                    "sent": "Value of our profession and even for the properties such as place of birth which everyone supposed to have.",
                    "label": 0
                },
                {
                    "sent": "There are 71% of entities.",
                    "label": 0
                },
                {
                    "sent": "Doesn't have this problem.",
                    "label": 0
                },
                {
                    "sent": "Doesn't have value for this property, even for the most frequent and annotates the top 100,000 frequent entities.",
                    "label": 0
                },
                {
                    "sent": "Personalities certain percent of persons are missing place of birth.",
                    "label": 0
                },
                {
                    "sent": "On the right side is a snippet taken from our experiment that has said where we randomly selected certain movies and circuit books from Wikipedia and then we get statistics about the top 15 popular properties.",
                    "label": 0
                },
                {
                    "sent": "See how many of these entities have values for these 15 properties in DB pedia, Freebase and Vicki data the space you see between the top of the bar and 100% are the.",
                    "label": 0
                },
                {
                    "sent": "Missing at the percentage of entities missing the respective property.",
                    "label": 0
                },
                {
                    "sent": "And most of these are the gaps that we want to fill in with our approach.",
                    "label": 0
                },
                {
                    "sent": "But one thing to note is that not every entity is supposed to have all these 15 properties populated.",
                    "label": 0
                },
                {
                    "sent": "For instance, for the movie entities, there's a property we look at is a word which suppose which is not supposed to be 100% for every entity.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But the good thing is that the.",
                    "label": 0
                },
                {
                    "sent": "Link data are the the knowledge base is still just a very small fraction of the web and there are a lot of knowledge buried in the unstructured or semi structured web data.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There are a lot of.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Works has developed approaches for enrich or augment knowledge base using web data.",
                    "label": 1
                },
                {
                    "sent": "For instance, there's a line of work using internal data from the knowledge basis, which is mainly influence on existing knowledge for new relations or new types.",
                    "label": 1
                },
                {
                    "sent": "And there are a lot of work using external data from the web.",
                    "label": 0
                },
                {
                    "sent": "And for instance, there are work using unstructured web documents, like the article Free tax article in Wikipedia or news articles or General Web page and product descriptions from Amazon for populating the knowledge base and our work using semistructured about data for instance using web tables or web markup as what we do in our work.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Web markup are the structure entertain information embedded into web pages using standard such as microformats, RDF, A, or macro data.",
                    "label": 1
                },
                {
                    "sent": "Right side of the of the slide is example of IMDb page of the movie Forrest Gump.",
                    "label": 0
                },
                {
                    "sent": "If you look at the source code of the page you can see that there are some entity information embedded in the webpage using macro data standard.",
                    "label": 0
                },
                {
                    "sent": "And for instance, you can see from the source code that Tom Hanks is value of the property name, which is.",
                    "label": 0
                },
                {
                    "sent": "Fact about a person about entity of type person and this person is the actor of the movie Forrest Gump.",
                    "label": 0
                },
                {
                    "sent": "So Web data Commons data set is Adair said that extracted from the web markup of web pages in the common crowd and for the update comes data set extracted from 2018 November version of of common crop.",
                    "label": 1
                },
                {
                    "sent": "They found that 77 more than 37% of web pages have this kind of web markup which covers more than 29% of pay level domains.",
                    "label": 0
                },
                {
                    "sent": "And constitute of more than 31 billions of RDF quotes, so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This medical Commons data set is of very large scale.",
                    "label": 1
                },
                {
                    "sent": "It contains up-to-date information.",
                    "label": 0
                },
                {
                    "sent": "It has information for long tail entities and properties, and it already used some kind of ontologies which are most the largest, most frequently usedontologyschema.org, and it has semantic annotations and relations and easier to pass the unstructured web content.",
                    "label": 0
                },
                {
                    "sent": "However, there are a lot of disadvantage of this data set as well.",
                    "label": 0
                },
                {
                    "sent": "For instance, there are a lot of common errors like wrong namespace or the website manager might use undefined types and properties just because this will color makes sense, but didn't really look up in the ontology definitions and.",
                    "label": 1
                },
                {
                    "sent": "Most of the entity.",
                    "label": 0
                },
                {
                    "sent": "Most of the instances things are unlinked.",
                    "label": 0
                },
                {
                    "sent": "For instance the.",
                    "label": 0
                },
                {
                    "sent": "Entity descriptions for the same entity are isolated with each other.",
                    "label": 0
                },
                {
                    "sent": "For instance, in the Commons data set, there are more than 10,000 instances, for if you search for iPhone 6, but all these nodes are isolated, you don't know that there are reference entities and this also resulting in very high redundancy of the data.",
                    "label": 1
                },
                {
                    "sent": "And there are a lot of semantically misused vocabularies.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it come too.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our approach of resolving these challenges we developed developed approach with three steps.",
                    "label": 0
                },
                {
                    "sent": "We were first we clean the data to fix the issues such as common errors and then we run entity matching step to find the references for the entity that we want to augment and in the end we fuse all this matched endocr Efron sanity descriptiones to find the gnu and correct fax for this entity that we want to augment.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first step into matching we use beyond 25 as a blocking step to run a fuzzy match for the name of the entity Descripcion in WDC data set and the query entity.",
                    "label": 0
                },
                {
                    "sent": "And then we run a pairwise comparison which is supervised classification approach.",
                    "label": 0
                },
                {
                    "sent": "We consider each property the properties as and as feature space requires that we think different properties have contribute different weights when you.",
                    "label": 1
                },
                {
                    "sent": "Decide if there's 20 to descriptions are the same for the same entity or not.",
                    "label": 0
                },
                {
                    "sent": "For instance, the author have higher, might have higher rate than publisher.",
                    "label": 0
                },
                {
                    "sent": "When you decide if two books are the same book or not.",
                    "label": 0
                },
                {
                    "sent": "And with this similarity vector, we run supervised classification to classify whether or not this entities are referenced.",
                    "label": 1
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And after we get all the matched entity descriptiones we extract features from both from different dimensions.",
                    "label": 0
                },
                {
                    "sent": "For instance, we consider the similarity.",
                    "label": 0
                },
                {
                    "sent": "Now we consider the quality of the source and the other dimensions to extract features and apply another supervised classification to classify defects from all these matched and the descriptions to decide whether or not this is a correct factor.",
                    "label": 1
                },
                {
                    "sent": "It's a false fact.",
                    "label": 0
                },
                {
                    "sent": "At this step will remove all the wrong facts.",
                    "label": 0
                },
                {
                    "sent": "For instance, Tom Cruise is not is not the actor of Forrest Gump and then we remove this fact from the pool and the next step we ensure the novelty of the fact that we found, which basically means the fact we extracted so should not be redundant among themselves and also not redundant with respect to the corresponding knowledge base.",
                    "label": 0
                },
                {
                    "sent": "We want to augment.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For this approach we have to run experiment on using Web data Commons 14 data set.",
                    "label": 0
                },
                {
                    "sent": "We use we create ground truth on the 30 movie and books randomly selected from Wikipedia.",
                    "label": 1
                },
                {
                    "sent": "And we augment DB pedia Freebase and Vicky data.",
                    "label": 0
                },
                {
                    "sent": "The properties are as shown to 15 top populated properties and we manually mapped the different ontologies across copies to schema.org.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Vocabularies so I briefly show some of the evaluation result.",
                    "label": 0
                },
                {
                    "sent": "We use prison record F1 to evaluate whether or not the fact we selected are correct.",
                    "label": 0
                },
                {
                    "sent": "So this is a evaluation of the correctness step and we see that F1 measure.",
                    "label": 0
                },
                {
                    "sent": "Outperforms our baselines, which we have one baseline which focus on the precision and quality of the facts, and the source of the facts.",
                    "label": 1
                },
                {
                    "sent": "And another approach.",
                    "label": 0
                },
                {
                    "sent": "Focus on the novelty of the effects so our approach outperforms both baselines, and with especially the recall is much higher than the other approaches.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also evaluate the novelty of the fact that we recalled from the data set.",
                    "label": 0
                },
                {
                    "sent": "And the novelty has we have over 90% novelty for the fact that we extracted for augmenting the KP.",
                    "label": 0
                },
                {
                    "sent": "And we also able to recall over 90% of no effects from the matching from all the matched entity descriptions.",
                    "label": 1
                },
                {
                    "sent": "And there's a slight tradeoff between art and recall.",
                    "label": 0
                },
                {
                    "sent": "If we set a threshold into different values.",
                    "label": 0
                },
                {
                    "sent": "But this treshold is 4.",
                    "label": 0
                },
                {
                    "sent": "Far filter filtering out the duplicate fact based on the similarity metric, which does not influence the values of numbers values has type of numbers or data and.",
                    "label": 0
                },
                {
                    "sent": "The date and time.",
                    "label": 1
                },
                {
                    "sent": "That's why it's not has a.",
                    "label": 0
                },
                {
                    "sent": "It doesn't have a very strong influence on the result.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And we can always notice that we can get for an average of 30% of movies without drawing has been augmented with at least one jar jar's and there are four other properties.",
                    "label": 1
                },
                {
                    "sent": "We also have increased coverage in the knowledge base after augmenting using web markup data.",
                    "label": 1
                },
                {
                    "sent": "So there are more evaluation results in the paper and.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We also evaluate the potential of this approach and we we notice that we can find a lot of new facts which properties has already been used by DB pedia.",
                    "label": 0
                },
                {
                    "sent": "And we can also find new facts of longtail properties that are not used by DPR yet.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here is a brief example of a book that we try to populate and we see that we can recall a lot of new facts.",
                    "label": 0
                },
                {
                    "sent": "From which hours do not exist in DB pedia?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And yeah, briefly, the conclusion is that we could find facts for augment machine readable knowledge basis from the web, and bad for some fact we need to consider to be more considerate on how to add it to the knowledge basis.",
                    "label": 1
                },
                {
                    "sent": "For instance, if we want to add the different add information about different version of books in the same page, or we want to consider as different entities, or some facts are only temporarily correct if we want to.",
                    "label": 0
                },
                {
                    "sent": "I haven't hit the database or not.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Are there any questions?",
                    "label": 0
                },
                {
                    "sent": "There's just a microphone.",
                    "label": 0
                },
                {
                    "sent": "OK, I guess you can't.",
                    "label": 0
                },
                {
                    "sent": "Today there is.",
                    "label": 0
                },
                {
                    "sent": "It's authentication people put false information on the web in text and I could imagine that if this technique is used a lot, the next step will do is try to embed semantic markup in there for the false information which could then provide a risk for this technique.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a good point.",
                    "label": 0
                },
                {
                    "sent": "It's actually true that there are already a lot of false information in the Mac app data set, but we try to add this kind of quality control into the features to Yonder to filter out the facts from the not so authorized resource.",
                    "label": 0
                },
                {
                    "sent": "But yeah, there are still some false information in the result.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and we need to continue working on that.",
                    "label": 0
                },
                {
                    "sent": "Are there any other questions?",
                    "label": 0
                },
                {
                    "sent": "So I have a question about and.",
                    "label": 0
                },
                {
                    "sent": "Do you do any analysis of which facts you want to find out?",
                    "label": 0
                },
                {
                    "sent": "So you are saying that there are some attributes that you wouldn't expect everything to have, and then there's presumably some attributes that are less interesting or less central to what, what, what, what it's about.",
                    "label": 0
                },
                {
                    "sent": "Do you?",
                    "label": 0
                },
                {
                    "sent": "Do you do any analysis of what the which ones you should be prioritizing, filling in which?",
                    "label": 0
                },
                {
                    "sent": "Which facts are them at the important missing ones or not?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I guess which properties are?",
                    "label": 0
                },
                {
                    "sent": "Facts are important?",
                    "label": 0
                },
                {
                    "sent": "Would more or less depends on the applications.",
                    "label": 0
                },
                {
                    "sent": "In our case we developed the approach for general purpose, so we assume that we could find the correct and the novel effects and effects.",
                    "label": 0
                },
                {
                    "sent": "It's important for specific user.",
                    "label": 0
                },
                {
                    "sent": "It would be more depend on the.",
                    "label": 0
                },
                {
                    "sent": "A specific application that this knowledge base aim to be used for.",
                    "label": 0
                },
                {
                    "sent": "Things have been interesting problem if you look at more specific domains.",
                    "label": 0
                },
                {
                    "sent": "Yeah, absolutely.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's another question.",
                    "label": 0
                },
                {
                    "sent": "OK, let's thank you speaker again, thank you, thank you.",
                    "label": 1
                }
            ]
        }
    }
}