{
    "id": "bzo35tebeahybtdrqd4ve4q5ap3b4o4n",
    "title": "A Unifying Perspective of Parametric Policy Search Methods for Markov Decision Processes",
    "info": {
        "author": [
            "Thomas Furmston, Department of Computer Science, University College London"
        ],
        "published": "Jan. 16, 2013",
        "recorded": "December 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Markov Processes",
            "Top->Computer Science->Machine Learning->Reinforcement Learning"
        ]
    },
    "url": "http://videolectures.net/nips2012_furmston_processes/",
    "segmentation": [
        [
            "OK, so this is work with David Barber at University College."
        ],
        [
            "London."
        ],
        [
            "So this this paper is on Markov decision processes, so these are general models for optimal control and so we have two examples here of possible Markov decision processes.",
            "So the one left is a game of Tetris.",
            "As soon as the game from the 80s and the name of the game is to complete, as many lines as possible before the board fills up in the game finishes.",
            "Another possible example is on the right, which is an envelope you later.",
            "So this is a model for a robot arm, and so the control problem in this case would be to configure that are in a certain position while using a minimal cost."
        ],
        [
            "OK, so I'm not going to introduce MDP's fully, I just want to introduce enough notation to give the flavor of the paper.",
            "So at any given time point, we're in a state S, we should notify Boulder, S and state the space of possible states.",
            "State spaces denoted by curly S given a state.",
            "We don't select the action for the action space, and this is denoted by A and the action spaces Curly A and then I'm interested in parametric methods.",
            "So I introduce a parameter space curly W and then the parameter is just W. Alright, so at any given time point, you're in a given state of the environment and you want to select a possible action.",
            "So the way this is done in Markov decision processes is through the policy, which is denoted by \u03c0.",
            "And this is a conditional distribution over the action space where you condition on the state, and because I'm considering parametric methods, I consider parametric policies parameterized by W. And can understand objective is to optimize the total expected reward.",
            "This is denoted by UFW and.",
            "This is given by the expectation of the reward function.",
            "So for example, in Tetris this might be a one if you complete a line and a 0 otherwise.",
            "So is your expectation of reward function with respect to P of gamma and soupir?",
            "Gamma can be thought of as where I'm going to be over the course of the planning horizon, so it's actually the summation of where you are over the course of planning horizon.",
            "And then the last time I want to introduce is what I called the state action value function and this basically corresponds to your mouth reward.",
            "You're going to receive given that your current state action pair is A&S.",
            "So.",
            "Intuitively, the syncopate gamma is where I'm going to be an Q as the amount of reward I'm going to receive."
        ],
        [
            "OK so MVP's have been around since the 50s.",
            "This various methods to solve them optimally, but generally they can't be extended to real world problems that people want to solve.",
            "You typically can't even evaluate the objective function, so trying to find a global optimum is just completely unrealistic.",
            "So instead what people do is."
        ],
        [
            "Focus on approximate solution methods.",
            "So what I mean by this is."
        ],
        [
            "Approximate dynamic programming."
        ],
        [
            "Linear programming."
        ],
        [
            "Research methods and what I'm going to focus on."
        ],
        [
            "They is parametric policy search methods, So what I mean by that is gradient based methods and bound optimization techniques such as expectation maximization."
        ],
        [
            "It's important before we go on to realize that the sort of problems that we're going to be considering.",
            "Evaluate the objective exactly.",
            "You typically can't evaluate the gradient exactly either, So what people do is generally have stochastic based estimates of the gradient and also the objective is non concave, so it's not very nice.",
            "Quite unpleasant stochastic optimization problem and that limits as to the scope of different optimization techniques that we can use.",
            "So the most."
        ],
        [
            "6 one and this has been around since the 80s is the steepest gradient descent?",
            "So in this case it's normal we just update the parameter by adding a scaled version of the gradient of the objective.",
            "And then the MDP framework we have that the gradient is this function here, so it's the expectation of the derivatives of the log policy ready expectation is taken with respect to P, gamma, Q.",
            "So basically this says that you wait the probability of being in the state action pair and you waited by the reward that you're going to receive and then you take the expectation over all those pairs and that will give you a gradient."
        ],
        [
            "OK, so that's very nice, but obviously steepest gradient sent suffers from numerous problems that make it an attractive in practice, so it has poor convergence, susceptible to scale up or scale."
        ],
        [
            "Ejective.",
            "So one of the most popular methods is natural gradient descent, and this was introduced in 98 by Marvin in the neural network literature and then extended to MVP's by Cardi in 2002.",
            "And so it's the same idea.",
            "But now we.",
            "We preconditioned gradient with the inverse of the Fisher information matrix.",
            "Aware deficient information matrix is just given by the expectation of the Hessian of the log policy, where the expectation is taken with respect to P. Of gamma just takes the expectation with respect to where we are.",
            "There's no knowledge of the reward function in this expectation."
        ],
        [
            "And then finally the third technique that's popular in the MDP literature is expectation maximization, so this is bound optimization technique from the 70s.",
            "It was done extended to the MDP framework in 97 by Peter Diane and Geoffrey Hinton and is since been extended by other people.",
            "So in this case we have that the parameter parameter update is given by.",
            "The argmax of this term on the right here and so the important point to notice is that.",
            "There's a decoupling in the parameters between the function that the expectation is being taken with respect to, and the function that taking the expectation and this decoupling allows you to take the optimization over W in some interesting cases."
        ],
        [
            "OK, so now is probably a good point.",
            "Actually highlight the contributions of the paper.",
            "So various people use these methods.",
            "But there's not really sort of understanding of which is well, which is best or how they're related to each other.",
            "So one of the aims of the papers to try and clarify that point somewhat, so we're going to do some analysis."
        ],
        [
            "The step direction that these methods take in parameter space.",
            "And then motivated by that now."
        ],
        [
            "This we're going to provide two novel approximate Newton methods for optimizing DPS.",
            "So sort of a motivation for this would be to compare it to the Gauss Newton method for nonlinear least squares, so we just keep some of the terms of the Hessian that have nice properties.",
            "Refer the rest away and we just use that as a preconditioner."
        ],
        [
            "OK, so as part of the analysis we're going to compare iemon natural gradient descent to the Newton method."
        ],
        [
            "So in the MDP, setting the Hessian takes this particular form, so it's a sum of two matrices H1 and H2.",
            "I can talk about H1 later if I have time, but the term I want to focus on is H2 and so if this is given at the bottom and it's very similar to the gradient, so we're taking the expectation with respect to P, gamma, Q.",
            "But now we're just taking the expectation of the log, the Hessian of the lock policy instead of drift availability, so that's the difference.",
            "OK, so how is natural gradient descent relate?"
        ],
        [
            "To the Newton method.",
            "So this is something that Cardi actually tried to do in his paper, but he didn't come up with any sort of satisfactory answers, but it is actually very easy if we just compare the Fisher information matrix G with the approximate approximate Hessian H2.",
            "You can see that the only difference between these two matrices you can ignore the - that's cancels out later on, and so the only difference is the fact that H2 takes into account the reward structure of the MDP.",
            "While in the Fisher Information matrix, that information is ignored.",
            "So basically you can just see that H2 has more information about the curvature of the objective function then Fisher information matrix."
        ],
        [
            "And then how is expectation maximization related to the Newton method?",
            "Well, this is a result we show in the paper.",
            "There's some technical details of admitted, but basically what you can show is that the M step, which is W, W -- W is up to 1st order equal to an approximate Newton step where I use H2 instead of H and I use a fixed step size of 1.",
            "And then there's a caveat on this.",
            "That is when the log policy is quadratic in the policy parameters, then this relations exact and what I mean by that is that the second term there is equal to 0, and so in this case EM is actually equivalent to an approximate in step with a fixed step size of 1."
        ],
        [
            "That's a quick summary of the analysis.",
            "What you can see is that both methods are actually closely related to a particular term in the Hessian, so one just ignores the reward waiting of H2 and uses a trained step size while the other up the 1st order uses H2 but has a fixed step size of 1.",
            "So the natural follow on from this."
        ],
        [
            "Is done.",
            "Just to consider actually using H2 in place of all these other methods, so this is the second part of the paper."
        ],
        [
            "So actually introduced two different methods, so there's a full approximate Newton method, and that's when you just preconditioned with negative inverse of H2.",
            "And then there's the diagonal approximate Newton method and then you preconditioned with negative invested D2 where D2 is just a diagonal of H2.",
            "OK, so we show that these methods actually have some nice properties that make them desirable for MDP optimization."
        ],
        [
            "So one of the most important ones is the.",
            "You can show that these matrices are negative definite under certain conditions.",
            "Ann this is required to ensure that you increase your objective function.",
            "So we call that UW is not concave to generate the Hessians not going negative definite and you could actually decrease the objective.",
            "So nice property of these approximate Newton methods is that under the condition that the policy is log concave and the policy parameters H2 and the two are both negative definite over the whole parameter space.",
            "So I'll give some examples.",
            "I don't have time to go through them now of some widely used policies that satisfy this condition, so it's actually quite reasonable condition and hold for some standard examples."
        ],
        [
            "Another property that we show is that these methods are invariant to have certain amount of variance to the basis of the parameter space.",
            "So steepest gradient descent is affected by the way you represent parameter space.",
            "So what this graph here shows is in blue two different trainings of steepest gradient descent.",
            "Wanna normal specular original space?",
            "The second in the transformed space that's done being transformed back and you can see that you actually get two different results, which is not very desirable obviously.",
            "So a nice property of the approximate user methods is that the full approximation method is a fine invariant.",
            "And the diagonal approximate method is scale invariant."
        ],
        [
            "For property, would talk about is the ease with which you can extend gradient evaluation techniques to these methods.",
            "So the main point is I want to make is that the gradient?",
            "You just take the expectation with respect to P, gamma, Q of the drift of the log policy and all these different sampling techniques do that to do the proximate methods I just concurrently take the expectation of the question below policy and that gives me my estimate for H2."
        ],
        [
            "OK, so we tried these methods on some various domains, so the first one we considered was Tetris domain."
        ],
        [
            "Considered on a full 20 by 10 board, so this is a standard set up for the game and after around 40 training iterations.",
            "So 40 parameter updates the full approximate user method gets a score of around 14,000 lines.",
            "And just put that into perspective are using the same features approximate dynamic programming?",
            "Which solution by bertsekas scores for a half thousand after around six iterations and then the solution deteriorates and then there's also an approximate linear programming solution with the same features and it gets around 4200."
        ],
        [
            "I also compared these policy search methods on that remain, so I have the full approximation method in red diagonal approximation method in blue natural in green and steepest and black.",
            "You can't do the maximization over W. At 4:00 AM, so I've admitted that one, but you get strong results.",
            "In this in this.",
            "Demaine the final."
        ],
        [
            "Well, next time."
        ],
        [
            "It was linear system, so I did this 'cause I can actually evaluate the search directions exactly.",
            "I don't have a huge amount of details unfortunately, but an important point I would like to say is that the Hessian actually scored around the same as natural gradient descent and the reason for that is that you have to correct the matrix because it's not negative definite and you lose a lot of information by doing that."
        ],
        [
            "And then the final experiment I considered going to a nonlinear navigation task."
        ],
        [
            "And the reason I consider this one, because in this case the policy is actually log quadratic and so in this case EM is actually equivalent to approximately 2 with a fixed step size of one, and so these results actually tell you a lot more now.",
            "So you can see that the blue curve is using.",
            "The Hessian, where you include information about the reward, but traffic step size of 1 natural gradient descent, you ignore the reward structure in the preconditioner, but you train the step size, but now we're taking the information for both of these.",
            "Are we precondition with award and then training step size and we get the red curve here, which is approximate Newton method.",
            "So it gives a nice interpretation."
        ],
        [
            "With these results."
        ],
        [
            "OK, just to summarize quickly, so I've given an analysis of current parametric policy search methods and then introduced to approximate using methods."
        ],
        [
            "And then there's some feature work by halftime."
        ],
        [
            "Thanks.",
            "Right, thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this is work with David Barber at University College.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "London.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this this paper is on Markov decision processes, so these are general models for optimal control and so we have two examples here of possible Markov decision processes.",
                    "label": 1
                },
                {
                    "sent": "So the one left is a game of Tetris.",
                    "label": 0
                },
                {
                    "sent": "As soon as the game from the 80s and the name of the game is to complete, as many lines as possible before the board fills up in the game finishes.",
                    "label": 0
                },
                {
                    "sent": "Another possible example is on the right, which is an envelope you later.",
                    "label": 0
                },
                {
                    "sent": "So this is a model for a robot arm, and so the control problem in this case would be to configure that are in a certain position while using a minimal cost.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so I'm not going to introduce MDP's fully, I just want to introduce enough notation to give the flavor of the paper.",
                    "label": 0
                },
                {
                    "sent": "So at any given time point, we're in a state S, we should notify Boulder, S and state the space of possible states.",
                    "label": 0
                },
                {
                    "sent": "State spaces denoted by curly S given a state.",
                    "label": 0
                },
                {
                    "sent": "We don't select the action for the action space, and this is denoted by A and the action spaces Curly A and then I'm interested in parametric methods.",
                    "label": 0
                },
                {
                    "sent": "So I introduce a parameter space curly W and then the parameter is just W. Alright, so at any given time point, you're in a given state of the environment and you want to select a possible action.",
                    "label": 0
                },
                {
                    "sent": "So the way this is done in Markov decision processes is through the policy, which is denoted by \u03c0.",
                    "label": 1
                },
                {
                    "sent": "And this is a conditional distribution over the action space where you condition on the state, and because I'm considering parametric methods, I consider parametric policies parameterized by W. And can understand objective is to optimize the total expected reward.",
                    "label": 1
                },
                {
                    "sent": "This is denoted by UFW and.",
                    "label": 0
                },
                {
                    "sent": "This is given by the expectation of the reward function.",
                    "label": 0
                },
                {
                    "sent": "So for example, in Tetris this might be a one if you complete a line and a 0 otherwise.",
                    "label": 0
                },
                {
                    "sent": "So is your expectation of reward function with respect to P of gamma and soupir?",
                    "label": 0
                },
                {
                    "sent": "Gamma can be thought of as where I'm going to be over the course of the planning horizon, so it's actually the summation of where you are over the course of planning horizon.",
                    "label": 0
                },
                {
                    "sent": "And then the last time I want to introduce is what I called the state action value function and this basically corresponds to your mouth reward.",
                    "label": 0
                },
                {
                    "sent": "You're going to receive given that your current state action pair is A&S.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Intuitively, the syncopate gamma is where I'm going to be an Q as the amount of reward I'm going to receive.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so MVP's have been around since the 50s.",
                    "label": 0
                },
                {
                    "sent": "This various methods to solve them optimally, but generally they can't be extended to real world problems that people want to solve.",
                    "label": 1
                },
                {
                    "sent": "You typically can't even evaluate the objective function, so trying to find a global optimum is just completely unrealistic.",
                    "label": 1
                },
                {
                    "sent": "So instead what people do is.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Focus on approximate solution methods.",
                    "label": 0
                },
                {
                    "sent": "So what I mean by this is.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Approximate dynamic programming.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Linear programming.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Research methods and what I'm going to focus on.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They is parametric policy search methods, So what I mean by that is gradient based methods and bound optimization techniques such as expectation maximization.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's important before we go on to realize that the sort of problems that we're going to be considering.",
                    "label": 0
                },
                {
                    "sent": "Evaluate the objective exactly.",
                    "label": 0
                },
                {
                    "sent": "You typically can't evaluate the gradient exactly either, So what people do is generally have stochastic based estimates of the gradient and also the objective is non concave, so it's not very nice.",
                    "label": 0
                },
                {
                    "sent": "Quite unpleasant stochastic optimization problem and that limits as to the scope of different optimization techniques that we can use.",
                    "label": 0
                },
                {
                    "sent": "So the most.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "6 one and this has been around since the 80s is the steepest gradient descent?",
                    "label": 1
                },
                {
                    "sent": "So in this case it's normal we just update the parameter by adding a scaled version of the gradient of the objective.",
                    "label": 0
                },
                {
                    "sent": "And then the MDP framework we have that the gradient is this function here, so it's the expectation of the derivatives of the log policy ready expectation is taken with respect to P, gamma, Q.",
                    "label": 0
                },
                {
                    "sent": "So basically this says that you wait the probability of being in the state action pair and you waited by the reward that you're going to receive and then you take the expectation over all those pairs and that will give you a gradient.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so that's very nice, but obviously steepest gradient sent suffers from numerous problems that make it an attractive in practice, so it has poor convergence, susceptible to scale up or scale.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Ejective.",
                    "label": 0
                },
                {
                    "sent": "So one of the most popular methods is natural gradient descent, and this was introduced in 98 by Marvin in the neural network literature and then extended to MVP's by Cardi in 2002.",
                    "label": 0
                },
                {
                    "sent": "And so it's the same idea.",
                    "label": 0
                },
                {
                    "sent": "But now we.",
                    "label": 0
                },
                {
                    "sent": "We preconditioned gradient with the inverse of the Fisher information matrix.",
                    "label": 1
                },
                {
                    "sent": "Aware deficient information matrix is just given by the expectation of the Hessian of the log policy, where the expectation is taken with respect to P. Of gamma just takes the expectation with respect to where we are.",
                    "label": 0
                },
                {
                    "sent": "There's no knowledge of the reward function in this expectation.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then finally the third technique that's popular in the MDP literature is expectation maximization, so this is bound optimization technique from the 70s.",
                    "label": 0
                },
                {
                    "sent": "It was done extended to the MDP framework in 97 by Peter Diane and Geoffrey Hinton and is since been extended by other people.",
                    "label": 0
                },
                {
                    "sent": "So in this case we have that the parameter parameter update is given by.",
                    "label": 1
                },
                {
                    "sent": "The argmax of this term on the right here and so the important point to notice is that.",
                    "label": 0
                },
                {
                    "sent": "There's a decoupling in the parameters between the function that the expectation is being taken with respect to, and the function that taking the expectation and this decoupling allows you to take the optimization over W in some interesting cases.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so now is probably a good point.",
                    "label": 0
                },
                {
                    "sent": "Actually highlight the contributions of the paper.",
                    "label": 0
                },
                {
                    "sent": "So various people use these methods.",
                    "label": 0
                },
                {
                    "sent": "But there's not really sort of understanding of which is well, which is best or how they're related to each other.",
                    "label": 0
                },
                {
                    "sent": "So one of the aims of the papers to try and clarify that point somewhat, so we're going to do some analysis.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The step direction that these methods take in parameter space.",
                    "label": 0
                },
                {
                    "sent": "And then motivated by that now.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This we're going to provide two novel approximate Newton methods for optimizing DPS.",
                    "label": 1
                },
                {
                    "sent": "So sort of a motivation for this would be to compare it to the Gauss Newton method for nonlinear least squares, so we just keep some of the terms of the Hessian that have nice properties.",
                    "label": 0
                },
                {
                    "sent": "Refer the rest away and we just use that as a preconditioner.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so as part of the analysis we're going to compare iemon natural gradient descent to the Newton method.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in the MDP, setting the Hessian takes this particular form, so it's a sum of two matrices H1 and H2.",
                    "label": 0
                },
                {
                    "sent": "I can talk about H1 later if I have time, but the term I want to focus on is H2 and so if this is given at the bottom and it's very similar to the gradient, so we're taking the expectation with respect to P, gamma, Q.",
                    "label": 0
                },
                {
                    "sent": "But now we're just taking the expectation of the log, the Hessian of the lock policy instead of drift availability, so that's the difference.",
                    "label": 0
                },
                {
                    "sent": "OK, so how is natural gradient descent relate?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To the Newton method.",
                    "label": 0
                },
                {
                    "sent": "So this is something that Cardi actually tried to do in his paper, but he didn't come up with any sort of satisfactory answers, but it is actually very easy if we just compare the Fisher information matrix G with the approximate approximate Hessian H2.",
                    "label": 0
                },
                {
                    "sent": "You can see that the only difference between these two matrices you can ignore the - that's cancels out later on, and so the only difference is the fact that H2 takes into account the reward structure of the MDP.",
                    "label": 0
                },
                {
                    "sent": "While in the Fisher Information matrix, that information is ignored.",
                    "label": 0
                },
                {
                    "sent": "So basically you can just see that H2 has more information about the curvature of the objective function then Fisher information matrix.",
                    "label": 1
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then how is expectation maximization related to the Newton method?",
                    "label": 1
                },
                {
                    "sent": "Well, this is a result we show in the paper.",
                    "label": 1
                },
                {
                    "sent": "There's some technical details of admitted, but basically what you can show is that the M step, which is W, W -- W is up to 1st order equal to an approximate Newton step where I use H2 instead of H and I use a fixed step size of 1.",
                    "label": 0
                },
                {
                    "sent": "And then there's a caveat on this.",
                    "label": 0
                },
                {
                    "sent": "That is when the log policy is quadratic in the policy parameters, then this relations exact and what I mean by that is that the second term there is equal to 0, and so in this case EM is actually equivalent to an approximate in step with a fixed step size of 1.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's a quick summary of the analysis.",
                    "label": 0
                },
                {
                    "sent": "What you can see is that both methods are actually closely related to a particular term in the Hessian, so one just ignores the reward waiting of H2 and uses a trained step size while the other up the 1st order uses H2 but has a fixed step size of 1.",
                    "label": 0
                },
                {
                    "sent": "So the natural follow on from this.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is done.",
                    "label": 0
                },
                {
                    "sent": "Just to consider actually using H2 in place of all these other methods, so this is the second part of the paper.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So actually introduced two different methods, so there's a full approximate Newton method, and that's when you just preconditioned with negative inverse of H2.",
                    "label": 1
                },
                {
                    "sent": "And then there's the diagonal approximate Newton method and then you preconditioned with negative invested D2 where D2 is just a diagonal of H2.",
                    "label": 0
                },
                {
                    "sent": "OK, so we show that these methods actually have some nice properties that make them desirable for MDP optimization.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one of the most important ones is the.",
                    "label": 0
                },
                {
                    "sent": "You can show that these matrices are negative definite under certain conditions.",
                    "label": 1
                },
                {
                    "sent": "Ann this is required to ensure that you increase your objective function.",
                    "label": 1
                },
                {
                    "sent": "So we call that UW is not concave to generate the Hessians not going negative definite and you could actually decrease the objective.",
                    "label": 0
                },
                {
                    "sent": "So nice property of these approximate Newton methods is that under the condition that the policy is log concave and the policy parameters H2 and the two are both negative definite over the whole parameter space.",
                    "label": 0
                },
                {
                    "sent": "So I'll give some examples.",
                    "label": 0
                },
                {
                    "sent": "I don't have time to go through them now of some widely used policies that satisfy this condition, so it's actually quite reasonable condition and hold for some standard examples.",
                    "label": 1
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another property that we show is that these methods are invariant to have certain amount of variance to the basis of the parameter space.",
                    "label": 0
                },
                {
                    "sent": "So steepest gradient descent is affected by the way you represent parameter space.",
                    "label": 0
                },
                {
                    "sent": "So what this graph here shows is in blue two different trainings of steepest gradient descent.",
                    "label": 0
                },
                {
                    "sent": "Wanna normal specular original space?",
                    "label": 0
                },
                {
                    "sent": "The second in the transformed space that's done being transformed back and you can see that you actually get two different results, which is not very desirable obviously.",
                    "label": 0
                },
                {
                    "sent": "So a nice property of the approximate user methods is that the full approximation method is a fine invariant.",
                    "label": 0
                },
                {
                    "sent": "And the diagonal approximate method is scale invariant.",
                    "label": 1
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For property, would talk about is the ease with which you can extend gradient evaluation techniques to these methods.",
                    "label": 1
                },
                {
                    "sent": "So the main point is I want to make is that the gradient?",
                    "label": 0
                },
                {
                    "sent": "You just take the expectation with respect to P, gamma, Q of the drift of the log policy and all these different sampling techniques do that to do the proximate methods I just concurrently take the expectation of the question below policy and that gives me my estimate for H2.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we tried these methods on some various domains, so the first one we considered was Tetris domain.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Considered on a full 20 by 10 board, so this is a standard set up for the game and after around 40 training iterations.",
                    "label": 1
                },
                {
                    "sent": "So 40 parameter updates the full approximate user method gets a score of around 14,000 lines.",
                    "label": 0
                },
                {
                    "sent": "And just put that into perspective are using the same features approximate dynamic programming?",
                    "label": 1
                },
                {
                    "sent": "Which solution by bertsekas scores for a half thousand after around six iterations and then the solution deteriorates and then there's also an approximate linear programming solution with the same features and it gets around 4200.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I also compared these policy search methods on that remain, so I have the full approximation method in red diagonal approximation method in blue natural in green and steepest and black.",
                    "label": 1
                },
                {
                    "sent": "You can't do the maximization over W. At 4:00 AM, so I've admitted that one, but you get strong results.",
                    "label": 0
                },
                {
                    "sent": "In this in this.",
                    "label": 0
                },
                {
                    "sent": "Demaine the final.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, next time.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It was linear system, so I did this 'cause I can actually evaluate the search directions exactly.",
                    "label": 0
                },
                {
                    "sent": "I don't have a huge amount of details unfortunately, but an important point I would like to say is that the Hessian actually scored around the same as natural gradient descent and the reason for that is that you have to correct the matrix because it's not negative definite and you lose a lot of information by doing that.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the final experiment I considered going to a nonlinear navigation task.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the reason I consider this one, because in this case the policy is actually log quadratic and so in this case EM is actually equivalent to approximately 2 with a fixed step size of one, and so these results actually tell you a lot more now.",
                    "label": 0
                },
                {
                    "sent": "So you can see that the blue curve is using.",
                    "label": 0
                },
                {
                    "sent": "The Hessian, where you include information about the reward, but traffic step size of 1 natural gradient descent, you ignore the reward structure in the preconditioner, but you train the step size, but now we're taking the information for both of these.",
                    "label": 0
                },
                {
                    "sent": "Are we precondition with award and then training step size and we get the red curve here, which is approximate Newton method.",
                    "label": 1
                },
                {
                    "sent": "So it gives a nice interpretation.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With these results.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, just to summarize quickly, so I've given an analysis of current parametric policy search methods and then introduced to approximate using methods.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then there's some feature work by halftime.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "Right, thank you.",
                    "label": 0
                }
            ]
        }
    }
}