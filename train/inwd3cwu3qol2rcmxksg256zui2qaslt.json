{
    "id": "inwd3cwu3qol2rcmxksg256zui2qaslt",
    "title": "Resampling Based Methods for Design and Evaluation of Neurotechnology",
    "info": {
        "author": [
            "Lars-Kai Hansen, Technical University of Denmark"
        ],
        "published": "Dec. 3, 2012",
        "recorded": "September 2012",
        "category": [
            "Top->Technology->Neurotechnology",
            "Top->Medicine->Neuroscience"
        ]
    },
    "url": "http://videolectures.net/bbci2012_hansen_neurotechnology/",
    "segmentation": [
        [
            "So what I'm going to speak about today is.",
            "Is a collection of methods that we have used for for for machine learning of mainly fMRI and most of my examples will be an fMRI.",
            "I will speak a little bit about e.g at the end.",
            "Also in showing a recent example of how we apply this now too too.",
            "E.g both imaging and also analysis.",
            "But most of it will be F MRI.",
            "I think the the particular type of data is not so important.",
            "It's not so much part of the take home message is really what we do to design and particularly valuate and visualize these methods.",
            "Is is what I'd like you to to bring from here.",
            "And then.",
            "As I said, this is something I've been going on for awhile and most of it has been in collaboration with my friend Steven Strawder from from Toronto.",
            "Um first in in a big American project, one of the human brain projects that were launched in.",
            "In the in the early.",
            "Miss Midnight season went into the early 2000s, and then of course a lot of my local collaborators from from Copenhagen that have contributed to this.",
            "The students and colleagues at the moment.",
            "So here's my outline."
        ],
        [
            "I want to 1st convince you that when you do machine learning in this context, there are really two equally important at the end as that we are trying to.",
            "To meet so one is the usual one in machine learning, that is that we would like the machine learning algorithms.",
            "To identify the statistical processes that we see in nature in this case in in the brain Imaging experiment, and for this we use the usual cross validation like resampling experiments.",
            "But then of course in scientific data mining, as this is an example of, there's another agenda.",
            "There's the agenda that once we have those processes identified would like to understand them better, and this is a much more ill defined agenda.",
            "I would say in the 1st place I mean, what is it that would like to get?",
            "Is it a picture, is it?",
            "Some kind of causal explanation, and this is where I see a lot of really interesting research going on at the moment.",
            "Is trying to formalize that second agenda, and this is probably what is most of the novel here is that I have two axis in my evaluation, so most often you will see something like a test error as a function of of data.",
            "That's what we call a learning curve.",
            "I'll show some examples of that, or it's a function of complexity of the algorithm that it's a bias variance tradeoff curve.",
            "And I have a second axis in these plots, and that's how reliable is the explanation.",
            "Trying to see if I can formalize that as well.",
            "So I'm going to give you an example right away of how this works in a particular case, and then I'm going to show some examples of that both in unsupervised.",
            "So this means we're looking for structure and data, and in supervised learning much more strong form of machine learning where you're trying to model a particular relation between two sets of variables and then at the end I'll speak a little bit about what I see.",
            "Is this some future possibilities here?",
            "One of my heroes is this fellow here that many of you may recognize.",
            "He looks a bit sort of.",
            "Concerned here, and his statement is do not multiply courses, and I think that's one of the more general things that one can take from this.",
            "Also, both for the visualization and for.",
            "For the generalizability issue on the How will you define the processes that one should always try to find the simplest explanation and not multiply causes beyond what is necessary, so we'll try and see if we can formalize that into a mathematical or statistical framework, that's the."
        ],
        [
            "That's the purpose of reviewing some of this.",
            "I should say that I'm not going to review machine learning in fMRI.",
            "Because that's such a big subject that that would not be possible.",
            "I'm going to take a very sort of ecocentric approach and focus mostly on what we have done ourselves.",
            "If you're interested in other super interesting work that is going on and have been going on for a long time now in in the machine learning applications in an fMRI, I mean valleys.",
            "All these wonderful reviews and many more.",
            "Actually these are just from one Journal named in your image.",
            "See.",
            "And videos were several years and special issues.",
            "Some of these papers are from special issues that have been devoted to this kind of.",
            "We're thinking about brain imaging.",
            "I'll put these slides up on the website when I get back.",
            "Thus, some of these nights already actually."
        ],
        [
            "So in summary, I let me just I'm sure most of you is not all are very familiar with the technique, so this is an example of an F MRI data set.",
            "So this is actually one of the datasets that I have seen ever with the best settings or noise ratio.",
            "So this is a an experiment where we have a subject in an MRI scanner and the subject is looking at the visual stimulus 8 Hertz flickering checker checkerboard.",
            "And you can imagine how that looks right if you haven't tried it, you can imagine that this is a very strong stimulus.",
            "You see all kinds of artifacts.",
            "I mean, you see colors and things that are moving and so on.",
            "Because this is such a strong stimulus to the visual system.",
            "So this slices, like here, parents with Carolyn Celko's, it's a single slice and this means that we can acquire that at this page here.",
            "3 Hertz acquisition.",
            "So this data set has been used for tuning a lot of different models in our lab, almost like a simulation.",
            "You could say, because we know exactly what to expect here.",
            "And.",
            "And we have some of the artifacts that they haunt.",
            "fMRI, like the other hemodynamics that I induced by facility.",
            "For example, we have sampling frequencies that are so high here that we that we have the heartbeat, for example, samples.",
            "But there are many challenges to to fMRI, and This is why it's so interesting for us to to apply machine learning methods.",
            "Um?",
            "Some of them are here is that they are multidimensional mixtures.",
            "So what I mean by that is that whatever we see in a particular location is a mixture of many different physical processes.",
            "Some of them have to do with information processing.",
            "So how the brain reacts to this particular stimulus here in the visual system.",
            "So let's see if we can start it again and now you will see that this is resting.",
            "And now we start the signal coming on.",
            "So this is the signal.",
            "The stuff we see down here, right?",
            "And you can see that there's a very high noise level.",
            "Most of it looks like TV noise, right?",
            "So this is what we're up against so often I mean the confound us.",
            "They have all kinds of interesting physical origin, these confounding signals.",
            "And they have higher variance often then.",
            "Then the stuff that we're looking for, and they are modeled at different types of mixtures, and some of our algorithms are trying to teach those mixtures apart by blind signal separation.",
            "Trying to answer this kind of question, what is signal is noise, right?",
            "Or papers was is called?",
            "So just to show you."
        ],
        [
            "An example of where this how we used this idea of both evaluating the accuracy of the process identification so the generalizability of the machine learning algorithm and at the same time the information that we got from the model, I'll just show you this little example here will not give you much of the details, but just sort of show the flavor of the type of argument that I'm going to to bring out a number of times.",
            "So this was an example of where we try to model the.",
            "The process that generates the so called boat signal that we acquire in MDMA, fMRI in the MRI scanner and the bold signal is pretty complicated.",
            "As multiple not independent but related components.",
            "So it has to do with the blood volume it has to do with the block blood oxygenation level and the blood flow.",
            "So those three together come together to form the boat signal and this has been modelled in several models and one of the more popular ones is called the balloon model.",
            "It's a nonlinear model, so the idea is that the from the basic neuronal activation in an area that make up, let's say one voxel of 1 pixel.",
            "In the experiment there's all this multi level Physiology going on that that will change the bold signal in that box look right and the model has a number of compartments and there altogether and number of parameters handful of parameters that we would like to know of.",
            "And if we have.",
            "A stimulus, say like the one we saw before, and we see the responding response in terms of the build signal.",
            "Then we can try and see if we can identify those parameters.",
            "So we set up a set of nonlinear differential equations.",
            "That involve these different compartments and how they interact, how things feed into other compartments and so on.",
            "That's the balloon model.",
            "The parameters are unknown, so those will have to estimate it.",
            "So what we do is that the first of all we set up a system that can integrate these equations for for the Times Band that we're interested in.",
            "That's that's that's one step of the process for fixed fixed parameters.",
            "Then we can compare the solution to to the actual measured signal, right?",
            "So this is like running the model forward and this means that we can put forward now kind of likelihood function as statistical models saying how accurate is the model with these parameters.",
            "So that's likely function right?",
            "We assume that we know the parameters that we can see how close is the simulated signal to the two signal.",
            "So this is what we can do here, and this means that we can now do Bayesian sampling in the space of parameters.",
            "So in this 5 dimensional space, for example, right, we can do patient sampling, we get, say Gibbs sampling.",
            "We can run the forward model, find another sample, run the forward model again and then we can get a sample from the posterior distribution in parameter space.",
            "So this what we're typically doing in machine learning is that will try to say something about the parameters of the model given the data.",
            "That's the posterior distribution.",
            "But, and we will often be happy about the posterior distribution in the sense that it can predict the signal right.",
            "This is how we will evaluate, maybe on a test signal.",
            "But if you're interested in Physiology like we were in this particular case, and there's a particular question we wanted to answer, so these were two different versions of the model that had two different types of neural input.",
            "One was rectangular, like a.",
            "The continuous activation while we stimulate and the other one was a fading stimulus with two competing hypothesis, we want to see which of these models are do the best job here.",
            "So we run this with a typical run of the Mill Basin analysis to get a posterior distribution.",
            "But now we ask the question that is not often asked how robust is the posterior distribution?",
            "How much do we actually learn from that posterior distribution?",
            "So the way we measure that was by the technique that I'm going to speak more about it.",
            "Namely, Split has a resampling.",
            "So we split the data set in two halves and built the whole posterior distribution on those two halves.",
            "I get two different posterior distributions.",
            "Now if everything was a OK, they should be identical, but of course because the data is noisy and small and so on, there not, and it's a quality of the model, and in this case how much we learn from the model that the posterior distribution is a similar between the two sets as possible.",
            "Now we cannot simply say that we want something that is as simple as possible, because then we lose the other part.",
            "The generalizability, because then we lose the expressiveness of the model is very simple.",
            "Model will always be more more similar in every sampling.",
            "For something, it just outputs zero, will always be the same.",
            "So we have to do both, right?",
            "We have to make sure that it predicts and predicts well at the same time we would like it to be robust under resampling so that we can be sure that the posterior distribution we get is trustworthy.",
            "So."
        ],
        [
            "In this case we have we have two different datasets and so these are the two experiments here.",
            "A and B1 is the one I showed you before, and this is another similar one that.",
            "So this is a slice and this was a volume experiment that we've run just to to make it a little bit richer in the type of experiment that we could explain.",
            "And then we have the two models sustained your input and fading input, and these are the two different simple tiers of squares are the fading input and the sustained neural input is the thing with the Diamond Tour and where we want to be is the following where we want to like to understand this plot is the following way and we want to be up in this corner here because out of 1 access I have generalizability.",
            "So this means how well do my model predict the data in test in the test set, right?",
            "That's the usual generalization that we have out here.",
            "And it's good to be high in this case, and rebels ability was."
        ],
        [
            "This measure of similarity so heavy use the KL distance between the two posterior distributions.",
            "Simply ask how similar are they in information measure?"
        ],
        [
            "Cable distance as you see here mutual information.",
            "Ansome this brings us to the following plot.",
            "So let's take the first one.",
            "So this was this experiment.",
            "You can see that the simpler model that has sustained neural input is much more.",
            "Good at your eyes at you, realizing it has a consistent high value, the corresponding to the highest values that you see in the other model.",
            "But the responsibility is much better.",
            "Case, it's also good to be top of this axis, minus Ko.",
            "So what we learn here is that this model is actually good at your last ability, but it also giving us something which is more trustworthy.",
            "And this is how we're going to evaluate.",
            "So we prefer that model over the other one.",
            "So that's the."
        ],
        [
            "That's the trick.",
            "So now I'm going to give you a little bit more.",
            "A formal description of what is going on, including how we set up the different types of models and so on, and show you some examples from unsupervised learning and some examples from supervised mode.",
            "So, um.",
            "If you want to formalize the type of experiment that we are doing here, we have to first of all of course define the variables that go into it.",
            "So we do brain scans.",
            "So this means that we acquire some kind of high dimensional signal as a function of time, for example.",
            "So this would be that video that we saw before.",
            "But we have a 2 dimensional like an image evolving in time, so that would be like a video.",
            "And of course if it's a 3 dimensional experiment, is like a 3 dimensional video.",
            "Yes, so that's this set of variables that we've called X here and the nature of those variables is that they.",
            "A high dimensional so they reflect the state of the brain in a local way.",
            "So we know something about particular areas, how they evolve as function of time.",
            "And that I don't think you can call it micro variables because they have caused you.",
            "Still interesting something about neurons, but we don't have.",
            "We do not have access to neurons in in Moscow with access to this coarse grained average on roughly a couple of millimeters.",
            "5 millimeters maybe.",
            "So there are many many neurons, so this was sometimes called these mesoscopic variables.",
            "I mean they but they still have this that they tell us something about what happens locally in the brain.",
            "And we want to find the relation between those variables and then the kind of variables that describe the whole brain as as a as a body.",
            "So those would be for example variables that describe the stimulus.",
            "So in the simple case where we had this block design where we had resting and then activated resting, activating again and so on, it would be a square wave type function like this S and there would be a single variable and will be a single binary variable in.",
            "One way to think about it zero when you are interesting and.",
            "Interesting States and one when you're simulating, but it could be much more complicated.",
            "Also, if we had other measures on the on the on the subject.",
            "We could add those to this so the idea is that these are local and these are global variables.",
            "Of course, they also sometimes have different structure in the sense that we can control some of it, but not the other.",
            "That's sort of a different level of awful things.",
            "Here we simply think of them as having those two different qualities that they're kind of local.",
            "Mesoscopic and macroscopic variables and, of course, that the objective of the of this whole exercise is to get something that can tell us about the music nation between the two sets of variables.",
            "So in the methods that I'm going to talk about then.",
            "Question that maybe sometimes S. This is not even known before hand.",
            "So this is what we were called unsupervised learning.",
            "Then we will try it.",
            "Then we would have to do with X alone and will try to see if they are stable hidden variables as we call it in this case.",
            "So this could be clustering.",
            "For example we group the scans so if we without telling the system know that we have a baseline study, we could hope that if we did a clustering on the raw data then we will find the two clusters.",
            "We find one that is corresponding to baseline and wonder corresponding to.",
            "Two, so the activated state and the hypothesis that in this space, if you think about X, so it starts being an image or or volume.",
            "But think of it now as as you string it out as a vector.",
            "So find some particular way of scanning the image, just like a TV for example, right?",
            "Or do the same thing for the volume.",
            "Now we have a vector and that vector can be thought of as coordinating a space of the same dimension as the length of the vector.",
            "Then the hypothesis here is that the measurement is so good that if stuff is close in that space, just your metrically close, like in Euclidean distance, it's roughly the same meaning.",
            "So we hope that what corresponds are good cluster in that space would be something that reflects a variable like S, right?",
            "Vice versa, we could say that this is what defined could variables is that they have that property.",
            "Sometimes you will have to find other ways of measuring distances because the spaces can be warped in certain ways because of the experiments, and This is why at some point we may need to look at more manifold based methods that where distance is not really measured so well in that original space.",
            "But we have to go to a certain latent space where the distances are more faithful to to the interesting structures.",
            "Here I'll show you some examples of that also."
        ],
        [
            "If we look at the very briefly at history, we can see that there's been many attempts of modeling this joint distribution for.",
            "Brain states an and.",
            "Panty Shannon psychological states yes, here, or the macroscopic states in the Mayan basis graphic states.",
            "So the two basic ways of analyzing a joint distribution is to factor it.",
            "Either of these two ways here, right?",
            "So I think I think of it as a as a conditional distribution of the brain image given the stimulus.",
            "And then we have to multiply by the probability of the stimulus.",
            "Or we can factor it the other way around.",
            "And this gives rise to two quite different types of machine learning methods.",
            "Once you do one or the other.",
            "So in this case.",
            "We can think of it as we control the stimulus and we simply look for what are the effects in the brain volume.",
            "And now you can see that this is the kind of thing that you do in SPM.",
            "For example, right?",
            "Because there you would pass it a model saying that each of the two states in this block design each one is a normal distribution of very simple one, because this is so high dimensional distribution we have to do something to simplified, and in the classical SPM package which has been extremely successful of course and has completely changed this whole.",
            "Came since fMRI came out and before that pet imaging and so on.",
            "The idea is to to model this distribution here, in the sense that we assume that all the different pixels are boxes.",
            "If it's a volume are independent, right?",
            "So this means that there is implicit in machine learning will call in the base assumption that that you can write this joint distribution in between all the different components of the vector as a product.",
            "So it's a factorized distribution.",
            "But you can also see that in this way you are definitely going to lose a lot.",
            "I mean, there's a lot of dependencies structure that you lose right away if you do that, and This is why when we started doing our machine learning methods in the mid 90s.",
            "We can tell that the other way around so that we said, OK, let's let's focus on on the dependency structures in the brain scan and then we have to look at this factorization where we try to predict the status of the brain condition on the brain scans.",
            "So this is what now called mind reading right?",
            "We try to predict what the brain is doing.",
            "And of course we can do that in a simple way if we have access to label data.",
            "So this would be the supervised case where we actually know is is a Nexus in a training set.",
            "Then we can train this conditional distribution here, right?",
            "And then modeling this part, which is again as high dimension of this.",
            "We have to make really strong simplifications and this we did, for example by principal component analysis or independent component analysis.",
            "As I'll talk about in a moment as an attempt of modeling this distribution here, but still not quite as to what dramatic simplification of this, because in ICA PCA we also do a factorization hypothesis, but not directly in the data, but in the latent variables.",
            "And that's a much less strong assumption.",
            "I mean, that's a much more likely thing to happen that the.",
            "Physical effects that cause S are independent.",
            "We can separate them that way, so this is for for PFX.",
            "Whatever we do, we will always try to think of these distributions are things that we can model by the machinery of machine learning.",
            "So this means that we will bring out parameterized families of models.",
            "So when we started doing all this for for for the conditional here, this was an artificial neural network algorithm, so this is a flexible family of functions where we have parameters describing exactly which function it is, and then we had a platoon, those and they.",
            "We sample a little bit the neural networks in the brain.",
            "This is why they were called artificial neural networks.",
            "Nowadays most people use support vector machines to model this condition solution.",
            "Here.",
            "This is what I'm going to talk about.",
            "Also towards the end.",
            "Um?",
            "Alright, let me run."
        ],
        [
            "So now once we have defined the the model family, we have to device the loss function.",
            "So when is it a good model that we have?",
            "Access to and the model will take this form here, so let's look at this conditional distribution again.",
            "So now instead of using just the true distribution that we don't know if we have to do something that depends on data.",
            "So we assume that we get some kind of data set.",
            "Now that we call D, yeah.",
            "And that data set will typically enter into the model as as an estimation of parameters or sampling the parameters if we want to do something that takes uncertainty into account.",
            "Also, and we think that's important then, then we will.",
            "Use this is called the predictive distribution, either directly with some point estimates where the data is converted into parameters, or we will have a sampling procedure where where this is computed as an average or posterior example.",
            "Either way we have access something like this that we can say something about for a future scan.",
            "What is the future expected label?",
            "And then of course we can play the trigger cross validation that we can compare it so in a data set where we know the truth that we hold out from between.",
            "So this would be the training set and then the average.",
            "So that's this angular bracket.",
            "It means that we average on test data.",
            "So we trained the parameters are trained the posterior distribution on data from the training data, and then we apply it to another data set so we can get an unbiased estimate, and that's the important of importance of a test data set is that it gives us an unbiased estimator of the performance.",
            "And the performance is then measured with different cost functions, right?",
            "And at the the one that most people would do something like this, the least squares tight fitting to use the squared error type expression.",
            "You can argue sometimes that this is the right thing to do, but sometimes it's simply convenient.",
            "So for example, if you have a classification system where IDs variable would takes zeros and ones, then this could be really useful because this is simply proportional to the number of times that that the predicted.",
            "This is different from the 2S.",
            "So you can use that even in this case.",
            "Also this but, but there are also other more accurate measures.",
            "I would say, for example, a very natural thing to do is to use this expression.",
            "Here the negative log probability.",
            "So this measures not only whether you make a mistake or not, as this would do, but also how certain why you when you made a mistake and it's of course worse in a sense to be making a mistake if you were really certain that it was the other thing that was right, right?",
            "So this would be a strong mistaken week mistakes, so you can measure that with these cost functions here.",
            "And interesting enough, and I think this is not so widely appreciated.",
            "This many people do.",
            "This is called the deviance right?",
            "But the deviance can be used also for.",
            "Also, for unsupervised learning and this is less well known that many people know that I'm, I think many of you have used this already for linear regression, support vector machines.",
            "So you use these test error methods all the time.",
            "We always do that, but you could do that also for the unsupervised learning algorithms.",
            "Exactly the same trick can be done for anything like principal component analysis, ICA, clustering algorithms.",
            "All these unsupervised hidden Markov models that produce it at densities in sequences and so on, all these models.",
            "That work from day to themselves and try to find a latent variable.",
            "They can be treated in exactly the same way, and this is I'm going to show you some examples of that using this likelihood function.",
            "Here the lock probability of that you use for training simply apply the same likelihood function to the test data.",
            "So that deviants works just as well for all the unsupervised algorithms as it does for supervised learning.",
            "And you can even combine if you're interested in modeling both the conditional and the density of the same footing.",
            "You can use this kind of useful information expression here that.",
            "That we actually when we first started doing the.",
            "The young evaluation schemes that I'm going to show you the moment this split half resampling and so on.",
            "We use this cost function here.",
            "Often the mutual information to qualifier the value of a model."
        ],
        [
            "Here's an example, an old one, so this is precisely this.",
            "Optimizing a unsupervised algorithm.",
            "So this is a very simple application of principal component analysis to the data set that I showed you in the beginning.",
            "This video of a single slice.",
            "So the principal component analysis is a model that says that the all the interesting dependency between voxels.",
            "So remember this is unsupervised, so we have no labels.",
            "We don't know when stimulation took place in or not.",
            "Right now we simply have the video itself and I'm trying to find some structure in the video.",
            "And then simply strengthen the video out as a vector so I have a sequence of those vectors and I'm going to look for.",
            "One single statistic, namely the covariance between pixels.",
            "Hoping that the direction that the high variance components in that so the principle component associated with interesting dynamics in the data.",
            "And the details of it, I'm not going to show right now.",
            "I'm coming back to this in a moment, but it actually does pick up some interesting dimensions, like the stimulus condition, the stimulus response response in the data to stimulus.",
            "It picks that up quite well already with principal component analysis.",
            "But then the question of course, is how many things are going on.",
            "So if you think of Frank component, notice this is finding the eigenvectors of the covariance matrix, and now the model is again a multivariate normal distribution.",
            "So we can build the likelihood function.",
            "That's the probability density function that goes with it.",
            "Articular covariance matrix and the covariance matrix.",
            "We can approximate by one actually, but I know I can vector so this is just a noise, uniform noise, right?",
            "Or we can approximate it by a single direction in space and then uniform noise in the rest of the space or any number of components plus noise in the rest?",
            "And the question is how many should there be?",
            "And this is exactly what is shown here.",
            "So here we have the negative log likelihood on test data.",
            "Lag.",
            "And this means that it's good to be low.",
            "Because if it's low means that the likelihood is high.",
            "So this means that on test data we have high probability of seeing them, so the model is good if it can produce a high likelihood, intestate, right?",
            "So the negative likelihood should be small.",
            "That's like an error.",
            "And here is the result of applying this to a test data set with different number of components.",
            "And there's a clear minimum right that you say this is given this data set that I have this particular size number of of of pixels and so on.",
            "Number of scans that I can look at.",
            "This is the model that produced the best fit.",
            "And then.",
            "What happens if I use less components?",
            "Then I'll have a misfit because if the model is trying to.",
            "If the model has two few components here, then then the.",
            "It will not be able to capture all the modes that at this thing.",
            "So this means that it will make Me 2 uniform, so it'll be 2 flats or put probability.",
            "But there's no need for it.",
            "So if I have two 2 strong directions, then there should be a lot of probability in that plane, but not another in other parts, right?",
            "So if I have a lot of signal in other other parts of space, then there would be missed here in the case so that we call that bias that the model is is kind of too low probability in two large an area and then the opposite thing happens here in the case that there are too many components that specializes, it'll put probability in certain directions that are good for the training data.",
            "But then when we go to the test data they don't reproduce, so This is why it's going to shoot up here and there's a.",
            "Compromise here in the middle.",
            "Now to other curves.",
            "So this is if you simply applied it to the training data, they will see that of course, the more you yes please.",
            "Yes.",
            "Yes, so so."
        ],
        [
            "This is this experiment here, and it's the mask is what you see here up scuse me.",
            "Backward.",
            "So there's also all these pixels here are inside the mass, so it's a couple of 1000 dimensions that it takes place in to start with.",
            "So this is a global PCA.",
            "Double pizza.",
            "And in in this data set it points to its relative small number of components."
        ],
        [
            "And then then there's a whole game.",
            "I mean, as a paper that reference that you can go and see what these components really are, and so on."
        ],
        [
            "The.",
            "Interpret and trying to understand the yes variable.",
            "Say I'll be more specific about how to read out the S and so on in a moment."
        ],
        [
            "So this is more about the method itself, so this is how we optimize the model.",
            "So we make sure that it actually does capture the physical process that is there.",
            "The best we can with this model framework within that family of models, I would say that the three components is the best.",
            "Now there's another way of mapping out this relation between model complexity data, amount of data, and the generalizability, and this is the form of I think I already mentioned, this is what we call the learning curve.",
            "So here the what we're trying to find out is the relation between how?"
        ],
        [
            "Complex and model.",
            "Can you expect to see in a certain amount of data, so you have this intuition?",
            "Of course, that the less data you have the similar model you have to stick with, because if you use complicated models on small amount of data then overfit us.",
            "We just saw it so we can map this in this way here.",
            "So here I have two different models, not a whole lot of models before 2 versions, so this could be if we did this for PCA.",
            "Again it will be two different numbers of components will compare, so here it's actually a supervised case, so it's finger tapping.",
            "In two different modalities.",
            "So this is a pet experiment.",
            "Is an MRI experiment and now we ask for what happens to the quality of the model as we increase the data set.",
            "And in this particular case it we compare a simple linear, so this will like linear discriminants.",
            "I simply cut that space we talked about before in two, and I hope that the baseline is on one side and active scans on the other side, right?",
            "So this we tune that on training data to find that.",
            "So that's the linear discriminate.",
            "And then there's a nonlinear classifier classifier that's the full curve, which is making it more complicated assumption about the division between the two groups.",
            "And now we ask for the mean classification error.",
            "So here it is simply using that S -- S had squared thing right?",
            "And so we measure the number of misclassification as a function of the number of scans that went into this and what you can see here is what you would expect.",
            "Also that these two lines they cross.",
            "But if you have a small number of data then you would can only justify a simple model.",
            "The linear model and then the number one will overfit because it had mostly group freedom.",
            "Fitting the noise, and this does not generalize to the to the test data, right and.",
            "Yes.",
            "Did it?",
            "Open Scott, sorry so this is a different modality like fMRI, so this isn't this is done in the fMRI scanner.",
            "This is done with a another type of scanner that we used in the old days to make the same kind of experiment as I talked about for fMRI.",
            "So it stands for Positron emission tomography, and it's simply a different scanner that measures more less the same signal.",
            "It officially is a little simpler way in pet scanning, so it's it's more so for an experimental point of view.",
            "It's actually a better experiment, but it has some.",
            "Pretty bad side effects, so it's using radioactive traces, so it means that there's a limit to how often you can do it and.",
            "It's a slow technique and so This is why people went onto to use that technique.",
            "We're talking mostly about.",
            "Sorry bout that.",
            "Um?"
        ],
        [
            "So this was analyzing one part of the of this, namely the quality of the model now moving onto to the interpretation.",
            "And then of course it gets much less firm, right?",
            "I mean, now it's it's rotation can be anything, right?",
            "We want to understand something for that model that's completely unspecified to start with.",
            "But in brain mapping out say that there's almost like there's a scheme now that is developed, and this is it goes with the word that what we're interested in is a brain map.",
            "So all these methods that we use all the machine learning methods that we develop support vector machines, you know, classifiers whatever used to understand the relation between S&X.",
            "We want at the end of the day after we have identified the process, the best we can using those tools we want to find out what that model learned about that particular experiment.",
            "What is it that that was the salient regions in the networks were involved in this particular information processing, and we want to to find that, and we want of course to be sure about what we find again.",
            "So this is what I'm trying to set up is a way of getting to something that is that can be quantified, and even the uncertainty can be quantified.",
            "So the brain map has a very informal definition, so this would be something that if there are regions that are involved in the information processing task.",
            "That defines an S. Then we would expect the brain Maps would be high and so high intensity as an image.",
            "Bright areas in the image would be associated with that and lower values if they're not involved.",
            "The ideal thing would be something that has value.",
            "One of the areas that are involved and zero in the other areas, but it's not so simple for many reasons.",
            "I mean partly because of the brain.",
            "Maybe everything Department takes part in anything, right?",
            "I mean, there's so much communication going on.",
            "So many hypothesis tested about any stimulus that it may be a better, maybe actually so that anything is any areas involved.",
            "But here we have caused talking about something that we can say with certainty, significance.",
            "We want to find the best and so on that many things that we can quantify to get some areas that are more involved than others.",
            "So we expect this to be higher values on the ones that are most involved and lessen the the other.",
            "And this is exactly what was the success of statistical parametric mapping.",
            "Of course was that it produced a map like that and This is why we have something called your image.",
            "All these new images are shown in that yeah.",
            "Um?",
            "So there's been a number of attempts to to do that, so we have two different versions of visualizations that we've been working on.",
            "1st, we had something we call a sailing to map, and this is the case that is shown here and kind of three dimensional plot, typically based on pet imaging.",
            "Also in the mid 90s and then later on we so we had some resistance towards the same zoom app.",
            "I think it's still a good definition.",
            "I won't talk much more about it, but just say that it's there.",
            "There's a reference if you want to see how we computed when I'm using now.",
            "Something we call the sensitivity Maps.",
            "It's almost the same, but it's measuring simply.",
            "And model how sensitive the model is to certain areas in the brain.",
            "So this is how we're going to measure the moment.",
            "Whatever we use that weighs also that you have some papers on how to to make consensus among different Maps.",
            "But I'm not going to speak about that, either.",
            "Also, front."
        ],
        [
            "1000 so once I have estimated my model.",
            "We could say that if the model really identified, the physical process, will that not that particular model, not by itself, sort of be the most robust representation of the data now?",
            "And if you.",
            "And follow that lead and sort of look into what the statistics have to say about that.",
            "So how how well determined other hidden parameters?",
            "So the question is if I find the model that that is generalizing the best when that are not also be the model that has the more brokers hidden variables.",
            "So maybe expect that to be the case, but it turns out that it's not unfortunately.",
            "So if you go to the.",
            "Positions and ask the question, then they will say that we know something from asymptotic theory, so I'm trying to do is the following that instead of having small ends as I showed here small above subjects, multiple scans per subject and so on, you now pretend that you have almost infinite amount of data, so you look at what happens to your model if the data set starts to to become infinite, then of course any parameter init distribution becomes peaked because there's so much information now that we can really say a lot about the.",
            "About the values of parameters or the distribution of parameters for latent variables.",
            "So in this limit one can start to do computer computation, so the there's a small parameter, so we can do a formal expansion of likelihood functions on test data.",
            "For example in terms of a small quantity, namely 1 divided by the number of samples, right?",
            "So if N is big, 1 divided by in, this is more parameter.",
            "We could expand whatever we have cost functions and so on in that parameter, and those expansions can give us some insight in that limit.",
            "How well the connection is, how tight the connection is between.",
            "Performance and the parameter fluctuation and what you find out if there's an old paper here we do it for different models in the cross validation example and what you learn is that if you do cross validation resampling, so this means that we split the data in multiple sets.",
            "Then if you actually fuse the models afterwards, then asymptotically you get the same performance as if you trained on the whole thing.",
            "So that's kind of a good news for us.",
            "They like to do cross validation because it is like we don't really do anything.",
            "Of course for small datasets we do.",
            "And we hope that we can keep it on the control by being clever about the way we do the resampling.",
            "But asymptotically, I mean there's kind of a good news statement there.",
            "There also, so these results are available for parametric models like in this paper.",
            "We looked at models where the size of the parameter space stays finite while the sample size goes to Infinity, and this is not does not cover what we call nonparametric models.",
            "So for example, support vector machines or kernel machines in general, what we call nonparametric in the sense that the.",
            "Dating variable spacing away expands with the sample size, and there we have some results also and they kind of give the same result as well.",
            "I'm going to say now as the finite privatizations, but much more complicated.",
            "Calculus two goes into it.",
            "So what we find out is that the.",
            "Predictive performance is the generalization error for the generalizability of the model.",
            "Has two components as we saw before, there's a bias term, meaning that the model maybe could be too simple for the problem.",
            "That's the bias term and there's a variance term saying that it could be too complicated near that it varies a lot.",
            "If you re sample from one data set to the next, so those are the two many times we can write the error as a sum of those two simply and other times it's a slightly more complicated relation.",
            "But these two components determining the error.",
            "And the variance component is directly coupled to the variance of the parameters.",
            "But the bias term is not.",
            "So that relation is nontrivial and This is why there are two axes in the plot that we have to quantify the performance and then we have to handle the potential buyers that we have in the in the best models.",
            "What kind of parameter fluctuation they lead lead to simply?",
            "So this is this is the reason that there's this sort of like 2 degrees of freedom, and even in the asymptotic regime where we can do these calculations.",
            "So this is simply the recipe how to compute the sensitivity map.",
            "So this was defined in this new image paper from 2002.",
            "So we take the last function, so that was the log of the probability of the predicted value given the scan.",
            "We simply ask this cost function."
        ],
        [
            "Sensitive is it to a particular location?",
            "Remember, we straightened out the image and now we have a location called that J.",
            "So we simply compute the derivative after cost function respect to the activation in that location, and we do that insert the value.",
            "All the scans that we have in the test data and then the average on test dates of the square of that value.",
            "The reason that we have to square it is that sometimes these derivatives can go in One Direction, sometimes they go in the other action because for a non linear model there's no simple relation between what the sensitive local sensitivities and the global sensitivity, so it could be.",
            "Way, and you can think of this as a vector, can point in all kinds of directions.",
            "There's a paper in your machine research from here from Berlin showing that how these vectors they change across space.",
            "So we do an average here to get a map and average map for the whole test set.",
            "And this is what we call the map, right?",
            "So we get a value for each location.",
            "And now we can ask for example something like the following that we split the data set like we did with the procedure distribution, split the data set in two halves.",
            "Estimate a model on each of the two and now we simply ask how, how much concurrence is that between those two Maps that we get?",
            "So I have this enough."
        ],
        [
            "Figure here, so that's the what we call the impasse.",
            "Scheme for for evaluation of algorithms like like we have talked about here and this is developed as a set together with Steven's brother.",
            "And many of these papers show how to apply the scheme and in many different contexts.",
            "So how does it work?",
            "So we take the full data set, so that's the combined set of scans that we have here, and this consists of the ex is and the variables that we're interested in.",
            "So let's imagine here for example.",
            "It's a supervised case.",
            "We're doing my tweaking right?",
            "So this would be labels.",
            "This would be like a design matrix.",
            "Or the S is that encodes this on off stimulus.",
            "So we have a green set and a blue set, and for the blue set we can estimate the model and we can compute the map that's got little SPM here just to make it something that we we know how to think about.",
            "That's the map that is high in the values that in the areas that are highly involved and low in the places that are not so involved we get an SPM from the blue set.",
            "Estimate the model and now we can do predictions on the green data, right?",
            "So we can supply these axes as input to the algorithm here and produce predictions and then we can compare that with the true labels that we have from the Queen set, right?",
            "So the blue is training set.",
            "Now the green is a test set.",
            "We compute the test error and we can compute the map.",
            "Of course we can flip the whole thing and just interchange blue and green.",
            "So now we get two measures of performance and we get 2 Maps and the two Maps.",
            "They should be identical right?",
            "Because I split the data in two halves so if everything was perfect in a perfect world, they would be identical and they'll be no?",
            "Not at all, but in the real world, of course there's a lot of noise, so they're very different.",
            "The two and the more different they are, the less we trust them.",
            "Because of the split half procedure, this estimate of uncertainty that you get simply by subtracting the two Maps and squaring it.",
            "Is an unbiased estimator for variance.",
            "That's because there are two independent sets.",
            "So now with this particular resampling scheme split half we get an unbiased estimate of the uncertainty of the map.",
            "Now, many other schemes that you could do to re sampling get information like as you.",
            "I'm sure many of you have heard of Bootstrap for example or the phone out we sampling.",
            "So they have their own advantages, but they also have this problem that you cannot be sure that the variances can be corrected to be unbiased, so there's some correction formulas for for Bootstrap, for example, but there are some starting again, so you can trust them.",
            "So if you really want to have something which is manifest on bias, you have to do something like this.",
            "Rum.",
            "So."
        ],
        [
            "You have some examples and how you can use it, so this is a case where.",
            "You have a.",
            "A subject that is trying to control a force by pressing a finger on the lever, and there's a visual feedback.",
            "And of course there's a whole network involved in that control process is pretty complicated cognitive task.",
            "It depends on how strong the pressure is of the feedback is on how well you attain that particular pressure.",
            "So there's a big network that is involved in this.",
            "So we run this whole machine Raider just talked about.",
            "And now we get for every location we get a mean map like.",
            "We can average all these Maps we have from the different datasets and hope that this is roughly what you would get from the if you have the full data set and now you can use the variances that we estimated, take the square root for example, divide that and then you get the score so expression and then you can start to to produce AC score map out of something as complicated as this London your machine learning algorithm is producing.",
            "So that's I think that's the sort of level that we have taken it so far is that we for any map where we can compute derivatives, that's the criteria.",
            "We should do something that looks like electric function or something similar to it.",
            "A cost function.",
            "Be able to compute the relative.",
            "Then we can can do this.",
            "So this means that we can do it for almost anything that.",
            "It has been applied in machine learning.",
            "Yes, yes.",
            "So of course I don't know in that particular case because I haven't looked at it this way, but I know that there's a really broad spectrum, so if you look at the values of that map, there's a broad spectrum and we know that you get more robust model.",
            "So this is something that comes out of this plot that I showed before.",
            "If you, if you're not too aggressive.",
            "So if you make the model too sparse, for example, so this would be eliminating those boxes.",
            "Maybe that you're talking about now if you eliminate too much and make it too sparse, then reproducibility will will fade.",
            "Then they will not be as reproducible.",
            "As if you include more boxes into the.",
            "Set that you that you're interested in.",
            "So this could be an effect like.",
            "So this means that, for example, in the resting state, that could be voxels that are systematically active, so to speak.",
            "I mean, if there's a resting state network that is not directly coupled to any S variable, it could still pay off to model that, right?",
            "I think that's that's maybe an effect like talking about yes.",
            "Um?",
            "OK, let me show you."
        ],
        [
            "So we should find out when to enter stuff inspected in here.",
            "What is the time limit that we have at the end system?",
            "So we we can go maybe up till about half past or something so I don't know if we should have a break or something.",
            "Is that?",
            "Do we need a break?",
            "But you just keep going.",
            "We'll keep going OK.",
            "So I'm going to show you some examples Now what one is for unsupervised learning.",
            "So unsupervised.",
            "Like Homer, he's left on his own right and some supervised examples where where it's classification, conventional mind reading.",
            "So the re/max."
        ],
        [
            "I'm going to to revert to share an unsupervised learning starts out with talking about a simple linear model.",
            "So this is what in statistics is called a factor model, so it posits a very simple relation between the neuroimage that's X and the hidden variables, S, namely a simple linear relation.",
            "So there's a set of coefficient that forms a matrix.",
            "Or assume that both of these are vectors.",
            "So let's think of it as a color column vector of S at a given time.",
            "In the latent variable.",
            "So the the information processing projects into a brain image that we can measure the bold signal, which is also a vector at the same time here right?",
            "And there's a simple you know relation, so a lot of coefficients that we collect in this matrix A and then there's additive noise.",
            "This model looks simple, but it's actually very rich and there's a lot of interesting models that people have applied in your imaging that that can be sort of captured in this simple framework.",
            "Depending on how we parameterize and how we think about S and the noise, and a right so thinking about AES and and the noise in different ways, we get to different models, so some of these are.",
            "Models like principal components that we talked about before in this case.",
            "This is a normal noise drawn from a normal distribution.",
            "That guy right here with a Sarah mean and some covariance matrix and the PCA.",
            "This covariance matrix is a very simple.",
            "Their diagonal matrix scaled by the variance, the isotropic part.",
            "And then this S variable here again also will be drawn from a normal distribution.",
            "And the coefficient eys these coefficients.",
            "Here you think of them as a matrix.",
            "The column will then often be model simply as a orthogonal vectors spanning the space of the subspace.",
            "That this part here give rise to.",
            "So that's the highest variance components.",
            "I see a means that instead of algorithms where you have assumptions, typically honest, at the other that are non normal, so other types of distributions or or correlations.",
            "In this time correlations for example, that you can measure.",
            "And you can also do something like a means.",
            "For example, if you allow these answers to only be binary variables, so there will be assignments to different components, different clusters in the mixture, same model with the client and saying like your function and so on.",
            "So this is how we now go from this definition of the model.",
            "So the likelihood function that I mentioned before that is to say OK, what is the likelihood function like?",
            "Your function is to say something about the distribution of data based on assumed parameter values OK?",
            "So we assume that we know a for the.",
            "Constructing the likelihood function and potentially other parameters that play a role.",
            "For example, in specifying the distribution of these latent variable hidden variables S. So we can write this language function as a product of the two things, so this is not easy to write out itself, but it's easy to write out is simply using this equation up here to write the probability of X or the PDF value of XX.",
            "Because here we say that if I know what S&A is, so assume that S is known also just for the construction of the model and the likelihood function.",
            "Assuming this is known as known and the variance and covariance of epsilon is known, then this is an easy distribution to do because fix A&S.",
            "Then this is just a vector itself right multiply a, unselect you get a new vector.",
            "But that's a fixed one.",
            "Now that's because it's on the conditional side.",
            "And then all the uncertainty about X is in epsilon.",
            "That's the noise contribution which we assume is a normal distribution.",
            "So this is simply a normal PDF that we have here.",
            "This is what I've written down here, right?",
            "So that's easy to write out, but now we didn't know what this was.",
            "This was the.",
            "The assumption is that S is an unknown hidden variable, so we have to put in an unknown distribution here.",
            "This is where these models they differ.",
            "Put in a normal distribution, we get to the principal component analysis and put in other distributions here.",
            "Then we get to these other models in any case.",
            "Choosing one of these, we can write down the likelihood function and just run the whole game as we as we laid it out.",
            "Yes, for those of you who have seen a lot of this before, we can also implement these other methods here that people sometimes use, for example, structural equations.",
            "They have to do with the specific assumption on it on a here so that there's a Markovian relation between the components of X.",
            "So this means that this guy is sparse 1 -- A minus first, and if everything is positive, both A&S, then these part based representations of people called non negative matrix factorization paper written in the same way.",
            "So if you're interested in this framework and also algorithms to to implement them and so on you take a look at these papers here.",
            "So This is why."
        ],
        [
            "They do, typically is that they tried to write the matrix that we have looked at, so we have the different pixels or boxes versus time, and then they write it as as a product of two smaller matrices.",
            "This is what we learn now from the.",
            "We learn these some typical patterns in the space that they will form the map, and then we learned the time series that activate those modes.",
            "So in the signal that we looked at before, we assume that one of these columns would be pointing to the visual cortex, like this would be the thing."
        ],
        [
            "Skip this in here."
        ],
        [
            "An example of it, so his and I see a case, so here's a column of a now put together as a picture again and we can see that it has high values in the areas of where we saw the signal, and this is the stimulus curve that we didn't tell the algorithm about and this is the response that we fit it from it, right?",
            "So we can see that it's the typical thing that if you know about the boat sinking that there's a delay.",
            "And also there's quite a bit of irritability.",
            "This is what you gain by this type of analysis unsupervised.",
            "Analysis is that you can detect subtle changes in the latent variable.",
            "So if we only knew about the square wave, so that would be the stimulus.",
            "So this would be like in supervised learning.",
            "Then we wouldn't learn the difference between the response here that actually ever responses itself is its own response, right?",
            "And there's something that seems to reproduce, but also a lot of things that are different from response to response, so there's a lot of variability in the brain state, and when this method was first applied to fMRI, this is in 98 by magic.",
            "Keone and coworkers.",
            "This is exactly what they were excited about.",
            "Was that this could detect this kind of transience in the response by by using unsupervised methods like like the ones we have here.",
            "There's a radio paper from 2000 and three of these things are discussed.",
            "This."
        ],
        [
            "Is the tool box so they have many really good tool boxes for running this kind of unsupervised linear factor.",
            "Model her so the one that we have produced has one particular aspect that you don't find in many of the others.",
            "That is this determination of the number of components in a computational quite easy way using another curve that also showed you without talking about that.",
            "That is a good proxy for the test error.",
            "I think I'm unfortunately going to skip some things here that we otherwise I won't be able to.",
            "To make it so."
        ],
        [
            "Think I'm going to talk a little bit about nonlinear versions of this.",
            "So I'm going to put these slides on the web when I get back and there are slide sets that are quite similar to it already.",
            "If you're interested in seeing more of the references, for example, and if some of you get interested in some of the stuff that I didn't get to talk about, your most welcome to write me also.",
            "And of course later in the afternoon if you can find me, I will be happy to talk about it.",
            "If you saw something and you said you'd like to discuss that in more detail.",
            "The lot of challenges to these simple factor models I should say right away, so maybe one of the more important one is that it may be way too simple to assume that there's a linear relation between the two sets of variables, and for that reason we have looked a lot into two nonlinear manifold description so that A&S kind of determines one value of this.",
            "Let's say the active state is like one curved manifold in this space rather than just being a linear subspace.",
            "And then the different states are maybe different manifolds or different parts of the manifold.",
            "So so this evolves around to nonlinear generations of the linear factor model.",
            "And of course there's a lot of temporal structure in these networks also, so they may have parts that are not In Sync.",
            "So both for fMRI and four.",
            "For EG, signals all the temporal dynamics would give you this idea that maybe different parts of the networks are not exactly In Sync, so the models that we have, they take a pattern and then they activated by a single time function, right?",
            "So this means that everything is In Sync in a component and this is way too simplistic for segments.",
            "So This is why sometimes these convoluted models that can allow different delays and so on.",
            "I used and also.",
            "There can be problems both that they can be too rich innocence.",
            "So if you have really high dimensional types of measures where there are lots of measurements going into to one experiment, for example multiple subjects, then the factor model can actually be too rich in a way so that could be too many parameters that you start to fit.",
            "So this means that you have a really massive random effects model, is what the equivalent of that is, and then you may need to do something which is simpler, so similar representations that try to detect conditional independence is that are so strong that you can decode.",
            "Modes for example.",
            "I'm not going to speak much about that, but this needs to these certain models that people refer to as multi way models like tensor algebra and so on.",
            "So."
        ],
        [
            "Just say a little bit about nonlinear models and why why we started looking at this, so this was one small experiment on the same data set as.",
            "So this visual stimulus type experiment that I showed before we have a strong.",
            "Stimulus at certain intervals.",
            "Here are regularly spaced intervals and now we look for the response.",
            "So this is the output of the classification algorithm and these are this classification algorithm works in two steps.",
            "So first there's this kind of unsupervised algorithm that kind of finds a better space to look at.",
            "So by principal component analysis would find a low dimensional space that represents hopefully the interesting structure that we have in the high dimensional space, and then you build your classifier that low dimensional space.",
            "OK, that's a very typical later today.",
            "Machine learning also to reduce dimensionality by.",
            "By the factor models.",
            "Now, if you do that, you can reduce dimensionality by PCA and get something like this.",
            "So here we show where the algorithm believes there's activation in five repetitions of the baseline activation sequence, and you can see that it pretty much gets it's it's a very low error error rate.",
            "I mean down to a few percent right?",
            "So it's really good at predicting it has to do with this really good signal to noise ratio that we had.",
            "But there are some instances, for example here within the middle of the baseline thinks that there's a stimulus so that it's clearly mistaking something here.",
            "And.",
            "Anthony, what we saw was that if we use something that projected nonlinearly answer to curved manifold rather than linear subspaces, then this went away.",
            "So this is the output of the nearest neighbor type.",
            "So just using this neighboring relations between scans so classify in the in the manifold space does a little bit better job, so it has a little bit better error rate, smaller error rates, but also it doesn't make this kind of the errors are now on the the transience here where it goes from baseline to active, where you would expect it to be uncertain.",
            "But now it's not here in the middle of something.",
            "So many of you may have heard about."
        ],
        [
            "The nonlinear generations of principal component analysis, like method done.",
            "Bring out here the equal PCA that was developed here in Berlin.",
            "So the idea is to.",
            "To take higher order dependencies between data into account, not just the covariances or or the parameterized dependencies that we have in the linear factor model, but allow for much more general manifold like dependences.",
            "So we do that by by shooting it into the data into some space that we don't make ever explicit, but implicitly it's very high dimensional that we call fire here and then we look for linear factor model so that, for example maximizing variances in principal component analysis in this space.",
            "So we find projections of the new vectors fire effects.",
            "So we have one for each data point.",
            "Have you want to find vectors in this high dimensional space?",
            "That explains most of the variance.",
            "So in effect running the factor model in the feature space instead.",
            "Because the this model is a linear one, then it turns out that all that we need now in this feature space the covariances and this is what this implication is that the manifold is in this mapping, not in the statistical model that we do.",
            "And now we have to parameterise the covariance.",
            "So this would be the relation the dependency between two scans in and prime here.",
            "And the typical thing that people do in kernel PCA simply to use a neighboring something that says that if things are closed, they should be more dependent than if they are far away.",
            "But by using this week definition, we can capture still manifold.",
            "So a manifold, something that if you look really close, it looks like a linear subspace.",
            "But if you go far away it curves so locali it still neighborhood neighborhood.",
            "But on the global scale neighborhoods are defined in a more complicated way, so that's the difference between these algorithms.",
            "So locally they are simple like what we had before.",
            "Open is there much more complicated.",
            "So when we do that, we can.",
            "We can do this kind of projection."
        ],
        [
            "In in a way that that takes for example, like this little simulated case here, where data is distributed on a circle rather than linear manifold, we can reduce dimensionality to one again.",
            "So we have two dimensional data, and now we can reduce it to by projection onto a circle for example.",
            "So this works really beautiful for this type of mapping that I talked about because locally of course it looks like a linear projection, but globally it's a circle, right?",
            "So now using this mechanism we can get the projection in feature space by simple PCA.",
            "But the problem is we need to get back to real space in order to see what is the result of the projection.",
            "And this leads to a whole lot of interesting complications.",
            "This mapping into feature space and mapping back and result also hear from Berlin by by the same group.",
            "Um?",
            "Showed us one way to do it and it turns out that this because of the non linearity that goes in that there's a lot of practical problems once you start to apply it.",
            "So part of what we did was to to invest."
        ],
        [
            "Gateways of stabilizing the pre image and there's some papers by it's been Abrahamson myself where we modest have solved some of the instabilities by using regularization and."
        ],
        [
            "Some regularization.",
            "That means that we don't allow it to do too big and noise reduction.",
            "So without constraints on how how crazy noise reductions we can do, and this pretty much solves these instability problems.",
            "And we of course we can do this in different ways so we can do it by like soft regularization by that some of you may have seen simply by constraining length of vectors we can also do more interesting regularization's like.",
            "Constraining the absolute values of the images in.",
            "This turns out that this promotes what is called sparsity that many of the parameters they will go to zero rather than taking some finite value.",
            "And there's some work here on regularization of the pre images with sparsity promoting.",
            "Prius or regularization terms that that worked really well for your images because new images.",
            "We precisely hope that we have areas that are not involved and in some areas that are involved and those are the ones that would like to find this as as noise.",
            "So we can use the Represe Bility performance plot again to optimize all the parameters that go into this.",
            "So if you remember, we."
        ],
        [
            "At this time.",
            "Function here the covariance matrix that determined how far away two things were supposed to be correlated.",
            "So you can see that here is the distance between the scans in the high dimensional space.",
            "The squared distance is measured relative to the scale.",
            "See now there's a.",
            "There's a kind of locality, so how much does it curve if this parameter C is very big, then you can have very big distances that are connected and then you have almost like a flat surface, and if she's really small it'll be very curved and very nonlinear, so this is controlling the nonlinearity of the map.",
            "And this."
        ],
        [
            "Is something that we would like to know.",
            "So we propose different models, different nonlinearities, and be evaluated against this reliability performance.",
            "You might, so we have.",
            "Classification performance up here in Brazil to here and maybe these models here that have you can see that this is very high.",
            "It's in the 95 range, right?",
            "So these are almost the same performance, but the rivers ability changes also maybe bye bye affected by.",
            "So the 10% or more right?",
            "So these models are much more reproducing than these ones are.",
            "Therefore, we choose that those parameters.",
            "Um?"
        ],
        [
            "That said, if you want to see more of the details and also some real experiments with these methods have been used and to evaluate nonlinear denoising, there's this recent neuromed paper that that shows that how to do it and then out to what results to expect.",
            "Let me end this by by skipping again a hole."
        ],
        [
            "Things unfortunately talk a little bit about supervised learning at the end.",
            "OK.",
            "So now of course we are much better off because with supervised learning.",
            "We can use the label structure directly in the model.",
            "We're not supposed to find it ourselves, right?",
            "So this is a situation where we should expect to get good results, right?",
            "It's like hypothesis testing in machine learning sense in the machine learning sense.",
            "So."
        ],
        [
            "What we do, for example, in the standard case that we have a set of labeled scans, how we model that?",
            "So the thing that people would maybe think of 1st would be to do a nonlinear kernel machine like the support vector machine.",
            "So again, we use the kernel from before describing what is what is closer when our neighbors.",
            "Neighbors and when they're not using a scale parameter again to determine how broad the neighborhoods are.",
            "And then the SVM we use the linear expansion on such kernels so that these coefficients.",
            "Here they can be plus or minus.",
            "So think of this as a variable that if it's plus it's active and it's negative, it's inactive, so a simple binary classifiers what we're building here on one side of the classifier.",
            "Decision boundaries is positive, the other side is negative.",
            "We want to find the point where it changes from zero to excuse me with zero from going from minus two to plus for example.",
            "So this is implemented as a local averaging scheme where we have this function here describing what is it.",
            "What are the neighbors that participate in the voting?",
            "So if I'm classifying pattern in prime, if XN is too far away from X prime, then this will be larger than this whole expression found be small and it doesn't give a contribution to the voting error.",
            "And the voting it comes about because these have positive and negative numbers that they add into the vote.",
            "For the SVM in load."
        ],
        [
            "Mental cases, many of these will be 0 if you use them for high dimensional problems like we're talking about here.",
            "If X is the whole image for example, then typically not many of them will be zero.",
            "I will get solutions that are quite dense on the on the alphas.",
            "So how do we visualize this?",
            "So we need to create a statistical parametric map.",
            "Now for the SVM."
        ],
        [
            "So of course we use the the method we already talked about, so there are different versions of the preimage that one could be."
        ],
        [
            "Matching using, but this will give us local visualization.",
            "We simply use this expression here.",
            "So I'm going to use precisely the scheme that I talked about before I'm going to take the SVM.",
            "Think of it as a simple generative model, and then compute derivatives of the kernel function, right?",
            "This is what this will amount to.",
            "And this will give us again an indication of every voxel that participate in the classification.",
            "How sensitive is the result?",
            "So that box is value and use that as a measure of involvement and this will produce a map.",
            "And here are some examples of the SVM.",
            "Here is in the middle."
        ],
        [
            "So this is again a particular fMRI study.",
            "There's something that's having going on, so we're looking at slices that have motor cortex for example.",
            "Down here, motor cortex involvement, and cerebellum involvement, and so on.",
            "And here we show that such a sensitivity map for an SVM that's been trained.",
            "So of course, once we have this weekend, we can compare it with other things as well, so that for those who have seen other alternatives that sub Asian alternative to the SVM that is called the relevance makes a machine that some would prefer to use because it gives more directly access to probabilities and you get very similar.",
            "Maps for the two.",
            "If you do simple logistic regression on Kern in kernel space, so this is using the same model but just a slightly different way of estimating the Alpha parameters.",
            "SVM, the SVM has one way and this has a different way using a likelihood function we can see that we get even more similar results in this case, so the.",
            "SVM is quite similar to logistic regression in encoding space as you would expect, because they are so similar in in.",
            "In specification.",
            "Of course you can plot these, so here we have volume so you can plot the pixel values and you can see that there is a lot of agreement between the two, so there's simply a scatter plot of the two Maps here.",
            "So not only can we.",
            "Use this kind of comparison to find out how similar the same model is on different datasets we've got.",
            "We can also use the same idea to to ask how similar two different models.",
            "That's what this plot.",
            "Explains"
        ],
        [
            "No.",
            "This slide here is a small simulation showing that this scheme is really super efficient in detecting complicated.",
            "Types of events in the imaging experiment.",
            "So of course this is not a real imaging experiment for assimilation.",
            "And the way the simulation setup was to say if we use linear discriminants in space, this means that we use functions that separate one state from the other by a simple hyperplane, right?",
            "So we have a linear model essentially in space.",
            "Then that types of events that we cannot separate and one very very simple pattern we cannot separate is that.",
            "Imagine you have four groups of events that can save place.",
            "Antonym, so I'm going to to make a little mental pictures.",
            "We have four groups of data and these two here are in one class and these two in the other class.",
            "Then there's no way that you can cut this by a linear separating hyperplane and get 2 on one side and two on the other sideline.",
            "This in engineering, this called the X or problem, so this is an image in version of the X or problem.",
            "Where am I have four regions that could be like areas that are producing activations or both signal and the two of them are coupled to the activation.",
            "So I have a baseline on activated instead of scans here and these two guys here are couples in this X or way.",
            "So this means that in one state.",
            "So this is the baseline these two.",
            "Are the same.",
            "They change my entirely in time like a binary sequence, so this what you see the red signal here, but in in the baseline they're the same and then the activated their difference.",
            "OK.",
            "So this is a very complicated multilateral relation between the stimulus and the response in the brain that we would not be able to detect because this has precisely the structural.",
            "The two other regions here, they're just random strong variances that simply inject noise, so this would be something like resting state patterns.",
            "For example, right that are just binary strong signals that go on and off at times.",
            "They are not associated with the stimulus and just running this algorithm that we show that we train a classifier to detect.",
            "So we have a nonlinear kernel method that can get a low error rate on such a problem here and now we run the.",
            "This visualization scheme aside, the sensitivity map and these are the sensitivity Maps that you get, so this isn't a good signal to noise ratio where the individual image looks like this.",
            "And this is in an even worse.",
            "We cannot even by the I you cannot see that, that is the patterns, and the noise is so strong that these four regions are simply disappear, right?",
            "So this is the sensitivity map that you get and you can see that there's a high value in the two areas that I taking.",
            "Part of the activation and very little in the confounders here that are the orthogonal sense.",
            "This is the true sensitivity map.",
            "If you would like to see.",
            "This is the RC curve between the two of these measures.",
            "How well if we cut this map in different levels, how well does it match this one?",
            "And of course your it's perfect.",
            "So this way of using nonlinear Maps and visualizing them using the sensitivity map can really detect the origin of really complicated interactions.",
            "This is what we show by this this example, so I can recommend you to do that for.",
            "Paradigms where you expect to have this kind of modular Tori.",
            "Phenomena going on in your data, right?",
            "Then you would not be able to see it with a linear discriminants or linear SVM.",
            "Escape a bit more here and."
        ],
        [
            "Go directly to two conclusions.",
            "So I think the most important message I did get through that right.",
            "This is that when we do machine learning in scientific data mining task like brain imaging, there always needs to agenda that.",
            "We would like them to identify the model and we would like to understand what it tells us about the data.",
            "And those two are about the same footing, right?",
            "I mean the equally important in many ways.",
            "And this is possible in the brain mapping case because we have a formal way of evaluating the interpretation we can do, we can simply get confidence interval on the visualization that we get the brain map.",
            "And we can do that for virtually any model that people use, right?",
            "Using these schemes are so general that you only need a likelihood function that can be directed.",
            "You can compute the derivative off, that's the only requirement that I have.",
            "Um?",
            "If we want the confidence interval, then we need to do some kind of resampling to measure an unbiased value of the variance, or find confidence intervals as I said and split half can do that.",
            "Some more complicated mechanisms can be modeled also.",
            "Then simple linear models.",
            "If we go to nonlinear manifolds in the unsupervised case or two nonlinear SVM's in the supervised case.",
            "And I should say that it's far from over like.",
            "I mean it's it's really still very much there.",
            "Lots and lots of things that one could discuss, and I hope that many of you will be interested in doing this and discuss it with us in.",
            "On the private or in conferences other places.",
            "I mean, there's so many things to to still investigate and let me give you a little bit of some outlook.",
            "What we're doing right now in Copenhagen.",
            "Um?"
        ],
        [
            "So one of the things that we're working on is to try to bring this whole framework into the wild.",
            "So we would like to take the Brain Imaging experiment outside the MRI scanner is not.",
            "It's not feasible at the moment to really do MRI scanning in large volumes.",
            "I mean there are some ideas to how to do that and may people may actually realize it at some point, but right now it's not feasible.",
            "So we have to go to another modality and EG.",
            "For example, would be such a let's see, that would be easier to move around.",
            "And this would take us from a situation where the brain is considered almost like a black box might be stimulated and see what happens to the brain, but it's not of the brain that stimulate itself.",
            "Now, some paradigms that are being developed now where that is the case and this is moving in the right direction, but only if we allow the brain imaging experiment to go on in the real world on the real interaction with other people, for example, it's going to be an ecological, valid experiment where you could say that this is a brain.",
            "Not an experimental subject.",
            "So that's the vision that we would like to to to create this too.",
            "And I should say also that we are definitely not alone in this.",
            "I mean, there's really lots and lots of activity.",
            "This area that are pursuing all kinds of tricks to create mobile brain imaging.",
            "So I'll take at the moment is is 2 fold.",
            "So we have one system that is based on on mobile smartphone platform.",
            "At the moment user very simple headset.",
            "So this is actually a very inexpensive setup also.",
            "So you can imagine doing it in a large population.",
            "Typical research, but yet this is the emotive Epoc headset that we use that speaks wirelessly to to a smart phone.",
            "And then you can do whatever type of experiments or things that you're interested in on the smart phone.",
            "Smart phones are really potent, so here, for example, we do real time 3D reconstruction of the huge segments we tried to reconstruct the source locations on a simplified brain, so it has like 1000 points in this particular implementation.",
            "All this code that runs in this case it's on a Nokia phone, but now it's been ported to.",
            "To an Android platform, all this code is available if you.",
            "If you're interested, you can download it.",
            "And use your own emotive two to set it up on your own smartphone.",
            "So this will allow us to do experiments on a much larger scale than than has been possible earlier on, for example, in in you could call it social neuroscience experiments, right?",
            "If you're really interested in learning about how what happens that much longer, times K, so this would be over years.",
            "For example, in a subject.",
            "So that's a completely different type of experiment that nobody is.",
            "That's really done in.",
            "Healthy population or even.",
            "In patients, that would be something like this.",
            "So this is a company that we're working with incorporating on hyper safe, so they built this little device here that is even higher comfort or much higher comfort than the motive that is implanted under the skin.",
            "And then it speaks again.",
            "Through a coupling.",
            "So in this case it's violence.",
            "So here's a magnetic coupling to a device that looks like a hearing aid that sits outside.",
            "And the clinical trials are going on at the moment with these devices.",
            "And there has been one round of clinical trials where people wore this for a month.",
            "And in the final device that they built, I mean they they expect to have this.",
            "Implanted for years and the patient group that they are looking at at the moment is.",
            "Diabetic population that cannot detect low blood sugar.",
            "So if they over regulate that treatment they get low blood sugar, which is dangerous.",
            "You can save them.",
            "Unpredictably, and so on and then using e.g you can detect those states in a small window before it happens and and this is the particular device that they're building at the moment is aimed at.",
            "But of course as a neuroscience type device, it's interesting from any other possibilities also.",
            "OK, let me."
        ],
        [
            "Um?",
            "Just say the ransom references here, then some acknowledgments, and then thank you for the patience.",
            "And I'm sorry that I've went overtime, but I hope it's worth it.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what I'm going to speak about today is.",
                    "label": 0
                },
                {
                    "sent": "Is a collection of methods that we have used for for for machine learning of mainly fMRI and most of my examples will be an fMRI.",
                    "label": 0
                },
                {
                    "sent": "I will speak a little bit about e.g at the end.",
                    "label": 0
                },
                {
                    "sent": "Also in showing a recent example of how we apply this now too too.",
                    "label": 0
                },
                {
                    "sent": "E.g both imaging and also analysis.",
                    "label": 0
                },
                {
                    "sent": "But most of it will be F MRI.",
                    "label": 0
                },
                {
                    "sent": "I think the the particular type of data is not so important.",
                    "label": 0
                },
                {
                    "sent": "It's not so much part of the take home message is really what we do to design and particularly valuate and visualize these methods.",
                    "label": 0
                },
                {
                    "sent": "Is is what I'd like you to to bring from here.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "As I said, this is something I've been going on for awhile and most of it has been in collaboration with my friend Steven Strawder from from Toronto.",
                    "label": 0
                },
                {
                    "sent": "Um first in in a big American project, one of the human brain projects that were launched in.",
                    "label": 0
                },
                {
                    "sent": "In the in the early.",
                    "label": 0
                },
                {
                    "sent": "Miss Midnight season went into the early 2000s, and then of course a lot of my local collaborators from from Copenhagen that have contributed to this.",
                    "label": 0
                },
                {
                    "sent": "The students and colleagues at the moment.",
                    "label": 0
                },
                {
                    "sent": "So here's my outline.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want to 1st convince you that when you do machine learning in this context, there are really two equally important at the end as that we are trying to.",
                    "label": 0
                },
                {
                    "sent": "To meet so one is the usual one in machine learning, that is that we would like the machine learning algorithms.",
                    "label": 1
                },
                {
                    "sent": "To identify the statistical processes that we see in nature in this case in in the brain Imaging experiment, and for this we use the usual cross validation like resampling experiments.",
                    "label": 0
                },
                {
                    "sent": "But then of course in scientific data mining, as this is an example of, there's another agenda.",
                    "label": 0
                },
                {
                    "sent": "There's the agenda that once we have those processes identified would like to understand them better, and this is a much more ill defined agenda.",
                    "label": 0
                },
                {
                    "sent": "I would say in the 1st place I mean, what is it that would like to get?",
                    "label": 0
                },
                {
                    "sent": "Is it a picture, is it?",
                    "label": 0
                },
                {
                    "sent": "Some kind of causal explanation, and this is where I see a lot of really interesting research going on at the moment.",
                    "label": 0
                },
                {
                    "sent": "Is trying to formalize that second agenda, and this is probably what is most of the novel here is that I have two axis in my evaluation, so most often you will see something like a test error as a function of of data.",
                    "label": 0
                },
                {
                    "sent": "That's what we call a learning curve.",
                    "label": 0
                },
                {
                    "sent": "I'll show some examples of that, or it's a function of complexity of the algorithm that it's a bias variance tradeoff curve.",
                    "label": 0
                },
                {
                    "sent": "And I have a second axis in these plots, and that's how reliable is the explanation.",
                    "label": 0
                },
                {
                    "sent": "Trying to see if I can formalize that as well.",
                    "label": 1
                },
                {
                    "sent": "So I'm going to give you an example right away of how this works in a particular case, and then I'm going to show some examples of that both in unsupervised.",
                    "label": 0
                },
                {
                    "sent": "So this means we're looking for structure and data, and in supervised learning much more strong form of machine learning where you're trying to model a particular relation between two sets of variables and then at the end I'll speak a little bit about what I see.",
                    "label": 0
                },
                {
                    "sent": "Is this some future possibilities here?",
                    "label": 0
                },
                {
                    "sent": "One of my heroes is this fellow here that many of you may recognize.",
                    "label": 0
                },
                {
                    "sent": "He looks a bit sort of.",
                    "label": 0
                },
                {
                    "sent": "Concerned here, and his statement is do not multiply courses, and I think that's one of the more general things that one can take from this.",
                    "label": 1
                },
                {
                    "sent": "Also, both for the visualization and for.",
                    "label": 0
                },
                {
                    "sent": "For the generalizability issue on the How will you define the processes that one should always try to find the simplest explanation and not multiply causes beyond what is necessary, so we'll try and see if we can formalize that into a mathematical or statistical framework, that's the.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's the purpose of reviewing some of this.",
                    "label": 0
                },
                {
                    "sent": "I should say that I'm not going to review machine learning in fMRI.",
                    "label": 0
                },
                {
                    "sent": "Because that's such a big subject that that would not be possible.",
                    "label": 0
                },
                {
                    "sent": "I'm going to take a very sort of ecocentric approach and focus mostly on what we have done ourselves.",
                    "label": 0
                },
                {
                    "sent": "If you're interested in other super interesting work that is going on and have been going on for a long time now in in the machine learning applications in an fMRI, I mean valleys.",
                    "label": 0
                },
                {
                    "sent": "All these wonderful reviews and many more.",
                    "label": 0
                },
                {
                    "sent": "Actually these are just from one Journal named in your image.",
                    "label": 0
                },
                {
                    "sent": "See.",
                    "label": 0
                },
                {
                    "sent": "And videos were several years and special issues.",
                    "label": 0
                },
                {
                    "sent": "Some of these papers are from special issues that have been devoted to this kind of.",
                    "label": 0
                },
                {
                    "sent": "We're thinking about brain imaging.",
                    "label": 0
                },
                {
                    "sent": "I'll put these slides up on the website when I get back.",
                    "label": 0
                },
                {
                    "sent": "Thus, some of these nights already actually.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in summary, I let me just I'm sure most of you is not all are very familiar with the technique, so this is an example of an F MRI data set.",
                    "label": 0
                },
                {
                    "sent": "So this is actually one of the datasets that I have seen ever with the best settings or noise ratio.",
                    "label": 0
                },
                {
                    "sent": "So this is a an experiment where we have a subject in an MRI scanner and the subject is looking at the visual stimulus 8 Hertz flickering checker checkerboard.",
                    "label": 0
                },
                {
                    "sent": "And you can imagine how that looks right if you haven't tried it, you can imagine that this is a very strong stimulus.",
                    "label": 0
                },
                {
                    "sent": "You see all kinds of artifacts.",
                    "label": 0
                },
                {
                    "sent": "I mean, you see colors and things that are moving and so on.",
                    "label": 0
                },
                {
                    "sent": "Because this is such a strong stimulus to the visual system.",
                    "label": 1
                },
                {
                    "sent": "So this slices, like here, parents with Carolyn Celko's, it's a single slice and this means that we can acquire that at this page here.",
                    "label": 0
                },
                {
                    "sent": "3 Hertz acquisition.",
                    "label": 0
                },
                {
                    "sent": "So this data set has been used for tuning a lot of different models in our lab, almost like a simulation.",
                    "label": 0
                },
                {
                    "sent": "You could say, because we know exactly what to expect here.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "And we have some of the artifacts that they haunt.",
                    "label": 0
                },
                {
                    "sent": "fMRI, like the other hemodynamics that I induced by facility.",
                    "label": 0
                },
                {
                    "sent": "For example, we have sampling frequencies that are so high here that we that we have the heartbeat, for example, samples.",
                    "label": 0
                },
                {
                    "sent": "But there are many challenges to to fMRI, and This is why it's so interesting for us to to apply machine learning methods.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Some of them are here is that they are multidimensional mixtures.",
                    "label": 0
                },
                {
                    "sent": "So what I mean by that is that whatever we see in a particular location is a mixture of many different physical processes.",
                    "label": 0
                },
                {
                    "sent": "Some of them have to do with information processing.",
                    "label": 0
                },
                {
                    "sent": "So how the brain reacts to this particular stimulus here in the visual system.",
                    "label": 0
                },
                {
                    "sent": "So let's see if we can start it again and now you will see that this is resting.",
                    "label": 0
                },
                {
                    "sent": "And now we start the signal coming on.",
                    "label": 0
                },
                {
                    "sent": "So this is the signal.",
                    "label": 0
                },
                {
                    "sent": "The stuff we see down here, right?",
                    "label": 0
                },
                {
                    "sent": "And you can see that there's a very high noise level.",
                    "label": 0
                },
                {
                    "sent": "Most of it looks like TV noise, right?",
                    "label": 0
                },
                {
                    "sent": "So this is what we're up against so often I mean the confound us.",
                    "label": 0
                },
                {
                    "sent": "They have all kinds of interesting physical origin, these confounding signals.",
                    "label": 0
                },
                {
                    "sent": "And they have higher variance often then.",
                    "label": 0
                },
                {
                    "sent": "Then the stuff that we're looking for, and they are modeled at different types of mixtures, and some of our algorithms are trying to teach those mixtures apart by blind signal separation.",
                    "label": 0
                },
                {
                    "sent": "Trying to answer this kind of question, what is signal is noise, right?",
                    "label": 1
                },
                {
                    "sent": "Or papers was is called?",
                    "label": 0
                },
                {
                    "sent": "So just to show you.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An example of where this how we used this idea of both evaluating the accuracy of the process identification so the generalizability of the machine learning algorithm and at the same time the information that we got from the model, I'll just show you this little example here will not give you much of the details, but just sort of show the flavor of the type of argument that I'm going to to bring out a number of times.",
                    "label": 0
                },
                {
                    "sent": "So this was an example of where we try to model the.",
                    "label": 0
                },
                {
                    "sent": "The process that generates the so called boat signal that we acquire in MDMA, fMRI in the MRI scanner and the bold signal is pretty complicated.",
                    "label": 0
                },
                {
                    "sent": "As multiple not independent but related components.",
                    "label": 0
                },
                {
                    "sent": "So it has to do with the blood volume it has to do with the block blood oxygenation level and the blood flow.",
                    "label": 0
                },
                {
                    "sent": "So those three together come together to form the boat signal and this has been modelled in several models and one of the more popular ones is called the balloon model.",
                    "label": 0
                },
                {
                    "sent": "It's a nonlinear model, so the idea is that the from the basic neuronal activation in an area that make up, let's say one voxel of 1 pixel.",
                    "label": 0
                },
                {
                    "sent": "In the experiment there's all this multi level Physiology going on that that will change the bold signal in that box look right and the model has a number of compartments and there altogether and number of parameters handful of parameters that we would like to know of.",
                    "label": 0
                },
                {
                    "sent": "And if we have.",
                    "label": 0
                },
                {
                    "sent": "A stimulus, say like the one we saw before, and we see the responding response in terms of the build signal.",
                    "label": 0
                },
                {
                    "sent": "Then we can try and see if we can identify those parameters.",
                    "label": 0
                },
                {
                    "sent": "So we set up a set of nonlinear differential equations.",
                    "label": 0
                },
                {
                    "sent": "That involve these different compartments and how they interact, how things feed into other compartments and so on.",
                    "label": 0
                },
                {
                    "sent": "That's the balloon model.",
                    "label": 0
                },
                {
                    "sent": "The parameters are unknown, so those will have to estimate it.",
                    "label": 0
                },
                {
                    "sent": "So what we do is that the first of all we set up a system that can integrate these equations for for the Times Band that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "That's that's that's one step of the process for fixed fixed parameters.",
                    "label": 0
                },
                {
                    "sent": "Then we can compare the solution to to the actual measured signal, right?",
                    "label": 0
                },
                {
                    "sent": "So this is like running the model forward and this means that we can put forward now kind of likelihood function as statistical models saying how accurate is the model with these parameters.",
                    "label": 0
                },
                {
                    "sent": "So that's likely function right?",
                    "label": 0
                },
                {
                    "sent": "We assume that we know the parameters that we can see how close is the simulated signal to the two signal.",
                    "label": 0
                },
                {
                    "sent": "So this is what we can do here, and this means that we can now do Bayesian sampling in the space of parameters.",
                    "label": 0
                },
                {
                    "sent": "So in this 5 dimensional space, for example, right, we can do patient sampling, we get, say Gibbs sampling.",
                    "label": 0
                },
                {
                    "sent": "We can run the forward model, find another sample, run the forward model again and then we can get a sample from the posterior distribution in parameter space.",
                    "label": 0
                },
                {
                    "sent": "So this what we're typically doing in machine learning is that will try to say something about the parameters of the model given the data.",
                    "label": 0
                },
                {
                    "sent": "That's the posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "But, and we will often be happy about the posterior distribution in the sense that it can predict the signal right.",
                    "label": 0
                },
                {
                    "sent": "This is how we will evaluate, maybe on a test signal.",
                    "label": 0
                },
                {
                    "sent": "But if you're interested in Physiology like we were in this particular case, and there's a particular question we wanted to answer, so these were two different versions of the model that had two different types of neural input.",
                    "label": 0
                },
                {
                    "sent": "One was rectangular, like a.",
                    "label": 0
                },
                {
                    "sent": "The continuous activation while we stimulate and the other one was a fading stimulus with two competing hypothesis, we want to see which of these models are do the best job here.",
                    "label": 0
                },
                {
                    "sent": "So we run this with a typical run of the Mill Basin analysis to get a posterior distribution.",
                    "label": 0
                },
                {
                    "sent": "But now we ask the question that is not often asked how robust is the posterior distribution?",
                    "label": 0
                },
                {
                    "sent": "How much do we actually learn from that posterior distribution?",
                    "label": 0
                },
                {
                    "sent": "So the way we measure that was by the technique that I'm going to speak more about it.",
                    "label": 0
                },
                {
                    "sent": "Namely, Split has a resampling.",
                    "label": 0
                },
                {
                    "sent": "So we split the data set in two halves and built the whole posterior distribution on those two halves.",
                    "label": 0
                },
                {
                    "sent": "I get two different posterior distributions.",
                    "label": 0
                },
                {
                    "sent": "Now if everything was a OK, they should be identical, but of course because the data is noisy and small and so on, there not, and it's a quality of the model, and in this case how much we learn from the model that the posterior distribution is a similar between the two sets as possible.",
                    "label": 0
                },
                {
                    "sent": "Now we cannot simply say that we want something that is as simple as possible, because then we lose the other part.",
                    "label": 0
                },
                {
                    "sent": "The generalizability, because then we lose the expressiveness of the model is very simple.",
                    "label": 0
                },
                {
                    "sent": "Model will always be more more similar in every sampling.",
                    "label": 0
                },
                {
                    "sent": "For something, it just outputs zero, will always be the same.",
                    "label": 0
                },
                {
                    "sent": "So we have to do both, right?",
                    "label": 0
                },
                {
                    "sent": "We have to make sure that it predicts and predicts well at the same time we would like it to be robust under resampling so that we can be sure that the posterior distribution we get is trustworthy.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In this case we have we have two different datasets and so these are the two experiments here.",
                    "label": 0
                },
                {
                    "sent": "A and B1 is the one I showed you before, and this is another similar one that.",
                    "label": 0
                },
                {
                    "sent": "So this is a slice and this was a volume experiment that we've run just to to make it a little bit richer in the type of experiment that we could explain.",
                    "label": 0
                },
                {
                    "sent": "And then we have the two models sustained your input and fading input, and these are the two different simple tiers of squares are the fading input and the sustained neural input is the thing with the Diamond Tour and where we want to be is the following where we want to like to understand this plot is the following way and we want to be up in this corner here because out of 1 access I have generalizability.",
                    "label": 1
                },
                {
                    "sent": "So this means how well do my model predict the data in test in the test set, right?",
                    "label": 0
                },
                {
                    "sent": "That's the usual generalization that we have out here.",
                    "label": 0
                },
                {
                    "sent": "And it's good to be high in this case, and rebels ability was.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This measure of similarity so heavy use the KL distance between the two posterior distributions.",
                    "label": 0
                },
                {
                    "sent": "Simply ask how similar are they in information measure?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Cable distance as you see here mutual information.",
                    "label": 0
                },
                {
                    "sent": "Ansome this brings us to the following plot.",
                    "label": 0
                },
                {
                    "sent": "So let's take the first one.",
                    "label": 0
                },
                {
                    "sent": "So this was this experiment.",
                    "label": 0
                },
                {
                    "sent": "You can see that the simpler model that has sustained neural input is much more.",
                    "label": 1
                },
                {
                    "sent": "Good at your eyes at you, realizing it has a consistent high value, the corresponding to the highest values that you see in the other model.",
                    "label": 0
                },
                {
                    "sent": "But the responsibility is much better.",
                    "label": 0
                },
                {
                    "sent": "Case, it's also good to be top of this axis, minus Ko.",
                    "label": 0
                },
                {
                    "sent": "So what we learn here is that this model is actually good at your last ability, but it also giving us something which is more trustworthy.",
                    "label": 0
                },
                {
                    "sent": "And this is how we're going to evaluate.",
                    "label": 0
                },
                {
                    "sent": "So we prefer that model over the other one.",
                    "label": 0
                },
                {
                    "sent": "So that's the.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's the trick.",
                    "label": 0
                },
                {
                    "sent": "So now I'm going to give you a little bit more.",
                    "label": 0
                },
                {
                    "sent": "A formal description of what is going on, including how we set up the different types of models and so on, and show you some examples from unsupervised learning and some examples from supervised mode.",
                    "label": 0
                },
                {
                    "sent": "So, um.",
                    "label": 0
                },
                {
                    "sent": "If you want to formalize the type of experiment that we are doing here, we have to first of all of course define the variables that go into it.",
                    "label": 0
                },
                {
                    "sent": "So we do brain scans.",
                    "label": 0
                },
                {
                    "sent": "So this means that we acquire some kind of high dimensional signal as a function of time, for example.",
                    "label": 0
                },
                {
                    "sent": "So this would be that video that we saw before.",
                    "label": 0
                },
                {
                    "sent": "But we have a 2 dimensional like an image evolving in time, so that would be like a video.",
                    "label": 0
                },
                {
                    "sent": "And of course if it's a 3 dimensional experiment, is like a 3 dimensional video.",
                    "label": 0
                },
                {
                    "sent": "Yes, so that's this set of variables that we've called X here and the nature of those variables is that they.",
                    "label": 0
                },
                {
                    "sent": "A high dimensional so they reflect the state of the brain in a local way.",
                    "label": 0
                },
                {
                    "sent": "So we know something about particular areas, how they evolve as function of time.",
                    "label": 0
                },
                {
                    "sent": "And that I don't think you can call it micro variables because they have caused you.",
                    "label": 0
                },
                {
                    "sent": "Still interesting something about neurons, but we don't have.",
                    "label": 0
                },
                {
                    "sent": "We do not have access to neurons in in Moscow with access to this coarse grained average on roughly a couple of millimeters.",
                    "label": 0
                },
                {
                    "sent": "5 millimeters maybe.",
                    "label": 0
                },
                {
                    "sent": "So there are many many neurons, so this was sometimes called these mesoscopic variables.",
                    "label": 0
                },
                {
                    "sent": "I mean they but they still have this that they tell us something about what happens locally in the brain.",
                    "label": 0
                },
                {
                    "sent": "And we want to find the relation between those variables and then the kind of variables that describe the whole brain as as a as a body.",
                    "label": 0
                },
                {
                    "sent": "So those would be for example variables that describe the stimulus.",
                    "label": 0
                },
                {
                    "sent": "So in the simple case where we had this block design where we had resting and then activated resting, activating again and so on, it would be a square wave type function like this S and there would be a single variable and will be a single binary variable in.",
                    "label": 0
                },
                {
                    "sent": "One way to think about it zero when you are interesting and.",
                    "label": 0
                },
                {
                    "sent": "Interesting States and one when you're simulating, but it could be much more complicated.",
                    "label": 0
                },
                {
                    "sent": "Also, if we had other measures on the on the on the subject.",
                    "label": 0
                },
                {
                    "sent": "We could add those to this so the idea is that these are local and these are global variables.",
                    "label": 0
                },
                {
                    "sent": "Of course, they also sometimes have different structure in the sense that we can control some of it, but not the other.",
                    "label": 0
                },
                {
                    "sent": "That's sort of a different level of awful things.",
                    "label": 0
                },
                {
                    "sent": "Here we simply think of them as having those two different qualities that they're kind of local.",
                    "label": 0
                },
                {
                    "sent": "Mesoscopic and macroscopic variables and, of course, that the objective of the of this whole exercise is to get something that can tell us about the music nation between the two sets of variables.",
                    "label": 0
                },
                {
                    "sent": "So in the methods that I'm going to talk about then.",
                    "label": 0
                },
                {
                    "sent": "Question that maybe sometimes S. This is not even known before hand.",
                    "label": 0
                },
                {
                    "sent": "So this is what we were called unsupervised learning.",
                    "label": 0
                },
                {
                    "sent": "Then we will try it.",
                    "label": 0
                },
                {
                    "sent": "Then we would have to do with X alone and will try to see if they are stable hidden variables as we call it in this case.",
                    "label": 0
                },
                {
                    "sent": "So this could be clustering.",
                    "label": 0
                },
                {
                    "sent": "For example we group the scans so if we without telling the system know that we have a baseline study, we could hope that if we did a clustering on the raw data then we will find the two clusters.",
                    "label": 0
                },
                {
                    "sent": "We find one that is corresponding to baseline and wonder corresponding to.",
                    "label": 0
                },
                {
                    "sent": "Two, so the activated state and the hypothesis that in this space, if you think about X, so it starts being an image or or volume.",
                    "label": 0
                },
                {
                    "sent": "But think of it now as as you string it out as a vector.",
                    "label": 0
                },
                {
                    "sent": "So find some particular way of scanning the image, just like a TV for example, right?",
                    "label": 0
                },
                {
                    "sent": "Or do the same thing for the volume.",
                    "label": 0
                },
                {
                    "sent": "Now we have a vector and that vector can be thought of as coordinating a space of the same dimension as the length of the vector.",
                    "label": 0
                },
                {
                    "sent": "Then the hypothesis here is that the measurement is so good that if stuff is close in that space, just your metrically close, like in Euclidean distance, it's roughly the same meaning.",
                    "label": 0
                },
                {
                    "sent": "So we hope that what corresponds are good cluster in that space would be something that reflects a variable like S, right?",
                    "label": 0
                },
                {
                    "sent": "Vice versa, we could say that this is what defined could variables is that they have that property.",
                    "label": 0
                },
                {
                    "sent": "Sometimes you will have to find other ways of measuring distances because the spaces can be warped in certain ways because of the experiments, and This is why at some point we may need to look at more manifold based methods that where distance is not really measured so well in that original space.",
                    "label": 0
                },
                {
                    "sent": "But we have to go to a certain latent space where the distances are more faithful to to the interesting structures.",
                    "label": 0
                },
                {
                    "sent": "Here I'll show you some examples of that also.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we look at the very briefly at history, we can see that there's been many attempts of modeling this joint distribution for.",
                    "label": 0
                },
                {
                    "sent": "Brain states an and.",
                    "label": 0
                },
                {
                    "sent": "Panty Shannon psychological states yes, here, or the macroscopic states in the Mayan basis graphic states.",
                    "label": 0
                },
                {
                    "sent": "So the two basic ways of analyzing a joint distribution is to factor it.",
                    "label": 0
                },
                {
                    "sent": "Either of these two ways here, right?",
                    "label": 0
                },
                {
                    "sent": "So I think I think of it as a as a conditional distribution of the brain image given the stimulus.",
                    "label": 0
                },
                {
                    "sent": "And then we have to multiply by the probability of the stimulus.",
                    "label": 0
                },
                {
                    "sent": "Or we can factor it the other way around.",
                    "label": 0
                },
                {
                    "sent": "And this gives rise to two quite different types of machine learning methods.",
                    "label": 0
                },
                {
                    "sent": "Once you do one or the other.",
                    "label": 0
                },
                {
                    "sent": "So in this case.",
                    "label": 0
                },
                {
                    "sent": "We can think of it as we control the stimulus and we simply look for what are the effects in the brain volume.",
                    "label": 0
                },
                {
                    "sent": "And now you can see that this is the kind of thing that you do in SPM.",
                    "label": 0
                },
                {
                    "sent": "For example, right?",
                    "label": 0
                },
                {
                    "sent": "Because there you would pass it a model saying that each of the two states in this block design each one is a normal distribution of very simple one, because this is so high dimensional distribution we have to do something to simplified, and in the classical SPM package which has been extremely successful of course and has completely changed this whole.",
                    "label": 0
                },
                {
                    "sent": "Came since fMRI came out and before that pet imaging and so on.",
                    "label": 0
                },
                {
                    "sent": "The idea is to to model this distribution here, in the sense that we assume that all the different pixels are boxes.",
                    "label": 0
                },
                {
                    "sent": "If it's a volume are independent, right?",
                    "label": 0
                },
                {
                    "sent": "So this means that there is implicit in machine learning will call in the base assumption that that you can write this joint distribution in between all the different components of the vector as a product.",
                    "label": 0
                },
                {
                    "sent": "So it's a factorized distribution.",
                    "label": 0
                },
                {
                    "sent": "But you can also see that in this way you are definitely going to lose a lot.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's a lot of dependencies structure that you lose right away if you do that, and This is why when we started doing our machine learning methods in the mid 90s.",
                    "label": 0
                },
                {
                    "sent": "We can tell that the other way around so that we said, OK, let's let's focus on on the dependency structures in the brain scan and then we have to look at this factorization where we try to predict the status of the brain condition on the brain scans.",
                    "label": 0
                },
                {
                    "sent": "So this is what now called mind reading right?",
                    "label": 0
                },
                {
                    "sent": "We try to predict what the brain is doing.",
                    "label": 0
                },
                {
                    "sent": "And of course we can do that in a simple way if we have access to label data.",
                    "label": 0
                },
                {
                    "sent": "So this would be the supervised case where we actually know is is a Nexus in a training set.",
                    "label": 0
                },
                {
                    "sent": "Then we can train this conditional distribution here, right?",
                    "label": 0
                },
                {
                    "sent": "And then modeling this part, which is again as high dimension of this.",
                    "label": 0
                },
                {
                    "sent": "We have to make really strong simplifications and this we did, for example by principal component analysis or independent component analysis.",
                    "label": 0
                },
                {
                    "sent": "As I'll talk about in a moment as an attempt of modeling this distribution here, but still not quite as to what dramatic simplification of this, because in ICA PCA we also do a factorization hypothesis, but not directly in the data, but in the latent variables.",
                    "label": 0
                },
                {
                    "sent": "And that's a much less strong assumption.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's a much more likely thing to happen that the.",
                    "label": 0
                },
                {
                    "sent": "Physical effects that cause S are independent.",
                    "label": 0
                },
                {
                    "sent": "We can separate them that way, so this is for for PFX.",
                    "label": 0
                },
                {
                    "sent": "Whatever we do, we will always try to think of these distributions are things that we can model by the machinery of machine learning.",
                    "label": 0
                },
                {
                    "sent": "So this means that we will bring out parameterized families of models.",
                    "label": 0
                },
                {
                    "sent": "So when we started doing all this for for for the conditional here, this was an artificial neural network algorithm, so this is a flexible family of functions where we have parameters describing exactly which function it is, and then we had a platoon, those and they.",
                    "label": 0
                },
                {
                    "sent": "We sample a little bit the neural networks in the brain.",
                    "label": 0
                },
                {
                    "sent": "This is why they were called artificial neural networks.",
                    "label": 0
                },
                {
                    "sent": "Nowadays most people use support vector machines to model this condition solution.",
                    "label": 0
                },
                {
                    "sent": "Here.",
                    "label": 0
                },
                {
                    "sent": "This is what I'm going to talk about.",
                    "label": 0
                },
                {
                    "sent": "Also towards the end.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Alright, let me run.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So now once we have defined the the model family, we have to device the loss function.",
                    "label": 0
                },
                {
                    "sent": "So when is it a good model that we have?",
                    "label": 0
                },
                {
                    "sent": "Access to and the model will take this form here, so let's look at this conditional distribution again.",
                    "label": 0
                },
                {
                    "sent": "So now instead of using just the true distribution that we don't know if we have to do something that depends on data.",
                    "label": 0
                },
                {
                    "sent": "So we assume that we get some kind of data set.",
                    "label": 0
                },
                {
                    "sent": "Now that we call D, yeah.",
                    "label": 0
                },
                {
                    "sent": "And that data set will typically enter into the model as as an estimation of parameters or sampling the parameters if we want to do something that takes uncertainty into account.",
                    "label": 0
                },
                {
                    "sent": "Also, and we think that's important then, then we will.",
                    "label": 0
                },
                {
                    "sent": "Use this is called the predictive distribution, either directly with some point estimates where the data is converted into parameters, or we will have a sampling procedure where where this is computed as an average or posterior example.",
                    "label": 0
                },
                {
                    "sent": "Either way we have access something like this that we can say something about for a future scan.",
                    "label": 0
                },
                {
                    "sent": "What is the future expected label?",
                    "label": 0
                },
                {
                    "sent": "And then of course we can play the trigger cross validation that we can compare it so in a data set where we know the truth that we hold out from between.",
                    "label": 0
                },
                {
                    "sent": "So this would be the training set and then the average.",
                    "label": 0
                },
                {
                    "sent": "So that's this angular bracket.",
                    "label": 0
                },
                {
                    "sent": "It means that we average on test data.",
                    "label": 0
                },
                {
                    "sent": "So we trained the parameters are trained the posterior distribution on data from the training data, and then we apply it to another data set so we can get an unbiased estimate, and that's the important of importance of a test data set is that it gives us an unbiased estimator of the performance.",
                    "label": 1
                },
                {
                    "sent": "And the performance is then measured with different cost functions, right?",
                    "label": 0
                },
                {
                    "sent": "And at the the one that most people would do something like this, the least squares tight fitting to use the squared error type expression.",
                    "label": 0
                },
                {
                    "sent": "You can argue sometimes that this is the right thing to do, but sometimes it's simply convenient.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you have a classification system where IDs variable would takes zeros and ones, then this could be really useful because this is simply proportional to the number of times that that the predicted.",
                    "label": 0
                },
                {
                    "sent": "This is different from the 2S.",
                    "label": 0
                },
                {
                    "sent": "So you can use that even in this case.",
                    "label": 0
                },
                {
                    "sent": "Also this but, but there are also other more accurate measures.",
                    "label": 0
                },
                {
                    "sent": "I would say, for example, a very natural thing to do is to use this expression.",
                    "label": 0
                },
                {
                    "sent": "Here the negative log probability.",
                    "label": 0
                },
                {
                    "sent": "So this measures not only whether you make a mistake or not, as this would do, but also how certain why you when you made a mistake and it's of course worse in a sense to be making a mistake if you were really certain that it was the other thing that was right, right?",
                    "label": 0
                },
                {
                    "sent": "So this would be a strong mistaken week mistakes, so you can measure that with these cost functions here.",
                    "label": 0
                },
                {
                    "sent": "And interesting enough, and I think this is not so widely appreciated.",
                    "label": 0
                },
                {
                    "sent": "This many people do.",
                    "label": 0
                },
                {
                    "sent": "This is called the deviance right?",
                    "label": 1
                },
                {
                    "sent": "But the deviance can be used also for.",
                    "label": 0
                },
                {
                    "sent": "Also, for unsupervised learning and this is less well known that many people know that I'm, I think many of you have used this already for linear regression, support vector machines.",
                    "label": 0
                },
                {
                    "sent": "So you use these test error methods all the time.",
                    "label": 0
                },
                {
                    "sent": "We always do that, but you could do that also for the unsupervised learning algorithms.",
                    "label": 0
                },
                {
                    "sent": "Exactly the same trick can be done for anything like principal component analysis, ICA, clustering algorithms.",
                    "label": 0
                },
                {
                    "sent": "All these unsupervised hidden Markov models that produce it at densities in sequences and so on, all these models.",
                    "label": 0
                },
                {
                    "sent": "That work from day to themselves and try to find a latent variable.",
                    "label": 0
                },
                {
                    "sent": "They can be treated in exactly the same way, and this is I'm going to show you some examples of that using this likelihood function.",
                    "label": 0
                },
                {
                    "sent": "Here the lock probability of that you use for training simply apply the same likelihood function to the test data.",
                    "label": 0
                },
                {
                    "sent": "So that deviants works just as well for all the unsupervised algorithms as it does for supervised learning.",
                    "label": 0
                },
                {
                    "sent": "And you can even combine if you're interested in modeling both the conditional and the density of the same footing.",
                    "label": 0
                },
                {
                    "sent": "You can use this kind of useful information expression here that.",
                    "label": 0
                },
                {
                    "sent": "That we actually when we first started doing the.",
                    "label": 0
                },
                {
                    "sent": "The young evaluation schemes that I'm going to show you the moment this split half resampling and so on.",
                    "label": 0
                },
                {
                    "sent": "We use this cost function here.",
                    "label": 0
                },
                {
                    "sent": "Often the mutual information to qualifier the value of a model.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here's an example, an old one, so this is precisely this.",
                    "label": 0
                },
                {
                    "sent": "Optimizing a unsupervised algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is a very simple application of principal component analysis to the data set that I showed you in the beginning.",
                    "label": 0
                },
                {
                    "sent": "This video of a single slice.",
                    "label": 0
                },
                {
                    "sent": "So the principal component analysis is a model that says that the all the interesting dependency between voxels.",
                    "label": 0
                },
                {
                    "sent": "So remember this is unsupervised, so we have no labels.",
                    "label": 0
                },
                {
                    "sent": "We don't know when stimulation took place in or not.",
                    "label": 0
                },
                {
                    "sent": "Right now we simply have the video itself and I'm trying to find some structure in the video.",
                    "label": 0
                },
                {
                    "sent": "And then simply strengthen the video out as a vector so I have a sequence of those vectors and I'm going to look for.",
                    "label": 0
                },
                {
                    "sent": "One single statistic, namely the covariance between pixels.",
                    "label": 0
                },
                {
                    "sent": "Hoping that the direction that the high variance components in that so the principle component associated with interesting dynamics in the data.",
                    "label": 0
                },
                {
                    "sent": "And the details of it, I'm not going to show right now.",
                    "label": 0
                },
                {
                    "sent": "I'm coming back to this in a moment, but it actually does pick up some interesting dimensions, like the stimulus condition, the stimulus response response in the data to stimulus.",
                    "label": 0
                },
                {
                    "sent": "It picks that up quite well already with principal component analysis.",
                    "label": 0
                },
                {
                    "sent": "But then the question of course, is how many things are going on.",
                    "label": 0
                },
                {
                    "sent": "So if you think of Frank component, notice this is finding the eigenvectors of the covariance matrix, and now the model is again a multivariate normal distribution.",
                    "label": 0
                },
                {
                    "sent": "So we can build the likelihood function.",
                    "label": 0
                },
                {
                    "sent": "That's the probability density function that goes with it.",
                    "label": 0
                },
                {
                    "sent": "Articular covariance matrix and the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "We can approximate by one actually, but I know I can vector so this is just a noise, uniform noise, right?",
                    "label": 0
                },
                {
                    "sent": "Or we can approximate it by a single direction in space and then uniform noise in the rest of the space or any number of components plus noise in the rest?",
                    "label": 0
                },
                {
                    "sent": "And the question is how many should there be?",
                    "label": 0
                },
                {
                    "sent": "And this is exactly what is shown here.",
                    "label": 0
                },
                {
                    "sent": "So here we have the negative log likelihood on test data.",
                    "label": 0
                },
                {
                    "sent": "Lag.",
                    "label": 0
                },
                {
                    "sent": "And this means that it's good to be low.",
                    "label": 0
                },
                {
                    "sent": "Because if it's low means that the likelihood is high.",
                    "label": 0
                },
                {
                    "sent": "So this means that on test data we have high probability of seeing them, so the model is good if it can produce a high likelihood, intestate, right?",
                    "label": 0
                },
                {
                    "sent": "So the negative likelihood should be small.",
                    "label": 0
                },
                {
                    "sent": "That's like an error.",
                    "label": 0
                },
                {
                    "sent": "And here is the result of applying this to a test data set with different number of components.",
                    "label": 0
                },
                {
                    "sent": "And there's a clear minimum right that you say this is given this data set that I have this particular size number of of of pixels and so on.",
                    "label": 0
                },
                {
                    "sent": "Number of scans that I can look at.",
                    "label": 0
                },
                {
                    "sent": "This is the model that produced the best fit.",
                    "label": 0
                },
                {
                    "sent": "And then.",
                    "label": 0
                },
                {
                    "sent": "What happens if I use less components?",
                    "label": 0
                },
                {
                    "sent": "Then I'll have a misfit because if the model is trying to.",
                    "label": 0
                },
                {
                    "sent": "If the model has two few components here, then then the.",
                    "label": 0
                },
                {
                    "sent": "It will not be able to capture all the modes that at this thing.",
                    "label": 0
                },
                {
                    "sent": "So this means that it will make Me 2 uniform, so it'll be 2 flats or put probability.",
                    "label": 0
                },
                {
                    "sent": "But there's no need for it.",
                    "label": 0
                },
                {
                    "sent": "So if I have two 2 strong directions, then there should be a lot of probability in that plane, but not another in other parts, right?",
                    "label": 0
                },
                {
                    "sent": "So if I have a lot of signal in other other parts of space, then there would be missed here in the case so that we call that bias that the model is is kind of too low probability in two large an area and then the opposite thing happens here in the case that there are too many components that specializes, it'll put probability in certain directions that are good for the training data.",
                    "label": 0
                },
                {
                    "sent": "But then when we go to the test data they don't reproduce, so This is why it's going to shoot up here and there's a.",
                    "label": 0
                },
                {
                    "sent": "Compromise here in the middle.",
                    "label": 0
                },
                {
                    "sent": "Now to other curves.",
                    "label": 0
                },
                {
                    "sent": "So this is if you simply applied it to the training data, they will see that of course, the more you yes please.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Yes, so so.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is this experiment here, and it's the mask is what you see here up scuse me.",
                    "label": 0
                },
                {
                    "sent": "Backward.",
                    "label": 0
                },
                {
                    "sent": "So there's also all these pixels here are inside the mass, so it's a couple of 1000 dimensions that it takes place in to start with.",
                    "label": 0
                },
                {
                    "sent": "So this is a global PCA.",
                    "label": 0
                },
                {
                    "sent": "Double pizza.",
                    "label": 0
                },
                {
                    "sent": "And in in this data set it points to its relative small number of components.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then then there's a whole game.",
                    "label": 0
                },
                {
                    "sent": "I mean, as a paper that reference that you can go and see what these components really are, and so on.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The.",
                    "label": 0
                },
                {
                    "sent": "Interpret and trying to understand the yes variable.",
                    "label": 0
                },
                {
                    "sent": "Say I'll be more specific about how to read out the S and so on in a moment.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is more about the method itself, so this is how we optimize the model.",
                    "label": 0
                },
                {
                    "sent": "So we make sure that it actually does capture the physical process that is there.",
                    "label": 0
                },
                {
                    "sent": "The best we can with this model framework within that family of models, I would say that the three components is the best.",
                    "label": 0
                },
                {
                    "sent": "Now there's another way of mapping out this relation between model complexity data, amount of data, and the generalizability, and this is the form of I think I already mentioned, this is what we call the learning curve.",
                    "label": 0
                },
                {
                    "sent": "So here the what we're trying to find out is the relation between how?",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Complex and model.",
                    "label": 0
                },
                {
                    "sent": "Can you expect to see in a certain amount of data, so you have this intuition?",
                    "label": 0
                },
                {
                    "sent": "Of course, that the less data you have the similar model you have to stick with, because if you use complicated models on small amount of data then overfit us.",
                    "label": 0
                },
                {
                    "sent": "We just saw it so we can map this in this way here.",
                    "label": 0
                },
                {
                    "sent": "So here I have two different models, not a whole lot of models before 2 versions, so this could be if we did this for PCA.",
                    "label": 0
                },
                {
                    "sent": "Again it will be two different numbers of components will compare, so here it's actually a supervised case, so it's finger tapping.",
                    "label": 0
                },
                {
                    "sent": "In two different modalities.",
                    "label": 0
                },
                {
                    "sent": "So this is a pet experiment.",
                    "label": 0
                },
                {
                    "sent": "Is an MRI experiment and now we ask for what happens to the quality of the model as we increase the data set.",
                    "label": 0
                },
                {
                    "sent": "And in this particular case it we compare a simple linear, so this will like linear discriminants.",
                    "label": 0
                },
                {
                    "sent": "I simply cut that space we talked about before in two, and I hope that the baseline is on one side and active scans on the other side, right?",
                    "label": 0
                },
                {
                    "sent": "So this we tune that on training data to find that.",
                    "label": 0
                },
                {
                    "sent": "So that's the linear discriminate.",
                    "label": 0
                },
                {
                    "sent": "And then there's a nonlinear classifier classifier that's the full curve, which is making it more complicated assumption about the division between the two groups.",
                    "label": 0
                },
                {
                    "sent": "And now we ask for the mean classification error.",
                    "label": 0
                },
                {
                    "sent": "So here it is simply using that S -- S had squared thing right?",
                    "label": 0
                },
                {
                    "sent": "And so we measure the number of misclassification as a function of the number of scans that went into this and what you can see here is what you would expect.",
                    "label": 0
                },
                {
                    "sent": "Also that these two lines they cross.",
                    "label": 0
                },
                {
                    "sent": "But if you have a small number of data then you would can only justify a simple model.",
                    "label": 0
                },
                {
                    "sent": "The linear model and then the number one will overfit because it had mostly group freedom.",
                    "label": 0
                },
                {
                    "sent": "Fitting the noise, and this does not generalize to the to the test data, right and.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Did it?",
                    "label": 0
                },
                {
                    "sent": "Open Scott, sorry so this is a different modality like fMRI, so this isn't this is done in the fMRI scanner.",
                    "label": 0
                },
                {
                    "sent": "This is done with a another type of scanner that we used in the old days to make the same kind of experiment as I talked about for fMRI.",
                    "label": 0
                },
                {
                    "sent": "So it stands for Positron emission tomography, and it's simply a different scanner that measures more less the same signal.",
                    "label": 0
                },
                {
                    "sent": "It officially is a little simpler way in pet scanning, so it's it's more so for an experimental point of view.",
                    "label": 0
                },
                {
                    "sent": "It's actually a better experiment, but it has some.",
                    "label": 0
                },
                {
                    "sent": "Pretty bad side effects, so it's using radioactive traces, so it means that there's a limit to how often you can do it and.",
                    "label": 0
                },
                {
                    "sent": "It's a slow technique and so This is why people went onto to use that technique.",
                    "label": 0
                },
                {
                    "sent": "We're talking mostly about.",
                    "label": 0
                },
                {
                    "sent": "Sorry bout that.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this was analyzing one part of the of this, namely the quality of the model now moving onto to the interpretation.",
                    "label": 1
                },
                {
                    "sent": "And then of course it gets much less firm, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, now it's it's rotation can be anything, right?",
                    "label": 0
                },
                {
                    "sent": "We want to understand something for that model that's completely unspecified to start with.",
                    "label": 1
                },
                {
                    "sent": "But in brain mapping out say that there's almost like there's a scheme now that is developed, and this is it goes with the word that what we're interested in is a brain map.",
                    "label": 0
                },
                {
                    "sent": "So all these methods that we use all the machine learning methods that we develop support vector machines, you know, classifiers whatever used to understand the relation between S&X.",
                    "label": 0
                },
                {
                    "sent": "We want at the end of the day after we have identified the process, the best we can using those tools we want to find out what that model learned about that particular experiment.",
                    "label": 0
                },
                {
                    "sent": "What is it that that was the salient regions in the networks were involved in this particular information processing, and we want to to find that, and we want of course to be sure about what we find again.",
                    "label": 0
                },
                {
                    "sent": "So this is what I'm trying to set up is a way of getting to something that is that can be quantified, and even the uncertainty can be quantified.",
                    "label": 0
                },
                {
                    "sent": "So the brain map has a very informal definition, so this would be something that if there are regions that are involved in the information processing task.",
                    "label": 1
                },
                {
                    "sent": "That defines an S. Then we would expect the brain Maps would be high and so high intensity as an image.",
                    "label": 0
                },
                {
                    "sent": "Bright areas in the image would be associated with that and lower values if they're not involved.",
                    "label": 0
                },
                {
                    "sent": "The ideal thing would be something that has value.",
                    "label": 0
                },
                {
                    "sent": "One of the areas that are involved and zero in the other areas, but it's not so simple for many reasons.",
                    "label": 0
                },
                {
                    "sent": "I mean partly because of the brain.",
                    "label": 0
                },
                {
                    "sent": "Maybe everything Department takes part in anything, right?",
                    "label": 0
                },
                {
                    "sent": "I mean, there's so much communication going on.",
                    "label": 0
                },
                {
                    "sent": "So many hypothesis tested about any stimulus that it may be a better, maybe actually so that anything is any areas involved.",
                    "label": 0
                },
                {
                    "sent": "But here we have caused talking about something that we can say with certainty, significance.",
                    "label": 0
                },
                {
                    "sent": "We want to find the best and so on that many things that we can quantify to get some areas that are more involved than others.",
                    "label": 1
                },
                {
                    "sent": "So we expect this to be higher values on the ones that are most involved and lessen the the other.",
                    "label": 0
                },
                {
                    "sent": "And this is exactly what was the success of statistical parametric mapping.",
                    "label": 0
                },
                {
                    "sent": "Of course was that it produced a map like that and This is why we have something called your image.",
                    "label": 0
                },
                {
                    "sent": "All these new images are shown in that yeah.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So there's been a number of attempts to to do that, so we have two different versions of visualizations that we've been working on.",
                    "label": 0
                },
                {
                    "sent": "1st, we had something we call a sailing to map, and this is the case that is shown here and kind of three dimensional plot, typically based on pet imaging.",
                    "label": 0
                },
                {
                    "sent": "Also in the mid 90s and then later on we so we had some resistance towards the same zoom app.",
                    "label": 0
                },
                {
                    "sent": "I think it's still a good definition.",
                    "label": 0
                },
                {
                    "sent": "I won't talk much more about it, but just say that it's there.",
                    "label": 1
                },
                {
                    "sent": "There's a reference if you want to see how we computed when I'm using now.",
                    "label": 0
                },
                {
                    "sent": "Something we call the sensitivity Maps.",
                    "label": 0
                },
                {
                    "sent": "It's almost the same, but it's measuring simply.",
                    "label": 0
                },
                {
                    "sent": "And model how sensitive the model is to certain areas in the brain.",
                    "label": 0
                },
                {
                    "sent": "So this is how we're going to measure the moment.",
                    "label": 0
                },
                {
                    "sent": "Whatever we use that weighs also that you have some papers on how to to make consensus among different Maps.",
                    "label": 0
                },
                {
                    "sent": "But I'm not going to speak about that, either.",
                    "label": 0
                },
                {
                    "sent": "Also, front.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "1000 so once I have estimated my model.",
                    "label": 0
                },
                {
                    "sent": "We could say that if the model really identified, the physical process, will that not that particular model, not by itself, sort of be the most robust representation of the data now?",
                    "label": 0
                },
                {
                    "sent": "And if you.",
                    "label": 0
                },
                {
                    "sent": "And follow that lead and sort of look into what the statistics have to say about that.",
                    "label": 0
                },
                {
                    "sent": "So how how well determined other hidden parameters?",
                    "label": 0
                },
                {
                    "sent": "So the question is if I find the model that that is generalizing the best when that are not also be the model that has the more brokers hidden variables.",
                    "label": 0
                },
                {
                    "sent": "So maybe expect that to be the case, but it turns out that it's not unfortunately.",
                    "label": 0
                },
                {
                    "sent": "So if you go to the.",
                    "label": 0
                },
                {
                    "sent": "Positions and ask the question, then they will say that we know something from asymptotic theory, so I'm trying to do is the following that instead of having small ends as I showed here small above subjects, multiple scans per subject and so on, you now pretend that you have almost infinite amount of data, so you look at what happens to your model if the data set starts to to become infinite, then of course any parameter init distribution becomes peaked because there's so much information now that we can really say a lot about the.",
                    "label": 0
                },
                {
                    "sent": "About the values of parameters or the distribution of parameters for latent variables.",
                    "label": 0
                },
                {
                    "sent": "So in this limit one can start to do computer computation, so the there's a small parameter, so we can do a formal expansion of likelihood functions on test data.",
                    "label": 0
                },
                {
                    "sent": "For example in terms of a small quantity, namely 1 divided by the number of samples, right?",
                    "label": 0
                },
                {
                    "sent": "So if N is big, 1 divided by in, this is more parameter.",
                    "label": 0
                },
                {
                    "sent": "We could expand whatever we have cost functions and so on in that parameter, and those expansions can give us some insight in that limit.",
                    "label": 0
                },
                {
                    "sent": "How well the connection is, how tight the connection is between.",
                    "label": 0
                },
                {
                    "sent": "Performance and the parameter fluctuation and what you find out if there's an old paper here we do it for different models in the cross validation example and what you learn is that if you do cross validation resampling, so this means that we split the data in multiple sets.",
                    "label": 0
                },
                {
                    "sent": "Then if you actually fuse the models afterwards, then asymptotically you get the same performance as if you trained on the whole thing.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of a good news for us.",
                    "label": 0
                },
                {
                    "sent": "They like to do cross validation because it is like we don't really do anything.",
                    "label": 0
                },
                {
                    "sent": "Of course for small datasets we do.",
                    "label": 0
                },
                {
                    "sent": "And we hope that we can keep it on the control by being clever about the way we do the resampling.",
                    "label": 0
                },
                {
                    "sent": "But asymptotically, I mean there's kind of a good news statement there.",
                    "label": 0
                },
                {
                    "sent": "There also, so these results are available for parametric models like in this paper.",
                    "label": 0
                },
                {
                    "sent": "We looked at models where the size of the parameter space stays finite while the sample size goes to Infinity, and this is not does not cover what we call nonparametric models.",
                    "label": 0
                },
                {
                    "sent": "So for example, support vector machines or kernel machines in general, what we call nonparametric in the sense that the.",
                    "label": 1
                },
                {
                    "sent": "Dating variable spacing away expands with the sample size, and there we have some results also and they kind of give the same result as well.",
                    "label": 0
                },
                {
                    "sent": "I'm going to say now as the finite privatizations, but much more complicated.",
                    "label": 0
                },
                {
                    "sent": "Calculus two goes into it.",
                    "label": 0
                },
                {
                    "sent": "So what we find out is that the.",
                    "label": 0
                },
                {
                    "sent": "Predictive performance is the generalization error for the generalizability of the model.",
                    "label": 0
                },
                {
                    "sent": "Has two components as we saw before, there's a bias term, meaning that the model maybe could be too simple for the problem.",
                    "label": 0
                },
                {
                    "sent": "That's the bias term and there's a variance term saying that it could be too complicated near that it varies a lot.",
                    "label": 0
                },
                {
                    "sent": "If you re sample from one data set to the next, so those are the two many times we can write the error as a sum of those two simply and other times it's a slightly more complicated relation.",
                    "label": 0
                },
                {
                    "sent": "But these two components determining the error.",
                    "label": 1
                },
                {
                    "sent": "And the variance component is directly coupled to the variance of the parameters.",
                    "label": 0
                },
                {
                    "sent": "But the bias term is not.",
                    "label": 0
                },
                {
                    "sent": "So that relation is nontrivial and This is why there are two axes in the plot that we have to quantify the performance and then we have to handle the potential buyers that we have in the in the best models.",
                    "label": 0
                },
                {
                    "sent": "What kind of parameter fluctuation they lead lead to simply?",
                    "label": 0
                },
                {
                    "sent": "So this is this is the reason that there's this sort of like 2 degrees of freedom, and even in the asymptotic regime where we can do these calculations.",
                    "label": 0
                },
                {
                    "sent": "So this is simply the recipe how to compute the sensitivity map.",
                    "label": 0
                },
                {
                    "sent": "So this was defined in this new image paper from 2002.",
                    "label": 0
                },
                {
                    "sent": "So we take the last function, so that was the log of the probability of the predicted value given the scan.",
                    "label": 0
                },
                {
                    "sent": "We simply ask this cost function.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sensitive is it to a particular location?",
                    "label": 0
                },
                {
                    "sent": "Remember, we straightened out the image and now we have a location called that J.",
                    "label": 0
                },
                {
                    "sent": "So we simply compute the derivative after cost function respect to the activation in that location, and we do that insert the value.",
                    "label": 0
                },
                {
                    "sent": "All the scans that we have in the test data and then the average on test dates of the square of that value.",
                    "label": 0
                },
                {
                    "sent": "The reason that we have to square it is that sometimes these derivatives can go in One Direction, sometimes they go in the other action because for a non linear model there's no simple relation between what the sensitive local sensitivities and the global sensitivity, so it could be.",
                    "label": 0
                },
                {
                    "sent": "Way, and you can think of this as a vector, can point in all kinds of directions.",
                    "label": 0
                },
                {
                    "sent": "There's a paper in your machine research from here from Berlin showing that how these vectors they change across space.",
                    "label": 0
                },
                {
                    "sent": "So we do an average here to get a map and average map for the whole test set.",
                    "label": 0
                },
                {
                    "sent": "And this is what we call the map, right?",
                    "label": 0
                },
                {
                    "sent": "So we get a value for each location.",
                    "label": 0
                },
                {
                    "sent": "And now we can ask for example something like the following that we split the data set like we did with the procedure distribution, split the data set in two halves.",
                    "label": 0
                },
                {
                    "sent": "Estimate a model on each of the two and now we simply ask how, how much concurrence is that between those two Maps that we get?",
                    "label": 0
                },
                {
                    "sent": "So I have this enough.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Figure here, so that's the what we call the impasse.",
                    "label": 0
                },
                {
                    "sent": "Scheme for for evaluation of algorithms like like we have talked about here and this is developed as a set together with Steven's brother.",
                    "label": 0
                },
                {
                    "sent": "And many of these papers show how to apply the scheme and in many different contexts.",
                    "label": 0
                },
                {
                    "sent": "So how does it work?",
                    "label": 0
                },
                {
                    "sent": "So we take the full data set, so that's the combined set of scans that we have here, and this consists of the ex is and the variables that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "So let's imagine here for example.",
                    "label": 0
                },
                {
                    "sent": "It's a supervised case.",
                    "label": 0
                },
                {
                    "sent": "We're doing my tweaking right?",
                    "label": 0
                },
                {
                    "sent": "So this would be labels.",
                    "label": 0
                },
                {
                    "sent": "This would be like a design matrix.",
                    "label": 0
                },
                {
                    "sent": "Or the S is that encodes this on off stimulus.",
                    "label": 0
                },
                {
                    "sent": "So we have a green set and a blue set, and for the blue set we can estimate the model and we can compute the map that's got little SPM here just to make it something that we we know how to think about.",
                    "label": 0
                },
                {
                    "sent": "That's the map that is high in the values that in the areas that are highly involved and low in the places that are not so involved we get an SPM from the blue set.",
                    "label": 0
                },
                {
                    "sent": "Estimate the model and now we can do predictions on the green data, right?",
                    "label": 0
                },
                {
                    "sent": "So we can supply these axes as input to the algorithm here and produce predictions and then we can compare that with the true labels that we have from the Queen set, right?",
                    "label": 0
                },
                {
                    "sent": "So the blue is training set.",
                    "label": 0
                },
                {
                    "sent": "Now the green is a test set.",
                    "label": 0
                },
                {
                    "sent": "We compute the test error and we can compute the map.",
                    "label": 0
                },
                {
                    "sent": "Of course we can flip the whole thing and just interchange blue and green.",
                    "label": 0
                },
                {
                    "sent": "So now we get two measures of performance and we get 2 Maps and the two Maps.",
                    "label": 0
                },
                {
                    "sent": "They should be identical right?",
                    "label": 0
                },
                {
                    "sent": "Because I split the data in two halves so if everything was perfect in a perfect world, they would be identical and they'll be no?",
                    "label": 0
                },
                {
                    "sent": "Not at all, but in the real world, of course there's a lot of noise, so they're very different.",
                    "label": 0
                },
                {
                    "sent": "The two and the more different they are, the less we trust them.",
                    "label": 0
                },
                {
                    "sent": "Because of the split half procedure, this estimate of uncertainty that you get simply by subtracting the two Maps and squaring it.",
                    "label": 0
                },
                {
                    "sent": "Is an unbiased estimator for variance.",
                    "label": 0
                },
                {
                    "sent": "That's because there are two independent sets.",
                    "label": 0
                },
                {
                    "sent": "So now with this particular resampling scheme split half we get an unbiased estimate of the uncertainty of the map.",
                    "label": 0
                },
                {
                    "sent": "Now, many other schemes that you could do to re sampling get information like as you.",
                    "label": 0
                },
                {
                    "sent": "I'm sure many of you have heard of Bootstrap for example or the phone out we sampling.",
                    "label": 0
                },
                {
                    "sent": "So they have their own advantages, but they also have this problem that you cannot be sure that the variances can be corrected to be unbiased, so there's some correction formulas for for Bootstrap, for example, but there are some starting again, so you can trust them.",
                    "label": 0
                },
                {
                    "sent": "So if you really want to have something which is manifest on bias, you have to do something like this.",
                    "label": 0
                },
                {
                    "sent": "Rum.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You have some examples and how you can use it, so this is a case where.",
                    "label": 0
                },
                {
                    "sent": "You have a.",
                    "label": 0
                },
                {
                    "sent": "A subject that is trying to control a force by pressing a finger on the lever, and there's a visual feedback.",
                    "label": 0
                },
                {
                    "sent": "And of course there's a whole network involved in that control process is pretty complicated cognitive task.",
                    "label": 0
                },
                {
                    "sent": "It depends on how strong the pressure is of the feedback is on how well you attain that particular pressure.",
                    "label": 0
                },
                {
                    "sent": "So there's a big network that is involved in this.",
                    "label": 0
                },
                {
                    "sent": "So we run this whole machine Raider just talked about.",
                    "label": 0
                },
                {
                    "sent": "And now we get for every location we get a mean map like.",
                    "label": 0
                },
                {
                    "sent": "We can average all these Maps we have from the different datasets and hope that this is roughly what you would get from the if you have the full data set and now you can use the variances that we estimated, take the square root for example, divide that and then you get the score so expression and then you can start to to produce AC score map out of something as complicated as this London your machine learning algorithm is producing.",
                    "label": 0
                },
                {
                    "sent": "So that's I think that's the sort of level that we have taken it so far is that we for any map where we can compute derivatives, that's the criteria.",
                    "label": 0
                },
                {
                    "sent": "We should do something that looks like electric function or something similar to it.",
                    "label": 0
                },
                {
                    "sent": "A cost function.",
                    "label": 0
                },
                {
                    "sent": "Be able to compute the relative.",
                    "label": 0
                },
                {
                    "sent": "Then we can can do this.",
                    "label": 0
                },
                {
                    "sent": "So this means that we can do it for almost anything that.",
                    "label": 0
                },
                {
                    "sent": "It has been applied in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "So of course I don't know in that particular case because I haven't looked at it this way, but I know that there's a really broad spectrum, so if you look at the values of that map, there's a broad spectrum and we know that you get more robust model.",
                    "label": 0
                },
                {
                    "sent": "So this is something that comes out of this plot that I showed before.",
                    "label": 0
                },
                {
                    "sent": "If you, if you're not too aggressive.",
                    "label": 0
                },
                {
                    "sent": "So if you make the model too sparse, for example, so this would be eliminating those boxes.",
                    "label": 0
                },
                {
                    "sent": "Maybe that you're talking about now if you eliminate too much and make it too sparse, then reproducibility will will fade.",
                    "label": 0
                },
                {
                    "sent": "Then they will not be as reproducible.",
                    "label": 0
                },
                {
                    "sent": "As if you include more boxes into the.",
                    "label": 0
                },
                {
                    "sent": "Set that you that you're interested in.",
                    "label": 0
                },
                {
                    "sent": "So this could be an effect like.",
                    "label": 0
                },
                {
                    "sent": "So this means that, for example, in the resting state, that could be voxels that are systematically active, so to speak.",
                    "label": 0
                },
                {
                    "sent": "I mean, if there's a resting state network that is not directly coupled to any S variable, it could still pay off to model that, right?",
                    "label": 0
                },
                {
                    "sent": "I think that's that's maybe an effect like talking about yes.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, let me show you.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we should find out when to enter stuff inspected in here.",
                    "label": 0
                },
                {
                    "sent": "What is the time limit that we have at the end system?",
                    "label": 0
                },
                {
                    "sent": "So we we can go maybe up till about half past or something so I don't know if we should have a break or something.",
                    "label": 0
                },
                {
                    "sent": "Is that?",
                    "label": 0
                },
                {
                    "sent": "Do we need a break?",
                    "label": 0
                },
                {
                    "sent": "But you just keep going.",
                    "label": 0
                },
                {
                    "sent": "We'll keep going OK.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to show you some examples Now what one is for unsupervised learning.",
                    "label": 1
                },
                {
                    "sent": "So unsupervised.",
                    "label": 0
                },
                {
                    "sent": "Like Homer, he's left on his own right and some supervised examples where where it's classification, conventional mind reading.",
                    "label": 0
                },
                {
                    "sent": "So the re/max.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to to revert to share an unsupervised learning starts out with talking about a simple linear model.",
                    "label": 0
                },
                {
                    "sent": "So this is what in statistics is called a factor model, so it posits a very simple relation between the neuroimage that's X and the hidden variables, S, namely a simple linear relation.",
                    "label": 0
                },
                {
                    "sent": "So there's a set of coefficient that forms a matrix.",
                    "label": 0
                },
                {
                    "sent": "Or assume that both of these are vectors.",
                    "label": 0
                },
                {
                    "sent": "So let's think of it as a color column vector of S at a given time.",
                    "label": 0
                },
                {
                    "sent": "In the latent variable.",
                    "label": 0
                },
                {
                    "sent": "So the the information processing projects into a brain image that we can measure the bold signal, which is also a vector at the same time here right?",
                    "label": 0
                },
                {
                    "sent": "And there's a simple you know relation, so a lot of coefficients that we collect in this matrix A and then there's additive noise.",
                    "label": 0
                },
                {
                    "sent": "This model looks simple, but it's actually very rich and there's a lot of interesting models that people have applied in your imaging that that can be sort of captured in this simple framework.",
                    "label": 0
                },
                {
                    "sent": "Depending on how we parameterize and how we think about S and the noise, and a right so thinking about AES and and the noise in different ways, we get to different models, so some of these are.",
                    "label": 0
                },
                {
                    "sent": "Models like principal components that we talked about before in this case.",
                    "label": 0
                },
                {
                    "sent": "This is a normal noise drawn from a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "That guy right here with a Sarah mean and some covariance matrix and the PCA.",
                    "label": 0
                },
                {
                    "sent": "This covariance matrix is a very simple.",
                    "label": 0
                },
                {
                    "sent": "Their diagonal matrix scaled by the variance, the isotropic part.",
                    "label": 0
                },
                {
                    "sent": "And then this S variable here again also will be drawn from a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "And the coefficient eys these coefficients.",
                    "label": 0
                },
                {
                    "sent": "Here you think of them as a matrix.",
                    "label": 0
                },
                {
                    "sent": "The column will then often be model simply as a orthogonal vectors spanning the space of the subspace.",
                    "label": 0
                },
                {
                    "sent": "That this part here give rise to.",
                    "label": 0
                },
                {
                    "sent": "So that's the highest variance components.",
                    "label": 0
                },
                {
                    "sent": "I see a means that instead of algorithms where you have assumptions, typically honest, at the other that are non normal, so other types of distributions or or correlations.",
                    "label": 0
                },
                {
                    "sent": "In this time correlations for example, that you can measure.",
                    "label": 0
                },
                {
                    "sent": "And you can also do something like a means.",
                    "label": 0
                },
                {
                    "sent": "For example, if you allow these answers to only be binary variables, so there will be assignments to different components, different clusters in the mixture, same model with the client and saying like your function and so on.",
                    "label": 0
                },
                {
                    "sent": "So this is how we now go from this definition of the model.",
                    "label": 0
                },
                {
                    "sent": "So the likelihood function that I mentioned before that is to say OK, what is the likelihood function like?",
                    "label": 0
                },
                {
                    "sent": "Your function is to say something about the distribution of data based on assumed parameter values OK?",
                    "label": 0
                },
                {
                    "sent": "So we assume that we know a for the.",
                    "label": 0
                },
                {
                    "sent": "Constructing the likelihood function and potentially other parameters that play a role.",
                    "label": 0
                },
                {
                    "sent": "For example, in specifying the distribution of these latent variable hidden variables S. So we can write this language function as a product of the two things, so this is not easy to write out itself, but it's easy to write out is simply using this equation up here to write the probability of X or the PDF value of XX.",
                    "label": 0
                },
                {
                    "sent": "Because here we say that if I know what S&A is, so assume that S is known also just for the construction of the model and the likelihood function.",
                    "label": 0
                },
                {
                    "sent": "Assuming this is known as known and the variance and covariance of epsilon is known, then this is an easy distribution to do because fix A&S.",
                    "label": 0
                },
                {
                    "sent": "Then this is just a vector itself right multiply a, unselect you get a new vector.",
                    "label": 0
                },
                {
                    "sent": "But that's a fixed one.",
                    "label": 0
                },
                {
                    "sent": "Now that's because it's on the conditional side.",
                    "label": 0
                },
                {
                    "sent": "And then all the uncertainty about X is in epsilon.",
                    "label": 0
                },
                {
                    "sent": "That's the noise contribution which we assume is a normal distribution.",
                    "label": 0
                },
                {
                    "sent": "So this is simply a normal PDF that we have here.",
                    "label": 0
                },
                {
                    "sent": "This is what I've written down here, right?",
                    "label": 0
                },
                {
                    "sent": "So that's easy to write out, but now we didn't know what this was.",
                    "label": 0
                },
                {
                    "sent": "This was the.",
                    "label": 0
                },
                {
                    "sent": "The assumption is that S is an unknown hidden variable, so we have to put in an unknown distribution here.",
                    "label": 0
                },
                {
                    "sent": "This is where these models they differ.",
                    "label": 0
                },
                {
                    "sent": "Put in a normal distribution, we get to the principal component analysis and put in other distributions here.",
                    "label": 0
                },
                {
                    "sent": "Then we get to these other models in any case.",
                    "label": 0
                },
                {
                    "sent": "Choosing one of these, we can write down the likelihood function and just run the whole game as we as we laid it out.",
                    "label": 0
                },
                {
                    "sent": "Yes, for those of you who have seen a lot of this before, we can also implement these other methods here that people sometimes use, for example, structural equations.",
                    "label": 0
                },
                {
                    "sent": "They have to do with the specific assumption on it on a here so that there's a Markovian relation between the components of X.",
                    "label": 0
                },
                {
                    "sent": "So this means that this guy is sparse 1 -- A minus first, and if everything is positive, both A&S, then these part based representations of people called non negative matrix factorization paper written in the same way.",
                    "label": 0
                },
                {
                    "sent": "So if you're interested in this framework and also algorithms to to implement them and so on you take a look at these papers here.",
                    "label": 0
                },
                {
                    "sent": "So This is why.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They do, typically is that they tried to write the matrix that we have looked at, so we have the different pixels or boxes versus time, and then they write it as as a product of two smaller matrices.",
                    "label": 0
                },
                {
                    "sent": "This is what we learn now from the.",
                    "label": 0
                },
                {
                    "sent": "We learn these some typical patterns in the space that they will form the map, and then we learned the time series that activate those modes.",
                    "label": 0
                },
                {
                    "sent": "So in the signal that we looked at before, we assume that one of these columns would be pointing to the visual cortex, like this would be the thing.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Skip this in here.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An example of it, so his and I see a case, so here's a column of a now put together as a picture again and we can see that it has high values in the areas of where we saw the signal, and this is the stimulus curve that we didn't tell the algorithm about and this is the response that we fit it from it, right?",
                    "label": 0
                },
                {
                    "sent": "So we can see that it's the typical thing that if you know about the boat sinking that there's a delay.",
                    "label": 0
                },
                {
                    "sent": "And also there's quite a bit of irritability.",
                    "label": 0
                },
                {
                    "sent": "This is what you gain by this type of analysis unsupervised.",
                    "label": 0
                },
                {
                    "sent": "Analysis is that you can detect subtle changes in the latent variable.",
                    "label": 0
                },
                {
                    "sent": "So if we only knew about the square wave, so that would be the stimulus.",
                    "label": 0
                },
                {
                    "sent": "So this would be like in supervised learning.",
                    "label": 0
                },
                {
                    "sent": "Then we wouldn't learn the difference between the response here that actually ever responses itself is its own response, right?",
                    "label": 0
                },
                {
                    "sent": "And there's something that seems to reproduce, but also a lot of things that are different from response to response, so there's a lot of variability in the brain state, and when this method was first applied to fMRI, this is in 98 by magic.",
                    "label": 0
                },
                {
                    "sent": "Keone and coworkers.",
                    "label": 0
                },
                {
                    "sent": "This is exactly what they were excited about.",
                    "label": 0
                },
                {
                    "sent": "Was that this could detect this kind of transience in the response by by using unsupervised methods like like the ones we have here.",
                    "label": 0
                },
                {
                    "sent": "There's a radio paper from 2000 and three of these things are discussed.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is the tool box so they have many really good tool boxes for running this kind of unsupervised linear factor.",
                    "label": 0
                },
                {
                    "sent": "Model her so the one that we have produced has one particular aspect that you don't find in many of the others.",
                    "label": 0
                },
                {
                    "sent": "That is this determination of the number of components in a computational quite easy way using another curve that also showed you without talking about that.",
                    "label": 0
                },
                {
                    "sent": "That is a good proxy for the test error.",
                    "label": 0
                },
                {
                    "sent": "I think I'm unfortunately going to skip some things here that we otherwise I won't be able to.",
                    "label": 0
                },
                {
                    "sent": "To make it so.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Think I'm going to talk a little bit about nonlinear versions of this.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to put these slides on the web when I get back and there are slide sets that are quite similar to it already.",
                    "label": 0
                },
                {
                    "sent": "If you're interested in seeing more of the references, for example, and if some of you get interested in some of the stuff that I didn't get to talk about, your most welcome to write me also.",
                    "label": 0
                },
                {
                    "sent": "And of course later in the afternoon if you can find me, I will be happy to talk about it.",
                    "label": 0
                },
                {
                    "sent": "If you saw something and you said you'd like to discuss that in more detail.",
                    "label": 0
                },
                {
                    "sent": "The lot of challenges to these simple factor models I should say right away, so maybe one of the more important one is that it may be way too simple to assume that there's a linear relation between the two sets of variables, and for that reason we have looked a lot into two nonlinear manifold description so that A&S kind of determines one value of this.",
                    "label": 0
                },
                {
                    "sent": "Let's say the active state is like one curved manifold in this space rather than just being a linear subspace.",
                    "label": 0
                },
                {
                    "sent": "And then the different states are maybe different manifolds or different parts of the manifold.",
                    "label": 0
                },
                {
                    "sent": "So so this evolves around to nonlinear generations of the linear factor model.",
                    "label": 1
                },
                {
                    "sent": "And of course there's a lot of temporal structure in these networks also, so they may have parts that are not In Sync.",
                    "label": 0
                },
                {
                    "sent": "So both for fMRI and four.",
                    "label": 0
                },
                {
                    "sent": "For EG, signals all the temporal dynamics would give you this idea that maybe different parts of the networks are not exactly In Sync, so the models that we have, they take a pattern and then they activated by a single time function, right?",
                    "label": 0
                },
                {
                    "sent": "So this means that everything is In Sync in a component and this is way too simplistic for segments.",
                    "label": 0
                },
                {
                    "sent": "So This is why sometimes these convoluted models that can allow different delays and so on.",
                    "label": 1
                },
                {
                    "sent": "I used and also.",
                    "label": 0
                },
                {
                    "sent": "There can be problems both that they can be too rich innocence.",
                    "label": 0
                },
                {
                    "sent": "So if you have really high dimensional types of measures where there are lots of measurements going into to one experiment, for example multiple subjects, then the factor model can actually be too rich in a way so that could be too many parameters that you start to fit.",
                    "label": 0
                },
                {
                    "sent": "So this means that you have a really massive random effects model, is what the equivalent of that is, and then you may need to do something which is simpler, so similar representations that try to detect conditional independence is that are so strong that you can decode.",
                    "label": 0
                },
                {
                    "sent": "Modes for example.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to speak much about that, but this needs to these certain models that people refer to as multi way models like tensor algebra and so on.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just say a little bit about nonlinear models and why why we started looking at this, so this was one small experiment on the same data set as.",
                    "label": 0
                },
                {
                    "sent": "So this visual stimulus type experiment that I showed before we have a strong.",
                    "label": 0
                },
                {
                    "sent": "Stimulus at certain intervals.",
                    "label": 0
                },
                {
                    "sent": "Here are regularly spaced intervals and now we look for the response.",
                    "label": 0
                },
                {
                    "sent": "So this is the output of the classification algorithm and these are this classification algorithm works in two steps.",
                    "label": 0
                },
                {
                    "sent": "So first there's this kind of unsupervised algorithm that kind of finds a better space to look at.",
                    "label": 0
                },
                {
                    "sent": "So by principal component analysis would find a low dimensional space that represents hopefully the interesting structure that we have in the high dimensional space, and then you build your classifier that low dimensional space.",
                    "label": 0
                },
                {
                    "sent": "OK, that's a very typical later today.",
                    "label": 0
                },
                {
                    "sent": "Machine learning also to reduce dimensionality by.",
                    "label": 0
                },
                {
                    "sent": "By the factor models.",
                    "label": 0
                },
                {
                    "sent": "Now, if you do that, you can reduce dimensionality by PCA and get something like this.",
                    "label": 0
                },
                {
                    "sent": "So here we show where the algorithm believes there's activation in five repetitions of the baseline activation sequence, and you can see that it pretty much gets it's it's a very low error error rate.",
                    "label": 0
                },
                {
                    "sent": "I mean down to a few percent right?",
                    "label": 0
                },
                {
                    "sent": "So it's really good at predicting it has to do with this really good signal to noise ratio that we had.",
                    "label": 0
                },
                {
                    "sent": "But there are some instances, for example here within the middle of the baseline thinks that there's a stimulus so that it's clearly mistaking something here.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "Anthony, what we saw was that if we use something that projected nonlinearly answer to curved manifold rather than linear subspaces, then this went away.",
                    "label": 0
                },
                {
                    "sent": "So this is the output of the nearest neighbor type.",
                    "label": 0
                },
                {
                    "sent": "So just using this neighboring relations between scans so classify in the in the manifold space does a little bit better job, so it has a little bit better error rate, smaller error rates, but also it doesn't make this kind of the errors are now on the the transience here where it goes from baseline to active, where you would expect it to be uncertain.",
                    "label": 0
                },
                {
                    "sent": "But now it's not here in the middle of something.",
                    "label": 0
                },
                {
                    "sent": "So many of you may have heard about.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The nonlinear generations of principal component analysis, like method done.",
                    "label": 0
                },
                {
                    "sent": "Bring out here the equal PCA that was developed here in Berlin.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to.",
                    "label": 1
                },
                {
                    "sent": "To take higher order dependencies between data into account, not just the covariances or or the parameterized dependencies that we have in the linear factor model, but allow for much more general manifold like dependences.",
                    "label": 1
                },
                {
                    "sent": "So we do that by by shooting it into the data into some space that we don't make ever explicit, but implicitly it's very high dimensional that we call fire here and then we look for linear factor model so that, for example maximizing variances in principal component analysis in this space.",
                    "label": 0
                },
                {
                    "sent": "So we find projections of the new vectors fire effects.",
                    "label": 0
                },
                {
                    "sent": "So we have one for each data point.",
                    "label": 0
                },
                {
                    "sent": "Have you want to find vectors in this high dimensional space?",
                    "label": 0
                },
                {
                    "sent": "That explains most of the variance.",
                    "label": 0
                },
                {
                    "sent": "So in effect running the factor model in the feature space instead.",
                    "label": 1
                },
                {
                    "sent": "Because the this model is a linear one, then it turns out that all that we need now in this feature space the covariances and this is what this implication is that the manifold is in this mapping, not in the statistical model that we do.",
                    "label": 0
                },
                {
                    "sent": "And now we have to parameterise the covariance.",
                    "label": 0
                },
                {
                    "sent": "So this would be the relation the dependency between two scans in and prime here.",
                    "label": 0
                },
                {
                    "sent": "And the typical thing that people do in kernel PCA simply to use a neighboring something that says that if things are closed, they should be more dependent than if they are far away.",
                    "label": 0
                },
                {
                    "sent": "But by using this week definition, we can capture still manifold.",
                    "label": 0
                },
                {
                    "sent": "So a manifold, something that if you look really close, it looks like a linear subspace.",
                    "label": 0
                },
                {
                    "sent": "But if you go far away it curves so locali it still neighborhood neighborhood.",
                    "label": 0
                },
                {
                    "sent": "But on the global scale neighborhoods are defined in a more complicated way, so that's the difference between these algorithms.",
                    "label": 0
                },
                {
                    "sent": "So locally they are simple like what we had before.",
                    "label": 0
                },
                {
                    "sent": "Open is there much more complicated.",
                    "label": 0
                },
                {
                    "sent": "So when we do that, we can.",
                    "label": 0
                },
                {
                    "sent": "We can do this kind of projection.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In in a way that that takes for example, like this little simulated case here, where data is distributed on a circle rather than linear manifold, we can reduce dimensionality to one again.",
                    "label": 0
                },
                {
                    "sent": "So we have two dimensional data, and now we can reduce it to by projection onto a circle for example.",
                    "label": 1
                },
                {
                    "sent": "So this works really beautiful for this type of mapping that I talked about because locally of course it looks like a linear projection, but globally it's a circle, right?",
                    "label": 0
                },
                {
                    "sent": "So now using this mechanism we can get the projection in feature space by simple PCA.",
                    "label": 1
                },
                {
                    "sent": "But the problem is we need to get back to real space in order to see what is the result of the projection.",
                    "label": 1
                },
                {
                    "sent": "And this leads to a whole lot of interesting complications.",
                    "label": 0
                },
                {
                    "sent": "This mapping into feature space and mapping back and result also hear from Berlin by by the same group.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Showed us one way to do it and it turns out that this because of the non linearity that goes in that there's a lot of practical problems once you start to apply it.",
                    "label": 0
                },
                {
                    "sent": "So part of what we did was to to invest.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gateways of stabilizing the pre image and there's some papers by it's been Abrahamson myself where we modest have solved some of the instabilities by using regularization and.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some regularization.",
                    "label": 0
                },
                {
                    "sent": "That means that we don't allow it to do too big and noise reduction.",
                    "label": 0
                },
                {
                    "sent": "So without constraints on how how crazy noise reductions we can do, and this pretty much solves these instability problems.",
                    "label": 0
                },
                {
                    "sent": "And we of course we can do this in different ways so we can do it by like soft regularization by that some of you may have seen simply by constraining length of vectors we can also do more interesting regularization's like.",
                    "label": 0
                },
                {
                    "sent": "Constraining the absolute values of the images in.",
                    "label": 0
                },
                {
                    "sent": "This turns out that this promotes what is called sparsity that many of the parameters they will go to zero rather than taking some finite value.",
                    "label": 0
                },
                {
                    "sent": "And there's some work here on regularization of the pre images with sparsity promoting.",
                    "label": 0
                },
                {
                    "sent": "Prius or regularization terms that that worked really well for your images because new images.",
                    "label": 0
                },
                {
                    "sent": "We precisely hope that we have areas that are not involved and in some areas that are involved and those are the ones that would like to find this as as noise.",
                    "label": 0
                },
                {
                    "sent": "So we can use the Represe Bility performance plot again to optimize all the parameters that go into this.",
                    "label": 0
                },
                {
                    "sent": "So if you remember, we.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At this time.",
                    "label": 0
                },
                {
                    "sent": "Function here the covariance matrix that determined how far away two things were supposed to be correlated.",
                    "label": 0
                },
                {
                    "sent": "So you can see that here is the distance between the scans in the high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "The squared distance is measured relative to the scale.",
                    "label": 0
                },
                {
                    "sent": "See now there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a kind of locality, so how much does it curve if this parameter C is very big, then you can have very big distances that are connected and then you have almost like a flat surface, and if she's really small it'll be very curved and very nonlinear, so this is controlling the nonlinearity of the map.",
                    "label": 0
                },
                {
                    "sent": "And this.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is something that we would like to know.",
                    "label": 0
                },
                {
                    "sent": "So we propose different models, different nonlinearities, and be evaluated against this reliability performance.",
                    "label": 0
                },
                {
                    "sent": "You might, so we have.",
                    "label": 0
                },
                {
                    "sent": "Classification performance up here in Brazil to here and maybe these models here that have you can see that this is very high.",
                    "label": 0
                },
                {
                    "sent": "It's in the 95 range, right?",
                    "label": 0
                },
                {
                    "sent": "So these are almost the same performance, but the rivers ability changes also maybe bye bye affected by.",
                    "label": 0
                },
                {
                    "sent": "So the 10% or more right?",
                    "label": 0
                },
                {
                    "sent": "So these models are much more reproducing than these ones are.",
                    "label": 0
                },
                {
                    "sent": "Therefore, we choose that those parameters.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That said, if you want to see more of the details and also some real experiments with these methods have been used and to evaluate nonlinear denoising, there's this recent neuromed paper that that shows that how to do it and then out to what results to expect.",
                    "label": 0
                },
                {
                    "sent": "Let me end this by by skipping again a hole.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things unfortunately talk a little bit about supervised learning at the end.",
                    "label": 1
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So now of course we are much better off because with supervised learning.",
                    "label": 0
                },
                {
                    "sent": "We can use the label structure directly in the model.",
                    "label": 0
                },
                {
                    "sent": "We're not supposed to find it ourselves, right?",
                    "label": 0
                },
                {
                    "sent": "So this is a situation where we should expect to get good results, right?",
                    "label": 0
                },
                {
                    "sent": "It's like hypothesis testing in machine learning sense in the machine learning sense.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we do, for example, in the standard case that we have a set of labeled scans, how we model that?",
                    "label": 0
                },
                {
                    "sent": "So the thing that people would maybe think of 1st would be to do a nonlinear kernel machine like the support vector machine.",
                    "label": 0
                },
                {
                    "sent": "So again, we use the kernel from before describing what is what is closer when our neighbors.",
                    "label": 0
                },
                {
                    "sent": "Neighbors and when they're not using a scale parameter again to determine how broad the neighborhoods are.",
                    "label": 0
                },
                {
                    "sent": "And then the SVM we use the linear expansion on such kernels so that these coefficients.",
                    "label": 0
                },
                {
                    "sent": "Here they can be plus or minus.",
                    "label": 0
                },
                {
                    "sent": "So think of this as a variable that if it's plus it's active and it's negative, it's inactive, so a simple binary classifiers what we're building here on one side of the classifier.",
                    "label": 0
                },
                {
                    "sent": "Decision boundaries is positive, the other side is negative.",
                    "label": 0
                },
                {
                    "sent": "We want to find the point where it changes from zero to excuse me with zero from going from minus two to plus for example.",
                    "label": 0
                },
                {
                    "sent": "So this is implemented as a local averaging scheme where we have this function here describing what is it.",
                    "label": 0
                },
                {
                    "sent": "What are the neighbors that participate in the voting?",
                    "label": 0
                },
                {
                    "sent": "So if I'm classifying pattern in prime, if XN is too far away from X prime, then this will be larger than this whole expression found be small and it doesn't give a contribution to the voting error.",
                    "label": 0
                },
                {
                    "sent": "And the voting it comes about because these have positive and negative numbers that they add into the vote.",
                    "label": 0
                },
                {
                    "sent": "For the SVM in load.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mental cases, many of these will be 0 if you use them for high dimensional problems like we're talking about here.",
                    "label": 0
                },
                {
                    "sent": "If X is the whole image for example, then typically not many of them will be zero.",
                    "label": 0
                },
                {
                    "sent": "I will get solutions that are quite dense on the on the alphas.",
                    "label": 0
                },
                {
                    "sent": "So how do we visualize this?",
                    "label": 0
                },
                {
                    "sent": "So we need to create a statistical parametric map.",
                    "label": 0
                },
                {
                    "sent": "Now for the SVM.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So of course we use the the method we already talked about, so there are different versions of the preimage that one could be.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matching using, but this will give us local visualization.",
                    "label": 0
                },
                {
                    "sent": "We simply use this expression here.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to use precisely the scheme that I talked about before I'm going to take the SVM.",
                    "label": 0
                },
                {
                    "sent": "Think of it as a simple generative model, and then compute derivatives of the kernel function, right?",
                    "label": 0
                },
                {
                    "sent": "This is what this will amount to.",
                    "label": 0
                },
                {
                    "sent": "And this will give us again an indication of every voxel that participate in the classification.",
                    "label": 0
                },
                {
                    "sent": "How sensitive is the result?",
                    "label": 0
                },
                {
                    "sent": "So that box is value and use that as a measure of involvement and this will produce a map.",
                    "label": 0
                },
                {
                    "sent": "And here are some examples of the SVM.",
                    "label": 0
                },
                {
                    "sent": "Here is in the middle.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is again a particular fMRI study.",
                    "label": 0
                },
                {
                    "sent": "There's something that's having going on, so we're looking at slices that have motor cortex for example.",
                    "label": 0
                },
                {
                    "sent": "Down here, motor cortex involvement, and cerebellum involvement, and so on.",
                    "label": 0
                },
                {
                    "sent": "And here we show that such a sensitivity map for an SVM that's been trained.",
                    "label": 0
                },
                {
                    "sent": "So of course, once we have this weekend, we can compare it with other things as well, so that for those who have seen other alternatives that sub Asian alternative to the SVM that is called the relevance makes a machine that some would prefer to use because it gives more directly access to probabilities and you get very similar.",
                    "label": 0
                },
                {
                    "sent": "Maps for the two.",
                    "label": 0
                },
                {
                    "sent": "If you do simple logistic regression on Kern in kernel space, so this is using the same model but just a slightly different way of estimating the Alpha parameters.",
                    "label": 0
                },
                {
                    "sent": "SVM, the SVM has one way and this has a different way using a likelihood function we can see that we get even more similar results in this case, so the.",
                    "label": 0
                },
                {
                    "sent": "SVM is quite similar to logistic regression in encoding space as you would expect, because they are so similar in in.",
                    "label": 0
                },
                {
                    "sent": "In specification.",
                    "label": 0
                },
                {
                    "sent": "Of course you can plot these, so here we have volume so you can plot the pixel values and you can see that there is a lot of agreement between the two, so there's simply a scatter plot of the two Maps here.",
                    "label": 0
                },
                {
                    "sent": "So not only can we.",
                    "label": 0
                },
                {
                    "sent": "Use this kind of comparison to find out how similar the same model is on different datasets we've got.",
                    "label": 0
                },
                {
                    "sent": "We can also use the same idea to to ask how similar two different models.",
                    "label": 0
                },
                {
                    "sent": "That's what this plot.",
                    "label": 0
                },
                {
                    "sent": "Explains",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "This slide here is a small simulation showing that this scheme is really super efficient in detecting complicated.",
                    "label": 0
                },
                {
                    "sent": "Types of events in the imaging experiment.",
                    "label": 0
                },
                {
                    "sent": "So of course this is not a real imaging experiment for assimilation.",
                    "label": 0
                },
                {
                    "sent": "And the way the simulation setup was to say if we use linear discriminants in space, this means that we use functions that separate one state from the other by a simple hyperplane, right?",
                    "label": 0
                },
                {
                    "sent": "So we have a linear model essentially in space.",
                    "label": 0
                },
                {
                    "sent": "Then that types of events that we cannot separate and one very very simple pattern we cannot separate is that.",
                    "label": 0
                },
                {
                    "sent": "Imagine you have four groups of events that can save place.",
                    "label": 0
                },
                {
                    "sent": "Antonym, so I'm going to to make a little mental pictures.",
                    "label": 0
                },
                {
                    "sent": "We have four groups of data and these two here are in one class and these two in the other class.",
                    "label": 0
                },
                {
                    "sent": "Then there's no way that you can cut this by a linear separating hyperplane and get 2 on one side and two on the other sideline.",
                    "label": 0
                },
                {
                    "sent": "This in engineering, this called the X or problem, so this is an image in version of the X or problem.",
                    "label": 0
                },
                {
                    "sent": "Where am I have four regions that could be like areas that are producing activations or both signal and the two of them are coupled to the activation.",
                    "label": 0
                },
                {
                    "sent": "So I have a baseline on activated instead of scans here and these two guys here are couples in this X or way.",
                    "label": 0
                },
                {
                    "sent": "So this means that in one state.",
                    "label": 0
                },
                {
                    "sent": "So this is the baseline these two.",
                    "label": 0
                },
                {
                    "sent": "Are the same.",
                    "label": 0
                },
                {
                    "sent": "They change my entirely in time like a binary sequence, so this what you see the red signal here, but in in the baseline they're the same and then the activated their difference.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is a very complicated multilateral relation between the stimulus and the response in the brain that we would not be able to detect because this has precisely the structural.",
                    "label": 0
                },
                {
                    "sent": "The two other regions here, they're just random strong variances that simply inject noise, so this would be something like resting state patterns.",
                    "label": 0
                },
                {
                    "sent": "For example, right that are just binary strong signals that go on and off at times.",
                    "label": 0
                },
                {
                    "sent": "They are not associated with the stimulus and just running this algorithm that we show that we train a classifier to detect.",
                    "label": 0
                },
                {
                    "sent": "So we have a nonlinear kernel method that can get a low error rate on such a problem here and now we run the.",
                    "label": 1
                },
                {
                    "sent": "This visualization scheme aside, the sensitivity map and these are the sensitivity Maps that you get, so this isn't a good signal to noise ratio where the individual image looks like this.",
                    "label": 0
                },
                {
                    "sent": "And this is in an even worse.",
                    "label": 0
                },
                {
                    "sent": "We cannot even by the I you cannot see that, that is the patterns, and the noise is so strong that these four regions are simply disappear, right?",
                    "label": 0
                },
                {
                    "sent": "So this is the sensitivity map that you get and you can see that there's a high value in the two areas that I taking.",
                    "label": 0
                },
                {
                    "sent": "Part of the activation and very little in the confounders here that are the orthogonal sense.",
                    "label": 0
                },
                {
                    "sent": "This is the true sensitivity map.",
                    "label": 0
                },
                {
                    "sent": "If you would like to see.",
                    "label": 0
                },
                {
                    "sent": "This is the RC curve between the two of these measures.",
                    "label": 0
                },
                {
                    "sent": "How well if we cut this map in different levels, how well does it match this one?",
                    "label": 0
                },
                {
                    "sent": "And of course your it's perfect.",
                    "label": 0
                },
                {
                    "sent": "So this way of using nonlinear Maps and visualizing them using the sensitivity map can really detect the origin of really complicated interactions.",
                    "label": 0
                },
                {
                    "sent": "This is what we show by this this example, so I can recommend you to do that for.",
                    "label": 0
                },
                {
                    "sent": "Paradigms where you expect to have this kind of modular Tori.",
                    "label": 0
                },
                {
                    "sent": "Phenomena going on in your data, right?",
                    "label": 0
                },
                {
                    "sent": "Then you would not be able to see it with a linear discriminants or linear SVM.",
                    "label": 0
                },
                {
                    "sent": "Escape a bit more here and.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Go directly to two conclusions.",
                    "label": 0
                },
                {
                    "sent": "So I think the most important message I did get through that right.",
                    "label": 0
                },
                {
                    "sent": "This is that when we do machine learning in scientific data mining task like brain imaging, there always needs to agenda that.",
                    "label": 1
                },
                {
                    "sent": "We would like them to identify the model and we would like to understand what it tells us about the data.",
                    "label": 0
                },
                {
                    "sent": "And those two are about the same footing, right?",
                    "label": 1
                },
                {
                    "sent": "I mean the equally important in many ways.",
                    "label": 0
                },
                {
                    "sent": "And this is possible in the brain mapping case because we have a formal way of evaluating the interpretation we can do, we can simply get confidence interval on the visualization that we get the brain map.",
                    "label": 0
                },
                {
                    "sent": "And we can do that for virtually any model that people use, right?",
                    "label": 0
                },
                {
                    "sent": "Using these schemes are so general that you only need a likelihood function that can be directed.",
                    "label": 0
                },
                {
                    "sent": "You can compute the derivative off, that's the only requirement that I have.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "If we want the confidence interval, then we need to do some kind of resampling to measure an unbiased value of the variance, or find confidence intervals as I said and split half can do that.",
                    "label": 0
                },
                {
                    "sent": "Some more complicated mechanisms can be modeled also.",
                    "label": 0
                },
                {
                    "sent": "Then simple linear models.",
                    "label": 0
                },
                {
                    "sent": "If we go to nonlinear manifolds in the unsupervised case or two nonlinear SVM's in the supervised case.",
                    "label": 0
                },
                {
                    "sent": "And I should say that it's far from over like.",
                    "label": 0
                },
                {
                    "sent": "I mean it's it's really still very much there.",
                    "label": 0
                },
                {
                    "sent": "Lots and lots of things that one could discuss, and I hope that many of you will be interested in doing this and discuss it with us in.",
                    "label": 0
                },
                {
                    "sent": "On the private or in conferences other places.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's so many things to to still investigate and let me give you a little bit of some outlook.",
                    "label": 0
                },
                {
                    "sent": "What we're doing right now in Copenhagen.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one of the things that we're working on is to try to bring this whole framework into the wild.",
                    "label": 0
                },
                {
                    "sent": "So we would like to take the Brain Imaging experiment outside the MRI scanner is not.",
                    "label": 0
                },
                {
                    "sent": "It's not feasible at the moment to really do MRI scanning in large volumes.",
                    "label": 0
                },
                {
                    "sent": "I mean there are some ideas to how to do that and may people may actually realize it at some point, but right now it's not feasible.",
                    "label": 0
                },
                {
                    "sent": "So we have to go to another modality and EG.",
                    "label": 0
                },
                {
                    "sent": "For example, would be such a let's see, that would be easier to move around.",
                    "label": 0
                },
                {
                    "sent": "And this would take us from a situation where the brain is considered almost like a black box might be stimulated and see what happens to the brain, but it's not of the brain that stimulate itself.",
                    "label": 0
                },
                {
                    "sent": "Now, some paradigms that are being developed now where that is the case and this is moving in the right direction, but only if we allow the brain imaging experiment to go on in the real world on the real interaction with other people, for example, it's going to be an ecological, valid experiment where you could say that this is a brain.",
                    "label": 0
                },
                {
                    "sent": "Not an experimental subject.",
                    "label": 0
                },
                {
                    "sent": "So that's the vision that we would like to to to create this too.",
                    "label": 0
                },
                {
                    "sent": "And I should say also that we are definitely not alone in this.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's really lots and lots of activity.",
                    "label": 0
                },
                {
                    "sent": "This area that are pursuing all kinds of tricks to create mobile brain imaging.",
                    "label": 0
                },
                {
                    "sent": "So I'll take at the moment is is 2 fold.",
                    "label": 0
                },
                {
                    "sent": "So we have one system that is based on on mobile smartphone platform.",
                    "label": 0
                },
                {
                    "sent": "At the moment user very simple headset.",
                    "label": 0
                },
                {
                    "sent": "So this is actually a very inexpensive setup also.",
                    "label": 0
                },
                {
                    "sent": "So you can imagine doing it in a large population.",
                    "label": 0
                },
                {
                    "sent": "Typical research, but yet this is the emotive Epoc headset that we use that speaks wirelessly to to a smart phone.",
                    "label": 0
                },
                {
                    "sent": "And then you can do whatever type of experiments or things that you're interested in on the smart phone.",
                    "label": 0
                },
                {
                    "sent": "Smart phones are really potent, so here, for example, we do real time 3D reconstruction of the huge segments we tried to reconstruct the source locations on a simplified brain, so it has like 1000 points in this particular implementation.",
                    "label": 1
                },
                {
                    "sent": "All this code that runs in this case it's on a Nokia phone, but now it's been ported to.",
                    "label": 0
                },
                {
                    "sent": "To an Android platform, all this code is available if you.",
                    "label": 0
                },
                {
                    "sent": "If you're interested, you can download it.",
                    "label": 0
                },
                {
                    "sent": "And use your own emotive two to set it up on your own smartphone.",
                    "label": 0
                },
                {
                    "sent": "So this will allow us to do experiments on a much larger scale than than has been possible earlier on, for example, in in you could call it social neuroscience experiments, right?",
                    "label": 0
                },
                {
                    "sent": "If you're really interested in learning about how what happens that much longer, times K, so this would be over years.",
                    "label": 0
                },
                {
                    "sent": "For example, in a subject.",
                    "label": 0
                },
                {
                    "sent": "So that's a completely different type of experiment that nobody is.",
                    "label": 0
                },
                {
                    "sent": "That's really done in.",
                    "label": 0
                },
                {
                    "sent": "Healthy population or even.",
                    "label": 0
                },
                {
                    "sent": "In patients, that would be something like this.",
                    "label": 0
                },
                {
                    "sent": "So this is a company that we're working with incorporating on hyper safe, so they built this little device here that is even higher comfort or much higher comfort than the motive that is implanted under the skin.",
                    "label": 0
                },
                {
                    "sent": "And then it speaks again.",
                    "label": 0
                },
                {
                    "sent": "Through a coupling.",
                    "label": 0
                },
                {
                    "sent": "So in this case it's violence.",
                    "label": 0
                },
                {
                    "sent": "So here's a magnetic coupling to a device that looks like a hearing aid that sits outside.",
                    "label": 0
                },
                {
                    "sent": "And the clinical trials are going on at the moment with these devices.",
                    "label": 0
                },
                {
                    "sent": "And there has been one round of clinical trials where people wore this for a month.",
                    "label": 0
                },
                {
                    "sent": "And in the final device that they built, I mean they they expect to have this.",
                    "label": 0
                },
                {
                    "sent": "Implanted for years and the patient group that they are looking at at the moment is.",
                    "label": 0
                },
                {
                    "sent": "Diabetic population that cannot detect low blood sugar.",
                    "label": 0
                },
                {
                    "sent": "So if they over regulate that treatment they get low blood sugar, which is dangerous.",
                    "label": 0
                },
                {
                    "sent": "You can save them.",
                    "label": 0
                },
                {
                    "sent": "Unpredictably, and so on and then using e.g you can detect those states in a small window before it happens and and this is the particular device that they're building at the moment is aimed at.",
                    "label": 0
                },
                {
                    "sent": "But of course as a neuroscience type device, it's interesting from any other possibilities also.",
                    "label": 0
                },
                {
                    "sent": "OK, let me.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Just say the ransom references here, then some acknowledgments, and then thank you for the patience.",
                    "label": 0
                },
                {
                    "sent": "And I'm sorry that I've went overtime, but I hope it's worth it.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        }
    }
}