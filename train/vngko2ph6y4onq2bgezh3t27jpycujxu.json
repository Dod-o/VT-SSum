{
    "id": "vngko2ph6y4onq2bgezh3t27jpycujxu",
    "title": "How to make latent factors interpretable by feeding Factorization machines with knowledge graphs",
    "info": {
        "author": [
            "Vito Walter Anelli, SisInf Lab - Information Systems Laboratory, Politecnico di Bari"
        ],
        "published": "Dec. 10, 2019",
        "recorded": "October 2019",
        "category": [
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/iswc2019_anelli_latent_factors/",
    "segmentation": [
        [
            "Good morning.",
            "I'm Walter and then my PhD candidate at the Polytechnic University of Buy in Italy.",
            "What we have worked on.",
            "As an you interpretable machine learning model for recommendation, exploiting knowledge graphs."
        ],
        [
            "Today, interpretability and transparency are considered as very important aspect in recommendation scenario, in particular interpretability and explanations help users to understand the reasons behind or conditions, and these implies faster and better decisions.",
            "Transparency eliminates the idea of a creepy systems find users.",
            "And allows them to enjoy the recommendations.",
            "The."
        ],
        [
            "Interpretation of a recommendation model.",
            "Usually reflects the recommendation approach.",
            "For instance, in collaborative filtering algorithms like neighborhood based recommender system.",
            "I.",
            "The items are suggested based on similar users history.",
            "Here the interpretation is quite limited to items and users similarities in content based recommender systems.",
            "Domain knowledge is directly used to produce recommendations.",
            "And.",
            "For this reason, these models are easily interpretable and explanations are really effective."
        ],
        [
            "The bad news is that the best performing models are collaborative, and many of them make use of latent factors to move the item and users.",
            "In detail, in a matrix factorization algorithm.",
            "Users and items are modelled through latent factors vectors.",
            "Let combined approximate the rating matrix."
        ],
        [
            "The problem here.",
            "Is that these models are not interpretable?",
            "Nobody knows the exact meaning of a latent factor, and nobody knows the exact reason why an item is suggested."
        ],
        [
            "And these leads us directly to research question.",
            "Let's try to summarize them briefly.",
            "What we want is to create a new recommendation model with at least the same performance of a latent factor model.",
            "However, we want that it should be completely intractable.",
            "Moreover, if we use some semantics in it.",
            "We want that after the machine learning process, this semantics is preserved and finally we want to check if this method is able to compute knew."
        ],
        [
            "Meaning full features.",
            "Now we can focus on a general framework for latent factors model.",
            "Calls cold.",
            "Factorization machines.",
            "This is a general and flexible predictor.",
            "We are using it in a recommendation scenario, but we can use it in any regression task.",
            "Each training case is described by a tuple of values associated with different features.",
            "Here we have two main kinds of features, users and items.",
            "Since we are in a super supervised learning setting, also, each tuple is associated with a single numerical value, the rating."
        ],
        [
            "Also, the prediction formula is quite easy and flexible it contains."
        ],
        [
            "Global buyers based on the distribution of the original data."
        ],
        [
            "Items and users biases.",
            "Anne."
        ],
        [
            "And then the most important part, the factorized contribution, which denotes the interaction between different features.",
            "This."
        ],
        [
            "Interaction is modeled.",
            "As a matrix multiplication in which the inner dimension is K. Which is the length of latent factor vectors.",
            "And it is usually defined by the system designer.",
            "Now let's try to."
        ],
        [
            "Modify it a little bit.",
            "We can consider a collection of items I.",
            "But in our world.",
            "An item is a Knowledge graph entity like Star Wars movie here."
        ],
        [
            "Then we can extract the subgraph of that entity and we discover that Tribeca has acted.",
            "And is made in that movie, and it was obviously born on Kashyyk.",
            "On the other side, we know that Harrison Ford was born in Chicago."
        ],
        [
            "In general.",
            "We can define chains of predicates objects.",
            "As a single features and we can denote as F, the set of all these possible features.",
            "Then we can associate to each pair item feature a single numerical value which the nodes the importance of that feature for that item.",
            "Then we can take all these numerical values and put them in a vector to describe the item.",
            "Now we can compute these numerical values in several ways.",
            "We can use graph based or information."
        ],
        [
            "Yes, we will.",
            "Based techniques like exclusiveness or BM25.",
            "In this case, we use a standard TF IDF weighting scheme."
        ],
        [
            "To sum up, we accepted some knowledge from a knowledge graph.",
            "We have waited it and we have put it in a vector to describe items.",
            "Now."
        ],
        [
            "As in vector space models, we can take the item vectors in users history and average them to obtain user profile."
        ],
        [
            "Oh yeah, items that are described by vectors, but each factor now is an explicit fee."
        ],
        [
            "That comes from a knowledge graph.",
            "Also, thanks to Vector Space models, we have modeled the users the same way using the same features."
        ],
        [
            "And this means that we can use knowledge graph information to compute these factorized contribution between users and items."
        ],
        [
            "But now.",
            "The dimension of the of the factorization is fixed to the overall number of the considered features."
        ],
        [
            "To be fair, for this second example, we have chosen a Star Trek movie.",
            "In the mean, though, you can find your original TF IDF values associated with the features.",
            "On the left you can see what happens.",
            "After the training.",
            "You may notice that they are sorted in a different way.",
            "In detail, we can see that our approach considers space adventure fields more important feature to describe Star Trek with respect to Android Fields."
        ],
        [
            "At this point we have several different ways to compute recommendations.",
            "In this case, we have chosen to use item item similarity's computer because I'm vector similarity."
        ],
        [
            "For every experimental evaluation.",
            "We have chosen two different datasets, Yahoo movies and Facebook movies.",
            "Let's provide public mappings to DB pedia entities.",
            "For the features, we have chosen three different settings.",
            "Categorical city in which we can see the only the path starting weird dizzy terms subject ontological city in which we can see that only the path starting with RDF type.",
            "And factual setting for all the remaining features.",
            "Also, we have removed some rare features.",
            "Using a threshold based on missing connections in the Knowledge Graph."
        ],
        [
            "We have compared our coach against you regional factorization machine formula.",
            "Some neighborhood based models like item, kenana, user Keenan and Vector Space model.",
            "On the left you can see that our approach is the best one in categorical and on to logical settings.",
            "And it is not the best one, but it shows good performance in factual setting.",
            "On the right side.",
            "You can see the evolution of performance considering the different iterations.",
            "And you may notice that using knowledge graph information.",
            "We can boost the training of factorization machines within the 5th iteration."
        ],
        [
            "Now, under the recommended system perspective, we are OK. Because we have created an new model with I performance and that is completely interpretable.",
            "But we are not.",
            "The question is.",
            "Is your regional semantics.",
            "Preserved after the training.",
            "To answer this."
        ],
        [
            "Listen.",
            "We can use this item.",
            "We can see that it is described by AM features, in this case 10.",
            "And.",
            "We can suppose that after the training, two of these features are not in the top.",
            "Then we may say that you regional semantics.",
            "Is preserved within."
        ],
        [
            "Accuracy of 0.8.",
            "Where different."
        ],
        [
            "Values of AM.",
            "You can see that our approach is able to preserve the original semantics.",
            "And in detail is able to preserve more or less an average of 11 features.",
            "Over 12"
        ],
        [
            "Now.",
            "What about the other features that enter the top end?",
            "Are they meaningful or errors?",
            "To check it.",
            "Weasyl"
        ],
        [
            "Did the best feature in your regional representation.",
            "And we removed it, setting its value to 0.",
            "Then we train the model."
        ],
        [
            "If the model is able to compute it again in the top M. Then we assign one to the model, otherwise 0."
        ],
        [
            "Also, in this case we can see that our approach is able to compute meaningful features.",
            "To sum up, we have proposed a new recommendation model.",
            "Where I performance that make use of knowledge graphs and it is completely interpretable.",
            "Moreover, we suggested a simple but effective way to check if the original semantics is preserved after the training.",
            "And finally, we have shown that this method is able to come."
        ],
        [
            "Cute, meaningful features that maybe should be considered in the original knowledge graph."
        ],
        [
            "Thank you.",
            "Thank you.",
            "Your.",
            "OK. How is work makes?",
            "The algorithms are the machine more interested to work for us.",
            "Would it make it more interpretable?",
            "Also for the end users, because OK. Yeah, yeah.",
            "Weather yeah yeah yeah, I think we can use.",
            "Also this slide."
        ],
        [
            "OK.",
            "Thank you for the question.",
            "It is a really important question because we have a certain perspective in which OK we are able to understand how the formula the prediction formula is computing the recommendation.",
            "But on the other side, what we want is to explain what is happening behind to the users.",
            "OK now whi?",
            "Disease interpretable for us because we have.",
            "Two different views view on a user profile, which is basically the same as these.",
            "OK, weird values as associated with different features and we know that for that user specific feature is really important.",
            "OK, on the other side we have the same representation for the items.",
            "But an explanation to the user is a completely another thing, so maybe we can combine like in formula these values.",
            "So what we obtain is that maybe I like sci fi so at the end the multiplication between this feature in one item and the sync feature in my profile as a very high value.",
            "OK so we can say two to the user.",
            "Look, you have chosen.",
            "You could choose DS item becausw you are so similar to these item.",
            "Under this perspective of that feature or these features.",
            "But this is what we call pointwise explanation.",
            "On the other side, another interesting thing on which we are working on is to go beyond this and it is to say OK.",
            "In our opinion you prefer.",
            "Star Trek Two Star Wars becausw.",
            "There is a combination of values that makes Star Wars to be above.",
            "Star Trek to be above Star Wars for you.",
            "And this is a bit more complex idea, but it is more natural and it goes towards a more natural way to explain recommendations.",
            "I hope to answer you.",
            "OK. More.",
            "Complex.",
            "Video regression model.",
            "OK. OK, there are a lot of answers for this question.",
            "The first is factorization machines in its original version is trained through gradients, and this is a very general model and it is an iterative model.",
            "But there is a reason because I avoided to explain the optimization method and it is that.",
            "In recommend, the system alters the first thing that they say against us.",
            "Is that OK?",
            "You want to use a lot of semantic features, but this introduces a lot of computational complexity.",
            "How can you deal with this?",
            "And in this case we have a trick to solve this problem because now we are not approximating.",
            "We're not estimating the real value of the rating.",
            "But now in the literature Research's moved toward the ranking, so learning to rank in this case what we're doing is using Bayesian personalized ranking criterion to train factorization machines.",
            "And this means that the computational complexity is related to the samples that we take from the transaction metric, and no, and none not from the dimension of these metrics.",
            "And this shows that we are better than them producing recommendation.",
            "And this is the first answer, the second one.",
            "Neural networks are more nonlinear models.",
            "Yes, we are working on it.",
            "We have worked on it the the only tricky part is that you have to remember that here we are seeing these structure like cells in a matrix.",
            "But when we move.",
            "Two neural networks.",
            "What we're doing here is waiting edges.",
            "So we are not putting these values into the weights of the nodes of the neurons of the neural networks, but we are modeling.",
            "The weights between the layers.",
            "And it is very interesting because we are doing really.",
            "We're really doing this one.",
            "We are modeling the interaction between the different neurons.",
            "And so we have good results.",
            "But there are many, many similarities between factorization machines and generalization of factorization machines and neural networks.",
            "So it was quite easy."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Good morning.",
                    "label": 0
                },
                {
                    "sent": "I'm Walter and then my PhD candidate at the Polytechnic University of Buy in Italy.",
                    "label": 0
                },
                {
                    "sent": "What we have worked on.",
                    "label": 0
                },
                {
                    "sent": "As an you interpretable machine learning model for recommendation, exploiting knowledge graphs.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Today, interpretability and transparency are considered as very important aspect in recommendation scenario, in particular interpretability and explanations help users to understand the reasons behind or conditions, and these implies faster and better decisions.",
                    "label": 0
                },
                {
                    "sent": "Transparency eliminates the idea of a creepy systems find users.",
                    "label": 0
                },
                {
                    "sent": "And allows them to enjoy the recommendations.",
                    "label": 0
                },
                {
                    "sent": "The.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interpretation of a recommendation model.",
                    "label": 0
                },
                {
                    "sent": "Usually reflects the recommendation approach.",
                    "label": 0
                },
                {
                    "sent": "For instance, in collaborative filtering algorithms like neighborhood based recommender system.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "The items are suggested based on similar users history.",
                    "label": 0
                },
                {
                    "sent": "Here the interpretation is quite limited to items and users similarities in content based recommender systems.",
                    "label": 0
                },
                {
                    "sent": "Domain knowledge is directly used to produce recommendations.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "For this reason, these models are easily interpretable and explanations are really effective.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The bad news is that the best performing models are collaborative, and many of them make use of latent factors to move the item and users.",
                    "label": 0
                },
                {
                    "sent": "In detail, in a matrix factorization algorithm.",
                    "label": 0
                },
                {
                    "sent": "Users and items are modelled through latent factors vectors.",
                    "label": 1
                },
                {
                    "sent": "Let combined approximate the rating matrix.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem here.",
                    "label": 0
                },
                {
                    "sent": "Is that these models are not interpretable?",
                    "label": 0
                },
                {
                    "sent": "Nobody knows the exact meaning of a latent factor, and nobody knows the exact reason why an item is suggested.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And these leads us directly to research question.",
                    "label": 0
                },
                {
                    "sent": "Let's try to summarize them briefly.",
                    "label": 0
                },
                {
                    "sent": "What we want is to create a new recommendation model with at least the same performance of a latent factor model.",
                    "label": 1
                },
                {
                    "sent": "However, we want that it should be completely intractable.",
                    "label": 0
                },
                {
                    "sent": "Moreover, if we use some semantics in it.",
                    "label": 0
                },
                {
                    "sent": "We want that after the machine learning process, this semantics is preserved and finally we want to check if this method is able to compute knew.",
                    "label": 1
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Meaning full features.",
                    "label": 0
                },
                {
                    "sent": "Now we can focus on a general framework for latent factors model.",
                    "label": 0
                },
                {
                    "sent": "Calls cold.",
                    "label": 0
                },
                {
                    "sent": "Factorization machines.",
                    "label": 0
                },
                {
                    "sent": "This is a general and flexible predictor.",
                    "label": 0
                },
                {
                    "sent": "We are using it in a recommendation scenario, but we can use it in any regression task.",
                    "label": 0
                },
                {
                    "sent": "Each training case is described by a tuple of values associated with different features.",
                    "label": 0
                },
                {
                    "sent": "Here we have two main kinds of features, users and items.",
                    "label": 0
                },
                {
                    "sent": "Since we are in a super supervised learning setting, also, each tuple is associated with a single numerical value, the rating.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, the prediction formula is quite easy and flexible it contains.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Global buyers based on the distribution of the original data.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Items and users biases.",
                    "label": 0
                },
                {
                    "sent": "Anne.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the most important part, the factorized contribution, which denotes the interaction between different features.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interaction is modeled.",
                    "label": 0
                },
                {
                    "sent": "As a matrix multiplication in which the inner dimension is K. Which is the length of latent factor vectors.",
                    "label": 0
                },
                {
                    "sent": "And it is usually defined by the system designer.",
                    "label": 0
                },
                {
                    "sent": "Now let's try to.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Modify it a little bit.",
                    "label": 0
                },
                {
                    "sent": "We can consider a collection of items I.",
                    "label": 0
                },
                {
                    "sent": "But in our world.",
                    "label": 0
                },
                {
                    "sent": "An item is a Knowledge graph entity like Star Wars movie here.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we can extract the subgraph of that entity and we discover that Tribeca has acted.",
                    "label": 0
                },
                {
                    "sent": "And is made in that movie, and it was obviously born on Kashyyk.",
                    "label": 0
                },
                {
                    "sent": "On the other side, we know that Harrison Ford was born in Chicago.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In general.",
                    "label": 0
                },
                {
                    "sent": "We can define chains of predicates objects.",
                    "label": 0
                },
                {
                    "sent": "As a single features and we can denote as F, the set of all these possible features.",
                    "label": 0
                },
                {
                    "sent": "Then we can associate to each pair item feature a single numerical value which the nodes the importance of that feature for that item.",
                    "label": 0
                },
                {
                    "sent": "Then we can take all these numerical values and put them in a vector to describe the item.",
                    "label": 0
                },
                {
                    "sent": "Now we can compute these numerical values in several ways.",
                    "label": 0
                },
                {
                    "sent": "We can use graph based or information.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, we will.",
                    "label": 0
                },
                {
                    "sent": "Based techniques like exclusiveness or BM25.",
                    "label": 0
                },
                {
                    "sent": "In this case, we use a standard TF IDF weighting scheme.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To sum up, we accepted some knowledge from a knowledge graph.",
                    "label": 1
                },
                {
                    "sent": "We have waited it and we have put it in a vector to describe items.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As in vector space models, we can take the item vectors in users history and average them to obtain user profile.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh yeah, items that are described by vectors, but each factor now is an explicit fee.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That comes from a knowledge graph.",
                    "label": 0
                },
                {
                    "sent": "Also, thanks to Vector Space models, we have modeled the users the same way using the same features.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this means that we can use knowledge graph information to compute these factorized contribution between users and items.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But now.",
                    "label": 0
                },
                {
                    "sent": "The dimension of the of the factorization is fixed to the overall number of the considered features.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To be fair, for this second example, we have chosen a Star Trek movie.",
                    "label": 0
                },
                {
                    "sent": "In the mean, though, you can find your original TF IDF values associated with the features.",
                    "label": 0
                },
                {
                    "sent": "On the left you can see what happens.",
                    "label": 0
                },
                {
                    "sent": "After the training.",
                    "label": 0
                },
                {
                    "sent": "You may notice that they are sorted in a different way.",
                    "label": 0
                },
                {
                    "sent": "In detail, we can see that our approach considers space adventure fields more important feature to describe Star Trek with respect to Android Fields.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At this point we have several different ways to compute recommendations.",
                    "label": 0
                },
                {
                    "sent": "In this case, we have chosen to use item item similarity's computer because I'm vector similarity.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For every experimental evaluation.",
                    "label": 0
                },
                {
                    "sent": "We have chosen two different datasets, Yahoo movies and Facebook movies.",
                    "label": 1
                },
                {
                    "sent": "Let's provide public mappings to DB pedia entities.",
                    "label": 1
                },
                {
                    "sent": "For the features, we have chosen three different settings.",
                    "label": 1
                },
                {
                    "sent": "Categorical city in which we can see the only the path starting weird dizzy terms subject ontological city in which we can see that only the path starting with RDF type.",
                    "label": 0
                },
                {
                    "sent": "And factual setting for all the remaining features.",
                    "label": 1
                },
                {
                    "sent": "Also, we have removed some rare features.",
                    "label": 0
                },
                {
                    "sent": "Using a threshold based on missing connections in the Knowledge Graph.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We have compared our coach against you regional factorization machine formula.",
                    "label": 0
                },
                {
                    "sent": "Some neighborhood based models like item, kenana, user Keenan and Vector Space model.",
                    "label": 0
                },
                {
                    "sent": "On the left you can see that our approach is the best one in categorical and on to logical settings.",
                    "label": 0
                },
                {
                    "sent": "And it is not the best one, but it shows good performance in factual setting.",
                    "label": 0
                },
                {
                    "sent": "On the right side.",
                    "label": 0
                },
                {
                    "sent": "You can see the evolution of performance considering the different iterations.",
                    "label": 0
                },
                {
                    "sent": "And you may notice that using knowledge graph information.",
                    "label": 0
                },
                {
                    "sent": "We can boost the training of factorization machines within the 5th iteration.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now, under the recommended system perspective, we are OK. Because we have created an new model with I performance and that is completely interpretable.",
                    "label": 0
                },
                {
                    "sent": "But we are not.",
                    "label": 0
                },
                {
                    "sent": "The question is.",
                    "label": 0
                },
                {
                    "sent": "Is your regional semantics.",
                    "label": 0
                },
                {
                    "sent": "Preserved after the training.",
                    "label": 0
                },
                {
                    "sent": "To answer this.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Listen.",
                    "label": 0
                },
                {
                    "sent": "We can use this item.",
                    "label": 0
                },
                {
                    "sent": "We can see that it is described by AM features, in this case 10.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We can suppose that after the training, two of these features are not in the top.",
                    "label": 0
                },
                {
                    "sent": "Then we may say that you regional semantics.",
                    "label": 0
                },
                {
                    "sent": "Is preserved within.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Accuracy of 0.8.",
                    "label": 0
                },
                {
                    "sent": "Where different.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Values of AM.",
                    "label": 0
                },
                {
                    "sent": "You can see that our approach is able to preserve the original semantics.",
                    "label": 0
                },
                {
                    "sent": "And in detail is able to preserve more or less an average of 11 features.",
                    "label": 0
                },
                {
                    "sent": "Over 12",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "What about the other features that enter the top end?",
                    "label": 0
                },
                {
                    "sent": "Are they meaningful or errors?",
                    "label": 0
                },
                {
                    "sent": "To check it.",
                    "label": 0
                },
                {
                    "sent": "Weasyl",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Did the best feature in your regional representation.",
                    "label": 0
                },
                {
                    "sent": "And we removed it, setting its value to 0.",
                    "label": 0
                },
                {
                    "sent": "Then we train the model.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If the model is able to compute it again in the top M. Then we assign one to the model, otherwise 0.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, in this case we can see that our approach is able to compute meaningful features.",
                    "label": 0
                },
                {
                    "sent": "To sum up, we have proposed a new recommendation model.",
                    "label": 0
                },
                {
                    "sent": "Where I performance that make use of knowledge graphs and it is completely interpretable.",
                    "label": 0
                },
                {
                    "sent": "Moreover, we suggested a simple but effective way to check if the original semantics is preserved after the training.",
                    "label": 0
                },
                {
                    "sent": "And finally, we have shown that this method is able to come.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Cute, meaningful features that maybe should be considered in the original knowledge graph.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "Your.",
                    "label": 0
                },
                {
                    "sent": "OK. How is work makes?",
                    "label": 0
                },
                {
                    "sent": "The algorithms are the machine more interested to work for us.",
                    "label": 0
                },
                {
                    "sent": "Would it make it more interpretable?",
                    "label": 0
                },
                {
                    "sent": "Also for the end users, because OK. Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Weather yeah yeah yeah, I think we can use.",
                    "label": 0
                },
                {
                    "sent": "Also this slide.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Thank you for the question.",
                    "label": 0
                },
                {
                    "sent": "It is a really important question because we have a certain perspective in which OK we are able to understand how the formula the prediction formula is computing the recommendation.",
                    "label": 0
                },
                {
                    "sent": "But on the other side, what we want is to explain what is happening behind to the users.",
                    "label": 0
                },
                {
                    "sent": "OK now whi?",
                    "label": 0
                },
                {
                    "sent": "Disease interpretable for us because we have.",
                    "label": 0
                },
                {
                    "sent": "Two different views view on a user profile, which is basically the same as these.",
                    "label": 0
                },
                {
                    "sent": "OK, weird values as associated with different features and we know that for that user specific feature is really important.",
                    "label": 0
                },
                {
                    "sent": "OK, on the other side we have the same representation for the items.",
                    "label": 0
                },
                {
                    "sent": "But an explanation to the user is a completely another thing, so maybe we can combine like in formula these values.",
                    "label": 0
                },
                {
                    "sent": "So what we obtain is that maybe I like sci fi so at the end the multiplication between this feature in one item and the sync feature in my profile as a very high value.",
                    "label": 0
                },
                {
                    "sent": "OK so we can say two to the user.",
                    "label": 0
                },
                {
                    "sent": "Look, you have chosen.",
                    "label": 0
                },
                {
                    "sent": "You could choose DS item becausw you are so similar to these item.",
                    "label": 0
                },
                {
                    "sent": "Under this perspective of that feature or these features.",
                    "label": 0
                },
                {
                    "sent": "But this is what we call pointwise explanation.",
                    "label": 0
                },
                {
                    "sent": "On the other side, another interesting thing on which we are working on is to go beyond this and it is to say OK.",
                    "label": 0
                },
                {
                    "sent": "In our opinion you prefer.",
                    "label": 0
                },
                {
                    "sent": "Star Trek Two Star Wars becausw.",
                    "label": 0
                },
                {
                    "sent": "There is a combination of values that makes Star Wars to be above.",
                    "label": 0
                },
                {
                    "sent": "Star Trek to be above Star Wars for you.",
                    "label": 0
                },
                {
                    "sent": "And this is a bit more complex idea, but it is more natural and it goes towards a more natural way to explain recommendations.",
                    "label": 0
                },
                {
                    "sent": "I hope to answer you.",
                    "label": 0
                },
                {
                    "sent": "OK. More.",
                    "label": 0
                },
                {
                    "sent": "Complex.",
                    "label": 0
                },
                {
                    "sent": "Video regression model.",
                    "label": 0
                },
                {
                    "sent": "OK. OK, there are a lot of answers for this question.",
                    "label": 0
                },
                {
                    "sent": "The first is factorization machines in its original version is trained through gradients, and this is a very general model and it is an iterative model.",
                    "label": 0
                },
                {
                    "sent": "But there is a reason because I avoided to explain the optimization method and it is that.",
                    "label": 0
                },
                {
                    "sent": "In recommend, the system alters the first thing that they say against us.",
                    "label": 0
                },
                {
                    "sent": "Is that OK?",
                    "label": 0
                },
                {
                    "sent": "You want to use a lot of semantic features, but this introduces a lot of computational complexity.",
                    "label": 0
                },
                {
                    "sent": "How can you deal with this?",
                    "label": 0
                },
                {
                    "sent": "And in this case we have a trick to solve this problem because now we are not approximating.",
                    "label": 0
                },
                {
                    "sent": "We're not estimating the real value of the rating.",
                    "label": 0
                },
                {
                    "sent": "But now in the literature Research's moved toward the ranking, so learning to rank in this case what we're doing is using Bayesian personalized ranking criterion to train factorization machines.",
                    "label": 0
                },
                {
                    "sent": "And this means that the computational complexity is related to the samples that we take from the transaction metric, and no, and none not from the dimension of these metrics.",
                    "label": 0
                },
                {
                    "sent": "And this shows that we are better than them producing recommendation.",
                    "label": 0
                },
                {
                    "sent": "And this is the first answer, the second one.",
                    "label": 0
                },
                {
                    "sent": "Neural networks are more nonlinear models.",
                    "label": 0
                },
                {
                    "sent": "Yes, we are working on it.",
                    "label": 0
                },
                {
                    "sent": "We have worked on it the the only tricky part is that you have to remember that here we are seeing these structure like cells in a matrix.",
                    "label": 0
                },
                {
                    "sent": "But when we move.",
                    "label": 0
                },
                {
                    "sent": "Two neural networks.",
                    "label": 0
                },
                {
                    "sent": "What we're doing here is waiting edges.",
                    "label": 0
                },
                {
                    "sent": "So we are not putting these values into the weights of the nodes of the neurons of the neural networks, but we are modeling.",
                    "label": 0
                },
                {
                    "sent": "The weights between the layers.",
                    "label": 0
                },
                {
                    "sent": "And it is very interesting because we are doing really.",
                    "label": 0
                },
                {
                    "sent": "We're really doing this one.",
                    "label": 0
                },
                {
                    "sent": "We are modeling the interaction between the different neurons.",
                    "label": 0
                },
                {
                    "sent": "And so we have good results.",
                    "label": 0
                },
                {
                    "sent": "But there are many, many similarities between factorization machines and generalization of factorization machines and neural networks.",
                    "label": 1
                },
                {
                    "sent": "So it was quite easy.",
                    "label": 0
                }
            ]
        }
    }
}