{
    "id": "okiroizuiasqrijf5grkdsyl5zde4zp4",
    "title": "Subspace Learning",
    "info": {
        "author": [
            "Alessandro Rudi, Istituto Italiano di Tecnologia"
        ],
        "published": "Aug. 26, 2013",
        "recorded": "July 2013",
        "category": [
            "Top->Computer Science->Optimization Methods",
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines",
            "Top->Computer Science->Compressed Sensing",
            "Top->Computer Science->Machine Learning->Regularization"
        ]
    },
    "url": "http://videolectures.net/roks2013_rudi_subspace/",
    "segmentation": [
        [
            "OK, this is a work I did.",
            "We did.",
            "We had Mccanna so Massachusetts Institute of Technology and Lorenzo Rosasco MIT and the University of Genoa.",
            "This is just a theoretical work so no new algorithms will be presented.",
            "And it is about a subspace learning problem.",
            "We give learning rates for this problem."
        ],
        [
            "So in our setting we have available space age, probability, distribution, role.",
            "And we rolled.",
            "It is the smallest linear subspace that contain the support of the distribution.",
            "The goal of the problem is to finance a."
        ],
        [
            "Anyway, we're all given a number of examples drawn from the distribution.",
            "OK."
        ],
        [
            "Here the Hilbert space can be finite dimensional.",
            "It is a good model because it can be the limit for high dimensional data.",
            "So when the number of examples is smaller than the number of dimension of the problem or it takes into account even cases in which we have embedded data, so we have a distribution on a measurable space said and we choose a suitable feature map to.",
            "To obtain our Hilbert space.",
            "So in this case there will be space.",
            "We will be the closure of the span of the image of said immune.",
            "The associated push forward measure."
        ],
        [
            "OK, in this presentation I will speak about two algorithms that are very sensitive to subspace learning rates.",
            "The first PCA was goal is to exactly estimate 0.",
            "Here is characterized as the smallest linear subspace which has the maximal variance.",
            "And it is a very important tax extension that is kernel PCA that performs PCA on the data embedded in H by official map C. Possibly given by a Colonel.",
            "Given by a Colonel."
        ],
        [
            "Another example is a kernel support estimation when we want to estimate the support of a distribution mu on a metric space, and we use a feature map.",
            "In this setting, Viro is characterized as the.",
            "Closure of this pane of the image of the of the support.",
            "If row is the separating property, we have, the M is exactly the set of all points was images belong to the set to the subspace viral, and so it is very important 'cause once we learn video we can exactly recover M. This property is miss started by the veto and others, and there are a lot of kernels.",
            "For example, in there today that show this property.",
            "For example, table kernel, the convex combination of product of kernel.",
            "With this property.",
            "An important fact is that Gaussian kernel is not so powerful to separate all the to have this property, so to separate all the closest subset of a given metric space."
        ],
        [
            "OK, now we can stay tomorrow formally the problem.",
            "So, given a number of examples drawn independently from row, we want to find an estimate or linear subspace such that we are able to control the distance of the estimator from the EDL.",
            "Subspace.",
            "Improbability now we have two questions.",
            "First of all, which estimator I should use and then which family of distances I should use?",
            "OK."
        ],
        [
            "Now we see the last characterization of zero.",
            "That is, just the closure of the span of the aging vectors of the covariance operator associated to the distribution role."
        ],
        [
            "This leads to the natural to this natural estimation estimator that is, the truncated estimator that is just the span of the first gate.",
            "The first K again vectors of the empirical covariance operator.",
            "OK, here we are.",
            "Two more questions, which is a good value of K and if we can use simply key equal to N. OK."
        ],
        [
            "Just to complete the settings, we have studied the family of distances that is quite wide and is completely operate.",
            "Oriel here sees the covariance operator PUMPVR the.",
            "Projection operators associated to do subspaces and the norm used is the Schatten norm.",
            "His family generalizes a lot of use.",
            "The distances on subspaces, for example, the principal angles distance with Alpha.",
            "Well, zero and P = 2 or the gap distance with the Alpha equals 0 equal in finite.",
            "But even more specific distances, for example, the one useful for the two problems we have, we have seen the two algorithms we have seen."
        ],
        [
            "For kernel PCA.",
            "Nick only used the distance.",
            "An error is the reconstruction error that is just the pointwise reprojection error mediated by roll, and it comes out that exactly the square of our distance with Alpha equals 1/2 equal to.",
            "The reconstruction error is good.",
            "A good distance becausw it respekt the natural order induced by the inclusion between linear subspaces."
        ],
        [
            "The other metric for support estimation is the following.",
            "We have to consider that this the subset M that is the support of the distribution is characterized by a membership function.",
            "It is simply the.",
            "The distance of the image vector from the linear space, viral and this leads to a natural estimation estimator that is just given by the approximation of the of the membership function.",
            "So we compute the distance of the image vector from the empirical subspace.",
            "Little cattle estimator.",
            "And in order to study the convergence of 1 set.",
            "To the other.",
            "Important is of interest to study the uniform convergence of the two functions and it we can see that it is controlled by the distance in our family with being finite and Alpha.",
            "That depends on the value decay of the covariance matrix."
        ],
        [
            "OK, now we see some properties on the general metric.",
            "OK, our family met rises.",
            "The collection of all subspaces of Arrow with Alpha between zero and one MP between one and finite, and so each truncated estimator belongs to this collection.",
            "So is materialized by the by distance in this family an so computing distance between?",
            "The truncated estimator and true subspace.",
            "Is it allowed?",
            "OK, and then this family of distances respect the natural order of the subset inclusions just to conclude on the metrics.",
            "This family of metric controller variety of metric that are classically used to measure distance between sets.",
            "Classical text on topology on closed convex sets.",
            "OK."
        ],
        [
            "Now we see."
        ],
        [
            "There is Alton.",
            "This is just a probability tail bound that controls the distance between the empirical estimator and invero, and it is done by two parts.",
            "The first T to the Alpha depends on the Kate again value of C and the 2nd is just a generalization of the affective dimension and takes into account the shape of the value decay of the covariance operator.",
            "To prove this theorem we use tools from spectral theory, lowner partial orderings, concentration bounds and operators.",
            "The one very useful on the Infinity norm that I found them.",
            "Interop 2010"
        ],
        [
            "OK, this is a special injection of the result when we assume that the gain value decay of the covariance operator is polynomial, we can find 2 regimes that depends on the number of components we select.",
            "And the first regime went when the components are smaller than a Kiss star number of components.",
            "We have a polynomial decay rate, while after Kiss Star we have a plateau.",
            "So we have a constant value of the error."
        ],
        [
            "OK, we can see it for the reconstruction error.",
            "It isn't monotonic.",
            "Decreasing, and this is the.",
            "The previous bound, written for the reconstruction error."
        ],
        [
            "Here I compared this result, so the learning rate our learning rate for kernel PCA reconstruction error with the important bounds in literature.",
            "The one of blood sharp 2007.",
            "Here I am showing the.",
            "Learning rate.",
            "Decay, the exponent of the decay with respect to the again value decay rate.",
            "So the higher is, the better and the dotted one is of blood Shard.",
            "It uses local Rademacher complexities, you statistics and further assumptions on the 4th order moment of eigenvalues of different order moment.",
            "Disease, our bound.",
            "The Purple one we use is only a assumption on the 2nd on the covariance operator, so that is taken value decay rate and this one is overshoot Taylor 2005.",
            "He he didn't use any aside if I remember well any assumption on value decay OK?",
            "And OK, the blush artist that I uploaded the blusher result when the 1st order a game value decay rate is the best possible when it is the worst possible is just equal to the one of show Taylor.",
            "OK."
        ],
        [
            "Then I specified the result for the distance of user in kernel support estimation.",
            "Yeah, we have the same structure but with an Alpha becausw the status parameterized by Alpha.",
            "We see that in all the cases, case star depends on on fractional power of the number of examples.",
            "OK then."
        ],
        [
            "I compare this result with the work of Lievito and we can see that our.",
            "The behavior of the distance.",
            "Of our bound is naturally reflects the.",
            "Then they order induced by the inclusion of linear subsets becausw we can we know that VK is a subset of VK plus one, so this is bound in fact our is decreasing the one of the veto is not so sharp and shows an artificial minimum.",
            "OK, underwrite are there is the.",
            "Is in the last case comparison on the.",
            "Exponent of the decay rate.",
            "Hour is.",
            "Far more better than the one of Divito."
        ],
        [
            "OK."
        ],
        [
            "Just for concluding, I did.",
            "An experiment just to see if our bound in some way close to what happens in reality.",
            "And so I choose.",
            "A problem this completely computable analytically.",
            "Those I select a uniform distribution on the interval 01 on the dimensional plane using the Dell one kernel and I repeated kernel PCA 1000 times with 1000 point independently drawn from the distribution we can see the.",
            "Distribution of the of the game value.",
            "Taking values of the associated covariance operator.",
            "Empirical covariance operator."
        ],
        [
            "OK, here I computed the exactly the distance.",
            "And the value for kistar what we see is that.",
            "This graph is very similar to the one to the theoretical one."
        ],
        [
            "This this one and even Kiss star.",
            "Is in the position."
        ],
        [
            "We should expect it to be OK. Just a qualitative result."
        ],
        [
            "OK, then I need another experiment.",
            "See then."
        ],
        [
            "Medical properties of the problem.",
            "So I did perform the computations of the previous experiment with 32 bits floating point precision and they computed the number of components from one to 1000 instead of 1, two 50.",
            "And what we see is that the behavior is like the one in the bound until K equal.",
            "What is this 100 OK and and then we have an exponential growth of there.",
            "What we see is that performing kernel PCA is very close to inverting a matrix.",
            "So when we select a number of components, we are just inverting very small eigenvalues.",
            "So using only 32 bits of floating point precision, what we have is that after a few components we start to have a huge numerical error.",
            "So probably a good tradeoff that should be interesting to study from.",
            "Point of view is between statistical error and numerical one."
        ],
        [
            "OK, so this work I gave learning rates for a wide range of metrics, linear subspaces, specific results for current PC and spectral support estimation, and an optimal value case start for the truncated estimation estimator.",
            "Hey concerning future work I am interested in studying theoretical analysis and statistical and computational tradeoff and then.",
            "A completely new question that is what happens if I have a model of noise on the data.",
            "This is the."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, this is a work I did.",
                    "label": 0
                },
                {
                    "sent": "We did.",
                    "label": 0
                },
                {
                    "sent": "We had Mccanna so Massachusetts Institute of Technology and Lorenzo Rosasco MIT and the University of Genoa.",
                    "label": 1
                },
                {
                    "sent": "This is just a theoretical work so no new algorithms will be presented.",
                    "label": 0
                },
                {
                    "sent": "And it is about a subspace learning problem.",
                    "label": 0
                },
                {
                    "sent": "We give learning rates for this problem.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in our setting we have available space age, probability, distribution, role.",
                    "label": 1
                },
                {
                    "sent": "And we rolled.",
                    "label": 0
                },
                {
                    "sent": "It is the smallest linear subspace that contain the support of the distribution.",
                    "label": 1
                },
                {
                    "sent": "The goal of the problem is to finance a.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anyway, we're all given a number of examples drawn from the distribution.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here the Hilbert space can be finite dimensional.",
                    "label": 0
                },
                {
                    "sent": "It is a good model because it can be the limit for high dimensional data.",
                    "label": 1
                },
                {
                    "sent": "So when the number of examples is smaller than the number of dimension of the problem or it takes into account even cases in which we have embedded data, so we have a distribution on a measurable space said and we choose a suitable feature map to.",
                    "label": 1
                },
                {
                    "sent": "To obtain our Hilbert space.",
                    "label": 0
                },
                {
                    "sent": "So in this case there will be space.",
                    "label": 0
                },
                {
                    "sent": "We will be the closure of the span of the image of said immune.",
                    "label": 0
                },
                {
                    "sent": "The associated push forward measure.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, in this presentation I will speak about two algorithms that are very sensitive to subspace learning rates.",
                    "label": 0
                },
                {
                    "sent": "The first PCA was goal is to exactly estimate 0.",
                    "label": 0
                },
                {
                    "sent": "Here is characterized as the smallest linear subspace which has the maximal variance.",
                    "label": 0
                },
                {
                    "sent": "And it is a very important tax extension that is kernel PCA that performs PCA on the data embedded in H by official map C. Possibly given by a Colonel.",
                    "label": 1
                },
                {
                    "sent": "Given by a Colonel.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Another example is a kernel support estimation when we want to estimate the support of a distribution mu on a metric space, and we use a feature map.",
                    "label": 0
                },
                {
                    "sent": "In this setting, Viro is characterized as the.",
                    "label": 0
                },
                {
                    "sent": "Closure of this pane of the image of the of the support.",
                    "label": 0
                },
                {
                    "sent": "If row is the separating property, we have, the M is exactly the set of all points was images belong to the set to the subspace viral, and so it is very important 'cause once we learn video we can exactly recover M. This property is miss started by the veto and others, and there are a lot of kernels.",
                    "label": 0
                },
                {
                    "sent": "For example, in there today that show this property.",
                    "label": 0
                },
                {
                    "sent": "For example, table kernel, the convex combination of product of kernel.",
                    "label": 1
                },
                {
                    "sent": "With this property.",
                    "label": 1
                },
                {
                    "sent": "An important fact is that Gaussian kernel is not so powerful to separate all the to have this property, so to separate all the closest subset of a given metric space.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now we can stay tomorrow formally the problem.",
                    "label": 0
                },
                {
                    "sent": "So, given a number of examples drawn independently from row, we want to find an estimate or linear subspace such that we are able to control the distance of the estimator from the EDL.",
                    "label": 1
                },
                {
                    "sent": "Subspace.",
                    "label": 0
                },
                {
                    "sent": "Improbability now we have two questions.",
                    "label": 0
                },
                {
                    "sent": "First of all, which estimator I should use and then which family of distances I should use?",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we see the last characterization of zero.",
                    "label": 0
                },
                {
                    "sent": "That is, just the closure of the span of the aging vectors of the covariance operator associated to the distribution role.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This leads to the natural to this natural estimation estimator that is, the truncated estimator that is just the span of the first gate.",
                    "label": 0
                },
                {
                    "sent": "The first K again vectors of the empirical covariance operator.",
                    "label": 1
                },
                {
                    "sent": "OK, here we are.",
                    "label": 0
                },
                {
                    "sent": "Two more questions, which is a good value of K and if we can use simply key equal to N. OK.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to complete the settings, we have studied the family of distances that is quite wide and is completely operate.",
                    "label": 0
                },
                {
                    "sent": "Oriel here sees the covariance operator PUMPVR the.",
                    "label": 1
                },
                {
                    "sent": "Projection operators associated to do subspaces and the norm used is the Schatten norm.",
                    "label": 0
                },
                {
                    "sent": "His family generalizes a lot of use.",
                    "label": 0
                },
                {
                    "sent": "The distances on subspaces, for example, the principal angles distance with Alpha.",
                    "label": 0
                },
                {
                    "sent": "Well, zero and P = 2 or the gap distance with the Alpha equals 0 equal in finite.",
                    "label": 0
                },
                {
                    "sent": "But even more specific distances, for example, the one useful for the two problems we have, we have seen the two algorithms we have seen.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For kernel PCA.",
                    "label": 0
                },
                {
                    "sent": "Nick only used the distance.",
                    "label": 0
                },
                {
                    "sent": "An error is the reconstruction error that is just the pointwise reprojection error mediated by roll, and it comes out that exactly the square of our distance with Alpha equals 1/2 equal to.",
                    "label": 0
                },
                {
                    "sent": "The reconstruction error is good.",
                    "label": 0
                },
                {
                    "sent": "A good distance becausw it respekt the natural order induced by the inclusion between linear subspaces.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other metric for support estimation is the following.",
                    "label": 1
                },
                {
                    "sent": "We have to consider that this the subset M that is the support of the distribution is characterized by a membership function.",
                    "label": 0
                },
                {
                    "sent": "It is simply the.",
                    "label": 0
                },
                {
                    "sent": "The distance of the image vector from the linear space, viral and this leads to a natural estimation estimator that is just given by the approximation of the of the membership function.",
                    "label": 0
                },
                {
                    "sent": "So we compute the distance of the image vector from the empirical subspace.",
                    "label": 0
                },
                {
                    "sent": "Little cattle estimator.",
                    "label": 0
                },
                {
                    "sent": "And in order to study the convergence of 1 set.",
                    "label": 1
                },
                {
                    "sent": "To the other.",
                    "label": 0
                },
                {
                    "sent": "Important is of interest to study the uniform convergence of the two functions and it we can see that it is controlled by the distance in our family with being finite and Alpha.",
                    "label": 1
                },
                {
                    "sent": "That depends on the value decay of the covariance matrix.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, now we see some properties on the general metric.",
                    "label": 0
                },
                {
                    "sent": "OK, our family met rises.",
                    "label": 0
                },
                {
                    "sent": "The collection of all subspaces of Arrow with Alpha between zero and one MP between one and finite, and so each truncated estimator belongs to this collection.",
                    "label": 1
                },
                {
                    "sent": "So is materialized by the by distance in this family an so computing distance between?",
                    "label": 0
                },
                {
                    "sent": "The truncated estimator and true subspace.",
                    "label": 0
                },
                {
                    "sent": "Is it allowed?",
                    "label": 0
                },
                {
                    "sent": "OK, and then this family of distances respect the natural order of the subset inclusions just to conclude on the metrics.",
                    "label": 0
                },
                {
                    "sent": "This family of metric controller variety of metric that are classically used to measure distance between sets.",
                    "label": 1
                },
                {
                    "sent": "Classical text on topology on closed convex sets.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we see.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There is Alton.",
                    "label": 0
                },
                {
                    "sent": "This is just a probability tail bound that controls the distance between the empirical estimator and invero, and it is done by two parts.",
                    "label": 0
                },
                {
                    "sent": "The first T to the Alpha depends on the Kate again value of C and the 2nd is just a generalization of the affective dimension and takes into account the shape of the value decay of the covariance operator.",
                    "label": 1
                },
                {
                    "sent": "To prove this theorem we use tools from spectral theory, lowner partial orderings, concentration bounds and operators.",
                    "label": 1
                },
                {
                    "sent": "The one very useful on the Infinity norm that I found them.",
                    "label": 0
                },
                {
                    "sent": "Interop 2010",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, this is a special injection of the result when we assume that the gain value decay of the covariance operator is polynomial, we can find 2 regimes that depends on the number of components we select.",
                    "label": 1
                },
                {
                    "sent": "And the first regime went when the components are smaller than a Kiss star number of components.",
                    "label": 1
                },
                {
                    "sent": "We have a polynomial decay rate, while after Kiss Star we have a plateau.",
                    "label": 0
                },
                {
                    "sent": "So we have a constant value of the error.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, we can see it for the reconstruction error.",
                    "label": 0
                },
                {
                    "sent": "It isn't monotonic.",
                    "label": 0
                },
                {
                    "sent": "Decreasing, and this is the.",
                    "label": 0
                },
                {
                    "sent": "The previous bound, written for the reconstruction error.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here I compared this result, so the learning rate our learning rate for kernel PCA reconstruction error with the important bounds in literature.",
                    "label": 1
                },
                {
                    "sent": "The one of blood sharp 2007.",
                    "label": 0
                },
                {
                    "sent": "Here I am showing the.",
                    "label": 0
                },
                {
                    "sent": "Learning rate.",
                    "label": 0
                },
                {
                    "sent": "Decay, the exponent of the decay with respect to the again value decay rate.",
                    "label": 0
                },
                {
                    "sent": "So the higher is, the better and the dotted one is of blood Shard.",
                    "label": 1
                },
                {
                    "sent": "It uses local Rademacher complexities, you statistics and further assumptions on the 4th order moment of eigenvalues of different order moment.",
                    "label": 0
                },
                {
                    "sent": "Disease, our bound.",
                    "label": 0
                },
                {
                    "sent": "The Purple one we use is only a assumption on the 2nd on the covariance operator, so that is taken value decay rate and this one is overshoot Taylor 2005.",
                    "label": 1
                },
                {
                    "sent": "He he didn't use any aside if I remember well any assumption on value decay OK?",
                    "label": 0
                },
                {
                    "sent": "And OK, the blush artist that I uploaded the blusher result when the 1st order a game value decay rate is the best possible when it is the worst possible is just equal to the one of show Taylor.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then I specified the result for the distance of user in kernel support estimation.",
                    "label": 1
                },
                {
                    "sent": "Yeah, we have the same structure but with an Alpha becausw the status parameterized by Alpha.",
                    "label": 0
                },
                {
                    "sent": "We see that in all the cases, case star depends on on fractional power of the number of examples.",
                    "label": 0
                },
                {
                    "sent": "OK then.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I compare this result with the work of Lievito and we can see that our.",
                    "label": 1
                },
                {
                    "sent": "The behavior of the distance.",
                    "label": 1
                },
                {
                    "sent": "Of our bound is naturally reflects the.",
                    "label": 0
                },
                {
                    "sent": "Then they order induced by the inclusion of linear subsets becausw we can we know that VK is a subset of VK plus one, so this is bound in fact our is decreasing the one of the veto is not so sharp and shows an artificial minimum.",
                    "label": 0
                },
                {
                    "sent": "OK, underwrite are there is the.",
                    "label": 1
                },
                {
                    "sent": "Is in the last case comparison on the.",
                    "label": 0
                },
                {
                    "sent": "Exponent of the decay rate.",
                    "label": 0
                },
                {
                    "sent": "Hour is.",
                    "label": 0
                },
                {
                    "sent": "Far more better than the one of Divito.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just for concluding, I did.",
                    "label": 0
                },
                {
                    "sent": "An experiment just to see if our bound in some way close to what happens in reality.",
                    "label": 0
                },
                {
                    "sent": "And so I choose.",
                    "label": 0
                },
                {
                    "sent": "A problem this completely computable analytically.",
                    "label": 0
                },
                {
                    "sent": "Those I select a uniform distribution on the interval 01 on the dimensional plane using the Dell one kernel and I repeated kernel PCA 1000 times with 1000 point independently drawn from the distribution we can see the.",
                    "label": 1
                },
                {
                    "sent": "Distribution of the of the game value.",
                    "label": 0
                },
                {
                    "sent": "Taking values of the associated covariance operator.",
                    "label": 1
                },
                {
                    "sent": "Empirical covariance operator.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, here I computed the exactly the distance.",
                    "label": 0
                },
                {
                    "sent": "And the value for kistar what we see is that.",
                    "label": 0
                },
                {
                    "sent": "This graph is very similar to the one to the theoretical one.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This this one and even Kiss star.",
                    "label": 0
                },
                {
                    "sent": "Is in the position.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We should expect it to be OK. Just a qualitative result.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, then I need another experiment.",
                    "label": 0
                },
                {
                    "sent": "See then.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Medical properties of the problem.",
                    "label": 1
                },
                {
                    "sent": "So I did perform the computations of the previous experiment with 32 bits floating point precision and they computed the number of components from one to 1000 instead of 1, two 50.",
                    "label": 1
                },
                {
                    "sent": "And what we see is that the behavior is like the one in the bound until K equal.",
                    "label": 0
                },
                {
                    "sent": "What is this 100 OK and and then we have an exponential growth of there.",
                    "label": 1
                },
                {
                    "sent": "What we see is that performing kernel PCA is very close to inverting a matrix.",
                    "label": 0
                },
                {
                    "sent": "So when we select a number of components, we are just inverting very small eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So using only 32 bits of floating point precision, what we have is that after a few components we start to have a huge numerical error.",
                    "label": 0
                },
                {
                    "sent": "So probably a good tradeoff that should be interesting to study from.",
                    "label": 0
                },
                {
                    "sent": "Point of view is between statistical error and numerical one.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so this work I gave learning rates for a wide range of metrics, linear subspaces, specific results for current PC and spectral support estimation, and an optimal value case start for the truncated estimation estimator.",
                    "label": 1
                },
                {
                    "sent": "Hey concerning future work I am interested in studying theoretical analysis and statistical and computational tradeoff and then.",
                    "label": 0
                },
                {
                    "sent": "A completely new question that is what happens if I have a model of noise on the data.",
                    "label": 0
                },
                {
                    "sent": "This is the.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}