{
    "id": "coc77jmdd7xflopeoubc355drqv6mpll",
    "title": "New Developments in the Theory of Clustering",
    "info": {
        "author": [
            "Sergei Vassilvitskii, Yahoo! Research",
            "Suresh Venkatasubramanian, School of Computing, University of Utah"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Machine Learning->Clustering"
        ]
    },
    "url": "http://videolectures.net/kdd2010_vassilvitskii_venkatasubramanian_ndt/",
    "segmentation": [
        [
            "I mean, we're going to talk about clustering and kind of new advances in new developments in the theory of clustering and the real tagline for the talk is the subtitle of the talk, so that's all very well in practice.",
            "But does it work in theory?",
            "And there's a second sub line that says, well?",
            "It's all very well in theory, but does it work in practice and so will balance off between these two as we go forward through the slides."
        ],
        [
            "So.",
            "What are we going to cover?",
            "So we're going to have two major themes, so we're going to cover it and talk about practical algorithms for clustering and actually give you some experimental results for some of these algorithms that actually have also, in addition, strong theoretical guarantees so we know it's not just a heuristic, but we can prove some bounds on the performance of these algorithms, weather and running time, or in approximations, or what have you.",
            "And we're also going to talk about models.",
            "Kind of theoretical models to explain the behavior observed in practice, so from theory point maybe some algorithm looks absolutely horrible and awful, and maybe that's why it hasn't been studied by the theory community, but maybe they were looking at it from a wrong lens.",
            "So if we look at it from the right way and we look about, we try to explain a capture the right properties of the algorithm or the data, then we actually get good results and we can explain what's going on and that gives us new insights as to what to try and how to improve on these algorithms.",
            "These are two goals, but theirs."
        ],
        [
            "A whole lot of stuff that we're not going to cover, so as kind of everybody knows, clustering is a huge field.",
            "That's an ancient field, and it's been around forever and the number of papers on clustering or that mention the word cluster has been growing exponentially for the past X hundreds of years together.",
            "But, and even more so, in the past couple of years.",
            "So because of that there's whole swaths of areas that we're not even going to touch.",
            "So, for example, we won't talk about meta clustering.",
            "We're not going to talk about privacy preserving clustering.",
            "There's a whole workshop on privacy and data mining going on.",
            "I think this afternoon we're not going to talk about clustering, where you have some underlying data assumptions.",
            "That said, data comes from a particular distribution, so this is a huge field in of itself.",
            "We're not going to.",
            "This is the only time you'll hear about it, and we're not going to delve too deeply into any of the proofs, for better or for worse, you might like this.",
            "You might not, but we're going to try and convince you weather in pictures with some handwaving, or just by giving intuition as to why the things we're saying are actually true.",
            "And we're not just doing some magic and pulling big O notation and other things out of thin air.",
            "Alright, so this is the scope and here's the."
        ],
        [
            "Rough outline, so we're going to spend the first roughly half of the talk on Euclidean clustering, and then particularly the kind of the famous K means algorithm, and you might think that."
        ],
        [
            "Not much to say about K means will know what it is has there with you."
        ],
        [
            "The entry on it?",
            "That's quite long, but there's actually a new developments that happen in the past three or four years that make it quite interesting.",
            "Then we're going to take a little 10 minute break.",
            "This will be about an hour."
        ],
        [
            "And and then we're going to talk about Bregman clustering and how you generalize K means to cluster under different metrics.",
            "So for example, if you want to cluster under KL divergences, which is very popular when you're doing distributional, and your cluster kind of different distributions of points, but kill divergance has these nasty properties that, well, it's not symmetric.",
            "It's not really a metric at all, it can be infinite between two points, and 0 otherwise.",
            "So how do you?",
            "Circumvent all of these barriers and still get to a practical and simple algorithms."
        ],
        [
            "And then in the third part, we're going to talk.",
            "I'm going to go a little bit more philosophical to some extent and talk about stability and say, well, what are these assumptions about clusterings that we're making, and how do we put them.",
            "We're not just clustering for the sake of clustering, where clustering because we're trying to do something else more.",
            "Clustering for classification or clustering for some other reason.",
            "And how do these?",
            "Final objectives how should they drive our thinking about what algorithm should we use and what kind of assumptions are we making when we're saying, oh, just minimize the variance of the clusters and then we're going to there.",
            "So.",
            "And a tiny little bit of more details for K means we're going to look about and say, well seething K means selecting the initial centers is really the hardest part about K means running K means anatone implementing K means on its own should take you no more than half a day.",
            "But really, how do you initialize K means to make it a good algorithm?",
            "Then we're going to talk about a little bit about the running times and then running K means on large datasets.",
            "What it means and how to do it.",
            "What are the problems?",
            "As I mentioned, Bregman is a generalization of K means, so we're going to sort of segue right into that, reviewing some of the results, and then talk about this stability aspect.",
            "How do we?",
            "So with that in mind, let's jump right in an.",
            "If you have any questions where sort of a small crowd.",
            "Note some of us are more or less hiding in the back, so just interrupt at anytime.",
            "Probably not by screaming you lie, but by some other methods.",
            "OK, so let's talk."
        ],
        [
            "So K means.",
            "So before we can really talked about K means."
        ],
        [
            "Well, what does it mean to cluster?",
            "So clustering is just very simple.",
            "I'm going to give you endpoints.",
            "I'm going to be nice and assume that they lie in some D dimensional Euclidean space and I want to split them into K groups.",
            "So I give your endpoints in some space.",
            "I give you a number of K and say give me your best split of these points into these cake groups."
        ],
        [
            "Well, what does best really mean?",
            "Right, so I have here example.",
            "There is maybe 25 points I want to split them into three groups.",
            "My split might be very different from your split with very different from."
        ],
        [
            "Russia split, so here's 1 split so we have the green points.",
            "We have the little cluster of Blues and a big thing of Reds on the side.",
            "Is this a good clustering?",
            "Well, maybe it depends what you want to do."
        ],
        [
            "Right, so let's try to define an objective and say, well, how do we define best?",
            "So one thing that we can do is to we can try to minimize the maximum radius of any cluster.",
            "So this sort of talks about the tightness of any particular cluster, and from here you can say, well, this is not quite optimal, right?",
            "So this blue one in particular is really small, and the red one is quite large and I can reduce the radius of red if I have.",
            "If I re classify a couple of these points as blue guys.",
            "And the red one will shrink.",
            "The blue one will grow.",
            "Quite a bit, but since I'm minimizing the maximum radius, they will actually improve the clustering with respect to the objective function."
        ],
        [
            "Well, maybe that's not what you want to do.",
            "Maybe you want to maximize the average Inter cluster distance, so you want to say these clusters are very well spread apart from each other, right?",
            "They're not just kind of overlapping there really.",
            "I have a real clustering.",
            "That's fine and well."
        ],
        [
            "One thing you can try to do, going back to the first one is to minimize the variance between the cluster, right?",
            "So we all want tight, well separated clusters and the question is how to do, How do we define that?",
            "So the variance is one of the measures, so I can say for a particular cluster I'm going to minimize the average distance between two points in that cluster and that will really lead me to the variance.",
            "So this is the one we're going to talk about a little bit more."
        ],
        [
            "So how do we define the variance?",
            "Well, let's talk about the expectation of a cluster.",
            "So the expectation is simple.",
            "So I give you a clustering and I give you.",
            "I say, these five points belong to a particular cluster.",
            "Well, the expectation is just.",
            "If I were to pick a point at random, where do I expect it to be?",
            "So it's just the mean of all of these points.",
            "So four points X that are in a particular cluster.",
            "Let's take the average, and that's the means here.",
            "Well, now we can write the variance, so the variance is just the sum over all the points.",
            "So if I have this as the variance of a particular cluster and now I have just the sum over all of the clustering, so I'm minimizing the sum of the variances over all of them.",
            "And this is my objective.",
            "OK, well there's one thing to define the objective, the sex."
        ],
        [
            "Part is now to solve it, so I have this great problem.",
            "I says well, given my points at X and a number of K, I'm going to find a clustering.",
            "So partition of X into group C1 through CK that minimizes exactly the variance of some of the clusters.",
            "This is the formula we had in the last line."
        ],
        [
            "Before I continue, I want to talk about one notion.",
            "That is close to the heart of any kind of approximations or theory person, and that's approximation algorithms.",
            "So I can say, well, this variance has some optimum value.",
            "So if I look over all possible ways to cluster this set of points, there's some optimum value of this variance fiestar.",
            "Right, but you can't get any better.",
            "This is just the property of your data well, and we're going to say that and some algorithm is Alpha approximate.",
            "If it's within the factor of Alpha.",
            "Of the optimum.",
            "So since the variance fee star is the optimum, you're never going to be smaller than it, so whatever you find is going to be larger.",
            "That's just by definition, but we're saying it's not going to be too large and it's going to be at most of factor of Alpha off and Alpha is greater than one.",
            "So if we're talking about a two approximation, then I guarantee to you that at the end of the run of your algorithm, whatever answer you find is going to be within a factor of two of the best you could have done.",
            "And maybe a factor of 2 doesn't sound that great an maybe some of the factors that we'll see later on will sound even worse, but this is a theory result, and in practice usually when you see a factor of two, it means 10%.",
            "Or it means 1%?",
            "I mean that now is data dependent, but for most of these there really is some example or some case where you will get a factor of two, so we can't prove a better result.",
            "But this is what we have.",
            "It doesn't mean that it's always a factor of two off, it means just adding the worst case.",
            "It's a factor of two off.",
            "OK, so now."
        ],
        [
            "Back to solving this variance problem.",
            "Not surprisingly, probably it's NP complete and it's a little bit surprising that it's NP complete even on the plane, so I give you the bunch of these points that are just lying on the plane, No 3 dimensions, No 4 dimensions, so you can very much visualize it and you still cannot find the absolute best variance clustering.",
            "But"
        ],
        [
            "Open solving this problem, or at least attempting it for over 50 years now and this is the famous K means algorithm.",
            "So there is many different citations.",
            "It was rediscovered at least three times back 50 and 40 three years ago.",
            "It's been rediscovered many more times since then.",
            "It's been part of many, many homework assignments I gather, and it's been implemented.",
            "I think it's fair to say 10s of thousands of times because it's."
        ],
        [
            "So simple, so how does key means work?"
        ],
        [
            "So again, we start with some data set.",
            "This is what we have.",
            "We have set of data points.",
            "This is going to be the running example for a whole lot of the talk, so we have these nine points and we want to cluster them into three groups.",
            "So what do we do well?"
        ],
        [
            "At first we select three centers at random.",
            "We're going to say, well, these three points.",
            "Green, red, purple are representative, and these are going to be our cluster centers.",
            "Now we say well, if these are the points.",
            "Now let's find the actual clustering.",
            "So let's assign each point to whichever."
        ],
        [
            "Center is nearest to it, so the green Point.",
            "This guy is slightly closer than the red one for the blue for these two points, this guys closer than the red."
        ],
        [
            "One or the green one, and so on.",
            "So you do this assignment and."
        ],
        [
            "You do the assignment.",
            "You can say, well, let me recompute the optimum centers.",
            "So if I have the position of where?",
            "If I have the partition of the points into the clustering, so I know that these two like green points belong in the same cluster, then I can find the optimum place to put the cluster center which is just exactly in between them.",
            "And it turns out it's the mean of the points, hence the name K means.",
            "And we."
        ],
        [
            "Pete, so now the cluster centers have moved, so the assignment of which point belongs to which cluster has changed.",
            "So."
        ],
        [
            "We're going to reassign and repeat again."
        ],
        [
            "Resign and now we reach a steady state or local minimum local optimum.",
            "So every point is assigned to its nearest cluster center.",
            "And every center is the mean of the points assigned to it.",
            "So if you were to try and run the algorithm on this configuration, it would just instantly stop and say a. I didn't have to do anything, I'm done right?",
            "If you started with a different configuration it runs for awhile and then it gets here.",
            "So this is K means.",
            "The first nontrivial thing is that."
        ],
        [
            "This actually terminates, right?",
            "You might try and think and say, well, if I have these points in multi dimensions can things go around in circles?",
            "Can strange things happen?",
            "No.",
            "So the one thing that we can absolutely guarantee is that the algorithm will terminate.",
            "Because the total variance this potential function is reduced at every single step.",
            "So in particular, if I assign each point to its nearest center, I'm going to for a single point, reduce its contribution to the total variance, because the contribution for a single point is the square of the distance between it and the nearest closest cluster center.",
            "On this flip side, when I have a fixed clustering and I say the center is the mean of all of the points in that clustering, this will also reduce the potential.",
            "This is not trivial.",
            "This requires a proof.",
            "The proof is not hard, but This is why the algorithm terminates.",
            "So at every single point I know that this variance reduction that I have starts going down, and if it goes strictly down then I can't repeat the same clustering.",
            "More than once because the potential has gone down, but now I know there's only so many different configurations.",
            "There's only so many ways to partition a set of endpoints into K clusters.",
            "There are a lot of them, but there's a finite number.",
            "Therefore this will terminate."
        ],
        [
            "OK, what else do we know about the algorithm?",
            "Well, we know it finds a local minimum.",
            "So for example, here's of potential output.",
            "So we have nine points, 3 on the left, three in the Middle, 3 on the right, and this if I have the horizontal version of clustering.",
            "So the green, the red and the purple.",
            "This is a perfectly good local minimum.",
            "So if you start K means this way and you say this is my assignment, or these are my cluster centers, it will say again, I'm done.",
            "Everything is ready.",
            "Each point is assigned to its nearest cluster center and each center is the mean of all of the points assigned to it.",
            "Of course, if I give you this nine points, an essay, cluster them into 3."
        ],
        [
            "Groups you're more likely to cluster them this way.",
            "Right, and this is one of the problems with K means.",
            "We know it gives you a locally optimum solution, so from that solution you cannot make any one of these local changes.",
            "In order to do it.",
            "But we don't know whether it's even close to a global solution.",
            "And as this example shows you, it could be arbitrarily far off.",
            "So 4K means itself you can't prove a bound that says K means is A5 approximation, attend approximation 100 approximation because no matter what you do, I can make this example larger.",
            "And everything breaks for you.",
            "OK."
        ],
        [
            "But one question that you should ask yourself is OK, that's all.",
            "Well in theory, but does this actually happen right?",
            "Is this some bad strange constructed theory example or I'm not going to get this if I actually run this on the real datasets?"
        ],
        [
            "Answer is that it does happen even on these simple datasets.",
            "So if I give you these four.",
            "This example and tell you cluster this into four groups.",
            "I think pretty much everybody in this room would say, well, this is obvious, right?",
            "Unfortunately, if you give this to K means it's not that obvious to K means and what's going to happen in the close.",
            "You can see too well, but we have a green cluster.",
            "This is perfect.",
            "This cluster got split into two, so there's half of it here and another half on the side.",
            "And these two guys are merged into one.",
            "Now, why did and you can check that this is a locally optimum solution, so all of these points are too far away from here, so they would rather go to here and so nothing changes.",
            "Now why would K means do such a strange thing?",
            "Well, it's the initialization.",
            "So initially I picked four points at random, and I happened to pick two from the same cluster, which is just a random event.",
            "It will happen now.",
            "I ran K means and this is what it's going to converge to.",
            "And the real problem is that if I have this example and I have many more of these, if I have 100 of these kind of well separated, reasonably well separated Gaussian clusters and I tried with 100 points, I will get something like this every single time.",
            "So no matter how often or how much you start and you reinitialize it and you pick a different set of random samples, you will still get this event.",
            "Two of them will be merged and two one of them will be split.",
            "Or more of them will be merged and some of them will be split.",
            "So."
        ],
        [
            "Finding this good initial point set was considered a black art and this is where a lot of the work on K means has been in the past 50 years and there are several things that you can do reason."
        ],
        [
            "You can try many times with different random seeds and you can say, well, this one didn't work.",
            "This is random.",
            "Let me start it again.",
            "I'm going to throw a bunch of different random points in the beginning.",
            "Something is going to come out and I'm going to do this fifty 100 times as we'll see in the second K means is very fast in terms of convergence, so I can afford to do this 100 * 1 of them is bound to be good and I'll take the best one and then I'll go on with my life.",
            "But as I mentioned just now, this has limited benefit even in this trivial case or trivial choice, because we see the global picture where things are very well separated.",
            "If I do this, I'm still going to break every single time.",
            "So."
        ],
        [
            "There's other methods, so there's literally hundreds of papers that have been written on this, and a lot of them are quite clever and a lot of them do quite well.",
            "There's preprocessing, maybe I'm going to pick more centers.",
            "There's post processing that says, well, maybe I'm going to cluster with two K centers and then merge some of these clusters together in the end to try to avoid this problem.",
            "Maybe I'm going to do some agglomerative or kind of different versions of clusterings and then use that to initialize K means and there's bisecting K means there's all sorts of things.",
            "But all of them break on some datasets, so most of them if you look at them closely, you can say, well, this doesn't.",
            "Here's the data set where this wouldn't work.",
            "Maybe it works on some data, but on some data it doesn't.",
            "And what I'm going to talk about for the next few minutes."
        ],
        [
            "Is that there is a simple initialization scheme and I'm going to try to convince you that it's simple that actually leads us with some provable guarantees and actually does work in practice overall."
        ],
        [
            "OK, So what was the problem with this Gaussian example or?"
        ],
        [
            "The problem really was that we merge some of these guys together."
        ],
        [
            "But there is a really simple fix it all you had to do is cluster these nice well separated points and they were nice and clean like this.",
            "Here's one thing that you can try to do, you can say."
        ],
        [
            "Well, I'm going to choose the first point at random again because I don't know how to do anything else.",
            "So the first point I'm just going to pluck it and say this is my first center.",
            "Then I'm going to say well which one of all of the remaining points is the furthest away from the center that I already have?"
        ],
        [
            "At end is going to be this guy over here.",
            "Now, given that these are well kind of clustered and there is a tight well separated clusters, I can do this again and I can say well out of these two, which one is the furthest away out of the remaining points?",
            "Which one is the furthest away from any of the ones that I picked?"
        ],
        [
            "One is going to be some guy here and you can see where this is."
        ],
        [
            "Going right, I repeat this again."
        ],
        [
            "And now, boom, I have a perfect clustering.",
            "So why am I even here, right?",
            "Like why don't we just use this all the time?",
            "This should have been discovered like 40 years ago, and then we'd be done from this."
        ],
        [
            "Well, the reason is your data doesn't look this nice right?",
            "If our data look this nice, there will be no Keedy.",
            "This is maybe a little bit more of what our data looks like, and if we try the further."
        ],
        [
            "Point heuristic here.",
            "With this one single nasty outlier, he's not really part of any of these three clusters, would say, well, let's pick the green Now let's."
        ],
        [
            "At the point furthest away.",
            "That's the outlier, whoops."
        ],
        [
            "Now let's look at some other point.",
            "Maybe it's even this red one."
        ],
        [
            "And now cluster and then we said, hey, the outliers, a single point?",
            "We have this huge cluster here which doesn't make much sense.",
            "If you started and then we have the perfect green cluster on the left.",
            "So that's a problem, so that's what we have to fix.",
            "But of course, if we were just doing random sampling and the initial version of K means we would never pick this single outlier point, right?",
            "If we were just repeat this many times, the single outlier point wouldn't exist because we pick one from here 1 from here and one from here, and we'd be done.",
            "So somehow?"
        ],
        [
            "You want to interpolate between these two methods.",
            "That one says, do everything randomly.",
            "It's pretty good.",
            "The other one says pick the furthest one.",
            "I need to say well that runs into trouble.",
            "That's two deterministic, so you want to somehow shift your mindset.",
            "It's picking stuff that's too far that's far away, but not be too deterministic about it.",
            "So how do we do this formally?"
        ],
        [
            "So let me define by capital D just the distance between the point and its nearest cluster center.",
            "Right, so this is the contribution to the variance and not just the distance.",
            "Rather and I'm going to choose the next point proportional to this distance D raised to some value Alpha, now how?"
        ],
        [
            "Does this work?",
            "So if Alpha is equal to 0, anything raised to zero is 1, so every point is equally likely to get selected, so this is my random initialization, right?",
            "I'm going to pick one point at random and go on."
        ],
        [
            "If Alpha is equal to Infinity, this just says, well, one of these dies.",
            "The largest one will dominate everybody else.",
            "And so I'm only going to pick.",
            "I'm going to put all of our my probability mass on the guy that's furthest away.",
            "So this is the furthest point, heuristic, right?",
            "So I'm going to pick the furthest guy away and now I have this little nob that I can tune and I can set it anywhere between zero and Infinity and I can interpolate between these two methods."
        ],
        [
            "Alpha equals two turns out to be the magic number.",
            "And we'll see why in a second, and this led to an algorithm which we call K means plus plus size K means, but tiny little bit better."
        ],
        [
            "So more generally, why does Alpha equals to the magic number here?",
            "Well, it's because we're trying to minimize the square of the distance is between a point and its nearest center.",
            "If we were just trying to minimize the distance is the sum of the distances between the point and its nearest center, we would sample.",
            "We would set Alpha equal to 1 and sample according to D. If you're trying to minimize the maximum distance, so this is going to minimize the radius of the cluster, you would sample according to to the Infinity you would use the furthest point heuristic and this is at this point this becomes kind of a famous known algorithm.",
            "But setting Alpha to equal to two or more generally saying well, if you contribute a lot to the overall error, if you contribute a lot to the variance in the end, then I'm going to pick you with high probability.",
            "If you contribute a little, I'm still going to have some probability of picking you, but not too much, and so this gives us kind of this.",
            "This is the rule of thumb probability proportional to its contribution to the error.",
            "OK."
        ],
        [
            "So the data set looks Gaussian, So what happens?"
        ],
        [
            "Here we picked the first point at random again.",
            "Now we have a choice.",
            "Well, this guy contributes a lot to the error, but there's only one of him.",
            "Whereas these guys each one contributes some amount to the error because they're still pretty far away.",
            "But there is a lot of them.",
            "So what we're going to do?"
        ],
        [
            "Who's going to pick one from here?",
            "From these two and again."
        ],
        [
            "And now."
        ],
        [
            "Now we have World War One.",
            "Yes, this is an outlier.",
            "It doesn't really matter which cluster he belongs to.",
            "We're in good shape."
        ],
        [
            "On the flip side, say this was our point set, so the outlier was actually really far away.",
            "Right, he wasn't just sitting somewhere close, but he was really, really far away.",
            "Now if you want to look for the optimum solution here, it turns out that this is no longer an outlier.",
            "If you're trying to minimize variance, this really should be its own cluster, because it's so far away if you put him into a cluster with anybody else, he's going to draw, and he's going to.",
            "His variance is going to be so high, and he's going to contribute too much to the variance of this cluster.",
            "So now I."
        ],
        [
            "We run green here.",
            "Well now this guy is so."
        ],
        [
            "Far away, imagine him down the street.",
            "Well, we should pick him, and again, there's only one of him, but he by himself contributes a lot to the overall error, so we can."
        ],
        [
            "Him and now merging the."
        ],
        [
            "Two clusters together is actually the right thing to do.",
            "And the algorithm attempts to do that.",
            "OK."
        ],
        [
            "So what can we say about the performance of K means plus plus?",
            "So we talked a lot about things should work well.",
            "Things should be fast.",
            "Can we say anything here?",
            "Not surprisingly, the answer is yes."
        ],
        [
            "And we can say actually two things which is.",
            "Nice, so there's one theorem that says this is always a lot K approximation and we talked about approximations before, and I warned you that a lot K sounds scary, because in particular grows with K and there is a big O sitting here.",
            "It's really like 16 log K and there really are examples where it is 16 log K, But as we'll see in a second, there are also many, many examples where it's not 16 lock A and it's something much better.",
            "We don't know how much better it is because it's an NP hard problem, so we don't know what the optimal solution is, but we know that it's pretty good.",
            "There is a."
        ],
        [
            "Different.",
            "Theorem by a different set of authors around the same time, and then said if we modified this algorithm a little bit but really keep the spirit of it the same, and we assume some condition on the data, we say that the data is nicely clusterable, which really says it means that it does look like a bunch of well separated.",
            "Clumps of points, then the same algorithm really gives you a constant approximation, so the only times that.",
            "You get this log K if your data is looks nasty.",
            "So let's dive this into this a little bit."
        ],
        [
            "So what does it mean to be nicely clusterable to?",
            "One way to say this?",
            "Well, if I went from K -- 1 clusters to K clusters, my total variance must have dropped by whole lot."
        ],
        [
            "In particular, it dropped by a constant, so appoint said is well separated.",
            "If the optimum solution using K clusters is much much less or constant factor less than optimal solution using K -- 1 clusters.",
            "But that's the intuition.",
            "So in the intuition is that it does look like a bunch of well separated clumps of points, then this gives."
        ],
        [
            "Constant approximation.",
            "Why well, why?",
            "There says like this is exactly the example from the pictures.",
            "So if the algorithm picks a point from a new cluster, that's.",
            "Part of the new optimal cluster.",
            "Every time we're doing this iteration, then it's doing pretty well, but if it doesn't pick a point.",
            "If it splits and optimal cluster into two, it means that other clusters are sort of weak.",
            "They have very few points and they don't contribute much, so it's OK to merge them.",
            "We're not going to be optimal, we're just going to be near optimal, so it's OK to merge them and continue."
        ],
        [
            "So as long as the clusters are well separated, the first condition holds.",
            "If they're not too well separated, then the second one kicks in and overall we get this either constant or log K."
        ],
        [
            "And these are the two theorems that I talked about before.",
            "So now you should say, well, that's all in great."
        ],
        [
            "And that works in theory.",
            "But"
        ],
        [
            "That actually work, right?",
            "Like does this?",
            "Is this practical?",
            "Does this give me a better solution?",
            "Does it actually work?"
        ],
        [
            "So here's the answer, so here's a table.",
            "Let me explain this a little bit.",
            "So this is the ratio between the potential of K means divided by K means plus plus.",
            "So if the numbers are larger, that means that the initialization worked better.",
            "So the number is 1.01.",
            "That means the initialization for K means plus plus gave you a 1% improvement overall if the number is 2.4, it means they give you a two and a half a factor of 2 1/2 improvement in terms of the total variance.",
            "If it's lower.",
            "So here there's a point 3% reduction.",
            "So this was done by a colleague of mine and that of 15 different datasets.",
            "There's different sizes, there's different dimensionalities, and so on, and different values of K. So depending on our clustering into just a few things, so we're clustering into many different buckets.",
            "And overall this works right?",
            "So sometimes it's a little bit worse.",
            "In one example, out of these 60, sometimes it's way much better, and all of these were done with random restarts and sort of how you would actually implement K means.",
            "But on average, by which I mean median, it's about 20% better.",
            "Any?"
        ],
        [
            "So here's one more kind of description of what this looks like, so here's on the Y axis.",
            "We have the error, so the total variance of the clusters on the X axis.",
            "We have this stage, the number of iterations run by each algorithm.",
            "So K means is here in the blue line as as we can see, it's monotonically decreasing, so that's what we claimed that the potential always goes down and it drops a lot in the very beginning because your random clustering initially is not very good, it's just random, and then it sort of levels off somewhere.",
            "And if you let it run, it will usually just stops, like right around here, but sometimes it will continue for awhile.",
            "The Red Line is a hybrid method which I'm not going to talk about much, and the green one says, well, use this other initialization for K means.",
            "So we start lower and this is where the theory bound comes in, and then we get even lower overall, right?",
            "So you drop even faster an.",
            "His batteries dying.",
            "So this is 1 typical run on one different data."
        ],
        [
            "Some of them look like this.",
            "This is your factor of.",
            "2050 hundred etc right?",
            "So there's K means and K means goes and it gets to some point, but it doesn't know how to get any lower and K means plus.",
            "Plus just start way lower.",
            "Right, so this is your Gaussian example of where things are well separated, and we know K means is not going to do well, and we know that this is exactly the fix that we're trying to do, and that's why you see this huge gap between the two.",
            "So the answer is yes, it does work, and it's a useful thing to do.",
            "But one thing that you can ask yourself on, judging by these plots is how fast this K means actually converge, right?",
            "And so we all know that it's going to converge eventually.",
            "We all know that there's some bound on it, which is huge.",
            "We just as well it doesn't repeat any particular clustering, but that's not sort of good enough.",
            "And if we stare at, these were going to say, well, no key means converged here.",
            "After like 60 iterations and if you run K means on any of your datasets, you will notice that it converges after 60 iterations.",
            "If it's a particularly bad data set, it will converge after 100 iterations, maybe 200.",
            "I don't think I've ever seen 200.",
            "So somewhere sometimes it's 10, sometimes it's 20.",
            "It will always converge, and usually it will converge very, very quickly."
        ],
        [
            "So how well does it converge?",
            "What can we say about this?",
            "Unfortunately, in the worst case, it doesn't really converge all that well at all, so there are examples.",
            "And Andrea who wrote this paper actually."
        ],
        [
            "Has a pro script that generates them that what says if you run on K means on this particular example and with this particular set of initial cluster centers it will take exponential time.",
            "So here is the exponential.",
            "Isn't K the number of centers?",
            "So if K is 100,000 etc there are examples will run as well and the surprising part again is that these examples live in the plane, so it's not that these examples are somehow relion multi dimensional structures or something.",
            "Strange that we can visualize these examples live in the plane, and if we want to make them kind of very nice and make sure that all of the distances between any two points are close to each other, then we have to move into 3 dimensions.",
            "But it's again still something that you can look and you can convince yourself.",
            "Yes, it really will take this long.",
            "On the plane Kimmins.",
            "In the worst case.",
            "But it runs in 100 iterations, right?",
            "Like I don't care that it could take two to the K time in the plane on any data set I've seen, it runs in 100 iterations."
        ],
        [
            "So what's going on like?",
            "Why does it work in practice?",
            "But it does not work in theory.",
            "What is it that's this?",
            "Is something that should kind of make us think a little bit?",
            "Because something there is something going on that's that we don't really quite understand.",
            "And really, what's going on is one thing to look at it."
        ],
        [
            "How robust are these worst case examples?",
            "And so we have this particular worst case example, and then this example.",
            "It takes exponential time, but if I were to change example just a tiny little bit, but it still take exponential time because we know that in real life our data is noisy, it's not like we're playing a game with the devil where the devil says uh-huh.",
            "This is where I'm going to put the next point, and this is where I'm going to put the next point.",
            "And this is how I'm going to choose your random seed so that you do the worst possible random initiation of these clusters.",
            "And now you're going to lose and you're going to wait exponential time.",
            "It feels that way more often than it should, but it doesn't really happen this way, right?",
            "There is a little bit of noise in the data.",
            "So what are we going to say is?",
            "Well, if we perturb the points a little bit, if we change, if we just add a little bit of noise to the points, does this change things?",
            "So here."
        ],
        [
            "So our data set from before and we're going."
        ],
        [
            "Change it a little bit.",
            "Again, here it was."
        ],
        [
            "It is now.",
            "The optimum solution does not change, right?",
            "I added a tiny little bit of noise to the points, maybe changes, maybe changes by a fraction of 1%.",
            "I don't really care if this is if my structure somehow critically depends on the data that I can't change it.",
            "Even a tiny little bit.",
            "Maybe my data is wrong.",
            "Maybe I'm solving the wrong problem and I should be able to change my data.",
            "My data is noisy, it's coming in from all sorts of things.",
            "Floating point errors, you name it.",
            "OK, so this is the formal statement of that problem, so we're going to take our data set."
        ],
        [
            "We're going to add a little bit of independent noise to every single point, so normal at zero with some variance Sigma squared.",
            "And now I'm going to look at the maximum running time over all the inputs of the expected.",
            "Running time subject to this noise so expectation over the noise of the running time and then take the maximum of the expectation.",
            "And this should somehow encode these brittle examples.",
            "If the worst case examples are brittle, then well, one of these points inside the expectation will be really bad, but the rest of them will be quite good, so things will average out if the worst examples are not brittle.",
            "If there's, here's a whole data set, no matter how you perturbate, you're still going to be in bad shape then this is not going to buy us anything.",
            "So this is called smooth analysis and it's actually been used to explain why some of the algorithms that perform very well in practice, notably the simplex method for solving linear programming, say, perform well in practice, but you horribly in theory, so there are worse case examples for all of them.",
            "Let's say they're not going to work in theory, but in practice they just work.",
            "So this is part of the disconnect the theorie.",
            "Worst case examples are very, very brittle.",
            "They are not robust to noise, and in reality our data is always a little bit noisy."
        ],
        [
            "OK, so given this definition, what can we say?",
            "Well, we can prove this scary looking bound.",
            "This is not great, right?",
            "So this is N to 34, K to 34.",
            "And even if both of these are equal to two, which will give you a trivial clustering, are bound is like 2 to the 70.",
            "Which is around 10 million iterations.",
            "More 10 billion iterations.",
            "Plus a whole bunch of other factors, right?",
            "So this is not great, but it's a theory bound and the main points notice here is that it says it's polynomial.",
            "It's no longer exponential.",
            "We're not saying it's not two to the K or two to the end, we're saying it's end to some power kata, some power capital D is the diameter of the point.",
            "Said there's some little.",
            "D is the dimensionality of the points, etc.",
            "So the bound is large.",
            "It's not exponential, and as K grows to Infinity for.",
            "Very large K, probably in the thousands.",
            "This will actually be true, so this will be a tighter bound.",
            "But the point is, again, this explains.",
            "It's not really that we're saying it's going to run an end to the 30.",
            "Four time.",
            "We're saying that This is why this says it shouldn't be exponential.",
            "This is what's going on.",
            "It really is polynomial, so it's sufficient if you.",
            "Do that."
        ],
        [
            "So again, the comparing balance.",
            "We say well, the worst case complexity of K means is really bad, so it has exponential worst case complexity, but.",
            "The smooth complexity, the complexity that we expect to see out of K means, is actually polynomial.",
            "Which is great, so the pathological examples are really brittle.",
            "So unless you do find yourself playing this game with the devil, you're not going to run into these worst case examples and a little bit of random noise.",
            "Even if you are playing this game, you're one kind of trick is to say, well, let me add a little bit of noise to your data cluster 8 and then return you the clustering on this noisy data 'cause the noise doesn't change the optimum all that much, but it does change the performance.",
            "At least what we can prove about the performance of the algorithm."
        ],
        [
            "So for the summary of K means we have.",
            "Exponential worst case.",
            "We have usual typical case M4."
        ],
        [
            "The solution quality.",
            "We know that K means by itself with a random initialization will take us to some arbitrary local optimum, so we're going to run it.",
            "We know that what we get in the end is a locally optimum solution.",
            "But we also know that a simple initialization will lead us to a good solution and will lead us to a solution that's actually usually much, much better than regular K means."
        ],
        [
            "So if we were to implement this, the implementation is pretty simple, so came in the initialization routine.",
            "Just requires you to draw K different points, but after every draw your distribution changes.",
            "So you're going to take ND time roughly to compute your distribution.",
            "Then you pick a point from that distribution.",
            "Then you take ND time again.",
            "You pick a point, etc.",
            "So the total running time is about NKD.",
            "You can make it a little bit smaller, but not by a huge factors and overall running time of regular K means is also in KD, right?",
            "Because at every single iteration for every single point and you want to compare its distance to every single one of its cluster centers, this is K of them and it's going to take you about the time in D dimensional space to do it.",
            "There are some data structures that you can use to make this faster.",
            "You can get this down to NK log D, an small dimensions, and.",
            "You can do other tricks to make this a little bit faster, so there's definitely a lot of work done on how to make a means much more efficient, more efficient than it already is, but it's pretty good overall even if you did just a naive implementation.",
            "The good part about it is that it's about 5060 lines of code.",
            "You can whip it up in a couple hours, 4 hours after debugging, and it will work.",
            "Any finishes after Council member runs great."
        ],
        [
            "But there's a question.",
            "And where at KDD both of our data is large, right?",
            "Like NKD looks like a very good bound.",
            "It's linear in all different three of these parameters.",
            "But we're multiplying these things together.",
            "So if N is an order of millions, K is on the order of thousands and dies on the order of thousands.",
            "Now we're talking about a billion operations for each one an.",
            "This computers are fast, but they're not that fast.",
            "So can we paralyze?",
            "So there are people right next door are talking about some ways to paralyze K means, and we're going to talk about slightly different ways to parallelize K means.",
            "And if you put both of these together then you get an even faster version of K means.",
            "But these are kind of two very disjoint avenues."
        ],
        [
            "So how do we paralyze?",
            "How do we make it MapReduce friendly?",
            "How do we make it?",
            "Who do friendly?",
            "How do we make it cluster friendly?",
            "So here's our basic approach.",
            "We're going to take our data, set X, and remember X is huge and we're going to split it just randomly in points roughly equal size.",
            "So if we're going to say we have M different machines, whether these are machines in the cluster or these are mappers and reducers in New York group environment, or something else?",
            "Roughly equal, they don't have to be identical."
        ],
        [
            "In parallel now we're going to cluster each one of these partitions, so we're going to find the same clustering sequence for KC KA good clustering.",
            "Now we know how to do that.",
            "We have them in different methods, so in parallel we find a clustering on each one, the partitions and we're going to remember how many points there are in each cluster.",
            "This turns out to be useful, so you want to know, or was this a Singleton cluster that was just the outlier, or did I have 1000 points associated with it?",
            "Then we."
        ],
        [
            "All of these clusters and we bring them back together and we treat them as just regular points with some weights attached to them.",
            "So we summarize each one of the all of the data in X1 just by its clustering C1.",
            "And then we cluster the clusters.",
            "So will come by why all of the clusterings?",
            "And then we find the clustering of Y and the only thing we need to be careful about is that when we're finding this clustering, we need to remember about these weights.",
            "So was this a single point?",
            "Was this 10?",
            "But this just goes through.",
            "You multiply the contribution of a particular point by the weight of it.",
            "Or when you're computing the distances in computing the means you do the same thing."
        ],
        [
            "OK, so again in pictures so we have X now.",
            "X is large X used to be small in the same data set nouns large.",
            "We're going to partition it."
        ],
        [
            "Into two groups.",
            "So we're going to have the.",
            "Filled in the solid ones an just a white bunch.",
            "Now we're going to find a clustering separately."
        ],
        [
            "Each one, so let's look at the white ones.",
            "We run our cluster."
        ],
        [
            "Maybe we'll find these four different cluster centers great."
        ],
        [
            "Look at the black ones.",
            "Now we're going to find."
        ],
        [
            "Clustering again, maybe here.",
            "We didn't do quite as well, so we have these two that emerged together.",
            "And we have the purple one on the right."
        ],
        [
            "Bring them together so these are the two clusterings from the two different from the black points and the white points so."
        ],
        [
            "Looks mildly representative of the data.",
            "We have too many points here, and kind of too few over here, but more or less the same.",
            "And now."
        ],
        [
            "We can cluster these clusters.",
            "So treat them as regular points, cluster them.",
            "Now if we."
        ],
        [
            "Look at this final clustering.",
            "With respect to the initial points."
        ],
        [
            "It's pretty good.",
            "Now of course, I've got to make the picture, so I get to claim and make the picture so that it's pretty good, but we can again prove something nontrivial here that this is not going to lose you too much as you do this.",
            "So."
        ],
        [
            "What happens the main question is when we approximate.",
            "Need approximation so we took our data set.",
            "We took each one of the partitions we sort of sketched it or approximated it by the clustering and now we are approximating and clustering on these approximations.",
            "So say we have two different qualities, so there's a beta in the gamma.",
            "There is a theorem."
        ],
        [
            "That's been reproved recently that says, you know, there's some way to combine these metrics.",
            "And basically they multiply, so there's a gamma times beta.",
            "So everything else, there's some order terms, but essentially they multiply.",
            "So if you were to do this three times, if your data is so large that you need to do this recursively little bit more, you would multiply them three times.",
            "If you do it twice, you just get a multiplication.",
            "OK, great and now we know."
        ],
        [
            "Kind of the running time scale, so the first running time gets reduced by M. Because we have M different machines to do this and the second phase running time they N the number of points gets replaced by MK because we had K clusterings on each of the machines, so that's our size of our data set as the input to the second problem.",
            "So."
        ],
        [
            "If we use again, K means plus plus or something along those lines, then this gives us a log squared.",
            "So we lost the log K in each one of these.",
            "If we didn't make any data assumptions.",
            "So this gives us a log squared overall.",
            "Which is good, but can we do better?"
        ],
        [
            "And.",
            "To improve the approximation guarantee to make even better, we can say, well, we have this leeway.",
            "In the end, we know that we have to find K different cluster centres, but when we're sketching when we're summarizing the intermediate partitions, nobody enforces the fact that we need to summarize them by using K centers in each summary.",
            "What if we use more centers in each summary?",
            "So instead of summarizing things by 10 centers, I get to summarize things using 50 centers, right?",
            "That's still way smaller than my data.",
            "My data might be on the size of 10s of thousands.",
            "100 occasions I'm going to summarize it instead of 10 points.",
            "50 points.",
            "Can't be worse, right?",
            "It should be only better and it is."
        ],
        [
            "So there is.",
            "Another result that says run the same initialization, but instead of stopping it at the end of K points, keep on running it.",
            "So run it for another K steps or maybe another 5K steps.",
            "So in the end you get 6K cluster centers.",
            "But now these 6K cluster centers give you a constant approximation to the overall, so the sketch their quality of this summary of your data becomes much better.",
            "So before we said well, if you want to summarize your data using K points, you get log K. Now we're saying, well, if you want to summarize your data using more points, well, it's not surprising that you get a better solution.",
            "What's nice is that you need only a constant factor more, and I think in the theorem this constant is 8.",
            "In reality, kind of achoo suffices.",
            "So just run it for 2K centers and just summarize things using twice as many data points."
        ],
        [
            "So now here's the final way.",
            "Well, we split this again, roughly equal size just for it."
        ],
        [
            "Agency, we're going to find a clustering using L more than K centers and each one.",
            "This is our summarization step.",
            "And then we're going to cluster the clusters.",
            "So we're going to take this going to cluster them.",
            "It's going to be some set of LM points and it's OK now use K means with the appropriate weights."
        ],
        [
            "Anne.",
            "We know that it gives us approximation that this should be now that I'm slightly wrong."
        ],
        [
            "OK, So what is the summary kind of of this overall section.",
            "So it used to be that K means was one of these things that was very widely used and studied in practice, right?",
            "As I said, there are many papers and people said this is we need to focus on the initialization or there's some postprocessing.",
            "K means is really good.",
            "It's really fast.",
            "That's the main approach for it.",
            "You usually clustering is usually some intermediate step in whatever application you're doing.",
            "And here we said, well, yes, it's an intermediate fast.",
            "Let's do it really quickly.",
            "Let's use K means run at 10 different times different random seeds, which is going to give us a result great.",
            "Right and the theory community were saying well, he means well, here's an example where does badly.",
            "Here's an example where it does badly.",
            "No, it's a bad algorithm.",
            "You should never use it, and more over here you know it runs in exponential time.",
            "Why would you ever use an algorithm that a runs in exponential time and B gives you a horrible horrible approximations?",
            "And then there's this huge disconnect.",
            "That said, one side said key means great, we love you and others say K means you know we should have come up with something better.",
            "We've had 50 years to do it.",
            "Now I want to claim that we've sort of merge these two and brought him back together and we said, well, yes.",
            "It does run in worst case exponential time, but in reality it doesn't and we can actually show this formally.",
            "So that objection goes away.",
            "On the flip side, yes, K means by itself does leave you lead you to potentially really bad solutions, and the reason that we maybe didn't know that quite as well is because there was nothing that did better.",
            "Some of these heuristics work, but not all the time, not consistently, etc.",
            "And so we said well here the main point is initialization.",
            "Well, let's initialize not just completely randomly and not completely using this for this point heuristic, but somewhere in between somewhere proportional to the error that we're going after and look loan.",
            "Behold, this satisfies the theory people because now we can say hey, this gives us an approximation guarantee and this is a constant approximation of the data is good.",
            "It's a lot K approximation of the data is not so good.",
            "But this is a well principled approach to do that, but at the same time it actually works, right?",
            "It actually beats regular K means, which is the reason why it's so popular 5060 years down the line is because it's hard to beat.",
            "There's K means, but it's it really works on any data you throw at it.",
            "It don't need to do any assumptions, so it's kind of hard to improve upon, so we have these two sides, and hopefully I think now these two, the two stories the two roads have merged again and sort of.",
            "We know a lot more about K means, and then the next section stress is going to talk about now using the same intuition now to cluster harder metrics 'cause our points don't always lie in just 234 some dimensional space.",
            "We want to cluster distribution which we want to look at mutual information with attention.",
            "People want to do all sorts of crazy things and how to.",
            "How can we take the insights that we learned in Euclidean spaces which we can picture and look kind of nice and translate them somewhere else?",
            "OK, so that's going to be the continuum."
        ],
        [
            "And I think we'll take a 10 minute break.",
            "Ten 1515 minute break.",
            "I'll be around here, so if you have any questions you can ask him using the mic or you can come up afterwards and then we'll continue on with this.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I mean, we're going to talk about clustering and kind of new advances in new developments in the theory of clustering and the real tagline for the talk is the subtitle of the talk, so that's all very well in practice.",
                    "label": 1
                },
                {
                    "sent": "But does it work in theory?",
                    "label": 0
                },
                {
                    "sent": "And there's a second sub line that says, well?",
                    "label": 0
                },
                {
                    "sent": "It's all very well in theory, but does it work in practice and so will balance off between these two as we go forward through the slides.",
                    "label": 1
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What are we going to cover?",
                    "label": 0
                },
                {
                    "sent": "So we're going to have two major themes, so we're going to cover it and talk about practical algorithms for clustering and actually give you some experimental results for some of these algorithms that actually have also, in addition, strong theoretical guarantees so we know it's not just a heuristic, but we can prove some bounds on the performance of these algorithms, weather and running time, or in approximations, or what have you.",
                    "label": 0
                },
                {
                    "sent": "And we're also going to talk about models.",
                    "label": 0
                },
                {
                    "sent": "Kind of theoretical models to explain the behavior observed in practice, so from theory point maybe some algorithm looks absolutely horrible and awful, and maybe that's why it hasn't been studied by the theory community, but maybe they were looking at it from a wrong lens.",
                    "label": 1
                },
                {
                    "sent": "So if we look at it from the right way and we look about, we try to explain a capture the right properties of the algorithm or the data, then we actually get good results and we can explain what's going on and that gives us new insights as to what to try and how to improve on these algorithms.",
                    "label": 0
                },
                {
                    "sent": "These are two goals, but theirs.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A whole lot of stuff that we're not going to cover, so as kind of everybody knows, clustering is a huge field.",
                    "label": 0
                },
                {
                    "sent": "That's an ancient field, and it's been around forever and the number of papers on clustering or that mention the word cluster has been growing exponentially for the past X hundreds of years together.",
                    "label": 0
                },
                {
                    "sent": "But, and even more so, in the past couple of years.",
                    "label": 0
                },
                {
                    "sent": "So because of that there's whole swaths of areas that we're not even going to touch.",
                    "label": 0
                },
                {
                    "sent": "So, for example, we won't talk about meta clustering.",
                    "label": 0
                },
                {
                    "sent": "We're not going to talk about privacy preserving clustering.",
                    "label": 1
                },
                {
                    "sent": "There's a whole workshop on privacy and data mining going on.",
                    "label": 1
                },
                {
                    "sent": "I think this afternoon we're not going to talk about clustering, where you have some underlying data assumptions.",
                    "label": 0
                },
                {
                    "sent": "That said, data comes from a particular distribution, so this is a huge field in of itself.",
                    "label": 0
                },
                {
                    "sent": "We're not going to.",
                    "label": 0
                },
                {
                    "sent": "This is the only time you'll hear about it, and we're not going to delve too deeply into any of the proofs, for better or for worse, you might like this.",
                    "label": 0
                },
                {
                    "sent": "You might not, but we're going to try and convince you weather in pictures with some handwaving, or just by giving intuition as to why the things we're saying are actually true.",
                    "label": 0
                },
                {
                    "sent": "And we're not just doing some magic and pulling big O notation and other things out of thin air.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is the scope and here's the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Rough outline, so we're going to spend the first roughly half of the talk on Euclidean clustering, and then particularly the kind of the famous K means algorithm, and you might think that.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Not much to say about K means will know what it is has there with you.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The entry on it?",
                    "label": 0
                },
                {
                    "sent": "That's quite long, but there's actually a new developments that happen in the past three or four years that make it quite interesting.",
                    "label": 0
                },
                {
                    "sent": "Then we're going to take a little 10 minute break.",
                    "label": 0
                },
                {
                    "sent": "This will be about an hour.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And and then we're going to talk about Bregman clustering and how you generalize K means to cluster under different metrics.",
                    "label": 0
                },
                {
                    "sent": "So for example, if you want to cluster under KL divergences, which is very popular when you're doing distributional, and your cluster kind of different distributions of points, but kill divergance has these nasty properties that, well, it's not symmetric.",
                    "label": 0
                },
                {
                    "sent": "It's not really a metric at all, it can be infinite between two points, and 0 otherwise.",
                    "label": 0
                },
                {
                    "sent": "So how do you?",
                    "label": 0
                },
                {
                    "sent": "Circumvent all of these barriers and still get to a practical and simple algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then in the third part, we're going to talk.",
                    "label": 0
                },
                {
                    "sent": "I'm going to go a little bit more philosophical to some extent and talk about stability and say, well, what are these assumptions about clusterings that we're making, and how do we put them.",
                    "label": 0
                },
                {
                    "sent": "We're not just clustering for the sake of clustering, where clustering because we're trying to do something else more.",
                    "label": 1
                },
                {
                    "sent": "Clustering for classification or clustering for some other reason.",
                    "label": 0
                },
                {
                    "sent": "And how do these?",
                    "label": 0
                },
                {
                    "sent": "Final objectives how should they drive our thinking about what algorithm should we use and what kind of assumptions are we making when we're saying, oh, just minimize the variance of the clusters and then we're going to there.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "And a tiny little bit of more details for K means we're going to look about and say, well seething K means selecting the initial centers is really the hardest part about K means running K means anatone implementing K means on its own should take you no more than half a day.",
                    "label": 0
                },
                {
                    "sent": "But really, how do you initialize K means to make it a good algorithm?",
                    "label": 1
                },
                {
                    "sent": "Then we're going to talk about a little bit about the running times and then running K means on large datasets.",
                    "label": 0
                },
                {
                    "sent": "What it means and how to do it.",
                    "label": 1
                },
                {
                    "sent": "What are the problems?",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, Bregman is a generalization of K means, so we're going to sort of segue right into that, reviewing some of the results, and then talk about this stability aspect.",
                    "label": 0
                },
                {
                    "sent": "How do we?",
                    "label": 0
                },
                {
                    "sent": "So with that in mind, let's jump right in an.",
                    "label": 0
                },
                {
                    "sent": "If you have any questions where sort of a small crowd.",
                    "label": 0
                },
                {
                    "sent": "Note some of us are more or less hiding in the back, so just interrupt at anytime.",
                    "label": 0
                },
                {
                    "sent": "Probably not by screaming you lie, but by some other methods.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's talk.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So K means.",
                    "label": 0
                },
                {
                    "sent": "So before we can really talked about K means.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, what does it mean to cluster?",
                    "label": 1
                },
                {
                    "sent": "So clustering is just very simple.",
                    "label": 0
                },
                {
                    "sent": "I'm going to give you endpoints.",
                    "label": 1
                },
                {
                    "sent": "I'm going to be nice and assume that they lie in some D dimensional Euclidean space and I want to split them into K groups.",
                    "label": 0
                },
                {
                    "sent": "So I give your endpoints in some space.",
                    "label": 0
                },
                {
                    "sent": "I give you a number of K and say give me your best split of these points into these cake groups.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, what does best really mean?",
                    "label": 0
                },
                {
                    "sent": "Right, so I have here example.",
                    "label": 0
                },
                {
                    "sent": "There is maybe 25 points I want to split them into three groups.",
                    "label": 0
                },
                {
                    "sent": "My split might be very different from your split with very different from.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Russia split, so here's 1 split so we have the green points.",
                    "label": 0
                },
                {
                    "sent": "We have the little cluster of Blues and a big thing of Reds on the side.",
                    "label": 0
                },
                {
                    "sent": "Is this a good clustering?",
                    "label": 0
                },
                {
                    "sent": "Well, maybe it depends what you want to do.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, so let's try to define an objective and say, well, how do we define best?",
                    "label": 1
                },
                {
                    "sent": "So one thing that we can do is to we can try to minimize the maximum radius of any cluster.",
                    "label": 0
                },
                {
                    "sent": "So this sort of talks about the tightness of any particular cluster, and from here you can say, well, this is not quite optimal, right?",
                    "label": 0
                },
                {
                    "sent": "So this blue one in particular is really small, and the red one is quite large and I can reduce the radius of red if I have.",
                    "label": 0
                },
                {
                    "sent": "If I re classify a couple of these points as blue guys.",
                    "label": 0
                },
                {
                    "sent": "And the red one will shrink.",
                    "label": 0
                },
                {
                    "sent": "The blue one will grow.",
                    "label": 0
                },
                {
                    "sent": "Quite a bit, but since I'm minimizing the maximum radius, they will actually improve the clustering with respect to the objective function.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, maybe that's not what you want to do.",
                    "label": 0
                },
                {
                    "sent": "Maybe you want to maximize the average Inter cluster distance, so you want to say these clusters are very well spread apart from each other, right?",
                    "label": 1
                },
                {
                    "sent": "They're not just kind of overlapping there really.",
                    "label": 0
                },
                {
                    "sent": "I have a real clustering.",
                    "label": 0
                },
                {
                    "sent": "That's fine and well.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One thing you can try to do, going back to the first one is to minimize the variance between the cluster, right?",
                    "label": 0
                },
                {
                    "sent": "So we all want tight, well separated clusters and the question is how to do, How do we define that?",
                    "label": 1
                },
                {
                    "sent": "So the variance is one of the measures, so I can say for a particular cluster I'm going to minimize the average distance between two points in that cluster and that will really lead me to the variance.",
                    "label": 0
                },
                {
                    "sent": "So this is the one we're going to talk about a little bit more.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we define the variance?",
                    "label": 1
                },
                {
                    "sent": "Well, let's talk about the expectation of a cluster.",
                    "label": 0
                },
                {
                    "sent": "So the expectation is simple.",
                    "label": 0
                },
                {
                    "sent": "So I give you a clustering and I give you.",
                    "label": 0
                },
                {
                    "sent": "I say, these five points belong to a particular cluster.",
                    "label": 0
                },
                {
                    "sent": "Well, the expectation is just.",
                    "label": 0
                },
                {
                    "sent": "If I were to pick a point at random, where do I expect it to be?",
                    "label": 1
                },
                {
                    "sent": "So it's just the mean of all of these points.",
                    "label": 0
                },
                {
                    "sent": "So four points X that are in a particular cluster.",
                    "label": 0
                },
                {
                    "sent": "Let's take the average, and that's the means here.",
                    "label": 0
                },
                {
                    "sent": "Well, now we can write the variance, so the variance is just the sum over all the points.",
                    "label": 0
                },
                {
                    "sent": "So if I have this as the variance of a particular cluster and now I have just the sum over all of the clustering, so I'm minimizing the sum of the variances over all of them.",
                    "label": 0
                },
                {
                    "sent": "And this is my objective.",
                    "label": 0
                },
                {
                    "sent": "OK, well there's one thing to define the objective, the sex.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Part is now to solve it, so I have this great problem.",
                    "label": 0
                },
                {
                    "sent": "I says well, given my points at X and a number of K, I'm going to find a clustering.",
                    "label": 1
                },
                {
                    "sent": "So partition of X into group C1 through CK that minimizes exactly the variance of some of the clusters.",
                    "label": 0
                },
                {
                    "sent": "This is the formula we had in the last line.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Before I continue, I want to talk about one notion.",
                    "label": 0
                },
                {
                    "sent": "That is close to the heart of any kind of approximations or theory person, and that's approximation algorithms.",
                    "label": 0
                },
                {
                    "sent": "So I can say, well, this variance has some optimum value.",
                    "label": 0
                },
                {
                    "sent": "So if I look over all possible ways to cluster this set of points, there's some optimum value of this variance fiestar.",
                    "label": 1
                },
                {
                    "sent": "Right, but you can't get any better.",
                    "label": 0
                },
                {
                    "sent": "This is just the property of your data well, and we're going to say that and some algorithm is Alpha approximate.",
                    "label": 1
                },
                {
                    "sent": "If it's within the factor of Alpha.",
                    "label": 1
                },
                {
                    "sent": "Of the optimum.",
                    "label": 0
                },
                {
                    "sent": "So since the variance fee star is the optimum, you're never going to be smaller than it, so whatever you find is going to be larger.",
                    "label": 0
                },
                {
                    "sent": "That's just by definition, but we're saying it's not going to be too large and it's going to be at most of factor of Alpha off and Alpha is greater than one.",
                    "label": 0
                },
                {
                    "sent": "So if we're talking about a two approximation, then I guarantee to you that at the end of the run of your algorithm, whatever answer you find is going to be within a factor of two of the best you could have done.",
                    "label": 0
                },
                {
                    "sent": "And maybe a factor of 2 doesn't sound that great an maybe some of the factors that we'll see later on will sound even worse, but this is a theory result, and in practice usually when you see a factor of two, it means 10%.",
                    "label": 0
                },
                {
                    "sent": "Or it means 1%?",
                    "label": 0
                },
                {
                    "sent": "I mean that now is data dependent, but for most of these there really is some example or some case where you will get a factor of two, so we can't prove a better result.",
                    "label": 0
                },
                {
                    "sent": "But this is what we have.",
                    "label": 0
                },
                {
                    "sent": "It doesn't mean that it's always a factor of two off, it means just adding the worst case.",
                    "label": 0
                },
                {
                    "sent": "It's a factor of two off.",
                    "label": 0
                },
                {
                    "sent": "OK, so now.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Back to solving this variance problem.",
                    "label": 0
                },
                {
                    "sent": "Not surprisingly, probably it's NP complete and it's a little bit surprising that it's NP complete even on the plane, so I give you the bunch of these points that are just lying on the plane, No 3 dimensions, No 4 dimensions, so you can very much visualize it and you still cannot find the absolute best variance clustering.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Open solving this problem, or at least attempting it for over 50 years now and this is the famous K means algorithm.",
                    "label": 1
                },
                {
                    "sent": "So there is many different citations.",
                    "label": 0
                },
                {
                    "sent": "It was rediscovered at least three times back 50 and 40 three years ago.",
                    "label": 0
                },
                {
                    "sent": "It's been rediscovered many more times since then.",
                    "label": 0
                },
                {
                    "sent": "It's been part of many, many homework assignments I gather, and it's been implemented.",
                    "label": 0
                },
                {
                    "sent": "I think it's fair to say 10s of thousands of times because it's.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So simple, so how does key means work?",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, we start with some data set.",
                    "label": 0
                },
                {
                    "sent": "This is what we have.",
                    "label": 0
                },
                {
                    "sent": "We have set of data points.",
                    "label": 1
                },
                {
                    "sent": "This is going to be the running example for a whole lot of the talk, so we have these nine points and we want to cluster them into three groups.",
                    "label": 0
                },
                {
                    "sent": "So what do we do well?",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At first we select three centers at random.",
                    "label": 1
                },
                {
                    "sent": "We're going to say, well, these three points.",
                    "label": 0
                },
                {
                    "sent": "Green, red, purple are representative, and these are going to be our cluster centers.",
                    "label": 0
                },
                {
                    "sent": "Now we say well, if these are the points.",
                    "label": 0
                },
                {
                    "sent": "Now let's find the actual clustering.",
                    "label": 0
                },
                {
                    "sent": "So let's assign each point to whichever.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Center is nearest to it, so the green Point.",
                    "label": 0
                },
                {
                    "sent": "This guy is slightly closer than the red one for the blue for these two points, this guys closer than the red.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One or the green one, and so on.",
                    "label": 0
                },
                {
                    "sent": "So you do this assignment and.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You do the assignment.",
                    "label": 0
                },
                {
                    "sent": "You can say, well, let me recompute the optimum centers.",
                    "label": 0
                },
                {
                    "sent": "So if I have the position of where?",
                    "label": 0
                },
                {
                    "sent": "If I have the partition of the points into the clustering, so I know that these two like green points belong in the same cluster, then I can find the optimum place to put the cluster center which is just exactly in between them.",
                    "label": 0
                },
                {
                    "sent": "And it turns out it's the mean of the points, hence the name K means.",
                    "label": 0
                },
                {
                    "sent": "And we.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Pete, so now the cluster centers have moved, so the assignment of which point belongs to which cluster has changed.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're going to reassign and repeat again.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Resign and now we reach a steady state or local minimum local optimum.",
                    "label": 0
                },
                {
                    "sent": "So every point is assigned to its nearest cluster center.",
                    "label": 0
                },
                {
                    "sent": "And every center is the mean of the points assigned to it.",
                    "label": 0
                },
                {
                    "sent": "So if you were to try and run the algorithm on this configuration, it would just instantly stop and say a. I didn't have to do anything, I'm done right?",
                    "label": 0
                },
                {
                    "sent": "If you started with a different configuration it runs for awhile and then it gets here.",
                    "label": 0
                },
                {
                    "sent": "So this is K means.",
                    "label": 0
                },
                {
                    "sent": "The first nontrivial thing is that.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This actually terminates, right?",
                    "label": 0
                },
                {
                    "sent": "You might try and think and say, well, if I have these points in multi dimensions can things go around in circles?",
                    "label": 0
                },
                {
                    "sent": "Can strange things happen?",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "So the one thing that we can absolutely guarantee is that the algorithm will terminate.",
                    "label": 0
                },
                {
                    "sent": "Because the total variance this potential function is reduced at every single step.",
                    "label": 0
                },
                {
                    "sent": "So in particular, if I assign each point to its nearest center, I'm going to for a single point, reduce its contribution to the total variance, because the contribution for a single point is the square of the distance between it and the nearest closest cluster center.",
                    "label": 1
                },
                {
                    "sent": "On this flip side, when I have a fixed clustering and I say the center is the mean of all of the points in that clustering, this will also reduce the potential.",
                    "label": 0
                },
                {
                    "sent": "This is not trivial.",
                    "label": 1
                },
                {
                    "sent": "This requires a proof.",
                    "label": 0
                },
                {
                    "sent": "The proof is not hard, but This is why the algorithm terminates.",
                    "label": 0
                },
                {
                    "sent": "So at every single point I know that this variance reduction that I have starts going down, and if it goes strictly down then I can't repeat the same clustering.",
                    "label": 0
                },
                {
                    "sent": "More than once because the potential has gone down, but now I know there's only so many different configurations.",
                    "label": 0
                },
                {
                    "sent": "There's only so many ways to partition a set of endpoints into K clusters.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of them, but there's a finite number.",
                    "label": 0
                },
                {
                    "sent": "Therefore this will terminate.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, what else do we know about the algorithm?",
                    "label": 1
                },
                {
                    "sent": "Well, we know it finds a local minimum.",
                    "label": 1
                },
                {
                    "sent": "So for example, here's of potential output.",
                    "label": 0
                },
                {
                    "sent": "So we have nine points, 3 on the left, three in the Middle, 3 on the right, and this if I have the horizontal version of clustering.",
                    "label": 0
                },
                {
                    "sent": "So the green, the red and the purple.",
                    "label": 0
                },
                {
                    "sent": "This is a perfectly good local minimum.",
                    "label": 0
                },
                {
                    "sent": "So if you start K means this way and you say this is my assignment, or these are my cluster centers, it will say again, I'm done.",
                    "label": 0
                },
                {
                    "sent": "Everything is ready.",
                    "label": 0
                },
                {
                    "sent": "Each point is assigned to its nearest cluster center and each center is the mean of all of the points assigned to it.",
                    "label": 0
                },
                {
                    "sent": "Of course, if I give you this nine points, an essay, cluster them into 3.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Groups you're more likely to cluster them this way.",
                    "label": 0
                },
                {
                    "sent": "Right, and this is one of the problems with K means.",
                    "label": 0
                },
                {
                    "sent": "We know it gives you a locally optimum solution, so from that solution you cannot make any one of these local changes.",
                    "label": 0
                },
                {
                    "sent": "In order to do it.",
                    "label": 0
                },
                {
                    "sent": "But we don't know whether it's even close to a global solution.",
                    "label": 0
                },
                {
                    "sent": "And as this example shows you, it could be arbitrarily far off.",
                    "label": 0
                },
                {
                    "sent": "So 4K means itself you can't prove a bound that says K means is A5 approximation, attend approximation 100 approximation because no matter what you do, I can make this example larger.",
                    "label": 0
                },
                {
                    "sent": "And everything breaks for you.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But one question that you should ask yourself is OK, that's all.",
                    "label": 0
                },
                {
                    "sent": "Well in theory, but does this actually happen right?",
                    "label": 1
                },
                {
                    "sent": "Is this some bad strange constructed theory example or I'm not going to get this if I actually run this on the real datasets?",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Answer is that it does happen even on these simple datasets.",
                    "label": 0
                },
                {
                    "sent": "So if I give you these four.",
                    "label": 0
                },
                {
                    "sent": "This example and tell you cluster this into four groups.",
                    "label": 0
                },
                {
                    "sent": "I think pretty much everybody in this room would say, well, this is obvious, right?",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, if you give this to K means it's not that obvious to K means and what's going to happen in the close.",
                    "label": 0
                },
                {
                    "sent": "You can see too well, but we have a green cluster.",
                    "label": 0
                },
                {
                    "sent": "This is perfect.",
                    "label": 0
                },
                {
                    "sent": "This cluster got split into two, so there's half of it here and another half on the side.",
                    "label": 0
                },
                {
                    "sent": "And these two guys are merged into one.",
                    "label": 0
                },
                {
                    "sent": "Now, why did and you can check that this is a locally optimum solution, so all of these points are too far away from here, so they would rather go to here and so nothing changes.",
                    "label": 0
                },
                {
                    "sent": "Now why would K means do such a strange thing?",
                    "label": 0
                },
                {
                    "sent": "Well, it's the initialization.",
                    "label": 0
                },
                {
                    "sent": "So initially I picked four points at random, and I happened to pick two from the same cluster, which is just a random event.",
                    "label": 0
                },
                {
                    "sent": "It will happen now.",
                    "label": 0
                },
                {
                    "sent": "I ran K means and this is what it's going to converge to.",
                    "label": 0
                },
                {
                    "sent": "And the real problem is that if I have this example and I have many more of these, if I have 100 of these kind of well separated, reasonably well separated Gaussian clusters and I tried with 100 points, I will get something like this every single time.",
                    "label": 0
                },
                {
                    "sent": "So no matter how often or how much you start and you reinitialize it and you pick a different set of random samples, you will still get this event.",
                    "label": 0
                },
                {
                    "sent": "Two of them will be merged and two one of them will be split.",
                    "label": 0
                },
                {
                    "sent": "Or more of them will be merged and some of them will be split.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Finding this good initial point set was considered a black art and this is where a lot of the work on K means has been in the past 50 years and there are several things that you can do reason.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can try many times with different random seeds and you can say, well, this one didn't work.",
                    "label": 1
                },
                {
                    "sent": "This is random.",
                    "label": 0
                },
                {
                    "sent": "Let me start it again.",
                    "label": 0
                },
                {
                    "sent": "I'm going to throw a bunch of different random points in the beginning.",
                    "label": 0
                },
                {
                    "sent": "Something is going to come out and I'm going to do this fifty 100 times as we'll see in the second K means is very fast in terms of convergence, so I can afford to do this 100 * 1 of them is bound to be good and I'll take the best one and then I'll go on with my life.",
                    "label": 1
                },
                {
                    "sent": "But as I mentioned just now, this has limited benefit even in this trivial case or trivial choice, because we see the global picture where things are very well separated.",
                    "label": 0
                },
                {
                    "sent": "If I do this, I'm still going to break every single time.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's other methods, so there's literally hundreds of papers that have been written on this, and a lot of them are quite clever and a lot of them do quite well.",
                    "label": 0
                },
                {
                    "sent": "There's preprocessing, maybe I'm going to pick more centers.",
                    "label": 0
                },
                {
                    "sent": "There's post processing that says, well, maybe I'm going to cluster with two K centers and then merge some of these clusters together in the end to try to avoid this problem.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'm going to do some agglomerative or kind of different versions of clusterings and then use that to initialize K means and there's bisecting K means there's all sorts of things.",
                    "label": 0
                },
                {
                    "sent": "But all of them break on some datasets, so most of them if you look at them closely, you can say, well, this doesn't.",
                    "label": 0
                },
                {
                    "sent": "Here's the data set where this wouldn't work.",
                    "label": 0
                },
                {
                    "sent": "Maybe it works on some data, but on some data it doesn't.",
                    "label": 0
                },
                {
                    "sent": "And what I'm going to talk about for the next few minutes.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that there is a simple initialization scheme and I'm going to try to convince you that it's simple that actually leads us with some provable guarantees and actually does work in practice overall.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what was the problem with this Gaussian example or?",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The problem really was that we merge some of these guys together.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there is a really simple fix it all you had to do is cluster these nice well separated points and they were nice and clean like this.",
                    "label": 0
                },
                {
                    "sent": "Here's one thing that you can try to do, you can say.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, I'm going to choose the first point at random again because I don't know how to do anything else.",
                    "label": 0
                },
                {
                    "sent": "So the first point I'm just going to pluck it and say this is my first center.",
                    "label": 0
                },
                {
                    "sent": "Then I'm going to say well which one of all of the remaining points is the furthest away from the center that I already have?",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At end is going to be this guy over here.",
                    "label": 0
                },
                {
                    "sent": "Now, given that these are well kind of clustered and there is a tight well separated clusters, I can do this again and I can say well out of these two, which one is the furthest away out of the remaining points?",
                    "label": 0
                },
                {
                    "sent": "Which one is the furthest away from any of the ones that I picked?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One is going to be some guy here and you can see where this is.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going right, I repeat this again.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now, boom, I have a perfect clustering.",
                    "label": 0
                },
                {
                    "sent": "So why am I even here, right?",
                    "label": 0
                },
                {
                    "sent": "Like why don't we just use this all the time?",
                    "label": 0
                },
                {
                    "sent": "This should have been discovered like 40 years ago, and then we'd be done from this.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Well, the reason is your data doesn't look this nice right?",
                    "label": 0
                },
                {
                    "sent": "If our data look this nice, there will be no Keedy.",
                    "label": 0
                },
                {
                    "sent": "This is maybe a little bit more of what our data looks like, and if we try the further.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Point heuristic here.",
                    "label": 0
                },
                {
                    "sent": "With this one single nasty outlier, he's not really part of any of these three clusters, would say, well, let's pick the green Now let's.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At the point furthest away.",
                    "label": 0
                },
                {
                    "sent": "That's the outlier, whoops.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now let's look at some other point.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's even this red one.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now cluster and then we said, hey, the outliers, a single point?",
                    "label": 0
                },
                {
                    "sent": "We have this huge cluster here which doesn't make much sense.",
                    "label": 0
                },
                {
                    "sent": "If you started and then we have the perfect green cluster on the left.",
                    "label": 0
                },
                {
                    "sent": "So that's a problem, so that's what we have to fix.",
                    "label": 0
                },
                {
                    "sent": "But of course, if we were just doing random sampling and the initial version of K means we would never pick this single outlier point, right?",
                    "label": 0
                },
                {
                    "sent": "If we were just repeat this many times, the single outlier point wouldn't exist because we pick one from here 1 from here and one from here, and we'd be done.",
                    "label": 0
                },
                {
                    "sent": "So somehow?",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You want to interpolate between these two methods.",
                    "label": 1
                },
                {
                    "sent": "That one says, do everything randomly.",
                    "label": 0
                },
                {
                    "sent": "It's pretty good.",
                    "label": 0
                },
                {
                    "sent": "The other one says pick the furthest one.",
                    "label": 0
                },
                {
                    "sent": "I need to say well that runs into trouble.",
                    "label": 0
                },
                {
                    "sent": "That's two deterministic, so you want to somehow shift your mindset.",
                    "label": 0
                },
                {
                    "sent": "It's picking stuff that's too far that's far away, but not be too deterministic about it.",
                    "label": 0
                },
                {
                    "sent": "So how do we do this formally?",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let me define by capital D just the distance between the point and its nearest cluster center.",
                    "label": 1
                },
                {
                    "sent": "Right, so this is the contribution to the variance and not just the distance.",
                    "label": 0
                },
                {
                    "sent": "Rather and I'm going to choose the next point proportional to this distance D raised to some value Alpha, now how?",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Does this work?",
                    "label": 0
                },
                {
                    "sent": "So if Alpha is equal to 0, anything raised to zero is 1, so every point is equally likely to get selected, so this is my random initialization, right?",
                    "label": 0
                },
                {
                    "sent": "I'm going to pick one point at random and go on.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "If Alpha is equal to Infinity, this just says, well, one of these dies.",
                    "label": 0
                },
                {
                    "sent": "The largest one will dominate everybody else.",
                    "label": 0
                },
                {
                    "sent": "And so I'm only going to pick.",
                    "label": 0
                },
                {
                    "sent": "I'm going to put all of our my probability mass on the guy that's furthest away.",
                    "label": 0
                },
                {
                    "sent": "So this is the furthest point, heuristic, right?",
                    "label": 1
                },
                {
                    "sent": "So I'm going to pick the furthest guy away and now I have this little nob that I can tune and I can set it anywhere between zero and Infinity and I can interpolate between these two methods.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Alpha equals two turns out to be the magic number.",
                    "label": 0
                },
                {
                    "sent": "And we'll see why in a second, and this led to an algorithm which we call K means plus plus size K means, but tiny little bit better.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So more generally, why does Alpha equals to the magic number here?",
                    "label": 0
                },
                {
                    "sent": "Well, it's because we're trying to minimize the square of the distance is between a point and its nearest center.",
                    "label": 1
                },
                {
                    "sent": "If we were just trying to minimize the distance is the sum of the distances between the point and its nearest center, we would sample.",
                    "label": 0
                },
                {
                    "sent": "We would set Alpha equal to 1 and sample according to D. If you're trying to minimize the maximum distance, so this is going to minimize the radius of the cluster, you would sample according to to the Infinity you would use the furthest point heuristic and this is at this point this becomes kind of a famous known algorithm.",
                    "label": 1
                },
                {
                    "sent": "But setting Alpha to equal to two or more generally saying well, if you contribute a lot to the overall error, if you contribute a lot to the variance in the end, then I'm going to pick you with high probability.",
                    "label": 1
                },
                {
                    "sent": "If you contribute a little, I'm still going to have some probability of picking you, but not too much, and so this gives us kind of this.",
                    "label": 0
                },
                {
                    "sent": "This is the rule of thumb probability proportional to its contribution to the error.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the data set looks Gaussian, So what happens?",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we picked the first point at random again.",
                    "label": 0
                },
                {
                    "sent": "Now we have a choice.",
                    "label": 0
                },
                {
                    "sent": "Well, this guy contributes a lot to the error, but there's only one of him.",
                    "label": 0
                },
                {
                    "sent": "Whereas these guys each one contributes some amount to the error because they're still pretty far away.",
                    "label": 0
                },
                {
                    "sent": "But there is a lot of them.",
                    "label": 0
                },
                {
                    "sent": "So what we're going to do?",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Who's going to pick one from here?",
                    "label": 0
                },
                {
                    "sent": "From these two and again.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And now.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we have World War One.",
                    "label": 0
                },
                {
                    "sent": "Yes, this is an outlier.",
                    "label": 0
                },
                {
                    "sent": "It doesn't really matter which cluster he belongs to.",
                    "label": 0
                },
                {
                    "sent": "We're in good shape.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "On the flip side, say this was our point set, so the outlier was actually really far away.",
                    "label": 0
                },
                {
                    "sent": "Right, he wasn't just sitting somewhere close, but he was really, really far away.",
                    "label": 0
                },
                {
                    "sent": "Now if you want to look for the optimum solution here, it turns out that this is no longer an outlier.",
                    "label": 0
                },
                {
                    "sent": "If you're trying to minimize variance, this really should be its own cluster, because it's so far away if you put him into a cluster with anybody else, he's going to draw, and he's going to.",
                    "label": 1
                },
                {
                    "sent": "His variance is going to be so high, and he's going to contribute too much to the variance of this cluster.",
                    "label": 0
                },
                {
                    "sent": "So now I.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We run green here.",
                    "label": 0
                },
                {
                    "sent": "Well now this guy is so.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Far away, imagine him down the street.",
                    "label": 0
                },
                {
                    "sent": "Well, we should pick him, and again, there's only one of him, but he by himself contributes a lot to the overall error, so we can.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Him and now merging the.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two clusters together is actually the right thing to do.",
                    "label": 0
                },
                {
                    "sent": "And the algorithm attempts to do that.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what can we say about the performance of K means plus plus?",
                    "label": 0
                },
                {
                    "sent": "So we talked a lot about things should work well.",
                    "label": 0
                },
                {
                    "sent": "Things should be fast.",
                    "label": 0
                },
                {
                    "sent": "Can we say anything here?",
                    "label": 0
                },
                {
                    "sent": "Not surprisingly, the answer is yes.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And we can say actually two things which is.",
                    "label": 0
                },
                {
                    "sent": "Nice, so there's one theorem that says this is always a lot K approximation and we talked about approximations before, and I warned you that a lot K sounds scary, because in particular grows with K and there is a big O sitting here.",
                    "label": 0
                },
                {
                    "sent": "It's really like 16 log K and there really are examples where it is 16 log K, But as we'll see in a second, there are also many, many examples where it's not 16 lock A and it's something much better.",
                    "label": 0
                },
                {
                    "sent": "We don't know how much better it is because it's an NP hard problem, so we don't know what the optimal solution is, but we know that it's pretty good.",
                    "label": 0
                },
                {
                    "sent": "There is a.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Different.",
                    "label": 0
                },
                {
                    "sent": "Theorem by a different set of authors around the same time, and then said if we modified this algorithm a little bit but really keep the spirit of it the same, and we assume some condition on the data, we say that the data is nicely clusterable, which really says it means that it does look like a bunch of well separated.",
                    "label": 1
                },
                {
                    "sent": "Clumps of points, then the same algorithm really gives you a constant approximation, so the only times that.",
                    "label": 0
                },
                {
                    "sent": "You get this log K if your data is looks nasty.",
                    "label": 0
                },
                {
                    "sent": "So let's dive this into this a little bit.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what does it mean to be nicely clusterable to?",
                    "label": 0
                },
                {
                    "sent": "One way to say this?",
                    "label": 0
                },
                {
                    "sent": "Well, if I went from K -- 1 clusters to K clusters, my total variance must have dropped by whole lot.",
                    "label": 1
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In particular, it dropped by a constant, so appoint said is well separated.",
                    "label": 0
                },
                {
                    "sent": "If the optimum solution using K clusters is much much less or constant factor less than optimal solution using K -- 1 clusters.",
                    "label": 1
                },
                {
                    "sent": "But that's the intuition.",
                    "label": 0
                },
                {
                    "sent": "So in the intuition is that it does look like a bunch of well separated clumps of points, then this gives.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Constant approximation.",
                    "label": 0
                },
                {
                    "sent": "Why well, why?",
                    "label": 0
                },
                {
                    "sent": "There says like this is exactly the example from the pictures.",
                    "label": 0
                },
                {
                    "sent": "So if the algorithm picks a point from a new cluster, that's.",
                    "label": 1
                },
                {
                    "sent": "Part of the new optimal cluster.",
                    "label": 0
                },
                {
                    "sent": "Every time we're doing this iteration, then it's doing pretty well, but if it doesn't pick a point.",
                    "label": 0
                },
                {
                    "sent": "If it splits and optimal cluster into two, it means that other clusters are sort of weak.",
                    "label": 0
                },
                {
                    "sent": "They have very few points and they don't contribute much, so it's OK to merge them.",
                    "label": 0
                },
                {
                    "sent": "We're not going to be optimal, we're just going to be near optimal, so it's OK to merge them and continue.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So as long as the clusters are well separated, the first condition holds.",
                    "label": 0
                },
                {
                    "sent": "If they're not too well separated, then the second one kicks in and overall we get this either constant or log K.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And these are the two theorems that I talked about before.",
                    "label": 0
                },
                {
                    "sent": "So now you should say, well, that's all in great.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And that works in theory.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That actually work, right?",
                    "label": 0
                },
                {
                    "sent": "Like does this?",
                    "label": 0
                },
                {
                    "sent": "Is this practical?",
                    "label": 0
                },
                {
                    "sent": "Does this give me a better solution?",
                    "label": 0
                },
                {
                    "sent": "Does it actually work?",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's the answer, so here's a table.",
                    "label": 0
                },
                {
                    "sent": "Let me explain this a little bit.",
                    "label": 0
                },
                {
                    "sent": "So this is the ratio between the potential of K means divided by K means plus plus.",
                    "label": 0
                },
                {
                    "sent": "So if the numbers are larger, that means that the initialization worked better.",
                    "label": 0
                },
                {
                    "sent": "So the number is 1.01.",
                    "label": 0
                },
                {
                    "sent": "That means the initialization for K means plus plus gave you a 1% improvement overall if the number is 2.4, it means they give you a two and a half a factor of 2 1/2 improvement in terms of the total variance.",
                    "label": 0
                },
                {
                    "sent": "If it's lower.",
                    "label": 0
                },
                {
                    "sent": "So here there's a point 3% reduction.",
                    "label": 0
                },
                {
                    "sent": "So this was done by a colleague of mine and that of 15 different datasets.",
                    "label": 0
                },
                {
                    "sent": "There's different sizes, there's different dimensionalities, and so on, and different values of K. So depending on our clustering into just a few things, so we're clustering into many different buckets.",
                    "label": 0
                },
                {
                    "sent": "And overall this works right?",
                    "label": 0
                },
                {
                    "sent": "So sometimes it's a little bit worse.",
                    "label": 0
                },
                {
                    "sent": "In one example, out of these 60, sometimes it's way much better, and all of these were done with random restarts and sort of how you would actually implement K means.",
                    "label": 0
                },
                {
                    "sent": "But on average, by which I mean median, it's about 20% better.",
                    "label": 0
                },
                {
                    "sent": "Any?",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here's one more kind of description of what this looks like, so here's on the Y axis.",
                    "label": 0
                },
                {
                    "sent": "We have the error, so the total variance of the clusters on the X axis.",
                    "label": 0
                },
                {
                    "sent": "We have this stage, the number of iterations run by each algorithm.",
                    "label": 0
                },
                {
                    "sent": "So K means is here in the blue line as as we can see, it's monotonically decreasing, so that's what we claimed that the potential always goes down and it drops a lot in the very beginning because your random clustering initially is not very good, it's just random, and then it sort of levels off somewhere.",
                    "label": 0
                },
                {
                    "sent": "And if you let it run, it will usually just stops, like right around here, but sometimes it will continue for awhile.",
                    "label": 0
                },
                {
                    "sent": "The Red Line is a hybrid method which I'm not going to talk about much, and the green one says, well, use this other initialization for K means.",
                    "label": 0
                },
                {
                    "sent": "So we start lower and this is where the theory bound comes in, and then we get even lower overall, right?",
                    "label": 0
                },
                {
                    "sent": "So you drop even faster an.",
                    "label": 0
                },
                {
                    "sent": "His batteries dying.",
                    "label": 0
                },
                {
                    "sent": "So this is 1 typical run on one different data.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Some of them look like this.",
                    "label": 0
                },
                {
                    "sent": "This is your factor of.",
                    "label": 0
                },
                {
                    "sent": "2050 hundred etc right?",
                    "label": 0
                },
                {
                    "sent": "So there's K means and K means goes and it gets to some point, but it doesn't know how to get any lower and K means plus.",
                    "label": 0
                },
                {
                    "sent": "Plus just start way lower.",
                    "label": 0
                },
                {
                    "sent": "Right, so this is your Gaussian example of where things are well separated, and we know K means is not going to do well, and we know that this is exactly the fix that we're trying to do, and that's why you see this huge gap between the two.",
                    "label": 1
                },
                {
                    "sent": "So the answer is yes, it does work, and it's a useful thing to do.",
                    "label": 0
                },
                {
                    "sent": "But one thing that you can ask yourself on, judging by these plots is how fast this K means actually converge, right?",
                    "label": 0
                },
                {
                    "sent": "And so we all know that it's going to converge eventually.",
                    "label": 0
                },
                {
                    "sent": "We all know that there's some bound on it, which is huge.",
                    "label": 1
                },
                {
                    "sent": "We just as well it doesn't repeat any particular clustering, but that's not sort of good enough.",
                    "label": 0
                },
                {
                    "sent": "And if we stare at, these were going to say, well, no key means converged here.",
                    "label": 0
                },
                {
                    "sent": "After like 60 iterations and if you run K means on any of your datasets, you will notice that it converges after 60 iterations.",
                    "label": 0
                },
                {
                    "sent": "If it's a particularly bad data set, it will converge after 100 iterations, maybe 200.",
                    "label": 0
                },
                {
                    "sent": "I don't think I've ever seen 200.",
                    "label": 0
                },
                {
                    "sent": "So somewhere sometimes it's 10, sometimes it's 20.",
                    "label": 0
                },
                {
                    "sent": "It will always converge, and usually it will converge very, very quickly.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how well does it converge?",
                    "label": 0
                },
                {
                    "sent": "What can we say about this?",
                    "label": 0
                },
                {
                    "sent": "Unfortunately, in the worst case, it doesn't really converge all that well at all, so there are examples.",
                    "label": 0
                },
                {
                    "sent": "And Andrea who wrote this paper actually.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Has a pro script that generates them that what says if you run on K means on this particular example and with this particular set of initial cluster centers it will take exponential time.",
                    "label": 1
                },
                {
                    "sent": "So here is the exponential.",
                    "label": 0
                },
                {
                    "sent": "Isn't K the number of centers?",
                    "label": 0
                },
                {
                    "sent": "So if K is 100,000 etc there are examples will run as well and the surprising part again is that these examples live in the plane, so it's not that these examples are somehow relion multi dimensional structures or something.",
                    "label": 0
                },
                {
                    "sent": "Strange that we can visualize these examples live in the plane, and if we want to make them kind of very nice and make sure that all of the distances between any two points are close to each other, then we have to move into 3 dimensions.",
                    "label": 0
                },
                {
                    "sent": "But it's again still something that you can look and you can convince yourself.",
                    "label": 0
                },
                {
                    "sent": "Yes, it really will take this long.",
                    "label": 0
                },
                {
                    "sent": "On the plane Kimmins.",
                    "label": 0
                },
                {
                    "sent": "In the worst case.",
                    "label": 1
                },
                {
                    "sent": "But it runs in 100 iterations, right?",
                    "label": 0
                },
                {
                    "sent": "Like I don't care that it could take two to the K time in the plane on any data set I've seen, it runs in 100 iterations.",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's going on like?",
                    "label": 0
                },
                {
                    "sent": "Why does it work in practice?",
                    "label": 0
                },
                {
                    "sent": "But it does not work in theory.",
                    "label": 1
                },
                {
                    "sent": "What is it that's this?",
                    "label": 0
                },
                {
                    "sent": "Is something that should kind of make us think a little bit?",
                    "label": 0
                },
                {
                    "sent": "Because something there is something going on that's that we don't really quite understand.",
                    "label": 0
                },
                {
                    "sent": "And really, what's going on is one thing to look at it.",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "How robust are these worst case examples?",
                    "label": 1
                },
                {
                    "sent": "And so we have this particular worst case example, and then this example.",
                    "label": 0
                },
                {
                    "sent": "It takes exponential time, but if I were to change example just a tiny little bit, but it still take exponential time because we know that in real life our data is noisy, it's not like we're playing a game with the devil where the devil says uh-huh.",
                    "label": 0
                },
                {
                    "sent": "This is where I'm going to put the next point, and this is where I'm going to put the next point.",
                    "label": 0
                },
                {
                    "sent": "And this is how I'm going to choose your random seed so that you do the worst possible random initiation of these clusters.",
                    "label": 0
                },
                {
                    "sent": "And now you're going to lose and you're going to wait exponential time.",
                    "label": 0
                },
                {
                    "sent": "It feels that way more often than it should, but it doesn't really happen this way, right?",
                    "label": 0
                },
                {
                    "sent": "There is a little bit of noise in the data.",
                    "label": 0
                },
                {
                    "sent": "So what are we going to say is?",
                    "label": 0
                },
                {
                    "sent": "Well, if we perturb the points a little bit, if we change, if we just add a little bit of noise to the points, does this change things?",
                    "label": 1
                },
                {
                    "sent": "So here.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So our data set from before and we're going.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Change it a little bit.",
                    "label": 0
                },
                {
                    "sent": "Again, here it was.",
                    "label": 0
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It is now.",
                    "label": 0
                },
                {
                    "sent": "The optimum solution does not change, right?",
                    "label": 0
                },
                {
                    "sent": "I added a tiny little bit of noise to the points, maybe changes, maybe changes by a fraction of 1%.",
                    "label": 1
                },
                {
                    "sent": "I don't really care if this is if my structure somehow critically depends on the data that I can't change it.",
                    "label": 0
                },
                {
                    "sent": "Even a tiny little bit.",
                    "label": 0
                },
                {
                    "sent": "Maybe my data is wrong.",
                    "label": 0
                },
                {
                    "sent": "Maybe I'm solving the wrong problem and I should be able to change my data.",
                    "label": 0
                },
                {
                    "sent": "My data is noisy, it's coming in from all sorts of things.",
                    "label": 0
                },
                {
                    "sent": "Floating point errors, you name it.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is the formal statement of that problem, so we're going to take our data set.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're going to add a little bit of independent noise to every single point, so normal at zero with some variance Sigma squared.",
                    "label": 0
                },
                {
                    "sent": "And now I'm going to look at the maximum running time over all the inputs of the expected.",
                    "label": 0
                },
                {
                    "sent": "Running time subject to this noise so expectation over the noise of the running time and then take the maximum of the expectation.",
                    "label": 1
                },
                {
                    "sent": "And this should somehow encode these brittle examples.",
                    "label": 0
                },
                {
                    "sent": "If the worst case examples are brittle, then well, one of these points inside the expectation will be really bad, but the rest of them will be quite good, so things will average out if the worst examples are not brittle.",
                    "label": 0
                },
                {
                    "sent": "If there's, here's a whole data set, no matter how you perturbate, you're still going to be in bad shape then this is not going to buy us anything.",
                    "label": 0
                },
                {
                    "sent": "So this is called smooth analysis and it's actually been used to explain why some of the algorithms that perform very well in practice, notably the simplex method for solving linear programming, say, perform well in practice, but you horribly in theory, so there are worse case examples for all of them.",
                    "label": 0
                },
                {
                    "sent": "Let's say they're not going to work in theory, but in practice they just work.",
                    "label": 0
                },
                {
                    "sent": "So this is part of the disconnect the theorie.",
                    "label": 0
                },
                {
                    "sent": "Worst case examples are very, very brittle.",
                    "label": 0
                },
                {
                    "sent": "They are not robust to noise, and in reality our data is always a little bit noisy.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so given this definition, what can we say?",
                    "label": 0
                },
                {
                    "sent": "Well, we can prove this scary looking bound.",
                    "label": 0
                },
                {
                    "sent": "This is not great, right?",
                    "label": 1
                },
                {
                    "sent": "So this is N to 34, K to 34.",
                    "label": 0
                },
                {
                    "sent": "And even if both of these are equal to two, which will give you a trivial clustering, are bound is like 2 to the 70.",
                    "label": 0
                },
                {
                    "sent": "Which is around 10 million iterations.",
                    "label": 0
                },
                {
                    "sent": "More 10 billion iterations.",
                    "label": 0
                },
                {
                    "sent": "Plus a whole bunch of other factors, right?",
                    "label": 0
                },
                {
                    "sent": "So this is not great, but it's a theory bound and the main points notice here is that it says it's polynomial.",
                    "label": 0
                },
                {
                    "sent": "It's no longer exponential.",
                    "label": 0
                },
                {
                    "sent": "We're not saying it's not two to the K or two to the end, we're saying it's end to some power kata, some power capital D is the diameter of the point.",
                    "label": 0
                },
                {
                    "sent": "Said there's some little.",
                    "label": 0
                },
                {
                    "sent": "D is the dimensionality of the points, etc.",
                    "label": 1
                },
                {
                    "sent": "So the bound is large.",
                    "label": 1
                },
                {
                    "sent": "It's not exponential, and as K grows to Infinity for.",
                    "label": 0
                },
                {
                    "sent": "Very large K, probably in the thousands.",
                    "label": 0
                },
                {
                    "sent": "This will actually be true, so this will be a tighter bound.",
                    "label": 0
                },
                {
                    "sent": "But the point is, again, this explains.",
                    "label": 0
                },
                {
                    "sent": "It's not really that we're saying it's going to run an end to the 30.",
                    "label": 0
                },
                {
                    "sent": "Four time.",
                    "label": 0
                },
                {
                    "sent": "We're saying that This is why this says it shouldn't be exponential.",
                    "label": 0
                },
                {
                    "sent": "This is what's going on.",
                    "label": 0
                },
                {
                    "sent": "It really is polynomial, so it's sufficient if you.",
                    "label": 0
                },
                {
                    "sent": "Do that.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, the comparing balance.",
                    "label": 0
                },
                {
                    "sent": "We say well, the worst case complexity of K means is really bad, so it has exponential worst case complexity, but.",
                    "label": 1
                },
                {
                    "sent": "The smooth complexity, the complexity that we expect to see out of K means, is actually polynomial.",
                    "label": 0
                },
                {
                    "sent": "Which is great, so the pathological examples are really brittle.",
                    "label": 1
                },
                {
                    "sent": "So unless you do find yourself playing this game with the devil, you're not going to run into these worst case examples and a little bit of random noise.",
                    "label": 1
                },
                {
                    "sent": "Even if you are playing this game, you're one kind of trick is to say, well, let me add a little bit of noise to your data cluster 8 and then return you the clustering on this noisy data 'cause the noise doesn't change the optimum all that much, but it does change the performance.",
                    "label": 0
                },
                {
                    "sent": "At least what we can prove about the performance of the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for the summary of K means we have.",
                    "label": 0
                },
                {
                    "sent": "Exponential worst case.",
                    "label": 0
                },
                {
                    "sent": "We have usual typical case M4.",
                    "label": 1
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The solution quality.",
                    "label": 0
                },
                {
                    "sent": "We know that K means by itself with a random initialization will take us to some arbitrary local optimum, so we're going to run it.",
                    "label": 1
                },
                {
                    "sent": "We know that what we get in the end is a locally optimum solution.",
                    "label": 0
                },
                {
                    "sent": "But we also know that a simple initialization will lead us to a good solution and will lead us to a solution that's actually usually much, much better than regular K means.",
                    "label": 1
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we were to implement this, the implementation is pretty simple, so came in the initialization routine.",
                    "label": 0
                },
                {
                    "sent": "Just requires you to draw K different points, but after every draw your distribution changes.",
                    "label": 0
                },
                {
                    "sent": "So you're going to take ND time roughly to compute your distribution.",
                    "label": 0
                },
                {
                    "sent": "Then you pick a point from that distribution.",
                    "label": 0
                },
                {
                    "sent": "Then you take ND time again.",
                    "label": 0
                },
                {
                    "sent": "You pick a point, etc.",
                    "label": 0
                },
                {
                    "sent": "So the total running time is about NKD.",
                    "label": 1
                },
                {
                    "sent": "You can make it a little bit smaller, but not by a huge factors and overall running time of regular K means is also in KD, right?",
                    "label": 1
                },
                {
                    "sent": "Because at every single iteration for every single point and you want to compare its distance to every single one of its cluster centers, this is K of them and it's going to take you about the time in D dimensional space to do it.",
                    "label": 0
                },
                {
                    "sent": "There are some data structures that you can use to make this faster.",
                    "label": 0
                },
                {
                    "sent": "You can get this down to NK log D, an small dimensions, and.",
                    "label": 0
                },
                {
                    "sent": "You can do other tricks to make this a little bit faster, so there's definitely a lot of work done on how to make a means much more efficient, more efficient than it already is, but it's pretty good overall even if you did just a naive implementation.",
                    "label": 0
                },
                {
                    "sent": "The good part about it is that it's about 5060 lines of code.",
                    "label": 0
                },
                {
                    "sent": "You can whip it up in a couple hours, 4 hours after debugging, and it will work.",
                    "label": 0
                },
                {
                    "sent": "Any finishes after Council member runs great.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But there's a question.",
                    "label": 0
                },
                {
                    "sent": "And where at KDD both of our data is large, right?",
                    "label": 0
                },
                {
                    "sent": "Like NKD looks like a very good bound.",
                    "label": 0
                },
                {
                    "sent": "It's linear in all different three of these parameters.",
                    "label": 0
                },
                {
                    "sent": "But we're multiplying these things together.",
                    "label": 0
                },
                {
                    "sent": "So if N is an order of millions, K is on the order of thousands and dies on the order of thousands.",
                    "label": 0
                },
                {
                    "sent": "Now we're talking about a billion operations for each one an.",
                    "label": 0
                },
                {
                    "sent": "This computers are fast, but they're not that fast.",
                    "label": 0
                },
                {
                    "sent": "So can we paralyze?",
                    "label": 0
                },
                {
                    "sent": "So there are people right next door are talking about some ways to paralyze K means, and we're going to talk about slightly different ways to parallelize K means.",
                    "label": 0
                },
                {
                    "sent": "And if you put both of these together then you get an even faster version of K means.",
                    "label": 0
                },
                {
                    "sent": "But these are kind of two very disjoint avenues.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how do we paralyze?",
                    "label": 0
                },
                {
                    "sent": "How do we make it MapReduce friendly?",
                    "label": 0
                },
                {
                    "sent": "How do we make it?",
                    "label": 0
                },
                {
                    "sent": "Who do friendly?",
                    "label": 0
                },
                {
                    "sent": "How do we make it cluster friendly?",
                    "label": 0
                },
                {
                    "sent": "So here's our basic approach.",
                    "label": 0
                },
                {
                    "sent": "We're going to take our data, set X, and remember X is huge and we're going to split it just randomly in points roughly equal size.",
                    "label": 1
                },
                {
                    "sent": "So if we're going to say we have M different machines, whether these are machines in the cluster or these are mappers and reducers in New York group environment, or something else?",
                    "label": 0
                },
                {
                    "sent": "Roughly equal, they don't have to be identical.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In parallel now we're going to cluster each one of these partitions, so we're going to find the same clustering sequence for KC KA good clustering.",
                    "label": 0
                },
                {
                    "sent": "Now we know how to do that.",
                    "label": 0
                },
                {
                    "sent": "We have them in different methods, so in parallel we find a clustering on each one, the partitions and we're going to remember how many points there are in each cluster.",
                    "label": 1
                },
                {
                    "sent": "This turns out to be useful, so you want to know, or was this a Singleton cluster that was just the outlier, or did I have 1000 points associated with it?",
                    "label": 0
                },
                {
                    "sent": "Then we.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "All of these clusters and we bring them back together and we treat them as just regular points with some weights attached to them.",
                    "label": 0
                },
                {
                    "sent": "So we summarize each one of the all of the data in X1 just by its clustering C1.",
                    "label": 0
                },
                {
                    "sent": "And then we cluster the clusters.",
                    "label": 1
                },
                {
                    "sent": "So will come by why all of the clusterings?",
                    "label": 1
                },
                {
                    "sent": "And then we find the clustering of Y and the only thing we need to be careful about is that when we're finding this clustering, we need to remember about these weights.",
                    "label": 0
                },
                {
                    "sent": "So was this a single point?",
                    "label": 0
                },
                {
                    "sent": "Was this 10?",
                    "label": 0
                },
                {
                    "sent": "But this just goes through.",
                    "label": 0
                },
                {
                    "sent": "You multiply the contribution of a particular point by the weight of it.",
                    "label": 0
                },
                {
                    "sent": "Or when you're computing the distances in computing the means you do the same thing.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so again in pictures so we have X now.",
                    "label": 0
                },
                {
                    "sent": "X is large X used to be small in the same data set nouns large.",
                    "label": 0
                },
                {
                    "sent": "We're going to partition it.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Into two groups.",
                    "label": 0
                },
                {
                    "sent": "So we're going to have the.",
                    "label": 0
                },
                {
                    "sent": "Filled in the solid ones an just a white bunch.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to find a clustering separately.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each one, so let's look at the white ones.",
                    "label": 0
                },
                {
                    "sent": "We run our cluster.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Maybe we'll find these four different cluster centers great.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at the black ones.",
                    "label": 0
                },
                {
                    "sent": "Now we're going to find.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Clustering again, maybe here.",
                    "label": 0
                },
                {
                    "sent": "We didn't do quite as well, so we have these two that emerged together.",
                    "label": 0
                },
                {
                    "sent": "And we have the purple one on the right.",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Bring them together so these are the two clusterings from the two different from the black points and the white points so.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Looks mildly representative of the data.",
                    "label": 1
                },
                {
                    "sent": "We have too many points here, and kind of too few over here, but more or less the same.",
                    "label": 0
                },
                {
                    "sent": "And now.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We can cluster these clusters.",
                    "label": 0
                },
                {
                    "sent": "So treat them as regular points, cluster them.",
                    "label": 0
                },
                {
                    "sent": "Now if we.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look at this final clustering.",
                    "label": 0
                },
                {
                    "sent": "With respect to the initial points.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "It's pretty good.",
                    "label": 0
                },
                {
                    "sent": "Now of course, I've got to make the picture, so I get to claim and make the picture so that it's pretty good, but we can again prove something nontrivial here that this is not going to lose you too much as you do this.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What happens the main question is when we approximate.",
                    "label": 1
                },
                {
                    "sent": "Need approximation so we took our data set.",
                    "label": 0
                },
                {
                    "sent": "We took each one of the partitions we sort of sketched it or approximated it by the clustering and now we are approximating and clustering on these approximations.",
                    "label": 0
                },
                {
                    "sent": "So say we have two different qualities, so there's a beta in the gamma.",
                    "label": 0
                },
                {
                    "sent": "There is a theorem.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That's been reproved recently that says, you know, there's some way to combine these metrics.",
                    "label": 0
                },
                {
                    "sent": "And basically they multiply, so there's a gamma times beta.",
                    "label": 0
                },
                {
                    "sent": "So everything else, there's some order terms, but essentially they multiply.",
                    "label": 0
                },
                {
                    "sent": "So if you were to do this three times, if your data is so large that you need to do this recursively little bit more, you would multiply them three times.",
                    "label": 0
                },
                {
                    "sent": "If you do it twice, you just get a multiplication.",
                    "label": 0
                },
                {
                    "sent": "OK, great and now we know.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Kind of the running time scale, so the first running time gets reduced by M. Because we have M different machines to do this and the second phase running time they N the number of points gets replaced by MK because we had K clusterings on each of the machines, so that's our size of our data set as the input to the second problem.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we use again, K means plus plus or something along those lines, then this gives us a log squared.",
                    "label": 0
                },
                {
                    "sent": "So we lost the log K in each one of these.",
                    "label": 0
                },
                {
                    "sent": "If we didn't make any data assumptions.",
                    "label": 0
                },
                {
                    "sent": "So this gives us a log squared overall.",
                    "label": 0
                },
                {
                    "sent": "Which is good, but can we do better?",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "To improve the approximation guarantee to make even better, we can say, well, we have this leeway.",
                    "label": 1
                },
                {
                    "sent": "In the end, we know that we have to find K different cluster centres, but when we're sketching when we're summarizing the intermediate partitions, nobody enforces the fact that we need to summarize them by using K centers in each summary.",
                    "label": 0
                },
                {
                    "sent": "What if we use more centers in each summary?",
                    "label": 0
                },
                {
                    "sent": "So instead of summarizing things by 10 centers, I get to summarize things using 50 centers, right?",
                    "label": 0
                },
                {
                    "sent": "That's still way smaller than my data.",
                    "label": 0
                },
                {
                    "sent": "My data might be on the size of 10s of thousands.",
                    "label": 0
                },
                {
                    "sent": "100 occasions I'm going to summarize it instead of 10 points.",
                    "label": 0
                },
                {
                    "sent": "50 points.",
                    "label": 0
                },
                {
                    "sent": "Can't be worse, right?",
                    "label": 0
                },
                {
                    "sent": "It should be only better and it is.",
                    "label": 0
                }
            ]
        },
        "clip_113": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So there is.",
                    "label": 0
                },
                {
                    "sent": "Another result that says run the same initialization, but instead of stopping it at the end of K points, keep on running it.",
                    "label": 0
                },
                {
                    "sent": "So run it for another K steps or maybe another 5K steps.",
                    "label": 0
                },
                {
                    "sent": "So in the end you get 6K cluster centers.",
                    "label": 0
                },
                {
                    "sent": "But now these 6K cluster centers give you a constant approximation to the overall, so the sketch their quality of this summary of your data becomes much better.",
                    "label": 0
                },
                {
                    "sent": "So before we said well, if you want to summarize your data using K points, you get log K. Now we're saying, well, if you want to summarize your data using more points, well, it's not surprising that you get a better solution.",
                    "label": 0
                },
                {
                    "sent": "What's nice is that you need only a constant factor more, and I think in the theorem this constant is 8.",
                    "label": 0
                },
                {
                    "sent": "In reality, kind of achoo suffices.",
                    "label": 0
                },
                {
                    "sent": "So just run it for 2K centers and just summarize things using twice as many data points.",
                    "label": 0
                }
            ]
        },
        "clip_114": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now here's the final way.",
                    "label": 0
                },
                {
                    "sent": "Well, we split this again, roughly equal size just for it.",
                    "label": 0
                }
            ]
        },
        "clip_115": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Agency, we're going to find a clustering using L more than K centers and each one.",
                    "label": 1
                },
                {
                    "sent": "This is our summarization step.",
                    "label": 0
                },
                {
                    "sent": "And then we're going to cluster the clusters.",
                    "label": 0
                },
                {
                    "sent": "So we're going to take this going to cluster them.",
                    "label": 0
                },
                {
                    "sent": "It's going to be some set of LM points and it's OK now use K means with the appropriate weights.",
                    "label": 0
                }
            ]
        },
        "clip_116": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Anne.",
                    "label": 0
                },
                {
                    "sent": "We know that it gives us approximation that this should be now that I'm slightly wrong.",
                    "label": 0
                }
            ]
        },
        "clip_117": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, So what is the summary kind of of this overall section.",
                    "label": 0
                },
                {
                    "sent": "So it used to be that K means was one of these things that was very widely used and studied in practice, right?",
                    "label": 1
                },
                {
                    "sent": "As I said, there are many papers and people said this is we need to focus on the initialization or there's some postprocessing.",
                    "label": 0
                },
                {
                    "sent": "K means is really good.",
                    "label": 0
                },
                {
                    "sent": "It's really fast.",
                    "label": 0
                },
                {
                    "sent": "That's the main approach for it.",
                    "label": 0
                },
                {
                    "sent": "You usually clustering is usually some intermediate step in whatever application you're doing.",
                    "label": 0
                },
                {
                    "sent": "And here we said, well, yes, it's an intermediate fast.",
                    "label": 0
                },
                {
                    "sent": "Let's do it really quickly.",
                    "label": 0
                },
                {
                    "sent": "Let's use K means run at 10 different times different random seeds, which is going to give us a result great.",
                    "label": 0
                },
                {
                    "sent": "Right and the theory community were saying well, he means well, here's an example where does badly.",
                    "label": 0
                },
                {
                    "sent": "Here's an example where it does badly.",
                    "label": 0
                },
                {
                    "sent": "No, it's a bad algorithm.",
                    "label": 0
                },
                {
                    "sent": "You should never use it, and more over here you know it runs in exponential time.",
                    "label": 1
                },
                {
                    "sent": "Why would you ever use an algorithm that a runs in exponential time and B gives you a horrible horrible approximations?",
                    "label": 0
                },
                {
                    "sent": "And then there's this huge disconnect.",
                    "label": 0
                },
                {
                    "sent": "That said, one side said key means great, we love you and others say K means you know we should have come up with something better.",
                    "label": 0
                },
                {
                    "sent": "We've had 50 years to do it.",
                    "label": 0
                },
                {
                    "sent": "Now I want to claim that we've sort of merge these two and brought him back together and we said, well, yes.",
                    "label": 1
                },
                {
                    "sent": "It does run in worst case exponential time, but in reality it doesn't and we can actually show this formally.",
                    "label": 0
                },
                {
                    "sent": "So that objection goes away.",
                    "label": 0
                },
                {
                    "sent": "On the flip side, yes, K means by itself does leave you lead you to potentially really bad solutions, and the reason that we maybe didn't know that quite as well is because there was nothing that did better.",
                    "label": 0
                },
                {
                    "sent": "Some of these heuristics work, but not all the time, not consistently, etc.",
                    "label": 0
                },
                {
                    "sent": "And so we said well here the main point is initialization.",
                    "label": 0
                },
                {
                    "sent": "Well, let's initialize not just completely randomly and not completely using this for this point heuristic, but somewhere in between somewhere proportional to the error that we're going after and look loan.",
                    "label": 0
                },
                {
                    "sent": "Behold, this satisfies the theory people because now we can say hey, this gives us an approximation guarantee and this is a constant approximation of the data is good.",
                    "label": 1
                },
                {
                    "sent": "It's a lot K approximation of the data is not so good.",
                    "label": 0
                },
                {
                    "sent": "But this is a well principled approach to do that, but at the same time it actually works, right?",
                    "label": 0
                },
                {
                    "sent": "It actually beats regular K means, which is the reason why it's so popular 5060 years down the line is because it's hard to beat.",
                    "label": 0
                },
                {
                    "sent": "There's K means, but it's it really works on any data you throw at it.",
                    "label": 0
                },
                {
                    "sent": "It don't need to do any assumptions, so it's kind of hard to improve upon, so we have these two sides, and hopefully I think now these two, the two stories the two roads have merged again and sort of.",
                    "label": 0
                },
                {
                    "sent": "We know a lot more about K means, and then the next section stress is going to talk about now using the same intuition now to cluster harder metrics 'cause our points don't always lie in just 234 some dimensional space.",
                    "label": 0
                },
                {
                    "sent": "We want to cluster distribution which we want to look at mutual information with attention.",
                    "label": 0
                },
                {
                    "sent": "People want to do all sorts of crazy things and how to.",
                    "label": 0
                },
                {
                    "sent": "How can we take the insights that we learned in Euclidean spaces which we can picture and look kind of nice and translate them somewhere else?",
                    "label": 0
                },
                {
                    "sent": "OK, so that's going to be the continuum.",
                    "label": 0
                }
            ]
        },
        "clip_118": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I think we'll take a 10 minute break.",
                    "label": 0
                },
                {
                    "sent": "Ten 1515 minute break.",
                    "label": 0
                },
                {
                    "sent": "I'll be around here, so if you have any questions you can ask him using the mic or you can come up afterwards and then we'll continue on with this.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}