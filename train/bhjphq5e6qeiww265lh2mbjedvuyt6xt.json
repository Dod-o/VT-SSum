{
    "id": "bhjphq5e6qeiww265lh2mbjedvuyt6xt",
    "title": "LODifier: Generating Linked Data from Unstructured Text",
    "info": {
        "author": [
            "Isabelle Augenstein, Department of Computer Science, University of Sheffield"
        ],
        "published": "July 4, 2012",
        "recorded": "May 2012",
        "category": [
            "Top->Computer Science->Machine Learning->Structured Data",
            "Top->Computer Science->Semantic Web",
            "Top->Computer Science->Computational Linguistics",
            "Top->Computer Science->Semantic Web->Ontologies",
            "Top->Computer Science->Semantic Web->RDF - Resource Description Framework"
        ]
    },
    "url": "http://videolectures.net/eswc2012_augenstein_lodifier/",
    "segmentation": [
        [
            "So today I'm talking about I want to talk about modifier which is a tool for generating link data from unstructured text.",
            "My talk."
        ],
        [
            "Structure like this.",
            "First I want to tell you about our motivation.",
            "Then go into detail about the modifier architecture and workflow.",
            "Then tell you about the evaluation in the document similarity task and.",
            "Then summarize the talk.",
            "OK, So what we do is."
        ],
        [
            "We want to represent information from natural language text as linked data.",
            "But in order to do that, we first have to create a graph based representation for unstructured plain text.",
            "And while this task is quite easy, if the text is already structured, for example in a database, it is challenging if we deal with unstructured text.",
            "So there are already other approaches that deal with this kind of problem, but usually they're very selective, which means they only extract some predefined relations.",
            "For example, is CEO of relations and what our approach does is it translates the text in its entire G into RDF.",
            "So there are possible use cases.",
            "We see there and we think that domain independent scenarios in which no predefined schema is available would be some use cases.",
            "OK, now in order to do this we use."
        ],
        [
            "Noise tolerant NLP pipeline to analyze the text and later translate the results into an RDF representation.",
            "We also embed our output into the NLP into the LD cloud by using vocabulary from the pedia and Word net."
        ],
        [
            "Here you can see an overview of our pipeline, the most important parts of the pipeline are parsing and deep semantic analysis, because this creates the basic structure of the output document.",
            "Then we also have named entity recognition and which senses inviation to get links to the two LD Cloud.",
            "And then there's tokenization and lemmatization.",
            "Just as pre processing steps.",
            "And after all these steps, the output of the different tools is integrated and RDF is generated.",
            "OK, now the first tool I want to talk about is Wiki Fire."
        ],
        [
            "Wiki Fire recognizes named entities and finds Wikipedia links for them.",
            "So this is pretty convenient for us because we can just take the Wikipedia URLs and convert them to DB pedia, your eyes.",
            "On the slide you can also see an example which I will also use another slide, so we'll just read it out.",
            "The New York Times reported that John McCarthy died.",
            "He invented the programming language Lisp.",
            "An in the bottom you can see the weekly fire output.",
            "Next tool is called U KB and is used for word sense immigration.",
            "UWP is an unsupervised graph base."
        ],
        [
            "Burton simulation tool and it finds word Netlinx forward senses so we can also use this to get RDF Wordnet your eyes to link to the Elodie Cloud.",
            "Now about the most important steps, they're parsing and semantic analysis."
        ],
        [
            "We parse our text with a CNC parser, which is a statistical parser that uses the combined category grammar.",
            "The combined category grammar or CCG is semantically motivated grammar and we can later use this to analyze the text.",
            "So what we do is we parse the text and we use this parsing result as input for Boxer.",
            "What Boxer does is it produces discourse representation structures.",
            "These are quite semantically deep models which model the meaning of texts in terms of relevant entities and relations between them.",
            "I will now just go into detail about the output because I assume you most of you don't know discourse representation structures."
        ],
        [
            "So in the top you can see the discourse reference or the relevant entities and in the bottom you can see the relations.",
            "There are different kinds of relations there.",
            "Predicate relations, for example X zero is male and there also relations between a number of entities.",
            "For example, X4 is the agent of X0.",
            "And then we also have embedded statements, for example X6 which says there is an event X7 which is die and the agent of this event is X0.",
            "So it's kind of hard to read at first.",
            "But what we then do is to converge all these discourse reference into blank nodes and all these relations into RDF so.",
            "Yeah, and what you also see is that we have different semantic roles like agent and patient.",
            "So we really have a very deep semantic model of the text.",
            "Now this.",
            "Is the RDF output for the test sentences.",
            "I will now just."
        ],
        [
            "Go through step by step."
        ],
        [
            "The first piece of information that we have here is that we have a person called John McCarthy, which is male and we also have the DP at link for John McCarthy.",
            "And we have a pro."
        ],
        [
            "Gramming language.",
            "We have"
        ],
        [
            "Noun phrase between a node one and node two that says programming language Lisp."
        ],
        [
            "Then we have the information about Lisp."
        ],
        [
            "The New York Times and then we have several events.",
            "The 1st."
        ],
        [
            "1st event is invent between the Agent Zero and the patient two.",
            "So we have John McCarthy invented Lisp.",
            "The next event is report, and it's between node."
        ],
        [
            "Three and six, so it says the New York Times reported something, and this is the last piece of information this is."
        ],
        [
            "And the embedded statement that we saw earlier.",
            "So what we do is we used rification to model that.",
            "So last piece of information is that we have we have an event which is Diane.",
            "The agent is not zero so it says John McCarthy died.",
            "OK, and now that I presented you the RDF output, I would also like to tell you about our evaluation so."
        ],
        [
            "Can see how we use modifier in the possible scenario.",
            "So what we do is we use modifier to assess document similarity.",
            "This is also called a story link detection task which is part of the topic detection and tracking family of tasks.",
            "So there is also a data set for that which is called TDT.",
            "It's a benchmark data set and it consists of newspaper TV and radio news in English, Arabic and Mandarin.",
            "And since modify only works for English as a language, we use this as a subset and we first restricted ourselves to newspaper articles to make sure we don't get.",
            "We don't have any noisy data.",
            "So All in all, we process 183 positive and negative documented pairs, so 183 document pairs which have the same topic.",
            "An 183 document pairs which don't have the same topic."
        ],
        [
            "Our method for this is to define various document similarity measures.",
            "We use measures with and without structural information from the RDF document.",
            "The documents are then predicted to have the same topic if they have a similarity of theater or more and Theta is a parameter which we determine with supervised learning.",
            "Now first I want to talk about the similarity measures that don't use structural information.",
            "We have a random baseline, a bag of words baseline."
        ],
        [
            "And the back of your eyes baseline.",
            "The random baseline is 50% since today's head is balanced.",
            "Then we have a bag of words baseline which measures the overlap between the two documents and document pair and the back of your eyes baseline which does the same for your eyes.",
            "Then four."
        ],
        [
            "The UI baseline and for all for all.",
            "Measures that use that are structured.",
            "We have different UI brians.",
            "Variance so they what they do is they take the relative importance of failures, your eye classes into account.",
            "The first one is we use all named entities that are identified by Wiki Fire an all words that are successfully disambiguated by UK be so there are also other named entities that are recognized by wiki, Fire and some words that couldn't be disseminated.",
            "The next step is to Add all named entities that are recognized by Boxer.",
            "Boxer also has a built-in named entity recognizer.",
            "And the third one is to further Add all your eyes for all words that were not recognized by wiki, Fire, KB, or Boxer.",
            "As I said before.",
            "So these are basically the other nouns, verbs, adjectives and adverbs."
        ],
        [
            "Now we also have an extended setting.",
            "The extended setting aims at drawing more information from the LG Cloud into the similarity computation.",
            "So what we do is we enrich the RDF graph by DPR categories and also worked at Sunset, San word net sense related to the UI's in the generated graph.",
            "Not the similarity measures that do you?"
        ],
        [
            "Structural information we first experimented with full fledged graph similarity measures.",
            "For example those based on isomorphic subgraphs, but it turned out that they were infeasible to the size of the RDF graph, so the computation time was just too high.",
            "What we then can we then came up with is to use similarity measures that are based on the shortest paths between relevant nodes and our intuition for that is the shortest shortest paths between your eye pairs denote relevant semantic information, and we expect that they are related by a short path and other documents as well.",
            "So this is how our similarity similarity measures with structural."
        ],
        [
            "Formation look like we call them prosim, which means path relevance, overlap similarity.",
            "And what they measure is.",
            "The relevant the overlap between elements, relevant documents in one document document and the other documents.",
            "So the denominator measures the relevant paths in document one and the numerator.",
            "The relevant paths in the intersection of relevant paths between document one and Document 2.",
            "And we also have different parameters.",
            "The first one is path length, so we thought that maybe not all kinds of paths are relevant, just the ones with a certain length.",
            "So we started experimenting with path lengths 6 to 12.",
            "Then we also have different kinds of semantic relations, and this is what I talked about earlier.",
            "The different your eye class variance.",
            "And then we have different kinds of similarity functions.",
            "They just basically they just weigh the paths differently.",
            "So the first one just counts the number of paths irrespective for length.",
            "And the second one gives less way to longer paths, and the third one as well.",
            "Now we can have a look at our at our results.",
            "So in the top we see the similarity measures that don't use."
        ],
        [
            "Structural information we first have the random baseline that is as expected, 50%.",
            "Then we have the bag of words on the back of your eyes baseline and you can see that we already have a huge improvement from the random baseline to the back of your eyes baseline.",
            "And you can also see that the similarity measures that don't use structural information benefit from the extended setting.",
            "When we now have a look at the similarity measures that do use structural information, we can see that there's no big difference between the normal and the extended settings.",
            "So if we add more.",
            "More data from the LG Cloud.",
            "It doesn't help any further.",
            "And we also see that we get the best results if we just use one as a weight.",
            "OK, and then we can also see that between the three different UI versions, the third one is the best.",
            "So if we take more information it's apparently better.",
            "And we can also see that the path length six or eight is the best, depending on which kind of model we use.",
            "So I'm I'm now just going to sum up my."
        ],
        [
            "Presentation.",
            "What are main contributions are?",
            "Are we think that we provide a service for the LG community that can be used in different scenarios that can profit from structure representation of text?",
            "What we did is we combined well established concepts and systems in a deep semantic pipeline and we also linked these NLP technologies to the LD cloud.",
            "We also performed an evaluation of Lada Fire in the topic link detection tasks task that gives clear evidence of its future potential.",
            "We also see possible future work directions, so I told you earlier what we do is we use a pipeline approach.",
            "That's not the best possible approach.",
            "It's usually better to simultaneously consider the information from all the components to allow interaction between them.",
            "So maybe we'll try that.",
            "And what we will definitely do is test modifier in different scenarios.",
            "So not just story link detection task but also other scenarios."
        ],
        [
            "OK, then thank you very much for your attention and feel free to ask questions.",
            "You evaluated your the performance of the effectiveness of the whole approach in a specific task.",
            "Yes, I would expect that you would do some kind of evaluation in a task independent form.",
            "I mean the the task is to transform an structured text to link data based on the ontology.",
            "So it makes sense to to measure how many of the detective relations are actually correct and how many.",
            "An accident is actually correct.",
            "Do you have any numbers from that you perform any kinds of these tests?",
            "No we didn't, but then you would have to ask humans to like look at the graphs and tell which ones are correct and which are not.",
            "Yeah, sampling of that for example to check for example.",
            "Neither seem to know what is the average accuracy of boxer.",
            "For example of this component or with fire could use a lot of 3rd party components.",
            "So if someone wants to use this approach in a practical application, it's good to know.",
            "Starts to say they probably effectiveness in general.",
            "I think.",
            "I just have a quick question about the pro SIM similarity measure, so it seems to me that you need to compute it.",
            "You need to be able to compute distances between any pairs of resources in the."
        ],
        [
            "Very large graph.",
            "So yeah, I was just wondering what did you try that at scale?",
            "Did it work?",
            "Yeah, so just wondering how scalable it is ready.",
            "OK, so how how huge the graphs are?",
            "Or yeah, I think the short largest graph was up was up to 19,000 nodes so it was pretty big and yeah also.",
            "And took quite some time to compute.",
            "The shortest path.",
            "So if you want to have just something that is not that computationally heavy, you would take the UI overlap baseline.",
            "So this could be computed on the fly and you know used as a demo or whatever.",
            "No."
        ],
        [
            "So one comment on the paper.",
            "I think it is.",
            "I agree it's great to see that.",
            "There has been over the years sufficient progress in the NLP community so that these things are becoming possible.",
            "So in that sense, I agree that the that the actual evaluation you know doesn't reflect actually the power of the representation that you're generating.",
            "It's good to see that there's also some progress there, but I think the main point is that you have now these very deep representations of language, and, for example, I think one very nice way forward for me would be to see can you now combine this with something like?",
            "I don't know.",
            "I will inferencing for example.",
            "To even show that with these representations you can get even more out of the text that is there.",
            "So can you do?",
            "I don't know question answering over these representations and even answer to questions that are not explicitly in the text so that that will be for me something that will be the non plus ultra.",
            "Let's say that would ultimately prove that you can use these deep representations with inferencing to to get much more than than than actually is in the source.",
            "So that's just a suggestion.",
            "I know it's not easy, but there are.",
            "There are some.",
            "I don't question, I mean, question answering is also something that is done, typically not based on such representations, but on much more shallower techniques.",
            "So if one could show that with these techniques, Now we're able to even be better than more shallow techniques.",
            "I think that would be a good proof of concept for for the for the work.",
            "I was actually thinking about doing that, but it's yeah you need quite some time to develop such a model, but there's a good chance that we'll do that in future work.",
            "One question, do you also try to match also the relations between the entities that you extract to the vocabulary of DB pedia so that no no.",
            "But that's quite impossible because the relations in DB Pedia are not really documented.",
            "So you could just do something like yeah matching pattern pattern matching.",
            "I know if they have the same name they might be the same relation but.",
            "I don't know.",
            "I don't think I don't think it's a really good approach, because if you if you just use the idea of Fortnite, we have much more fine grained relations.",
            "So.",
            "I personally don't see any point in matching this to DB pedia.",
            "So what So what do you think is the relation between these approach that kind of produce very fine grained representation in RDF of the data with respect to other relation extraction approaches like the it's your knee or the one from vacuum.",
            "That kind of approaches that actually tried to extract.",
            "Add the relation from text to build like the linked open data, right?",
            "Is it?",
            "Because I don't have, I would like to see the the graphs and it would be interesting to see if those graphs can be plugged into their link and open data or they are too complex.",
            "Yeah, So what relation extraction systems usually do?",
            "Is they really just extract certain types of relations and what we do is we really translate the whole text into RDF, so this is kind of a difference.",
            "Just a suggestion for the evaluation problem.",
            "I mean there are tools that turn RDF into text that work morale as well.",
            "So what you could do is you could take some RDF, run some of those tools and get some text, and then run your tool and get RDF back and then compare what.",
            "How much is the difference?",
            "So I mean, this is not a perfect evaluation, but it would give you at least a hint of how good that really works.",
            "And you don't need humans for that.",
            "When does the comment on the question of DB pedia?",
            "I mean you said it would not make sense to to map the DB pedia?",
            "Well, I would.",
            "I would tend to disagree.",
            "Yeah, well, so two relations or concepts or whatever in indep or I mean let's let's not talk about the peer, but I mean any other data set.",
            "Any other ontology out there.",
            "I'm I'm coming to my earlier question.",
            "I mean, if you have a number of calories that are suitably axiomatized, I mean by doing this you will get also a number of inferences.",
            "So in some ways it makes sense to to map to this first for the question of interoperability.",
            "So if you map DB pedia then other people will be able to reuse that that that data better than if you just have your proper Terry Boxer specific vocabulary.",
            "So that's the first issue and.",
            "And the second was you, can you can more inferences because you map to some vocabularies that are already properly axiomatized, for example.",
            "And the Third Point is DB pedia.",
            "We found that is in many cases much more detailed than word NET.",
            "So you have even more specific sense distinction.",
            "Sometimes in something that Peter then then then in Word net.",
            "Yeah, but if you have a look at the relations in DB Pedia, sometimes they are missing because they are generated from the tables and the info boxes in Wikipedia.",
            "So it's.",
            "Yeah, not all of them.",
            "I mean you couldn't map it because but then you would.",
            "You would lose the fine grayness of word, net.",
            "I mean, it's just a matter of how you want to represent your relations.",
            "But yeah, I agree, you could do it, but I mean, personally, I wouldn't.",
            "I wouldn't like to do it, just yeah."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So today I'm talking about I want to talk about modifier which is a tool for generating link data from unstructured text.",
                    "label": 0
                },
                {
                    "sent": "My talk.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Structure like this.",
                    "label": 0
                },
                {
                    "sent": "First I want to tell you about our motivation.",
                    "label": 0
                },
                {
                    "sent": "Then go into detail about the modifier architecture and workflow.",
                    "label": 1
                },
                {
                    "sent": "Then tell you about the evaluation in the document similarity task and.",
                    "label": 1
                },
                {
                    "sent": "Then summarize the talk.",
                    "label": 0
                },
                {
                    "sent": "OK, So what we do is.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We want to represent information from natural language text as linked data.",
                    "label": 1
                },
                {
                    "sent": "But in order to do that, we first have to create a graph based representation for unstructured plain text.",
                    "label": 0
                },
                {
                    "sent": "And while this task is quite easy, if the text is already structured, for example in a database, it is challenging if we deal with unstructured text.",
                    "label": 1
                },
                {
                    "sent": "So there are already other approaches that deal with this kind of problem, but usually they're very selective, which means they only extract some predefined relations.",
                    "label": 1
                },
                {
                    "sent": "For example, is CEO of relations and what our approach does is it translates the text in its entire G into RDF.",
                    "label": 0
                },
                {
                    "sent": "So there are possible use cases.",
                    "label": 0
                },
                {
                    "sent": "We see there and we think that domain independent scenarios in which no predefined schema is available would be some use cases.",
                    "label": 1
                },
                {
                    "sent": "OK, now in order to do this we use.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Noise tolerant NLP pipeline to analyze the text and later translate the results into an RDF representation.",
                    "label": 0
                },
                {
                    "sent": "We also embed our output into the NLP into the LD cloud by using vocabulary from the pedia and Word net.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Here you can see an overview of our pipeline, the most important parts of the pipeline are parsing and deep semantic analysis, because this creates the basic structure of the output document.",
                    "label": 1
                },
                {
                    "sent": "Then we also have named entity recognition and which senses inviation to get links to the two LD Cloud.",
                    "label": 1
                },
                {
                    "sent": "And then there's tokenization and lemmatization.",
                    "label": 0
                },
                {
                    "sent": "Just as pre processing steps.",
                    "label": 0
                },
                {
                    "sent": "And after all these steps, the output of the different tools is integrated and RDF is generated.",
                    "label": 0
                },
                {
                    "sent": "OK, now the first tool I want to talk about is Wiki Fire.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Wiki Fire recognizes named entities and finds Wikipedia links for them.",
                    "label": 0
                },
                {
                    "sent": "So this is pretty convenient for us because we can just take the Wikipedia URLs and convert them to DB pedia, your eyes.",
                    "label": 0
                },
                {
                    "sent": "On the slide you can also see an example which I will also use another slide, so we'll just read it out.",
                    "label": 0
                },
                {
                    "sent": "The New York Times reported that John McCarthy died.",
                    "label": 1
                },
                {
                    "sent": "He invented the programming language Lisp.",
                    "label": 0
                },
                {
                    "sent": "An in the bottom you can see the weekly fire output.",
                    "label": 0
                },
                {
                    "sent": "Next tool is called U KB and is used for word sense immigration.",
                    "label": 0
                },
                {
                    "sent": "UWP is an unsupervised graph base.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Burton simulation tool and it finds word Netlinx forward senses so we can also use this to get RDF Wordnet your eyes to link to the Elodie Cloud.",
                    "label": 0
                },
                {
                    "sent": "Now about the most important steps, they're parsing and semantic analysis.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We parse our text with a CNC parser, which is a statistical parser that uses the combined category grammar.",
                    "label": 1
                },
                {
                    "sent": "The combined category grammar or CCG is semantically motivated grammar and we can later use this to analyze the text.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we parse the text and we use this parsing result as input for Boxer.",
                    "label": 1
                },
                {
                    "sent": "What Boxer does is it produces discourse representation structures.",
                    "label": 0
                },
                {
                    "sent": "These are quite semantically deep models which model the meaning of texts in terms of relevant entities and relations between them.",
                    "label": 1
                },
                {
                    "sent": "I will now just go into detail about the output because I assume you most of you don't know discourse representation structures.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the top you can see the discourse reference or the relevant entities and in the bottom you can see the relations.",
                    "label": 0
                },
                {
                    "sent": "There are different kinds of relations there.",
                    "label": 0
                },
                {
                    "sent": "Predicate relations, for example X zero is male and there also relations between a number of entities.",
                    "label": 0
                },
                {
                    "sent": "For example, X4 is the agent of X0.",
                    "label": 0
                },
                {
                    "sent": "And then we also have embedded statements, for example X6 which says there is an event X7 which is die and the agent of this event is X0.",
                    "label": 0
                },
                {
                    "sent": "So it's kind of hard to read at first.",
                    "label": 0
                },
                {
                    "sent": "But what we then do is to converge all these discourse reference into blank nodes and all these relations into RDF so.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and what you also see is that we have different semantic roles like agent and patient.",
                    "label": 0
                },
                {
                    "sent": "So we really have a very deep semantic model of the text.",
                    "label": 0
                },
                {
                    "sent": "Now this.",
                    "label": 0
                },
                {
                    "sent": "Is the RDF output for the test sentences.",
                    "label": 1
                },
                {
                    "sent": "I will now just.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Go through step by step.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first piece of information that we have here is that we have a person called John McCarthy, which is male and we also have the DP at link for John McCarthy.",
                    "label": 0
                },
                {
                    "sent": "And we have a pro.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gramming language.",
                    "label": 0
                },
                {
                    "sent": "We have",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Noun phrase between a node one and node two that says programming language Lisp.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we have the information about Lisp.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The New York Times and then we have several events.",
                    "label": 0
                },
                {
                    "sent": "The 1st.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1st event is invent between the Agent Zero and the patient two.",
                    "label": 0
                },
                {
                    "sent": "So we have John McCarthy invented Lisp.",
                    "label": 0
                },
                {
                    "sent": "The next event is report, and it's between node.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Three and six, so it says the New York Times reported something, and this is the last piece of information this is.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the embedded statement that we saw earlier.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we used rification to model that.",
                    "label": 0
                },
                {
                    "sent": "So last piece of information is that we have we have an event which is Diane.",
                    "label": 0
                },
                {
                    "sent": "The agent is not zero so it says John McCarthy died.",
                    "label": 0
                },
                {
                    "sent": "OK, and now that I presented you the RDF output, I would also like to tell you about our evaluation so.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Can see how we use modifier in the possible scenario.",
                    "label": 0
                },
                {
                    "sent": "So what we do is we use modifier to assess document similarity.",
                    "label": 1
                },
                {
                    "sent": "This is also called a story link detection task which is part of the topic detection and tracking family of tasks.",
                    "label": 1
                },
                {
                    "sent": "So there is also a data set for that which is called TDT.",
                    "label": 0
                },
                {
                    "sent": "It's a benchmark data set and it consists of newspaper TV and radio news in English, Arabic and Mandarin.",
                    "label": 1
                },
                {
                    "sent": "And since modify only works for English as a language, we use this as a subset and we first restricted ourselves to newspaper articles to make sure we don't get.",
                    "label": 1
                },
                {
                    "sent": "We don't have any noisy data.",
                    "label": 0
                },
                {
                    "sent": "So All in all, we process 183 positive and negative documented pairs, so 183 document pairs which have the same topic.",
                    "label": 0
                },
                {
                    "sent": "An 183 document pairs which don't have the same topic.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our method for this is to define various document similarity measures.",
                    "label": 1
                },
                {
                    "sent": "We use measures with and without structural information from the RDF document.",
                    "label": 0
                },
                {
                    "sent": "The documents are then predicted to have the same topic if they have a similarity of theater or more and Theta is a parameter which we determine with supervised learning.",
                    "label": 1
                },
                {
                    "sent": "Now first I want to talk about the similarity measures that don't use structural information.",
                    "label": 0
                },
                {
                    "sent": "We have a random baseline, a bag of words baseline.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the back of your eyes baseline.",
                    "label": 0
                },
                {
                    "sent": "The random baseline is 50% since today's head is balanced.",
                    "label": 1
                },
                {
                    "sent": "Then we have a bag of words baseline which measures the overlap between the two documents and document pair and the back of your eyes baseline which does the same for your eyes.",
                    "label": 1
                },
                {
                    "sent": "Then four.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The UI baseline and for all for all.",
                    "label": 0
                },
                {
                    "sent": "Measures that use that are structured.",
                    "label": 0
                },
                {
                    "sent": "We have different UI brians.",
                    "label": 0
                },
                {
                    "sent": "Variance so they what they do is they take the relative importance of failures, your eye classes into account.",
                    "label": 1
                },
                {
                    "sent": "The first one is we use all named entities that are identified by Wiki Fire an all words that are successfully disambiguated by UK be so there are also other named entities that are recognized by wiki, Fire and some words that couldn't be disseminated.",
                    "label": 1
                },
                {
                    "sent": "The next step is to Add all named entities that are recognized by Boxer.",
                    "label": 0
                },
                {
                    "sent": "Boxer also has a built-in named entity recognizer.",
                    "label": 0
                },
                {
                    "sent": "And the third one is to further Add all your eyes for all words that were not recognized by wiki, Fire, KB, or Boxer.",
                    "label": 1
                },
                {
                    "sent": "As I said before.",
                    "label": 0
                },
                {
                    "sent": "So these are basically the other nouns, verbs, adjectives and adverbs.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now we also have an extended setting.",
                    "label": 0
                },
                {
                    "sent": "The extended setting aims at drawing more information from the LG Cloud into the similarity computation.",
                    "label": 1
                },
                {
                    "sent": "So what we do is we enrich the RDF graph by DPR categories and also worked at Sunset, San word net sense related to the UI's in the generated graph.",
                    "label": 0
                },
                {
                    "sent": "Not the similarity measures that do you?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Structural information we first experimented with full fledged graph similarity measures.",
                    "label": 0
                },
                {
                    "sent": "For example those based on isomorphic subgraphs, but it turned out that they were infeasible to the size of the RDF graph, so the computation time was just too high.",
                    "label": 1
                },
                {
                    "sent": "What we then can we then came up with is to use similarity measures that are based on the shortest paths between relevant nodes and our intuition for that is the shortest shortest paths between your eye pairs denote relevant semantic information, and we expect that they are related by a short path and other documents as well.",
                    "label": 1
                },
                {
                    "sent": "So this is how our similarity similarity measures with structural.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Formation look like we call them prosim, which means path relevance, overlap similarity.",
                    "label": 0
                },
                {
                    "sent": "And what they measure is.",
                    "label": 0
                },
                {
                    "sent": "The relevant the overlap between elements, relevant documents in one document document and the other documents.",
                    "label": 0
                },
                {
                    "sent": "So the denominator measures the relevant paths in document one and the numerator.",
                    "label": 0
                },
                {
                    "sent": "The relevant paths in the intersection of relevant paths between document one and Document 2.",
                    "label": 0
                },
                {
                    "sent": "And we also have different parameters.",
                    "label": 0
                },
                {
                    "sent": "The first one is path length, so we thought that maybe not all kinds of paths are relevant, just the ones with a certain length.",
                    "label": 0
                },
                {
                    "sent": "So we started experimenting with path lengths 6 to 12.",
                    "label": 0
                },
                {
                    "sent": "Then we also have different kinds of semantic relations, and this is what I talked about earlier.",
                    "label": 0
                },
                {
                    "sent": "The different your eye class variance.",
                    "label": 0
                },
                {
                    "sent": "And then we have different kinds of similarity functions.",
                    "label": 0
                },
                {
                    "sent": "They just basically they just weigh the paths differently.",
                    "label": 0
                },
                {
                    "sent": "So the first one just counts the number of paths irrespective for length.",
                    "label": 1
                },
                {
                    "sent": "And the second one gives less way to longer paths, and the third one as well.",
                    "label": 1
                },
                {
                    "sent": "Now we can have a look at our at our results.",
                    "label": 0
                },
                {
                    "sent": "So in the top we see the similarity measures that don't use.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Structural information we first have the random baseline that is as expected, 50%.",
                    "label": 0
                },
                {
                    "sent": "Then we have the bag of words on the back of your eyes baseline and you can see that we already have a huge improvement from the random baseline to the back of your eyes baseline.",
                    "label": 1
                },
                {
                    "sent": "And you can also see that the similarity measures that don't use structural information benefit from the extended setting.",
                    "label": 0
                },
                {
                    "sent": "When we now have a look at the similarity measures that do use structural information, we can see that there's no big difference between the normal and the extended settings.",
                    "label": 0
                },
                {
                    "sent": "So if we add more.",
                    "label": 0
                },
                {
                    "sent": "More data from the LG Cloud.",
                    "label": 0
                },
                {
                    "sent": "It doesn't help any further.",
                    "label": 0
                },
                {
                    "sent": "And we also see that we get the best results if we just use one as a weight.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we can also see that between the three different UI versions, the third one is the best.",
                    "label": 0
                },
                {
                    "sent": "So if we take more information it's apparently better.",
                    "label": 0
                },
                {
                    "sent": "And we can also see that the path length six or eight is the best, depending on which kind of model we use.",
                    "label": 0
                },
                {
                    "sent": "So I'm I'm now just going to sum up my.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Presentation.",
                    "label": 0
                },
                {
                    "sent": "What are main contributions are?",
                    "label": 0
                },
                {
                    "sent": "Are we think that we provide a service for the LG community that can be used in different scenarios that can profit from structure representation of text?",
                    "label": 1
                },
                {
                    "sent": "What we did is we combined well established concepts and systems in a deep semantic pipeline and we also linked these NLP technologies to the LD cloud.",
                    "label": 0
                },
                {
                    "sent": "We also performed an evaluation of Lada Fire in the topic link detection tasks task that gives clear evidence of its future potential.",
                    "label": 0
                },
                {
                    "sent": "We also see possible future work directions, so I told you earlier what we do is we use a pipeline approach.",
                    "label": 1
                },
                {
                    "sent": "That's not the best possible approach.",
                    "label": 0
                },
                {
                    "sent": "It's usually better to simultaneously consider the information from all the components to allow interaction between them.",
                    "label": 0
                },
                {
                    "sent": "So maybe we'll try that.",
                    "label": 0
                },
                {
                    "sent": "And what we will definitely do is test modifier in different scenarios.",
                    "label": 0
                },
                {
                    "sent": "So not just story link detection task but also other scenarios.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, then thank you very much for your attention and feel free to ask questions.",
                    "label": 1
                },
                {
                    "sent": "You evaluated your the performance of the effectiveness of the whole approach in a specific task.",
                    "label": 0
                },
                {
                    "sent": "Yes, I would expect that you would do some kind of evaluation in a task independent form.",
                    "label": 0
                },
                {
                    "sent": "I mean the the task is to transform an structured text to link data based on the ontology.",
                    "label": 0
                },
                {
                    "sent": "So it makes sense to to measure how many of the detective relations are actually correct and how many.",
                    "label": 0
                },
                {
                    "sent": "An accident is actually correct.",
                    "label": 0
                },
                {
                    "sent": "Do you have any numbers from that you perform any kinds of these tests?",
                    "label": 0
                },
                {
                    "sent": "No we didn't, but then you would have to ask humans to like look at the graphs and tell which ones are correct and which are not.",
                    "label": 0
                },
                {
                    "sent": "Yeah, sampling of that for example to check for example.",
                    "label": 0
                },
                {
                    "sent": "Neither seem to know what is the average accuracy of boxer.",
                    "label": 0
                },
                {
                    "sent": "For example of this component or with fire could use a lot of 3rd party components.",
                    "label": 0
                },
                {
                    "sent": "So if someone wants to use this approach in a practical application, it's good to know.",
                    "label": 0
                },
                {
                    "sent": "Starts to say they probably effectiveness in general.",
                    "label": 0
                },
                {
                    "sent": "I think.",
                    "label": 0
                },
                {
                    "sent": "I just have a quick question about the pro SIM similarity measure, so it seems to me that you need to compute it.",
                    "label": 0
                },
                {
                    "sent": "You need to be able to compute distances between any pairs of resources in the.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very large graph.",
                    "label": 0
                },
                {
                    "sent": "So yeah, I was just wondering what did you try that at scale?",
                    "label": 0
                },
                {
                    "sent": "Did it work?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so just wondering how scalable it is ready.",
                    "label": 0
                },
                {
                    "sent": "OK, so how how huge the graphs are?",
                    "label": 0
                },
                {
                    "sent": "Or yeah, I think the short largest graph was up was up to 19,000 nodes so it was pretty big and yeah also.",
                    "label": 0
                },
                {
                    "sent": "And took quite some time to compute.",
                    "label": 0
                },
                {
                    "sent": "The shortest path.",
                    "label": 0
                },
                {
                    "sent": "So if you want to have just something that is not that computationally heavy, you would take the UI overlap baseline.",
                    "label": 0
                },
                {
                    "sent": "So this could be computed on the fly and you know used as a demo or whatever.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So one comment on the paper.",
                    "label": 0
                },
                {
                    "sent": "I think it is.",
                    "label": 0
                },
                {
                    "sent": "I agree it's great to see that.",
                    "label": 0
                },
                {
                    "sent": "There has been over the years sufficient progress in the NLP community so that these things are becoming possible.",
                    "label": 0
                },
                {
                    "sent": "So in that sense, I agree that the that the actual evaluation you know doesn't reflect actually the power of the representation that you're generating.",
                    "label": 0
                },
                {
                    "sent": "It's good to see that there's also some progress there, but I think the main point is that you have now these very deep representations of language, and, for example, I think one very nice way forward for me would be to see can you now combine this with something like?",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I will inferencing for example.",
                    "label": 0
                },
                {
                    "sent": "To even show that with these representations you can get even more out of the text that is there.",
                    "label": 0
                },
                {
                    "sent": "So can you do?",
                    "label": 0
                },
                {
                    "sent": "I don't know question answering over these representations and even answer to questions that are not explicitly in the text so that that will be for me something that will be the non plus ultra.",
                    "label": 0
                },
                {
                    "sent": "Let's say that would ultimately prove that you can use these deep representations with inferencing to to get much more than than than actually is in the source.",
                    "label": 0
                },
                {
                    "sent": "So that's just a suggestion.",
                    "label": 0
                },
                {
                    "sent": "I know it's not easy, but there are.",
                    "label": 0
                },
                {
                    "sent": "There are some.",
                    "label": 0
                },
                {
                    "sent": "I don't question, I mean, question answering is also something that is done, typically not based on such representations, but on much more shallower techniques.",
                    "label": 0
                },
                {
                    "sent": "So if one could show that with these techniques, Now we're able to even be better than more shallow techniques.",
                    "label": 0
                },
                {
                    "sent": "I think that would be a good proof of concept for for the for the work.",
                    "label": 0
                },
                {
                    "sent": "I was actually thinking about doing that, but it's yeah you need quite some time to develop such a model, but there's a good chance that we'll do that in future work.",
                    "label": 0
                },
                {
                    "sent": "One question, do you also try to match also the relations between the entities that you extract to the vocabulary of DB pedia so that no no.",
                    "label": 0
                },
                {
                    "sent": "But that's quite impossible because the relations in DB Pedia are not really documented.",
                    "label": 0
                },
                {
                    "sent": "So you could just do something like yeah matching pattern pattern matching.",
                    "label": 0
                },
                {
                    "sent": "I know if they have the same name they might be the same relation but.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I don't think I don't think it's a really good approach, because if you if you just use the idea of Fortnite, we have much more fine grained relations.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I personally don't see any point in matching this to DB pedia.",
                    "label": 0
                },
                {
                    "sent": "So what So what do you think is the relation between these approach that kind of produce very fine grained representation in RDF of the data with respect to other relation extraction approaches like the it's your knee or the one from vacuum.",
                    "label": 0
                },
                {
                    "sent": "That kind of approaches that actually tried to extract.",
                    "label": 0
                },
                {
                    "sent": "Add the relation from text to build like the linked open data, right?",
                    "label": 0
                },
                {
                    "sent": "Is it?",
                    "label": 0
                },
                {
                    "sent": "Because I don't have, I would like to see the the graphs and it would be interesting to see if those graphs can be plugged into their link and open data or they are too complex.",
                    "label": 0
                },
                {
                    "sent": "Yeah, So what relation extraction systems usually do?",
                    "label": 0
                },
                {
                    "sent": "Is they really just extract certain types of relations and what we do is we really translate the whole text into RDF, so this is kind of a difference.",
                    "label": 0
                },
                {
                    "sent": "Just a suggestion for the evaluation problem.",
                    "label": 0
                },
                {
                    "sent": "I mean there are tools that turn RDF into text that work morale as well.",
                    "label": 0
                },
                {
                    "sent": "So what you could do is you could take some RDF, run some of those tools and get some text, and then run your tool and get RDF back and then compare what.",
                    "label": 0
                },
                {
                    "sent": "How much is the difference?",
                    "label": 0
                },
                {
                    "sent": "So I mean, this is not a perfect evaluation, but it would give you at least a hint of how good that really works.",
                    "label": 0
                },
                {
                    "sent": "And you don't need humans for that.",
                    "label": 0
                },
                {
                    "sent": "When does the comment on the question of DB pedia?",
                    "label": 0
                },
                {
                    "sent": "I mean you said it would not make sense to to map the DB pedia?",
                    "label": 0
                },
                {
                    "sent": "Well, I would.",
                    "label": 0
                },
                {
                    "sent": "I would tend to disagree.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well, so two relations or concepts or whatever in indep or I mean let's let's not talk about the peer, but I mean any other data set.",
                    "label": 0
                },
                {
                    "sent": "Any other ontology out there.",
                    "label": 0
                },
                {
                    "sent": "I'm I'm coming to my earlier question.",
                    "label": 0
                },
                {
                    "sent": "I mean, if you have a number of calories that are suitably axiomatized, I mean by doing this you will get also a number of inferences.",
                    "label": 0
                },
                {
                    "sent": "So in some ways it makes sense to to map to this first for the question of interoperability.",
                    "label": 0
                },
                {
                    "sent": "So if you map DB pedia then other people will be able to reuse that that that data better than if you just have your proper Terry Boxer specific vocabulary.",
                    "label": 0
                },
                {
                    "sent": "So that's the first issue and.",
                    "label": 0
                },
                {
                    "sent": "And the second was you, can you can more inferences because you map to some vocabularies that are already properly axiomatized, for example.",
                    "label": 0
                },
                {
                    "sent": "And the Third Point is DB pedia.",
                    "label": 0
                },
                {
                    "sent": "We found that is in many cases much more detailed than word NET.",
                    "label": 0
                },
                {
                    "sent": "So you have even more specific sense distinction.",
                    "label": 0
                },
                {
                    "sent": "Sometimes in something that Peter then then then in Word net.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but if you have a look at the relations in DB Pedia, sometimes they are missing because they are generated from the tables and the info boxes in Wikipedia.",
                    "label": 0
                },
                {
                    "sent": "So it's.",
                    "label": 0
                },
                {
                    "sent": "Yeah, not all of them.",
                    "label": 0
                },
                {
                    "sent": "I mean you couldn't map it because but then you would.",
                    "label": 0
                },
                {
                    "sent": "You would lose the fine grayness of word, net.",
                    "label": 0
                },
                {
                    "sent": "I mean, it's just a matter of how you want to represent your relations.",
                    "label": 0
                },
                {
                    "sent": "But yeah, I agree, you could do it, but I mean, personally, I wouldn't.",
                    "label": 0
                },
                {
                    "sent": "I wouldn't like to do it, just yeah.",
                    "label": 0
                }
            ]
        }
    }
}