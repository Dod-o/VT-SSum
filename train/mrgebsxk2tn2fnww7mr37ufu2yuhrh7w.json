{
    "id": "mrgebsxk2tn2fnww7mr37ufu2yuhrh7w",
    "title": "A Factor Model for Learning Higher Order Features in Natural Images",
    "info": {
        "author": [
            "Yan Karklin, Center for Neural Science, New York University (NYU)"
        ],
        "published": "Aug. 26, 2009",
        "recorded": "June 2009",
        "category": [
            "Top->Computer Science->Image Analysis"
        ]
    },
    "url": "http://videolectures.net/icml09_karklin_fmlh/",
    "segmentation": [
        [
            "So first of all, there's saturation in the response of the cell.",
            "So as you increase contrast, the response goes up, but it doesn't quite go up as fast at high contrast.",
            "There's some kind of contrast gain control going on even at this early stage.",
            "Also, as you increase the size of the grading past some optimal size, the response starts to decrease and this decreases depending on the contrast of the grading.",
            "So for low contrast ratings, it kind of plateaus and for high contrast it actually goes down."
        ],
        [
            "So this kind of behavior can be is typically modeled with the device of enormous normalization model.",
            "We have responsible linear filter to an image is divided by some kind of normalization signal, an often that normalization signal is computed as a weighted combination of.",
            "Squared outputs of other linear features.",
            "And in British Paper in 2001, at either Schwarzenegger, Simoncelli showed how a model like this can account for whole lot of nonlinear behaviors in cells in the visual cortex, an in auditory neurons.",
            "And recently, there's been some interest in incorporating this kind of nonlinear processing in.",
            "Hierarchical visual systems, right?",
            "But why do we want to incorporate this kind of nonlinear computation?",
            "Besides the fact that it replicates what we see in visual neurons."
        ],
        [
            "Well, it turns out that this this computation is motivated by statistical regularity, is a natural images.",
            "So if you look at if you take image patches from natural images and you compute the output of two different filters, you see that there the variance of 1 depends on the magnitude of the other.",
            "So this image shows the slices in this image are histograms of the output of this filter conditioned on the value of this filter.",
            "And as you go out, you've got this classic bow tie shape.",
            "So as the magnitude of this filter increases, the variance of that that one goes up.",
            "And if you apply this kind of divisive normalization model to the output of these filters, you get a normalized response that doesn't show this this dependency, so you've got.",
            "Now we've got coefficients that are more independent in distributions, more homogeneous, and you might think that OK, that's sensible thing.",
            "For a sensory system to use in subsequent stages, so an intuitive explanation for this kind of statistical regularity."
        ],
        [
            "Is that contrast in natural images various systematically?",
            "So you have regions and natural images that have low contrasts and regions that have high contrast.",
            "So here I took small image patches from this region and from that region that just showed the output of two linear filters.",
            "On this image, matches and you see so four in one region.",
            "The output of these two filters and.",
            "Pretty much of all filters.",
            "The variance will be lower than in this one, so the contrast goes up and down.",
            "As we move across the scene.",
            "And hence you get this.",
            "This dependence in variance, and if you normalize that out, you get more independent.",
            "More independent time coefficients.",
            "So in the rest of the talk, I want to focus on two ideas related to this observation.",
            "First of all, do you want to take this normalization signal and normalize the signal and then throw it away?",
            "Is at least in this example, it seems like you should be able to tell.",
            "Well, I'm in a high contrast high contrast area of the image or low contrast area, so there may be some information there.",
            "And the second idea is that these kind of nonstationary statistics in natural images.",
            "Are a lot more complex than simple variation in magnitudes and in contrast, So what?"
        ],
        [
            "I mean by that.",
            "So if you take.",
            "Whole bunch of linear filters, so here I'm plotting joint distribution joint distributions with the outputs of of different linear filters applied to image patches.",
            "From these you see that.",
            "So, and these... are schematic, so they represent the variance in the correlation between these different pairs of filters.",
            "You see that for different regions the variance changes, but it kind of changes in the pattern and this also changes in correlation across the different regions now."
        ],
        [
            "Take image patches from.",
            "Other regions you see that this fairly rich pattern of statistical regularity's here.",
            "In this joint.",
            "Joint distributions."
        ],
        [
            "So the idea?",
            "Is if you normalize this.",
            "If you have very good normalization.",
            "Computation, which you're going to end up with some kind of factorial distributions that are the same for different contexts, so you might have a very in principle a very efficient representation.",
            "Proficients are, your representation is independent, but you've lost a lot of the information in terms of sort, of which context here when you're in.",
            "So what I want to describe is a model that can capture these kinds of patterns so it can learn these patterns in correlations and learn what context you are.",
            "The image patcher you're taking is from."
        ],
        [
            "So if we look at these.",
            "You can see that for different image contexts, the variance of coefficients and the correlations change.",
            "So this suggests that we should be able to to model image patches using.",
            "A covariance matrix.",
            "To capture these kinds of patterns so the idea would be to model distributions, I'm actually going to model pixels here.",
            "I showed the outputs of linear filters, but if it's a linear transformation it's it's just.",
            "It's a linear transformation you can absorb to front end.",
            "So the idea is to model image patches as a multivariate Gaussian distribution.",
            "And have.",
            "Covariance matrices that represent this kind of context, and they'll be functions of some latent variables.",
            "Right, so how can we parameterise covariance matrices to give us this kind of rich?",
            "Describe these rich statistics.",
            "We don't have a mixture."
        ],
        [
            "Because so I showed right here, 4 different classes and different kind of covariance matrices in this large space of image patches.",
            "But these are just for examples I made up in practice.",
            "There's a continuum, different kinds of infrastructure.",
            "You don't assume that there's a fixed number of classes that you're going to model your distribution with, so."
        ],
        [
            "Be great if we could.",
            "They couldn't make it and have some kind of a basis representation for it, so we could continuously transition between different types of structure.",
            "Well, this actually doesn't work very well because covariance matrix has to be certain rules and if you have.",
            "It's fairly hard to come up with a with a linear basis of this form that's flexible enough, but turns out if you."
        ],
        [
            "Look in the matrix logarithm space.",
            "So if you.",
            "Take a matrix, something transform of your covariance matrix.",
            "You can define a linear basis of these AJS and these coefficients that works really well.",
            "The matrix logarithm is just the inverse of the matrix exponential and you can think of that as a generalization of the scalar exponential.",
            "So it's a be defined as a power series in matrices.",
            "And.",
            "So all you need in this case is for these these basis functions AJ's.",
            "And coefficients yjs, and as long as the basis functions are symmetric.",
            "Once you add these up and you exponentially, you get a valid covariance matrix.",
            "If your coefficients are all zero, you get identity, sort of like in the scalar case.",
            "If you exponentially 01, so we can call this this isotropic Gaussian with identity covariance matrix.",
            "Our Canonical distribution, and if we widen our data to start with, this is kind of the default case.",
            "So when all the coefficients are zero, which you're describing is kind of an average, my covariance matrix is isotropic.",
            "Distribution and then activation of these.",
            "Is coefficients going to tell you how your commence matrix deviates from your Canonical?",
            "Matrix.",
            "Right so.",
            "One thing we have to address first is that this is.",
            "This can be very large space if you if you want to learn this kind of generative model and learn these parameters from data.",
            "This space of the covariance matrix is the square of your dimensionality of your data, so the number of parameters if you want to learn a full basis on that is just too big.",
            "So the way we can address."
        ],
        [
            "That is too.",
            "Instead of allowing.",
            "Covariance matrices for different image context to vary in arbitrary directions.",
            "We can notice that.",
            "In natural images, there's only certain types of covariance matrices that might occur.",
            "So we want to constrain our representation or parameter space using the data, and the idea is so fat.",
            "Two different distribution of images.",
            "So seeing some conference context, this textured bark and some comfort foliage and they have these different kind of correlational structures, what we're going to do?"
        ],
        [
            "Say is we're going to find common directions in image space and only allow the covariance matrices to be stretched along those fixed number of directions.",
            "So what that means is that our basis functions for constructing these covariance matrices are going to be constructed using a weighted combination of outer products of these vectors BK.",
            "So we're going to have an over complete set of these these vectors, so there's a lot of them, but there's not so many that each each matrix is completely unconstrained, because each basis function here is forced to use the same set of these vectors, just in a weighted form, and you can see this has some relationship to SVD, so writing a symmetric matrix as a.",
            "Weighted combination of other products, but the trick here is that all these different basic functions are using the same vector, just weighted differently.",
            "And these vectors are unit norm, so they're just directions in space."
        ],
        [
            "So the full model then is we're going to have.",
            "Our image patches come from a multivariate Gaussian distribution with zero mean and some covariance matrix.",
            "The logarithm of that is generated as some weighted combination of basis functions and the basis functions themselves are defined this way.",
            "And then if we put a prior in our case, we put a sparse pictorial prior on the coefficients.",
            "Then we have a full generative model that we can train on natural images and graphically what this looks like.",
            "Is there some kind of distribution here which is always a multivariate?",
            "Multivariate Gaussian distribution conditioned on those variables.",
            "Each each latent variable here has some weights to the same set of.",
            "These vectors.",
            "Matrix."
        ],
        [
            "So what happens if you turn 1 coefficient on?",
            "In this model you deform your Canonical.",
            "Covariance matrix that identity which is the dashed line here along some dimensions and basically if it's.",
            "If the weight from this coefficient to one particular vector is positive, that means that when you turn that coefficient on to be positive, you're expanding the distribution smoothly along the direction you're stretching it out.",
            "And if this wait here W is negative, then you're gently contracting the distribution."
        ],
        [
            "And for a different coefficient mind code, a different covariance."
        ],
        [
            "Matrix.",
            "And we have a full activity pattern up here in this vector coefficient you have some other, some fully encoded.",
            "Correlational structure and this these examples are all in two D But the whole point here is to be able to do things in this very large dimensional space with just a few parameters.",
            "Now the ideas for inference in this model you're given one image Patch which is just one point in the space is very high dimensional space and you're using the knobs appear of these coefficients defined.",
            "Try to find the covariance matrix that best kind of covers that point right.",
            "That explains that the model thinks.",
            "Is the distribution that generated that image Patch?"
        ],
        [
            "So to relate this back to the normalization model that I started with."
        ],
        [
            "You can think of this inference of the optimal covariance matrix as a nonlinear, so we actually do it in an iterative procedure where you have to find the maximum posterior value of these coefficients given your image Patch, which can be hard, but you can think of that as some kind of nonlinear computation that gives you a really powerful normalization that.",
            "Normalization machine that you can use.",
            "So once you have the convergence convergence matrix, you can multiply your image."
        ],
        [
            "Matches by.",
            "The square root of the covariance matrix and you get a normalized response that accounts for hopefully a much more rich pattern of correlations and changes in variance.",
            "Then just contrast gain.",
            "Right?"
        ],
        [
            "So.",
            "Just a very quick details about learning this week.",
            "This is maximum likelihood, learning just straight gradient descent ascent on the likelihood would change it on 20 by 20 inch patches sampled from a lot of natural images, and these numbers are sort of somewhat arbitrarily chosen, but used 1000 of these vectors, so this is 2 1/2 times over complete in the model, 'cause it's a 400 dimensional space, and now you have 1000 directions in that space long, which you can kind of move your distribution around.",
            "And the representation in the model is this 150.",
            "Units and that's.",
            "Yeah.",
            "That's the size of the image Patch, which I'm calling X. Yeah, so there's no.",
            "Take a bath.",
            "This actually is a model for user straight on pixels.",
            "Yeah, so if you have an overcomplete representation into a filterbank that's trickier, and that's more different, But if you imagine just a complete set of filters sayin oriented Gabor basis, that's just a rotation of the space, and that rotation can be absorbed, absorbed basically in these vectors up here.",
            "These are filters, right?",
            "So the model that the version that I'm going to talk about there's no filters.",
            "You start with pixels.",
            "Yes, they build the bees themselves are vectors in some space in image space, which you can interpret filters, right?",
            "Although you never really have to project directly on them, you're not computing their coefficients.",
            "So is the linear combination of.",
            "Space collective matrix and then after exponential, is that still semidefinite on yes?",
            "Yes, as long as it, as long as it's symmetric.",
            "So the matrix exponential actually what it does is it exponentiates the eigenvalues of the matrix.",
            "So if you have some matrix, you apply matrix.",
            "Potentially you just all you're doing is element wise.",
            "You're computing dragon spectrum elementwise.",
            "Exponentiate the eigenvalues so as long as those eigenvalues.",
            "R. Real is fine."
        ],
        [
            "OK, so we can look and this is kind of answers your question.",
            "What are these?",
            "Because look like.",
            "Well if you look if you plot them in image space they look like the board so you get again you get oriented edge detectors if you will.",
            "So this is an example 25.",
            "Of these, because out of 1000 projected back in image space, the model strangest in pixels, so there's no.",
            "You don't actually need to enforce, so in this model I had sparsity.",
            "As in the prior on these wise, but even if you didn't, you would still learn these things, but essentially.",
            "The reason you get you get Sparks some.",
            "I'm going off in attention here a little bit, but so we know that these kinds of oriented filters are also the sparsest directions and images right there.",
            "The most, the most cryptic, the most non Gaussian in this model are used to stretch this covariance out.",
            "So it kind of makes sense that those directions in space that have wisps of some sort are the ones that this model latches onto to model the all the different covariances that are.",
            "Together can describe the data.",
            "You couldn't even use this model this year.",
            "It's past the online because you have another computer.",
            "No, you can.",
            "You can have Gaussian priors and why you could have any kind of prior.",
            "Some otherwise are very not related in very nonlinear way to this.",
            "Yeah, in fact these emerge even if you have very few wise up here.",
            "But as long as you can tell apart, it's kind of combinatorial.",
            "OK, so now what do these weights look like?",
            "An in order to so I don't know.",
            "Visualization of these high order parameters, but it's pretty hard.",
            "So in order to visualize them, what I'm going to do is I'm going to take these oriented."
        ],
        [
            "Filters and represent them as lines, so this is the space of the image patches 20 pixels by 20 pixels.",
            "These all the green ones are all thousand of these oriented filters and the black ones are these.",
            "These 25 and the longer they are the lower frequency there are, so this is.",
            "This is the kind of weather, weather or feature is and also the orientation in the spatial frequency and now."
        ],
        [
            "So yes, one more one unit in this Model 1.",
            "Why doesn't code global contrast so that these are weights to all of these features plotted for one particular unit and red is positive Blues negative, but there's no negative weights here.",
            "So what this unit encodes is the global variance variance in away, so it will be on when there's high energy or high variance.",
            "Along any of these features, in other words, it will.",
            "You know covariance matrix that basic can expand out along all directions.",
            "So just call this the global contrast in the image Patch.",
            "Now we can take this one particular unit can do.",
            "We should be able to normalize, do game control using by."
        ],
        [
            "So this is the same image that I showed before.",
            "I'm just just cut it up into 2020 image patches an I subtracted the mean from each image Patch.",
            "That's why it looks a little weird and now we can compute the covariance matrix using only that one in Ferd unit."
        ],
        [
            "The model that this one does."
        ],
        [
            "This global contrast one and compute covariance matrix and then normalize each individual image Patch by its own covariance matrix that uses that value of that one."
        ],
        [
            "Unit what we see is yes, it does contrast."
        ],
        [
            "Normalization so."
        ],
        [
            "I go back and forth.",
            "You can see the low contrast areas become.",
            "High contrast right?",
            "OK, so this kind of general gain control, but that's not the."
        ],
        [
            "Interesting, there's a lot more units in the model, so this is the global contrast one.",
            "But the model also learns that contrast in images can be localized, and it in a way it learns a Gabor.",
            "Representation of local contrast.",
            "So again, for intuition would read weights mean higher variance, higher energy in these oriented filters blue means lower variance.",
            "Actually, these ways can be positive and negative, so the converse pattern is also encoded by this one, so there's a.",
            "There's a kind of a basis of contrast using."
        ],
        [
            "There's a more interesting ones in the model that encode kind of oriented energy.",
            "So when this particular unit is on, it means that in the image Patch variation along over here and for example directions higher than you than average than the kind of Canonical distribution along these.",
            "It's lower.",
            "But then there's also these cross if you can see over here, the vertical ones tend to be high.",
            "So you can think of this as a higher order.",
            "You can think of it as a complex elephenor scientists.",
            "You can think of this as a kind of high order edge detector that's invariant to the polarity or phase.",
            "Or you can think of this maybe as a boundary detector.",
            "So it's a little more clear here.",
            "This unit might like a lot of oriented structure here a lot of horizontal structure here, vertical here, and kind of not a lot of this one."
        ],
        [
            "So now we can take.",
            "Then I have more complex units, but I don't really have time to go into this."
        ],
        [
            "Now could take this whole model and do what?",
            "I did with him.",
            "That one unit.",
            "So again for each image Patch will take each image attachment for the maximum posteriori value of the whole vector of the latent variable representation in the model of all those wise.",
            "Use that one to construct a covariance matrix.",
            "So that's the distribution that the model thinks the image Patch came from.",
            "And now we can normalize it.",
            "So basically the multiplied by the.",
            "C to the negative 1/2."
        ],
        [
            "So this is just again using only that one unit."
        ],
        [
            "What does global contract?"
        ],
        [
            "Right, so does this now."
        ],
        [
            "Use all of them.",
            "You get that so you normalizing out.",
            "A lot more than just contrasts and it makes sense if you have units that encoding orientation, spatial frequency, and that's all.",
            "All context that this model learns using this kind of more powerful normalization computation."
        ],
        [
            "So again, this way."
        ],
        [
            "Just contrast normalized and this is.",
            "Normalized by that whole covariance matrix.",
            "So there's still some structure left.",
            "You can vaguely see kind of outline the trees, but see a lot of the oriented structures disappeared?",
            "And spatial frequency has been equalized."
        ],
        [
            "So.",
            "Machine gun.",
            "OK, so 5 minutes I want to really quickly just relate this model to a type of high order PBM's that have been used in Italy.",
            "Also for modeling images just to show you that some interesting relationships.",
            "And then I'm just going to wrap up so restricted Boltzmann machine have been mentioned here already and generally just a general statistical model, usually defined using energy between indivisible units, a hidden units and a very simple flavors just uses this kind of linear energy function with.",
            "There's ways that go between visible and hidden, and that's your energy.",
            "So high order PBM, they go back to the 80s and then use a little a few more times more recently used instead of using just these.",
            "2nd order or linear?",
            "How you want to look at it.",
            "Interactions use higher."
        ],
        [
            "So the example I want to talk about is this paper by Mr. Chin Hinton where.",
            "So this is a little bit trickier.",
            "Actually divided the visible units into two groups X&Y, 'cause they were modeling an image sequence, and these were two frames image sequence, but the general idea is that your weights now our Model 3 way multiplicative interactions, so in their case it was in these hidden.",
            "Hidden variables modulated.",
            "the OR gated.",
            "The weighting between one frame and another.",
            "The correlations between one another.",
            "So instead of having a meet matrix for W have this tensor.",
            "W and the interactions are even at this multiplicative interactions.",
            "So another way to write this thing is as as if these hidden units fold together a bunch of these WK matrices to give you the final kind of interaction between XX&Y and this model is more powerful, powerful because it's instead of the hidden units, just modulating the bias of you visible there really modulating the correlations in the data that you see."
        ],
        [
            "So to compare it to kind of this weird architecture that I just described, if you concatenate, if you treat these.",
            "Just.",
            "Together as all of your visible units and concatenate them and now you have your visible units and and the hidden units here.",
            "You get something that's similar to the log likelihood that you see in this model, where this is just the Gaussian log likelihood and the covariance matrix with the inverse covariance matrix that would normally go here has this exponential form so.",
            "Just it looks pretty similar.",
            "There are some differences in terms of here working in exponential.",
            "Space, so when you have no activity or zero activity, you have identity matrix.",
            "Here here usually working with binary matrices and you're not really so worried about kind of normalizing that distribution.",
            "But I think they have.",
            "These two architectures have similar flavor and similar capacity form."
        ],
        [
            "Darling so.",
            "Just to conclude quickly, I talked about this kind of nonlinear.",
            "Computations that they are motivated by things that not nearly as that we observe in the visual system.",
            "And I talked about only but very.",
            "Early stages, the visual system and some of the things that I showed you.",
            "You do seen in higher visual areas and I showed you how they can capture context and removing it.",
            "Doesn't leave you with too much structure, so you might.",
            "Think hard about like once you do this, which signal do you keep in?",
            "Which do you model?",
            "And ideally you would you would be able to.",
            "Model this contact signal and.",
            "Maybe the.",
            "The representation that's left after normalization by by this by this model, so that that's kind of one of the questions.",
            "That's left for future work.",
            "Another thing would be really nice to be able to extend these kinds of models to large images.",
            "This is all modeling, little image patches, and as you know, there's only so much structure you can capture in 20 by 20 pixels, and it's not trivial at all to think about how to extend this to large images.",
            "So just kind of replicating it.",
            "Right so and for me, as someone working in our science, the most interesting question is kind of how to map these kinds of computations derived from statistics and natural images.",
            "Two brain areas and.",
            "Yeah, so where are they implemented in the brain and for what purpose?",
            "So I have a question so.",
            "So the motivation of research is from side of modeling images of statistiques.",
            "A lot of people here boy interested in also classification recognition.",
            "And how can we benefit from the study of images statistics then getting useful features?",
            "Right, I think that questions come up quite a quite a bit right?",
            "Like when do you want to do unsupervised learning and why do you want to model the data without any task and?",
            "I mean, I know you guys know this better than I do, but my take on this is that a lot of the tasks that we'd really want these visual systems to do are hard and require very.",
            "Deep networks that are very powerful is the classification object recognition.",
            "And when you're training these models, the signals you get back to the training signals are going to be really weak in a big system.",
            "So you sort of want to get as far as you can using either unsupervised learning or tasks that are just some kind of mid level task and we really don't have very much of this in computer vision.",
            "Having tests that don't require you know, solving the whole problem at once so.",
            "I haven't used this for classification and I think some people have have talked about it and might try later, but imagine doing this and kind of building better systems to better understand what's in an image is just the only way to go.",
            "I don't think you can.",
            "Yeah, go straight for classification.",
            "The other part is normalized away sort of normalized.",
            "Right?",
            "Now.",
            "Somewhere as well.",
            "Yeah, I have no idea.",
            "It's.",
            "Well, yeah.",
            "Yeah, and I don't really know what information is in here either.",
            "This this looks.",
            "Wait, but maybe it's just efficient and.",
            "There's no obvious dependence, but they're just independent, but very informative.",
            "I don't know what their information between this and the original image actually.",
            "Yeah, yeah.",
            "Somebody.",
            "Try to use translation.",
            "Reply.",
            "And then take older teachers responses.",
            "Within a certain neighborhood and then replace it with the output by itself divided by the distribution center.",
            "So it's the same thing you're talking about.",
            "Put the new module in the conversation.",
            "That is enormous differences.",
            "Things that didn't work before going to work with their children examples, it seems that enormous effect too.",
            "Ready to send an?",
            "Pause.",
            "Appearances, but it also might be proposed effects of.",
            "Competition for the filters.",
            "Yeah, when you different.",
            "So I think there's a lot of magical things that happened.",
            "We used.",
            "This will be.",
            "Yeah, although I was trying to emphasize here that you might want to keep that signal around and try to figure out what to do with it, because even when you're doing something not not like this full generative model but you're doing divisive normalization, but your weights from one oriented feature to the other ones are maybe not uniform, somehow they learned.",
            "That's some kind of function that's a complicated function that transforming it could be doing something almost as complicated as this.",
            "It could be doing other normalization out of context of spatial frequency variation of other things and.",
            "I don't really know how to relate that information to this stuff that comes out.",
            "Where was the wedding enabled?",
            "So why was the whitening enabled?",
            "It will remove usually when people talk about whitening they talk about global lightning, so you have all your data on sample and then you make the covariance of that data right?",
            "So if you do that, that's basically natural images.",
            "You're removing the 1 / F so.",
            "Versus that.",
            "The local structure has, yeah.",
            "Oh oh sorry yeah.",
            "OK, in my case why?",
            "First of all, this polarity.",
            "So this is you're dividing things, but if.",
            "It's where it is in the waiting space already, but.",
            "You're not.",
            "Um?",
            "One way to think about it is since you're multiplying by this rotation matrix.",
            "You're kind of keeping the same quadrant.",
            "That's not really true, because this happens in the white in space, but you can imagine keeping the same quadrant so that if pixels were white, they'll stay white.",
            "In other words, when you model the covariance matrix, you're not modeling the polarity of the image Patch.",
            "So if I give you an image Patch that's white, or that's black is going to have the same covariance matrix.",
            "And in this case.",
            "All of these happen to be, say, white, and next them happen to be white black, so I mean, that's a clear statistical regularity here that the model just can't capture, and one way to go would be to have a more complex model of context.",
            "So what is it?",
            "Better soon as I know it might make it 150, so it's smaller than X.",
            "It was what was feasible.",
            "Basically we tried training this with more neurons and they were not really interpretable and they didn't really contribute 200 the likelihood so.",
            "It's an arbitrary number.",
            "It would be different, yeah?",
            "So basically you have to manually do the number.",
            "Yeah."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first of all, there's saturation in the response of the cell.",
                    "label": 1
                },
                {
                    "sent": "So as you increase contrast, the response goes up, but it doesn't quite go up as fast at high contrast.",
                    "label": 0
                },
                {
                    "sent": "There's some kind of contrast gain control going on even at this early stage.",
                    "label": 0
                },
                {
                    "sent": "Also, as you increase the size of the grading past some optimal size, the response starts to decrease and this decreases depending on the contrast of the grading.",
                    "label": 0
                },
                {
                    "sent": "So for low contrast ratings, it kind of plateaus and for high contrast it actually goes down.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this kind of behavior can be is typically modeled with the device of enormous normalization model.",
                    "label": 0
                },
                {
                    "sent": "We have responsible linear filter to an image is divided by some kind of normalization signal, an often that normalization signal is computed as a weighted combination of.",
                    "label": 0
                },
                {
                    "sent": "Squared outputs of other linear features.",
                    "label": 0
                },
                {
                    "sent": "And in British Paper in 2001, at either Schwarzenegger, Simoncelli showed how a model like this can account for whole lot of nonlinear behaviors in cells in the visual cortex, an in auditory neurons.",
                    "label": 0
                },
                {
                    "sent": "And recently, there's been some interest in incorporating this kind of nonlinear processing in.",
                    "label": 0
                },
                {
                    "sent": "Hierarchical visual systems, right?",
                    "label": 0
                },
                {
                    "sent": "But why do we want to incorporate this kind of nonlinear computation?",
                    "label": 0
                },
                {
                    "sent": "Besides the fact that it replicates what we see in visual neurons.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, it turns out that this this computation is motivated by statistical regularity, is a natural images.",
                    "label": 1
                },
                {
                    "sent": "So if you look at if you take image patches from natural images and you compute the output of two different filters, you see that there the variance of 1 depends on the magnitude of the other.",
                    "label": 0
                },
                {
                    "sent": "So this image shows the slices in this image are histograms of the output of this filter conditioned on the value of this filter.",
                    "label": 0
                },
                {
                    "sent": "And as you go out, you've got this classic bow tie shape.",
                    "label": 0
                },
                {
                    "sent": "So as the magnitude of this filter increases, the variance of that that one goes up.",
                    "label": 0
                },
                {
                    "sent": "And if you apply this kind of divisive normalization model to the output of these filters, you get a normalized response that doesn't show this this dependency, so you've got.",
                    "label": 0
                },
                {
                    "sent": "Now we've got coefficients that are more independent in distributions, more homogeneous, and you might think that OK, that's sensible thing.",
                    "label": 0
                },
                {
                    "sent": "For a sensory system to use in subsequent stages, so an intuitive explanation for this kind of statistical regularity.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is that contrast in natural images various systematically?",
                    "label": 0
                },
                {
                    "sent": "So you have regions and natural images that have low contrasts and regions that have high contrast.",
                    "label": 0
                },
                {
                    "sent": "So here I took small image patches from this region and from that region that just showed the output of two linear filters.",
                    "label": 0
                },
                {
                    "sent": "On this image, matches and you see so four in one region.",
                    "label": 0
                },
                {
                    "sent": "The output of these two filters and.",
                    "label": 0
                },
                {
                    "sent": "Pretty much of all filters.",
                    "label": 0
                },
                {
                    "sent": "The variance will be lower than in this one, so the contrast goes up and down.",
                    "label": 0
                },
                {
                    "sent": "As we move across the scene.",
                    "label": 0
                },
                {
                    "sent": "And hence you get this.",
                    "label": 0
                },
                {
                    "sent": "This dependence in variance, and if you normalize that out, you get more independent.",
                    "label": 0
                },
                {
                    "sent": "More independent time coefficients.",
                    "label": 0
                },
                {
                    "sent": "So in the rest of the talk, I want to focus on two ideas related to this observation.",
                    "label": 0
                },
                {
                    "sent": "First of all, do you want to take this normalization signal and normalize the signal and then throw it away?",
                    "label": 0
                },
                {
                    "sent": "Is at least in this example, it seems like you should be able to tell.",
                    "label": 0
                },
                {
                    "sent": "Well, I'm in a high contrast high contrast area of the image or low contrast area, so there may be some information there.",
                    "label": 0
                },
                {
                    "sent": "And the second idea is that these kind of nonstationary statistics in natural images.",
                    "label": 0
                },
                {
                    "sent": "Are a lot more complex than simple variation in magnitudes and in contrast, So what?",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I mean by that.",
                    "label": 0
                },
                {
                    "sent": "So if you take.",
                    "label": 0
                },
                {
                    "sent": "Whole bunch of linear filters, so here I'm plotting joint distribution joint distributions with the outputs of of different linear filters applied to image patches.",
                    "label": 0
                },
                {
                    "sent": "From these you see that.",
                    "label": 0
                },
                {
                    "sent": "So, and these... are schematic, so they represent the variance in the correlation between these different pairs of filters.",
                    "label": 0
                },
                {
                    "sent": "You see that for different regions the variance changes, but it kind of changes in the pattern and this also changes in correlation across the different regions now.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Take image patches from.",
                    "label": 0
                },
                {
                    "sent": "Other regions you see that this fairly rich pattern of statistical regularity's here.",
                    "label": 0
                },
                {
                    "sent": "In this joint.",
                    "label": 0
                },
                {
                    "sent": "Joint distributions.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the idea?",
                    "label": 0
                },
                {
                    "sent": "Is if you normalize this.",
                    "label": 0
                },
                {
                    "sent": "If you have very good normalization.",
                    "label": 0
                },
                {
                    "sent": "Computation, which you're going to end up with some kind of factorial distributions that are the same for different contexts, so you might have a very in principle a very efficient representation.",
                    "label": 0
                },
                {
                    "sent": "Proficients are, your representation is independent, but you've lost a lot of the information in terms of sort, of which context here when you're in.",
                    "label": 0
                },
                {
                    "sent": "So what I want to describe is a model that can capture these kinds of patterns so it can learn these patterns in correlations and learn what context you are.",
                    "label": 0
                },
                {
                    "sent": "The image patcher you're taking is from.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So if we look at these.",
                    "label": 0
                },
                {
                    "sent": "You can see that for different image contexts, the variance of coefficients and the correlations change.",
                    "label": 0
                },
                {
                    "sent": "So this suggests that we should be able to to model image patches using.",
                    "label": 0
                },
                {
                    "sent": "A covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "To capture these kinds of patterns so the idea would be to model distributions, I'm actually going to model pixels here.",
                    "label": 0
                },
                {
                    "sent": "I showed the outputs of linear filters, but if it's a linear transformation it's it's just.",
                    "label": 0
                },
                {
                    "sent": "It's a linear transformation you can absorb to front end.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to model image patches as a multivariate Gaussian distribution.",
                    "label": 0
                },
                {
                    "sent": "And have.",
                    "label": 0
                },
                {
                    "sent": "Covariance matrices that represent this kind of context, and they'll be functions of some latent variables.",
                    "label": 1
                },
                {
                    "sent": "Right, so how can we parameterise covariance matrices to give us this kind of rich?",
                    "label": 0
                },
                {
                    "sent": "Describe these rich statistics.",
                    "label": 0
                },
                {
                    "sent": "We don't have a mixture.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Because so I showed right here, 4 different classes and different kind of covariance matrices in this large space of image patches.",
                    "label": 0
                },
                {
                    "sent": "But these are just for examples I made up in practice.",
                    "label": 0
                },
                {
                    "sent": "There's a continuum, different kinds of infrastructure.",
                    "label": 0
                },
                {
                    "sent": "You don't assume that there's a fixed number of classes that you're going to model your distribution with, so.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Be great if we could.",
                    "label": 0
                },
                {
                    "sent": "They couldn't make it and have some kind of a basis representation for it, so we could continuously transition between different types of structure.",
                    "label": 1
                },
                {
                    "sent": "Well, this actually doesn't work very well because covariance matrix has to be certain rules and if you have.",
                    "label": 0
                },
                {
                    "sent": "It's fairly hard to come up with a with a linear basis of this form that's flexible enough, but turns out if you.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Look in the matrix logarithm space.",
                    "label": 0
                },
                {
                    "sent": "So if you.",
                    "label": 0
                },
                {
                    "sent": "Take a matrix, something transform of your covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "You can define a linear basis of these AJS and these coefficients that works really well.",
                    "label": 0
                },
                {
                    "sent": "The matrix logarithm is just the inverse of the matrix exponential and you can think of that as a generalization of the scalar exponential.",
                    "label": 0
                },
                {
                    "sent": "So it's a be defined as a power series in matrices.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "So all you need in this case is for these these basis functions AJ's.",
                    "label": 0
                },
                {
                    "sent": "And coefficients yjs, and as long as the basis functions are symmetric.",
                    "label": 0
                },
                {
                    "sent": "Once you add these up and you exponentially, you get a valid covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "If your coefficients are all zero, you get identity, sort of like in the scalar case.",
                    "label": 0
                },
                {
                    "sent": "If you exponentially 01, so we can call this this isotropic Gaussian with identity covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "Our Canonical distribution, and if we widen our data to start with, this is kind of the default case.",
                    "label": 0
                },
                {
                    "sent": "So when all the coefficients are zero, which you're describing is kind of an average, my covariance matrix is isotropic.",
                    "label": 0
                },
                {
                    "sent": "Distribution and then activation of these.",
                    "label": 0
                },
                {
                    "sent": "Is coefficients going to tell you how your commence matrix deviates from your Canonical?",
                    "label": 0
                },
                {
                    "sent": "Matrix.",
                    "label": 0
                },
                {
                    "sent": "Right so.",
                    "label": 0
                },
                {
                    "sent": "One thing we have to address first is that this is.",
                    "label": 0
                },
                {
                    "sent": "This can be very large space if you if you want to learn this kind of generative model and learn these parameters from data.",
                    "label": 0
                },
                {
                    "sent": "This space of the covariance matrix is the square of your dimensionality of your data, so the number of parameters if you want to learn a full basis on that is just too big.",
                    "label": 0
                },
                {
                    "sent": "So the way we can address.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That is too.",
                    "label": 0
                },
                {
                    "sent": "Instead of allowing.",
                    "label": 0
                },
                {
                    "sent": "Covariance matrices for different image context to vary in arbitrary directions.",
                    "label": 0
                },
                {
                    "sent": "We can notice that.",
                    "label": 0
                },
                {
                    "sent": "In natural images, there's only certain types of covariance matrices that might occur.",
                    "label": 0
                },
                {
                    "sent": "So we want to constrain our representation or parameter space using the data, and the idea is so fat.",
                    "label": 0
                },
                {
                    "sent": "Two different distribution of images.",
                    "label": 0
                },
                {
                    "sent": "So seeing some conference context, this textured bark and some comfort foliage and they have these different kind of correlational structures, what we're going to do?",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Say is we're going to find common directions in image space and only allow the covariance matrices to be stretched along those fixed number of directions.",
                    "label": 1
                },
                {
                    "sent": "So what that means is that our basis functions for constructing these covariance matrices are going to be constructed using a weighted combination of outer products of these vectors BK.",
                    "label": 0
                },
                {
                    "sent": "So we're going to have an over complete set of these these vectors, so there's a lot of them, but there's not so many that each each matrix is completely unconstrained, because each basis function here is forced to use the same set of these vectors, just in a weighted form, and you can see this has some relationship to SVD, so writing a symmetric matrix as a.",
                    "label": 0
                },
                {
                    "sent": "Weighted combination of other products, but the trick here is that all these different basic functions are using the same vector, just weighted differently.",
                    "label": 0
                },
                {
                    "sent": "And these vectors are unit norm, so they're just directions in space.",
                    "label": 1
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the full model then is we're going to have.",
                    "label": 0
                },
                {
                    "sent": "Our image patches come from a multivariate Gaussian distribution with zero mean and some covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "The logarithm of that is generated as some weighted combination of basis functions and the basis functions themselves are defined this way.",
                    "label": 0
                },
                {
                    "sent": "And then if we put a prior in our case, we put a sparse pictorial prior on the coefficients.",
                    "label": 0
                },
                {
                    "sent": "Then we have a full generative model that we can train on natural images and graphically what this looks like.",
                    "label": 0
                },
                {
                    "sent": "Is there some kind of distribution here which is always a multivariate?",
                    "label": 0
                },
                {
                    "sent": "Multivariate Gaussian distribution conditioned on those variables.",
                    "label": 0
                },
                {
                    "sent": "Each each latent variable here has some weights to the same set of.",
                    "label": 0
                },
                {
                    "sent": "These vectors.",
                    "label": 0
                },
                {
                    "sent": "Matrix.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what happens if you turn 1 coefficient on?",
                    "label": 0
                },
                {
                    "sent": "In this model you deform your Canonical.",
                    "label": 0
                },
                {
                    "sent": "Covariance matrix that identity which is the dashed line here along some dimensions and basically if it's.",
                    "label": 0
                },
                {
                    "sent": "If the weight from this coefficient to one particular vector is positive, that means that when you turn that coefficient on to be positive, you're expanding the distribution smoothly along the direction you're stretching it out.",
                    "label": 0
                },
                {
                    "sent": "And if this wait here W is negative, then you're gently contracting the distribution.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for a different coefficient mind code, a different covariance.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matrix.",
                    "label": 0
                },
                {
                    "sent": "And we have a full activity pattern up here in this vector coefficient you have some other, some fully encoded.",
                    "label": 0
                },
                {
                    "sent": "Correlational structure and this these examples are all in two D But the whole point here is to be able to do things in this very large dimensional space with just a few parameters.",
                    "label": 0
                },
                {
                    "sent": "Now the ideas for inference in this model you're given one image Patch which is just one point in the space is very high dimensional space and you're using the knobs appear of these coefficients defined.",
                    "label": 0
                },
                {
                    "sent": "Try to find the covariance matrix that best kind of covers that point right.",
                    "label": 0
                },
                {
                    "sent": "That explains that the model thinks.",
                    "label": 0
                },
                {
                    "sent": "Is the distribution that generated that image Patch?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to relate this back to the normalization model that I started with.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You can think of this inference of the optimal covariance matrix as a nonlinear, so we actually do it in an iterative procedure where you have to find the maximum posterior value of these coefficients given your image Patch, which can be hard, but you can think of that as some kind of nonlinear computation that gives you a really powerful normalization that.",
                    "label": 0
                },
                {
                    "sent": "Normalization machine that you can use.",
                    "label": 0
                },
                {
                    "sent": "So once you have the convergence convergence matrix, you can multiply your image.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Matches by.",
                    "label": 0
                },
                {
                    "sent": "The square root of the covariance matrix and you get a normalized response that accounts for hopefully a much more rich pattern of correlations and changes in variance.",
                    "label": 1
                },
                {
                    "sent": "Then just contrast gain.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Just a very quick details about learning this week.",
                    "label": 0
                },
                {
                    "sent": "This is maximum likelihood, learning just straight gradient descent ascent on the likelihood would change it on 20 by 20 inch patches sampled from a lot of natural images, and these numbers are sort of somewhat arbitrarily chosen, but used 1000 of these vectors, so this is 2 1/2 times over complete in the model, 'cause it's a 400 dimensional space, and now you have 1000 directions in that space long, which you can kind of move your distribution around.",
                    "label": 0
                },
                {
                    "sent": "And the representation in the model is this 150.",
                    "label": 1
                },
                {
                    "sent": "Units and that's.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "That's the size of the image Patch, which I'm calling X. Yeah, so there's no.",
                    "label": 0
                },
                {
                    "sent": "Take a bath.",
                    "label": 0
                },
                {
                    "sent": "This actually is a model for user straight on pixels.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so if you have an overcomplete representation into a filterbank that's trickier, and that's more different, But if you imagine just a complete set of filters sayin oriented Gabor basis, that's just a rotation of the space, and that rotation can be absorbed, absorbed basically in these vectors up here.",
                    "label": 0
                },
                {
                    "sent": "These are filters, right?",
                    "label": 0
                },
                {
                    "sent": "So the model that the version that I'm going to talk about there's no filters.",
                    "label": 0
                },
                {
                    "sent": "You start with pixels.",
                    "label": 0
                },
                {
                    "sent": "Yes, they build the bees themselves are vectors in some space in image space, which you can interpret filters, right?",
                    "label": 0
                },
                {
                    "sent": "Although you never really have to project directly on them, you're not computing their coefficients.",
                    "label": 0
                },
                {
                    "sent": "So is the linear combination of.",
                    "label": 0
                },
                {
                    "sent": "Space collective matrix and then after exponential, is that still semidefinite on yes?",
                    "label": 0
                },
                {
                    "sent": "Yes, as long as it, as long as it's symmetric.",
                    "label": 0
                },
                {
                    "sent": "So the matrix exponential actually what it does is it exponentiates the eigenvalues of the matrix.",
                    "label": 0
                },
                {
                    "sent": "So if you have some matrix, you apply matrix.",
                    "label": 0
                },
                {
                    "sent": "Potentially you just all you're doing is element wise.",
                    "label": 0
                },
                {
                    "sent": "You're computing dragon spectrum elementwise.",
                    "label": 0
                },
                {
                    "sent": "Exponentiate the eigenvalues so as long as those eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "R. Real is fine.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so we can look and this is kind of answers your question.",
                    "label": 0
                },
                {
                    "sent": "What are these?",
                    "label": 0
                },
                {
                    "sent": "Because look like.",
                    "label": 0
                },
                {
                    "sent": "Well if you look if you plot them in image space they look like the board so you get again you get oriented edge detectors if you will.",
                    "label": 0
                },
                {
                    "sent": "So this is an example 25.",
                    "label": 0
                },
                {
                    "sent": "Of these, because out of 1000 projected back in image space, the model strangest in pixels, so there's no.",
                    "label": 0
                },
                {
                    "sent": "You don't actually need to enforce, so in this model I had sparsity.",
                    "label": 0
                },
                {
                    "sent": "As in the prior on these wise, but even if you didn't, you would still learn these things, but essentially.",
                    "label": 0
                },
                {
                    "sent": "The reason you get you get Sparks some.",
                    "label": 0
                },
                {
                    "sent": "I'm going off in attention here a little bit, but so we know that these kinds of oriented filters are also the sparsest directions and images right there.",
                    "label": 0
                },
                {
                    "sent": "The most, the most cryptic, the most non Gaussian in this model are used to stretch this covariance out.",
                    "label": 0
                },
                {
                    "sent": "So it kind of makes sense that those directions in space that have wisps of some sort are the ones that this model latches onto to model the all the different covariances that are.",
                    "label": 0
                },
                {
                    "sent": "Together can describe the data.",
                    "label": 0
                },
                {
                    "sent": "You couldn't even use this model this year.",
                    "label": 0
                },
                {
                    "sent": "It's past the online because you have another computer.",
                    "label": 0
                },
                {
                    "sent": "No, you can.",
                    "label": 0
                },
                {
                    "sent": "You can have Gaussian priors and why you could have any kind of prior.",
                    "label": 0
                },
                {
                    "sent": "Some otherwise are very not related in very nonlinear way to this.",
                    "label": 0
                },
                {
                    "sent": "Yeah, in fact these emerge even if you have very few wise up here.",
                    "label": 0
                },
                {
                    "sent": "But as long as you can tell apart, it's kind of combinatorial.",
                    "label": 0
                },
                {
                    "sent": "OK, so now what do these weights look like?",
                    "label": 0
                },
                {
                    "sent": "An in order to so I don't know.",
                    "label": 0
                },
                {
                    "sent": "Visualization of these high order parameters, but it's pretty hard.",
                    "label": 0
                },
                {
                    "sent": "So in order to visualize them, what I'm going to do is I'm going to take these oriented.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Filters and represent them as lines, so this is the space of the image patches 20 pixels by 20 pixels.",
                    "label": 0
                },
                {
                    "sent": "These all the green ones are all thousand of these oriented filters and the black ones are these.",
                    "label": 0
                },
                {
                    "sent": "These 25 and the longer they are the lower frequency there are, so this is.",
                    "label": 0
                },
                {
                    "sent": "This is the kind of weather, weather or feature is and also the orientation in the spatial frequency and now.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yes, one more one unit in this Model 1.",
                    "label": 1
                },
                {
                    "sent": "Why doesn't code global contrast so that these are weights to all of these features plotted for one particular unit and red is positive Blues negative, but there's no negative weights here.",
                    "label": 0
                },
                {
                    "sent": "So what this unit encodes is the global variance variance in away, so it will be on when there's high energy or high variance.",
                    "label": 0
                },
                {
                    "sent": "Along any of these features, in other words, it will.",
                    "label": 0
                },
                {
                    "sent": "You know covariance matrix that basic can expand out along all directions.",
                    "label": 0
                },
                {
                    "sent": "So just call this the global contrast in the image Patch.",
                    "label": 0
                },
                {
                    "sent": "Now we can take this one particular unit can do.",
                    "label": 0
                },
                {
                    "sent": "We should be able to normalize, do game control using by.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is the same image that I showed before.",
                    "label": 0
                },
                {
                    "sent": "I'm just just cut it up into 2020 image patches an I subtracted the mean from each image Patch.",
                    "label": 0
                },
                {
                    "sent": "That's why it looks a little weird and now we can compute the covariance matrix using only that one in Ferd unit.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The model that this one does.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This global contrast one and compute covariance matrix and then normalize each individual image Patch by its own covariance matrix that uses that value of that one.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Unit what we see is yes, it does contrast.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Normalization so.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I go back and forth.",
                    "label": 0
                },
                {
                    "sent": "You can see the low contrast areas become.",
                    "label": 0
                },
                {
                    "sent": "High contrast right?",
                    "label": 0
                },
                {
                    "sent": "OK, so this kind of general gain control, but that's not the.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Interesting, there's a lot more units in the model, so this is the global contrast one.",
                    "label": 0
                },
                {
                    "sent": "But the model also learns that contrast in images can be localized, and it in a way it learns a Gabor.",
                    "label": 0
                },
                {
                    "sent": "Representation of local contrast.",
                    "label": 0
                },
                {
                    "sent": "So again, for intuition would read weights mean higher variance, higher energy in these oriented filters blue means lower variance.",
                    "label": 0
                },
                {
                    "sent": "Actually, these ways can be positive and negative, so the converse pattern is also encoded by this one, so there's a.",
                    "label": 0
                },
                {
                    "sent": "There's a kind of a basis of contrast using.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's a more interesting ones in the model that encode kind of oriented energy.",
                    "label": 0
                },
                {
                    "sent": "So when this particular unit is on, it means that in the image Patch variation along over here and for example directions higher than you than average than the kind of Canonical distribution along these.",
                    "label": 0
                },
                {
                    "sent": "It's lower.",
                    "label": 0
                },
                {
                    "sent": "But then there's also these cross if you can see over here, the vertical ones tend to be high.",
                    "label": 0
                },
                {
                    "sent": "So you can think of this as a higher order.",
                    "label": 0
                },
                {
                    "sent": "You can think of it as a complex elephenor scientists.",
                    "label": 0
                },
                {
                    "sent": "You can think of this as a kind of high order edge detector that's invariant to the polarity or phase.",
                    "label": 0
                },
                {
                    "sent": "Or you can think of this maybe as a boundary detector.",
                    "label": 0
                },
                {
                    "sent": "So it's a little more clear here.",
                    "label": 0
                },
                {
                    "sent": "This unit might like a lot of oriented structure here a lot of horizontal structure here, vertical here, and kind of not a lot of this one.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we can take.",
                    "label": 0
                },
                {
                    "sent": "Then I have more complex units, but I don't really have time to go into this.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now could take this whole model and do what?",
                    "label": 0
                },
                {
                    "sent": "I did with him.",
                    "label": 0
                },
                {
                    "sent": "That one unit.",
                    "label": 0
                },
                {
                    "sent": "So again for each image Patch will take each image attachment for the maximum posteriori value of the whole vector of the latent variable representation in the model of all those wise.",
                    "label": 0
                },
                {
                    "sent": "Use that one to construct a covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "So that's the distribution that the model thinks the image Patch came from.",
                    "label": 0
                },
                {
                    "sent": "And now we can normalize it.",
                    "label": 0
                },
                {
                    "sent": "So basically the multiplied by the.",
                    "label": 0
                },
                {
                    "sent": "C to the negative 1/2.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is just again using only that one unit.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What does global contract?",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, so does this now.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Use all of them.",
                    "label": 0
                },
                {
                    "sent": "You get that so you normalizing out.",
                    "label": 0
                },
                {
                    "sent": "A lot more than just contrasts and it makes sense if you have units that encoding orientation, spatial frequency, and that's all.",
                    "label": 0
                },
                {
                    "sent": "All context that this model learns using this kind of more powerful normalization computation.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, this way.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just contrast normalized and this is.",
                    "label": 0
                },
                {
                    "sent": "Normalized by that whole covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "So there's still some structure left.",
                    "label": 0
                },
                {
                    "sent": "You can vaguely see kind of outline the trees, but see a lot of the oriented structures disappeared?",
                    "label": 0
                },
                {
                    "sent": "And spatial frequency has been equalized.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Machine gun.",
                    "label": 0
                },
                {
                    "sent": "OK, so 5 minutes I want to really quickly just relate this model to a type of high order PBM's that have been used in Italy.",
                    "label": 0
                },
                {
                    "sent": "Also for modeling images just to show you that some interesting relationships.",
                    "label": 0
                },
                {
                    "sent": "And then I'm just going to wrap up so restricted Boltzmann machine have been mentioned here already and generally just a general statistical model, usually defined using energy between indivisible units, a hidden units and a very simple flavors just uses this kind of linear energy function with.",
                    "label": 0
                },
                {
                    "sent": "There's ways that go between visible and hidden, and that's your energy.",
                    "label": 0
                },
                {
                    "sent": "So high order PBM, they go back to the 80s and then use a little a few more times more recently used instead of using just these.",
                    "label": 0
                },
                {
                    "sent": "2nd order or linear?",
                    "label": 0
                },
                {
                    "sent": "How you want to look at it.",
                    "label": 0
                },
                {
                    "sent": "Interactions use higher.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the example I want to talk about is this paper by Mr. Chin Hinton where.",
                    "label": 0
                },
                {
                    "sent": "So this is a little bit trickier.",
                    "label": 0
                },
                {
                    "sent": "Actually divided the visible units into two groups X&Y, 'cause they were modeling an image sequence, and these were two frames image sequence, but the general idea is that your weights now our Model 3 way multiplicative interactions, so in their case it was in these hidden.",
                    "label": 0
                },
                {
                    "sent": "Hidden variables modulated.",
                    "label": 0
                },
                {
                    "sent": "the OR gated.",
                    "label": 0
                },
                {
                    "sent": "The weighting between one frame and another.",
                    "label": 0
                },
                {
                    "sent": "The correlations between one another.",
                    "label": 0
                },
                {
                    "sent": "So instead of having a meet matrix for W have this tensor.",
                    "label": 0
                },
                {
                    "sent": "W and the interactions are even at this multiplicative interactions.",
                    "label": 0
                },
                {
                    "sent": "So another way to write this thing is as as if these hidden units fold together a bunch of these WK matrices to give you the final kind of interaction between XX&Y and this model is more powerful, powerful because it's instead of the hidden units, just modulating the bias of you visible there really modulating the correlations in the data that you see.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So to compare it to kind of this weird architecture that I just described, if you concatenate, if you treat these.",
                    "label": 0
                },
                {
                    "sent": "Just.",
                    "label": 0
                },
                {
                    "sent": "Together as all of your visible units and concatenate them and now you have your visible units and and the hidden units here.",
                    "label": 0
                },
                {
                    "sent": "You get something that's similar to the log likelihood that you see in this model, where this is just the Gaussian log likelihood and the covariance matrix with the inverse covariance matrix that would normally go here has this exponential form so.",
                    "label": 0
                },
                {
                    "sent": "Just it looks pretty similar.",
                    "label": 0
                },
                {
                    "sent": "There are some differences in terms of here working in exponential.",
                    "label": 0
                },
                {
                    "sent": "Space, so when you have no activity or zero activity, you have identity matrix.",
                    "label": 0
                },
                {
                    "sent": "Here here usually working with binary matrices and you're not really so worried about kind of normalizing that distribution.",
                    "label": 0
                },
                {
                    "sent": "But I think they have.",
                    "label": 0
                },
                {
                    "sent": "These two architectures have similar flavor and similar capacity form.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Darling so.",
                    "label": 0
                },
                {
                    "sent": "Just to conclude quickly, I talked about this kind of nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Computations that they are motivated by things that not nearly as that we observe in the visual system.",
                    "label": 0
                },
                {
                    "sent": "And I talked about only but very.",
                    "label": 0
                },
                {
                    "sent": "Early stages, the visual system and some of the things that I showed you.",
                    "label": 0
                },
                {
                    "sent": "You do seen in higher visual areas and I showed you how they can capture context and removing it.",
                    "label": 1
                },
                {
                    "sent": "Doesn't leave you with too much structure, so you might.",
                    "label": 0
                },
                {
                    "sent": "Think hard about like once you do this, which signal do you keep in?",
                    "label": 0
                },
                {
                    "sent": "Which do you model?",
                    "label": 0
                },
                {
                    "sent": "And ideally you would you would be able to.",
                    "label": 0
                },
                {
                    "sent": "Model this contact signal and.",
                    "label": 0
                },
                {
                    "sent": "Maybe the.",
                    "label": 0
                },
                {
                    "sent": "The representation that's left after normalization by by this by this model, so that that's kind of one of the questions.",
                    "label": 0
                },
                {
                    "sent": "That's left for future work.",
                    "label": 0
                },
                {
                    "sent": "Another thing would be really nice to be able to extend these kinds of models to large images.",
                    "label": 0
                },
                {
                    "sent": "This is all modeling, little image patches, and as you know, there's only so much structure you can capture in 20 by 20 pixels, and it's not trivial at all to think about how to extend this to large images.",
                    "label": 0
                },
                {
                    "sent": "So just kind of replicating it.",
                    "label": 0
                },
                {
                    "sent": "Right so and for me, as someone working in our science, the most interesting question is kind of how to map these kinds of computations derived from statistics and natural images.",
                    "label": 0
                },
                {
                    "sent": "Two brain areas and.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so where are they implemented in the brain and for what purpose?",
                    "label": 1
                },
                {
                    "sent": "So I have a question so.",
                    "label": 0
                },
                {
                    "sent": "So the motivation of research is from side of modeling images of statistiques.",
                    "label": 0
                },
                {
                    "sent": "A lot of people here boy interested in also classification recognition.",
                    "label": 0
                },
                {
                    "sent": "And how can we benefit from the study of images statistics then getting useful features?",
                    "label": 0
                },
                {
                    "sent": "Right, I think that questions come up quite a quite a bit right?",
                    "label": 0
                },
                {
                    "sent": "Like when do you want to do unsupervised learning and why do you want to model the data without any task and?",
                    "label": 0
                },
                {
                    "sent": "I mean, I know you guys know this better than I do, but my take on this is that a lot of the tasks that we'd really want these visual systems to do are hard and require very.",
                    "label": 0
                },
                {
                    "sent": "Deep networks that are very powerful is the classification object recognition.",
                    "label": 0
                },
                {
                    "sent": "And when you're training these models, the signals you get back to the training signals are going to be really weak in a big system.",
                    "label": 0
                },
                {
                    "sent": "So you sort of want to get as far as you can using either unsupervised learning or tasks that are just some kind of mid level task and we really don't have very much of this in computer vision.",
                    "label": 0
                },
                {
                    "sent": "Having tests that don't require you know, solving the whole problem at once so.",
                    "label": 0
                },
                {
                    "sent": "I haven't used this for classification and I think some people have have talked about it and might try later, but imagine doing this and kind of building better systems to better understand what's in an image is just the only way to go.",
                    "label": 0
                },
                {
                    "sent": "I don't think you can.",
                    "label": 0
                },
                {
                    "sent": "Yeah, go straight for classification.",
                    "label": 0
                },
                {
                    "sent": "The other part is normalized away sort of normalized.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Somewhere as well.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I have no idea.",
                    "label": 0
                },
                {
                    "sent": "It's.",
                    "label": 0
                },
                {
                    "sent": "Well, yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah, and I don't really know what information is in here either.",
                    "label": 0
                },
                {
                    "sent": "This this looks.",
                    "label": 0
                },
                {
                    "sent": "Wait, but maybe it's just efficient and.",
                    "label": 0
                },
                {
                    "sent": "There's no obvious dependence, but they're just independent, but very informative.",
                    "label": 0
                },
                {
                    "sent": "I don't know what their information between this and the original image actually.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Somebody.",
                    "label": 0
                },
                {
                    "sent": "Try to use translation.",
                    "label": 0
                },
                {
                    "sent": "Reply.",
                    "label": 0
                },
                {
                    "sent": "And then take older teachers responses.",
                    "label": 0
                },
                {
                    "sent": "Within a certain neighborhood and then replace it with the output by itself divided by the distribution center.",
                    "label": 0
                },
                {
                    "sent": "So it's the same thing you're talking about.",
                    "label": 0
                },
                {
                    "sent": "Put the new module in the conversation.",
                    "label": 0
                },
                {
                    "sent": "That is enormous differences.",
                    "label": 0
                },
                {
                    "sent": "Things that didn't work before going to work with their children examples, it seems that enormous effect too.",
                    "label": 0
                },
                {
                    "sent": "Ready to send an?",
                    "label": 0
                },
                {
                    "sent": "Pause.",
                    "label": 0
                },
                {
                    "sent": "Appearances, but it also might be proposed effects of.",
                    "label": 0
                },
                {
                    "sent": "Competition for the filters.",
                    "label": 0
                },
                {
                    "sent": "Yeah, when you different.",
                    "label": 0
                },
                {
                    "sent": "So I think there's a lot of magical things that happened.",
                    "label": 0
                },
                {
                    "sent": "We used.",
                    "label": 0
                },
                {
                    "sent": "This will be.",
                    "label": 0
                },
                {
                    "sent": "Yeah, although I was trying to emphasize here that you might want to keep that signal around and try to figure out what to do with it, because even when you're doing something not not like this full generative model but you're doing divisive normalization, but your weights from one oriented feature to the other ones are maybe not uniform, somehow they learned.",
                    "label": 0
                },
                {
                    "sent": "That's some kind of function that's a complicated function that transforming it could be doing something almost as complicated as this.",
                    "label": 0
                },
                {
                    "sent": "It could be doing other normalization out of context of spatial frequency variation of other things and.",
                    "label": 0
                },
                {
                    "sent": "I don't really know how to relate that information to this stuff that comes out.",
                    "label": 0
                },
                {
                    "sent": "Where was the wedding enabled?",
                    "label": 0
                },
                {
                    "sent": "So why was the whitening enabled?",
                    "label": 0
                },
                {
                    "sent": "It will remove usually when people talk about whitening they talk about global lightning, so you have all your data on sample and then you make the covariance of that data right?",
                    "label": 0
                },
                {
                    "sent": "So if you do that, that's basically natural images.",
                    "label": 0
                },
                {
                    "sent": "You're removing the 1 / F so.",
                    "label": 0
                },
                {
                    "sent": "Versus that.",
                    "label": 0
                },
                {
                    "sent": "The local structure has, yeah.",
                    "label": 0
                },
                {
                    "sent": "Oh oh sorry yeah.",
                    "label": 0
                },
                {
                    "sent": "OK, in my case why?",
                    "label": 0
                },
                {
                    "sent": "First of all, this polarity.",
                    "label": 0
                },
                {
                    "sent": "So this is you're dividing things, but if.",
                    "label": 0
                },
                {
                    "sent": "It's where it is in the waiting space already, but.",
                    "label": 0
                },
                {
                    "sent": "You're not.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "One way to think about it is since you're multiplying by this rotation matrix.",
                    "label": 0
                },
                {
                    "sent": "You're kind of keeping the same quadrant.",
                    "label": 0
                },
                {
                    "sent": "That's not really true, because this happens in the white in space, but you can imagine keeping the same quadrant so that if pixels were white, they'll stay white.",
                    "label": 0
                },
                {
                    "sent": "In other words, when you model the covariance matrix, you're not modeling the polarity of the image Patch.",
                    "label": 0
                },
                {
                    "sent": "So if I give you an image Patch that's white, or that's black is going to have the same covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "And in this case.",
                    "label": 0
                },
                {
                    "sent": "All of these happen to be, say, white, and next them happen to be white black, so I mean, that's a clear statistical regularity here that the model just can't capture, and one way to go would be to have a more complex model of context.",
                    "label": 0
                },
                {
                    "sent": "So what is it?",
                    "label": 0
                },
                {
                    "sent": "Better soon as I know it might make it 150, so it's smaller than X.",
                    "label": 0
                },
                {
                    "sent": "It was what was feasible.",
                    "label": 0
                },
                {
                    "sent": "Basically we tried training this with more neurons and they were not really interpretable and they didn't really contribute 200 the likelihood so.",
                    "label": 0
                },
                {
                    "sent": "It's an arbitrary number.",
                    "label": 0
                },
                {
                    "sent": "It would be different, yeah?",
                    "label": 0
                },
                {
                    "sent": "So basically you have to manually do the number.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        }
    }
}