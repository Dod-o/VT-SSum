{
    "id": "545ktqfjcfr3ekfagwpsxmjgs75kpvsn",
    "title": "Optimization in Machine Learning: Recent Developments and Current Challenges",
    "info": {
        "author": [
            "Stephen J. Wright, Computer Sciences Department, University of Wisconsin-Madison"
        ],
        "published": "Dec. 20, 2008",
        "recorded": "December 2008",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/opt08_wright_oimlr/",
    "segmentation": [
        [
            "OK, so I'd like to start by thanking the organizers very much.",
            "They've done a fantastic job with the organization for.",
            "I had to do almost nothing, except maybe write a talk.",
            "And it's nice to have it in such a beautiful place and thank you all for coming.",
            "So I think the introduction actually set the stage very well for what I want to talk about.",
            "I I'm obviously an outsider to this community, but I've had an interest for going back quite a few years on machine learning problems simply because they are a real challenge for large scale optimization.",
            "So my students, saying, was kind enough to go through the literature from the last three or four years and and scour the literature and find all the papers that.",
            "Had optimization content in the machine learning literature so so in part, my talk is kind of a review of the last three or four years of activity, and then I want to finish up by mentioning some current areas of interest in large scale optimization that I think might be relevant to solving support vector machines.",
            "I'm really focusing on support vector machines here."
        ],
        [
            "Um?",
            "OK, I want to start though by just putting machine learning problems in a bit of a bit of context.",
            "It's really there's really been this upsurge of interest in the last five years or so in what I call sparse or regularised optimization, so it's really a departure from the traditional opt."
        ],
        [
            "Azatian paradigm where you give me a function and a set of variables.",
            "There all well defined, deterministic and my job is to come up with an exact minimum.",
            "That's kind of the traditional paradigm that optimizes it worked with, but very often.",
            "What the user wants is not so much an exact solution.",
            "I'd like to get a solution in some cases quickly.",
            "I'd like a solution that's innocence simple, you know, sparse you maybe not so many nonzeros in the in the solution.",
            "And there are various reasons for that.",
            "One, they might have a restricted computational budget.",
            "Secondly, there's this this reason that you don't want to overfit the data, right?",
            "You want to get this tradeoff between generalizability and empirical fit.",
            "So anyway, I want to start out by talking about a bunch of other.",
            "Problems in this class just to put.",
            "Support vector machines in context, so these are three of my themes.",
            "Obviously the problems in this area are difficult, mostly because of their size, but also because of the Hill conditioning.",
            "If using a kernel and I'll make the comment that the machine learning community really has made excellent use of optimization of being some really excellent adaptations of fundamental algorithms and very creative ideas coming out of your community.",
            "And I really think it's got a lot to contribute to other problems in sparse optimization.",
            "And that will end up as I said by talking about."
        ],
        [
            "Some other problems, so I've already kind of mentioned this.",
            "Talking about sparse optimization or regularised optimization, and just make the comment that if these are your goals, not so much an exact solution, but a simple approximate solution, it really changes the."
        ],
        [
            "The game as to which algorithm is is suitable for what you want to solve.",
            "So here's this quote from Vapnik's thin book.",
            "He says that often there's a tradeoff between equality approximation of the given data and the complexity of the approximating function.",
            "And that's really the message in regularize optimization.",
            "Quite often simplicity is manifested in some senses, is sparsity in the solution vector, and there are a lot of problems that you can formulate as the somewhere you're trying to minimize.",
            "The sum of a model function and a regularization function with some weighting parameter Lambda.",
            "So if your goal were just to find the exact solution, it sort of fits the data as well as possible.",
            "You would leave out the second term.",
            "And just minimize F and that's the traditional paradigm.",
            "But now we've got this regularization term, sometimes it's non smooth.",
            "We often don't have a very good idea what Lambda should be.",
            "So often you want to solve for a variety of values of Lambda and then have some outer loop to pick the right one.",
            "And So what I'll do is quote some examples."
        ],
        [
            "Problems in this class.",
            "One that you might have already heard about in the main conference if you went to Emmanuel Candace's talk is compressed sensing and this has been a big area in the last three years or so.",
            "It's been attacked by theoretical computer scientists, statisticians, electrical engineers and is also being interested in the optimization problem in actually solving this thing.",
            "It's a very simple problem to state.",
            "It's sort of an undetermined system of linear equations, but you want to find a sparse approximate solution.",
            "And it turns out that under certain assumptions on the Matrix A, the way to do that is to add on Lambda times the one norm of X OK, and it turns out that for an appropriate choice of Lambda, this this is as good as applying some sort of cardinality constraint on the number of nonzeros allowed in X, but it's a much simpler problem then a cardinality constraint problem.",
            "So obviously by manipulating Lambda you can control how sparse the solution, as if Lambda zero you just get a, usually a solution whose number of nonzeros equals the number of rows in a.",
            "If Lambda is big, you get X = 0 as a solution.",
            "So Lambda somewhere in between is usually what you want and this also ties into the lasso idea.",
            "In statistics at dates from mid 90s or so for."
        ],
        [
            "Selection and statistics.",
            "Here is another example image denoising.",
            "So given an image over a 2 dimensional domain, you want to basically remove the noise from the image by doing a least squares fit.",
            "But you also want to regularize it by.",
            "You assume that you're dealing with an image with large sort of flat areas and some a few edges, so you basically want to remove all the special in the flat areas and so the norm.",
            "The regularization term involves a spatial gradient of you.",
            "The thing you're trying to recover, and you integrate that OK and again the Lambda controls how close are you to the image.",
            "If Lambda is zero, obviously U = F. But as you increase Lambda, you typically get more more and more cartoon like image OK and it turns out that again.",
            "We've shown in other recent work that if you apply grading projection to the jewel of this, you can get very."
        ],
        [
            "Approximate solutions very quickly.",
            "Here's another much more applied example is getting something that I've worked on a number of us Wisconsin have worked on is radiation treatment planning in cancer.",
            "So when you've got a patient with a tumor inside their body, you're trying to fire beams of radiation into the body from all sorts of different angles, and the aim is that you're trying to ZAP the tumor, but not kill too much of the tissue surrounding it.",
            "So that's the idea of moving the beam around is that it's sort of focused on the tumor, but the dose to the normal tissue gets spread around.",
            "So there's an infinite number of possibilities from how you can choose the beam angle or beam shape, the exposure times, and so on.",
            "And you want to come up with a plan that just does it in three or four shots, or five shots or something for all kinds of practical reasons.",
            "You want you don't want the patient lying on the table for ages while you kind of fiddle around with the beam and so on.",
            "So you're looking for a sparse solution."
        ],
        [
            "Again, OK, and here's another one matrix completion that you may also have heard about.",
            "The idea here is we've got an unknown M by N matrix X.",
            "We've got a certain number of linear observations on that matrix.",
            "For example, we might be observing some elements of the matrix, and we're looking for an X that's low rank, but consistent with the observed data or nearly consistent with the observed data, and this is very much like compressed sensing.",
            "You can see from the way I've written it here instead of the one norm here, we have what's called the nuclear norm, which is the sum of singular values.",
            "And that turns out to be a good surrogate for the rank of X OK?",
            "And some of the algorithms are actually.",
            "This is an area of active research, but some of the algorithms that are actually close to the ones that we."
        ],
        [
            "For compressed sensing, so I've given you four or five examples of regularised optimization problems, and now we're going to talk about support vector machines.",
            "But before we do, I'll just note that all of these applications are very different properties, and there's no one size fits all algorithm.",
            "But there are some sort of algorithm approaches that do translated across different problems, and so there is a bit of cross fertilization, and I think this is one way that all the work in SVM that's been done can be kind of leveraged in some of these other applications.",
            "Potentially, I'll note also that duality is often a key to getting a formulation that you can solve.",
            "And we'll see that we certainly see that in SVM, and as I've already noted, you often don't know what Lambda is in advance.",
            "You want to solve for a bunch of Lambda values.",
            "Often I'll just make as a general comment, often as a choice between methods that have fast asymptotic convergence.",
            "These are things like interior point methods or sequential quadratic quadratic programming, but their individual steps are very expensive and at the other extreme there are methods that have very slow convergence, asymptotically linear or sub linear, but the steps are very cheap to compute, and they may be only need approximate gradient information so often there's kind of a attention or a.",
            "A hybrid of these two is the thing that's."
        ],
        [
            "Appropriate in different circumstances.",
            "OK, so I'm taking it there.",
            "Probably most of you.",
            "I've got a fair amount of sort of elementary material and SVM, but probably it's quite familiar to most of you, so I won't dwell too much on it.",
            "But really I'm just trying to fix notation here.",
            "I'm going to assume you've got a bunch of feature vectors in RN, an binary labels an in the linear problem you're looking.",
            "I've got the eye on the wrong place here in the linear problem you're looking for a classifier, which is a weighted sum of the components of the feature vector.",
            "Plus some intercept B.",
            "An ideally would like to find a W such that you correctly classify every feature vector according to this criterion here, so inessential trying to keep the classification errors small and I call denote those by Zita.",
            "I.",
            "And what do you mean by small?",
            "Well, you have some.",
            "Usually it's a separable function.",
            "It's increasing in the Zetas.",
            "OK, so here's a classic formulation where you have a regularization term, which in this case is smooth, involving the W. You have a penalty term, which in this case is just an L1 norm of the classification errors, and you have some constraints that define the Zita eyes.",
            "And as you all know, using this term here gives you a in the case of perfect separability, gives you a maximum margin separating hyperplane, so this is a paradigm that a lot."
        ],
        [
            "People have studied.",
            "And also you can take the jewel of that as you know the problem I wrote on the previous slide is a quadratic program, it's convex.",
            "The jewel is also a convex quadratic program.",
            "It's got this coefficient matrix K which is the inner products of the feature vectors, appropriately scaled by labels.",
            "It's got these bound constraints on the alphas, which are LaGrange multipliers for the.",
            "Constraints defining the classification errors.",
            "It's got this additional constraint linear constraint which will talk more about in a moment, and if you write down the KKT conditions for the primal or the dual, you get this relationship between the LaGrange multipliers.",
            "And the primal weight vector that you're looking for OK, and the Intercept B turns out to be the LaGrange multiplier for this problem in the dual formulation.",
            "So given this, you're able to write down the classification function.",
            "If I just substitute for W in that previous slide.",
            "I can get directly from the almost directly from the jewel.",
            "I can get a classification classifier.",
            "OK, so that's my."
        ],
        [
            "Notation now you will know that.",
            "The beauty of going one of the beauties of going to the dual formulation is you can do the so called kernel trick.",
            "And that's that corresponds to projecting into a high dimensional space by some function Phi, and you can do all the classification in that space.",
            "So it's just the primal formulation applied to this fire vexi rather than XI.",
            "OK, and that leads to if you use that, use defy.",
            "The kernel matrix is redefined.",
            "This way everything else stays the same in the dual formulation and the classifier is redefined this way with just 5X replacing X OK and then of course is this classic observation that well, actually you don't even need to use file explicitly at all.",
            "Or do you really need the inner products involving 5X with five at another point X bar?",
            "And you can do everything in terms of a kernel function.",
            "Which is a function of of X&X bar, and there's this theorem that says that if you pick a function, a kernel function that's continuous, symmetric, positive definite, then there is afie out there somewhere such that K is equal to five.",
            "Transpose five.",
            "OK, so everything can be done now in terms of the kernel function, K never need to look at that FI."
        ],
        [
            "So that's that's one of the reasons that the dual formulation is very popular.",
            "The classifier, in terms of the kernel can be written like this, so you can just plug in your X and.",
            "Evaluate F and decide whether that should be a plus one or minus one.",
            "So here are some popular kernels.",
            "Probably the Gaussian is linear in the Gaussian.",
            "That seems to be the two most popular, although a few others crop up in the literature.",
            "I'll just make the comment that this kind of the Gaussian kernel in particular is often quite ill conditioned, and this is a challenge.",
            "If you just using a naive saying interior point method on the dual, it sort of doesn't like the fact that the.",
            "The Colonel is."
        ],
        [
            "Well conditioned.",
            "OK, now what I want to do is talk about algorithms for solving those two formulations, the primal and the dual that I just mentioned.",
            "Particularly things that have cropped up in the literature in the last since a 2005.",
            "Although I mentioned one or two things from before that.",
            "It still seems to be an active area, although people have been beating on it for about 10 years.",
            "A lot of the methods are based on optimization methods, or at least we can think of them.",
            "We can put him in an optimization setting.",
            "How do we decide which one is best?",
            "Well, of course it all depends on what you're what you're looking for.",
            "You can talk about how fast are these methods in finding a solution of a given quality, measured according to the error rate, or how close they come to the optimal objective value and certain amount of time.",
            "Alot of people in your community seem to like to do theoretical analysis of efficiency to see a lot of that.",
            "In the literature.",
            "There are also a lot of nitty gritty issues that you need to address in implementing these things because.",
            "Storing one of these kernel matrices, usually for a reasonable sized datasets, isn't really an option.",
            "And so there are all sorts of things about caching and selective.",
            "Storing pieces of the matrix selectively.",
            "You always like algorithms that are simple 'cause they're easy.",
            "You can program in a page of Matlab and get some nice results, so that's you know practical priority, and there's been a bit of work recently on parallel parallel is ability, which I'll say a little bit more about."
        ],
        [
            "OK so first let me talk about algorithms that solve this dual formulation.",
            "And as I mentioned, it's a convex QP.",
            "It's mostly got bound constraints.",
            "Passion can be a bit of a challenge.",
            "Not just because the soil condition, but because it's usually dance and and also hard to evaluate.",
            "If you've got to evaluate the Hessian piece by piece and you have to take an exponential every time.",
            "That's kind of expensive.",
            "And also, I'll note that this linear constraint is kind of a nuisance.",
            "There's been a lot of work around for a long time, unbound constraint problems, but throwing in that linear constraint makes things."
        ],
        [
            "A little bit more interesting.",
            "OK, some very recent work here on coordinate descent for the dual formulation so.",
            "This and other papers get deal with this constraint by just throwing it away.",
            "OK, the Y transpose Alpha equals 0 so that that correspond.",
            "You can think of this in two ways.",
            "You can think of this either as adding a 1/2 B squared term to the objective, or you can think of it as augmenting the feature vector by one component, namely at one.",
            "And just.",
            "Making the last element of W equal to the Intercept.",
            "OK, so it means that the classification function no longer has a B in it, it's just got W transpose X.",
            "So the problem simplifies the primal problem.",
            "Simplifies in this way.",
            "The dual problem simplifies.",
            "In this way it gets rid of this extra constraint.",
            "So now you can.",
            "Do you know all kinds of gradient projection type ideas for this this particular paper and I can give you more precise references later on if you like.",
            "I had a big pile of these things, but this particular paper sort of cycles around the elements of Alpha in this formulation.",
            "Anne fixes all the others and just does a solve.",
            "Just in a single element of Alpha, OK?",
            "So it's just a scalar constraint problem.",
            "You can solve it in closed form.",
            "So it updates that element of Alpha updates K times off.",
            "It has to keep track of K times the latest Alpha.",
            "So if you change one component of Alpha, you have to evaluate a column of K and update K Alpha OK. And it sort of cycles around.",
            "I think there's also a variant where they pick I at random.",
            "Now this is an example of an algorithm, but of course it takes a lot of iterations, but each one is relatively cheap and you can think of imaginative ways to speed up, for example, this part of it here.",
            "For example, if you know that certain alphas are almost certainly at either the upper bound of the lower bound, you don't really need to evaluate K Alpha for those components.",
            "If you think they're going to be stuck there, that's sort of a shrinking idea that."
        ],
        [
            "Keeps cropping up in other methods as well.",
            "Here's another approach due to Diane Fletcher.",
            "These are optimized most optimization people who've worked a little bit on this problem, and their idea is you don't throw away this constraint.",
            "You consider the standard dual formulation.",
            "But you do gradient projection OK, and the idea of gradient projection and Professor Bertsekas is kind of a pioneer in this area.",
            "I don't know if he's here yet, but the idea of gradient projection is you look in the negative gradient direction of the objective.",
            "Take a step in that direction and then project the resulting point back onto the constraint set.",
            "Ann, you manipulate the step length gamma here to get usually to get a dissent of some kind.",
            "OK, now the catch with this.",
            "Applying gradient projection in this context is that this major is more complicated than you want it to be.",
            "Because of this, this extra linear constraint.",
            "So it's actually a little bit more work to project onto a mega then it would be if you just had bounds.",
            "If you just have mounds, it's trivial.",
            "You just do a clipping procedure, but now it becomes a little bit more tricky.",
            "You sort of have to solve a little subproblem and do this projection.",
            "But it's not too hard.",
            "It's still fairly cheap.",
            "OK.",
            "So how do you choose the gamma?",
            "So there are a bunch of different techniques for that.",
            "One is that the basic gradient projection technique is that you make some initial guess of gamma.",
            "You do this, you see if the Alpha is better in terms of decreasing queue.",
            "If it's not, you say cut gamma in half and try again and you keep cutting back until you get a descent of some kind.",
            "So that's one of the standard approaches.",
            "But these are authors, tried using Barzelay born formulated to choose the gamma.",
            "Now Buzz, Light Boy and I'm not going to spend too much time explaining it, but it's a it's a method that's been around for about 20 years and the idea is it's sort of like a quasi Newton method.",
            "Where you choose the step length to kind of mimic the behavior of the inverse of the Hessian.",
            "OK, because the classic Newton's method is to take Alpha minus Hessian inverse times gradient.",
            "That's classic Newton method.",
            "So buzz like boy and say, well, let's choose the gamma to kind of act like the inverse of the Hessian.",
            "Now what do you mean by that?",
            "Well, it means that you want gamma to sort of behave as the Hessian would have behaved over the last step that you took.",
            "And how would the Hessian of behave well if you multiply the Hessian times that last step?",
            "You should get approximately the difference of the gradients by Taylor's theorem, so those sort of choose the gamma so that it in the least squares sense it satisfies gamma times S approximately equal to L to.",
            "Why OK?",
            "That's one of the motivations, and so you get this very simple formula for gamma.",
            "You plug it in.",
            "Sometimes the thing goes uphill, usually just accept it anyway, even if it gives you an increase in the function.",
            "And often this is."
        ],
        [
            "Converges faster than a monotone method.",
            "And we've been using Buzz Libor and methods in a bunch of other applications as well, with some success.",
            "OK, here's another thing that keeps cropping up in algorithms for solving the dual.",
            "This idea of decomposition.",
            "It's very natural, because when you've got a lot of data, Alpha is a very big vector and it's very hard to sort of tackle all the components at once.",
            "So the idea of decomposition is you just.",
            "You have an outer loop where you pick a subset of Alpha components.",
            "And you solve a reduced quadratic program on that subset.",
            "OK, you solve it approximately, usually an then, and then you go back to the outer loop and pick another subset.",
            "Might be completely different, or it might just change if you component.",
            "So there are many different variants.",
            "In fact, most of the codes that are floating around out there for this problem make use of this strategy.",
            "And they sort of differ by what's your strategy for picking subsets?",
            "How big do you make the subsets and what strategy to use to solve the reduced subproblem when in just the subset of the variables?",
            "And this has been around since at least 1998.",
            "There's the Smosh idea where the sort of the extreme idea where the subset just has two components.",
            "And you pick them to get a decrease in the objective.",
            "Maybe it's not the extreme.",
            "I guess the extreme is just to have one component, and that's more like that decomposition strategy that I mentioned earlier.",
            "SVM light that usually chooses a fairly small subproblem size, and it picks the components to include by some first order heuristic, again based on a somewhat greedily on which components will give you the biggest dissent in the objective.",
            "According to some linearized model.",
            "I just note that this heuristic uses a cardinality constraint.",
            "It sort of fixes the number that you want to put in the subset and tries to solve this cardinality constraint linear program.",
            "And I just wonder if you could use an L1 penalty instead of as a surrogate for the cardinality constraint.",
            "I don't know if that's been tried, that would actually give you a simple linear program to solve.",
            "OK, there's another code out there by our Italian collaborators where they use a gradient projection method on the subproblem.",
            "In fact, I think they use the method.",
            "One of the methods they use is that Diane Fletcher method that I talked about on the previous slide and they have a."
        ],
        [
            "Our little implementation.",
            "Actually, the Lib SVM code uses in Smosh type framework with kind of a variety of heuristics for picking the two components to relax.",
            "And then they solve a 2 dimensional QP.",
            "To get this step.",
            "And as I mentioned earlier, all of these ideas are really interesting.",
            "But to make them work practically you have to do a lot of nitty gritty stuff such As for example, shrinking.",
            "This idea has been around for quite awhile.",
            "This is where you know you sort of make some guess of which alphas are strictly between the two bounds, and you try to focus on those.",
            "In other words, you try to the alphas that look like they're either at zero or C. You sort of temporarily ignore them, and you can save a lot of computer time by doing that.",
            "You might have to do a final optimality check or periodically check that you really are making a good decision about.",
            "Which ones need to be at their bounds?",
            "But essentially you can ignore them for most of the computation.",
            "And then, as I've also mentioned, this idea of caching you only want to sort of store parts of K or the kind letter of current interest.",
            "So in general these are I've described at least four codes here.",
            "They've all been used well for the last 10 years quite successfully.",
            "One point to make though is that the solutions are often not particularly sparse, particularly using, say, a Gaussian kernel.",
            "The alphas in other sparse optimization applications, that number of nonzeros in the solution is often a tiny sliver of the total dimensionality of the solution, and that's often not the case here.",
            "It's not a typical to have 10% of the Alpha.",
            "Non zeros, it seems to me.",
            "And so decomposition can sometimes struggle with that.",
            "OK, 'cause it's gotta pay attention to substantial subset of the solution.",
            "So there are the sum."
        ],
        [
            "Opinion out there that these methods sort of running can run into a wall if those size of the data set gets really big.",
            "OK, I just mentioned active set methods.",
            "This is a classic approach to QP.",
            "Catcher Scheinberg has a paper from just a couple of years ago where she does a very careful implementation of an active set method and gets seems to get pretty good results.",
            "The idea of an active set method is that you.",
            "You sort of.",
            "You maintain an active set of solutions that seem to be interesting.",
            "That is, the ones that are strictly between their bounds, and you change that set, typically by just one index at each iteration, and you do that either by bumping into abound, in which case the active set drops by one, or by moving away from about, in which case you throw something else into the active set.",
            "So the basic story is you have to maintain a factorization of the interesting part of K. So you have to do nice linear algebra for updating the factorization and so on.",
            "And she also uses a strategy and seems to on some test problem seems to get good results.",
            "There's another paper by Shelton, but mostly actually by a couple of the authors are optimization people, including my colleague Danny Ralph.",
            "And I do sort of a similar thing, but to a minmax formulation where they leave the B in the jewel so they have sort of a combined primal dual problem.",
            "And they do an active set on that using some of the same ideas.",
            "One of the thing."
        ],
        [
            "Just to mention about this approach is that first of all it can be warm, started.",
            "Active set methods are very good at that.",
            "If you have a solution that's that's optimal for one of these commands, C and you change the slightly, you can use your previous solution as a warm start to get to the you know to modify it quite efficiently, and the other thing is that it's useful in this incremental context.",
            "We're bringing data points in one by one or in batches."
        ],
        [
            "So in other words, you can solve the problem for you, know the current Alpha.",
            "You can then throw in a bunch more data.",
            "All that corresponds to doing is increasing K by a certain number of rows and columns and increasing Alpha by a certain number of elements, and so you can just augment the problem and then just keep going from there.",
            "OK, so it's a very."
        ],
        [
            "Natural framework to use if you want to sort of income."
        ],
        [
            "Mental learning.",
            "OK, and then finally I'll mention Interior point methods.",
            "This goes back a while too.",
            "Fine and Scheinberg paper from 21 where they just apply the classic primal dual interior point approach.",
            "The critical operation in Interior Point methods are distinguished by being by having a small number of iterations, but the iterations are very expensive.",
            "Typically, in this case you have to solve a linear system of this form with kernel plus some diagonal some positive diagonals.",
            "At every iteration.",
            "And I sort of have to do well.",
            "I first take note of the fact that K is often ill conditioned, so you can come up with a low rank approximation, decay and sort of use that as A to approximately solve that linear system.",
            "So how do you get this low rank approximation?",
            "Well, good question.",
            "There are.",
            "There are some, there's been some recent work on this trim type methods that use sampling that can get sort of approximate decompositions of a large matrix like this, and.",
            "And I think these are being used in this context.",
            "You can also just call the I guess command in Matlab, which applies in an ALDI method which will just find you approximately the top five eigenvectors and eigenvalues.",
            "So you could do that as well.",
            "OK."
        ],
        [
            "I'll just note that if you if you just want to approximate K by VV transpose for some long narrow V and you just want to solve the problem for that approximation, that turns out to be a very simple problem that you can solve efficiently within active set method.",
            "So you can just call seaplex the active set one of the active set options in.",
            "See Plex, you can just introduce some extra variables gamma to represent V transpose Alpha.",
            "So this is a problem with bounds in a few equality constraints.",
            "And so we've tried this.",
            "The quality of the classifiers often is not that great, so you seem to lose quite a bit by ignoring by taking just the top five or 10 or 20 or whatever it is eigenvectors."
        ],
        [
            "But it certainly gives you a lot simpler problem.",
            "OK, there are some other issues with the dual.",
            "I'll just skip through this 'cause I gotta finish in what 1520 minutes?",
            "Something like that.",
            "OK so I'll just note there's been some work on parallel implementations.",
            "There's another thing here that if you want to multiply by K, that's quite expensive.",
            "If using a Gaussian kernel, there was a fairly recent paper on using sort of an approximate.",
            "Gaussian kernel times vector multiply.",
            "It seems to work well if you've got very short feature vectors.",
            "It's not so clear if it's sufficient otherwise, but that's certainly something to work on if you can get away with his expense of having to calculate exponentials all the time, that would certainly be good.",
            "There's a very recent paper on a GPU implementation using a graphical processing unit, and there's been a bit of a trend recently in numerical analysis on making use of this commodity hardware, which is in pretty much all PC's these days and.",
            "Is very cheap, but if you're doing certain restricted numerical operations on."
        ],
        [
            "They can.",
            "They can be done very very fast.",
            "OK, I want to say a few words about.",
            "There's been some recent activity, particularly or I think, a bit of a resurgence in actually addressing the primal formulation rather than the jewel that I've been talking about.",
            "So this is going back to that formulation I wrote up earlier on the limitation of this approach.",
            "It would seem is that you lose the ability to use a kernel.",
            "But it's actually not so clear that you you do, because you could do the projection manually.",
            "In a sense, you could go back to trying to pick the feyen, projecting the feature vector into some large space manually, or there's a paper by Chappelle where he sort of he brings the kernel in, and sort of implicitly replaces the feature vector by this kernelized feature vector.",
            "So in a sense, he takes the RN natural feature vector and expands it into a vector of length big N, which is the number of data points by using the kernel.",
            "To do the transformation.",
            "To get an efficient problem he has to replace this term by a weighted norm.",
            "OK, but he ends up with a formulation that looks a lot like that and sort of does incorporate the kernel."
        ],
        [
            "It works with that.",
            "OK, but all I'm going to talk about is going to ignore that.",
            "I'm just going to assume you use the given feature vector, whatever it is.",
            "OK, so here's one approach is being tried by a couple of people in recent years, York, Eames, and these authors in 28?",
            "And that is to recognize that this primal problem is a nice, very nice smooth problem, plus a piecewise linear convex problem function is piecewise linear, convex in its arguments, and there's a classic approach for solving problems with this piecewise linear convex structure, and that's his cutting plane approach.",
            "The idea of a cutting plane approaches that you visit a sequence of iterates, and at each iterate you find a subgradient and that sort of gives you a lower bound on the function that you're actually trying to minimize.",
            "So you end up you start off with this very complicated piecewise linear problem, and you build.",
            "You build up an approximation to it by just adding cuts.",
            "So you're sort of building up this lower bounding approximation.",
            "And it can be implemented efficiently provided you can find subgradients cheaply.",
            "And in fact, in this application for this problem, it's very cheap to find subgradients, so it's quite quite a natural thing to try.",
            "Your keys implements this.",
            "In this code SVM Purf there's an observation made by these authors that these methods, as in their purest form, are not guaranteed to give you monotonic improvement at every iteration, so they have a variant that sort of does line searches and so on.",
            "To guarantee some kind of monotonicity, there are probably a lot of other things you could do to soup these up.",
            "You could add multiple subgradients at each steps.",
            "You could have iterates that remove old subgradients that don't seem to be relevant anymore.",
            "I've worked on these kinds of methods in the context of stochastic programming about 8:00 or 10 years ago, and there's quite a lot of things you can do to sort of improve."
        ],
        [
            "Heuristically, here's another approach.",
            "Again, it's been around, and one of our latest speakers is a pioneer in this area, and that's using stochastic gradient descent, and this is very much in the tradition of algorithms.",
            "Take a lot of steps at a very cheap to calculate, but can be good at finding approximate solutions.",
            "So the idea here seems to be that you just take a very small subset of the data.",
            "And write down an approximation to the primal problem that you really want to solve and use a subgradient of that as a search direction and take a step along that direction, OK?",
            "The in the extreme case, you can just take a single data point and use that as the basis of a of the search direction.",
            "So this is cheap if the icon is a small, there's some.",
            "There's a code Pegasus that's again quite recent, where they choose ikk ranging in size from one up to something much bigger.",
            "And I have this enhancement where after they take each step, they pull it back and projected onto a bull, because you know ultimately you're looking for a W with a fairly small two norm, so they explicitly do that after they take a step, and they can sometimes help the performance, and we've noticed that actually in the image denoising application."
        ],
        [
            "Projection seems to really help.",
            "There's another approach recently that involves taking a subgradient, but then putting in a curvature term that you guessed by a quasi Newton approximation.",
            "I'm not going to say too much about that.",
            "But it's nontrivial because you have to make sure that you get a descent step so it's not.",
            "It's not as simple as just taking a subgrade and adding on a second order term, the performer."
        ],
        [
            "Seems to be similar to cutting plane methods.",
            "There's a little bit I've got a few slides on alternative formulations.",
            "Of course, the formulation I've been discussing is not the only game in town you can square the violations like this and get a problem of this form.",
            "Let's see, and there's been some work by my colleague Living magazine Arian dating back quite awhile where and more recently it's been picked up by some other people where he basically does a Newton type method on this formulation, throws out the Intercept term.",
            "So now he's got sort of a quadratic looking function that's unconstrained.",
            "The only problem is it's not totally kosher because there's a discontinuity in the second derivative.",
            "But you can kind of ignore that.",
            "Take some element of the Hessian space generalization space, and do a Newton step on that on that basis."
        ],
        [
            "And if you include a line search, it often often seems to workout being quite efficient.",
            "The main operation is to calculate the Newton step you have to solve a problem like this where this the matrix X is the matrix built up from all the feature vectors.",
            "So this is sort of a regularised least squares problem.",
            "You can do, you can solve that using some sort of iterative method.",
            "OK, so there's been some."
        ],
        [
            "Success with that approach, there's another approach again, my colleague.",
            "All the magazine is being very much involved in this idea, where instead of a W squared term, you've just got a one norm of W here, and the advantage of this is you just got a nice linear program.",
            "You can't just usually hand this to see Plex, though, because if you're dealing with humongous datasets this can be too big for seaplex to deal with and to dance, and so you."
        ],
        [
            "You sort of have to do.",
            "Some sort of customized LP solvers to make that work.",
            "I'll just mention this.",
            "There's been there's an idea in the statistics literature from a couple of years ago of the elastic net where you are looking for a sparse weight vector.",
            "But at the same time, you want to keep this property of, you know, separating hyperplanes and so on.",
            "So the idea is to throw in both an L2 term ananel one term.",
            "Into the classic primal formulation.",
            "Does anyone has anyone tried this in this context?",
            "Do you know?",
            "I'd just be interested to see because it's been the statisticians have been using this, they they.",
            "The story is that they think it does a better job with Lasso at group selecting elements of the weight vector.",
            "So if you've got a number of features that are somehow correlated, apparently this kind of formulation is good at selecting or deselecting those features as a group.",
            "OK, so it."
        ],
        [
            "Would be an interesting thing to try.",
            "Logistic regression, I just thought I'd mention this is comes from some recent work I did with Grace Wahba and her students at Wiscconsin where, where, where again we've got labeled data.",
            "Feature vectors and labels and so on, and we construct a log likelihood function and we want to maximize the log likelihood.",
            "But we're looking again for a sparse feature vector.",
            "We're trying to identify the important features and typically we're looking for a very small number of features, like 5 maybe.",
            "But our feature vectors is hugely long because one of the applications of this was trying to predict whether someone would get rheumatoid arthritis based on their genome.",
            "OK, so you'd be looking at 10,000 base pairs from a genome sequence together with some other risk factors, and you'd be looking not just at those 10,000 pairs, but at interactions between them.",
            "So you had a potentially at least polynomial.",
            "You know 10,000 squared or 10,000 cube number of features, so we had a.",
            "Very big problem.",
            "There's all sorts of ways of reducing that, but in the end we so we add on this regularization term, which is an L1 term.",
            "So we've got a negative log likelihood in an L1 term, and we."
        ],
        [
            "Up using sort of a gradient projection approach.",
            "Souped up a little bit with second order.",
            "Second root of information so we sort of take steps based on minimizing this very simple approximation to the problem in which we approximate the Hessian of L bye nice separable term.",
            "And then periodically we sort of explore the manifold that we were on, so we would look at the current set of nonzeros of features that are away from zero and we take reduced Newton steps in those features and we had some sort of randomization as a proxy for shrinking and various other heuristics, and we solve reasonable size."
        ],
        [
            "Problems, but I have no doubt that we could do.",
            "There's more we can do.",
            "OK in the last few minutes I want to talk about a couple of tools that have come up recently in the optimization literature that have sparked a lot of renewed interest.",
            "Then I'm not sure if they've been people have tried to apply them to SVM yet, but they might be of current interest and.",
            "The first one is this work of Nesterov and the Russian School on Optimal 1st order methods.",
            "OK, so these are methods where you're taking steps based on function values and gradients.",
            "You're not trying to look at second derivatives and your overall aim is to sort of.",
            "You're often working with a given number of evaluations.",
            "You assume that you can't do more than an evaluation, so you ask the question.",
            "What's the most highest level of accuracy I can get in a given number of evaluations?",
            "So it's more.",
            "It's take more of a global perspective rather than this very greedy local perspective that you typically do in optimization where you just say OK.",
            "Here's my search direction.",
            "How much bank and I get on this iteration out of this search direction.",
            "You sort of take a more global view of aiming, you know, in the long term, to get as accurate a solution as possible.",
            "OK, so I've got a couple of slides, is very briefly introducing this.",
            "Often they make use of parameters.",
            "I'm just going to be assuming here trying to minimize the function F which is fairly smooth.",
            "So often the Lipschitz constant for the gradient enters into the algorithm and the convexity parameter for app.",
            "So the strong convexity parameter for F. OK, now you don't have to know these.",
            "There are variants of these methods where you can approximate them OK.",
            "So."
        ],
        [
            "A classic gradient method is where you just take a step in the negative gradient direction.",
            "I think it's all very well known, particularly by people in this community that if you take a very short step length 1 / L, where L is the Lipschitz constant for the gradient, you get to send it every iteration, and you're able to show that after K iterations, the difference between your objective and the optimal objective is order 1 / K, so it's sub linear.",
            "If you've got, if you know that your function F is strongly convex, you can do a little bit better.",
            "OK, you can get a sort of a geometric convergence rate.",
            "OK, now optimal methods try to do better than that.",
            "They try to get a K squared.",
            "In this case Anna better geometric konst."
        ],
        [
            "In this case.",
            "OK, so so they typically get these kind of rates if you don't.",
            "If you don't have strong convexity, get 1 / K ^2.",
            "If you do have strong convexity, you get a significantly better."
        ],
        [
            "Each term here.",
            "Now how do you get to that?",
            "Well, it turns out you pay a price in at.",
            "The algorithms become much more complicated than just taking a short step in the negative gradient direction.",
            "The methods typically maintain not just a single sequence of X is but a sequence of X is wise, and sometimes Y, sometimes 3 sub three sequences that are all kind of interleaved and interacting, and so on.",
            "But here's a simple instance of of a method that works for a strongly convex function.",
            "So you maintain two sequences XC and YC.",
            "You get the X case by doing the classic short steepest descent.",
            "From the YK.",
            "But then you update way K by doing kind of this weighted average between the last two X case.",
            "They don't ask me to explain why this works.",
            "Even people at work on these methods have a very hard time developing an intuition for what's going on.",
            "The analysis is very, very technical.",
            "OK in the weekly, even if you go to the weakly convex case and you don't have mu equals 0.",
            "The method becomes significantly more complicated.",
            "Again, you maintain the vexes and wiser definition of the X is the same, but now the definition of wise depends on this sort of very complicated scalar formula.",
            "OK, which I won't even try to explain."
        ],
        [
            "OK, and it sort of takes off from there.",
            "It becomes even more complicated after that, so you can extend these methods to the case where you've got a constraint where you've got a regularization term, and even where you've got nonsmooth terms, and I think this is where the application to SVM might come in.",
            "People have applied these to compress sensing problem."
        ],
        [
            "And they seem to work fairly well.",
            "OK, here is the Nesterov as a 2005 paper which I think is become very influential already, and he considers convex problems where you're trying to where the objective can be expressed.",
            "In this way F of X you can get it by plugging X into this this function here and taking the Max over some other set.",
            "So it's like a min Max type formulation.",
            "You're trying to minimize the maximum minimize over X and maximize over you.",
            "OK.",
            "So this is the problem you really want to solve, but he convexified it by adding to this function a strongly convex function D and a regularization parameter mu.",
            "An if you maximize over this function instead, you sort of get a smooth version of F which will call F of mu, so you're able to replace this function, which is often non smooth with a smooth variant, and then he essentially applies one of those optimal methods to this smooth function OK, and is able to show that given an iteration budget, suppose you're told you can only take N iterations.",
            "He's able to come up with a method first of all for choosing the appropriate value of mu, you in fact choose it according this complex, complicated formula that.",
            "Order of an inverse and then you run an iterations of your method and you end up with an accuracy of order 1 / N OK. Now these seem like very weak bounds, but they."
        ],
        [
            "Seem to often work quite well in practice.",
            "And Lastly, I'm going to end up with another tool that we found to be.",
            "It's kind of related to the method I just mentioned.",
            "That we found to be very useful in the image denoising problem for solving min Max type problems like the one I had in the previous slide.",
            "So the idea is that you take alternating gradient steps in the primal and the dual space, or in the men in the Max space and each time you project back onto the feasible set like that Pegasus code does.",
            "OK, so you take a step in X according to some step length gradient projection step you project back onto Big Axe, you take, then you take a step in V. Projecting back on the Big V OK, and.",
            "I worked with Tony Chan and a student on applying things like this to image denoising.",
            "The student ran off and sort of started doing really crazy things like instead of picking these step links to go to zero, he picked one of them to go to Infinity instead, and amazingly it gave much better performance.",
            "So he wrote that up himself and publish it separately, which is which is great, 'cause I would never have."
        ],
        [
            "Along with it, if you'd have, you try to get me involved.",
            "So, but we have no idea, and I think it's fair to say that the student doesn't have any idea either.",
            "Last time I checked, he was working on Wall Street, but I'm not sure if that's still true.",
            "We have no idea why this works and we've I've had.",
            "I've tried to involve a couple of my colleagues in trying to understand why this method works so well in practice and we don't know.",
            "OK, but obviously it's the kind of thing you could also try on a primal dual."
        ],
        [
            "Formulation of SVM OK."
        ],
        [
            "And I'll finish there.",
            "So thanks very much for your attention.",
            "We have this movie.",
            "Anybody?",
            "Summarize.",
            "Situations regular SPM?",
            "Yeah, I wouldn't attempt to do that because 80% of the people in the room would jump down my throat, but I don't have.",
            "I don't have enough personal experience to be able to say, but I it's clear from even from the last three years of literature there's still a very active, subjective discussion.",
            "And of course everyone puts numerical results in their paper that make their method look good.",
            "I have no personal way of saying.",
            "Based on my own experience with what's a person.",
            "I have a question please.",
            "The primary methods that you mentioned in the last we around the public are they still?",
            "Are they published on the under the Nesterov stuff?",
            "Yeah, I can give you some check report.",
            "So yeah, at least one is being published.",
            "Stuff OK.",
            "Feeling is that you are going to need any help to region of the problem or the other one I see.",
            "Current no, I have no experience with that.",
            "I just thought I'd what one thing I've done, which is kind of similar is to use that standard regularised formulation, but with a.",
            "The regularization term is kind of a group regularization term, so it's for example a sum of L2 norms of sub vectors.",
            "So that way if you know our priority that certain features are correlated, you can sort of put them in the same sub vector, and so we have some experience of that.",
            "But in the compressed sensing context, not in SVM.",
            "So OK, that's good to know that it's been tried.",
            "You have OK. No, I haven't tried the elastic net myself.",
            "So I guess do we have an estimate about the?",
            "The scalability of this message is this scale at square and is there any method that scales linearly and wallpaper?",
            "Some authors my claim of claims about linear scaling.",
            "For example, I think the cutting plane method is supposed to scale linearly if you implement it carefully.",
            "But this is not.",
            "Yeah, I don't know.",
            "Probably to some given accuracy, I don't know.",
            "So this is not an area that I personally take a lot of interest in.",
            "I'm kind of more interested in practical performances rather than theoretical error bounds, but I know there's a lot.",
            "There are a lot of people that do the latter, so I can't really give you.",
            "I'm kind of coming at.",
            "This is a bit of an outsider, so I don't.",
            "I'm not immersed enough in the culture that too.",
            "Have all these facts at my fingertips.",
            "Customized.",
            "Yeah, I don't think it's column generation, but I can refer you to the papers of the people that have worked on that.",
            "I can't recall right now what they did.",
            "Column generation would be a natural thing to try.",
            "But it might have been some sort of augmented Lagrangian type approach or something, maybe Kristen knows.",
            "No.",
            "Sorry, I mentioned that for the L when you use the L1 norm of W, you're going to LP and that you use specialized algorithms.",
            "Do you remember what algorithms?",
            "Overused and.",
            "There was column generation OK. OK. Yep.",
            "How about the natural method continuation method that raises the no.",
            "No, it's not a continuation method.",
            "What he does in the in the smoothing thing that I said at the end is just a one shot choice of the smoothing parameter.",
            "There are a couple of other people, or there's one other paper very recently that's considered a continuation variant.",
            "Will you try to choose a sequence of of smoothing parameters and kind of track the solution, but that hasn't.",
            "That's one thing that I would really like to look at.",
            "It's been on my To Do List for the last nine months or so.",
            "But now currently he just says you've got a fixed budget of N. And given that you've got in evaluations here is the optimal choice of the smoothing parameter.",
            "And then and.",
            "It is not.",
            "It's not an obvious way to improve on that."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so I'd like to start by thanking the organizers very much.",
                    "label": 0
                },
                {
                    "sent": "They've done a fantastic job with the organization for.",
                    "label": 0
                },
                {
                    "sent": "I had to do almost nothing, except maybe write a talk.",
                    "label": 0
                },
                {
                    "sent": "And it's nice to have it in such a beautiful place and thank you all for coming.",
                    "label": 0
                },
                {
                    "sent": "So I think the introduction actually set the stage very well for what I want to talk about.",
                    "label": 0
                },
                {
                    "sent": "I I'm obviously an outsider to this community, but I've had an interest for going back quite a few years on machine learning problems simply because they are a real challenge for large scale optimization.",
                    "label": 0
                },
                {
                    "sent": "So my students, saying, was kind enough to go through the literature from the last three or four years and and scour the literature and find all the papers that.",
                    "label": 0
                },
                {
                    "sent": "Had optimization content in the machine learning literature so so in part, my talk is kind of a review of the last three or four years of activity, and then I want to finish up by mentioning some current areas of interest in large scale optimization that I think might be relevant to solving support vector machines.",
                    "label": 0
                },
                {
                    "sent": "I'm really focusing on support vector machines here.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, I want to start though by just putting machine learning problems in a bit of a bit of context.",
                    "label": 0
                },
                {
                    "sent": "It's really there's really been this upsurge of interest in the last five years or so in what I call sparse or regularised optimization, so it's really a departure from the traditional opt.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Azatian paradigm where you give me a function and a set of variables.",
                    "label": 0
                },
                {
                    "sent": "There all well defined, deterministic and my job is to come up with an exact minimum.",
                    "label": 0
                },
                {
                    "sent": "That's kind of the traditional paradigm that optimizes it worked with, but very often.",
                    "label": 0
                },
                {
                    "sent": "What the user wants is not so much an exact solution.",
                    "label": 0
                },
                {
                    "sent": "I'd like to get a solution in some cases quickly.",
                    "label": 0
                },
                {
                    "sent": "I'd like a solution that's innocence simple, you know, sparse you maybe not so many nonzeros in the in the solution.",
                    "label": 0
                },
                {
                    "sent": "And there are various reasons for that.",
                    "label": 0
                },
                {
                    "sent": "One, they might have a restricted computational budget.",
                    "label": 0
                },
                {
                    "sent": "Secondly, there's this this reason that you don't want to overfit the data, right?",
                    "label": 0
                },
                {
                    "sent": "You want to get this tradeoff between generalizability and empirical fit.",
                    "label": 0
                },
                {
                    "sent": "So anyway, I want to start out by talking about a bunch of other.",
                    "label": 0
                },
                {
                    "sent": "Problems in this class just to put.",
                    "label": 0
                },
                {
                    "sent": "Support vector machines in context, so these are three of my themes.",
                    "label": 0
                },
                {
                    "sent": "Obviously the problems in this area are difficult, mostly because of their size, but also because of the Hill conditioning.",
                    "label": 1
                },
                {
                    "sent": "If using a kernel and I'll make the comment that the machine learning community really has made excellent use of optimization of being some really excellent adaptations of fundamental algorithms and very creative ideas coming out of your community.",
                    "label": 1
                },
                {
                    "sent": "And I really think it's got a lot to contribute to other problems in sparse optimization.",
                    "label": 0
                },
                {
                    "sent": "And that will end up as I said by talking about.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some other problems, so I've already kind of mentioned this.",
                    "label": 0
                },
                {
                    "sent": "Talking about sparse optimization or regularised optimization, and just make the comment that if these are your goals, not so much an exact solution, but a simple approximate solution, it really changes the.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The game as to which algorithm is is suitable for what you want to solve.",
                    "label": 0
                },
                {
                    "sent": "So here's this quote from Vapnik's thin book.",
                    "label": 0
                },
                {
                    "sent": "He says that often there's a tradeoff between equality approximation of the given data and the complexity of the approximating function.",
                    "label": 1
                },
                {
                    "sent": "And that's really the message in regularize optimization.",
                    "label": 1
                },
                {
                    "sent": "Quite often simplicity is manifested in some senses, is sparsity in the solution vector, and there are a lot of problems that you can formulate as the somewhere you're trying to minimize.",
                    "label": 0
                },
                {
                    "sent": "The sum of a model function and a regularization function with some weighting parameter Lambda.",
                    "label": 0
                },
                {
                    "sent": "So if your goal were just to find the exact solution, it sort of fits the data as well as possible.",
                    "label": 0
                },
                {
                    "sent": "You would leave out the second term.",
                    "label": 0
                },
                {
                    "sent": "And just minimize F and that's the traditional paradigm.",
                    "label": 0
                },
                {
                    "sent": "But now we've got this regularization term, sometimes it's non smooth.",
                    "label": 0
                },
                {
                    "sent": "We often don't have a very good idea what Lambda should be.",
                    "label": 0
                },
                {
                    "sent": "So often you want to solve for a variety of values of Lambda and then have some outer loop to pick the right one.",
                    "label": 0
                },
                {
                    "sent": "And So what I'll do is quote some examples.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problems in this class.",
                    "label": 0
                },
                {
                    "sent": "One that you might have already heard about in the main conference if you went to Emmanuel Candace's talk is compressed sensing and this has been a big area in the last three years or so.",
                    "label": 0
                },
                {
                    "sent": "It's been attacked by theoretical computer scientists, statisticians, electrical engineers and is also being interested in the optimization problem in actually solving this thing.",
                    "label": 0
                },
                {
                    "sent": "It's a very simple problem to state.",
                    "label": 0
                },
                {
                    "sent": "It's sort of an undetermined system of linear equations, but you want to find a sparse approximate solution.",
                    "label": 0
                },
                {
                    "sent": "And it turns out that under certain assumptions on the Matrix A, the way to do that is to add on Lambda times the one norm of X OK, and it turns out that for an appropriate choice of Lambda, this this is as good as applying some sort of cardinality constraint on the number of nonzeros allowed in X, but it's a much simpler problem then a cardinality constraint problem.",
                    "label": 0
                },
                {
                    "sent": "So obviously by manipulating Lambda you can control how sparse the solution, as if Lambda zero you just get a, usually a solution whose number of nonzeros equals the number of rows in a.",
                    "label": 0
                },
                {
                    "sent": "If Lambda is big, you get X = 0 as a solution.",
                    "label": 0
                },
                {
                    "sent": "So Lambda somewhere in between is usually what you want and this also ties into the lasso idea.",
                    "label": 0
                },
                {
                    "sent": "In statistics at dates from mid 90s or so for.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Selection and statistics.",
                    "label": 0
                },
                {
                    "sent": "Here is another example image denoising.",
                    "label": 1
                },
                {
                    "sent": "So given an image over a 2 dimensional domain, you want to basically remove the noise from the image by doing a least squares fit.",
                    "label": 1
                },
                {
                    "sent": "But you also want to regularize it by.",
                    "label": 0
                },
                {
                    "sent": "You assume that you're dealing with an image with large sort of flat areas and some a few edges, so you basically want to remove all the special in the flat areas and so the norm.",
                    "label": 0
                },
                {
                    "sent": "The regularization term involves a spatial gradient of you.",
                    "label": 1
                },
                {
                    "sent": "The thing you're trying to recover, and you integrate that OK and again the Lambda controls how close are you to the image.",
                    "label": 0
                },
                {
                    "sent": "If Lambda is zero, obviously U = F. But as you increase Lambda, you typically get more more and more cartoon like image OK and it turns out that again.",
                    "label": 0
                },
                {
                    "sent": "We've shown in other recent work that if you apply grading projection to the jewel of this, you can get very.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Approximate solutions very quickly.",
                    "label": 0
                },
                {
                    "sent": "Here's another much more applied example is getting something that I've worked on a number of us Wisconsin have worked on is radiation treatment planning in cancer.",
                    "label": 1
                },
                {
                    "sent": "So when you've got a patient with a tumor inside their body, you're trying to fire beams of radiation into the body from all sorts of different angles, and the aim is that you're trying to ZAP the tumor, but not kill too much of the tissue surrounding it.",
                    "label": 0
                },
                {
                    "sent": "So that's the idea of moving the beam around is that it's sort of focused on the tumor, but the dose to the normal tissue gets spread around.",
                    "label": 1
                },
                {
                    "sent": "So there's an infinite number of possibilities from how you can choose the beam angle or beam shape, the exposure times, and so on.",
                    "label": 0
                },
                {
                    "sent": "And you want to come up with a plan that just does it in three or four shots, or five shots or something for all kinds of practical reasons.",
                    "label": 0
                },
                {
                    "sent": "You want you don't want the patient lying on the table for ages while you kind of fiddle around with the beam and so on.",
                    "label": 0
                },
                {
                    "sent": "So you're looking for a sparse solution.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Again, OK, and here's another one matrix completion that you may also have heard about.",
                    "label": 0
                },
                {
                    "sent": "The idea here is we've got an unknown M by N matrix X.",
                    "label": 1
                },
                {
                    "sent": "We've got a certain number of linear observations on that matrix.",
                    "label": 0
                },
                {
                    "sent": "For example, we might be observing some elements of the matrix, and we're looking for an X that's low rank, but consistent with the observed data or nearly consistent with the observed data, and this is very much like compressed sensing.",
                    "label": 0
                },
                {
                    "sent": "You can see from the way I've written it here instead of the one norm here, we have what's called the nuclear norm, which is the sum of singular values.",
                    "label": 1
                },
                {
                    "sent": "And that turns out to be a good surrogate for the rank of X OK?",
                    "label": 0
                },
                {
                    "sent": "And some of the algorithms are actually.",
                    "label": 0
                },
                {
                    "sent": "This is an area of active research, but some of the algorithms that are actually close to the ones that we.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "For compressed sensing, so I've given you four or five examples of regularised optimization problems, and now we're going to talk about support vector machines.",
                    "label": 0
                },
                {
                    "sent": "But before we do, I'll just note that all of these applications are very different properties, and there's no one size fits all algorithm.",
                    "label": 1
                },
                {
                    "sent": "But there are some sort of algorithm approaches that do translated across different problems, and so there is a bit of cross fertilization, and I think this is one way that all the work in SVM that's been done can be kind of leveraged in some of these other applications.",
                    "label": 0
                },
                {
                    "sent": "Potentially, I'll note also that duality is often a key to getting a formulation that you can solve.",
                    "label": 1
                },
                {
                    "sent": "And we'll see that we certainly see that in SVM, and as I've already noted, you often don't know what Lambda is in advance.",
                    "label": 0
                },
                {
                    "sent": "You want to solve for a bunch of Lambda values.",
                    "label": 1
                },
                {
                    "sent": "Often I'll just make as a general comment, often as a choice between methods that have fast asymptotic convergence.",
                    "label": 0
                },
                {
                    "sent": "These are things like interior point methods or sequential quadratic quadratic programming, but their individual steps are very expensive and at the other extreme there are methods that have very slow convergence, asymptotically linear or sub linear, but the steps are very cheap to compute, and they may be only need approximate gradient information so often there's kind of a attention or a.",
                    "label": 0
                },
                {
                    "sent": "A hybrid of these two is the thing that's.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Appropriate in different circumstances.",
                    "label": 0
                },
                {
                    "sent": "OK, so I'm taking it there.",
                    "label": 0
                },
                {
                    "sent": "Probably most of you.",
                    "label": 0
                },
                {
                    "sent": "I've got a fair amount of sort of elementary material and SVM, but probably it's quite familiar to most of you, so I won't dwell too much on it.",
                    "label": 0
                },
                {
                    "sent": "But really I'm just trying to fix notation here.",
                    "label": 0
                },
                {
                    "sent": "I'm going to assume you've got a bunch of feature vectors in RN, an binary labels an in the linear problem you're looking.",
                    "label": 1
                },
                {
                    "sent": "I've got the eye on the wrong place here in the linear problem you're looking for a classifier, which is a weighted sum of the components of the feature vector.",
                    "label": 0
                },
                {
                    "sent": "Plus some intercept B.",
                    "label": 1
                },
                {
                    "sent": "An ideally would like to find a W such that you correctly classify every feature vector according to this criterion here, so inessential trying to keep the classification errors small and I call denote those by Zita.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "And what do you mean by small?",
                    "label": 1
                },
                {
                    "sent": "Well, you have some.",
                    "label": 0
                },
                {
                    "sent": "Usually it's a separable function.",
                    "label": 0
                },
                {
                    "sent": "It's increasing in the Zetas.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's a classic formulation where you have a regularization term, which in this case is smooth, involving the W. You have a penalty term, which in this case is just an L1 norm of the classification errors, and you have some constraints that define the Zita eyes.",
                    "label": 0
                },
                {
                    "sent": "And as you all know, using this term here gives you a in the case of perfect separability, gives you a maximum margin separating hyperplane, so this is a paradigm that a lot.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "People have studied.",
                    "label": 0
                },
                {
                    "sent": "And also you can take the jewel of that as you know the problem I wrote on the previous slide is a quadratic program, it's convex.",
                    "label": 0
                },
                {
                    "sent": "The jewel is also a convex quadratic program.",
                    "label": 1
                },
                {
                    "sent": "It's got this coefficient matrix K which is the inner products of the feature vectors, appropriately scaled by labels.",
                    "label": 0
                },
                {
                    "sent": "It's got these bound constraints on the alphas, which are LaGrange multipliers for the.",
                    "label": 0
                },
                {
                    "sent": "Constraints defining the classification errors.",
                    "label": 0
                },
                {
                    "sent": "It's got this additional constraint linear constraint which will talk more about in a moment, and if you write down the KKT conditions for the primal or the dual, you get this relationship between the LaGrange multipliers.",
                    "label": 1
                },
                {
                    "sent": "And the primal weight vector that you're looking for OK, and the Intercept B turns out to be the LaGrange multiplier for this problem in the dual formulation.",
                    "label": 0
                },
                {
                    "sent": "So given this, you're able to write down the classification function.",
                    "label": 0
                },
                {
                    "sent": "If I just substitute for W in that previous slide.",
                    "label": 0
                },
                {
                    "sent": "I can get directly from the almost directly from the jewel.",
                    "label": 0
                },
                {
                    "sent": "I can get a classification classifier.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's my.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Notation now you will know that.",
                    "label": 0
                },
                {
                    "sent": "The beauty of going one of the beauties of going to the dual formulation is you can do the so called kernel trick.",
                    "label": 1
                },
                {
                    "sent": "And that's that corresponds to projecting into a high dimensional space by some function Phi, and you can do all the classification in that space.",
                    "label": 1
                },
                {
                    "sent": "So it's just the primal formulation applied to this fire vexi rather than XI.",
                    "label": 0
                },
                {
                    "sent": "OK, and that leads to if you use that, use defy.",
                    "label": 0
                },
                {
                    "sent": "The kernel matrix is redefined.",
                    "label": 0
                },
                {
                    "sent": "This way everything else stays the same in the dual formulation and the classifier is redefined this way with just 5X replacing X OK and then of course is this classic observation that well, actually you don't even need to use file explicitly at all.",
                    "label": 1
                },
                {
                    "sent": "Or do you really need the inner products involving 5X with five at another point X bar?",
                    "label": 0
                },
                {
                    "sent": "And you can do everything in terms of a kernel function.",
                    "label": 0
                },
                {
                    "sent": "Which is a function of of X&X bar, and there's this theorem that says that if you pick a function, a kernel function that's continuous, symmetric, positive definite, then there is afie out there somewhere such that K is equal to five.",
                    "label": 1
                },
                {
                    "sent": "Transpose five.",
                    "label": 0
                },
                {
                    "sent": "OK, so everything can be done now in terms of the kernel function, K never need to look at that FI.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that's that's one of the reasons that the dual formulation is very popular.",
                    "label": 1
                },
                {
                    "sent": "The classifier, in terms of the kernel can be written like this, so you can just plug in your X and.",
                    "label": 0
                },
                {
                    "sent": "Evaluate F and decide whether that should be a plus one or minus one.",
                    "label": 0
                },
                {
                    "sent": "So here are some popular kernels.",
                    "label": 1
                },
                {
                    "sent": "Probably the Gaussian is linear in the Gaussian.",
                    "label": 0
                },
                {
                    "sent": "That seems to be the two most popular, although a few others crop up in the literature.",
                    "label": 0
                },
                {
                    "sent": "I'll just make the comment that this kind of the Gaussian kernel in particular is often quite ill conditioned, and this is a challenge.",
                    "label": 0
                },
                {
                    "sent": "If you just using a naive saying interior point method on the dual, it sort of doesn't like the fact that the.",
                    "label": 0
                },
                {
                    "sent": "The Colonel is.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well conditioned.",
                    "label": 0
                },
                {
                    "sent": "OK, now what I want to do is talk about algorithms for solving those two formulations, the primal and the dual that I just mentioned.",
                    "label": 1
                },
                {
                    "sent": "Particularly things that have cropped up in the literature in the last since a 2005.",
                    "label": 0
                },
                {
                    "sent": "Although I mentioned one or two things from before that.",
                    "label": 0
                },
                {
                    "sent": "It still seems to be an active area, although people have been beating on it for about 10 years.",
                    "label": 0
                },
                {
                    "sent": "A lot of the methods are based on optimization methods, or at least we can think of them.",
                    "label": 1
                },
                {
                    "sent": "We can put him in an optimization setting.",
                    "label": 0
                },
                {
                    "sent": "How do we decide which one is best?",
                    "label": 0
                },
                {
                    "sent": "Well, of course it all depends on what you're what you're looking for.",
                    "label": 1
                },
                {
                    "sent": "You can talk about how fast are these methods in finding a solution of a given quality, measured according to the error rate, or how close they come to the optimal objective value and certain amount of time.",
                    "label": 0
                },
                {
                    "sent": "Alot of people in your community seem to like to do theoretical analysis of efficiency to see a lot of that.",
                    "label": 0
                },
                {
                    "sent": "In the literature.",
                    "label": 0
                },
                {
                    "sent": "There are also a lot of nitty gritty issues that you need to address in implementing these things because.",
                    "label": 0
                },
                {
                    "sent": "Storing one of these kernel matrices, usually for a reasonable sized datasets, isn't really an option.",
                    "label": 0
                },
                {
                    "sent": "And so there are all sorts of things about caching and selective.",
                    "label": 0
                },
                {
                    "sent": "Storing pieces of the matrix selectively.",
                    "label": 0
                },
                {
                    "sent": "You always like algorithms that are simple 'cause they're easy.",
                    "label": 0
                },
                {
                    "sent": "You can program in a page of Matlab and get some nice results, so that's you know practical priority, and there's been a bit of work recently on parallel parallel is ability, which I'll say a little bit more about.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK so first let me talk about algorithms that solve this dual formulation.",
                    "label": 0
                },
                {
                    "sent": "And as I mentioned, it's a convex QP.",
                    "label": 1
                },
                {
                    "sent": "It's mostly got bound constraints.",
                    "label": 1
                },
                {
                    "sent": "Passion can be a bit of a challenge.",
                    "label": 0
                },
                {
                    "sent": "Not just because the soil condition, but because it's usually dance and and also hard to evaluate.",
                    "label": 0
                },
                {
                    "sent": "If you've got to evaluate the Hessian piece by piece and you have to take an exponential every time.",
                    "label": 0
                },
                {
                    "sent": "That's kind of expensive.",
                    "label": 0
                },
                {
                    "sent": "And also, I'll note that this linear constraint is kind of a nuisance.",
                    "label": 1
                },
                {
                    "sent": "There's been a lot of work around for a long time, unbound constraint problems, but throwing in that linear constraint makes things.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A little bit more interesting.",
                    "label": 0
                },
                {
                    "sent": "OK, some very recent work here on coordinate descent for the dual formulation so.",
                    "label": 0
                },
                {
                    "sent": "This and other papers get deal with this constraint by just throwing it away.",
                    "label": 0
                },
                {
                    "sent": "OK, the Y transpose Alpha equals 0 so that that correspond.",
                    "label": 0
                },
                {
                    "sent": "You can think of this in two ways.",
                    "label": 0
                },
                {
                    "sent": "You can think of this either as adding a 1/2 B squared term to the objective, or you can think of it as augmenting the feature vector by one component, namely at one.",
                    "label": 0
                },
                {
                    "sent": "And just.",
                    "label": 0
                },
                {
                    "sent": "Making the last element of W equal to the Intercept.",
                    "label": 0
                },
                {
                    "sent": "OK, so it means that the classification function no longer has a B in it, it's just got W transpose X.",
                    "label": 0
                },
                {
                    "sent": "So the problem simplifies the primal problem.",
                    "label": 0
                },
                {
                    "sent": "Simplifies in this way.",
                    "label": 0
                },
                {
                    "sent": "The dual problem simplifies.",
                    "label": 0
                },
                {
                    "sent": "In this way it gets rid of this extra constraint.",
                    "label": 0
                },
                {
                    "sent": "So now you can.",
                    "label": 0
                },
                {
                    "sent": "Do you know all kinds of gradient projection type ideas for this this particular paper and I can give you more precise references later on if you like.",
                    "label": 0
                },
                {
                    "sent": "I had a big pile of these things, but this particular paper sort of cycles around the elements of Alpha in this formulation.",
                    "label": 0
                },
                {
                    "sent": "Anne fixes all the others and just does a solve.",
                    "label": 0
                },
                {
                    "sent": "Just in a single element of Alpha, OK?",
                    "label": 0
                },
                {
                    "sent": "So it's just a scalar constraint problem.",
                    "label": 0
                },
                {
                    "sent": "You can solve it in closed form.",
                    "label": 0
                },
                {
                    "sent": "So it updates that element of Alpha updates K times off.",
                    "label": 0
                },
                {
                    "sent": "It has to keep track of K times the latest Alpha.",
                    "label": 0
                },
                {
                    "sent": "So if you change one component of Alpha, you have to evaluate a column of K and update K Alpha OK. And it sort of cycles around.",
                    "label": 0
                },
                {
                    "sent": "I think there's also a variant where they pick I at random.",
                    "label": 0
                },
                {
                    "sent": "Now this is an example of an algorithm, but of course it takes a lot of iterations, but each one is relatively cheap and you can think of imaginative ways to speed up, for example, this part of it here.",
                    "label": 0
                },
                {
                    "sent": "For example, if you know that certain alphas are almost certainly at either the upper bound of the lower bound, you don't really need to evaluate K Alpha for those components.",
                    "label": 0
                },
                {
                    "sent": "If you think they're going to be stuck there, that's sort of a shrinking idea that.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Keeps cropping up in other methods as well.",
                    "label": 0
                },
                {
                    "sent": "Here's another approach due to Diane Fletcher.",
                    "label": 0
                },
                {
                    "sent": "These are optimized most optimization people who've worked a little bit on this problem, and their idea is you don't throw away this constraint.",
                    "label": 0
                },
                {
                    "sent": "You consider the standard dual formulation.",
                    "label": 0
                },
                {
                    "sent": "But you do gradient projection OK, and the idea of gradient projection and Professor Bertsekas is kind of a pioneer in this area.",
                    "label": 1
                },
                {
                    "sent": "I don't know if he's here yet, but the idea of gradient projection is you look in the negative gradient direction of the objective.",
                    "label": 0
                },
                {
                    "sent": "Take a step in that direction and then project the resulting point back onto the constraint set.",
                    "label": 0
                },
                {
                    "sent": "Ann, you manipulate the step length gamma here to get usually to get a dissent of some kind.",
                    "label": 0
                },
                {
                    "sent": "OK, now the catch with this.",
                    "label": 0
                },
                {
                    "sent": "Applying gradient projection in this context is that this major is more complicated than you want it to be.",
                    "label": 0
                },
                {
                    "sent": "Because of this, this extra linear constraint.",
                    "label": 0
                },
                {
                    "sent": "So it's actually a little bit more work to project onto a mega then it would be if you just had bounds.",
                    "label": 0
                },
                {
                    "sent": "If you just have mounds, it's trivial.",
                    "label": 0
                },
                {
                    "sent": "You just do a clipping procedure, but now it becomes a little bit more tricky.",
                    "label": 0
                },
                {
                    "sent": "You sort of have to solve a little subproblem and do this projection.",
                    "label": 1
                },
                {
                    "sent": "But it's not too hard.",
                    "label": 0
                },
                {
                    "sent": "It's still fairly cheap.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So how do you choose the gamma?",
                    "label": 0
                },
                {
                    "sent": "So there are a bunch of different techniques for that.",
                    "label": 0
                },
                {
                    "sent": "One is that the basic gradient projection technique is that you make some initial guess of gamma.",
                    "label": 0
                },
                {
                    "sent": "You do this, you see if the Alpha is better in terms of decreasing queue.",
                    "label": 0
                },
                {
                    "sent": "If it's not, you say cut gamma in half and try again and you keep cutting back until you get a descent of some kind.",
                    "label": 0
                },
                {
                    "sent": "So that's one of the standard approaches.",
                    "label": 0
                },
                {
                    "sent": "But these are authors, tried using Barzelay born formulated to choose the gamma.",
                    "label": 1
                },
                {
                    "sent": "Now Buzz, Light Boy and I'm not going to spend too much time explaining it, but it's a it's a method that's been around for about 20 years and the idea is it's sort of like a quasi Newton method.",
                    "label": 0
                },
                {
                    "sent": "Where you choose the step length to kind of mimic the behavior of the inverse of the Hessian.",
                    "label": 0
                },
                {
                    "sent": "OK, because the classic Newton's method is to take Alpha minus Hessian inverse times gradient.",
                    "label": 0
                },
                {
                    "sent": "That's classic Newton method.",
                    "label": 1
                },
                {
                    "sent": "So buzz like boy and say, well, let's choose the gamma to kind of act like the inverse of the Hessian.",
                    "label": 0
                },
                {
                    "sent": "Now what do you mean by that?",
                    "label": 0
                },
                {
                    "sent": "Well, it means that you want gamma to sort of behave as the Hessian would have behaved over the last step that you took.",
                    "label": 0
                },
                {
                    "sent": "And how would the Hessian of behave well if you multiply the Hessian times that last step?",
                    "label": 0
                },
                {
                    "sent": "You should get approximately the difference of the gradients by Taylor's theorem, so those sort of choose the gamma so that it in the least squares sense it satisfies gamma times S approximately equal to L to.",
                    "label": 0
                },
                {
                    "sent": "Why OK?",
                    "label": 0
                },
                {
                    "sent": "That's one of the motivations, and so you get this very simple formula for gamma.",
                    "label": 0
                },
                {
                    "sent": "You plug it in.",
                    "label": 0
                },
                {
                    "sent": "Sometimes the thing goes uphill, usually just accept it anyway, even if it gives you an increase in the function.",
                    "label": 0
                },
                {
                    "sent": "And often this is.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Converges faster than a monotone method.",
                    "label": 0
                },
                {
                    "sent": "And we've been using Buzz Libor and methods in a bunch of other applications as well, with some success.",
                    "label": 0
                },
                {
                    "sent": "OK, here's another thing that keeps cropping up in algorithms for solving the dual.",
                    "label": 1
                },
                {
                    "sent": "This idea of decomposition.",
                    "label": 0
                },
                {
                    "sent": "It's very natural, because when you've got a lot of data, Alpha is a very big vector and it's very hard to sort of tackle all the components at once.",
                    "label": 1
                },
                {
                    "sent": "So the idea of decomposition is you just.",
                    "label": 1
                },
                {
                    "sent": "You have an outer loop where you pick a subset of Alpha components.",
                    "label": 0
                },
                {
                    "sent": "And you solve a reduced quadratic program on that subset.",
                    "label": 0
                },
                {
                    "sent": "OK, you solve it approximately, usually an then, and then you go back to the outer loop and pick another subset.",
                    "label": 1
                },
                {
                    "sent": "Might be completely different, or it might just change if you component.",
                    "label": 0
                },
                {
                    "sent": "So there are many different variants.",
                    "label": 1
                },
                {
                    "sent": "In fact, most of the codes that are floating around out there for this problem make use of this strategy.",
                    "label": 0
                },
                {
                    "sent": "And they sort of differ by what's your strategy for picking subsets?",
                    "label": 1
                },
                {
                    "sent": "How big do you make the subsets and what strategy to use to solve the reduced subproblem when in just the subset of the variables?",
                    "label": 1
                },
                {
                    "sent": "And this has been around since at least 1998.",
                    "label": 0
                },
                {
                    "sent": "There's the Smosh idea where the sort of the extreme idea where the subset just has two components.",
                    "label": 0
                },
                {
                    "sent": "And you pick them to get a decrease in the objective.",
                    "label": 0
                },
                {
                    "sent": "Maybe it's not the extreme.",
                    "label": 0
                },
                {
                    "sent": "I guess the extreme is just to have one component, and that's more like that decomposition strategy that I mentioned earlier.",
                    "label": 0
                },
                {
                    "sent": "SVM light that usually chooses a fairly small subproblem size, and it picks the components to include by some first order heuristic, again based on a somewhat greedily on which components will give you the biggest dissent in the objective.",
                    "label": 0
                },
                {
                    "sent": "According to some linearized model.",
                    "label": 1
                },
                {
                    "sent": "I just note that this heuristic uses a cardinality constraint.",
                    "label": 0
                },
                {
                    "sent": "It sort of fixes the number that you want to put in the subset and tries to solve this cardinality constraint linear program.",
                    "label": 0
                },
                {
                    "sent": "And I just wonder if you could use an L1 penalty instead of as a surrogate for the cardinality constraint.",
                    "label": 1
                },
                {
                    "sent": "I don't know if that's been tried, that would actually give you a simple linear program to solve.",
                    "label": 0
                },
                {
                    "sent": "OK, there's another code out there by our Italian collaborators where they use a gradient projection method on the subproblem.",
                    "label": 0
                },
                {
                    "sent": "In fact, I think they use the method.",
                    "label": 0
                },
                {
                    "sent": "One of the methods they use is that Diane Fletcher method that I talked about on the previous slide and they have a.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Our little implementation.",
                    "label": 0
                },
                {
                    "sent": "Actually, the Lib SVM code uses in Smosh type framework with kind of a variety of heuristics for picking the two components to relax.",
                    "label": 1
                },
                {
                    "sent": "And then they solve a 2 dimensional QP.",
                    "label": 0
                },
                {
                    "sent": "To get this step.",
                    "label": 0
                },
                {
                    "sent": "And as I mentioned earlier, all of these ideas are really interesting.",
                    "label": 0
                },
                {
                    "sent": "But to make them work practically you have to do a lot of nitty gritty stuff such As for example, shrinking.",
                    "label": 0
                },
                {
                    "sent": "This idea has been around for quite awhile.",
                    "label": 0
                },
                {
                    "sent": "This is where you know you sort of make some guess of which alphas are strictly between the two bounds, and you try to focus on those.",
                    "label": 0
                },
                {
                    "sent": "In other words, you try to the alphas that look like they're either at zero or C. You sort of temporarily ignore them, and you can save a lot of computer time by doing that.",
                    "label": 0
                },
                {
                    "sent": "You might have to do a final optimality check or periodically check that you really are making a good decision about.",
                    "label": 1
                },
                {
                    "sent": "Which ones need to be at their bounds?",
                    "label": 0
                },
                {
                    "sent": "But essentially you can ignore them for most of the computation.",
                    "label": 0
                },
                {
                    "sent": "And then, as I've also mentioned, this idea of caching you only want to sort of store parts of K or the kind letter of current interest.",
                    "label": 1
                },
                {
                    "sent": "So in general these are I've described at least four codes here.",
                    "label": 0
                },
                {
                    "sent": "They've all been used well for the last 10 years quite successfully.",
                    "label": 0
                },
                {
                    "sent": "One point to make though is that the solutions are often not particularly sparse, particularly using, say, a Gaussian kernel.",
                    "label": 1
                },
                {
                    "sent": "The alphas in other sparse optimization applications, that number of nonzeros in the solution is often a tiny sliver of the total dimensionality of the solution, and that's often not the case here.",
                    "label": 0
                },
                {
                    "sent": "It's not a typical to have 10% of the Alpha.",
                    "label": 0
                },
                {
                    "sent": "Non zeros, it seems to me.",
                    "label": 0
                },
                {
                    "sent": "And so decomposition can sometimes struggle with that.",
                    "label": 0
                },
                {
                    "sent": "OK, 'cause it's gotta pay attention to substantial subset of the solution.",
                    "label": 0
                },
                {
                    "sent": "So there are the sum.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Opinion out there that these methods sort of running can run into a wall if those size of the data set gets really big.",
                    "label": 0
                },
                {
                    "sent": "OK, I just mentioned active set methods.",
                    "label": 0
                },
                {
                    "sent": "This is a classic approach to QP.",
                    "label": 1
                },
                {
                    "sent": "Catcher Scheinberg has a paper from just a couple of years ago where she does a very careful implementation of an active set method and gets seems to get pretty good results.",
                    "label": 0
                },
                {
                    "sent": "The idea of an active set method is that you.",
                    "label": 0
                },
                {
                    "sent": "You sort of.",
                    "label": 1
                },
                {
                    "sent": "You maintain an active set of solutions that seem to be interesting.",
                    "label": 0
                },
                {
                    "sent": "That is, the ones that are strictly between their bounds, and you change that set, typically by just one index at each iteration, and you do that either by bumping into abound, in which case the active set drops by one, or by moving away from about, in which case you throw something else into the active set.",
                    "label": 1
                },
                {
                    "sent": "So the basic story is you have to maintain a factorization of the interesting part of K. So you have to do nice linear algebra for updating the factorization and so on.",
                    "label": 0
                },
                {
                    "sent": "And she also uses a strategy and seems to on some test problem seems to get good results.",
                    "label": 0
                },
                {
                    "sent": "There's another paper by Shelton, but mostly actually by a couple of the authors are optimization people, including my colleague Danny Ralph.",
                    "label": 1
                },
                {
                    "sent": "And I do sort of a similar thing, but to a minmax formulation where they leave the B in the jewel so they have sort of a combined primal dual problem.",
                    "label": 0
                },
                {
                    "sent": "And they do an active set on that using some of the same ideas.",
                    "label": 0
                },
                {
                    "sent": "One of the thing.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just to mention about this approach is that first of all it can be warm, started.",
                    "label": 0
                },
                {
                    "sent": "Active set methods are very good at that.",
                    "label": 1
                },
                {
                    "sent": "If you have a solution that's that's optimal for one of these commands, C and you change the slightly, you can use your previous solution as a warm start to get to the you know to modify it quite efficiently, and the other thing is that it's useful in this incremental context.",
                    "label": 0
                },
                {
                    "sent": "We're bringing data points in one by one or in batches.",
                    "label": 1
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So in other words, you can solve the problem for you, know the current Alpha.",
                    "label": 0
                },
                {
                    "sent": "You can then throw in a bunch more data.",
                    "label": 0
                },
                {
                    "sent": "All that corresponds to doing is increasing K by a certain number of rows and columns and increasing Alpha by a certain number of elements, and so you can just augment the problem and then just keep going from there.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a very.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Natural framework to use if you want to sort of income.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mental learning.",
                    "label": 0
                },
                {
                    "sent": "OK, and then finally I'll mention Interior point methods.",
                    "label": 0
                },
                {
                    "sent": "This goes back a while too.",
                    "label": 0
                },
                {
                    "sent": "Fine and Scheinberg paper from 21 where they just apply the classic primal dual interior point approach.",
                    "label": 0
                },
                {
                    "sent": "The critical operation in Interior Point methods are distinguished by being by having a small number of iterations, but the iterations are very expensive.",
                    "label": 0
                },
                {
                    "sent": "Typically, in this case you have to solve a linear system of this form with kernel plus some diagonal some positive diagonals.",
                    "label": 0
                },
                {
                    "sent": "At every iteration.",
                    "label": 0
                },
                {
                    "sent": "And I sort of have to do well.",
                    "label": 0
                },
                {
                    "sent": "I first take note of the fact that K is often ill conditioned, so you can come up with a low rank approximation, decay and sort of use that as A to approximately solve that linear system.",
                    "label": 0
                },
                {
                    "sent": "So how do you get this low rank approximation?",
                    "label": 0
                },
                {
                    "sent": "Well, good question.",
                    "label": 0
                },
                {
                    "sent": "There are.",
                    "label": 0
                },
                {
                    "sent": "There are some, there's been some recent work on this trim type methods that use sampling that can get sort of approximate decompositions of a large matrix like this, and.",
                    "label": 0
                },
                {
                    "sent": "And I think these are being used in this context.",
                    "label": 0
                },
                {
                    "sent": "You can also just call the I guess command in Matlab, which applies in an ALDI method which will just find you approximately the top five eigenvectors and eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So you could do that as well.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll just note that if you if you just want to approximate K by VV transpose for some long narrow V and you just want to solve the problem for that approximation, that turns out to be a very simple problem that you can solve efficiently within active set method.",
                    "label": 0
                },
                {
                    "sent": "So you can just call seaplex the active set one of the active set options in.",
                    "label": 0
                },
                {
                    "sent": "See Plex, you can just introduce some extra variables gamma to represent V transpose Alpha.",
                    "label": 0
                },
                {
                    "sent": "So this is a problem with bounds in a few equality constraints.",
                    "label": 0
                },
                {
                    "sent": "And so we've tried this.",
                    "label": 0
                },
                {
                    "sent": "The quality of the classifiers often is not that great, so you seem to lose quite a bit by ignoring by taking just the top five or 10 or 20 or whatever it is eigenvectors.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But it certainly gives you a lot simpler problem.",
                    "label": 0
                },
                {
                    "sent": "OK, there are some other issues with the dual.",
                    "label": 1
                },
                {
                    "sent": "I'll just skip through this 'cause I gotta finish in what 1520 minutes?",
                    "label": 0
                },
                {
                    "sent": "Something like that.",
                    "label": 0
                },
                {
                    "sent": "OK so I'll just note there's been some work on parallel implementations.",
                    "label": 0
                },
                {
                    "sent": "There's another thing here that if you want to multiply by K, that's quite expensive.",
                    "label": 0
                },
                {
                    "sent": "If using a Gaussian kernel, there was a fairly recent paper on using sort of an approximate.",
                    "label": 0
                },
                {
                    "sent": "Gaussian kernel times vector multiply.",
                    "label": 1
                },
                {
                    "sent": "It seems to work well if you've got very short feature vectors.",
                    "label": 1
                },
                {
                    "sent": "It's not so clear if it's sufficient otherwise, but that's certainly something to work on if you can get away with his expense of having to calculate exponentials all the time, that would certainly be good.",
                    "label": 0
                },
                {
                    "sent": "There's a very recent paper on a GPU implementation using a graphical processing unit, and there's been a bit of a trend recently in numerical analysis on making use of this commodity hardware, which is in pretty much all PC's these days and.",
                    "label": 0
                },
                {
                    "sent": "Is very cheap, but if you're doing certain restricted numerical operations on.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "They can.",
                    "label": 0
                },
                {
                    "sent": "They can be done very very fast.",
                    "label": 1
                },
                {
                    "sent": "OK, I want to say a few words about.",
                    "label": 1
                },
                {
                    "sent": "There's been some recent activity, particularly or I think, a bit of a resurgence in actually addressing the primal formulation rather than the jewel that I've been talking about.",
                    "label": 0
                },
                {
                    "sent": "So this is going back to that formulation I wrote up earlier on the limitation of this approach.",
                    "label": 1
                },
                {
                    "sent": "It would seem is that you lose the ability to use a kernel.",
                    "label": 0
                },
                {
                    "sent": "But it's actually not so clear that you you do, because you could do the projection manually.",
                    "label": 0
                },
                {
                    "sent": "In a sense, you could go back to trying to pick the feyen, projecting the feature vector into some large space manually, or there's a paper by Chappelle where he sort of he brings the kernel in, and sort of implicitly replaces the feature vector by this kernelized feature vector.",
                    "label": 1
                },
                {
                    "sent": "So in a sense, he takes the RN natural feature vector and expands it into a vector of length big N, which is the number of data points by using the kernel.",
                    "label": 1
                },
                {
                    "sent": "To do the transformation.",
                    "label": 0
                },
                {
                    "sent": "To get an efficient problem he has to replace this term by a weighted norm.",
                    "label": 0
                },
                {
                    "sent": "OK, but he ends up with a formulation that looks a lot like that and sort of does incorporate the kernel.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "It works with that.",
                    "label": 0
                },
                {
                    "sent": "OK, but all I'm going to talk about is going to ignore that.",
                    "label": 0
                },
                {
                    "sent": "I'm just going to assume you use the given feature vector, whatever it is.",
                    "label": 0
                },
                {
                    "sent": "OK, so here's one approach is being tried by a couple of people in recent years, York, Eames, and these authors in 28?",
                    "label": 0
                },
                {
                    "sent": "And that is to recognize that this primal problem is a nice, very nice smooth problem, plus a piecewise linear convex problem function is piecewise linear, convex in its arguments, and there's a classic approach for solving problems with this piecewise linear convex structure, and that's his cutting plane approach.",
                    "label": 1
                },
                {
                    "sent": "The idea of a cutting plane approaches that you visit a sequence of iterates, and at each iterate you find a subgradient and that sort of gives you a lower bound on the function that you're actually trying to minimize.",
                    "label": 0
                },
                {
                    "sent": "So you end up you start off with this very complicated piecewise linear problem, and you build.",
                    "label": 1
                },
                {
                    "sent": "You build up an approximation to it by just adding cuts.",
                    "label": 0
                },
                {
                    "sent": "So you're sort of building up this lower bounding approximation.",
                    "label": 0
                },
                {
                    "sent": "And it can be implemented efficiently provided you can find subgradients cheaply.",
                    "label": 0
                },
                {
                    "sent": "And in fact, in this application for this problem, it's very cheap to find subgradients, so it's quite quite a natural thing to try.",
                    "label": 0
                },
                {
                    "sent": "Your keys implements this.",
                    "label": 0
                },
                {
                    "sent": "In this code SVM Purf there's an observation made by these authors that these methods, as in their purest form, are not guaranteed to give you monotonic improvement at every iteration, so they have a variant that sort of does line searches and so on.",
                    "label": 0
                },
                {
                    "sent": "To guarantee some kind of monotonicity, there are probably a lot of other things you could do to soup these up.",
                    "label": 0
                },
                {
                    "sent": "You could add multiple subgradients at each steps.",
                    "label": 0
                },
                {
                    "sent": "You could have iterates that remove old subgradients that don't seem to be relevant anymore.",
                    "label": 0
                },
                {
                    "sent": "I've worked on these kinds of methods in the context of stochastic programming about 8:00 or 10 years ago, and there's quite a lot of things you can do to sort of improve.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Heuristically, here's another approach.",
                    "label": 0
                },
                {
                    "sent": "Again, it's been around, and one of our latest speakers is a pioneer in this area, and that's using stochastic gradient descent, and this is very much in the tradition of algorithms.",
                    "label": 1
                },
                {
                    "sent": "Take a lot of steps at a very cheap to calculate, but can be good at finding approximate solutions.",
                    "label": 0
                },
                {
                    "sent": "So the idea here seems to be that you just take a very small subset of the data.",
                    "label": 1
                },
                {
                    "sent": "And write down an approximation to the primal problem that you really want to solve and use a subgradient of that as a search direction and take a step along that direction, OK?",
                    "label": 0
                },
                {
                    "sent": "The in the extreme case, you can just take a single data point and use that as the basis of a of the search direction.",
                    "label": 1
                },
                {
                    "sent": "So this is cheap if the icon is a small, there's some.",
                    "label": 0
                },
                {
                    "sent": "There's a code Pegasus that's again quite recent, where they choose ikk ranging in size from one up to something much bigger.",
                    "label": 0
                },
                {
                    "sent": "And I have this enhancement where after they take each step, they pull it back and projected onto a bull, because you know ultimately you're looking for a W with a fairly small two norm, so they explicitly do that after they take a step, and they can sometimes help the performance, and we've noticed that actually in the image denoising application.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Projection seems to really help.",
                    "label": 0
                },
                {
                    "sent": "There's another approach recently that involves taking a subgradient, but then putting in a curvature term that you guessed by a quasi Newton approximation.",
                    "label": 0
                },
                {
                    "sent": "I'm not going to say too much about that.",
                    "label": 0
                },
                {
                    "sent": "But it's nontrivial because you have to make sure that you get a descent step so it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not as simple as just taking a subgrade and adding on a second order term, the performer.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Seems to be similar to cutting plane methods.",
                    "label": 0
                },
                {
                    "sent": "There's a little bit I've got a few slides on alternative formulations.",
                    "label": 1
                },
                {
                    "sent": "Of course, the formulation I've been discussing is not the only game in town you can square the violations like this and get a problem of this form.",
                    "label": 0
                },
                {
                    "sent": "Let's see, and there's been some work by my colleague Living magazine Arian dating back quite awhile where and more recently it's been picked up by some other people where he basically does a Newton type method on this formulation, throws out the Intercept term.",
                    "label": 0
                },
                {
                    "sent": "So now he's got sort of a quadratic looking function that's unconstrained.",
                    "label": 0
                },
                {
                    "sent": "The only problem is it's not totally kosher because there's a discontinuity in the second derivative.",
                    "label": 1
                },
                {
                    "sent": "But you can kind of ignore that.",
                    "label": 0
                },
                {
                    "sent": "Take some element of the Hessian space generalization space, and do a Newton step on that on that basis.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if you include a line search, it often often seems to workout being quite efficient.",
                    "label": 0
                },
                {
                    "sent": "The main operation is to calculate the Newton step you have to solve a problem like this where this the matrix X is the matrix built up from all the feature vectors.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of a regularised least squares problem.",
                    "label": 0
                },
                {
                    "sent": "You can do, you can solve that using some sort of iterative method.",
                    "label": 0
                },
                {
                    "sent": "OK, so there's been some.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Success with that approach, there's another approach again, my colleague.",
                    "label": 0
                },
                {
                    "sent": "All the magazine is being very much involved in this idea, where instead of a W squared term, you've just got a one norm of W here, and the advantage of this is you just got a nice linear program.",
                    "label": 0
                },
                {
                    "sent": "You can't just usually hand this to see Plex, though, because if you're dealing with humongous datasets this can be too big for seaplex to deal with and to dance, and so you.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You sort of have to do.",
                    "label": 0
                },
                {
                    "sent": "Some sort of customized LP solvers to make that work.",
                    "label": 0
                },
                {
                    "sent": "I'll just mention this.",
                    "label": 0
                },
                {
                    "sent": "There's been there's an idea in the statistics literature from a couple of years ago of the elastic net where you are looking for a sparse weight vector.",
                    "label": 1
                },
                {
                    "sent": "But at the same time, you want to keep this property of, you know, separating hyperplanes and so on.",
                    "label": 0
                },
                {
                    "sent": "So the idea is to throw in both an L2 term ananel one term.",
                    "label": 0
                },
                {
                    "sent": "Into the classic primal formulation.",
                    "label": 0
                },
                {
                    "sent": "Does anyone has anyone tried this in this context?",
                    "label": 0
                },
                {
                    "sent": "Do you know?",
                    "label": 0
                },
                {
                    "sent": "I'd just be interested to see because it's been the statisticians have been using this, they they.",
                    "label": 1
                },
                {
                    "sent": "The story is that they think it does a better job with Lasso at group selecting elements of the weight vector.",
                    "label": 1
                },
                {
                    "sent": "So if you've got a number of features that are somehow correlated, apparently this kind of formulation is good at selecting or deselecting those features as a group.",
                    "label": 0
                },
                {
                    "sent": "OK, so it.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Would be an interesting thing to try.",
                    "label": 0
                },
                {
                    "sent": "Logistic regression, I just thought I'd mention this is comes from some recent work I did with Grace Wahba and her students at Wiscconsin where, where, where again we've got labeled data.",
                    "label": 0
                },
                {
                    "sent": "Feature vectors and labels and so on, and we construct a log likelihood function and we want to maximize the log likelihood.",
                    "label": 0
                },
                {
                    "sent": "But we're looking again for a sparse feature vector.",
                    "label": 0
                },
                {
                    "sent": "We're trying to identify the important features and typically we're looking for a very small number of features, like 5 maybe.",
                    "label": 0
                },
                {
                    "sent": "But our feature vectors is hugely long because one of the applications of this was trying to predict whether someone would get rheumatoid arthritis based on their genome.",
                    "label": 0
                },
                {
                    "sent": "OK, so you'd be looking at 10,000 base pairs from a genome sequence together with some other risk factors, and you'd be looking not just at those 10,000 pairs, but at interactions between them.",
                    "label": 0
                },
                {
                    "sent": "So you had a potentially at least polynomial.",
                    "label": 0
                },
                {
                    "sent": "You know 10,000 squared or 10,000 cube number of features, so we had a.",
                    "label": 0
                },
                {
                    "sent": "Very big problem.",
                    "label": 0
                },
                {
                    "sent": "There's all sorts of ways of reducing that, but in the end we so we add on this regularization term, which is an L1 term.",
                    "label": 0
                },
                {
                    "sent": "So we've got a negative log likelihood in an L1 term, and we.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Up using sort of a gradient projection approach.",
                    "label": 0
                },
                {
                    "sent": "Souped up a little bit with second order.",
                    "label": 0
                },
                {
                    "sent": "Second root of information so we sort of take steps based on minimizing this very simple approximation to the problem in which we approximate the Hessian of L bye nice separable term.",
                    "label": 0
                },
                {
                    "sent": "And then periodically we sort of explore the manifold that we were on, so we would look at the current set of nonzeros of features that are away from zero and we take reduced Newton steps in those features and we had some sort of randomization as a proxy for shrinking and various other heuristics, and we solve reasonable size.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problems, but I have no doubt that we could do.",
                    "label": 0
                },
                {
                    "sent": "There's more we can do.",
                    "label": 0
                },
                {
                    "sent": "OK in the last few minutes I want to talk about a couple of tools that have come up recently in the optimization literature that have sparked a lot of renewed interest.",
                    "label": 0
                },
                {
                    "sent": "Then I'm not sure if they've been people have tried to apply them to SVM yet, but they might be of current interest and.",
                    "label": 0
                },
                {
                    "sent": "The first one is this work of Nesterov and the Russian School on Optimal 1st order methods.",
                    "label": 0
                },
                {
                    "sent": "OK, so these are methods where you're taking steps based on function values and gradients.",
                    "label": 0
                },
                {
                    "sent": "You're not trying to look at second derivatives and your overall aim is to sort of.",
                    "label": 0
                },
                {
                    "sent": "You're often working with a given number of evaluations.",
                    "label": 0
                },
                {
                    "sent": "You assume that you can't do more than an evaluation, so you ask the question.",
                    "label": 0
                },
                {
                    "sent": "What's the most highest level of accuracy I can get in a given number of evaluations?",
                    "label": 0
                },
                {
                    "sent": "So it's more.",
                    "label": 0
                },
                {
                    "sent": "It's take more of a global perspective rather than this very greedy local perspective that you typically do in optimization where you just say OK.",
                    "label": 0
                },
                {
                    "sent": "Here's my search direction.",
                    "label": 0
                },
                {
                    "sent": "How much bank and I get on this iteration out of this search direction.",
                    "label": 0
                },
                {
                    "sent": "You sort of take a more global view of aiming, you know, in the long term, to get as accurate a solution as possible.",
                    "label": 0
                },
                {
                    "sent": "OK, so I've got a couple of slides, is very briefly introducing this.",
                    "label": 0
                },
                {
                    "sent": "Often they make use of parameters.",
                    "label": 1
                },
                {
                    "sent": "I'm just going to be assuming here trying to minimize the function F which is fairly smooth.",
                    "label": 1
                },
                {
                    "sent": "So often the Lipschitz constant for the gradient enters into the algorithm and the convexity parameter for app.",
                    "label": 1
                },
                {
                    "sent": "So the strong convexity parameter for F. OK, now you don't have to know these.",
                    "label": 0
                },
                {
                    "sent": "There are variants of these methods where you can approximate them OK.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A classic gradient method is where you just take a step in the negative gradient direction.",
                    "label": 0
                },
                {
                    "sent": "I think it's all very well known, particularly by people in this community that if you take a very short step length 1 / L, where L is the Lipschitz constant for the gradient, you get to send it every iteration, and you're able to show that after K iterations, the difference between your objective and the optimal objective is order 1 / K, so it's sub linear.",
                    "label": 0
                },
                {
                    "sent": "If you've got, if you know that your function F is strongly convex, you can do a little bit better.",
                    "label": 0
                },
                {
                    "sent": "OK, you can get a sort of a geometric convergence rate.",
                    "label": 0
                },
                {
                    "sent": "OK, now optimal methods try to do better than that.",
                    "label": 0
                },
                {
                    "sent": "They try to get a K squared.",
                    "label": 0
                },
                {
                    "sent": "In this case Anna better geometric konst.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "OK, so so they typically get these kind of rates if you don't.",
                    "label": 0
                },
                {
                    "sent": "If you don't have strong convexity, get 1 / K ^2.",
                    "label": 0
                },
                {
                    "sent": "If you do have strong convexity, you get a significantly better.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Each term here.",
                    "label": 0
                },
                {
                    "sent": "Now how do you get to that?",
                    "label": 0
                },
                {
                    "sent": "Well, it turns out you pay a price in at.",
                    "label": 0
                },
                {
                    "sent": "The algorithms become much more complicated than just taking a short step in the negative gradient direction.",
                    "label": 0
                },
                {
                    "sent": "The methods typically maintain not just a single sequence of X is but a sequence of X is wise, and sometimes Y, sometimes 3 sub three sequences that are all kind of interleaved and interacting, and so on.",
                    "label": 0
                },
                {
                    "sent": "But here's a simple instance of of a method that works for a strongly convex function.",
                    "label": 0
                },
                {
                    "sent": "So you maintain two sequences XC and YC.",
                    "label": 0
                },
                {
                    "sent": "You get the X case by doing the classic short steepest descent.",
                    "label": 0
                },
                {
                    "sent": "From the YK.",
                    "label": 0
                },
                {
                    "sent": "But then you update way K by doing kind of this weighted average between the last two X case.",
                    "label": 0
                },
                {
                    "sent": "They don't ask me to explain why this works.",
                    "label": 0
                },
                {
                    "sent": "Even people at work on these methods have a very hard time developing an intuition for what's going on.",
                    "label": 0
                },
                {
                    "sent": "The analysis is very, very technical.",
                    "label": 0
                },
                {
                    "sent": "OK in the weekly, even if you go to the weakly convex case and you don't have mu equals 0.",
                    "label": 0
                },
                {
                    "sent": "The method becomes significantly more complicated.",
                    "label": 0
                },
                {
                    "sent": "Again, you maintain the vexes and wiser definition of the X is the same, but now the definition of wise depends on this sort of very complicated scalar formula.",
                    "label": 0
                },
                {
                    "sent": "OK, which I won't even try to explain.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, and it sort of takes off from there.",
                    "label": 0
                },
                {
                    "sent": "It becomes even more complicated after that, so you can extend these methods to the case where you've got a constraint where you've got a regularization term, and even where you've got nonsmooth terms, and I think this is where the application to SVM might come in.",
                    "label": 0
                },
                {
                    "sent": "People have applied these to compress sensing problem.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And they seem to work fairly well.",
                    "label": 0
                },
                {
                    "sent": "OK, here is the Nesterov as a 2005 paper which I think is become very influential already, and he considers convex problems where you're trying to where the objective can be expressed.",
                    "label": 0
                },
                {
                    "sent": "In this way F of X you can get it by plugging X into this this function here and taking the Max over some other set.",
                    "label": 0
                },
                {
                    "sent": "So it's like a min Max type formulation.",
                    "label": 0
                },
                {
                    "sent": "You're trying to minimize the maximum minimize over X and maximize over you.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So this is the problem you really want to solve, but he convexified it by adding to this function a strongly convex function D and a regularization parameter mu.",
                    "label": 1
                },
                {
                    "sent": "An if you maximize over this function instead, you sort of get a smooth version of F which will call F of mu, so you're able to replace this function, which is often non smooth with a smooth variant, and then he essentially applies one of those optimal methods to this smooth function OK, and is able to show that given an iteration budget, suppose you're told you can only take N iterations.",
                    "label": 0
                },
                {
                    "sent": "He's able to come up with a method first of all for choosing the appropriate value of mu, you in fact choose it according this complex, complicated formula that.",
                    "label": 0
                },
                {
                    "sent": "Order of an inverse and then you run an iterations of your method and you end up with an accuracy of order 1 / N OK. Now these seem like very weak bounds, but they.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Seem to often work quite well in practice.",
                    "label": 0
                },
                {
                    "sent": "And Lastly, I'm going to end up with another tool that we found to be.",
                    "label": 0
                },
                {
                    "sent": "It's kind of related to the method I just mentioned.",
                    "label": 0
                },
                {
                    "sent": "That we found to be very useful in the image denoising problem for solving min Max type problems like the one I had in the previous slide.",
                    "label": 0
                },
                {
                    "sent": "So the idea is that you take alternating gradient steps in the primal and the dual space, or in the men in the Max space and each time you project back onto the feasible set like that Pegasus code does.",
                    "label": 0
                },
                {
                    "sent": "OK, so you take a step in X according to some step length gradient projection step you project back onto Big Axe, you take, then you take a step in V. Projecting back on the Big V OK, and.",
                    "label": 0
                },
                {
                    "sent": "I worked with Tony Chan and a student on applying things like this to image denoising.",
                    "label": 0
                },
                {
                    "sent": "The student ran off and sort of started doing really crazy things like instead of picking these step links to go to zero, he picked one of them to go to Infinity instead, and amazingly it gave much better performance.",
                    "label": 0
                },
                {
                    "sent": "So he wrote that up himself and publish it separately, which is which is great, 'cause I would never have.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Along with it, if you'd have, you try to get me involved.",
                    "label": 0
                },
                {
                    "sent": "So, but we have no idea, and I think it's fair to say that the student doesn't have any idea either.",
                    "label": 0
                },
                {
                    "sent": "Last time I checked, he was working on Wall Street, but I'm not sure if that's still true.",
                    "label": 0
                },
                {
                    "sent": "We have no idea why this works and we've I've had.",
                    "label": 0
                },
                {
                    "sent": "I've tried to involve a couple of my colleagues in trying to understand why this method works so well in practice and we don't know.",
                    "label": 0
                },
                {
                    "sent": "OK, but obviously it's the kind of thing you could also try on a primal dual.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Formulation of SVM OK.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'll finish there.",
                    "label": 0
                },
                {
                    "sent": "So thanks very much for your attention.",
                    "label": 0
                },
                {
                    "sent": "We have this movie.",
                    "label": 0
                },
                {
                    "sent": "Anybody?",
                    "label": 0
                },
                {
                    "sent": "Summarize.",
                    "label": 0
                },
                {
                    "sent": "Situations regular SPM?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I wouldn't attempt to do that because 80% of the people in the room would jump down my throat, but I don't have.",
                    "label": 0
                },
                {
                    "sent": "I don't have enough personal experience to be able to say, but I it's clear from even from the last three years of literature there's still a very active, subjective discussion.",
                    "label": 0
                },
                {
                    "sent": "And of course everyone puts numerical results in their paper that make their method look good.",
                    "label": 0
                },
                {
                    "sent": "I have no personal way of saying.",
                    "label": 0
                },
                {
                    "sent": "Based on my own experience with what's a person.",
                    "label": 0
                },
                {
                    "sent": "I have a question please.",
                    "label": 0
                },
                {
                    "sent": "The primary methods that you mentioned in the last we around the public are they still?",
                    "label": 0
                },
                {
                    "sent": "Are they published on the under the Nesterov stuff?",
                    "label": 0
                },
                {
                    "sent": "Yeah, I can give you some check report.",
                    "label": 0
                },
                {
                    "sent": "So yeah, at least one is being published.",
                    "label": 0
                },
                {
                    "sent": "Stuff OK.",
                    "label": 0
                },
                {
                    "sent": "Feeling is that you are going to need any help to region of the problem or the other one I see.",
                    "label": 0
                },
                {
                    "sent": "Current no, I have no experience with that.",
                    "label": 0
                },
                {
                    "sent": "I just thought I'd what one thing I've done, which is kind of similar is to use that standard regularised formulation, but with a.",
                    "label": 0
                },
                {
                    "sent": "The regularization term is kind of a group regularization term, so it's for example a sum of L2 norms of sub vectors.",
                    "label": 0
                },
                {
                    "sent": "So that way if you know our priority that certain features are correlated, you can sort of put them in the same sub vector, and so we have some experience of that.",
                    "label": 0
                },
                {
                    "sent": "But in the compressed sensing context, not in SVM.",
                    "label": 0
                },
                {
                    "sent": "So OK, that's good to know that it's been tried.",
                    "label": 0
                },
                {
                    "sent": "You have OK. No, I haven't tried the elastic net myself.",
                    "label": 0
                },
                {
                    "sent": "So I guess do we have an estimate about the?",
                    "label": 0
                },
                {
                    "sent": "The scalability of this message is this scale at square and is there any method that scales linearly and wallpaper?",
                    "label": 0
                },
                {
                    "sent": "Some authors my claim of claims about linear scaling.",
                    "label": 0
                },
                {
                    "sent": "For example, I think the cutting plane method is supposed to scale linearly if you implement it carefully.",
                    "label": 0
                },
                {
                    "sent": "But this is not.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't know.",
                    "label": 0
                },
                {
                    "sent": "Probably to some given accuracy, I don't know.",
                    "label": 0
                },
                {
                    "sent": "So this is not an area that I personally take a lot of interest in.",
                    "label": 0
                },
                {
                    "sent": "I'm kind of more interested in practical performances rather than theoretical error bounds, but I know there's a lot.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of people that do the latter, so I can't really give you.",
                    "label": 0
                },
                {
                    "sent": "I'm kind of coming at.",
                    "label": 0
                },
                {
                    "sent": "This is a bit of an outsider, so I don't.",
                    "label": 0
                },
                {
                    "sent": "I'm not immersed enough in the culture that too.",
                    "label": 0
                },
                {
                    "sent": "Have all these facts at my fingertips.",
                    "label": 0
                },
                {
                    "sent": "Customized.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I don't think it's column generation, but I can refer you to the papers of the people that have worked on that.",
                    "label": 0
                },
                {
                    "sent": "I can't recall right now what they did.",
                    "label": 0
                },
                {
                    "sent": "Column generation would be a natural thing to try.",
                    "label": 0
                },
                {
                    "sent": "But it might have been some sort of augmented Lagrangian type approach or something, maybe Kristen knows.",
                    "label": 0
                },
                {
                    "sent": "No.",
                    "label": 0
                },
                {
                    "sent": "Sorry, I mentioned that for the L when you use the L1 norm of W, you're going to LP and that you use specialized algorithms.",
                    "label": 0
                },
                {
                    "sent": "Do you remember what algorithms?",
                    "label": 0
                },
                {
                    "sent": "Overused and.",
                    "label": 0
                },
                {
                    "sent": "There was column generation OK. OK. Yep.",
                    "label": 0
                },
                {
                    "sent": "How about the natural method continuation method that raises the no.",
                    "label": 0
                },
                {
                    "sent": "No, it's not a continuation method.",
                    "label": 0
                },
                {
                    "sent": "What he does in the in the smoothing thing that I said at the end is just a one shot choice of the smoothing parameter.",
                    "label": 1
                },
                {
                    "sent": "There are a couple of other people, or there's one other paper very recently that's considered a continuation variant.",
                    "label": 0
                },
                {
                    "sent": "Will you try to choose a sequence of of smoothing parameters and kind of track the solution, but that hasn't.",
                    "label": 0
                },
                {
                    "sent": "That's one thing that I would really like to look at.",
                    "label": 0
                },
                {
                    "sent": "It's been on my To Do List for the last nine months or so.",
                    "label": 0
                },
                {
                    "sent": "But now currently he just says you've got a fixed budget of N. And given that you've got in evaluations here is the optimal choice of the smoothing parameter.",
                    "label": 0
                },
                {
                    "sent": "And then and.",
                    "label": 0
                },
                {
                    "sent": "It is not.",
                    "label": 0
                },
                {
                    "sent": "It's not an obvious way to improve on that.",
                    "label": 0
                }
            ]
        }
    }
}