{
    "id": "angycw2ly3qmt6txpi5k73e3vjbjh4op",
    "title": "Nonnegative Shared Subspace Learning and Its Application to Social Media Retrieval",
    "info": {
        "author": [
            "Sunil Kumar Gupta, Curtin University of Technology"
        ],
        "published": "Oct. 1, 2010",
        "recorded": "July 2010",
        "category": [
            "Top->Computer Science->Information Retrieval"
        ]
    },
    "url": "http://videolectures.net/kdd2010_gupta_nssli/",
    "segmentation": [
        [
            "Hello everybody, so today I'm presenting on non negative shared subspace learning.",
            "Basically it's.",
            "An example of unsupervised.",
            "Task learning, which will be."
        ],
        [
            "Clear from the later talks so.",
            "The Today on the today's talk today.",
            "I'll be basically giving an introduction along with motivation and.",
            "And then we talk about shared subspace learning and then some applications on social media retrieval.",
            "Excuse me it's going automatic so.",
            "Well so."
        ],
        [
            "So when we talk about social tags, so basically they have they have a lot of potentials to improve the search and personal organization and have been very instrumental in the popularity of Delicious Flickr and YouTube sites and their work.",
            "So they have used it too.",
            "I mean good extent, but still the tags have some problems because they are very subjective to users.",
            "They are very ambiguous, certain times and.",
            "Most of the time they are incomplete due to the lack of constraints imposed on the users while their creation."
        ],
        [
            "So these tags, if they are directly used for the retrieval applications.",
            "Generally the performance is not very good, so.",
            "The question is can we improve this retrieval using transfer learning so there there exists some other tagging systems, also called folksonomies which actually have been created for different purposes like label me for object detection and the users.",
            "I mean basically the creators are asked to annotate all the items properly without much noise, an subjectiveness.",
            "So basically they are given a control dictionary to create this an.",
            "That that makes the label me type of data set very basically less noisy, so.",
            "The question is, can we do some knowledge transfer because this label me data set has got the similar type of patterns in the eyes in the Flickr tags.",
            "So."
        ],
        [
            "Like here is an example showing Flickr image with it stacks along with label image label My image with tags.",
            "So the first one shows that like there are some tags which are.",
            "I mean maybe relevant at semantic level but not directly relevant with the existence of the image, whereas the label me image has got all those which is visible in the image as their tags.",
            "So there are some related works in which model it'll basically have studied user tagging behavior.",
            "They say what's the motivation for the user to tag an how the folksonomies developed?",
            "And then little presented method to basically compute the relevance of these tags.",
            "How good are how relevant A tag is to the image?",
            "So basically these give some important through the tag, like when we consider any datamining tags and task using these tags.",
            "So how much importance we should give to each tag?",
            "Similarly long?",
            "It'll also do content based processing an.",
            "They also do some text based retrieval and fuse the two results to get the good retrieval."
        ],
        [
            "Comments.",
            "So the framework now I'm going to propose is like if we have a noisy data set an at the same time, which is a cleaner data set.",
            "So can we transfer the knowledge from the cleaner data set to the noisy data set?",
            "So since it's a text mining type of application, which currently we are considering using tags, so our framework is based on MF like framework in which a non negative matrix is factorized using.",
            "I mean in two other non negative matrix, basically a subspace and it's encoding metrics."
        ],
        [
            "So so the framework which we propose is called J SMF.",
            "Basically non negative shared subspace learning in which.",
            "So we have a data set like label me which is not so noisy whereas the other one which is noisy.",
            "So since both the datasets are related so there exists some common attributes which basically encode the so there is a common subspace which basically in which lies the common attributes and the individual sub space between the both the data set.",
            "Because both I mean each data set has its own differences.",
            "So the framework is now we try to factorize the two data matrices.",
            "So we basically form two data matrices from the two data sources and try to factorize them in such a way that like exes, WH&Y is doubl.",
            "So W is something which is common between the two and tries to encode the commonality between the two data sources.",
            "And since W is also learned from both the data sources so it learns the good knowledge from the cleaner data set.",
            "There's an in effect.",
            "In effect, the knowledge is transferred.",
            "So for that we try to to learn this metric factorization.",
            "We try to learn this basically optimize this cost function which is shown below.",
            "So for this this basically it's kind of multiplicative algorithm, so the proof for the convergence of this optimization goes along the lines of."
        ],
        [
            "Nemeth so to illustrate the idea here, just consider two synthetic datasets like one in the red color and in the leftmost plot an other in the blue color.",
            "So if we use enemies separately on the two, we get 2 red vectors, whereas on the blue data set we get to blue vectors.",
            "So we get for the shared portion where the two clusters are close by you get two different vectors which are kind of subjective in its own domain, but if you use that, if you see the third plot.",
            "In which this proposed framework is used so the subjectivity reduces because you get a common vector for the shared one which is now influenced by both the cleaner and noisy data set.",
            "So that helps in getting a better basis vector for that and at the same time you also have their individual vectors.",
            "Which are in the red and blue.",
            "So at the same time not using the domain specific information.",
            "So."
        ],
        [
            "Once you learn the subspace which is.",
            "Which is this WU&H?",
            "So now we can use this subspace for social media retrieval.",
            "So given any query I mean query set of words and vocabulary, we can create a query vector in the subspace and then project it into the learn software which is now W. So.",
            "After we get the query representation in this modified subspace, we can get it get the similarity with the documents because we have already projected the documents into that then finally we rank the similarities an get the retrieved items."
        ],
        [
            "So basically to test the idea, we got the data set.",
            "We created data set because this kind of like having commonalities and difference data set does not exist along with the ground truth.",
            "So we created one with downloading 50,000, I mean meta data for 50,000 images from Flickr an some 12,000 videos and use some label me images because label me images have tags which are kind of cleaner so.",
            "And for downloaded we use the variety of concepts."
        ],
        [
            "So the question is now when we share between the two data sources, what should be the dimension dimensionality of the sharing metrics?",
            "Because a lot of things depend on that.",
            "So one approach was like just find the common number of tags between the two data sources an.",
            "I mean, there was some.",
            "It's called rule of thumb searches by media it'll so they say that like like in K means.",
            "If you have let's say thousand 100,000 number of data points.",
            "So how do you select how many number of clusters?",
            "So it's kind of same same problem.",
            "So some people prefer log off this feature.",
            "Well I mean lock up the number of data points.",
            "So somebody suggest that you can choose like this.",
            "But there is, I think more system."
        ],
        [
            "Way to choose the value of sharing dimensionality, which is like if you have data set X and data set Y and representing matrices encoded by X&Y.",
            "So if you take the rank of X, transpose Y, assuming that the subspace matrices are now mutually orthogonal so the key is rank of K is nothing but dimensionality of the shared subspace which is rank of Now X transpose Y.",
            "But in case of NMF as we are all aware that the subspace matrices are not mutually orthogonal so but they are approximately orthogonal because MFS part based.",
            "In addition, so the K when we check heuristically that it's almost equal to rank of X transpose Y so it gives a good indication on what subspace dimensionality should we choose."
        ],
        [
            "And for the shared one.",
            "So, so for the comparing we choose two baselines, in which first case we just take the NMF without any sharing through level me or any other auxiliary sources.",
            "The second baseline we choose with like instead of retaining the individual subspace of both the data set, we say that OK, all we do is like we take two data sources and combine them and have a common one.",
            "That's it, no need.",
            "I mean we're not creating any individual or different subspaces, so.",
            "When we do that, we can see that from the plot that for different level of sharing, I mean when the sharing dimension changes, we say we see a very interesting some sort of curve which has a optimum sharing dimensionality.",
            "So if you do not share anything, so we see that.",
            "So you can see from the table that for Flickr we got for sharing some 50% precision at recall .1, whereas for baseline two we got only 46% because we did not respect the data by.",
            "Enforcing too much sharing between them, whereas GSM with optimal value for Flickr.",
            "It was like equal to 15 for the given data set, so we got 58% performance whereas for YouTube the same was 48.",
            "So this basically gives you 10 to 12% improvement in the performance."
        ],
        [
            "So these are the retrieval results.",
            "Basically I mean.",
            "Depicting in terms of different measures.",
            "So one is like precision scope, which is like how many items are relevant in first.",
            "In our first 20.",
            "So that's called MIP at 10.",
            "So the bar plot shows that like the first one, which is like Pierre, 10, P. 20 and P. 8:50.",
            "So basically we can see that the GSM works better than only NMF and fully shared cases an the 4th bar plot is also basically gives map figures.",
            "Mean average precision, so we also see more than six 7% improvement in map values as well.",
            "So the second plot on the right side, it shows 11 point interpolated precision.",
            "Recall precision recall curve."
        ],
        [
            "So the similar results we can see for YouTube retrieval.",
            "There is a big difference between the GSM F and the other counterparts.",
            "No sharing and fully sharing cases."
        ],
        [
            "So so basically, sharing sharing in a controlled fashion becomes fruitful for, and we show that this is a successful example of transfer learning, provided the sharing is controlled.",
            "So basically, to conclude, we presented a non negative shared subspace learning framework which can be applied to different tasks.",
            "Though we demonstrated its application today on A tag and social media retrieval task, we empirically demonstrated that control sharing is crucial to avoid any whatsoever negative transfer negative knowledge transfer from auxiliary data sources.",
            "Because when we did fully shared it led to a. I mean reduce performance.",
            "So with this I finish the talk and I'll be happy to take some questions, if any."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Hello everybody, so today I'm presenting on non negative shared subspace learning.",
                    "label": 1
                },
                {
                    "sent": "Basically it's.",
                    "label": 0
                },
                {
                    "sent": "An example of unsupervised.",
                    "label": 0
                },
                {
                    "sent": "Task learning, which will be.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Clear from the later talks so.",
                    "label": 0
                },
                {
                    "sent": "The Today on the today's talk today.",
                    "label": 0
                },
                {
                    "sent": "I'll be basically giving an introduction along with motivation and.",
                    "label": 0
                },
                {
                    "sent": "And then we talk about shared subspace learning and then some applications on social media retrieval.",
                    "label": 1
                },
                {
                    "sent": "Excuse me it's going automatic so.",
                    "label": 0
                },
                {
                    "sent": "Well so.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So when we talk about social tags, so basically they have they have a lot of potentials to improve the search and personal organization and have been very instrumental in the popularity of Delicious Flickr and YouTube sites and their work.",
                    "label": 1
                },
                {
                    "sent": "So they have used it too.",
                    "label": 0
                },
                {
                    "sent": "I mean good extent, but still the tags have some problems because they are very subjective to users.",
                    "label": 0
                },
                {
                    "sent": "They are very ambiguous, certain times and.",
                    "label": 1
                },
                {
                    "sent": "Most of the time they are incomplete due to the lack of constraints imposed on the users while their creation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these tags, if they are directly used for the retrieval applications.",
                    "label": 1
                },
                {
                    "sent": "Generally the performance is not very good, so.",
                    "label": 0
                },
                {
                    "sent": "The question is can we improve this retrieval using transfer learning so there there exists some other tagging systems, also called folksonomies which actually have been created for different purposes like label me for object detection and the users.",
                    "label": 1
                },
                {
                    "sent": "I mean basically the creators are asked to annotate all the items properly without much noise, an subjectiveness.",
                    "label": 0
                },
                {
                    "sent": "So basically they are given a control dictionary to create this an.",
                    "label": 0
                },
                {
                    "sent": "That that makes the label me type of data set very basically less noisy, so.",
                    "label": 0
                },
                {
                    "sent": "The question is, can we do some knowledge transfer because this label me data set has got the similar type of patterns in the eyes in the Flickr tags.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Like here is an example showing Flickr image with it stacks along with label image label My image with tags.",
                    "label": 0
                },
                {
                    "sent": "So the first one shows that like there are some tags which are.",
                    "label": 0
                },
                {
                    "sent": "I mean maybe relevant at semantic level but not directly relevant with the existence of the image, whereas the label me image has got all those which is visible in the image as their tags.",
                    "label": 0
                },
                {
                    "sent": "So there are some related works in which model it'll basically have studied user tagging behavior.",
                    "label": 1
                },
                {
                    "sent": "They say what's the motivation for the user to tag an how the folksonomies developed?",
                    "label": 1
                },
                {
                    "sent": "And then little presented method to basically compute the relevance of these tags.",
                    "label": 0
                },
                {
                    "sent": "How good are how relevant A tag is to the image?",
                    "label": 0
                },
                {
                    "sent": "So basically these give some important through the tag, like when we consider any datamining tags and task using these tags.",
                    "label": 0
                },
                {
                    "sent": "So how much importance we should give to each tag?",
                    "label": 0
                },
                {
                    "sent": "Similarly long?",
                    "label": 0
                },
                {
                    "sent": "It'll also do content based processing an.",
                    "label": 1
                },
                {
                    "sent": "They also do some text based retrieval and fuse the two results to get the good retrieval.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Comments.",
                    "label": 0
                },
                {
                    "sent": "So the framework now I'm going to propose is like if we have a noisy data set an at the same time, which is a cleaner data set.",
                    "label": 0
                },
                {
                    "sent": "So can we transfer the knowledge from the cleaner data set to the noisy data set?",
                    "label": 0
                },
                {
                    "sent": "So since it's a text mining type of application, which currently we are considering using tags, so our framework is based on MF like framework in which a non negative matrix is factorized using.",
                    "label": 0
                },
                {
                    "sent": "I mean in two other non negative matrix, basically a subspace and it's encoding metrics.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so the framework which we propose is called J SMF.",
                    "label": 0
                },
                {
                    "sent": "Basically non negative shared subspace learning in which.",
                    "label": 1
                },
                {
                    "sent": "So we have a data set like label me which is not so noisy whereas the other one which is noisy.",
                    "label": 0
                },
                {
                    "sent": "So since both the datasets are related so there exists some common attributes which basically encode the so there is a common subspace which basically in which lies the common attributes and the individual sub space between the both the data set.",
                    "label": 0
                },
                {
                    "sent": "Because both I mean each data set has its own differences.",
                    "label": 1
                },
                {
                    "sent": "So the framework is now we try to factorize the two data matrices.",
                    "label": 0
                },
                {
                    "sent": "So we basically form two data matrices from the two data sources and try to factorize them in such a way that like exes, WH&Y is doubl.",
                    "label": 0
                },
                {
                    "sent": "So W is something which is common between the two and tries to encode the commonality between the two data sources.",
                    "label": 0
                },
                {
                    "sent": "And since W is also learned from both the data sources so it learns the good knowledge from the cleaner data set.",
                    "label": 0
                },
                {
                    "sent": "There's an in effect.",
                    "label": 0
                },
                {
                    "sent": "In effect, the knowledge is transferred.",
                    "label": 1
                },
                {
                    "sent": "So for that we try to to learn this metric factorization.",
                    "label": 0
                },
                {
                    "sent": "We try to learn this basically optimize this cost function which is shown below.",
                    "label": 0
                },
                {
                    "sent": "So for this this basically it's kind of multiplicative algorithm, so the proof for the convergence of this optimization goes along the lines of.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nemeth so to illustrate the idea here, just consider two synthetic datasets like one in the red color and in the leftmost plot an other in the blue color.",
                    "label": 0
                },
                {
                    "sent": "So if we use enemies separately on the two, we get 2 red vectors, whereas on the blue data set we get to blue vectors.",
                    "label": 0
                },
                {
                    "sent": "So we get for the shared portion where the two clusters are close by you get two different vectors which are kind of subjective in its own domain, but if you use that, if you see the third plot.",
                    "label": 0
                },
                {
                    "sent": "In which this proposed framework is used so the subjectivity reduces because you get a common vector for the shared one which is now influenced by both the cleaner and noisy data set.",
                    "label": 0
                },
                {
                    "sent": "So that helps in getting a better basis vector for that and at the same time you also have their individual vectors.",
                    "label": 0
                },
                {
                    "sent": "Which are in the red and blue.",
                    "label": 0
                },
                {
                    "sent": "So at the same time not using the domain specific information.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Once you learn the subspace which is.",
                    "label": 1
                },
                {
                    "sent": "Which is this WU&H?",
                    "label": 1
                },
                {
                    "sent": "So now we can use this subspace for social media retrieval.",
                    "label": 1
                },
                {
                    "sent": "So given any query I mean query set of words and vocabulary, we can create a query vector in the subspace and then project it into the learn software which is now W. So.",
                    "label": 1
                },
                {
                    "sent": "After we get the query representation in this modified subspace, we can get it get the similarity with the documents because we have already projected the documents into that then finally we rank the similarities an get the retrieved items.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So basically to test the idea, we got the data set.",
                    "label": 0
                },
                {
                    "sent": "We created data set because this kind of like having commonalities and difference data set does not exist along with the ground truth.",
                    "label": 0
                },
                {
                    "sent": "So we created one with downloading 50,000, I mean meta data for 50,000 images from Flickr an some 12,000 videos and use some label me images because label me images have tags which are kind of cleaner so.",
                    "label": 1
                },
                {
                    "sent": "And for downloaded we use the variety of concepts.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the question is now when we share between the two data sources, what should be the dimension dimensionality of the sharing metrics?",
                    "label": 0
                },
                {
                    "sent": "Because a lot of things depend on that.",
                    "label": 0
                },
                {
                    "sent": "So one approach was like just find the common number of tags between the two data sources an.",
                    "label": 1
                },
                {
                    "sent": "I mean, there was some.",
                    "label": 1
                },
                {
                    "sent": "It's called rule of thumb searches by media it'll so they say that like like in K means.",
                    "label": 0
                },
                {
                    "sent": "If you have let's say thousand 100,000 number of data points.",
                    "label": 0
                },
                {
                    "sent": "So how do you select how many number of clusters?",
                    "label": 0
                },
                {
                    "sent": "So it's kind of same same problem.",
                    "label": 1
                },
                {
                    "sent": "So some people prefer log off this feature.",
                    "label": 0
                },
                {
                    "sent": "Well I mean lock up the number of data points.",
                    "label": 0
                },
                {
                    "sent": "So somebody suggest that you can choose like this.",
                    "label": 0
                },
                {
                    "sent": "But there is, I think more system.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Way to choose the value of sharing dimensionality, which is like if you have data set X and data set Y and representing matrices encoded by X&Y.",
                    "label": 0
                },
                {
                    "sent": "So if you take the rank of X, transpose Y, assuming that the subspace matrices are now mutually orthogonal so the key is rank of K is nothing but dimensionality of the shared subspace which is rank of Now X transpose Y.",
                    "label": 0
                },
                {
                    "sent": "But in case of NMF as we are all aware that the subspace matrices are not mutually orthogonal so but they are approximately orthogonal because MFS part based.",
                    "label": 0
                },
                {
                    "sent": "In addition, so the K when we check heuristically that it's almost equal to rank of X transpose Y so it gives a good indication on what subspace dimensionality should we choose.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for the shared one.",
                    "label": 0
                },
                {
                    "sent": "So, so for the comparing we choose two baselines, in which first case we just take the NMF without any sharing through level me or any other auxiliary sources.",
                    "label": 0
                },
                {
                    "sent": "The second baseline we choose with like instead of retaining the individual subspace of both the data set, we say that OK, all we do is like we take two data sources and combine them and have a common one.",
                    "label": 0
                },
                {
                    "sent": "That's it, no need.",
                    "label": 0
                },
                {
                    "sent": "I mean we're not creating any individual or different subspaces, so.",
                    "label": 0
                },
                {
                    "sent": "When we do that, we can see that from the plot that for different level of sharing, I mean when the sharing dimension changes, we say we see a very interesting some sort of curve which has a optimum sharing dimensionality.",
                    "label": 0
                },
                {
                    "sent": "So if you do not share anything, so we see that.",
                    "label": 0
                },
                {
                    "sent": "So you can see from the table that for Flickr we got for sharing some 50% precision at recall .1, whereas for baseline two we got only 46% because we did not respect the data by.",
                    "label": 0
                },
                {
                    "sent": "Enforcing too much sharing between them, whereas GSM with optimal value for Flickr.",
                    "label": 0
                },
                {
                    "sent": "It was like equal to 15 for the given data set, so we got 58% performance whereas for YouTube the same was 48.",
                    "label": 0
                },
                {
                    "sent": "So this basically gives you 10 to 12% improvement in the performance.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So these are the retrieval results.",
                    "label": 0
                },
                {
                    "sent": "Basically I mean.",
                    "label": 0
                },
                {
                    "sent": "Depicting in terms of different measures.",
                    "label": 0
                },
                {
                    "sent": "So one is like precision scope, which is like how many items are relevant in first.",
                    "label": 0
                },
                {
                    "sent": "In our first 20.",
                    "label": 0
                },
                {
                    "sent": "So that's called MIP at 10.",
                    "label": 0
                },
                {
                    "sent": "So the bar plot shows that like the first one, which is like Pierre, 10, P. 20 and P. 8:50.",
                    "label": 0
                },
                {
                    "sent": "So basically we can see that the GSM works better than only NMF and fully shared cases an the 4th bar plot is also basically gives map figures.",
                    "label": 1
                },
                {
                    "sent": "Mean average precision, so we also see more than six 7% improvement in map values as well.",
                    "label": 0
                },
                {
                    "sent": "So the second plot on the right side, it shows 11 point interpolated precision.",
                    "label": 0
                },
                {
                    "sent": "Recall precision recall curve.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the similar results we can see for YouTube retrieval.",
                    "label": 0
                },
                {
                    "sent": "There is a big difference between the GSM F and the other counterparts.",
                    "label": 0
                },
                {
                    "sent": "No sharing and fully sharing cases.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So so basically, sharing sharing in a controlled fashion becomes fruitful for, and we show that this is a successful example of transfer learning, provided the sharing is controlled.",
                    "label": 0
                },
                {
                    "sent": "So basically, to conclude, we presented a non negative shared subspace learning framework which can be applied to different tasks.",
                    "label": 1
                },
                {
                    "sent": "Though we demonstrated its application today on A tag and social media retrieval task, we empirically demonstrated that control sharing is crucial to avoid any whatsoever negative transfer negative knowledge transfer from auxiliary data sources.",
                    "label": 1
                },
                {
                    "sent": "Because when we did fully shared it led to a. I mean reduce performance.",
                    "label": 0
                },
                {
                    "sent": "So with this I finish the talk and I'll be happy to take some questions, if any.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}