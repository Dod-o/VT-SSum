{
    "id": "5b2zric4a5m5rdqdbecf3eaykct6g5mq",
    "title": "Limitations of kernel and multiple kernel learning",
    "info": {
        "author": [
            "John Shawe-Taylor, Centre for Computational Statistics and Machine Learning, University College London"
        ],
        "published": "Oct. 17, 2011",
        "recorded": "September 2011",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods"
        ]
    },
    "url": "http://videolectures.net/simbad2011_shawe_taylor_kernel/",
    "segmentation": [
        [
            "Yes, I was thinking about what I should speak about to an audience that is.",
            "Interested in similarity functions and not kernels and.",
            "Believes that kernels aren't the answer.",
            "So this was a challenge for somebody who spent 10 years trying to promote kernel methods.",
            "But I think there are some very interesting questions here and and some quite.",
            "Insightful results that are worth talking about.",
            "However, I'm going to.",
            "Pitch it slightly.",
            "In the tone of the Empire fights back in that I'm going to argue that multiple kernel learning can overcome some of the limitations that that have been thrown at kernel methods.",
            "Now in terms."
        ],
        [
            "Of the so this is just an overview, so I'm going to talk first about the relationship to Simbad and.",
            "Argue that there are two ways of looking at the question of whether similarity is necessary.",
            "Similarity functions that are not Euclidean, unnecessary or not.",
            "I'm then going to talk about some previous work that.",
            "Talks about this question of when function classes can be represented using kernel methods.",
            "I'm gonna dip into some theory which will motivate some of the later results are mention, so I'll talk.",
            "Give a very brief introduction to Rademacher complexity, which is used to bound the generalization of.",
            "The classifiers or or other learning systems generally and that will lead me to motivate an algorithm that I'm calling linear programming boosting, which is a kind of adaptation of the boosting approach.",
            "And that I will argue can address some of the weaknesses in this.",
            "I highlighted in this earlier work on.",
            "The limitations of kernel methods and, but I'll then sort of join the two together with this multiple kernel learning, so I'll present multiple kernel learning if you like as an extension of boosting and then show you some new bounds that apply that can.",
            "Justify using really large number of different kernels in multiple kernel learning, so I think multiple kind of learnings, perhaps not been approached in the right way and you know I would like to think that the theory can motivate being a bit more.",
            "Courageous in using multiple kernels."
        ],
        [
            "So then I'll draw some conclusions.",
            "So first relation to Simbad and the question is when can we learn using Euclidean representations?",
            "I would say that was quite a sort of fundamental starting point for this embed philosophy, which was to argue that we need to move to deal with similarity functions that perhaps can't be represented as distances in a Euclidean space.",
            "I would say that you could ask that question at 2 two levels.",
            "The first is is the data naturally separable in the given representation?",
            "In other words, we receive the input data or the data with a particular representation, and we ask, can we use that representation?",
            "Or perhaps adjust that representation to ensure that it is separable?",
            "So in some sense it's something about the way the data is given to us.",
            "But there is an alternative way.",
            "Which now relates to really a class if the set of classifiers over some inputs X.",
            "And here we can just ask the question, is there an embedding so we don't worry about actually how X is represented?",
            "We just think of X as.",
            "Markers for inputs and we're asking, you know, can there be any?",
            "So we're not asking can we find a mapping from the input space that we're given some nice mapping that will give it into a space, but just is there any mapping is there it?",
            "Does there exist a mapping into a space in which we can separate these concepts?",
            "With linear threshold we can represent these concepts with linear threshold functions, so I hope that distinction is clear that the question here.",
            "Is simply in principle representation not actually how to construct it explicitly, so I'm going to consider this second question.",
            "So, given a set of classifiers, when or or can we say when or if it's likely to be the case that we can find a space where we map our inputs and then represent all the functions in H as linear threshold functions?"
        ],
        [
            "So first thing to notice is if we ignore the how the actual data is given to us in terms of representation, it's clear that any any classifier can be represented by a large margin linear threshold.",
            "Classifier simply by choosing this embedding.",
            "So let me just get my.",
            "Fancy pointer here.",
            "So if we consider the embedding that just takes the input and Maps it to its classification, right?",
            "So plus one if it's a positive minus one.",
            "If it's a negative, then clearly.",
            "We have a large margin classifier which just takes the sign of that value and gives perfect classification.",
            "So clearly I mean this is just a tautology really, but I'm all I'm pointing out is that large margin classification is certainly possible if we restrict ourselves to a single function and don't worry about how we actually compute this mapping.",
            "Of course you know this is jumping the question of how we compute this mapping.",
            "So the question that I want to address is, sorry, what if we have a set of classifiers?",
            "Is there an embedding that's good for all of them?",
            "So this is just for one function, we can find that embedding, But what about if we have a whole set of classifiers?",
            "Can we find an embedding that works for all of them?",
            "At."
        ],
        [
            "Once.",
            "And this is being studied.",
            "A few years ago, and surprisingly, there are many classes that are known to be learnable for which no such embedding exists.",
            "So this was work.",
            "The original paper in this area, I think, was this one by Bendavid, Iran and Simon and sorry semen in 2002.",
            "And what they show is that basically there for.",
            "If you look at classes of VC Dimension D on M inputs, then only a vanishing fraction can be embedded into Euclidean space of dimension much less than M. So meaning that you're in a VC, you will not be able to learn in such a space.",
            "So what they're saying here is sorry, I should perhaps just make a note that VC dimension D means that.",
            "These functions are learnable with an error bound scaling as the square root of DRM.",
            "I'm not considering here the practical computational issues, just the statistical question of whether we can from a certain amount of data, reliably identify the correct function in the class, and the error will scale as we learn from M examples in a VC dimension D space, the error bound will scale as the square root of Deon M. Plus the error term or even D on EM if we have the exact representative, there's no noise in the in the training data, so certainly learnable, but we aren't able to for the vast majority of these VC classes, we aren't able to embed them into a Euclidean space in such a way that we could learn using linear threshold functions.",
            "So this seems like a quite serious result and the only downside is this accounting argument that they use.",
            "So basically there are just too many VC classes or another way of viewing it.",
            "There are two few linear threshold classes, so simply there's no way we're going to get every BC class represented by a linear threshold class of small dimension."
        ],
        [
            "Right?",
            "But you might think, well, wait a minute.",
            "You know, we know that we can't learn with low dimensional spaces.",
            "That's everyone knows that you just use support vector machines, right?",
            "That's the whole idea of support vector machines you project into high dimensional space and then you learn in that high space and you don't have to worry about the dimension because you overcome the curse of dimensionality by maximizing the margin.",
            "Brilliant.",
            "And here's the band that we would get from this Rademacher.",
            "Stuff I'll talk about in a minute so we would have the probability of misclassification of a randomly drawn test point is bounded by some terms.",
            "This is the slack variables from the optimization of the SVM.",
            "This is the margin gamma.",
            "So clearly as the margin grows we get a.",
            "A reasonable bound on the on the off training set performance.",
            "And if we consider a Gaussian kernel, this even simplifies further because this term just becomes the square root of M, and so we get this.",
            "This kind of scaling with a clear dependence on the.",
            "Essentially training error.",
            "This is the slack variables measuring how many points fail to achieve the margin, and this is the margin.",
            "So that all looks fine surely though."
        ],
        [
            "Is not a problem then unfortunately there is and this is actually spawned.",
            "Another interesting direction of research which has shown an intimate connection between the effect of having a large margin and resilience to random projection.",
            "So Balcan, Blum and Vempala in 2004 studied this this property.",
            "And basically what they show is that bounds on large margin classification can be obtained showing that random projections into a low dimensional space.",
            "Ensure that you still have good linear separability.",
            "So if you project randomly into a space of this dimension, there's a few, maybe a little bit glib.",
            "What I'm saying it's a little bit complicated how they do it, but but basically it boils down to projecting into a space of this dimension, and you'll get reasonably good linear separation, and so your large margin actually corresponds to low dimension and.",
            "We're back to square one, so actually those non representability results also apply to the support vector machines.",
            "If we could learn the class with the support vector machines, we would be able to project it and we'd have a low dimensional representation of that class, which we know for most VC classes is not possible by the previous result.",
            "So results already obtained by Ben David and Ann coauthors.",
            "Showed this as well.",
            "Actually they included this this idea, so they're actually very powerful results.",
            "I mean, stepping back what they say is for the overwhelming majority of classes that can be learned.",
            "You know they're learnable.",
            "No kernel that will render them learnable by SVM.",
            "You know it's something that perhaps isn't dwelled on, because what do you do with it?",
            "But it's it's quite a powerful, I think, sort of wake up call if you like.",
            "Perhaps one weakness that I'll talk about a little bit is the fact that it's only an existence proof.",
            "One would like to see one of these classes and see what they actually look like, and perhaps get some insight."
        ],
        [
            "So work has been done by Foster ET al on advancing the technology of analyzing this problem, and basically they've developed some some technology that relies on the analysis of this sign matrix N of the concepts in the class.",
            "So basically, if you think of M as having rows indexed by examples and columns by classifiers, and then there's a plus one in the entry of.",
            "It's a positive classification of that example and a -- 1 if it's a negative.",
            "And then you can they manage to lower bound the dimension that you need to represent this.",
            "This sort of set of concepts.",
            "By a linear classifiers, no margin here, just just.",
            "Threshold threshold linear functions by this this quantity.",
            "So this is the spectral norm of M square root of the product of the number of well, number of entries.",
            "And this is another lower bound that involves the sum of the first D spectral singular values.",
            "So using these techniques."
        ],
        [
            "So they are able to apply them to a few concrete functioning classes, and I actually was a bit disappointed when I was looking sort of checking out this stuff to put this talk together, 'cause I thought there was some very kind of clear examples where you know they got lower bounds that were much higher than what could be learned by just say a standard VC dimension bound, but in fact.",
            "There didn't seem to be any, so I was a little bit disappointed by this.",
            "So the one of the examples that they give is monomial's over N Boolean variables and all functions that representable by conjunctions of literals, variables or their negations.",
            "And they show that this class can only be embedded with a margin of at most 1 / sqrt N. But actually that doesn't make really a big distinction because it's got VC dimension end.",
            "So this looks about the right dimension to embed in.",
            "So I was a little bit disappointed.",
            "I thought there was some quite nice examples of that type, so this I think is a nice open question.",
            "Could we actually generate some?",
            "Some good, you know, difficult classes, sorry, easy classes, but.",
            "Have very high dimensional.",
            "You need a high dimension to embed them into linear threshold functions.",
            "I should say that the technology is now being used by razborov to get lower bounds for a C nought and DNF embeddings, but.",
            "That again, I don't think the way he's done it.",
            "We can quite read off as it work as he's taken the class of all polynomially sized DNA functions, so this is, you know.",
            "I think a fruitful area of research, but it's it hasn't quite given what I was expecting at this point.",
            "So anyway, I still you know, I would think that it should be possible to find a concrete function class.",
            "Which would be nice in terms of understanding what's going on more."
        ],
        [
            "Carefully.",
            "So anyway.",
            "That's just to give you a background on this question.",
            "Now what I want to do in this talk is revisit this impasse by turning to an alternative large margin approach.",
            "And what I'm going to look at is maximizing the margin while controlling the one norm of the weight vector, and I'm going to argue that this would demonstrate this gives a form of boosting, so we're going to sort of take the SVM idea, but rather than the two norm, have the one norm.",
            "So and then I'm going to show that you can actually also combine so you can make a sort of mixture of the two in what's becomes multiple kernel learning.",
            "But first I would like to sort of just set the scene as far as the error bounds are concerned, because I'll be using those later on when I come to bounding the performance of multiple kernel learning because I'll need that to argue that we can actually do this.",
            "Actually embed using this approach in a way that we can learn all."
        ],
        [
            "C classes, so here's the main Rademacher theorem.",
            "So if we take a function class F. Then here it is, sort of.",
            "Calligraphic F. If we choose any function in F, then with probability 1 minus Delta, it's expected value.",
            "Think of this as a loss function.",
            "If you like, you know in the classification this be the 01 indicator function of a misclassification or something similar.",
            "This is the empirical version.",
            "I've put a hat on it to indicate empirical version.",
            "So the true error is bounded by its empirical error plus a correction term and a second sort of log one on Delta.",
            "So this is sort of a small term will ignore that.",
            "This is the sort of complexity term that measures the complexity of this function class and it's known as the Rademacher complexity, and it has this form.",
            "So this is the very nice sort of closed form for if you like a general template for doing regular generalization.",
            "So what is this Rademacher complexity?",
            "Well, it measures.",
            "Firstly, it's an expectation over a random sample of size M that that's those points said I hear.",
            "So just think of your training sample, but this is expectation over that, and then it's an expectation over what are known as random Acker variables and these are M + -- 1 randomly generated values uniformly plus minus one.",
            "And what it asks is for any given Sigma, how well can I align my function choice of function with the Sigma so in other words, how well can I kind of line up with random noise?",
            "Is a way of interpreting this and I average that over all possible sets of random noise that I could generate.",
            "So for any for some random signals we may be able to do a good job, but if on average this is actually?",
            "A small quantity than our empirical error is going to be close to our true error.",
            "It's a very natural idea.",
            "You're just saying if my function class is weak in the sense that it can't align with random noise very often, then if I can align with my training data, then I'm going to be good on test data.",
            "By the way, if you have any questions, do to ask.",
            "Now this is a little unfortunate because it involves this expectation over the randomly generated training set, but we actually only have one training set and we want to use it so."
        ],
        [
            "But you can move to what's known as empirical Rademacher complexity, which is the again empirical version given the particular training set that we observe.",
            "And this quantity is actually concentrated, and if we apply something known as Macdiarmid's theorem, we can obtain this slightly.",
            "Well, more useful, I would say.",
            "Rademacher bound, where we now have the empirical.",
            "Sorry the true error.",
            "The empirical error, empirical Rademacher but a slightly bigger term here than on the previous.",
            "Slide if you see here it's slightly smaller.",
            "We've paid a little bit in terms of that, but we are able to now use the Rademacher complexity measured on the particular sample that we observe.",
            "I don't want to go into too much detail."
        ],
        [
            "But just for completeness, here is McDonald's inequality.",
            "It's basically sort of generalization of Hurting's inequality.",
            "Hurting is if we just taking.",
            "Averages of random variables.",
            "These are independent random variables and we have some function fixed function of them and it has to have a property that if we substitute a single entry, it doesn't change by too much, so we've just taken XI and replace it by XI hat and it hasn't changed that much.",
            "We have a sort of concentration, it's it's the probability that a particular observation is far from its expected value is exponentially decaying with the distance, so it's a concentration.",
            "So you simply apply that to this quantity here, and it's very simple to show that swapping 1X will not change this by very much one said sorry.",
            "OK, so hopefully that's that's reasonably.",
            "Believable hope so."
        ],
        [
            "So you know the the straight application of that."
        ],
        [
            "To that bound to support vector machines, what I showed you before that bound that I showed you before was it was just a straight translation of this.",
            "You just have to compute this Rademacher complexity for the linear function classes that are at the heart of support vector machines and basically plug and play and outcomes measure the empirical, which will be you have to introduce a loss function.",
            "I'll talk about it a little later, but it's fairly straightforward application."
        ],
        [
            "OK, so Rademacher complexity comes into its own for boosting and SVM as I've already shown the SVM bound.",
            "We've already seen what I'm going to do now is investigate this boosting application and which will enable the conversion of EC classes into this convex margin."
        ],
        [
            "My zation, so I will argue that we can view boosting as seeking a function from the class.",
            "The following class, where we take a linear combination of these weak learners.",
            "This is the H is the set of weak learners, but where the.",
            "One norm of the of the coefficients is bounded by B, so this is the convex Hull with a multiplication by B of the function of the functions.",
            "The weak learner class right?",
            "Typically I think either weak learners are being a finite set, but will move to using an infinite set later actually.",
            "And we do that with.",
            "In this class we optimize over some function of the margin distribution.",
            "If you use Adaboost, that corresponds to an exponential function of the margin over this set of functions.",
            "And.",
            "We will see that we can include the margin, how to include the margin the moment, but concentrate on computing this Rademacher complexity of this function class so we can apply the crank the handle."
        ],
        [
            "OK.",
            "So we want to be able to show that when we represent our VC classes, we are going to get you know, real learning, kicking in as it were, so the Rademacher complexity has this very nice property of convex hulls.",
            "Basically we can show that here's this empirical Rademacher complexity of the convex Hull and essentially you can bring the some AI's out and you just get a factor B times the Rademacher complexity of the.",
            "Original functions the weak learner class, so you know this, we're thinking might be a finite class.",
            "Even an we've.",
            "This is now all the functions in the convex Hull, and all we've paid is this B."
        ],
        [
            "And indeed, if B was one, we take the convex Hull of a set of functions.",
            "We pay nothing, so you actually move from.",
            "The finite set of functions some complexity associated with them to this convex Hull.",
            "Set and and you don't pay anything in terms of generalization cost.",
            "If you like complexity cost.",
            "So there's been a lot of argument about boosting, and whether bagging a more is better than boosting or boosting and bagging, and wipe it back.",
            "Boosting works better than bagging.",
            "If it does, and.",
            "I think for me this is perhaps you know at the heart of what boosting does for you.",
            "In some sense it it actually moves you to a more complex class, but actually you don't pay in terms of complexity.",
            "So I think this is a very interesting fact.",
            "So just now I'll run you through exactly how we get the bound for boosting, and so the margin is incorporated into the Rademacher theorem through using a loss function which is piecewise linear, so we don't think of the you know the loss.",
            "We would naturally think about using a 01 loss.",
            "You know if you get it right, zero get it wrong.",
            "Sorry if you get it wrong one you know you make a loss, but we have to sort of soften that and so.",
            "What we use is this piecewise linear loss function, so its loss is 0 if the margin is bigger than gamma.",
            "So we have to we have to sort of be a bit more aggressive in terms of penalising functions you know, even if they get it right if they don't get it right.",
            "With a big enough margin, they pay some loss and the loss linearly increases between that margin gamma and with a slope one on gamma so that it reaches one when it gets to 0.",
            "So it's sort of, you know.",
            "Flat up to gamma, then up to one at 0 and then it's going to be flat from the normal.",
            "OK, so it has a steepest slope of one on gamma.",
            "And of course if it's negative, meaning misclassification, it has lost one."
        ],
        [
            "OK, so this gives us the following boosting bound, which is that the probability of misclassification which is the expected value of the heavyside function on this minus Y.",
            "This is sort of like the margin.",
            "This is the you know just the the.",
            "Thresholding at zero.",
            "And of course that is smaller than this, which is this slope function with the A here actually making a bigger loss and now we can apply our Rademacher bound to this thing and we actually get the slack variables and the Rademacher complexity of the weak learner class plus time.",
            "Sorry, there's some the one norm of the of the coefficients, so this corresponds to a sort of like a one norm bound.",
            "On the generalization.",
            "OK, so again you know this same idea is used for the support vector approach.",
            "OK, so hopefully that gives you the idea and all we need now to apply this is to compute the Rademacher complexity of the weak learners."
        ],
        [
            "Um?",
            "We, but before I do that, we can convert this into an optimization.",
            "Using the one norm of the slack variables which appeared in the bound and this one norm of the coefficients, this is the thing we want to minimize subject to this constraint.",
            "OK, this tells us that these are the slack variables that measure the amount by which we fail to reach the margin.",
            "So we actually end up with a linear program."
        ],
        [
            "And if we view the age of XI as a sort of matrix of weak learners with the J indexing the set of weak learners, and I the inputs, so it's that probably sign matrix, then here is the optimization we have.",
            "So minimize the one norm of the coefficients plus this slack variable contribution subject to this constraint that the margin is at least one.",
            "Minus cyan that the AI says.",
            "Here you can make the eyes negative, but if you are worried about that you can just throw in a negative version of each week learner.",
            "So then you can restrict them to be positive."
        ],
        [
            "There's actually an equivalent optimization, which includes the margin, rather than making the margin gamma.",
            "So we make the margin part of the optimization, and we maximize the margin minus the sum of the slack variables and make the constraint gamma minus six I I so it's basically the same idea, and this has a dual that looks like this, so these are just linear programs, so the dual is minimized, but beta subject to this constraint, which is now 4.",
            "All of the weak learners.",
            "So each week learner has to satisfy that the sum of this is.",
            "Minimizing over a U vector which has a some UI equals one.",
            "Now if you're familiar with with boosting, say Adaboost.",
            "What happens there is that you maintain a distribution over the examples.",
            "And you update when you do each round of boosting, you learn based on that distribution and then you update the distribution based on the error of the weak learner that you choose at that round and then add that weak learner with with a particular weight to your linear combination of weak learners.",
            "Now, in this case, the UI actually is a distribution over examples, but it's a distribution that comes out of this optimization.",
            "And it actually turns out that this quantity is exactly the quantity that you optimize when you choose your weak learner in Adaboost.",
            "So maximizing this quantity is exactly the the thing you would do in doing weak learners.",
            "So actually what you can do to solve this optimization this sorry, this linear program, which has a very large number of constraints, is apply something called."
        ],
        [
            "Generation and what you end up with is what turns out to be a.",
            "An equivalent of a boosting algorithm.",
            "So you start with a uniform distribution as you would in Adaboost, you choose the weak learner that maximizes this quantity as you would in Adaboost, and now what you do is you check whether sorry, this value of this is less than the optimal value beta from the previous round of the optimization.",
            "This pizza here, if it is, you actually know that you can stop, you solve the primal and exit, but otherwise you add this qstar to your set of.",
            "Weak learners and then you solve the dual restrict restricted to this SAT J.",
            "So you solve this do."
        ],
        [
            "No problem here.",
            "Restricted to that set, J and this will give you the beta that you need for the next round and effectively also give you EU that you need to wait the."
        ],
        [
            "The examples so that you actually generate the weak learner relative to that you and test it relative to that beta when you go back here.",
            "So this is your boosting round, but now you actually update.",
            "You're you in a much more complex way through solving a linear program rather than through the very simple Adaboost update."
        ],
        [
            "So UI is a distribution on the examples J is added.",
            "It acts like an additional week learner.",
            "FJ is simply the weighted classification accuracy and so we get what is effectively a boosting algorithm.",
            "And the nice properties compared to Adaboost is you have guaranteed convergence and soft stopping criterion.",
            "You actually can.",
            "Measure the duality gap and you know that you will never achieve a bigger increase than the duality gap.",
            "Even if you continue to complete convergence, so you can actually see that you're wasting your time, perhaps carrying on any further so, but you also know when you've got to finish, which you don't without a boost.",
            "Um?"
        ],
        [
            "So.",
            "Some are, some are my just wanted to present that to show that there is a way of representing this large margin optimization as a tractable convex linear program.",
            "Now what I'm going to show you is that we can actually embed any VC class into a boosting algorithm.",
            "Now what we do is we make the elements of the class are weak learners.",
            "If we restrict to a finite set of training data, then by Sauer's lemma we know that there are most this many distinct classifiers in the pass.",
            "So we obtain a linear program with polynomially many constraints.",
            "Um?",
            "OK, it's quite a high degree polynomial, but it's still polynomial.",
            "And actually, we're going to end up with potentially a richer class of functions, because remember, we're taking the convex Hull of this set of functions, so we don't only have our weak learner class, but we extend it potentially.",
            "To further functions.",
            "But the bound that we get an I've left out one little piece, which was this bound on the Rademacher complexity of the weak learner class.",
            "I will come back to it, but let me just say, for now, that it's the logarithm of this number that comes in so the logarithm of this that comes in.",
            "So again it's just D log this that you would get the normal VC bound that you get, so you get the D on M quantity.",
            "So if I go back to that.",
            "This quickly.",
            "Bow"
        ],
        [
            "And here it is so that our head of H here.",
            "Sorry there should be a sorry it's it's log DD log sqrt D / M that you get in this RH here.",
            "So exactly as you would like.",
            "OK, so."
        ],
        [
            "The only thing we've done, of course, is Duck.",
            "The problem of how to index the functions.",
            "Because, you know, in a sense that depends on this particular set of training data and so on.",
            "But I would argue that actually if you could learn in that class anyway, all you have to do is to when you want to do a weak learner is just apply your standard learning algorithm for that class and you will indeed get the function out.",
            "So I mean, of course the slight worry is that you've you've just.",
            "Re interpreted your functions as as.",
            "You know linear representations.",
            "And still you know.",
            "So I in a way I don't think this is the right way to to learn necessarily, but what I think what I want to show is that it opens up a way of linking this with support vector machines, so you may be able to make a soft tradeoff between doing what would be a full enumeration here, and perhaps you know the other extreme would be where you can represent all the functions in.",
            "In a single function class, but maybe there's a compromise where you can have a smaller set of representations that together represent all the functions.",
            "So I'll come to that in a minute."
        ],
        [
            "So can we move between boosting and SVM's?",
            "To ameliorate this problem of with explicitly enumerating all the classifiers as constraints?",
            "And multiple kernel learning is, I think, the way to do this.",
            "It aims to combine a number of different kernels and select a subset of them as part of the training algorithm.",
            "So this is standard multiple kernel learning.",
            "It uses this optimization.",
            "So what it is looking at, if again, you know, don't don't be phased by this, it's very similar to a normal support vector machine.",
            "The difference is that rather than have a single weight vector.",
            "You have the sum of the norms of.",
            "In this case, NN weight vectors.",
            "Sorry there should be a T here.",
            "That's a sorry T = 1 here.",
            "And then you use the.",
            "The actual output is the sum of those weight vectors in a product of those weight vectors with the corresponding FI for each of the different kernels.",
            "So there are N different kernels and therefore N different projections into different feature spaces.",
            "And you have a weight vector in each of those feature spec spaces.",
            "You sum the inner product over.",
            "Sorry all of those inner products to give the output and then you.",
            "Add a threshold and and so on.",
            "So that's the standard multiple kernel learning."
        ],
        [
            "Optimization.",
            "But equivalently, you can see MCL as putting A1 norm constraint on a linear combination of kernels.",
            "So you're trying to create a kernel that is a linear combination of these base kernels with positive coefficients that has some of those said T equal to 1, and as part of the training of the SVM you also optimize the value of these.",
            "Coefficient said T and it.",
            "It turns out it is a convex problem.",
            "It's very closely related to group lasso.",
            "Lasso is doing one norm regularization for regression.",
            "And this group Lasso has this same idea of having several groupings of the features.",
            "Of course, here the features can be very large sets of features 'cause they're represented by kernels.",
            "OK. Now it turns out that's also equivalent to performing linear programming, boosting over the following set of weak learners, where now we have a class of weak learners corresponding to each kernel, which is the set of functions that we can represent using that kernel with a weight vector bounded.",
            "The two norm of the weight vector now bounded by by one.",
            "So unit, most unit norm weight vector, so this now is an infinite set of.",
            "Weak learners.",
            "But so you know, we may have some trouble identifying the best weak learner in this class.",
            "Perhaps we'll look at that in a minute, but this actually I'm not going to show the detail of that, but it's perhaps believable when you see the way that we can represent this function class.",
            "That this indeed does look like.",
            "We're just taking a linear combination of functions that can be represented by this kernel, and so we're sort of taking a two norm within the function class.",
            "The one norm when we combine the different function classes together.",
            "So I mean, I'm skipping a little bit of detail here, but I hope you accept that a reasonable thing to happen.",
            "OK, so now the question would be can we?",
            "How can we identify the weak learner in this that we need to in the in the boosting iteration?",
            "You know, we've given now awaiting over the examples and we have to maximize that.",
            "Entity that appears in the in the boosting algorithm."
        ],
        [
            "So this is it's we want to implement MKL.",
            "So we have to maximize this quantity for a given UI over the set of weak learners.",
            "But it's actually quite easy because we can see that too.",
            "If we want to optimize over a particular function class FT. What we need to do is maximize over a choice of weight vector norm at most one this quantity, and if we move this sum inside, we can see that actually we know how to do this, we just choose W parallel to this vector of norm one and we maximize this inner product.",
            "So we can indeed, and we even have a dual representation of W which is just given by.",
            "The dual variables UI iy so we can actually implement that efficiently in a kernel space."
        ],
        [
            "So we can apply our bread maker.",
            "Bound for boosting.",
            "And here it is.",
            "All we need now is to compute the Rademacher complexity of.",
            "I've slightly altered it in terms of the way I've presented, but it's the same bound we need to compute the Rademacher complexity of the Union of these function classes, so we're still missing.",
            "Essentially, that was the thing I skipped over last time, but by doing it in this more general version, it will also apply to the previous, because we could think of FT is just being a single function if we wanted.",
            "So how are we going to do that well?"
        ],
        [
            "Um?",
            "This is another sort of little trick that we can apply for.",
            "Application of Macdermid tells us that we can actually.",
            "This is this Rademacher complexity is also concentrated in terms of the Rademacher variables, so we can pick a particular random random actor.",
            "You know one example, the Rademacher variable 11 instantiation, rather than the expectation over the Rademacher variables.",
            "We can pick one instantiation and have a bound of the following form which says for that single instantiation plus correction term, which it comes from Macdiarmid, where we're able to upper bound the Rademacher complexity of the function class H. And similarly, we can effectively lower bound the individual sorry upper bound.",
            "The sorry lower bound, the.",
            "Rademacher complexity of the individual function classes by the same instantiation, but with a bit subtracted off, which I put onto the other side.",
            "So this game with probability Delta T here and D0 there.",
            "That should have been Delta 0 number.",
            "So the thing about having a concrete Sigma is that we can then actually compute the Mac."
        ],
        [
            "Some quite easily, so we get the following if you use Delta T equal to this quantity.",
            "Then we actually have for this concrete Sigma star.",
            "This is the H, the upper bound that we had.",
            "And now we can choose the maximum.",
            "Over this class, but now we know that actually this because it's now just looking at the class FT is upper bounded by the Rademacher complexity of FT. That's exactly the second line of the previous slide, so actually this is now just the maximum of the Rademacher complexity of the individual classes, plus this correction term.",
            "And as I indicated, the correction term has a log of the number.",
            "Functions or function classes functions if we had individual functions, or in this case log of the number of function classes, so this sort of suggests that we can actually throw in an awful lot of function classes and it won't cost us too much.",
            "In in terms of the generalization, and this was how you know with the number of functions in the simple translation, two of the VC class.",
            "This was the going to be the growth function that comes from sours, Lemus OEM overdyed the power do you log of it gives us the D so we get sqrt D / M that we need to get the generalization in that case.",
            "So, so my the point I'm making here is that we're actually if I go back just one slide.",
            "Here we are able to now."
        ],
        [
            "Two slides were now able to apply a convex optimization which involves someone norm.",
            "Some two norm of big class of kernels and have a generalization bound.",
            "Here it is that actually is benign in terms of the influence of the number of kernels that we've included, and by including large numbers of kernels we can certainly represent all VC classes and actually wear it.",
            "In a sense more flexible because we get combinations of.",
            "The functions that are in those classes.",
            "Now what I was hoping to be able to do was to sort of show that one of those nasty examples that they'd shown that couldn't be done.",
            "We could somehow do with this, but as I couldn't even find a nasty example, I'm not able to do that anyway, so."
        ],
        [
            "But anyway, I think it's an interesting question to find a concrete example where this approach could be really practical for a problem that was genuinely difficult in the in the normal SVM case as it."
        ],
        [
            "High dimensional.",
            "So to summarize, just I've examined the limitations of learning with linear function classes and reviewed these negative results for SVM's when compared to general SVC classes, and I've showed how this can be overcome, maybe in a slightly sort of read.",
            "Representation way, but I think by extending to this multiple kernel learning there is genuinely here a sort of interesting direction that might be able to tackle a wider range of problems and convert them into this sort of convex optimization framework that seems to work so well in practice, so sort of linking if you like some of the theoretical.",
            "Classes that one would like to be able to learn with the practical algorithms that people are using in the field.",
            "OK, thanks very much.",
            "Let me let me ask a question.",
            "Physical abilities by now, bonds have people actually looked into what the other market technology has to do with permutation testing statistics.",
            "Because there is also very, very frequently used way of deciding whether you are sort of guessing over easy structure.",
            "Yeah, I, I think people haven't done enough to see, you know.",
            "I think there is a connection.",
            "I mean, for instance you know.",
            "Picking that concrete permutation that I did in the last, you could imagine doing that in practice.",
            "You just pick a random Sigma.",
            "And actually compute this value if I go back to that.",
            "Parent here, right?",
            "So don't think now.",
            "Just think of a single function class that you want to compute the Rademacher complexity of.",
            "You just choose a random Sigma, compute this value.",
            "That's like a randomized test, and you've got an upper bound, a concrete upper bound, you know.",
            "Normally what we would do to compute this is use some analysis technique, for instance for SVM as we compute, because it's a linear function class, we can compute an upper bound on this, or you know I showed you with the boosting kind of idea.",
            "But there's no reason why we shouldn't try and do it in a very concrete case by just computing this value.",
            "The only problem is that this is.",
            "A bit like doing agnostic learning because you're trying to learn a.",
            "You know, a nasty you know a lot of errors there.",
            "There's going to be a lot of errors.",
            "You're hoping there going to be a lot of errors, because otherwise you know you want it to be a very poor learner of this random thing.",
            "So often learning Agnostically is much harder than learning.",
            "You know when you can actually get a good classification, but there was some work that.",
            "Was done with decision trees where somebody built a decision tree and then pruned it using this Rademacher band.",
            "So where you actually?",
            "Because there when you're pruning, you can prune optimally against to get the minimum number of errors, so I think, but I think more should be done.",
            "I mean, there's a really interesting connection there, yeah?",
            "Make sure it comes up every day, at least for practitioners.",
            "The complaint about the balance in statistical learning, theory of computation, as it should say, computational field there that they are so loose when you actually look at your budget for real experiment, you never have enough and enough samples to even push your bound below 1 so.",
            "Has this disease of the balance be sort of cute little bit by by the technology or I'm still not doing that in the realistic parameter space where where you could help the practitioners you guys here with experiments.",
            "OK, so so very good question like that.",
            "So 2 answers.",
            "One is that for the Rademacher bounds I would say we're not in that space that it still unrealistic.",
            "But, and however recently we've done some experiments using the Rademacher bounds too.",
            "2.",
            "Do model selection with remarkably good results.",
            "I was really, really surprised.",
            "I mean, I actually was doing it to prove it doesn't work.",
            "And it turned out to work surprisingly well.",
            "So I mean, even though the bounds were very weak, obviously what we have the bounds that are actually.",
            "Good, much better than than these are the PAC Bayes methods which I haven't talked about today, but the PAC Bayes bounds are nontrivial.",
            "You know they're less than one and even less than .5.",
            "We had a workshop you know bounds less than .5 with John Langford a few years back.",
            "And yeah, I know they are actually, you know, a factor, sometimes three worse or even smaller than the test error.",
            "So they genuinely give.",
            "A very tight bound, unfortunately, and we've had various ways of, you know, adjusting the prior to get them tighter still and so on.",
            "But often what we found is that as you refine the.",
            "Bound by tweaking it, the ability for it to do model selection actually can decrease in a very sort of disappointing way, so maybe you're tuning the bound 222 well to what it can do, and therefore it's not picking up the something else that the structure of the data carries.",
            "So you make the wrong choices in model selection, because in the sense you know bound has two values.",
            "One is to drive a model selection and the other is to tell the practitioner yes, it's.",
            "It's it's this is a real estimate of the generalization.",
            "At the time it might have the qualitatively correct behavior, since the numbers are wrong, you are on thin ice in some sense, because you can't really distinguish the case where it's qualitatively correct, but but all of magnitude larger.",
            "Well, actually available improvement of rebound would get adrift in the minimum, and then you will select a different model.",
            "So what I would what I would claim is the following PAC Bayes with the sort of adjustments that I've talked about gives very tight.",
            "Sounds you know Factor 3, perhaps you know so non trivial.",
            "And is as good as cross validation.",
            "Plus or minus epsilon, occasionally slightly better syndication is only worth so, so I would say we're, you know we aren't what we would like to be able to do, so we do better than cross validation, but I can't say that, but I think we're able to manage.",
            "It's very, but at least the bands are, you know non non trivial and interesting and possibly useful for practitioners and they're able to drive.",
            "So I think you know it's a big advance from what we were at say 10 years ago.",
            "Personalization is actually a really for the computer science students.",
            "There's no excuse not to do it, and they don't have to understand another field for.",
            "OK, I see no comments, and so let's release the audience for the coffee break.",
            "And thanks again, Jennifer."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yes, I was thinking about what I should speak about to an audience that is.",
                    "label": 0
                },
                {
                    "sent": "Interested in similarity functions and not kernels and.",
                    "label": 0
                },
                {
                    "sent": "Believes that kernels aren't the answer.",
                    "label": 0
                },
                {
                    "sent": "So this was a challenge for somebody who spent 10 years trying to promote kernel methods.",
                    "label": 0
                },
                {
                    "sent": "But I think there are some very interesting questions here and and some quite.",
                    "label": 0
                },
                {
                    "sent": "Insightful results that are worth talking about.",
                    "label": 0
                },
                {
                    "sent": "However, I'm going to.",
                    "label": 0
                },
                {
                    "sent": "Pitch it slightly.",
                    "label": 0
                },
                {
                    "sent": "In the tone of the Empire fights back in that I'm going to argue that multiple kernel learning can overcome some of the limitations that that have been thrown at kernel methods.",
                    "label": 0
                },
                {
                    "sent": "Now in terms.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of the so this is just an overview, so I'm going to talk first about the relationship to Simbad and.",
                    "label": 1
                },
                {
                    "sent": "Argue that there are two ways of looking at the question of whether similarity is necessary.",
                    "label": 0
                },
                {
                    "sent": "Similarity functions that are not Euclidean, unnecessary or not.",
                    "label": 0
                },
                {
                    "sent": "I'm then going to talk about some previous work that.",
                    "label": 0
                },
                {
                    "sent": "Talks about this question of when function classes can be represented using kernel methods.",
                    "label": 0
                },
                {
                    "sent": "I'm gonna dip into some theory which will motivate some of the later results are mention, so I'll talk.",
                    "label": 0
                },
                {
                    "sent": "Give a very brief introduction to Rademacher complexity, which is used to bound the generalization of.",
                    "label": 0
                },
                {
                    "sent": "The classifiers or or other learning systems generally and that will lead me to motivate an algorithm that I'm calling linear programming boosting, which is a kind of adaptation of the boosting approach.",
                    "label": 0
                },
                {
                    "sent": "And that I will argue can address some of the weaknesses in this.",
                    "label": 0
                },
                {
                    "sent": "I highlighted in this earlier work on.",
                    "label": 0
                },
                {
                    "sent": "The limitations of kernel methods and, but I'll then sort of join the two together with this multiple kernel learning, so I'll present multiple kernel learning if you like as an extension of boosting and then show you some new bounds that apply that can.",
                    "label": 1
                },
                {
                    "sent": "Justify using really large number of different kernels in multiple kernel learning, so I think multiple kind of learnings, perhaps not been approached in the right way and you know I would like to think that the theory can motivate being a bit more.",
                    "label": 0
                },
                {
                    "sent": "Courageous in using multiple kernels.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then I'll draw some conclusions.",
                    "label": 0
                },
                {
                    "sent": "So first relation to Simbad and the question is when can we learn using Euclidean representations?",
                    "label": 1
                },
                {
                    "sent": "I would say that was quite a sort of fundamental starting point for this embed philosophy, which was to argue that we need to move to deal with similarity functions that perhaps can't be represented as distances in a Euclidean space.",
                    "label": 0
                },
                {
                    "sent": "I would say that you could ask that question at 2 two levels.",
                    "label": 1
                },
                {
                    "sent": "The first is is the data naturally separable in the given representation?",
                    "label": 1
                },
                {
                    "sent": "In other words, we receive the input data or the data with a particular representation, and we ask, can we use that representation?",
                    "label": 0
                },
                {
                    "sent": "Or perhaps adjust that representation to ensure that it is separable?",
                    "label": 1
                },
                {
                    "sent": "So in some sense it's something about the way the data is given to us.",
                    "label": 0
                },
                {
                    "sent": "But there is an alternative way.",
                    "label": 0
                },
                {
                    "sent": "Which now relates to really a class if the set of classifiers over some inputs X.",
                    "label": 0
                },
                {
                    "sent": "And here we can just ask the question, is there an embedding so we don't worry about actually how X is represented?",
                    "label": 0
                },
                {
                    "sent": "We just think of X as.",
                    "label": 0
                },
                {
                    "sent": "Markers for inputs and we're asking, you know, can there be any?",
                    "label": 0
                },
                {
                    "sent": "So we're not asking can we find a mapping from the input space that we're given some nice mapping that will give it into a space, but just is there any mapping is there it?",
                    "label": 0
                },
                {
                    "sent": "Does there exist a mapping into a space in which we can separate these concepts?",
                    "label": 0
                },
                {
                    "sent": "With linear threshold we can represent these concepts with linear threshold functions, so I hope that distinction is clear that the question here.",
                    "label": 0
                },
                {
                    "sent": "Is simply in principle representation not actually how to construct it explicitly, so I'm going to consider this second question.",
                    "label": 0
                },
                {
                    "sent": "So, given a set of classifiers, when or or can we say when or if it's likely to be the case that we can find a space where we map our inputs and then represent all the functions in H as linear threshold functions?",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So first thing to notice is if we ignore the how the actual data is given to us in terms of representation, it's clear that any any classifier can be represented by a large margin linear threshold.",
                    "label": 1
                },
                {
                    "sent": "Classifier simply by choosing this embedding.",
                    "label": 0
                },
                {
                    "sent": "So let me just get my.",
                    "label": 0
                },
                {
                    "sent": "Fancy pointer here.",
                    "label": 0
                },
                {
                    "sent": "So if we consider the embedding that just takes the input and Maps it to its classification, right?",
                    "label": 0
                },
                {
                    "sent": "So plus one if it's a positive minus one.",
                    "label": 0
                },
                {
                    "sent": "If it's a negative, then clearly.",
                    "label": 0
                },
                {
                    "sent": "We have a large margin classifier which just takes the sign of that value and gives perfect classification.",
                    "label": 0
                },
                {
                    "sent": "So clearly I mean this is just a tautology really, but I'm all I'm pointing out is that large margin classification is certainly possible if we restrict ourselves to a single function and don't worry about how we actually compute this mapping.",
                    "label": 0
                },
                {
                    "sent": "Of course you know this is jumping the question of how we compute this mapping.",
                    "label": 0
                },
                {
                    "sent": "So the question that I want to address is, sorry, what if we have a set of classifiers?",
                    "label": 0
                },
                {
                    "sent": "Is there an embedding that's good for all of them?",
                    "label": 1
                },
                {
                    "sent": "So this is just for one function, we can find that embedding, But what about if we have a whole set of classifiers?",
                    "label": 1
                },
                {
                    "sent": "Can we find an embedding that works for all of them?",
                    "label": 0
                },
                {
                    "sent": "At.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Once.",
                    "label": 0
                },
                {
                    "sent": "And this is being studied.",
                    "label": 0
                },
                {
                    "sent": "A few years ago, and surprisingly, there are many classes that are known to be learnable for which no such embedding exists.",
                    "label": 1
                },
                {
                    "sent": "So this was work.",
                    "label": 0
                },
                {
                    "sent": "The original paper in this area, I think, was this one by Bendavid, Iran and Simon and sorry semen in 2002.",
                    "label": 0
                },
                {
                    "sent": "And what they show is that basically there for.",
                    "label": 0
                },
                {
                    "sent": "If you look at classes of VC Dimension D on M inputs, then only a vanishing fraction can be embedded into Euclidean space of dimension much less than M. So meaning that you're in a VC, you will not be able to learn in such a space.",
                    "label": 1
                },
                {
                    "sent": "So what they're saying here is sorry, I should perhaps just make a note that VC dimension D means that.",
                    "label": 0
                },
                {
                    "sent": "These functions are learnable with an error bound scaling as the square root of DRM.",
                    "label": 1
                },
                {
                    "sent": "I'm not considering here the practical computational issues, just the statistical question of whether we can from a certain amount of data, reliably identify the correct function in the class, and the error will scale as we learn from M examples in a VC dimension D space, the error bound will scale as the square root of Deon M. Plus the error term or even D on EM if we have the exact representative, there's no noise in the in the training data, so certainly learnable, but we aren't able to for the vast majority of these VC classes, we aren't able to embed them into a Euclidean space in such a way that we could learn using linear threshold functions.",
                    "label": 0
                },
                {
                    "sent": "So this seems like a quite serious result and the only downside is this accounting argument that they use.",
                    "label": 0
                },
                {
                    "sent": "So basically there are just too many VC classes or another way of viewing it.",
                    "label": 0
                },
                {
                    "sent": "There are two few linear threshold classes, so simply there's no way we're going to get every BC class represented by a linear threshold class of small dimension.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "But you might think, well, wait a minute.",
                    "label": 0
                },
                {
                    "sent": "You know, we know that we can't learn with low dimensional spaces.",
                    "label": 0
                },
                {
                    "sent": "That's everyone knows that you just use support vector machines, right?",
                    "label": 1
                },
                {
                    "sent": "That's the whole idea of support vector machines you project into high dimensional space and then you learn in that high space and you don't have to worry about the dimension because you overcome the curse of dimensionality by maximizing the margin.",
                    "label": 1
                },
                {
                    "sent": "Brilliant.",
                    "label": 0
                },
                {
                    "sent": "And here's the band that we would get from this Rademacher.",
                    "label": 0
                },
                {
                    "sent": "Stuff I'll talk about in a minute so we would have the probability of misclassification of a randomly drawn test point is bounded by some terms.",
                    "label": 0
                },
                {
                    "sent": "This is the slack variables from the optimization of the SVM.",
                    "label": 0
                },
                {
                    "sent": "This is the margin gamma.",
                    "label": 0
                },
                {
                    "sent": "So clearly as the margin grows we get a.",
                    "label": 1
                },
                {
                    "sent": "A reasonable bound on the on the off training set performance.",
                    "label": 0
                },
                {
                    "sent": "And if we consider a Gaussian kernel, this even simplifies further because this term just becomes the square root of M, and so we get this.",
                    "label": 0
                },
                {
                    "sent": "This kind of scaling with a clear dependence on the.",
                    "label": 0
                },
                {
                    "sent": "Essentially training error.",
                    "label": 0
                },
                {
                    "sent": "This is the slack variables measuring how many points fail to achieve the margin, and this is the margin.",
                    "label": 0
                },
                {
                    "sent": "So that all looks fine surely though.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is not a problem then unfortunately there is and this is actually spawned.",
                    "label": 0
                },
                {
                    "sent": "Another interesting direction of research which has shown an intimate connection between the effect of having a large margin and resilience to random projection.",
                    "label": 1
                },
                {
                    "sent": "So Balcan, Blum and Vempala in 2004 studied this this property.",
                    "label": 1
                },
                {
                    "sent": "And basically what they show is that bounds on large margin classification can be obtained showing that random projections into a low dimensional space.",
                    "label": 1
                },
                {
                    "sent": "Ensure that you still have good linear separability.",
                    "label": 0
                },
                {
                    "sent": "So if you project randomly into a space of this dimension, there's a few, maybe a little bit glib.",
                    "label": 0
                },
                {
                    "sent": "What I'm saying it's a little bit complicated how they do it, but but basically it boils down to projecting into a space of this dimension, and you'll get reasonably good linear separation, and so your large margin actually corresponds to low dimension and.",
                    "label": 0
                },
                {
                    "sent": "We're back to square one, so actually those non representability results also apply to the support vector machines.",
                    "label": 0
                },
                {
                    "sent": "If we could learn the class with the support vector machines, we would be able to project it and we'd have a low dimensional representation of that class, which we know for most VC classes is not possible by the previous result.",
                    "label": 0
                },
                {
                    "sent": "So results already obtained by Ben David and Ann coauthors.",
                    "label": 0
                },
                {
                    "sent": "Showed this as well.",
                    "label": 1
                },
                {
                    "sent": "Actually they included this this idea, so they're actually very powerful results.",
                    "label": 0
                },
                {
                    "sent": "I mean, stepping back what they say is for the overwhelming majority of classes that can be learned.",
                    "label": 1
                },
                {
                    "sent": "You know they're learnable.",
                    "label": 0
                },
                {
                    "sent": "No kernel that will render them learnable by SVM.",
                    "label": 1
                },
                {
                    "sent": "You know it's something that perhaps isn't dwelled on, because what do you do with it?",
                    "label": 0
                },
                {
                    "sent": "But it's it's quite a powerful, I think, sort of wake up call if you like.",
                    "label": 0
                },
                {
                    "sent": "Perhaps one weakness that I'll talk about a little bit is the fact that it's only an existence proof.",
                    "label": 0
                },
                {
                    "sent": "One would like to see one of these classes and see what they actually look like, and perhaps get some insight.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So work has been done by Foster ET al on advancing the technology of analyzing this problem, and basically they've developed some some technology that relies on the analysis of this sign matrix N of the concepts in the class.",
                    "label": 1
                },
                {
                    "sent": "So basically, if you think of M as having rows indexed by examples and columns by classifiers, and then there's a plus one in the entry of.",
                    "label": 1
                },
                {
                    "sent": "It's a positive classification of that example and a -- 1 if it's a negative.",
                    "label": 0
                },
                {
                    "sent": "And then you can they manage to lower bound the dimension that you need to represent this.",
                    "label": 0
                },
                {
                    "sent": "This sort of set of concepts.",
                    "label": 0
                },
                {
                    "sent": "By a linear classifiers, no margin here, just just.",
                    "label": 0
                },
                {
                    "sent": "Threshold threshold linear functions by this this quantity.",
                    "label": 0
                },
                {
                    "sent": "So this is the spectral norm of M square root of the product of the number of well, number of entries.",
                    "label": 0
                },
                {
                    "sent": "And this is another lower bound that involves the sum of the first D spectral singular values.",
                    "label": 0
                },
                {
                    "sent": "So using these techniques.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So they are able to apply them to a few concrete functioning classes, and I actually was a bit disappointed when I was looking sort of checking out this stuff to put this talk together, 'cause I thought there was some very kind of clear examples where you know they got lower bounds that were much higher than what could be learned by just say a standard VC dimension bound, but in fact.",
                    "label": 0
                },
                {
                    "sent": "There didn't seem to be any, so I was a little bit disappointed by this.",
                    "label": 0
                },
                {
                    "sent": "So the one of the examples that they give is monomial's over N Boolean variables and all functions that representable by conjunctions of literals, variables or their negations.",
                    "label": 1
                },
                {
                    "sent": "And they show that this class can only be embedded with a margin of at most 1 / sqrt N. But actually that doesn't make really a big distinction because it's got VC dimension end.",
                    "label": 1
                },
                {
                    "sent": "So this looks about the right dimension to embed in.",
                    "label": 0
                },
                {
                    "sent": "So I was a little bit disappointed.",
                    "label": 0
                },
                {
                    "sent": "I thought there was some quite nice examples of that type, so this I think is a nice open question.",
                    "label": 0
                },
                {
                    "sent": "Could we actually generate some?",
                    "label": 0
                },
                {
                    "sent": "Some good, you know, difficult classes, sorry, easy classes, but.",
                    "label": 0
                },
                {
                    "sent": "Have very high dimensional.",
                    "label": 0
                },
                {
                    "sent": "You need a high dimension to embed them into linear threshold functions.",
                    "label": 0
                },
                {
                    "sent": "I should say that the technology is now being used by razborov to get lower bounds for a C nought and DNF embeddings, but.",
                    "label": 0
                },
                {
                    "sent": "That again, I don't think the way he's done it.",
                    "label": 0
                },
                {
                    "sent": "We can quite read off as it work as he's taken the class of all polynomially sized DNA functions, so this is, you know.",
                    "label": 0
                },
                {
                    "sent": "I think a fruitful area of research, but it's it hasn't quite given what I was expecting at this point.",
                    "label": 0
                },
                {
                    "sent": "So anyway, I still you know, I would think that it should be possible to find a concrete function class.",
                    "label": 0
                },
                {
                    "sent": "Which would be nice in terms of understanding what's going on more.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Carefully.",
                    "label": 0
                },
                {
                    "sent": "So anyway.",
                    "label": 0
                },
                {
                    "sent": "That's just to give you a background on this question.",
                    "label": 0
                },
                {
                    "sent": "Now what I want to do in this talk is revisit this impasse by turning to an alternative large margin approach.",
                    "label": 1
                },
                {
                    "sent": "And what I'm going to look at is maximizing the margin while controlling the one norm of the weight vector, and I'm going to argue that this would demonstrate this gives a form of boosting, so we're going to sort of take the SVM idea, but rather than the two norm, have the one norm.",
                    "label": 0
                },
                {
                    "sent": "So and then I'm going to show that you can actually also combine so you can make a sort of mixture of the two in what's becomes multiple kernel learning.",
                    "label": 0
                },
                {
                    "sent": "But first I would like to sort of just set the scene as far as the error bounds are concerned, because I'll be using those later on when I come to bounding the performance of multiple kernel learning because I'll need that to argue that we can actually do this.",
                    "label": 0
                },
                {
                    "sent": "Actually embed using this approach in a way that we can learn all.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "C classes, so here's the main Rademacher theorem.",
                    "label": 1
                },
                {
                    "sent": "So if we take a function class F. Then here it is, sort of.",
                    "label": 0
                },
                {
                    "sent": "Calligraphic F. If we choose any function in F, then with probability 1 minus Delta, it's expected value.",
                    "label": 0
                },
                {
                    "sent": "Think of this as a loss function.",
                    "label": 0
                },
                {
                    "sent": "If you like, you know in the classification this be the 01 indicator function of a misclassification or something similar.",
                    "label": 0
                },
                {
                    "sent": "This is the empirical version.",
                    "label": 0
                },
                {
                    "sent": "I've put a hat on it to indicate empirical version.",
                    "label": 0
                },
                {
                    "sent": "So the true error is bounded by its empirical error plus a correction term and a second sort of log one on Delta.",
                    "label": 0
                },
                {
                    "sent": "So this is sort of a small term will ignore that.",
                    "label": 0
                },
                {
                    "sent": "This is the sort of complexity term that measures the complexity of this function class and it's known as the Rademacher complexity, and it has this form.",
                    "label": 1
                },
                {
                    "sent": "So this is the very nice sort of closed form for if you like a general template for doing regular generalization.",
                    "label": 0
                },
                {
                    "sent": "So what is this Rademacher complexity?",
                    "label": 0
                },
                {
                    "sent": "Well, it measures.",
                    "label": 0
                },
                {
                    "sent": "Firstly, it's an expectation over a random sample of size M that that's those points said I hear.",
                    "label": 0
                },
                {
                    "sent": "So just think of your training sample, but this is expectation over that, and then it's an expectation over what are known as random Acker variables and these are M + -- 1 randomly generated values uniformly plus minus one.",
                    "label": 0
                },
                {
                    "sent": "And what it asks is for any given Sigma, how well can I align my function choice of function with the Sigma so in other words, how well can I kind of line up with random noise?",
                    "label": 0
                },
                {
                    "sent": "Is a way of interpreting this and I average that over all possible sets of random noise that I could generate.",
                    "label": 0
                },
                {
                    "sent": "So for any for some random signals we may be able to do a good job, but if on average this is actually?",
                    "label": 0
                },
                {
                    "sent": "A small quantity than our empirical error is going to be close to our true error.",
                    "label": 0
                },
                {
                    "sent": "It's a very natural idea.",
                    "label": 0
                },
                {
                    "sent": "You're just saying if my function class is weak in the sense that it can't align with random noise very often, then if I can align with my training data, then I'm going to be good on test data.",
                    "label": 0
                },
                {
                    "sent": "By the way, if you have any questions, do to ask.",
                    "label": 0
                },
                {
                    "sent": "Now this is a little unfortunate because it involves this expectation over the randomly generated training set, but we actually only have one training set and we want to use it so.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But you can move to what's known as empirical Rademacher complexity, which is the again empirical version given the particular training set that we observe.",
                    "label": 0
                },
                {
                    "sent": "And this quantity is actually concentrated, and if we apply something known as Macdiarmid's theorem, we can obtain this slightly.",
                    "label": 0
                },
                {
                    "sent": "Well, more useful, I would say.",
                    "label": 0
                },
                {
                    "sent": "Rademacher bound, where we now have the empirical.",
                    "label": 0
                },
                {
                    "sent": "Sorry the true error.",
                    "label": 0
                },
                {
                    "sent": "The empirical error, empirical Rademacher but a slightly bigger term here than on the previous.",
                    "label": 0
                },
                {
                    "sent": "Slide if you see here it's slightly smaller.",
                    "label": 0
                },
                {
                    "sent": "We've paid a little bit in terms of that, but we are able to now use the Rademacher complexity measured on the particular sample that we observe.",
                    "label": 0
                },
                {
                    "sent": "I don't want to go into too much detail.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But just for completeness, here is McDonald's inequality.",
                    "label": 0
                },
                {
                    "sent": "It's basically sort of generalization of Hurting's inequality.",
                    "label": 0
                },
                {
                    "sent": "Hurting is if we just taking.",
                    "label": 0
                },
                {
                    "sent": "Averages of random variables.",
                    "label": 0
                },
                {
                    "sent": "These are independent random variables and we have some function fixed function of them and it has to have a property that if we substitute a single entry, it doesn't change by too much, so we've just taken XI and replace it by XI hat and it hasn't changed that much.",
                    "label": 0
                },
                {
                    "sent": "We have a sort of concentration, it's it's the probability that a particular observation is far from its expected value is exponentially decaying with the distance, so it's a concentration.",
                    "label": 0
                },
                {
                    "sent": "So you simply apply that to this quantity here, and it's very simple to show that swapping 1X will not change this by very much one said sorry.",
                    "label": 0
                },
                {
                    "sent": "OK, so hopefully that's that's reasonably.",
                    "label": 0
                },
                {
                    "sent": "Believable hope so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you know the the straight application of that.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "To that bound to support vector machines, what I showed you before that bound that I showed you before was it was just a straight translation of this.",
                    "label": 0
                },
                {
                    "sent": "You just have to compute this Rademacher complexity for the linear function classes that are at the heart of support vector machines and basically plug and play and outcomes measure the empirical, which will be you have to introduce a loss function.",
                    "label": 0
                },
                {
                    "sent": "I'll talk about it a little later, but it's fairly straightforward application.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so Rademacher complexity comes into its own for boosting and SVM as I've already shown the SVM bound.",
                    "label": 0
                },
                {
                    "sent": "We've already seen what I'm going to do now is investigate this boosting application and which will enable the conversion of EC classes into this convex margin.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "My zation, so I will argue that we can view boosting as seeking a function from the class.",
                    "label": 1
                },
                {
                    "sent": "The following class, where we take a linear combination of these weak learners.",
                    "label": 1
                },
                {
                    "sent": "This is the H is the set of weak learners, but where the.",
                    "label": 0
                },
                {
                    "sent": "One norm of the of the coefficients is bounded by B, so this is the convex Hull with a multiplication by B of the function of the functions.",
                    "label": 0
                },
                {
                    "sent": "The weak learner class right?",
                    "label": 0
                },
                {
                    "sent": "Typically I think either weak learners are being a finite set, but will move to using an infinite set later actually.",
                    "label": 1
                },
                {
                    "sent": "And we do that with.",
                    "label": 0
                },
                {
                    "sent": "In this class we optimize over some function of the margin distribution.",
                    "label": 0
                },
                {
                    "sent": "If you use Adaboost, that corresponds to an exponential function of the margin over this set of functions.",
                    "label": 1
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We will see that we can include the margin, how to include the margin the moment, but concentrate on computing this Rademacher complexity of this function class so we can apply the crank the handle.",
                    "label": 1
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So we want to be able to show that when we represent our VC classes, we are going to get you know, real learning, kicking in as it were, so the Rademacher complexity has this very nice property of convex hulls.",
                    "label": 1
                },
                {
                    "sent": "Basically we can show that here's this empirical Rademacher complexity of the convex Hull and essentially you can bring the some AI's out and you just get a factor B times the Rademacher complexity of the.",
                    "label": 0
                },
                {
                    "sent": "Original functions the weak learner class, so you know this, we're thinking might be a finite class.",
                    "label": 0
                },
                {
                    "sent": "Even an we've.",
                    "label": 0
                },
                {
                    "sent": "This is now all the functions in the convex Hull, and all we've paid is this B.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And indeed, if B was one, we take the convex Hull of a set of functions.",
                    "label": 1
                },
                {
                    "sent": "We pay nothing, so you actually move from.",
                    "label": 0
                },
                {
                    "sent": "The finite set of functions some complexity associated with them to this convex Hull.",
                    "label": 0
                },
                {
                    "sent": "Set and and you don't pay anything in terms of generalization cost.",
                    "label": 0
                },
                {
                    "sent": "If you like complexity cost.",
                    "label": 0
                },
                {
                    "sent": "So there's been a lot of argument about boosting, and whether bagging a more is better than boosting or boosting and bagging, and wipe it back.",
                    "label": 0
                },
                {
                    "sent": "Boosting works better than bagging.",
                    "label": 0
                },
                {
                    "sent": "If it does, and.",
                    "label": 0
                },
                {
                    "sent": "I think for me this is perhaps you know at the heart of what boosting does for you.",
                    "label": 0
                },
                {
                    "sent": "In some sense it it actually moves you to a more complex class, but actually you don't pay in terms of complexity.",
                    "label": 0
                },
                {
                    "sent": "So I think this is a very interesting fact.",
                    "label": 0
                },
                {
                    "sent": "So just now I'll run you through exactly how we get the bound for boosting, and so the margin is incorporated into the Rademacher theorem through using a loss function which is piecewise linear, so we don't think of the you know the loss.",
                    "label": 0
                },
                {
                    "sent": "We would naturally think about using a 01 loss.",
                    "label": 0
                },
                {
                    "sent": "You know if you get it right, zero get it wrong.",
                    "label": 0
                },
                {
                    "sent": "Sorry if you get it wrong one you know you make a loss, but we have to sort of soften that and so.",
                    "label": 0
                },
                {
                    "sent": "What we use is this piecewise linear loss function, so its loss is 0 if the margin is bigger than gamma.",
                    "label": 1
                },
                {
                    "sent": "So we have to we have to sort of be a bit more aggressive in terms of penalising functions you know, even if they get it right if they don't get it right.",
                    "label": 0
                },
                {
                    "sent": "With a big enough margin, they pay some loss and the loss linearly increases between that margin gamma and with a slope one on gamma so that it reaches one when it gets to 0.",
                    "label": 0
                },
                {
                    "sent": "So it's sort of, you know.",
                    "label": 0
                },
                {
                    "sent": "Flat up to gamma, then up to one at 0 and then it's going to be flat from the normal.",
                    "label": 0
                },
                {
                    "sent": "OK, so it has a steepest slope of one on gamma.",
                    "label": 0
                },
                {
                    "sent": "And of course if it's negative, meaning misclassification, it has lost one.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so this gives us the following boosting bound, which is that the probability of misclassification which is the expected value of the heavyside function on this minus Y.",
                    "label": 0
                },
                {
                    "sent": "This is sort of like the margin.",
                    "label": 0
                },
                {
                    "sent": "This is the you know just the the.",
                    "label": 0
                },
                {
                    "sent": "Thresholding at zero.",
                    "label": 0
                },
                {
                    "sent": "And of course that is smaller than this, which is this slope function with the A here actually making a bigger loss and now we can apply our Rademacher bound to this thing and we actually get the slack variables and the Rademacher complexity of the weak learner class plus time.",
                    "label": 0
                },
                {
                    "sent": "Sorry, there's some the one norm of the of the coefficients, so this corresponds to a sort of like a one norm bound.",
                    "label": 0
                },
                {
                    "sent": "On the generalization.",
                    "label": 0
                },
                {
                    "sent": "OK, so again you know this same idea is used for the support vector approach.",
                    "label": 0
                },
                {
                    "sent": "OK, so hopefully that gives you the idea and all we need now to apply this is to compute the Rademacher complexity of the weak learners.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "We, but before I do that, we can convert this into an optimization.",
                    "label": 0
                },
                {
                    "sent": "Using the one norm of the slack variables which appeared in the bound and this one norm of the coefficients, this is the thing we want to minimize subject to this constraint.",
                    "label": 1
                },
                {
                    "sent": "OK, this tells us that these are the slack variables that measure the amount by which we fail to reach the margin.",
                    "label": 0
                },
                {
                    "sent": "So we actually end up with a linear program.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And if we view the age of XI as a sort of matrix of weak learners with the J indexing the set of weak learners, and I the inputs, so it's that probably sign matrix, then here is the optimization we have.",
                    "label": 1
                },
                {
                    "sent": "So minimize the one norm of the coefficients plus this slack variable contribution subject to this constraint that the margin is at least one.",
                    "label": 0
                },
                {
                    "sent": "Minus cyan that the AI says.",
                    "label": 0
                },
                {
                    "sent": "Here you can make the eyes negative, but if you are worried about that you can just throw in a negative version of each week learner.",
                    "label": 0
                },
                {
                    "sent": "So then you can restrict them to be positive.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "There's actually an equivalent optimization, which includes the margin, rather than making the margin gamma.",
                    "label": 0
                },
                {
                    "sent": "So we make the margin part of the optimization, and we maximize the margin minus the sum of the slack variables and make the constraint gamma minus six I I so it's basically the same idea, and this has a dual that looks like this, so these are just linear programs, so the dual is minimized, but beta subject to this constraint, which is now 4.",
                    "label": 0
                },
                {
                    "sent": "All of the weak learners.",
                    "label": 0
                },
                {
                    "sent": "So each week learner has to satisfy that the sum of this is.",
                    "label": 0
                },
                {
                    "sent": "Minimizing over a U vector which has a some UI equals one.",
                    "label": 0
                },
                {
                    "sent": "Now if you're familiar with with boosting, say Adaboost.",
                    "label": 0
                },
                {
                    "sent": "What happens there is that you maintain a distribution over the examples.",
                    "label": 0
                },
                {
                    "sent": "And you update when you do each round of boosting, you learn based on that distribution and then you update the distribution based on the error of the weak learner that you choose at that round and then add that weak learner with with a particular weight to your linear combination of weak learners.",
                    "label": 0
                },
                {
                    "sent": "Now, in this case, the UI actually is a distribution over examples, but it's a distribution that comes out of this optimization.",
                    "label": 0
                },
                {
                    "sent": "And it actually turns out that this quantity is exactly the quantity that you optimize when you choose your weak learner in Adaboost.",
                    "label": 0
                },
                {
                    "sent": "So maximizing this quantity is exactly the the thing you would do in doing weak learners.",
                    "label": 0
                },
                {
                    "sent": "So actually what you can do to solve this optimization this sorry, this linear program, which has a very large number of constraints, is apply something called.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Generation and what you end up with is what turns out to be a.",
                    "label": 0
                },
                {
                    "sent": "An equivalent of a boosting algorithm.",
                    "label": 0
                },
                {
                    "sent": "So you start with a uniform distribution as you would in Adaboost, you choose the weak learner that maximizes this quantity as you would in Adaboost, and now what you do is you check whether sorry, this value of this is less than the optimal value beta from the previous round of the optimization.",
                    "label": 0
                },
                {
                    "sent": "This pizza here, if it is, you actually know that you can stop, you solve the primal and exit, but otherwise you add this qstar to your set of.",
                    "label": 1
                },
                {
                    "sent": "Weak learners and then you solve the dual restrict restricted to this SAT J.",
                    "label": 1
                },
                {
                    "sent": "So you solve this do.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "No problem here.",
                    "label": 0
                },
                {
                    "sent": "Restricted to that set, J and this will give you the beta that you need for the next round and effectively also give you EU that you need to wait the.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The examples so that you actually generate the weak learner relative to that you and test it relative to that beta when you go back here.",
                    "label": 0
                },
                {
                    "sent": "So this is your boosting round, but now you actually update.",
                    "label": 0
                },
                {
                    "sent": "You're you in a much more complex way through solving a linear program rather than through the very simple Adaboost update.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So UI is a distribution on the examples J is added.",
                    "label": 1
                },
                {
                    "sent": "It acts like an additional week learner.",
                    "label": 0
                },
                {
                    "sent": "FJ is simply the weighted classification accuracy and so we get what is effectively a boosting algorithm.",
                    "label": 1
                },
                {
                    "sent": "And the nice properties compared to Adaboost is you have guaranteed convergence and soft stopping criterion.",
                    "label": 0
                },
                {
                    "sent": "You actually can.",
                    "label": 0
                },
                {
                    "sent": "Measure the duality gap and you know that you will never achieve a bigger increase than the duality gap.",
                    "label": 0
                },
                {
                    "sent": "Even if you continue to complete convergence, so you can actually see that you're wasting your time, perhaps carrying on any further so, but you also know when you've got to finish, which you don't without a boost.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "Some are, some are my just wanted to present that to show that there is a way of representing this large margin optimization as a tractable convex linear program.",
                    "label": 0
                },
                {
                    "sent": "Now what I'm going to show you is that we can actually embed any VC class into a boosting algorithm.",
                    "label": 0
                },
                {
                    "sent": "Now what we do is we make the elements of the class are weak learners.",
                    "label": 0
                },
                {
                    "sent": "If we restrict to a finite set of training data, then by Sauer's lemma we know that there are most this many distinct classifiers in the pass.",
                    "label": 1
                },
                {
                    "sent": "So we obtain a linear program with polynomially many constraints.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, it's quite a high degree polynomial, but it's still polynomial.",
                    "label": 0
                },
                {
                    "sent": "And actually, we're going to end up with potentially a richer class of functions, because remember, we're taking the convex Hull of this set of functions, so we don't only have our weak learner class, but we extend it potentially.",
                    "label": 0
                },
                {
                    "sent": "To further functions.",
                    "label": 0
                },
                {
                    "sent": "But the bound that we get an I've left out one little piece, which was this bound on the Rademacher complexity of the weak learner class.",
                    "label": 0
                },
                {
                    "sent": "I will come back to it, but let me just say, for now, that it's the logarithm of this number that comes in so the logarithm of this that comes in.",
                    "label": 0
                },
                {
                    "sent": "So again it's just D log this that you would get the normal VC bound that you get, so you get the D on M quantity.",
                    "label": 0
                },
                {
                    "sent": "So if I go back to that.",
                    "label": 0
                },
                {
                    "sent": "This quickly.",
                    "label": 0
                },
                {
                    "sent": "Bow",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here it is so that our head of H here.",
                    "label": 0
                },
                {
                    "sent": "Sorry there should be a sorry it's it's log DD log sqrt D / M that you get in this RH here.",
                    "label": 0
                },
                {
                    "sent": "So exactly as you would like.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The only thing we've done, of course, is Duck.",
                    "label": 0
                },
                {
                    "sent": "The problem of how to index the functions.",
                    "label": 1
                },
                {
                    "sent": "Because, you know, in a sense that depends on this particular set of training data and so on.",
                    "label": 1
                },
                {
                    "sent": "But I would argue that actually if you could learn in that class anyway, all you have to do is to when you want to do a weak learner is just apply your standard learning algorithm for that class and you will indeed get the function out.",
                    "label": 0
                },
                {
                    "sent": "So I mean, of course the slight worry is that you've you've just.",
                    "label": 0
                },
                {
                    "sent": "Re interpreted your functions as as.",
                    "label": 0
                },
                {
                    "sent": "You know linear representations.",
                    "label": 0
                },
                {
                    "sent": "And still you know.",
                    "label": 0
                },
                {
                    "sent": "So I in a way I don't think this is the right way to to learn necessarily, but what I think what I want to show is that it opens up a way of linking this with support vector machines, so you may be able to make a soft tradeoff between doing what would be a full enumeration here, and perhaps you know the other extreme would be where you can represent all the functions in.",
                    "label": 0
                },
                {
                    "sent": "In a single function class, but maybe there's a compromise where you can have a smaller set of representations that together represent all the functions.",
                    "label": 0
                },
                {
                    "sent": "So I'll come to that in a minute.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So can we move between boosting and SVM's?",
                    "label": 1
                },
                {
                    "sent": "To ameliorate this problem of with explicitly enumerating all the classifiers as constraints?",
                    "label": 1
                },
                {
                    "sent": "And multiple kernel learning is, I think, the way to do this.",
                    "label": 0
                },
                {
                    "sent": "It aims to combine a number of different kernels and select a subset of them as part of the training algorithm.",
                    "label": 1
                },
                {
                    "sent": "So this is standard multiple kernel learning.",
                    "label": 0
                },
                {
                    "sent": "It uses this optimization.",
                    "label": 0
                },
                {
                    "sent": "So what it is looking at, if again, you know, don't don't be phased by this, it's very similar to a normal support vector machine.",
                    "label": 0
                },
                {
                    "sent": "The difference is that rather than have a single weight vector.",
                    "label": 0
                },
                {
                    "sent": "You have the sum of the norms of.",
                    "label": 0
                },
                {
                    "sent": "In this case, NN weight vectors.",
                    "label": 0
                },
                {
                    "sent": "Sorry there should be a T here.",
                    "label": 0
                },
                {
                    "sent": "That's a sorry T = 1 here.",
                    "label": 0
                },
                {
                    "sent": "And then you use the.",
                    "label": 0
                },
                {
                    "sent": "The actual output is the sum of those weight vectors in a product of those weight vectors with the corresponding FI for each of the different kernels.",
                    "label": 0
                },
                {
                    "sent": "So there are N different kernels and therefore N different projections into different feature spaces.",
                    "label": 0
                },
                {
                    "sent": "And you have a weight vector in each of those feature spec spaces.",
                    "label": 1
                },
                {
                    "sent": "You sum the inner product over.",
                    "label": 0
                },
                {
                    "sent": "Sorry all of those inner products to give the output and then you.",
                    "label": 0
                },
                {
                    "sent": "Add a threshold and and so on.",
                    "label": 0
                },
                {
                    "sent": "So that's the standard multiple kernel learning.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Optimization.",
                    "label": 0
                },
                {
                    "sent": "But equivalently, you can see MCL as putting A1 norm constraint on a linear combination of kernels.",
                    "label": 1
                },
                {
                    "sent": "So you're trying to create a kernel that is a linear combination of these base kernels with positive coefficients that has some of those said T equal to 1, and as part of the training of the SVM you also optimize the value of these.",
                    "label": 0
                },
                {
                    "sent": "Coefficient said T and it.",
                    "label": 1
                },
                {
                    "sent": "It turns out it is a convex problem.",
                    "label": 0
                },
                {
                    "sent": "It's very closely related to group lasso.",
                    "label": 0
                },
                {
                    "sent": "Lasso is doing one norm regularization for regression.",
                    "label": 0
                },
                {
                    "sent": "And this group Lasso has this same idea of having several groupings of the features.",
                    "label": 0
                },
                {
                    "sent": "Of course, here the features can be very large sets of features 'cause they're represented by kernels.",
                    "label": 1
                },
                {
                    "sent": "OK. Now it turns out that's also equivalent to performing linear programming, boosting over the following set of weak learners, where now we have a class of weak learners corresponding to each kernel, which is the set of functions that we can represent using that kernel with a weight vector bounded.",
                    "label": 0
                },
                {
                    "sent": "The two norm of the weight vector now bounded by by one.",
                    "label": 0
                },
                {
                    "sent": "So unit, most unit norm weight vector, so this now is an infinite set of.",
                    "label": 0
                },
                {
                    "sent": "Weak learners.",
                    "label": 0
                },
                {
                    "sent": "But so you know, we may have some trouble identifying the best weak learner in this class.",
                    "label": 0
                },
                {
                    "sent": "Perhaps we'll look at that in a minute, but this actually I'm not going to show the detail of that, but it's perhaps believable when you see the way that we can represent this function class.",
                    "label": 0
                },
                {
                    "sent": "That this indeed does look like.",
                    "label": 0
                },
                {
                    "sent": "We're just taking a linear combination of functions that can be represented by this kernel, and so we're sort of taking a two norm within the function class.",
                    "label": 0
                },
                {
                    "sent": "The one norm when we combine the different function classes together.",
                    "label": 0
                },
                {
                    "sent": "So I mean, I'm skipping a little bit of detail here, but I hope you accept that a reasonable thing to happen.",
                    "label": 0
                },
                {
                    "sent": "OK, so now the question would be can we?",
                    "label": 0
                },
                {
                    "sent": "How can we identify the weak learner in this that we need to in the in the boosting iteration?",
                    "label": 0
                },
                {
                    "sent": "You know, we've given now awaiting over the examples and we have to maximize that.",
                    "label": 0
                },
                {
                    "sent": "Entity that appears in the in the boosting algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So this is it's we want to implement MKL.",
                    "label": 1
                },
                {
                    "sent": "So we have to maximize this quantity for a given UI over the set of weak learners.",
                    "label": 0
                },
                {
                    "sent": "But it's actually quite easy because we can see that too.",
                    "label": 0
                },
                {
                    "sent": "If we want to optimize over a particular function class FT. What we need to do is maximize over a choice of weight vector norm at most one this quantity, and if we move this sum inside, we can see that actually we know how to do this, we just choose W parallel to this vector of norm one and we maximize this inner product.",
                    "label": 1
                },
                {
                    "sent": "So we can indeed, and we even have a dual representation of W which is just given by.",
                    "label": 0
                },
                {
                    "sent": "The dual variables UI iy so we can actually implement that efficiently in a kernel space.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So we can apply our bread maker.",
                    "label": 0
                },
                {
                    "sent": "Bound for boosting.",
                    "label": 0
                },
                {
                    "sent": "And here it is.",
                    "label": 0
                },
                {
                    "sent": "All we need now is to compute the Rademacher complexity of.",
                    "label": 0
                },
                {
                    "sent": "I've slightly altered it in terms of the way I've presented, but it's the same bound we need to compute the Rademacher complexity of the Union of these function classes, so we're still missing.",
                    "label": 1
                },
                {
                    "sent": "Essentially, that was the thing I skipped over last time, but by doing it in this more general version, it will also apply to the previous, because we could think of FT is just being a single function if we wanted.",
                    "label": 0
                },
                {
                    "sent": "So how are we going to do that well?",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This is another sort of little trick that we can apply for.",
                    "label": 0
                },
                {
                    "sent": "Application of Macdermid tells us that we can actually.",
                    "label": 0
                },
                {
                    "sent": "This is this Rademacher complexity is also concentrated in terms of the Rademacher variables, so we can pick a particular random random actor.",
                    "label": 0
                },
                {
                    "sent": "You know one example, the Rademacher variable 11 instantiation, rather than the expectation over the Rademacher variables.",
                    "label": 0
                },
                {
                    "sent": "We can pick one instantiation and have a bound of the following form which says for that single instantiation plus correction term, which it comes from Macdiarmid, where we're able to upper bound the Rademacher complexity of the function class H. And similarly, we can effectively lower bound the individual sorry upper bound.",
                    "label": 0
                },
                {
                    "sent": "The sorry lower bound, the.",
                    "label": 0
                },
                {
                    "sent": "Rademacher complexity of the individual function classes by the same instantiation, but with a bit subtracted off, which I put onto the other side.",
                    "label": 0
                },
                {
                    "sent": "So this game with probability Delta T here and D0 there.",
                    "label": 0
                },
                {
                    "sent": "That should have been Delta 0 number.",
                    "label": 0
                },
                {
                    "sent": "So the thing about having a concrete Sigma is that we can then actually compute the Mac.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some quite easily, so we get the following if you use Delta T equal to this quantity.",
                    "label": 0
                },
                {
                    "sent": "Then we actually have for this concrete Sigma star.",
                    "label": 0
                },
                {
                    "sent": "This is the H, the upper bound that we had.",
                    "label": 0
                },
                {
                    "sent": "And now we can choose the maximum.",
                    "label": 0
                },
                {
                    "sent": "Over this class, but now we know that actually this because it's now just looking at the class FT is upper bounded by the Rademacher complexity of FT. That's exactly the second line of the previous slide, so actually this is now just the maximum of the Rademacher complexity of the individual classes, plus this correction term.",
                    "label": 0
                },
                {
                    "sent": "And as I indicated, the correction term has a log of the number.",
                    "label": 0
                },
                {
                    "sent": "Functions or function classes functions if we had individual functions, or in this case log of the number of function classes, so this sort of suggests that we can actually throw in an awful lot of function classes and it won't cost us too much.",
                    "label": 0
                },
                {
                    "sent": "In in terms of the generalization, and this was how you know with the number of functions in the simple translation, two of the VC class.",
                    "label": 0
                },
                {
                    "sent": "This was the going to be the growth function that comes from sours, Lemus OEM overdyed the power do you log of it gives us the D so we get sqrt D / M that we need to get the generalization in that case.",
                    "label": 0
                },
                {
                    "sent": "So, so my the point I'm making here is that we're actually if I go back just one slide.",
                    "label": 0
                },
                {
                    "sent": "Here we are able to now.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two slides were now able to apply a convex optimization which involves someone norm.",
                    "label": 0
                },
                {
                    "sent": "Some two norm of big class of kernels and have a generalization bound.",
                    "label": 0
                },
                {
                    "sent": "Here it is that actually is benign in terms of the influence of the number of kernels that we've included, and by including large numbers of kernels we can certainly represent all VC classes and actually wear it.",
                    "label": 0
                },
                {
                    "sent": "In a sense more flexible because we get combinations of.",
                    "label": 0
                },
                {
                    "sent": "The functions that are in those classes.",
                    "label": 0
                },
                {
                    "sent": "Now what I was hoping to be able to do was to sort of show that one of those nasty examples that they'd shown that couldn't be done.",
                    "label": 0
                },
                {
                    "sent": "We could somehow do with this, but as I couldn't even find a nasty example, I'm not able to do that anyway, so.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But anyway, I think it's an interesting question to find a concrete example where this approach could be really practical for a problem that was genuinely difficult in the in the normal SVM case as it.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "High dimensional.",
                    "label": 0
                },
                {
                    "sent": "So to summarize, just I've examined the limitations of learning with linear function classes and reviewed these negative results for SVM's when compared to general SVC classes, and I've showed how this can be overcome, maybe in a slightly sort of read.",
                    "label": 1
                },
                {
                    "sent": "Representation way, but I think by extending to this multiple kernel learning there is genuinely here a sort of interesting direction that might be able to tackle a wider range of problems and convert them into this sort of convex optimization framework that seems to work so well in practice, so sort of linking if you like some of the theoretical.",
                    "label": 0
                },
                {
                    "sent": "Classes that one would like to be able to learn with the practical algorithms that people are using in the field.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks very much.",
                    "label": 0
                },
                {
                    "sent": "Let me let me ask a question.",
                    "label": 0
                },
                {
                    "sent": "Physical abilities by now, bonds have people actually looked into what the other market technology has to do with permutation testing statistics.",
                    "label": 0
                },
                {
                    "sent": "Because there is also very, very frequently used way of deciding whether you are sort of guessing over easy structure.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I, I think people haven't done enough to see, you know.",
                    "label": 0
                },
                {
                    "sent": "I think there is a connection.",
                    "label": 0
                },
                {
                    "sent": "I mean, for instance you know.",
                    "label": 0
                },
                {
                    "sent": "Picking that concrete permutation that I did in the last, you could imagine doing that in practice.",
                    "label": 0
                },
                {
                    "sent": "You just pick a random Sigma.",
                    "label": 0
                },
                {
                    "sent": "And actually compute this value if I go back to that.",
                    "label": 0
                },
                {
                    "sent": "Parent here, right?",
                    "label": 0
                },
                {
                    "sent": "So don't think now.",
                    "label": 0
                },
                {
                    "sent": "Just think of a single function class that you want to compute the Rademacher complexity of.",
                    "label": 0
                },
                {
                    "sent": "You just choose a random Sigma, compute this value.",
                    "label": 0
                },
                {
                    "sent": "That's like a randomized test, and you've got an upper bound, a concrete upper bound, you know.",
                    "label": 0
                },
                {
                    "sent": "Normally what we would do to compute this is use some analysis technique, for instance for SVM as we compute, because it's a linear function class, we can compute an upper bound on this, or you know I showed you with the boosting kind of idea.",
                    "label": 0
                },
                {
                    "sent": "But there's no reason why we shouldn't try and do it in a very concrete case by just computing this value.",
                    "label": 0
                },
                {
                    "sent": "The only problem is that this is.",
                    "label": 0
                },
                {
                    "sent": "A bit like doing agnostic learning because you're trying to learn a.",
                    "label": 0
                },
                {
                    "sent": "You know, a nasty you know a lot of errors there.",
                    "label": 0
                },
                {
                    "sent": "There's going to be a lot of errors.",
                    "label": 0
                },
                {
                    "sent": "You're hoping there going to be a lot of errors, because otherwise you know you want it to be a very poor learner of this random thing.",
                    "label": 0
                },
                {
                    "sent": "So often learning Agnostically is much harder than learning.",
                    "label": 0
                },
                {
                    "sent": "You know when you can actually get a good classification, but there was some work that.",
                    "label": 0
                },
                {
                    "sent": "Was done with decision trees where somebody built a decision tree and then pruned it using this Rademacher band.",
                    "label": 0
                },
                {
                    "sent": "So where you actually?",
                    "label": 0
                },
                {
                    "sent": "Because there when you're pruning, you can prune optimally against to get the minimum number of errors, so I think, but I think more should be done.",
                    "label": 0
                },
                {
                    "sent": "I mean, there's a really interesting connection there, yeah?",
                    "label": 0
                },
                {
                    "sent": "Make sure it comes up every day, at least for practitioners.",
                    "label": 0
                },
                {
                    "sent": "The complaint about the balance in statistical learning, theory of computation, as it should say, computational field there that they are so loose when you actually look at your budget for real experiment, you never have enough and enough samples to even push your bound below 1 so.",
                    "label": 0
                },
                {
                    "sent": "Has this disease of the balance be sort of cute little bit by by the technology or I'm still not doing that in the realistic parameter space where where you could help the practitioners you guys here with experiments.",
                    "label": 0
                },
                {
                    "sent": "OK, so so very good question like that.",
                    "label": 0
                },
                {
                    "sent": "So 2 answers.",
                    "label": 0
                },
                {
                    "sent": "One is that for the Rademacher bounds I would say we're not in that space that it still unrealistic.",
                    "label": 0
                },
                {
                    "sent": "But, and however recently we've done some experiments using the Rademacher bounds too.",
                    "label": 0
                },
                {
                    "sent": "2.",
                    "label": 0
                },
                {
                    "sent": "Do model selection with remarkably good results.",
                    "label": 0
                },
                {
                    "sent": "I was really, really surprised.",
                    "label": 0
                },
                {
                    "sent": "I mean, I actually was doing it to prove it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "And it turned out to work surprisingly well.",
                    "label": 0
                },
                {
                    "sent": "So I mean, even though the bounds were very weak, obviously what we have the bounds that are actually.",
                    "label": 0
                },
                {
                    "sent": "Good, much better than than these are the PAC Bayes methods which I haven't talked about today, but the PAC Bayes bounds are nontrivial.",
                    "label": 0
                },
                {
                    "sent": "You know they're less than one and even less than .5.",
                    "label": 0
                },
                {
                    "sent": "We had a workshop you know bounds less than .5 with John Langford a few years back.",
                    "label": 0
                },
                {
                    "sent": "And yeah, I know they are actually, you know, a factor, sometimes three worse or even smaller than the test error.",
                    "label": 0
                },
                {
                    "sent": "So they genuinely give.",
                    "label": 0
                },
                {
                    "sent": "A very tight bound, unfortunately, and we've had various ways of, you know, adjusting the prior to get them tighter still and so on.",
                    "label": 0
                },
                {
                    "sent": "But often what we found is that as you refine the.",
                    "label": 0
                },
                {
                    "sent": "Bound by tweaking it, the ability for it to do model selection actually can decrease in a very sort of disappointing way, so maybe you're tuning the bound 222 well to what it can do, and therefore it's not picking up the something else that the structure of the data carries.",
                    "label": 0
                },
                {
                    "sent": "So you make the wrong choices in model selection, because in the sense you know bound has two values.",
                    "label": 0
                },
                {
                    "sent": "One is to drive a model selection and the other is to tell the practitioner yes, it's.",
                    "label": 0
                },
                {
                    "sent": "It's it's this is a real estimate of the generalization.",
                    "label": 0
                },
                {
                    "sent": "At the time it might have the qualitatively correct behavior, since the numbers are wrong, you are on thin ice in some sense, because you can't really distinguish the case where it's qualitatively correct, but but all of magnitude larger.",
                    "label": 0
                },
                {
                    "sent": "Well, actually available improvement of rebound would get adrift in the minimum, and then you will select a different model.",
                    "label": 0
                },
                {
                    "sent": "So what I would what I would claim is the following PAC Bayes with the sort of adjustments that I've talked about gives very tight.",
                    "label": 0
                },
                {
                    "sent": "Sounds you know Factor 3, perhaps you know so non trivial.",
                    "label": 0
                },
                {
                    "sent": "And is as good as cross validation.",
                    "label": 0
                },
                {
                    "sent": "Plus or minus epsilon, occasionally slightly better syndication is only worth so, so I would say we're, you know we aren't what we would like to be able to do, so we do better than cross validation, but I can't say that, but I think we're able to manage.",
                    "label": 0
                },
                {
                    "sent": "It's very, but at least the bands are, you know non non trivial and interesting and possibly useful for practitioners and they're able to drive.",
                    "label": 0
                },
                {
                    "sent": "So I think you know it's a big advance from what we were at say 10 years ago.",
                    "label": 0
                },
                {
                    "sent": "Personalization is actually a really for the computer science students.",
                    "label": 0
                },
                {
                    "sent": "There's no excuse not to do it, and they don't have to understand another field for.",
                    "label": 0
                },
                {
                    "sent": "OK, I see no comments, and so let's release the audience for the coffee break.",
                    "label": 0
                },
                {
                    "sent": "And thanks again, Jennifer.",
                    "label": 0
                }
            ]
        }
    }
}