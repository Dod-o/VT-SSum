{
    "id": "cjeb4pvwcuiuv4cgljkg3y34x7opv36z",
    "title": "Training Support Vector Machines: Status and Challenges",
    "info": {
        "author": [
            "Chih-Jen Lin, Department of Computer Science and Information Engineering, National Taiwan University"
        ],
        "published": "Sept. 1, 2008",
        "recorded": "July 2008",
        "category": [
            "Top->Computer Science->Machine Learning->Kernel Methods->Support Vector Machines"
        ]
    },
    "url": "http://videolectures.net/icml08_lin_tsvm/",
    "segmentation": [
        [
            "OK.",
            "So in this talk I'll try to maybe give an overview of training support vector machines.",
            "And they say something about what I I think about this research topic right now."
        ],
        [
            "So, so here's some motivation.",
            "We know support Vector machine is well right now is a popular classification method.",
            "But sometimes the training is not an easy task.",
            "So in this talk we will try to check some existing training techniques and because now people talk about large datasets, so we try to say some approaches and discuss different considerations when you have very large datasets.",
            "This morning that people mention that an issue of organizing competition is that in quite a few papers that comparisons are will be the controversial well, so maybe he will try to partially answer high while it happens."
        ],
        [
            "So firstly I will see."
        ],
        [
            "Will give a brief introduction to support vector machines so far until now.",
            "Oh talks.",
            "Oh, SVM talks today about linear SVM, but we will do nonlinear.",
            "So that's how SVM was popular.",
            "There is usually we say SVM and kernel methods.",
            "So now we're given L training instances.",
            "They are.",
            "That's a pair of Zion.",
            "Why so?",
            "Why is it a class label?",
            "It is either plus or minus one.",
            "So X Isley instance.",
            "So it's a it's a vector in RN space, so we assume that there are L training instances.",
            "So the basic idea of SVM is to maximize the margin or to minimize this regularization term.",
            "But we have to allow certain training errors, so we have this loss function and the PC is is a parameter to balance those two terms and we want all we hope that most data they can be fully separated.",
            "So we have this left hand side to be greater equal to 1.",
            "But if sometimes training errors happen then we put a select variable here.",
            "So an important thing of SVM is that we map data to a high dimensional space by a mapping function.",
            "Then in that high dimensional space we do linear separation.",
            "So this is very important by using."
        ],
        [
            "Mapping function, but practically because we cannot."
        ],
        [
            "Solve this optimization problem after making the data to high dimensional space.",
            "This is going to be a very long vector, so there's only we may have infinitely many variables, because here W is is one of the."
        ],
        [
            "Variables.",
            "So people consider the dual problem.",
            "So the nice thing of this deal is that you have a finite number of variables.",
            "So instead of minimizing with respect to W, Now we minimize with respect to Alpha.",
            "So Alpha is a vector and the size is the same as the number of training data.",
            "We have a square term here, now at least after transpose Q over in the queue is a square matrix with my J component to be related to the inner product between the eyes and the largest training instances, and this is a linear term.",
            "This is a vector of all ones and after I must be between zero and C will remember us sees that parameter and there is a linear constraint.",
            "So why is is a vector of either plus or minus one to indicate indicate the?",
            "Are the class label of training data, so we have a linear constraint right?",
            "Why transpose Alpha should be zero and according to the primal dual relationship that it's optimal OK?",
            "If you can solve this to get the optimal Alpha, learn the optimal W of the original SVM problem is actually a linear combination of your training data.",
            "So we have this formulation that W is is essentially that some mention of Alpha IYY times your training vectors.",
            "But so, because we make data to a high dimensional space, so this inner product may not be easy.",
            "Yeah, so that's why people develop the kernel trick.",
            "So by certain by some spatial mapping functions.",
            "Then you can easily calculate the inner product.",
            "So usually we call this the kernel function.",
            "So that's in general how people.",
            "Chanan SVM classifier.",
            "Linda question is how to solve this quadratic programming problem, right?"
        ],
        [
            "So how's the difficulty of solving this deal?",
            "Yeah, we solve the deal because we have difficulty for the primer right?",
            "For the because the variable W maybe a long vector.",
            "So now so this one the number of variables is a finite well, that's good.",
            "However, you have this matrix Q.",
            "Here it is an L by L. In general, in general fully dense matrix and I remember there is a number of instances.",
            "The QA component involves the inner product between two training vectors, so in general it is not zero, so you have dense quadratic programming problem, so that's where the difficulty occurs.",
            "We are suppose if you have 50,000 training points, then for this dual problem you have 50,000 variables.",
            "Well, in order to store, this matrix will need a square and assume double precision, but taking the symmetry property, then you still need 10 gigabytes of RAM.",
            "To store this square matrix, therefore, traditional optimization methods such as Newton or quasi Newton, they cannot be directly applied.",
            "To solve this SVM dual optimization problem."
        ],
        [
            "So then.",
            "Then people develop some some spatial optimization methods to solve less VM deal.",
            "A basic idea is that you just work on some variables at one iteration.",
            "So we're roughly.",
            "We call them some kind of decomposition methods, so this is actually similar to coordinate wise minimization in optimization.",
            "So the idea is that at each iteration you select a so-called working set B.",
            "And the only change variables in this working set and all.",
            "All come all Alpha I components in in this sedan are fixed.",
            "So it locates iteration.",
            "We solve a subproblem with variable Alpha B.",
            "So these are Valley contains components in this small subset we only yeah, so you can see here I have other KN will use K2 to indicate that this vector is actually fixed.",
            "So the only variable is Alpha V. So we rightly the original objective function again and also the same constraints.",
            "But if we rewrite this from you."
        ],
        [
            "Nation a little bit.",
            "Let the new objective function well is like this.",
            "Well, because things not related to Alpha B, they are considered as constant at least iteration.",
            "So, so we still have a square time.",
            "OK, at least involves a small square matrix called cubed.",
            "And then there's a linear term, so probably is still variable here, but this vector is a constant, and what we need is this square matrix QBB an another rectangular submatrix called cubed.",
            "So this cube plus cube together.",
            "That means you need the columns or say B rose views Q is symmetric of this matrix.",
            "And how about the size of this working set will do to the linear constraint here where we need the working set B to have at least two elements, otherwise you cannot move Alpha, but in general the size of V is chosen to be a small number, usually no more than 100.",
            "So the basic idea is that because now we cannot store the whole matrix Q, therefore low skew components are just calculated when needed.",
            "So conceptually it's like we trade time for space.",
            "You know we cannot handle the memory difficulty, so we just we just use more training time.",
            "In the so far popular SVM generation software software like say SVM, lightly based more SVM torch.",
            "There all of these type of methods."
        ],
        [
            "Then the question is.",
            "How do such decomposition methods work and is it fixed or not?",
            "Because at each iteration you only change certain variables well, so sometimes the convergence is not faced where, but we're doing well, not exactly solving solving the optimization problem where we're doing a classification, so there's no need to have a very accurate Alpha, yes, so the slow convergence maybe OK, yeah, yeah, because if you don't have a very accurate Alpha, actually your prediction is not affected that much.",
            "And a good thing is in some situations.",
            "Even number of support vectors is much smaller than the number of training data.",
            "Then this kind of methods is actually quite effective.",
            "So in our iteration we always keep the so called feasibility.",
            "Our Alpha always satisfy those conditions.",
            "So a very natural initial point is the zero vector.",
            "If you don't know beta Alpha, then you just start from zero.",
            "But due to this nonnegative constraints.",
            "So it's possible that in the end quite a few other I layer just zero.",
            "And the last zero.",
            "So for those training data whose Alpha R0 they are not considered as support vectors, and they're not used for prediction?",
            "So if that's the case, and if because you initialize with Alpha.",
            "Then this possible a certain after I just never changed.",
            "Yeah, then that's a good situation because ascential you are minimizing a smaller."
        ],
        [
            "Optimization problem, But let's see an example.",
            "We are trying to train the 50,000 data set, so here we are using the basement to train that.",
            "This is actually from another competition that's a few years ago.",
            "So after 80 seconds, 80 seconds on a regular PC, then then the training is done and this is nonlinear.",
            "We look the reason why this can be so fast is because the small number of support vectors.",
            "So among the 50,000 training instances there are only 3000 support vectors, so this is a very good situation 'cause if you go to calculate the whole matrix Q OK using the same computer and then it may already have taken more time.",
            "Then 80 seconds.",
            "But of course, this good situation may not happen all the time if sometimes the percentage of support vectors is high."
        ],
        [
            "Yeah, so people people keep developing such decomposition methods because there are several techniques to have a faster, faster implementation, in particular that we can try to store some recently used kernel elements so you don't always calculate them when needed.",
            "So there are a lot of other techniques, but we're not going to talk about details here.",
            "So right now I think for medium size datasets.",
            "Will such problems are OK and no matter as linear nonlinear, you just use it?",
            "But the problem is.",
            "If you want to train very large datasets, using such methods may still be difficult.",
            "Yeah, because.",
            "The main problem is that.",
            "I mean anyway, the kernel matrix is always square to the number of training data.",
            "Yeah, so such a complexity can can cause very long training time.",
            "Yeah, so if you want to train millions of data, so let's think about some may be possible approaches.",
            "We still want to do decomposition."
        ],
        [
            "Solve nonlinear."
        ],
        [
            "So one thing is to go parallel or this is a very direct way.",
            "You don't change the algorithm at all.",
            "And those we have, as we said earlier, most computation over the decomposition methods is on calculating the kernels.",
            "So if you try to paralyze this part of computation, then you can get quite good speedup.",
            "So here I'm going to show an example.",
            "Suppose we are.",
            "We are working on a shared memory or just a multicore environment.",
            "They know you can easily paralyze this calculation by open MP.",
            "Well, I don't know if people here are familiar with this open MP, but this is a simple way for you to paralyze your code.",
            "So it's like before the for loop.",
            "Just add something to say I want to parallelize this for loop.",
            "Then the code will be paralyzed.",
            "So if you if you do just so this is true if you do a one line change of leave SVM then you can paralyze the decomposition method.",
            "And here I'm going to show some results.",
            "Actually I'm running the same 50,000 data used in in the earlier slide.",
            "So if I use just one core, then the training time is 80 seconds.",
            "And if I use eight cores, then this is reduced to 27 seconds.",
            "And a simple profiling shows that actually kernel evaluations take about 80% of time.",
            "Yeah, so if you multiply 80 by 20%, that means 1616 seconds are not paralyzed, and a plus leads 8.",
            "Let's see so 18 plus so this times 80% of 6464 / 8 + 8 seconds.",
            "So 8 + 16.",
            "That's about 24.",
            "So you see, the speedup is quite good.",
            "And this is the result on the shared memory machine.",
            "So this is something simple and easy and very easy.",
            "And if you if you are more aggressive then then you can do.",
            "You don't think open feels good enough.",
            "Yeah, like yesterday there was a talk about using GPU to do parallelization.",
            "Actually, idea is similar, but maybe the difference is now you cannot have just one line change.",
            "You need to do a few more lines because using GPU is still slightly more complicated.",
            "So this is a simple and effective way, but we are assuming that use shared memory or multicore.",
            "So you still use just one big machine.",
            "And that we so, Sir."
        ],
        [
            "An amount of RAM, but what if your data just cannot fit into memory?",
            "Then you have to use distributed environments.",
            "So people have tried that.",
            "So here are some examples.",
            "So there is a software called.",
            "SVM.",
            "This is actually this actually was done at Google Research in Beijing, so they have a code here here they do incomplete Scholastica factorization of the kernel matrix.",
            "Buy buy a pair away then then they use an interior point method to do the minimization.",
            "There's another software package called Pius via I think this one directly paralyze device via.",
            "There is another package called.",
            "GG PDT.",
            "This one is also a decomposition method.",
            "They all use MPI, four layer parallelization and they do report good speedup.",
            "But however you must be a little bit careful on the environment Lee they used.",
            "So for example, like I think one of my my friends tried this and he said that on a shared memory environment will be this one.",
            "Also works on both on distributed and shared memory architectures and unleash shared memory machine.",
            "Then the speedup is quite good.",
            "However, if you move to a distributed environment then the speed up is not so good.",
            "So actually communication costs easier concern.",
            "I mean, once you use more nodes than the communication cost may quickly increase so.",
            "Indeed, I think there are still quite a few difficulties here if you want to parallelize SVM on a distributed environment."
        ],
        [
            "But instead of I mean so.",
            "So I just say that without changing changing any algorithm, you just do parallelization.",
            "But instead of that we can we say OK, how about like we just give up solving the SVM dual optimization problem?",
            "So how to do that?",
            "Oh, so the idea is to just approximately solve the SVM quadratic programming problem.",
            "Well, you can think about many simple methods.",
            "The easiest one will be subsampling.",
            "And actually subsampling is is very simple and often effective.",
            "So wait, I always tell people that if you have a data and you find that it is too large, then try this first.",
            "But from subsampling then there are many more advanced techniques.",
            "Well, Francine from some simple ones to some complicated ones.",
            "So here I give a simple example where this was back to the early days of SVM development.",
            "It's right time later say OK. How about do a simple split of data to 10 parts?",
            "Is you know that support vectors are a small portion of the training data, but so it's difficult to train the whole set.",
            "You just train the first part, you get some support vectors, then you add the load support vectors to the second part, and then you train that so so then you are able to get a solution.",
            "And for example, you can say OK, Now the number of data is too large.",
            "Then try to select some good points."
        ],
        [
            "There are many other such techniques.",
            "This is an incomplete classification, so the idea is to basically approximate the kernel.",
            "This is a nice strong approximation.",
            "Yeah, that's again another way to approximate the kernel and this is similar.",
            "So instead of really approximating the kernel you just use part of the kernel is like.",
            "This technique is called reduced support vector machines.",
            "The basic idea is that you don't use the whole kernel matrix uses several columns.",
            "And you can do more like greedy ways.",
            "Actually, there are quite a few greedy implementations, so there are many approximation techniques.",
            "Some are simple but some."
        ],
        [
            "Very complicated, so then you say, should we do parallelization or approximation?",
            "Will essentially this confused here is when I civilization I mean that we still try to.",
            "Somehow accurately solve the SVM dual quadratic problem, but it's difficult to say well for four.",
            "Approaches there are more general, so you don't worry about different situations, but approximation can be simpler in some situations, for example, like if subsampling does work.",
            "But we can also do both.",
            "You can you can do approximation inipi realize it that's also possible.",
            "Yeah, but for set up.",
            "I think there is still a need for parallelization because for certain problem."
        ],
        [
            "Somehow a process approximation does not easily work, so let's try to have a comparison here.",
            "Let's try to datasets.",
            "Well, layer about health million.",
            "Not particularly large, but like this data set is already one of the largest in UCM machine learning repository.",
            "So if we do subsampling, then if you just use 110th of the data then the testing this is testing accuracy is just 92.5, so that's 6% different from using the whole set.",
            "But for this another data set, the size is about similar, but if you use 110th of the data then the testing accuracy is already close to two lot of using the whole set.",
            "So for this data set, clearly using subsampling, easier way to do.",
            "Then, but this was not only subsampling, doesn't quite work, of course I."
        ],
        [
            "Tried some approximation techniques like close things actually is also more difficult.",
            "I mean relatively more diff."
        ],
        [
            "And then if you try those approximation techniques on this data set, yeah, so.",
            "So it seems we are in a situation that if you have very large datasets that select some suitable approach for that particular problem is quite essential.",
            "Yeah, so so I will quickly illustrate this point using linear SVM for document classification to show that for certain problems then.",
            "You can."
        ],
        [
            "Gate very quick, quick training implementation."
        ],
        [
            "Now let's move to linear SVM.",
            "Well, we know linear means we don't make the data to the higher dimensional space, but in theory we can prove that if you use RBF kernel with parameter selection then.",
            "Leave one out on accuracy of using RBF kernel is at least as good as using linear kernel.",
            "So so from this statement you should say OK, we should never use linear support vector machines, right?",
            "We should just use nonlinear.",
            "We sell a Gaussian kernel, yeah, but the difference is that sometimes you can easily solve much larger linear SVM's OK and the accuracy maybe sometimes similar or even just slightly lower.",
            "So how do I do think that?",
            "Is that training of linear and nonlinear SVM's should be separately considered.",
            "So in other words.",
            "If you are designing a linear SVM method, you may not want or you should not compare it with something designed for general knowledge."
        ],
        [
            "Nearest via.",
            "Yeah, so so we know linear speed is going to be useful.",
            "It's accuracy is similar and you can train a larger larger set."
        ],
        [
            "Yeah.",
            "So I want to talk about the the situation that if your number of features is large for document classification, use a bag of words model.",
            "Then you have a large number of features.",
            "So recently this is an active research topic, so there are quite a few new soft."
        ],
        [
            "We're taking ages.",
            "So let's rewrite the.",
            "Lesbian formulation again, but using linear well, so this is Primal deal where the difference is I don't have the bias term B, but somehow I just admit it for simplification so I don't get the linear constraint here.",
            "Yeah, I only have bounded constraints, so that Alpha is between zero and the sea.",
            "Because I use linear, so this the square matrix Q, the edge component is just directly the inner product."
        ],
        [
            "And and I'm going to say to show you an approach that for such data, yeah, say, like this document, data code, MCV.",
            "One, if you use liberal Lib SVM by the linear kernel then it may take you 10 hours to train a data set.",
            "But by some other more suitable methods specially designed for such for document data.",
            "Then you can finish the computation in less than 5 seconds.",
            "And in fact, the difficulty becomes on the eye of the data set.",
            "It takes more time on reading the data set into the computer memory.",
            "And here I'm using the same stopping condition, so this comparison is actually fair and under the accuracy for this data set is a similar tool too if you use."
        ],
        [
            "Nina so how?",
            "How to achieve that?",
            "We still want to use decomposition methods but just use it in a different way.",
            "So now we revisit the decomposition methods that I mentioned earlier.",
            "But now I want to take an extreme situation to update just one variable at a time.",
            "So you have a one variable optimization problem to solve.",
            "Then you get an update rule like this.",
            "So from if if the ice component is selected to be to be changed, then we will.",
            "So we do some modification of Alpha and if it is, it is not in the in the in the interval of zero in the sea we map it back and that's our new Alpha.",
            "And here this gradient, the ice component of F is is the gradient of the deal.",
            "So, so it is actually the the ice component of cuaba minus one.",
            "So if we go back to the nonlinear situation, well, yeah yeah, remember that for nonlinear SVM.",
            "Up, once we select a subset then we need so many columns of the kernel matrix.",
            "Now we need 1 one column of the kernel matrix.",
            "So if N is the number of features and there is a number of data, then you need so so many operations to calculate."
        ],
        [
            "One row of the matrix, but for linear SVM will you don't need to do that because you don't make the data, so you have X.",
            "So you can define a vector W. Then by using this W to calculate the ice component of the gradient, you just do an inner product.",
            "So let's only older and so then the question is how to maintain W?",
            "Well, now you just change one component of Alpha.",
            "So if you have your original original W then only you just you just check the change of Alpha and the times that vector.",
            "So if the vector has length and so this is still an older man."
        ],
        [
            "Operation."
        ],
        [
            "So well, if you were in my students talk yesterday, then there are some other implementation details.",
            "But the point here is we're still using decomposition methods, but the key is that.",
            "For general case linear or nonlinear, our cluster is actually older NLP iteration, but by this new way actually you have older emperor iteration.",
            "OK, but but there are some other differences is about how we select the working set.",
            "For for general nonlinear case, people pay more attention on selecting the working set.",
            "The reason is that I mean you already.",
            "You need so much efforts in order to do the update, and then we find out that by such such efforts you can maintain a gradient so people try to use more information on selecting the working set.",
            "But here essentially we give up on doing that.",
            "We don't really do working set selection.",
            "So it but, but because this is a huge reduction, if the number of iterations OK, even this one, the number of iterations maybe more.",
            "But if it is not at all times more than your total computational time is still less.",
            "Yeah, but by such methods of course have is only suitable for at least so far for document data it has certain limitations that it say.",
            "For example, if you have a large penalty parameter C or if you're a number of features is small, then it's not that Latif."
        ],
        [
            "Active So what I want to say here is you must be careful on comparisons.",
            "You essentially now I have two decomposition methods, one for general nonlinear case but another for for linear.",
            "Yeah, and if you do theoretical analysis, I think they have similar convergence rates, but they have very different practical behaviors for certain problems.",
            "And here we can see why.",
            "Why sometimes controversial comparisons happen.",
            "So this morning, sorry given given examples.",
            "So in your paper, people say SVM perviz faster than SVM light, but then another one, say, well, SVM light is faster than SVM curve.",
            "Well, here I can give you another example so."
        ],
        [
            "Just show that.",
            "This new method for linear is actually less effective if you if you have number of features, small or large penalty parameter.",
            "So it is even possible that if you are not doing document classification, OK, So what I have shown is that if you do document classification then this new method is actually faster than the original 1.",
            "So I just say that Liblinear is faster then leave SVN, but if you go to try such datasets where it is very possible that maybe.",
            "The basement becomes faster then leave linear, so so you see different datasets.",
            "Then you can."
        ],
        [
            "They are very different."
        ],
        [
            "Situations.",
            "Yeah, so so seems like that if you want to handle very large datasets you want to have different training strategies.",
            "So even just for linear SVM.",
            "Will for linear SVM you can roughly consider three situations.",
            "So why is that?",
            "If your number of data is much larger than number of features?",
            "And this is the other way around.",
            "And another situation that I just talked about is that both are very large.",
            "Well, actually, if you use different methods then you can get very fast methods.",
            "So for example like this one, if you're a number of data is much larger than number of features, you really should use a primal based methods.",
            "But on the other hand this is also the case that you should use nonlinear right now if you have a small number of features, you should make data to higher dimensional space."
        ],
        [
            "Yeah."
        ],
        [
            "So, so let's go back to the issue of linear versus nonlinear.",
            "Using this competition, we find out that a lot of people they use Nunley and they use linear well.",
            "I'm not talking about linear trick, I mean for the wild trick people use linear, so while it happen.",
            "We usually think that for certain problems use nonlinear.",
            "Then you should get better accuracy.",
            "Yeah, but somehow that happened.",
            "I mean for for for people ages will they do supply linear to the whole older 10 problems even they know that the accuracy may be worse?",
            "So earlier I just mentioned that we can either still try to accurately solve the dual problem SVM problem or to do an approximation will in a sense actually linear is essentially an approximation technique.",
            "Of non linear SVM so well, if I'm a participant over the competition for this wild trick and you'll find out there are so many huge datasets and I find out that it is difficult to analyze which one is more suitable for nonlinear.",
            "But I do know that if I just apply suitable linear SVM techniques to all of them and I am able to.",
            "To get some solutions then maybe I made such decision, yeah?",
            "So I think that's one reason maybe.",
            "Why in this competition, so many use linear.",
            "So I've said several times that seems for large datasets, selecting a right approach is essential, but.",
            "Doing that is difficult for just the typical user.",
            "How can they do that now they cannot."
        ],
        [
            "Yeah, so this is this problem.",
            "I mean there are too many approaches is indeed very bad from the viewpoint of designing machine learning software packages because we must remember that negative success of software like Live SVM or SVM light is because they are simple and general.",
            "So people just don't worry about I should use linear or nonlinear or something else, we just run it.",
            "So seems that we need developments in both directions.",
            "So here from Ice Age general is something like you do GPU computing.",
            "Hurry up the parallel stuff, but specific, maybe more related to approximation where we should do both directions.",
            "Then hopefully that will help to advance this.",
            "Type this research topic.",
            "So thank you very much.",
            "2 one question.",
            "About properties of the problems that make one method better than another method.",
            "For example, larger number of training examples and features and and stuff like that.",
            "So this types of properties are.",
            "Can you basically yeah.",
            "Yeah yeah yeah yeah yeah.",
            "Yeah yeah, yeah.",
            "There are more.",
            "Property yeah.",
            "Yeah yeah yeah yeah."
        ],
        [
            "Yeah, exactly.",
            "I mean, every time when I say this statement, people ask the same question.",
            "So say, is there any intelligent away?",
            "'cause if you get the data then.",
            "Then maybe there's a script doing some analysis to give you some recommendations.",
            "Will let something I'm still thinking.",
            "Yeah, I do have some guidelines."
        ],
        [
            "So for example like if I'm doing linear SVM and for each situation.",
            "I really know what you should do, but for nonlinear is more difficult.",
            "Yes.",
            "Overall, I think this is still very challenging issue.",
            "I actually.",
            "This methylamine problem has been tackled also in the constraints of this faction community, and actually the first step towards solving this problem is trying to find some older para meters.",
            "I made this so called constraint density and constraint tightness and I feel that we are not yet at this level of sophistication.",
            "I mean, having good, not."
        ],
        [
            "Hello all the parameters and that's really one of the goals of the of the large shapes challenge was to find weather with the synthetic datasets.",
            "We could explore some other apartments as an income coasters for difficulty or whatever.",
            "Yeah, yeah, so we still have some something to do.",
            "Kind of all the parameters that we might want to explore when generating new work.",
            "Synthetic datasets would be just great.",
            "Yeah, yeah.",
            "Well, generating data well I'm not good at such things here.",
            "If you have some feelings about what could make some problems more difficult than others, oh Oh yeah.",
            "Well I think for me Linea."
        ],
        [
            "The linear situation is quite clear that usually separate linear SVM tools, three situations and issues quite very different methods.",
            "But for nonlinear will.",
            "Yeah, well, I think instead of doing using artificial data which tries to collect more real data, don't you think that's that's more realistic?",
            "History, I mean people were trying to tackle this meta learning problem based on the UCI.",
            "So happens, oh OK, yeah.",
            "It's great but but you see the Legends, Disease or Overman.",
            "Yeah yeah, maybe yeah yeah.",
            "Well yeah we can do that.",
            "Yeah yeah, sure."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So in this talk I'll try to maybe give an overview of training support vector machines.",
                    "label": 1
                },
                {
                    "sent": "And they say something about what I I think about this research topic right now.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so here's some motivation.",
                    "label": 0
                },
                {
                    "sent": "We know support Vector machine is well right now is a popular classification method.",
                    "label": 0
                },
                {
                    "sent": "But sometimes the training is not an easy task.",
                    "label": 0
                },
                {
                    "sent": "So in this talk we will try to check some existing training techniques and because now people talk about large datasets, so we try to say some approaches and discuss different considerations when you have very large datasets.",
                    "label": 1
                },
                {
                    "sent": "This morning that people mention that an issue of organizing competition is that in quite a few papers that comparisons are will be the controversial well, so maybe he will try to partially answer high while it happens.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So firstly I will see.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will give a brief introduction to support vector machines so far until now.",
                    "label": 1
                },
                {
                    "sent": "Oh talks.",
                    "label": 0
                },
                {
                    "sent": "Oh, SVM talks today about linear SVM, but we will do nonlinear.",
                    "label": 0
                },
                {
                    "sent": "So that's how SVM was popular.",
                    "label": 0
                },
                {
                    "sent": "There is usually we say SVM and kernel methods.",
                    "label": 0
                },
                {
                    "sent": "So now we're given L training instances.",
                    "label": 0
                },
                {
                    "sent": "They are.",
                    "label": 0
                },
                {
                    "sent": "That's a pair of Zion.",
                    "label": 0
                },
                {
                    "sent": "Why so?",
                    "label": 0
                },
                {
                    "sent": "Why is it a class label?",
                    "label": 0
                },
                {
                    "sent": "It is either plus or minus one.",
                    "label": 0
                },
                {
                    "sent": "So X Isley instance.",
                    "label": 0
                },
                {
                    "sent": "So it's a it's a vector in RN space, so we assume that there are L training instances.",
                    "label": 1
                },
                {
                    "sent": "So the basic idea of SVM is to maximize the margin or to minimize this regularization term.",
                    "label": 0
                },
                {
                    "sent": "But we have to allow certain training errors, so we have this loss function and the PC is is a parameter to balance those two terms and we want all we hope that most data they can be fully separated.",
                    "label": 0
                },
                {
                    "sent": "So we have this left hand side to be greater equal to 1.",
                    "label": 0
                },
                {
                    "sent": "But if sometimes training errors happen then we put a select variable here.",
                    "label": 0
                },
                {
                    "sent": "So an important thing of SVM is that we map data to a high dimensional space by a mapping function.",
                    "label": 1
                },
                {
                    "sent": "Then in that high dimensional space we do linear separation.",
                    "label": 0
                },
                {
                    "sent": "So this is very important by using.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mapping function, but practically because we cannot.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solve this optimization problem after making the data to high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "This is going to be a very long vector, so there's only we may have infinitely many variables, because here W is is one of the.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Variables.",
                    "label": 0
                },
                {
                    "sent": "So people consider the dual problem.",
                    "label": 1
                },
                {
                    "sent": "So the nice thing of this deal is that you have a finite number of variables.",
                    "label": 0
                },
                {
                    "sent": "So instead of minimizing with respect to W, Now we minimize with respect to Alpha.",
                    "label": 0
                },
                {
                    "sent": "So Alpha is a vector and the size is the same as the number of training data.",
                    "label": 0
                },
                {
                    "sent": "We have a square term here, now at least after transpose Q over in the queue is a square matrix with my J component to be related to the inner product between the eyes and the largest training instances, and this is a linear term.",
                    "label": 0
                },
                {
                    "sent": "This is a vector of all ones and after I must be between zero and C will remember us sees that parameter and there is a linear constraint.",
                    "label": 0
                },
                {
                    "sent": "So why is is a vector of either plus or minus one to indicate indicate the?",
                    "label": 0
                },
                {
                    "sent": "Are the class label of training data, so we have a linear constraint right?",
                    "label": 0
                },
                {
                    "sent": "Why transpose Alpha should be zero and according to the primal dual relationship that it's optimal OK?",
                    "label": 0
                },
                {
                    "sent": "If you can solve this to get the optimal Alpha, learn the optimal W of the original SVM problem is actually a linear combination of your training data.",
                    "label": 0
                },
                {
                    "sent": "So we have this formulation that W is is essentially that some mention of Alpha IYY times your training vectors.",
                    "label": 0
                },
                {
                    "sent": "But so, because we make data to a high dimensional space, so this inner product may not be easy.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's why people develop the kernel trick.",
                    "label": 0
                },
                {
                    "sent": "So by certain by some spatial mapping functions.",
                    "label": 0
                },
                {
                    "sent": "Then you can easily calculate the inner product.",
                    "label": 0
                },
                {
                    "sent": "So usually we call this the kernel function.",
                    "label": 0
                },
                {
                    "sent": "So that's in general how people.",
                    "label": 0
                },
                {
                    "sent": "Chanan SVM classifier.",
                    "label": 0
                },
                {
                    "sent": "Linda question is how to solve this quadratic programming problem, right?",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So how's the difficulty of solving this deal?",
                    "label": 0
                },
                {
                    "sent": "Yeah, we solve the deal because we have difficulty for the primer right?",
                    "label": 0
                },
                {
                    "sent": "For the because the variable W maybe a long vector.",
                    "label": 0
                },
                {
                    "sent": "So now so this one the number of variables is a finite well, that's good.",
                    "label": 0
                },
                {
                    "sent": "However, you have this matrix Q.",
                    "label": 0
                },
                {
                    "sent": "Here it is an L by L. In general, in general fully dense matrix and I remember there is a number of instances.",
                    "label": 1
                },
                {
                    "sent": "The QA component involves the inner product between two training vectors, so in general it is not zero, so you have dense quadratic programming problem, so that's where the difficulty occurs.",
                    "label": 0
                },
                {
                    "sent": "We are suppose if you have 50,000 training points, then for this dual problem you have 50,000 variables.",
                    "label": 0
                },
                {
                    "sent": "Well, in order to store, this matrix will need a square and assume double precision, but taking the symmetry property, then you still need 10 gigabytes of RAM.",
                    "label": 0
                },
                {
                    "sent": "To store this square matrix, therefore, traditional optimization methods such as Newton or quasi Newton, they cannot be directly applied.",
                    "label": 1
                },
                {
                    "sent": "To solve this SVM dual optimization problem.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So then.",
                    "label": 0
                },
                {
                    "sent": "Then people develop some some spatial optimization methods to solve less VM deal.",
                    "label": 0
                },
                {
                    "sent": "A basic idea is that you just work on some variables at one iteration.",
                    "label": 1
                },
                {
                    "sent": "So we're roughly.",
                    "label": 1
                },
                {
                    "sent": "We call them some kind of decomposition methods, so this is actually similar to coordinate wise minimization in optimization.",
                    "label": 1
                },
                {
                    "sent": "So the idea is that at each iteration you select a so-called working set B.",
                    "label": 0
                },
                {
                    "sent": "And the only change variables in this working set and all.",
                    "label": 0
                },
                {
                    "sent": "All come all Alpha I components in in this sedan are fixed.",
                    "label": 0
                },
                {
                    "sent": "So it locates iteration.",
                    "label": 0
                },
                {
                    "sent": "We solve a subproblem with variable Alpha B.",
                    "label": 0
                },
                {
                    "sent": "So these are Valley contains components in this small subset we only yeah, so you can see here I have other KN will use K2 to indicate that this vector is actually fixed.",
                    "label": 0
                },
                {
                    "sent": "So the only variable is Alpha V. So we rightly the original objective function again and also the same constraints.",
                    "label": 0
                },
                {
                    "sent": "But if we rewrite this from you.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nation a little bit.",
                    "label": 0
                },
                {
                    "sent": "Let the new objective function well is like this.",
                    "label": 1
                },
                {
                    "sent": "Well, because things not related to Alpha B, they are considered as constant at least iteration.",
                    "label": 0
                },
                {
                    "sent": "So, so we still have a square time.",
                    "label": 0
                },
                {
                    "sent": "OK, at least involves a small square matrix called cubed.",
                    "label": 0
                },
                {
                    "sent": "And then there's a linear term, so probably is still variable here, but this vector is a constant, and what we need is this square matrix QBB an another rectangular submatrix called cubed.",
                    "label": 0
                },
                {
                    "sent": "So this cube plus cube together.",
                    "label": 0
                },
                {
                    "sent": "That means you need the columns or say B rose views Q is symmetric of this matrix.",
                    "label": 0
                },
                {
                    "sent": "And how about the size of this working set will do to the linear constraint here where we need the working set B to have at least two elements, otherwise you cannot move Alpha, but in general the size of V is chosen to be a small number, usually no more than 100.",
                    "label": 0
                },
                {
                    "sent": "So the basic idea is that because now we cannot store the whole matrix Q, therefore low skew components are just calculated when needed.",
                    "label": 1
                },
                {
                    "sent": "So conceptually it's like we trade time for space.",
                    "label": 0
                },
                {
                    "sent": "You know we cannot handle the memory difficulty, so we just we just use more training time.",
                    "label": 0
                },
                {
                    "sent": "In the so far popular SVM generation software software like say SVM, lightly based more SVM torch.",
                    "label": 0
                },
                {
                    "sent": "There all of these type of methods.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Then the question is.",
                    "label": 0
                },
                {
                    "sent": "How do such decomposition methods work and is it fixed or not?",
                    "label": 1
                },
                {
                    "sent": "Because at each iteration you only change certain variables well, so sometimes the convergence is not faced where, but we're doing well, not exactly solving solving the optimization problem where we're doing a classification, so there's no need to have a very accurate Alpha, yes, so the slow convergence maybe OK, yeah, yeah, because if you don't have a very accurate Alpha, actually your prediction is not affected that much.",
                    "label": 1
                },
                {
                    "sent": "And a good thing is in some situations.",
                    "label": 0
                },
                {
                    "sent": "Even number of support vectors is much smaller than the number of training data.",
                    "label": 0
                },
                {
                    "sent": "Then this kind of methods is actually quite effective.",
                    "label": 0
                },
                {
                    "sent": "So in our iteration we always keep the so called feasibility.",
                    "label": 0
                },
                {
                    "sent": "Our Alpha always satisfy those conditions.",
                    "label": 0
                },
                {
                    "sent": "So a very natural initial point is the zero vector.",
                    "label": 0
                },
                {
                    "sent": "If you don't know beta Alpha, then you just start from zero.",
                    "label": 0
                },
                {
                    "sent": "But due to this nonnegative constraints.",
                    "label": 0
                },
                {
                    "sent": "So it's possible that in the end quite a few other I layer just zero.",
                    "label": 0
                },
                {
                    "sent": "And the last zero.",
                    "label": 0
                },
                {
                    "sent": "So for those training data whose Alpha R0 they are not considered as support vectors, and they're not used for prediction?",
                    "label": 0
                },
                {
                    "sent": "So if that's the case, and if because you initialize with Alpha.",
                    "label": 0
                },
                {
                    "sent": "Then this possible a certain after I just never changed.",
                    "label": 0
                },
                {
                    "sent": "Yeah, then that's a good situation because ascential you are minimizing a smaller.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Optimization problem, But let's see an example.",
                    "label": 1
                },
                {
                    "sent": "We are trying to train the 50,000 data set, so here we are using the basement to train that.",
                    "label": 0
                },
                {
                    "sent": "This is actually from another competition that's a few years ago.",
                    "label": 1
                },
                {
                    "sent": "So after 80 seconds, 80 seconds on a regular PC, then then the training is done and this is nonlinear.",
                    "label": 0
                },
                {
                    "sent": "We look the reason why this can be so fast is because the small number of support vectors.",
                    "label": 0
                },
                {
                    "sent": "So among the 50,000 training instances there are only 3000 support vectors, so this is a very good situation 'cause if you go to calculate the whole matrix Q OK using the same computer and then it may already have taken more time.",
                    "label": 1
                },
                {
                    "sent": "Then 80 seconds.",
                    "label": 1
                },
                {
                    "sent": "But of course, this good situation may not happen all the time if sometimes the percentage of support vectors is high.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so people people keep developing such decomposition methods because there are several techniques to have a faster, faster implementation, in particular that we can try to store some recently used kernel elements so you don't always calculate them when needed.",
                    "label": 1
                },
                {
                    "sent": "So there are a lot of other techniques, but we're not going to talk about details here.",
                    "label": 0
                },
                {
                    "sent": "So right now I think for medium size datasets.",
                    "label": 0
                },
                {
                    "sent": "Will such problems are OK and no matter as linear nonlinear, you just use it?",
                    "label": 0
                },
                {
                    "sent": "But the problem is.",
                    "label": 0
                },
                {
                    "sent": "If you want to train very large datasets, using such methods may still be difficult.",
                    "label": 0
                },
                {
                    "sent": "Yeah, because.",
                    "label": 0
                },
                {
                    "sent": "The main problem is that.",
                    "label": 0
                },
                {
                    "sent": "I mean anyway, the kernel matrix is always square to the number of training data.",
                    "label": 1
                },
                {
                    "sent": "Yeah, so such a complexity can can cause very long training time.",
                    "label": 1
                },
                {
                    "sent": "Yeah, so if you want to train millions of data, so let's think about some may be possible approaches.",
                    "label": 0
                },
                {
                    "sent": "We still want to do decomposition.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Solve nonlinear.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one thing is to go parallel or this is a very direct way.",
                    "label": 0
                },
                {
                    "sent": "You don't change the algorithm at all.",
                    "label": 0
                },
                {
                    "sent": "And those we have, as we said earlier, most computation over the decomposition methods is on calculating the kernels.",
                    "label": 1
                },
                {
                    "sent": "So if you try to paralyze this part of computation, then you can get quite good speedup.",
                    "label": 0
                },
                {
                    "sent": "So here I'm going to show an example.",
                    "label": 0
                },
                {
                    "sent": "Suppose we are.",
                    "label": 0
                },
                {
                    "sent": "We are working on a shared memory or just a multicore environment.",
                    "label": 0
                },
                {
                    "sent": "They know you can easily paralyze this calculation by open MP.",
                    "label": 0
                },
                {
                    "sent": "Well, I don't know if people here are familiar with this open MP, but this is a simple way for you to paralyze your code.",
                    "label": 0
                },
                {
                    "sent": "So it's like before the for loop.",
                    "label": 0
                },
                {
                    "sent": "Just add something to say I want to parallelize this for loop.",
                    "label": 0
                },
                {
                    "sent": "Then the code will be paralyzed.",
                    "label": 0
                },
                {
                    "sent": "So if you if you do just so this is true if you do a one line change of leave SVM then you can paralyze the decomposition method.",
                    "label": 1
                },
                {
                    "sent": "And here I'm going to show some results.",
                    "label": 1
                },
                {
                    "sent": "Actually I'm running the same 50,000 data used in in the earlier slide.",
                    "label": 0
                },
                {
                    "sent": "So if I use just one core, then the training time is 80 seconds.",
                    "label": 0
                },
                {
                    "sent": "And if I use eight cores, then this is reduced to 27 seconds.",
                    "label": 0
                },
                {
                    "sent": "And a simple profiling shows that actually kernel evaluations take about 80% of time.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so if you multiply 80 by 20%, that means 1616 seconds are not paralyzed, and a plus leads 8.",
                    "label": 0
                },
                {
                    "sent": "Let's see so 18 plus so this times 80% of 6464 / 8 + 8 seconds.",
                    "label": 0
                },
                {
                    "sent": "So 8 + 16.",
                    "label": 0
                },
                {
                    "sent": "That's about 24.",
                    "label": 0
                },
                {
                    "sent": "So you see, the speedup is quite good.",
                    "label": 0
                },
                {
                    "sent": "And this is the result on the shared memory machine.",
                    "label": 0
                },
                {
                    "sent": "So this is something simple and easy and very easy.",
                    "label": 1
                },
                {
                    "sent": "And if you if you are more aggressive then then you can do.",
                    "label": 0
                },
                {
                    "sent": "You don't think open feels good enough.",
                    "label": 0
                },
                {
                    "sent": "Yeah, like yesterday there was a talk about using GPU to do parallelization.",
                    "label": 0
                },
                {
                    "sent": "Actually, idea is similar, but maybe the difference is now you cannot have just one line change.",
                    "label": 0
                },
                {
                    "sent": "You need to do a few more lines because using GPU is still slightly more complicated.",
                    "label": 0
                },
                {
                    "sent": "So this is a simple and effective way, but we are assuming that use shared memory or multicore.",
                    "label": 0
                },
                {
                    "sent": "So you still use just one big machine.",
                    "label": 0
                },
                {
                    "sent": "And that we so, Sir.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An amount of RAM, but what if your data just cannot fit into memory?",
                    "label": 1
                },
                {
                    "sent": "Then you have to use distributed environments.",
                    "label": 0
                },
                {
                    "sent": "So people have tried that.",
                    "label": 0
                },
                {
                    "sent": "So here are some examples.",
                    "label": 0
                },
                {
                    "sent": "So there is a software called.",
                    "label": 0
                },
                {
                    "sent": "SVM.",
                    "label": 0
                },
                {
                    "sent": "This is actually this actually was done at Google Research in Beijing, so they have a code here here they do incomplete Scholastica factorization of the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "Buy buy a pair away then then they use an interior point method to do the minimization.",
                    "label": 0
                },
                {
                    "sent": "There's another software package called Pius via I think this one directly paralyze device via.",
                    "label": 0
                },
                {
                    "sent": "There is another package called.",
                    "label": 0
                },
                {
                    "sent": "GG PDT.",
                    "label": 0
                },
                {
                    "sent": "This one is also a decomposition method.",
                    "label": 0
                },
                {
                    "sent": "They all use MPI, four layer parallelization and they do report good speedup.",
                    "label": 1
                },
                {
                    "sent": "But however you must be a little bit careful on the environment Lee they used.",
                    "label": 0
                },
                {
                    "sent": "So for example, like I think one of my my friends tried this and he said that on a shared memory environment will be this one.",
                    "label": 0
                },
                {
                    "sent": "Also works on both on distributed and shared memory architectures and unleash shared memory machine.",
                    "label": 0
                },
                {
                    "sent": "Then the speedup is quite good.",
                    "label": 0
                },
                {
                    "sent": "However, if you move to a distributed environment then the speed up is not so good.",
                    "label": 0
                },
                {
                    "sent": "So actually communication costs easier concern.",
                    "label": 0
                },
                {
                    "sent": "I mean, once you use more nodes than the communication cost may quickly increase so.",
                    "label": 0
                },
                {
                    "sent": "Indeed, I think there are still quite a few difficulties here if you want to parallelize SVM on a distributed environment.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "But instead of I mean so.",
                    "label": 0
                },
                {
                    "sent": "So I just say that without changing changing any algorithm, you just do parallelization.",
                    "label": 0
                },
                {
                    "sent": "But instead of that we can we say OK, how about like we just give up solving the SVM dual optimization problem?",
                    "label": 0
                },
                {
                    "sent": "So how to do that?",
                    "label": 0
                },
                {
                    "sent": "Oh, so the idea is to just approximately solve the SVM quadratic programming problem.",
                    "label": 0
                },
                {
                    "sent": "Well, you can think about many simple methods.",
                    "label": 0
                },
                {
                    "sent": "The easiest one will be subsampling.",
                    "label": 0
                },
                {
                    "sent": "And actually subsampling is is very simple and often effective.",
                    "label": 1
                },
                {
                    "sent": "So wait, I always tell people that if you have a data and you find that it is too large, then try this first.",
                    "label": 1
                },
                {
                    "sent": "But from subsampling then there are many more advanced techniques.",
                    "label": 0
                },
                {
                    "sent": "Well, Francine from some simple ones to some complicated ones.",
                    "label": 0
                },
                {
                    "sent": "So here I give a simple example where this was back to the early days of SVM development.",
                    "label": 0
                },
                {
                    "sent": "It's right time later say OK. How about do a simple split of data to 10 parts?",
                    "label": 0
                },
                {
                    "sent": "Is you know that support vectors are a small portion of the training data, but so it's difficult to train the whole set.",
                    "label": 0
                },
                {
                    "sent": "You just train the first part, you get some support vectors, then you add the load support vectors to the second part, and then you train that so so then you are able to get a solution.",
                    "label": 1
                },
                {
                    "sent": "And for example, you can say OK, Now the number of data is too large.",
                    "label": 0
                },
                {
                    "sent": "Then try to select some good points.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are many other such techniques.",
                    "label": 0
                },
                {
                    "sent": "This is an incomplete classification, so the idea is to basically approximate the kernel.",
                    "label": 0
                },
                {
                    "sent": "This is a nice strong approximation.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's again another way to approximate the kernel and this is similar.",
                    "label": 0
                },
                {
                    "sent": "So instead of really approximating the kernel you just use part of the kernel is like.",
                    "label": 1
                },
                {
                    "sent": "This technique is called reduced support vector machines.",
                    "label": 0
                },
                {
                    "sent": "The basic idea is that you don't use the whole kernel matrix uses several columns.",
                    "label": 0
                },
                {
                    "sent": "And you can do more like greedy ways.",
                    "label": 0
                },
                {
                    "sent": "Actually, there are quite a few greedy implementations, so there are many approximation techniques.",
                    "label": 1
                },
                {
                    "sent": "Some are simple but some.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very complicated, so then you say, should we do parallelization or approximation?",
                    "label": 1
                },
                {
                    "sent": "Will essentially this confused here is when I civilization I mean that we still try to.",
                    "label": 1
                },
                {
                    "sent": "Somehow accurately solve the SVM dual quadratic problem, but it's difficult to say well for four.",
                    "label": 0
                },
                {
                    "sent": "Approaches there are more general, so you don't worry about different situations, but approximation can be simpler in some situations, for example, like if subsampling does work.",
                    "label": 0
                },
                {
                    "sent": "But we can also do both.",
                    "label": 1
                },
                {
                    "sent": "You can you can do approximation inipi realize it that's also possible.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but for set up.",
                    "label": 0
                },
                {
                    "sent": "I think there is still a need for parallelization because for certain problem.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Somehow a process approximation does not easily work, so let's try to have a comparison here.",
                    "label": 0
                },
                {
                    "sent": "Let's try to datasets.",
                    "label": 0
                },
                {
                    "sent": "Well, layer about health million.",
                    "label": 0
                },
                {
                    "sent": "Not particularly large, but like this data set is already one of the largest in UCM machine learning repository.",
                    "label": 0
                },
                {
                    "sent": "So if we do subsampling, then if you just use 110th of the data then the testing this is testing accuracy is just 92.5, so that's 6% different from using the whole set.",
                    "label": 0
                },
                {
                    "sent": "But for this another data set, the size is about similar, but if you use 110th of the data then the testing accuracy is already close to two lot of using the whole set.",
                    "label": 0
                },
                {
                    "sent": "So for this data set, clearly using subsampling, easier way to do.",
                    "label": 0
                },
                {
                    "sent": "Then, but this was not only subsampling, doesn't quite work, of course I.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Tried some approximation techniques like close things actually is also more difficult.",
                    "label": 0
                },
                {
                    "sent": "I mean relatively more diff.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then if you try those approximation techniques on this data set, yeah, so.",
                    "label": 0
                },
                {
                    "sent": "So it seems we are in a situation that if you have very large datasets that select some suitable approach for that particular problem is quite essential.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so I will quickly illustrate this point using linear SVM for document classification to show that for certain problems then.",
                    "label": 1
                },
                {
                    "sent": "You can.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Gate very quick, quick training implementation.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now let's move to linear SVM.",
                    "label": 1
                },
                {
                    "sent": "Well, we know linear means we don't make the data to the higher dimensional space, but in theory we can prove that if you use RBF kernel with parameter selection then.",
                    "label": 0
                },
                {
                    "sent": "Leave one out on accuracy of using RBF kernel is at least as good as using linear kernel.",
                    "label": 0
                },
                {
                    "sent": "So so from this statement you should say OK, we should never use linear support vector machines, right?",
                    "label": 1
                },
                {
                    "sent": "We should just use nonlinear.",
                    "label": 0
                },
                {
                    "sent": "We sell a Gaussian kernel, yeah, but the difference is that sometimes you can easily solve much larger linear SVM's OK and the accuracy maybe sometimes similar or even just slightly lower.",
                    "label": 1
                },
                {
                    "sent": "So how do I do think that?",
                    "label": 0
                },
                {
                    "sent": "Is that training of linear and nonlinear SVM's should be separately considered.",
                    "label": 1
                },
                {
                    "sent": "So in other words.",
                    "label": 0
                },
                {
                    "sent": "If you are designing a linear SVM method, you may not want or you should not compare it with something designed for general knowledge.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Nearest via.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so we know linear speed is going to be useful.",
                    "label": 0
                },
                {
                    "sent": "It's accuracy is similar and you can train a larger larger set.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So I want to talk about the the situation that if your number of features is large for document classification, use a bag of words model.",
                    "label": 1
                },
                {
                    "sent": "Then you have a large number of features.",
                    "label": 1
                },
                {
                    "sent": "So recently this is an active research topic, so there are quite a few new soft.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're taking ages.",
                    "label": 0
                },
                {
                    "sent": "So let's rewrite the.",
                    "label": 0
                },
                {
                    "sent": "Lesbian formulation again, but using linear well, so this is Primal deal where the difference is I don't have the bias term B, but somehow I just admit it for simplification so I don't get the linear constraint here.",
                    "label": 1
                },
                {
                    "sent": "Yeah, I only have bounded constraints, so that Alpha is between zero and the sea.",
                    "label": 0
                },
                {
                    "sent": "Because I use linear, so this the square matrix Q, the edge component is just directly the inner product.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And and I'm going to say to show you an approach that for such data, yeah, say, like this document, data code, MCV.",
                    "label": 0
                },
                {
                    "sent": "One, if you use liberal Lib SVM by the linear kernel then it may take you 10 hours to train a data set.",
                    "label": 1
                },
                {
                    "sent": "But by some other more suitable methods specially designed for such for document data.",
                    "label": 1
                },
                {
                    "sent": "Then you can finish the computation in less than 5 seconds.",
                    "label": 0
                },
                {
                    "sent": "And in fact, the difficulty becomes on the eye of the data set.",
                    "label": 0
                },
                {
                    "sent": "It takes more time on reading the data set into the computer memory.",
                    "label": 1
                },
                {
                    "sent": "And here I'm using the same stopping condition, so this comparison is actually fair and under the accuracy for this data set is a similar tool too if you use.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nina so how?",
                    "label": 0
                },
                {
                    "sent": "How to achieve that?",
                    "label": 0
                },
                {
                    "sent": "We still want to use decomposition methods but just use it in a different way.",
                    "label": 0
                },
                {
                    "sent": "So now we revisit the decomposition methods that I mentioned earlier.",
                    "label": 1
                },
                {
                    "sent": "But now I want to take an extreme situation to update just one variable at a time.",
                    "label": 1
                },
                {
                    "sent": "So you have a one variable optimization problem to solve.",
                    "label": 0
                },
                {
                    "sent": "Then you get an update rule like this.",
                    "label": 0
                },
                {
                    "sent": "So from if if the ice component is selected to be to be changed, then we will.",
                    "label": 0
                },
                {
                    "sent": "So we do some modification of Alpha and if it is, it is not in the in the in the interval of zero in the sea we map it back and that's our new Alpha.",
                    "label": 0
                },
                {
                    "sent": "And here this gradient, the ice component of F is is the gradient of the deal.",
                    "label": 0
                },
                {
                    "sent": "So, so it is actually the the ice component of cuaba minus one.",
                    "label": 0
                },
                {
                    "sent": "So if we go back to the nonlinear situation, well, yeah yeah, remember that for nonlinear SVM.",
                    "label": 0
                },
                {
                    "sent": "Up, once we select a subset then we need so many columns of the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "Now we need 1 one column of the kernel matrix.",
                    "label": 0
                },
                {
                    "sent": "So if N is the number of features and there is a number of data, then you need so so many operations to calculate.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "One row of the matrix, but for linear SVM will you don't need to do that because you don't make the data, so you have X.",
                    "label": 1
                },
                {
                    "sent": "So you can define a vector W. Then by using this W to calculate the ice component of the gradient, you just do an inner product.",
                    "label": 0
                },
                {
                    "sent": "So let's only older and so then the question is how to maintain W?",
                    "label": 1
                },
                {
                    "sent": "Well, now you just change one component of Alpha.",
                    "label": 0
                },
                {
                    "sent": "So if you have your original original W then only you just you just check the change of Alpha and the times that vector.",
                    "label": 0
                },
                {
                    "sent": "So if the vector has length and so this is still an older man.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Operation.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So well, if you were in my students talk yesterday, then there are some other implementation details.",
                    "label": 1
                },
                {
                    "sent": "But the point here is we're still using decomposition methods, but the key is that.",
                    "label": 0
                },
                {
                    "sent": "For general case linear or nonlinear, our cluster is actually older NLP iteration, but by this new way actually you have older emperor iteration.",
                    "label": 0
                },
                {
                    "sent": "OK, but but there are some other differences is about how we select the working set.",
                    "label": 0
                },
                {
                    "sent": "For for general nonlinear case, people pay more attention on selecting the working set.",
                    "label": 0
                },
                {
                    "sent": "The reason is that I mean you already.",
                    "label": 0
                },
                {
                    "sent": "You need so much efforts in order to do the update, and then we find out that by such such efforts you can maintain a gradient so people try to use more information on selecting the working set.",
                    "label": 0
                },
                {
                    "sent": "But here essentially we give up on doing that.",
                    "label": 0
                },
                {
                    "sent": "We don't really do working set selection.",
                    "label": 0
                },
                {
                    "sent": "So it but, but because this is a huge reduction, if the number of iterations OK, even this one, the number of iterations maybe more.",
                    "label": 1
                },
                {
                    "sent": "But if it is not at all times more than your total computational time is still less.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but by such methods of course have is only suitable for at least so far for document data it has certain limitations that it say.",
                    "label": 0
                },
                {
                    "sent": "For example, if you have a large penalty parameter C or if you're a number of features is small, then it's not that Latif.",
                    "label": 1
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Active So what I want to say here is you must be careful on comparisons.",
                    "label": 1
                },
                {
                    "sent": "You essentially now I have two decomposition methods, one for general nonlinear case but another for for linear.",
                    "label": 1
                },
                {
                    "sent": "Yeah, and if you do theoretical analysis, I think they have similar convergence rates, but they have very different practical behaviors for certain problems.",
                    "label": 1
                },
                {
                    "sent": "And here we can see why.",
                    "label": 0
                },
                {
                    "sent": "Why sometimes controversial comparisons happen.",
                    "label": 0
                },
                {
                    "sent": "So this morning, sorry given given examples.",
                    "label": 0
                },
                {
                    "sent": "So in your paper, people say SVM perviz faster than SVM light, but then another one, say, well, SVM light is faster than SVM curve.",
                    "label": 0
                },
                {
                    "sent": "Well, here I can give you another example so.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just show that.",
                    "label": 0
                },
                {
                    "sent": "This new method for linear is actually less effective if you if you have number of features, small or large penalty parameter.",
                    "label": 1
                },
                {
                    "sent": "So it is even possible that if you are not doing document classification, OK, So what I have shown is that if you do document classification then this new method is actually faster than the original 1.",
                    "label": 0
                },
                {
                    "sent": "So I just say that Liblinear is faster then leave SVN, but if you go to try such datasets where it is very possible that maybe.",
                    "label": 0
                },
                {
                    "sent": "The basement becomes faster then leave linear, so so you see different datasets.",
                    "label": 0
                },
                {
                    "sent": "Then you can.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "They are very different.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Situations.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so so seems like that if you want to handle very large datasets you want to have different training strategies.",
                    "label": 1
                },
                {
                    "sent": "So even just for linear SVM.",
                    "label": 1
                },
                {
                    "sent": "Will for linear SVM you can roughly consider three situations.",
                    "label": 0
                },
                {
                    "sent": "So why is that?",
                    "label": 0
                },
                {
                    "sent": "If your number of data is much larger than number of features?",
                    "label": 0
                },
                {
                    "sent": "And this is the other way around.",
                    "label": 1
                },
                {
                    "sent": "And another situation that I just talked about is that both are very large.",
                    "label": 0
                },
                {
                    "sent": "Well, actually, if you use different methods then you can get very fast methods.",
                    "label": 0
                },
                {
                    "sent": "So for example like this one, if you're a number of data is much larger than number of features, you really should use a primal based methods.",
                    "label": 0
                },
                {
                    "sent": "But on the other hand this is also the case that you should use nonlinear right now if you have a small number of features, you should make data to higher dimensional space.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So, so let's go back to the issue of linear versus nonlinear.",
                    "label": 1
                },
                {
                    "sent": "Using this competition, we find out that a lot of people they use Nunley and they use linear well.",
                    "label": 0
                },
                {
                    "sent": "I'm not talking about linear trick, I mean for the wild trick people use linear, so while it happen.",
                    "label": 0
                },
                {
                    "sent": "We usually think that for certain problems use nonlinear.",
                    "label": 0
                },
                {
                    "sent": "Then you should get better accuracy.",
                    "label": 0
                },
                {
                    "sent": "Yeah, but somehow that happened.",
                    "label": 1
                },
                {
                    "sent": "I mean for for for people ages will they do supply linear to the whole older 10 problems even they know that the accuracy may be worse?",
                    "label": 1
                },
                {
                    "sent": "So earlier I just mentioned that we can either still try to accurately solve the dual problem SVM problem or to do an approximation will in a sense actually linear is essentially an approximation technique.",
                    "label": 0
                },
                {
                    "sent": "Of non linear SVM so well, if I'm a participant over the competition for this wild trick and you'll find out there are so many huge datasets and I find out that it is difficult to analyze which one is more suitable for nonlinear.",
                    "label": 0
                },
                {
                    "sent": "But I do know that if I just apply suitable linear SVM techniques to all of them and I am able to.",
                    "label": 1
                },
                {
                    "sent": "To get some solutions then maybe I made such decision, yeah?",
                    "label": 0
                },
                {
                    "sent": "So I think that's one reason maybe.",
                    "label": 0
                },
                {
                    "sent": "Why in this competition, so many use linear.",
                    "label": 1
                },
                {
                    "sent": "So I've said several times that seems for large datasets, selecting a right approach is essential, but.",
                    "label": 1
                },
                {
                    "sent": "Doing that is difficult for just the typical user.",
                    "label": 0
                },
                {
                    "sent": "How can they do that now they cannot.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Yeah, so this is this problem.",
                    "label": 0
                },
                {
                    "sent": "I mean there are too many approaches is indeed very bad from the viewpoint of designing machine learning software packages because we must remember that negative success of software like Live SVM or SVM light is because they are simple and general.",
                    "label": 1
                },
                {
                    "sent": "So people just don't worry about I should use linear or nonlinear or something else, we just run it.",
                    "label": 1
                },
                {
                    "sent": "So seems that we need developments in both directions.",
                    "label": 0
                },
                {
                    "sent": "So here from Ice Age general is something like you do GPU computing.",
                    "label": 0
                },
                {
                    "sent": "Hurry up the parallel stuff, but specific, maybe more related to approximation where we should do both directions.",
                    "label": 1
                },
                {
                    "sent": "Then hopefully that will help to advance this.",
                    "label": 0
                },
                {
                    "sent": "Type this research topic.",
                    "label": 0
                },
                {
                    "sent": "So thank you very much.",
                    "label": 0
                },
                {
                    "sent": "2 one question.",
                    "label": 0
                },
                {
                    "sent": "About properties of the problems that make one method better than another method.",
                    "label": 0
                },
                {
                    "sent": "For example, larger number of training examples and features and and stuff like that.",
                    "label": 0
                },
                {
                    "sent": "So this types of properties are.",
                    "label": 0
                },
                {
                    "sent": "Can you basically yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah yeah yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "There are more.",
                    "label": 0
                },
                {
                    "sent": "Property yeah.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah yeah yeah.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, exactly.",
                    "label": 0
                },
                {
                    "sent": "I mean, every time when I say this statement, people ask the same question.",
                    "label": 0
                },
                {
                    "sent": "So say, is there any intelligent away?",
                    "label": 0
                },
                {
                    "sent": "'cause if you get the data then.",
                    "label": 0
                },
                {
                    "sent": "Then maybe there's a script doing some analysis to give you some recommendations.",
                    "label": 0
                },
                {
                    "sent": "Will let something I'm still thinking.",
                    "label": 0
                },
                {
                    "sent": "Yeah, I do have some guidelines.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So for example like if I'm doing linear SVM and for each situation.",
                    "label": 1
                },
                {
                    "sent": "I really know what you should do, but for nonlinear is more difficult.",
                    "label": 0
                },
                {
                    "sent": "Yes.",
                    "label": 0
                },
                {
                    "sent": "Overall, I think this is still very challenging issue.",
                    "label": 0
                },
                {
                    "sent": "I actually.",
                    "label": 0
                },
                {
                    "sent": "This methylamine problem has been tackled also in the constraints of this faction community, and actually the first step towards solving this problem is trying to find some older para meters.",
                    "label": 0
                },
                {
                    "sent": "I made this so called constraint density and constraint tightness and I feel that we are not yet at this level of sophistication.",
                    "label": 0
                },
                {
                    "sent": "I mean, having good, not.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hello all the parameters and that's really one of the goals of the of the large shapes challenge was to find weather with the synthetic datasets.",
                    "label": 0
                },
                {
                    "sent": "We could explore some other apartments as an income coasters for difficulty or whatever.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, so we still have some something to do.",
                    "label": 0
                },
                {
                    "sent": "Kind of all the parameters that we might want to explore when generating new work.",
                    "label": 0
                },
                {
                    "sent": "Synthetic datasets would be just great.",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah.",
                    "label": 0
                },
                {
                    "sent": "Well, generating data well I'm not good at such things here.",
                    "label": 0
                },
                {
                    "sent": "If you have some feelings about what could make some problems more difficult than others, oh Oh yeah.",
                    "label": 0
                },
                {
                    "sent": "Well I think for me Linea.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The linear situation is quite clear that usually separate linear SVM tools, three situations and issues quite very different methods.",
                    "label": 1
                },
                {
                    "sent": "But for nonlinear will.",
                    "label": 0
                },
                {
                    "sent": "Yeah, well, I think instead of doing using artificial data which tries to collect more real data, don't you think that's that's more realistic?",
                    "label": 0
                },
                {
                    "sent": "History, I mean people were trying to tackle this meta learning problem based on the UCI.",
                    "label": 0
                },
                {
                    "sent": "So happens, oh OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "It's great but but you see the Legends, Disease or Overman.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, maybe yeah yeah.",
                    "label": 0
                },
                {
                    "sent": "Well yeah we can do that.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, sure.",
                    "label": 0
                }
            ]
        }
    }
}