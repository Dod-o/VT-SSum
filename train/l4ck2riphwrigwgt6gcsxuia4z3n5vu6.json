{
    "id": "l4ck2riphwrigwgt6gcsxuia4z3n5vu6",
    "title": "Unbiased Offline Evaluation of Contextual-bandit-based News Article Recommendation Algorithms",
    "info": {
        "author": [
            "Lihong Li, Microsoft Research"
        ],
        "published": "Aug. 9, 2011",
        "recorded": "February 2011",
        "category": [
            "Top->Computer Science->Web Mining"
        ]
    },
    "url": "http://videolectures.net/wsdm2011_li_uoe/",
    "segmentation": [
        [
            "Thanks Jeremy for introduction.",
            "So my name is Lyons.",
            "This is joint work with my colleagues that Yahoo Way true John M for an Jennifer Wong.",
            "So first of all I apologize for the long very long title of this paper.",
            "It was we chose this long title to emphasize the application that we focus on this paper.",
            "However, the technique that we develop in investigating this paper actually applies to a lot of applications beyond news.",
            "Article recommendation.",
            "So."
        ],
        [
            "Let me start with the.",
            "A big picture of the Yahoo user interaction, so user constantly interact with Yahoo in in a certain way.",
            "So for every user visit Yahoo can achieve can retrieve the information of the user like John, Age, gender control occasion etc.",
            "And basically based on this user information at Yahoo has some serving policy to decide what advertisement is, display what news article to recommend or ranking function to working results to display OK. And based on this, yeah this contents recommended by Yahoo.",
            "The user may provide feedbacks in the form of either clicks conversion or revenue etc to Yahoo for Yahoo to improve this serving policy to benefit the next user.",
            "So this is a loop between Yahoo and user and this process is interactive.",
            "An hour contribution is paper.",
            "Yeah, so yeah.",
            "So in this work we called this user information as context and.",
            "The recommendation by Yahoo is action and the user feedback is some sort of numerical signal called reward, and the starting policy is called policy here.",
            "OK, so the contribution of this work is an unbiased offline evaluation for this kind of interactive process.",
            "So the three keywords unbiased, offline, interactive which are explained with more details later."
        ],
        [
            "OK, so here's an outline.",
            "This work, we focus on news article recommendation.",
            "We formulate that problem is a contextual bandit problem an and then I'll explain why this why this problem is difficult.",
            "Wide offline evaluation is difficult, and then I'll describe our vice offline evaluation method, followed by some case studies in this Yahoo Front page news recommendation."
        ],
        [
            "Problem.",
            "So this is a Yahoo front page.",
            "There are a lot of stars and on this page and one of the most prominent module here is this module called today module.",
            "OK, so let me zoom in and have a.",
            "Better look at it.",
            "So in this big module, if you can look at this, there's a number here, so this number is tells you that there is a small pool of article chosen by human editors that Yahoo can choose to recommend to a user when he visit Yahoo Front page.",
            "OK, so these articles are high quality articles and this big box is what we call the feature article.",
            "So this article is the most prominent or the part in this module that receives most attention.",
            "And therefore is Yahoo.",
            "We tried to recommend the most interesting news article to a user based on his interest and previous browsing history."
        ],
        [
            "So overall, in general, the goal of this kind of product is to maximize the number of clicks.",
            "At least one of the goals at least.",
            "One way to formulate this to maximize the number of clicks or city are in this module.",
            "However, in this kind of problem, we only receive user click feedback for the articles that we display in this feature articles spot.",
            "So there's two conflicting goals here.",
            "One is called exploit where you want to choose good articles to serve the user better to interest the user, and to do that, we need to have some sort of good estimate of the articles CTR.",
            "But in order to get a good city R, we actually need to explore, which is to choose novel articles to collect user feedback, which are our training data to refine the CR estate model so that in the future we can have a better exploit phase.",
            "So this too.",
            "Are ghosts are sometimes conflicting and there is a need for a good tradeoff between these two goals, and the same thing happens in advertisement.",
            "An also surgeon, a lot of application important applique."
        ],
        [
            "Things on the web.",
            "So here this is a formal formulation of this kind of problem that we call contextual bandit problem.",
            "So this is a iterative process.",
            "User comes in one by one, so by X we mean that any feature that we can capture about a user and then we have K arms or K choices, for example K articles that we can display to the user, and then we can pick one of the articles and displays the user and see how the user.",
            "Reacts to the article.",
            "If so, and then this process repeats.",
            "And the long term, the goal is to maximize the sum of rewards that we collect from the user like clicks or revenue.",
            "Are the key challenge and contextual bandit?",
            "Is that the reward signal is only received for the arms?",
            "That says that we choose, not for other ways that we don't?",
            "We didn't choose, and therefore that's why the exploitation exploitation dilemma comes in.",
            "So in our example of today module.",
            "So here are how we interpret the symbols.",
            "So the set of articles, the set of arms is the set of available articles in today module Annex the user feature A is the.",
            "Article that we chose to display in this feature article spot, and then if we are interested in maximizing, we click on a number of clicks.",
            "Then we can define our to be one where user clicks on the article in 0 where the user doesn't."
        ],
        [
            "So, so we have talk about how to define this problem.",
            "This kind of interactive problem is contextual bandit problem, and then I will explain why offline evaluation is difficult an."
        ],
        [
            "Swipe our solution to this problem.",
            "So what I mean by estimating a bandit algorithm in this kind of problem is that suppose I have a policy or serving policy Pi which Maps user information to the article or any arm that you want to choose, and then suppose that user comes in in following some certain distribution and then I want to maximize the average reward per user.",
            "So this is R is the reward function that depends on the user.",
            "And the selected arm.",
            "An offline evaluation is useful for this kind of problem because it's cheap.",
            "Is risk free.",
            "You don't have to implement algorithm on in the real system, which can be risky to user experience or other reason an and then you can use this kind of offline invitation to have some idea of how well the algorithm is and avoid bucket testing and promising candidates.",
            "And finally, if you can do use data to do offline evaluation, then the results are replicable and you can have.",
            "A lot of Fair comparisons between algorithms."
        ],
        [
            "And this kind of offline evaluation is common in non interactive problems.",
            "For example, in supervised learning there are lots of benchmark data that are organized in the form of input and output pairs.",
            "But in the interaction problem here evaluation is not straightforward because of the actions that are involved.",
            "So data in the bandits problems are usually in this form context X user information.",
            "The arm that we displayed to user in the history and the reward signal the research.",
            "Provided to us in history, however, this data doesn't provide reward signal for the arms that we didn't choose to the user.",
            "So when you use the data to evaluate an algorithm, a new algorithm of flying, and when the new algorithm does not choose the same arm as in the data, then you don't have the reward signal to evaluate the algorithm.",
            "And a common practice, or a very straightforward solution is that is as follows, since we don't know, we don't always have reward signal the data, so one solution will be estimated reward.",
            "So this kind of algorithm works this way.",
            "Let's say we have data organized in this form, a set of XAR data, and then this is the algorithm that you want to estimate evaluate.",
            "And then you can use all sorts of statistics and machine learning techniques to build a reward estimator that's similar to what the user would have done if I have.",
            "If I chose the action A to the user in the history and then I trust this reward estimator and use and compare my algorithm against this year simulator OK, However, the limitation of this approach is that the first step, the estimation step, it's very difficult and is often biased and consequently.",
            "The evaluation result of your pain in the second state is also unreliable, and in contrast, our solution avoids explicit model building, which is simpler and also more importantly, it gives unbiased evaluation result, which means that we can get reliable numbers."
        ],
        [
            "So here's how it works.",
            "The idea is based on the.",
            "Replay so I just want to remind that mind you, that this is the quantity that would try to estimate given the policy Pi that selects an ARM action a based on user interest based on user feature and then we try to estimate on average was the reward that I can observe.",
            "I can receive by following this policy per user visit an again data in this form and we try to estimate the value of this algorithm.",
            "So a key requirement or key assumption about this data.",
            "Here we need for this work is that the arms A in this data are chosen randomly in history.",
            "So what I'm So what it means is that in the Today module, for instance, we can assign a small fraction of use of visits into a random bucket and for user visiting that random bucket we can display a random action or random articles to the user and see how the user reacts.",
            "And then for this kind of data we can use the.",
            "Evaluation processes follows, so I go through all the data one by one.",
            "OK, an AI first reveals the user information to the algorithm and the algorithm will recommend an action a based on the policy \u03c0, and then I'll see whether this a hat is the same as the A in the log data.",
            "OK, when is the same?",
            "Then I call it a match and then I use the reward to evaluate the algorithm with return reward to the algorithm to allow it to update this model.",
            "OK, and if it doesn't match then I ignore that data point and pretend it doesn't exist anymore and don't use it.",
            "And at the end the output or the estimate we had is the sum of the masked rewards or the some rewards in the matched data points multiplied by a normalization factor to account for this.",
            "It's matching that description OK. OK, and so this is the final output.",
            "We had to estimate this quantity that we're interested in.",
            "So now the question is, how does we had related."
        ],
        [
            "To the true quantitative pie.",
            "So we have some theoretical guarantees.",
            "The first one is that the estimator is unbiased, and formally it means that on average the expectation of the estimate is the same as the quantity that you want to estimate, even though you don't have the choice, you don't have the power to run the algorithm in the real system, so.",
            "This is a nice property.",
            "Anne II theoretical result is that the estimation error goes down to zero when you have more and more data, so the error estimation error here is the absolute difference between the V high and we had an.",
            "If you have our data and K candidates per per step then on then the error case to zero in this rate.",
            "So suppose you have Alvin 1,000,000, then one square root one over will be on the other point.",
            "01 K so when you have more and more data than you can expect greater and greater."
        ],
        [
            "Accuracy.",
            "OK, so now let me show you some exciting results based on today module data."
        ],
        [
            "At Yahoo Front page, so we have a lot of real user traffic in this module and then we've tested a number of policies including on personalized recommendation policy, which is a spatial temporal model that Gravel ET al developed in 2009 and also some personalized improvement variants of the NPR policy.",
            "And then since this, these policies are implemented in the real system, then we can just collect the bucket data and then count.",
            "Online performance or online CPR?",
            "In this case for those policies and treat them as the truth.",
            "OK, and then we also have some random bucket data and we use the data to run the offline evaluation method for those policies and then we get some offline estimates.",
            "Now we have two numbers for each policy.",
            "One is the online online bucket data data traders truth.",
            "The artist offline estimate.",
            "Now the goal of this experiment is to see whether these two numbers are closed.",
            "They closed.",
            "It means that the offline evaluation method is reliable."
        ],
        [
            "So in the first plot I'm showing you some articles, City are so the city are normalized here.",
            "So I'm showing you some city up, lots of in the personalized policy.",
            "So on the X axis is the online city of each particle and the Y axis is the offline estimate of the article, and each point corresponds to one article.",
            "So if so, if the offline estimate is precise, then we expect that all points lie on this line y = X.",
            "But as you can see, all the points are live very close to this line, meaning that the offline in violation without is very close to the online number.",
            "So we can see that the offline estimators and."
        ],
        [
            "Unbiased.",
            "And the second plot.",
            "We show that the Daily City are so here the X axis means that the three 10 days and then for each day we compute the aggregate city are in the bucket OK, and then the red line.",
            "Here is the online CPR is gonna see there's some variation.",
            "Daily variation of CR and the Black Line is the offline estimate of corresponding ACR, and there's a strong correlation between these online offline numbers.",
            "So again, this corroborates the.",
            "Radical anti that the offline estimators."
        ],
        [
            "Reliable Circuit of the online number and then in the third plot I'm showing you the how the estimation error decays when you have more and more data in the random bucket.",
            "OK, so the excess is is the number of data points you have and the Y axis the estimation error, which is the absolute difference between the online number an the offline evaluation number an.",
            "An as predicted by our theorem, error decays roughly on this order.",
            "One square root 1 / L OK, so this result fits very well with our theory."
        ],
        [
            "An in the final plot, so I'm showing you that in the online system there are usually some business rules that we cannot control or we cannot replicate in the offline way.",
            "OK, so for example, human editors may have some rules that overrules the recommendation algorithms choice, and our analysis shows that even though the business rules may have changed, the online bucket results.",
            "However they change the.",
            "Online numbers in a multiplicative way, so instead of showing you the online versus online results in previous plot here I'm showing you the ratio of these two plots for different policies.",
            "So this is the EMP policy which is the.",
            "Here is the offline estimate divided by online and here I have another policy but using the same ratio of offline online to see Cdr.",
            "So if the online offline city are, is that true or is a?",
            "Reliable estimate online city are then we again we expect that all points along this line.",
            "OK, thanks.",
            "The impact of business rules as multiplicative to the online CDR's.",
            "OK, so there's a little bit more variation than in previous grass, but overall order points are still lying around this line, meaning that even in this case where our numbers are corrupted by business rule, the evaluation result can still reveal a relative performance of different policies.",
            "So which is a good thing about this method?"
        ],
        [
            "Hey so this concludes the experiment section."
        ],
        [
            "So extensions you may wonder what happens we have we don't have uniform data, so the good news is that there's still hope.",
            "OK, so when you have some sort of random data you can use importance weighting to correct the bias in the data even though it is not uniform.",
            "So there's some somewhere here, and also there's the doubly robust technique from statistics that you can employ to decrease the variance of the estimate.",
            "Which we also find it helpful in our in our own experience and 2nd.",
            "What happens if the number of arms K is too large so there are different ways to handle this problem.",
            "One way that we have tries to prefilter on promising candidates and only randomize over the promising candidates then that also finds useful and there's some open issues.",
            "One of them should be emphasized here is that.",
            "In this way, we haven't considered history to constraints on the arms, so in certain applications there are.",
            "You may have constraint dependent history like advertising and this open question how to incorporate this kind of constraints in the offline evaluation method."
        ],
        [
            "OK, so let me conclude the talk so.",
            "We have shown that the interactive machine learning is common on the web in web applications OK and then here in this work we investigate an offline evaluation method that gives you unbiased result.",
            "It also is error also decays very fast when you have more and more data and then we showed it is it gives very accurate result using data from Yahoo today module and this kind.",
            "So therefore this kind of results can be used to do offline evaluation.",
            "Of many, many important applications without pain and risk to implementing the algorithm in the real system.",
            "OK, so.",
            "This concludes the talk and thank you very much and I'm happy to take questions, thanks.",
            "Thanks.",
            "I thought that was neat, thanks.",
            "So the work that you are talking about that allows you to handle nonuniform sampling.",
            "I'm wondering if it's possible that that can be helpful to avoid the fact that you can only use a small fraction of the data that you sampled.",
            "If you have some knowledge about what you expect, the distribution of arms to look like for the algorithm, is it possible to be more efficient in the data that you sample?",
            "Yeah, that's a good.",
            "Yeah, that's a good question, so this actually relates to one of the limitations that we talked about here, when whatever the number of arms case too large.",
            "OK, so so yeah.",
            "So if you know if you have prior information that two arms give, you are similar results then you can definitely take into account that information or if possible too.",
            "To adapt the reward of 1 arm when actually in offline case deposit, choose another arm that is very similar, but it depends on the similarity metric that you use.",
            "If it's good then it's good, but.",
            "Then the general Advice County is such memory is not is lost, but maybe it's possible, but we haven't tried.",
            "Thanks."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thanks Jeremy for introduction.",
                    "label": 0
                },
                {
                    "sent": "So my name is Lyons.",
                    "label": 0
                },
                {
                    "sent": "This is joint work with my colleagues that Yahoo Way true John M for an Jennifer Wong.",
                    "label": 0
                },
                {
                    "sent": "So first of all I apologize for the long very long title of this paper.",
                    "label": 0
                },
                {
                    "sent": "It was we chose this long title to emphasize the application that we focus on this paper.",
                    "label": 0
                },
                {
                    "sent": "However, the technique that we develop in investigating this paper actually applies to a lot of applications beyond news.",
                    "label": 0
                },
                {
                    "sent": "Article recommendation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Let me start with the.",
                    "label": 0
                },
                {
                    "sent": "A big picture of the Yahoo user interaction, so user constantly interact with Yahoo in in a certain way.",
                    "label": 0
                },
                {
                    "sent": "So for every user visit Yahoo can achieve can retrieve the information of the user like John, Age, gender control occasion etc.",
                    "label": 0
                },
                {
                    "sent": "And basically based on this user information at Yahoo has some serving policy to decide what advertisement is, display what news article to recommend or ranking function to working results to display OK. And based on this, yeah this contents recommended by Yahoo.",
                    "label": 0
                },
                {
                    "sent": "The user may provide feedbacks in the form of either clicks conversion or revenue etc to Yahoo for Yahoo to improve this serving policy to benefit the next user.",
                    "label": 0
                },
                {
                    "sent": "So this is a loop between Yahoo and user and this process is interactive.",
                    "label": 0
                },
                {
                    "sent": "An hour contribution is paper.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so yeah.",
                    "label": 0
                },
                {
                    "sent": "So in this work we called this user information as context and.",
                    "label": 0
                },
                {
                    "sent": "The recommendation by Yahoo is action and the user feedback is some sort of numerical signal called reward, and the starting policy is called policy here.",
                    "label": 0
                },
                {
                    "sent": "OK, so the contribution of this work is an unbiased offline evaluation for this kind of interactive process.",
                    "label": 1
                },
                {
                    "sent": "So the three keywords unbiased, offline, interactive which are explained with more details later.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so here's an outline.",
                    "label": 0
                },
                {
                    "sent": "This work, we focus on news article recommendation.",
                    "label": 0
                },
                {
                    "sent": "We formulate that problem is a contextual bandit problem an and then I'll explain why this why this problem is difficult.",
                    "label": 1
                },
                {
                    "sent": "Wide offline evaluation is difficult, and then I'll describe our vice offline evaluation method, followed by some case studies in this Yahoo Front page news recommendation.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Problem.",
                    "label": 0
                },
                {
                    "sent": "So this is a Yahoo front page.",
                    "label": 0
                },
                {
                    "sent": "There are a lot of stars and on this page and one of the most prominent module here is this module called today module.",
                    "label": 0
                },
                {
                    "sent": "OK, so let me zoom in and have a.",
                    "label": 0
                },
                {
                    "sent": "Better look at it.",
                    "label": 0
                },
                {
                    "sent": "So in this big module, if you can look at this, there's a number here, so this number is tells you that there is a small pool of article chosen by human editors that Yahoo can choose to recommend to a user when he visit Yahoo Front page.",
                    "label": 1
                },
                {
                    "sent": "OK, so these articles are high quality articles and this big box is what we call the feature article.",
                    "label": 0
                },
                {
                    "sent": "So this article is the most prominent or the part in this module that receives most attention.",
                    "label": 0
                },
                {
                    "sent": "And therefore is Yahoo.",
                    "label": 0
                },
                {
                    "sent": "We tried to recommend the most interesting news article to a user based on his interest and previous browsing history.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So overall, in general, the goal of this kind of product is to maximize the number of clicks.",
                    "label": 1
                },
                {
                    "sent": "At least one of the goals at least.",
                    "label": 0
                },
                {
                    "sent": "One way to formulate this to maximize the number of clicks or city are in this module.",
                    "label": 1
                },
                {
                    "sent": "However, in this kind of problem, we only receive user click feedback for the articles that we display in this feature articles spot.",
                    "label": 1
                },
                {
                    "sent": "So there's two conflicting goals here.",
                    "label": 0
                },
                {
                    "sent": "One is called exploit where you want to choose good articles to serve the user better to interest the user, and to do that, we need to have some sort of good estimate of the articles CTR.",
                    "label": 0
                },
                {
                    "sent": "But in order to get a good city R, we actually need to explore, which is to choose novel articles to collect user feedback, which are our training data to refine the CR estate model so that in the future we can have a better exploit phase.",
                    "label": 0
                },
                {
                    "sent": "So this too.",
                    "label": 0
                },
                {
                    "sent": "Are ghosts are sometimes conflicting and there is a need for a good tradeoff between these two goals, and the same thing happens in advertisement.",
                    "label": 0
                },
                {
                    "sent": "An also surgeon, a lot of application important applique.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things on the web.",
                    "label": 0
                },
                {
                    "sent": "So here this is a formal formulation of this kind of problem that we call contextual bandit problem.",
                    "label": 1
                },
                {
                    "sent": "So this is a iterative process.",
                    "label": 0
                },
                {
                    "sent": "User comes in one by one, so by X we mean that any feature that we can capture about a user and then we have K arms or K choices, for example K articles that we can display to the user, and then we can pick one of the articles and displays the user and see how the user.",
                    "label": 0
                },
                {
                    "sent": "Reacts to the article.",
                    "label": 0
                },
                {
                    "sent": "If so, and then this process repeats.",
                    "label": 0
                },
                {
                    "sent": "And the long term, the goal is to maximize the sum of rewards that we collect from the user like clicks or revenue.",
                    "label": 0
                },
                {
                    "sent": "Are the key challenge and contextual bandit?",
                    "label": 1
                },
                {
                    "sent": "Is that the reward signal is only received for the arms?",
                    "label": 0
                },
                {
                    "sent": "That says that we choose, not for other ways that we don't?",
                    "label": 0
                },
                {
                    "sent": "We didn't choose, and therefore that's why the exploitation exploitation dilemma comes in.",
                    "label": 0
                },
                {
                    "sent": "So in our example of today module.",
                    "label": 0
                },
                {
                    "sent": "So here are how we interpret the symbols.",
                    "label": 0
                },
                {
                    "sent": "So the set of articles, the set of arms is the set of available articles in today module Annex the user feature A is the.",
                    "label": 1
                },
                {
                    "sent": "Article that we chose to display in this feature article spot, and then if we are interested in maximizing, we click on a number of clicks.",
                    "label": 0
                },
                {
                    "sent": "Then we can define our to be one where user clicks on the article in 0 where the user doesn't.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So, so we have talk about how to define this problem.",
                    "label": 0
                },
                {
                    "sent": "This kind of interactive problem is contextual bandit problem, and then I will explain why offline evaluation is difficult an.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Swipe our solution to this problem.",
                    "label": 0
                },
                {
                    "sent": "So what I mean by estimating a bandit algorithm in this kind of problem is that suppose I have a policy or serving policy Pi which Maps user information to the article or any arm that you want to choose, and then suppose that user comes in in following some certain distribution and then I want to maximize the average reward per user.",
                    "label": 0
                },
                {
                    "sent": "So this is R is the reward function that depends on the user.",
                    "label": 0
                },
                {
                    "sent": "And the selected arm.",
                    "label": 0
                },
                {
                    "sent": "An offline evaluation is useful for this kind of problem because it's cheap.",
                    "label": 0
                },
                {
                    "sent": "Is risk free.",
                    "label": 0
                },
                {
                    "sent": "You don't have to implement algorithm on in the real system, which can be risky to user experience or other reason an and then you can use this kind of offline invitation to have some idea of how well the algorithm is and avoid bucket testing and promising candidates.",
                    "label": 0
                },
                {
                    "sent": "And finally, if you can do use data to do offline evaluation, then the results are replicable and you can have.",
                    "label": 0
                },
                {
                    "sent": "A lot of Fair comparisons between algorithms.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And this kind of offline evaluation is common in non interactive problems.",
                    "label": 0
                },
                {
                    "sent": "For example, in supervised learning there are lots of benchmark data that are organized in the form of input and output pairs.",
                    "label": 0
                },
                {
                    "sent": "But in the interaction problem here evaluation is not straightforward because of the actions that are involved.",
                    "label": 0
                },
                {
                    "sent": "So data in the bandits problems are usually in this form context X user information.",
                    "label": 0
                },
                {
                    "sent": "The arm that we displayed to user in the history and the reward signal the research.",
                    "label": 0
                },
                {
                    "sent": "Provided to us in history, however, this data doesn't provide reward signal for the arms that we didn't choose to the user.",
                    "label": 0
                },
                {
                    "sent": "So when you use the data to evaluate an algorithm, a new algorithm of flying, and when the new algorithm does not choose the same arm as in the data, then you don't have the reward signal to evaluate the algorithm.",
                    "label": 0
                },
                {
                    "sent": "And a common practice, or a very straightforward solution is that is as follows, since we don't know, we don't always have reward signal the data, so one solution will be estimated reward.",
                    "label": 0
                },
                {
                    "sent": "So this kind of algorithm works this way.",
                    "label": 0
                },
                {
                    "sent": "Let's say we have data organized in this form, a set of XAR data, and then this is the algorithm that you want to estimate evaluate.",
                    "label": 0
                },
                {
                    "sent": "And then you can use all sorts of statistics and machine learning techniques to build a reward estimator that's similar to what the user would have done if I have.",
                    "label": 0
                },
                {
                    "sent": "If I chose the action A to the user in the history and then I trust this reward estimator and use and compare my algorithm against this year simulator OK, However, the limitation of this approach is that the first step, the estimation step, it's very difficult and is often biased and consequently.",
                    "label": 0
                },
                {
                    "sent": "The evaluation result of your pain in the second state is also unreliable, and in contrast, our solution avoids explicit model building, which is simpler and also more importantly, it gives unbiased evaluation result, which means that we can get reliable numbers.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here's how it works.",
                    "label": 0
                },
                {
                    "sent": "The idea is based on the.",
                    "label": 0
                },
                {
                    "sent": "Replay so I just want to remind that mind you, that this is the quantity that would try to estimate given the policy Pi that selects an ARM action a based on user interest based on user feature and then we try to estimate on average was the reward that I can observe.",
                    "label": 1
                },
                {
                    "sent": "I can receive by following this policy per user visit an again data in this form and we try to estimate the value of this algorithm.",
                    "label": 1
                },
                {
                    "sent": "So a key requirement or key assumption about this data.",
                    "label": 0
                },
                {
                    "sent": "Here we need for this work is that the arms A in this data are chosen randomly in history.",
                    "label": 0
                },
                {
                    "sent": "So what I'm So what it means is that in the Today module, for instance, we can assign a small fraction of use of visits into a random bucket and for user visiting that random bucket we can display a random action or random articles to the user and see how the user reacts.",
                    "label": 0
                },
                {
                    "sent": "And then for this kind of data we can use the.",
                    "label": 0
                },
                {
                    "sent": "Evaluation processes follows, so I go through all the data one by one.",
                    "label": 0
                },
                {
                    "sent": "OK, an AI first reveals the user information to the algorithm and the algorithm will recommend an action a based on the policy \u03c0, and then I'll see whether this a hat is the same as the A in the log data.",
                    "label": 1
                },
                {
                    "sent": "OK, when is the same?",
                    "label": 0
                },
                {
                    "sent": "Then I call it a match and then I use the reward to evaluate the algorithm with return reward to the algorithm to allow it to update this model.",
                    "label": 0
                },
                {
                    "sent": "OK, and if it doesn't match then I ignore that data point and pretend it doesn't exist anymore and don't use it.",
                    "label": 0
                },
                {
                    "sent": "And at the end the output or the estimate we had is the sum of the masked rewards or the some rewards in the matched data points multiplied by a normalization factor to account for this.",
                    "label": 0
                },
                {
                    "sent": "It's matching that description OK. OK, and so this is the final output.",
                    "label": 0
                },
                {
                    "sent": "We had to estimate this quantity that we're interested in.",
                    "label": 0
                },
                {
                    "sent": "So now the question is, how does we had related.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To the true quantitative pie.",
                    "label": 0
                },
                {
                    "sent": "So we have some theoretical guarantees.",
                    "label": 1
                },
                {
                    "sent": "The first one is that the estimator is unbiased, and formally it means that on average the expectation of the estimate is the same as the quantity that you want to estimate, even though you don't have the choice, you don't have the power to run the algorithm in the real system, so.",
                    "label": 1
                },
                {
                    "sent": "This is a nice property.",
                    "label": 0
                },
                {
                    "sent": "Anne II theoretical result is that the estimation error goes down to zero when you have more and more data, so the error estimation error here is the absolute difference between the V high and we had an.",
                    "label": 0
                },
                {
                    "sent": "If you have our data and K candidates per per step then on then the error case to zero in this rate.",
                    "label": 0
                },
                {
                    "sent": "So suppose you have Alvin 1,000,000, then one square root one over will be on the other point.",
                    "label": 0
                },
                {
                    "sent": "01 K so when you have more and more data than you can expect greater and greater.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Accuracy.",
                    "label": 0
                },
                {
                    "sent": "OK, so now let me show you some exciting results based on today module data.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "At Yahoo Front page, so we have a lot of real user traffic in this module and then we've tested a number of policies including on personalized recommendation policy, which is a spatial temporal model that Gravel ET al developed in 2009 and also some personalized improvement variants of the NPR policy.",
                    "label": 1
                },
                {
                    "sent": "And then since this, these policies are implemented in the real system, then we can just collect the bucket data and then count.",
                    "label": 0
                },
                {
                    "sent": "Online performance or online CPR?",
                    "label": 0
                },
                {
                    "sent": "In this case for those policies and treat them as the truth.",
                    "label": 0
                },
                {
                    "sent": "OK, and then we also have some random bucket data and we use the data to run the offline evaluation method for those policies and then we get some offline estimates.",
                    "label": 0
                },
                {
                    "sent": "Now we have two numbers for each policy.",
                    "label": 1
                },
                {
                    "sent": "One is the online online bucket data data traders truth.",
                    "label": 0
                },
                {
                    "sent": "The artist offline estimate.",
                    "label": 0
                },
                {
                    "sent": "Now the goal of this experiment is to see whether these two numbers are closed.",
                    "label": 0
                },
                {
                    "sent": "They closed.",
                    "label": 0
                },
                {
                    "sent": "It means that the offline evaluation method is reliable.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the first plot I'm showing you some articles, City are so the city are normalized here.",
                    "label": 0
                },
                {
                    "sent": "So I'm showing you some city up, lots of in the personalized policy.",
                    "label": 0
                },
                {
                    "sent": "So on the X axis is the online city of each particle and the Y axis is the offline estimate of the article, and each point corresponds to one article.",
                    "label": 0
                },
                {
                    "sent": "So if so, if the offline estimate is precise, then we expect that all points lie on this line y = X.",
                    "label": 1
                },
                {
                    "sent": "But as you can see, all the points are live very close to this line, meaning that the offline in violation without is very close to the online number.",
                    "label": 0
                },
                {
                    "sent": "So we can see that the offline estimators and.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Unbiased.",
                    "label": 0
                },
                {
                    "sent": "And the second plot.",
                    "label": 0
                },
                {
                    "sent": "We show that the Daily City are so here the X axis means that the three 10 days and then for each day we compute the aggregate city are in the bucket OK, and then the red line.",
                    "label": 0
                },
                {
                    "sent": "Here is the online CPR is gonna see there's some variation.",
                    "label": 0
                },
                {
                    "sent": "Daily variation of CR and the Black Line is the offline estimate of corresponding ACR, and there's a strong correlation between these online offline numbers.",
                    "label": 1
                },
                {
                    "sent": "So again, this corroborates the.",
                    "label": 0
                },
                {
                    "sent": "Radical anti that the offline estimators.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Reliable Circuit of the online number and then in the third plot I'm showing you the how the estimation error decays when you have more and more data in the random bucket.",
                    "label": 0
                },
                {
                    "sent": "OK, so the excess is is the number of data points you have and the Y axis the estimation error, which is the absolute difference between the online number an the offline evaluation number an.",
                    "label": 1
                },
                {
                    "sent": "An as predicted by our theorem, error decays roughly on this order.",
                    "label": 1
                },
                {
                    "sent": "One square root 1 / L OK, so this result fits very well with our theory.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "An in the final plot, so I'm showing you that in the online system there are usually some business rules that we cannot control or we cannot replicate in the offline way.",
                    "label": 0
                },
                {
                    "sent": "OK, so for example, human editors may have some rules that overrules the recommendation algorithms choice, and our analysis shows that even though the business rules may have changed, the online bucket results.",
                    "label": 1
                },
                {
                    "sent": "However they change the.",
                    "label": 0
                },
                {
                    "sent": "Online numbers in a multiplicative way, so instead of showing you the online versus online results in previous plot here I'm showing you the ratio of these two plots for different policies.",
                    "label": 0
                },
                {
                    "sent": "So this is the EMP policy which is the.",
                    "label": 0
                },
                {
                    "sent": "Here is the offline estimate divided by online and here I have another policy but using the same ratio of offline online to see Cdr.",
                    "label": 0
                },
                {
                    "sent": "So if the online offline city are, is that true or is a?",
                    "label": 0
                },
                {
                    "sent": "Reliable estimate online city are then we again we expect that all points along this line.",
                    "label": 0
                },
                {
                    "sent": "OK, thanks.",
                    "label": 0
                },
                {
                    "sent": "The impact of business rules as multiplicative to the online CDR's.",
                    "label": 1
                },
                {
                    "sent": "OK, so there's a little bit more variation than in previous grass, but overall order points are still lying around this line, meaning that even in this case where our numbers are corrupted by business rule, the evaluation result can still reveal a relative performance of different policies.",
                    "label": 0
                },
                {
                    "sent": "So which is a good thing about this method?",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hey so this concludes the experiment section.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So extensions you may wonder what happens we have we don't have uniform data, so the good news is that there's still hope.",
                    "label": 1
                },
                {
                    "sent": "OK, so when you have some sort of random data you can use importance weighting to correct the bias in the data even though it is not uniform.",
                    "label": 1
                },
                {
                    "sent": "So there's some somewhere here, and also there's the doubly robust technique from statistics that you can employ to decrease the variance of the estimate.",
                    "label": 0
                },
                {
                    "sent": "Which we also find it helpful in our in our own experience and 2nd.",
                    "label": 0
                },
                {
                    "sent": "What happens if the number of arms K is too large so there are different ways to handle this problem.",
                    "label": 1
                },
                {
                    "sent": "One way that we have tries to prefilter on promising candidates and only randomize over the promising candidates then that also finds useful and there's some open issues.",
                    "label": 0
                },
                {
                    "sent": "One of them should be emphasized here is that.",
                    "label": 0
                },
                {
                    "sent": "In this way, we haven't considered history to constraints on the arms, so in certain applications there are.",
                    "label": 0
                },
                {
                    "sent": "You may have constraint dependent history like advertising and this open question how to incorporate this kind of constraints in the offline evaluation method.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so let me conclude the talk so.",
                    "label": 0
                },
                {
                    "sent": "We have shown that the interactive machine learning is common on the web in web applications OK and then here in this work we investigate an offline evaluation method that gives you unbiased result.",
                    "label": 1
                },
                {
                    "sent": "It also is error also decays very fast when you have more and more data and then we showed it is it gives very accurate result using data from Yahoo today module and this kind.",
                    "label": 0
                },
                {
                    "sent": "So therefore this kind of results can be used to do offline evaluation.",
                    "label": 0
                },
                {
                    "sent": "Of many, many important applications without pain and risk to implementing the algorithm in the real system.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "This concludes the talk and thank you very much and I'm happy to take questions, thanks.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                },
                {
                    "sent": "I thought that was neat, thanks.",
                    "label": 0
                },
                {
                    "sent": "So the work that you are talking about that allows you to handle nonuniform sampling.",
                    "label": 0
                },
                {
                    "sent": "I'm wondering if it's possible that that can be helpful to avoid the fact that you can only use a small fraction of the data that you sampled.",
                    "label": 0
                },
                {
                    "sent": "If you have some knowledge about what you expect, the distribution of arms to look like for the algorithm, is it possible to be more efficient in the data that you sample?",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a good.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a good question, so this actually relates to one of the limitations that we talked about here, when whatever the number of arms case too large.",
                    "label": 0
                },
                {
                    "sent": "OK, so so yeah.",
                    "label": 0
                },
                {
                    "sent": "So if you know if you have prior information that two arms give, you are similar results then you can definitely take into account that information or if possible too.",
                    "label": 0
                },
                {
                    "sent": "To adapt the reward of 1 arm when actually in offline case deposit, choose another arm that is very similar, but it depends on the similarity metric that you use.",
                    "label": 0
                },
                {
                    "sent": "If it's good then it's good, but.",
                    "label": 0
                },
                {
                    "sent": "Then the general Advice County is such memory is not is lost, but maybe it's possible, but we haven't tried.",
                    "label": 0
                },
                {
                    "sent": "Thanks.",
                    "label": 0
                }
            ]
        }
    }
}