{
    "id": "ycuwa5k6euu4qfkmuan4dlhjpd63qclc",
    "title": "The Multidimensional Wisdom of Crowds",
    "info": {
        "author": [
            "Peter Welinder, California Institute of Technology (Caltech)"
        ],
        "published": "Jan. 12, 2011",
        "recorded": "December 2010",
        "category": [
            "Top->Computer Science->Data Mining"
        ]
    },
    "url": "http://videolectures.net/nips2010_welinder_mwc/",
    "segmentation": [
        [
            "Thank you.",
            "So in."
        ],
        [
            "In computer vision and in particularly visual recognition, which is feel I'm in, we like to collect data sets like this.",
            "So here we have visual category, the bird species, indigo bunting, and a set of training images.",
            "So if you ask me to create this data set from scratch, what I would probably do would be to go onto some kind of image database and I would type in indigo bunting.",
            "And I will start downloading some of the images.",
            "Some of the thousands of images that I would get back.",
            "Now."
        ],
        [
            "Problem would be that not all of the images are really of indigo bunting.",
            "Some of the images will be irrelevant and I need to manually filter out irrelevant images so this can be a problem since it takes a very long time and particularly in our lab where we need to download hundreds of different categories for each category we need to filter thousands of images.",
            "So the."
        ],
        [
            "We deal with it and a lot of people do with it today is to use some kind of crowdsourcing service such as Amazon Mechanical Turk.",
            "So these kind of services less you distribute thousands of small jobs, each taking a minute or so to complete to hundreds of different workers.",
            "So in our case, a typical task would be a binary question.",
            "Is there an indigo bunting in the image or not?",
            "So we send this task out to hundreds of annotators and they all work in it."
        ],
        [
            "Parallel and the kind of labels that we could would get back for."
        ],
        [
            "Task looks like this, so here annotator a labeled all of the images in the top row with green meaning yes and all in the bottom row with red meaning no.",
            "So unfortunately as we ask more annotators to labels."
        ],
        [
            "Same images, so here annotators BC&D.",
            "Chances are that they will differ in their labels on some of the images, and in particular for all but some of the images.",
            "The labels will be inconsistent."
        ],
        [
            "So the way people usually deal with this noisy labels from multiple annotators is that they use some kind of touristics, such as the most popular one being majority voting or using traps where you know the ground truth for some of the images, or using some kind of qualification tests.",
            "So there are also being."
        ],
        [
            "I work in the past on modeling annotators as well as modeling the difficulty in annotating certain images.",
            "So in this talk I'll take this one step further by looking a little bit closer.",
            "Look at how annotators annotate images, and in particular I present a generative probabilistic model for the annotation process and argue that by making some careful assumptions about our annotators, we can get quite a lot of binary labels."
        ],
        [
            "So the first thing to note is that since we have binary labels, we have two kinds of errors, misses and false alarms.",
            "And this means that we can characterize each annotators in terms of their hit rate and the rate are correct prediction.",
            "So in order to see the spread of the different annotators in terms of these error rates, we collected a data set with about 200.",
            "Images were of different bird species, and then we asked additive to select only images containing indigo bunting, and so each point here on this plot is a different annotator and the way to look at this."
        ],
        [
            "If we plot.",
            "Curves are equal errors, so two annotators ending up on the same curve here will have the same total error, but the proportion of the different errors that make will be different.",
            "We cannot see."
        ],
        [
            "In the upper right corner we have competent annotators, so these are annotated with very low error rates, but."
        ],
        [
            "Along the diagonal we have these bots so bots in some way are very confused and they provide us with a sort of more or less random labels, so they're almost like a script that somebody is coded up just to get money from you from Mturk.",
            "So these are not the only interesting parts of this plot."
        ],
        [
            "In the upper left corner we have the optimists, so these are annotators are very happy.",
            "In some sense they select almost every image that they see.",
            "And then in the."
        ],
        [
            "Right corner we have the opposite is sort of pessimistic.",
            "These are very cautious annotators, so these are things we have very low false alarm rates.",
            "So why is it important to identify these groups?",
            "Well, it's because in some cases they can provide us with quite a lot of information.",
            "So for example, if a pessimistic annotators provides a Yes label, we can be pretty sure that labor is correct, because the probability that that annotator will provide a false alarm is very low and then."
        ],
        [
            "The last big group of annotators is the adversaries, so these annotators actually provide us with labels that are opposite from what we want them to give us.",
            "This is OK as long as we can catch them, because then we can just flip their labels.",
            "Of course, in so this was all computer when we have sort of the ground truth.",
            "In reality we don't really have the ground truth, and that's why we sent out the task in the 1st place.",
            "So in order to find these groups of annotators when we don't have the ground truth, we have to make some assumptions of how they interpret images."
        ],
        [
            "So let's first look at how annotators decide the label on images of different difficulty.",
            "So let's introduce some notation.",
            "So let's see I be a binary variable indicating whether an object is present or not.",
            "An image I and that capital I be the actual image pixels that each annotator sees.",
            "So we make the assumption that these image pixels are influenced by some nuisance factors.",
            "Now this is pretty hard to model, so to make things a little bit simpler to model."
        ],
        [
            "What we do is that we make the assumption that in the head of an ideal annotator, there are some newer populations that that.",
            "That process, the image and the output of this processing is some scalar variable XI and this scalar variable measure some characteristic of the object.",
            "So in the case of birds, for example, this might be the color of the perimeter of the bird, or maybe the sheep of the peak of the bird.",
            "So we call this variable XI this signal and is indicative of whether the obvious present in the image or not.",
            "Then to keep things simple, we make the assumption that the conditional distribution on.",
            "Sigh, the signal given object presence are to Gaussian separated by some distance."
        ],
        [
            "So now let's look at an example task again, where the task is to select images with indigo bunting and now assume that we know that there is no indigo bunting in the image.",
            "So Z = 0 and then we sample."
        ],
        [
            "An image we get an image like this, so this is not an enigma."
        ],
        [
            "So the the signal will be sampled from the negative distribution.",
            "Now in order to be able to characterize annotators of different competence."
        ],
        [
            "We need to introduce another variable so we introduce variable Sigma J which is a measure of this."
        ],
        [
            "Competence and we make the assumption that signal that the annotator actually season make their decision on is a version of the is basically the ideal signal XI corrupted by some Gaussian noise, where this Sigma J for each annotator JS the width of that noise."
        ],
        [
            "So different."
        ],
        [
            "Annotators will see this same image with different clarity and make their decision upon that signal."
        ],
        [
            "So we can look at some other images so."
        ],
        [
            "For example, this is clearly an indigo bunting, and."
        ],
        [
            "So we sampled this image from the positive distribution, but."
        ],
        [
            "Might also encounter images like this, so I'm not entirely sure what happened to this bird, but I cannot be entirely sure that it's an indigo bunting, so it could be some other kind of bird that had some accidents or something.",
            "So the point here is that in some way we can model this."
        ],
        [
            "By saying that the exercise close to 0 so."
        ],
        [
            "This means that even a very competent annotator like Lisa will have a hard time deciding the label on that image.",
            "So that tells us in some way that XI is also a measure of difficulty of the image or the ambiguity of the image where is closest Retiro image will be more ambitious?"
        ],
        [
            "So once we have this framework is quite easy to introduce to think about optimists versus pessimists, and the way to do this is that we introduce a threshold Audi A and."
        ],
        [
            "The signal that the energy disease is above this threshold, the labels one and 0 otherwise."
        ],
        [
            "And as we move this threshold to."
        ],
        [
            "Right, we will have the optimal the pessimistic annotators with very low false alarm rates, and then I."
        ],
        [
            "Remove the left.",
            "We have the optimistic annotators with high false alarm rates and."
        ],
        [
            "Neutral annotators would be somewhere in between."
        ],
        [
            "Now there's nothing to say that we have to restrict ourselves to scalar signals, so we can loosen this assumption that a little bit, and we can.",
            "We can imagine that XI is actually a vector, and in that case we can think of each dimension of this vector as being the output of some newer population in the brain of the annotator.",
            "So in some way this creates an embedding in the head of the annotator which the annotator uses to make the decision on the label.",
            "So for example, we never know really what this these dimensions."
        ],
        [
            "But we can take an example for example, might be the task of discriminating Hawks versus Seagulls.",
            "And then the two dimensions might be the color of the permission of the bird and the shape of the beak of the bird."
        ],
        [
            "In this case, we also make the assumption now since we have a higher dimensional space at each annotator, is a linear classifier and we already have the distance from the origin tower of their decision plane as well as the margin Sigma.",
            "So all we have to introduce is sort of the orientation of this linear classifier in this embedding, which we call WJ OK."
        ],
        [
            "So we can put all of this together into generative probabilistic model.",
            "So here the things that we actually observe."
        ],
        [
            "Here are the binary variable binary labels Ellijay from for image I annotated J.",
            "So this is the only thing that we observe, and usually what we want to rest."
        ],
        [
            "Mate is whether a particular object is present in the image or not, so this is binary variable ZI.",
            "So in this model we get a few things for freedom."
        ],
        [
            "You get the image difficulty in the signal XI and based on the annotate."
        ],
        [
            "Is competence we get corrupted signal that annotator sees and then the."
        ],
        [
            "Annotator uses his expertise in some way contained in this orientation of the decision plane as well as its optimism to make the decision on the label."
        ],
        [
            "So I'm."
        ],
        [
            "In order to see whether this model could actually be used to estimate object presence as well as these other things, we carried out a series of experiments, some of which I will show you here, some of which you will be able to see in the poster.",
            "So in the first thing we want to see is if we can actually estimate this object presence, and we did this by having the same tasks before we have a series of images and we know some of them will be of indigo bunting, and we ask Anna to select only images containing indigo bunting."
        ],
        [
            "And so we send these out Amazon Mechanical Turk.",
            "All of these images and we get back a lot of binary labels from the annotators we feed them into the model.",
            "And we do inference in order to estimate the object presence."
        ],
        [
            "So here you see the results.",
            "So where we compare pair our model which is in blue to majority voting and two other methods and the thing to see here is.",
            "So the number as the number of annotators that get to see and label each image increases, the total error would go down.",
            "So the way to read the plot is."
        ],
        [
            "Quickly that for some given error, say 20% error, the the amount of labels that are model needs is at most half as much as the next best methods.",
            "So because you have as much to use in some sense."
        ],
        [
            "So now we know that we can use this model in order to get to label get ground truth labels for category datasets so but we have these other things.",
            "We have this multidimensional.",
            "XI and so we want to see can we actually pick out attributes and images.",
            "So the way we did this is that we created a synthetic data set of synthetic images of one kind of sort of a kind of greebles agree but disagree bowl is discriminated by highlighting colors.",
            "It's this small figure and we created them from two groups, two Gaussians, one is tall and green and the other one is short in yellow and then we."
        ],
        [
            "And all of these images are created to two groups of annotators, and we give the annotators different instructions.",
            "The first group of annotators we asked to select only the green greebles and the second group of annotators we asked to select only the tall greebles."
        ],
        [
            "So again, we sent this out to Mechanical Turk.",
            "We got back the binary labels we fed them into the modern under the assumption of a 2 dimensional embedding and we didn't inference.",
            "Now you should note that the modern knows nothing about these two groups of annotators order two groups of images.",
            "All it has is bunch of binary labels."
        ],
        [
            "So the embedding that that the model reconstructor looks like this.",
            "So here each point."
        ],
        [
            "Is a different image."
        ],
        [
            "And right now we don't know what these two dimensions of this embedding really means, but as we color the points by."
        ],
        [
            "Ground truth we will get some clues.",
            "So here we color the points by which ground truth truth categories belong to and we can see that the model is actually able to separate out the images based on which visual category they belong to or what people looked for in the images.",
            "So."
        ],
        [
            "So the second thing we can plot is the annotator.",
            "So each line here is a different annotator and.",
            "And it's basically decision plane of that annotator and we see that they are grouped roughly into two groups.",
            "We have one horizontal and one vertical as we."
        ],
        [
            "Color these by the groups of annotators that we gave different instructions.",
            "We can actually see that the vertical one group is the ones that are looking for the green green balls and the horizontal one is 1 looking for tall greebles.",
            "So this also gives us some clues to what this embedding means.",
            "In this case it seems to mean that the horizontal axis here is the color of degree balls and the vertical one is the height of the greebles.",
            "So this experiment tells us that we can pick out different groups of annotators based on what they're looking for in images, and we can group images based on the richer content."
        ],
        [
            "So in the last experiment we wanted to see whether we can do the same thing in more realistic images and we want to see if we can find real experts on Amazon Mechanical Turk.",
            "So what we did was that we collected a data set of series of images of different bird speeches.",
            "And the birds are mallards, American black ducks, Canada geese and redneck groups, as well as some images containing numbers at all.",
            "And now we."
        ],
        [
            "As the annotators only to select the images containing ducks, so only mallards and American black ducks are actually ducks and our hypothesis was that only a subset of the annotators on Amazon Mechanical Turk would know about this and the rest of the annotators would basically confuse Canada geese and redneck creeps for being ducks.",
            "So again, we send these images out diamonds Mechanical Turk.",
            "We did inference.",
            "We got back the binary labels and we did influence in the model and what we got."
        ],
        [
            "Back looks like this, so now we don't know at all what this embedding means, but hopefully the model should be able to separate out."
        ],
        [
            "Images based on the visual content and this is what happens.",
            "So here we color each image by which.",
            "Category it belongs to which bird species on bird and the first thing to note is is that the images are spread in place in this space?",
            "Depending on what category they belong to."
        ],
        [
            "If we look a little bit closer, we see that all of the ducks, their manners, and American black ducks are intermixed.",
            "This is expected, since we did, we didn't ask.",
            "Annotators actually discriminate between the duck species.",
            "The second thing to notice is that all of the images in black hair containing no birds are sort of separated from the rest.",
            "What we didn't expect was that they grebes ended up between the geese and ducks.",
            "So why is this so?",
            "The clue sort of comes if we start looking at our annotators."
        ],
        [
            "So again, each line here is a different annotator that was labeling this data set, and so it's basically their decision, plain and we see that are roughly three groups of annotators."
        ],
        [
            "The first group of annotators are able to separate out the Ducks from the rest of the birds, as well as the numbered images.",
            "So in some way these are the experts that we were looking for."
        ],
        [
            "Then we also have another group of annotators that seem to confuse grieves for ducks, but they're able to separate out the Canada geese and the document is, so they're sort of semi experts."
        ],
        [
            "And then the last group of annotators are able to separate, separate out the numbered images, but they don't know very much about birds, so they basically group all of the birds together.",
            "So this experiment tells us that we were able to pick out sort of different schools of thoughts on Amazon Mechanical Turk based on only the binary labels that we get back and.",
            "And we're also able to separate out images based on what people are looking for in these images.",
            "So."
        ],
        [
            "Was it likes to experiment and I hope that it convinced you that using this model you can estimate reliable ground truth labels for your category datasets and and in addition to that, the model gives you some sense of how competent, optimistic and how much expertise your annotators have.",
            "And it also gives you a sense of how difficult or how ambiguous your images are, and like more implicitly, the model gives you some sense of grouping images based on the visual content and also picks out different schools of thoughts amongst your Anna."
        ],
        [
            "Writers, so thank you very much and if you want to learn more about some of the details, please come by the poster W 33.",
            "We have time for questions.",
            "Hi, this reminds me a little bit of the way we used to.",
            "Re weight reviewers scores in program committees and I was wondering if you thought of applying this algorithm to reviews of papers.",
            "Yeah, that's a good idea.",
            "This is maybe what we should use next time for an inside I.",
            "It's possible you could use them or something like that, I don't know.",
            "The reviewers don't get paid though.",
            "So.",
            "5 cents.",
            "OK, so.",
            "Big problem in practice of say, White Hill model, which is much simpler than what you're proposing, is that you usually need lots of overlap, meaning the same image has to go to multiple judges before it can make any reliable estimates, even when you clamp the difficulty of the image to be the same for all of them, like your model seems to be, significantly more complicated has more variables.",
            "What do you think is the dependence?",
            "Or like how reliable the inferences based on how many judges will see each example?",
            "OK so I I can't hear if I get this right.",
            "You're asking how reliable, like how many annotators that gets.",
            "It needs to see each image and how many images each annotator need to see in order to get reliable estimates of both the image labels as well as the annotated labels.",
            "Yeah, the first one actually.",
            "Yeah, each image should go to how many annotators before we get reliable estimates.",
            "OK, so it depends a little bit so some images.",
            "So the point here is that some images will be more ambiguous and usually in order to get a reliable estimate of those you need to send them out to more annotators.",
            "Well, images which you are very easy to classify, they will be.",
            "You will be able to pick out that they have a very sort of their very not.",
            "Not very ambiguous, pretty quickly and then you will only be able.",
            "You will only need to send them out to like two or three annotators in that case.",
            "So on average to like when we use this we usually set it out to between three and six annotators parameters.",
            "So actually, like again, I'll have to look more into this model, but whitehills model can be shown to be completely not working if you're sending it to just two annotators like mathematically, it just like it just regresses to the prior so.",
            "Sorry, can you repeat that so white white heels model can be shown mathematically to be completely not working if you're sending it to just two annotators, it just just regresses to the prior, basically to the most popular class and I'll have to look at your model in more details to see if that similar effect can be happening, but I would be careful with sending that to just to yeah, if you if you only sent it to annotate, it should be very careful like the point here is that usually the way uses kind of model is that you sent it out to several.",
            "Different annotators that they sort of model your datasets.",
            "They label your datasets partially, so not all annotators would get to see all of your images, and so on, and so they will.",
            "Each label maybe 100 images each, and each image would be labeled by only between three and six annotators each, so that's usually the way you should use it if you want reliable estimates.",
            "OK, so let's take one quick question over there.",
            "OK, this is probably more common, but I'll try to disguise it as a question.",
            "The observer model you have is basically the standard signal detection psychophysics model.",
            "Now in psychophysics people have observed that it doesn't work very well and you need to add additional variables to account for things like lapses where people basically aren't looking at the screen and press a button anyway, but at other times are actually paying attention so that they're not completely random overall, but occasionally there's sort of a mixture model over 2 behaviors and also probably other terms to account for your adversaries that you didn't actually have in the model at the end.",
            "Have you explored trying to add those on and do you think some of those lines would actually tighten up in your plots?",
            "If you had those?",
            "Yeah, so that's that's an excellent question and we in the task that we have used so far we didn't encounter.",
            "Examples where we actually need to account for, say attention of annotator and so on, where they actually paid attention to task over if they just answered randomly or something like that.",
            "However, now we moved on to other Council tax where this actually seems to be important and then in that case what this model will do is that it will see.",
            "Look at your annotators as being very noisy in some sense and that's not very accurate.",
            "You need to sort of pick out other attributes of your annotators, so there might be room for extending this.",
            "This kind of model to include these kind of factors.",
            "Alright, OK let's thank Peter again."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Thank you.",
                    "label": 0
                },
                {
                    "sent": "So in.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In computer vision and in particularly visual recognition, which is feel I'm in, we like to collect data sets like this.",
                    "label": 0
                },
                {
                    "sent": "So here we have visual category, the bird species, indigo bunting, and a set of training images.",
                    "label": 0
                },
                {
                    "sent": "So if you ask me to create this data set from scratch, what I would probably do would be to go onto some kind of image database and I would type in indigo bunting.",
                    "label": 0
                },
                {
                    "sent": "And I will start downloading some of the images.",
                    "label": 0
                },
                {
                    "sent": "Some of the thousands of images that I would get back.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Problem would be that not all of the images are really of indigo bunting.",
                    "label": 0
                },
                {
                    "sent": "Some of the images will be irrelevant and I need to manually filter out irrelevant images so this can be a problem since it takes a very long time and particularly in our lab where we need to download hundreds of different categories for each category we need to filter thousands of images.",
                    "label": 0
                },
                {
                    "sent": "So the.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We deal with it and a lot of people do with it today is to use some kind of crowdsourcing service such as Amazon Mechanical Turk.",
                    "label": 0
                },
                {
                    "sent": "So these kind of services less you distribute thousands of small jobs, each taking a minute or so to complete to hundreds of different workers.",
                    "label": 0
                },
                {
                    "sent": "So in our case, a typical task would be a binary question.",
                    "label": 0
                },
                {
                    "sent": "Is there an indigo bunting in the image or not?",
                    "label": 1
                },
                {
                    "sent": "So we send this task out to hundreds of annotators and they all work in it.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parallel and the kind of labels that we could would get back for.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Task looks like this, so here annotator a labeled all of the images in the top row with green meaning yes and all in the bottom row with red meaning no.",
                    "label": 0
                },
                {
                    "sent": "So unfortunately as we ask more annotators to labels.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same images, so here annotators BC&D.",
                    "label": 0
                },
                {
                    "sent": "Chances are that they will differ in their labels on some of the images, and in particular for all but some of the images.",
                    "label": 0
                },
                {
                    "sent": "The labels will be inconsistent.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the way people usually deal with this noisy labels from multiple annotators is that they use some kind of touristics, such as the most popular one being majority voting or using traps where you know the ground truth for some of the images, or using some kind of qualification tests.",
                    "label": 0
                },
                {
                    "sent": "So there are also being.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I work in the past on modeling annotators as well as modeling the difficulty in annotating certain images.",
                    "label": 0
                },
                {
                    "sent": "So in this talk I'll take this one step further by looking a little bit closer.",
                    "label": 0
                },
                {
                    "sent": "Look at how annotators annotate images, and in particular I present a generative probabilistic model for the annotation process and argue that by making some careful assumptions about our annotators, we can get quite a lot of binary labels.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the first thing to note is that since we have binary labels, we have two kinds of errors, misses and false alarms.",
                    "label": 0
                },
                {
                    "sent": "And this means that we can characterize each annotators in terms of their hit rate and the rate are correct prediction.",
                    "label": 1
                },
                {
                    "sent": "So in order to see the spread of the different annotators in terms of these error rates, we collected a data set with about 200.",
                    "label": 0
                },
                {
                    "sent": "Images were of different bird species, and then we asked additive to select only images containing indigo bunting, and so each point here on this plot is a different annotator and the way to look at this.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we plot.",
                    "label": 0
                },
                {
                    "sent": "Curves are equal errors, so two annotators ending up on the same curve here will have the same total error, but the proportion of the different errors that make will be different.",
                    "label": 0
                },
                {
                    "sent": "We cannot see.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the upper right corner we have competent annotators, so these are annotated with very low error rates, but.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Along the diagonal we have these bots so bots in some way are very confused and they provide us with a sort of more or less random labels, so they're almost like a script that somebody is coded up just to get money from you from Mturk.",
                    "label": 0
                },
                {
                    "sent": "So these are not the only interesting parts of this plot.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the upper left corner we have the optimists, so these are annotators are very happy.",
                    "label": 0
                },
                {
                    "sent": "In some sense they select almost every image that they see.",
                    "label": 0
                },
                {
                    "sent": "And then in the.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right corner we have the opposite is sort of pessimistic.",
                    "label": 0
                },
                {
                    "sent": "These are very cautious annotators, so these are things we have very low false alarm rates.",
                    "label": 0
                },
                {
                    "sent": "So why is it important to identify these groups?",
                    "label": 0
                },
                {
                    "sent": "Well, it's because in some cases they can provide us with quite a lot of information.",
                    "label": 0
                },
                {
                    "sent": "So for example, if a pessimistic annotators provides a Yes label, we can be pretty sure that labor is correct, because the probability that that annotator will provide a false alarm is very low and then.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The last big group of annotators is the adversaries, so these annotators actually provide us with labels that are opposite from what we want them to give us.",
                    "label": 0
                },
                {
                    "sent": "This is OK as long as we can catch them, because then we can just flip their labels.",
                    "label": 0
                },
                {
                    "sent": "Of course, in so this was all computer when we have sort of the ground truth.",
                    "label": 0
                },
                {
                    "sent": "In reality we don't really have the ground truth, and that's why we sent out the task in the 1st place.",
                    "label": 0
                },
                {
                    "sent": "So in order to find these groups of annotators when we don't have the ground truth, we have to make some assumptions of how they interpret images.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So let's first look at how annotators decide the label on images of different difficulty.",
                    "label": 0
                },
                {
                    "sent": "So let's introduce some notation.",
                    "label": 0
                },
                {
                    "sent": "So let's see I be a binary variable indicating whether an object is present or not.",
                    "label": 0
                },
                {
                    "sent": "An image I and that capital I be the actual image pixels that each annotator sees.",
                    "label": 1
                },
                {
                    "sent": "So we make the assumption that these image pixels are influenced by some nuisance factors.",
                    "label": 0
                },
                {
                    "sent": "Now this is pretty hard to model, so to make things a little bit simpler to model.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What we do is that we make the assumption that in the head of an ideal annotator, there are some newer populations that that.",
                    "label": 0
                },
                {
                    "sent": "That process, the image and the output of this processing is some scalar variable XI and this scalar variable measure some characteristic of the object.",
                    "label": 0
                },
                {
                    "sent": "So in the case of birds, for example, this might be the color of the perimeter of the bird, or maybe the sheep of the peak of the bird.",
                    "label": 0
                },
                {
                    "sent": "So we call this variable XI this signal and is indicative of whether the obvious present in the image or not.",
                    "label": 0
                },
                {
                    "sent": "Then to keep things simple, we make the assumption that the conditional distribution on.",
                    "label": 0
                },
                {
                    "sent": "Sigh, the signal given object presence are to Gaussian separated by some distance.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now let's look at an example task again, where the task is to select images with indigo bunting and now assume that we know that there is no indigo bunting in the image.",
                    "label": 0
                },
                {
                    "sent": "So Z = 0 and then we sample.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "An image we get an image like this, so this is not an enigma.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the the signal will be sampled from the negative distribution.",
                    "label": 0
                },
                {
                    "sent": "Now in order to be able to characterize annotators of different competence.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We need to introduce another variable so we introduce variable Sigma J which is a measure of this.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Competence and we make the assumption that signal that the annotator actually season make their decision on is a version of the is basically the ideal signal XI corrupted by some Gaussian noise, where this Sigma J for each annotator JS the width of that noise.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So different.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Annotators will see this same image with different clarity and make their decision upon that signal.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can look at some other images so.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For example, this is clearly an indigo bunting, and.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we sampled this image from the positive distribution, but.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Might also encounter images like this, so I'm not entirely sure what happened to this bird, but I cannot be entirely sure that it's an indigo bunting, so it could be some other kind of bird that had some accidents or something.",
                    "label": 0
                },
                {
                    "sent": "So the point here is that in some way we can model this.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By saying that the exercise close to 0 so.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This means that even a very competent annotator like Lisa will have a hard time deciding the label on that image.",
                    "label": 0
                },
                {
                    "sent": "So that tells us in some way that XI is also a measure of difficulty of the image or the ambiguity of the image where is closest Retiro image will be more ambitious?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So once we have this framework is quite easy to introduce to think about optimists versus pessimists, and the way to do this is that we introduce a threshold Audi A and.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The signal that the energy disease is above this threshold, the labels one and 0 otherwise.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And as we move this threshold to.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Right, we will have the optimal the pessimistic annotators with very low false alarm rates, and then I.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Remove the left.",
                    "label": 0
                },
                {
                    "sent": "We have the optimistic annotators with high false alarm rates and.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Neutral annotators would be somewhere in between.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now there's nothing to say that we have to restrict ourselves to scalar signals, so we can loosen this assumption that a little bit, and we can.",
                    "label": 0
                },
                {
                    "sent": "We can imagine that XI is actually a vector, and in that case we can think of each dimension of this vector as being the output of some newer population in the brain of the annotator.",
                    "label": 0
                },
                {
                    "sent": "So in some way this creates an embedding in the head of the annotator which the annotator uses to make the decision on the label.",
                    "label": 0
                },
                {
                    "sent": "So for example, we never know really what this these dimensions.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But we can take an example for example, might be the task of discriminating Hawks versus Seagulls.",
                    "label": 0
                },
                {
                    "sent": "And then the two dimensions might be the color of the permission of the bird and the shape of the beak of the bird.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In this case, we also make the assumption now since we have a higher dimensional space at each annotator, is a linear classifier and we already have the distance from the origin tower of their decision plane as well as the margin Sigma.",
                    "label": 0
                },
                {
                    "sent": "So all we have to introduce is sort of the orientation of this linear classifier in this embedding, which we call WJ OK.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we can put all of this together into generative probabilistic model.",
                    "label": 0
                },
                {
                    "sent": "So here the things that we actually observe.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here are the binary variable binary labels Ellijay from for image I annotated J.",
                    "label": 0
                },
                {
                    "sent": "So this is the only thing that we observe, and usually what we want to rest.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Mate is whether a particular object is present in the image or not, so this is binary variable ZI.",
                    "label": 0
                },
                {
                    "sent": "So in this model we get a few things for freedom.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You get the image difficulty in the signal XI and based on the annotate.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is competence we get corrupted signal that annotator sees and then the.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Annotator uses his expertise in some way contained in this orientation of the decision plane as well as its optimism to make the decision on the label.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I'm.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In order to see whether this model could actually be used to estimate object presence as well as these other things, we carried out a series of experiments, some of which I will show you here, some of which you will be able to see in the poster.",
                    "label": 0
                },
                {
                    "sent": "So in the first thing we want to see is if we can actually estimate this object presence, and we did this by having the same tasks before we have a series of images and we know some of them will be of indigo bunting, and we ask Anna to select only images containing indigo bunting.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And so we send these out Amazon Mechanical Turk.",
                    "label": 0
                },
                {
                    "sent": "All of these images and we get back a lot of binary labels from the annotators we feed them into the model.",
                    "label": 1
                },
                {
                    "sent": "And we do inference in order to estimate the object presence.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So here you see the results.",
                    "label": 0
                },
                {
                    "sent": "So where we compare pair our model which is in blue to majority voting and two other methods and the thing to see here is.",
                    "label": 0
                },
                {
                    "sent": "So the number as the number of annotators that get to see and label each image increases, the total error would go down.",
                    "label": 1
                },
                {
                    "sent": "So the way to read the plot is.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Quickly that for some given error, say 20% error, the the amount of labels that are model needs is at most half as much as the next best methods.",
                    "label": 0
                },
                {
                    "sent": "So because you have as much to use in some sense.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we know that we can use this model in order to get to label get ground truth labels for category datasets so but we have these other things.",
                    "label": 0
                },
                {
                    "sent": "We have this multidimensional.",
                    "label": 0
                },
                {
                    "sent": "XI and so we want to see can we actually pick out attributes and images.",
                    "label": 0
                },
                {
                    "sent": "So the way we did this is that we created a synthetic data set of synthetic images of one kind of sort of a kind of greebles agree but disagree bowl is discriminated by highlighting colors.",
                    "label": 0
                },
                {
                    "sent": "It's this small figure and we created them from two groups, two Gaussians, one is tall and green and the other one is short in yellow and then we.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And all of these images are created to two groups of annotators, and we give the annotators different instructions.",
                    "label": 0
                },
                {
                    "sent": "The first group of annotators we asked to select only the green greebles and the second group of annotators we asked to select only the tall greebles.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So again, we sent this out to Mechanical Turk.",
                    "label": 0
                },
                {
                    "sent": "We got back the binary labels we fed them into the modern under the assumption of a 2 dimensional embedding and we didn't inference.",
                    "label": 0
                },
                {
                    "sent": "Now you should note that the modern knows nothing about these two groups of annotators order two groups of images.",
                    "label": 0
                },
                {
                    "sent": "All it has is bunch of binary labels.",
                    "label": 1
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the embedding that that the model reconstructor looks like this.",
                    "label": 0
                },
                {
                    "sent": "So here each point.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is a different image.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And right now we don't know what these two dimensions of this embedding really means, but as we color the points by.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ground truth we will get some clues.",
                    "label": 0
                },
                {
                    "sent": "So here we color the points by which ground truth truth categories belong to and we can see that the model is actually able to separate out the images based on which visual category they belong to or what people looked for in the images.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So the second thing we can plot is the annotator.",
                    "label": 0
                },
                {
                    "sent": "So each line here is a different annotator and.",
                    "label": 0
                },
                {
                    "sent": "And it's basically decision plane of that annotator and we see that they are grouped roughly into two groups.",
                    "label": 0
                },
                {
                    "sent": "We have one horizontal and one vertical as we.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Color these by the groups of annotators that we gave different instructions.",
                    "label": 0
                },
                {
                    "sent": "We can actually see that the vertical one group is the ones that are looking for the green green balls and the horizontal one is 1 looking for tall greebles.",
                    "label": 0
                },
                {
                    "sent": "So this also gives us some clues to what this embedding means.",
                    "label": 0
                },
                {
                    "sent": "In this case it seems to mean that the horizontal axis here is the color of degree balls and the vertical one is the height of the greebles.",
                    "label": 0
                },
                {
                    "sent": "So this experiment tells us that we can pick out different groups of annotators based on what they're looking for in images, and we can group images based on the richer content.",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So in the last experiment we wanted to see whether we can do the same thing in more realistic images and we want to see if we can find real experts on Amazon Mechanical Turk.",
                    "label": 0
                },
                {
                    "sent": "So what we did was that we collected a data set of series of images of different bird speeches.",
                    "label": 0
                },
                {
                    "sent": "And the birds are mallards, American black ducks, Canada geese and redneck groups, as well as some images containing numbers at all.",
                    "label": 1
                },
                {
                    "sent": "And now we.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "As the annotators only to select the images containing ducks, so only mallards and American black ducks are actually ducks and our hypothesis was that only a subset of the annotators on Amazon Mechanical Turk would know about this and the rest of the annotators would basically confuse Canada geese and redneck creeps for being ducks.",
                    "label": 0
                },
                {
                    "sent": "So again, we send these images out diamonds Mechanical Turk.",
                    "label": 0
                },
                {
                    "sent": "We did inference.",
                    "label": 0
                },
                {
                    "sent": "We got back the binary labels and we did influence in the model and what we got.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Back looks like this, so now we don't know at all what this embedding means, but hopefully the model should be able to separate out.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Images based on the visual content and this is what happens.",
                    "label": 0
                },
                {
                    "sent": "So here we color each image by which.",
                    "label": 0
                },
                {
                    "sent": "Category it belongs to which bird species on bird and the first thing to note is is that the images are spread in place in this space?",
                    "label": 0
                },
                {
                    "sent": "Depending on what category they belong to.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we look a little bit closer, we see that all of the ducks, their manners, and American black ducks are intermixed.",
                    "label": 0
                },
                {
                    "sent": "This is expected, since we did, we didn't ask.",
                    "label": 0
                },
                {
                    "sent": "Annotators actually discriminate between the duck species.",
                    "label": 0
                },
                {
                    "sent": "The second thing to notice is that all of the images in black hair containing no birds are sort of separated from the rest.",
                    "label": 0
                },
                {
                    "sent": "What we didn't expect was that they grebes ended up between the geese and ducks.",
                    "label": 0
                },
                {
                    "sent": "So why is this so?",
                    "label": 0
                },
                {
                    "sent": "The clue sort of comes if we start looking at our annotators.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So again, each line here is a different annotator that was labeling this data set, and so it's basically their decision, plain and we see that are roughly three groups of annotators.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The first group of annotators are able to separate out the Ducks from the rest of the birds, as well as the numbered images.",
                    "label": 0
                },
                {
                    "sent": "So in some way these are the experts that we were looking for.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Then we also have another group of annotators that seem to confuse grieves for ducks, but they're able to separate out the Canada geese and the document is, so they're sort of semi experts.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then the last group of annotators are able to separate, separate out the numbered images, but they don't know very much about birds, so they basically group all of the birds together.",
                    "label": 0
                },
                {
                    "sent": "So this experiment tells us that we were able to pick out sort of different schools of thoughts on Amazon Mechanical Turk based on only the binary labels that we get back and.",
                    "label": 0
                },
                {
                    "sent": "And we're also able to separate out images based on what people are looking for in these images.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Was it likes to experiment and I hope that it convinced you that using this model you can estimate reliable ground truth labels for your category datasets and and in addition to that, the model gives you some sense of how competent, optimistic and how much expertise your annotators have.",
                    "label": 0
                },
                {
                    "sent": "And it also gives you a sense of how difficult or how ambiguous your images are, and like more implicitly, the model gives you some sense of grouping images based on the visual content and also picks out different schools of thoughts amongst your Anna.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Writers, so thank you very much and if you want to learn more about some of the details, please come by the poster W 33.",
                    "label": 1
                },
                {
                    "sent": "We have time for questions.",
                    "label": 0
                },
                {
                    "sent": "Hi, this reminds me a little bit of the way we used to.",
                    "label": 0
                },
                {
                    "sent": "Re weight reviewers scores in program committees and I was wondering if you thought of applying this algorithm to reviews of papers.",
                    "label": 0
                },
                {
                    "sent": "Yeah, that's a good idea.",
                    "label": 0
                },
                {
                    "sent": "This is maybe what we should use next time for an inside I.",
                    "label": 0
                },
                {
                    "sent": "It's possible you could use them or something like that, I don't know.",
                    "label": 0
                },
                {
                    "sent": "The reviewers don't get paid though.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "5 cents.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Big problem in practice of say, White Hill model, which is much simpler than what you're proposing, is that you usually need lots of overlap, meaning the same image has to go to multiple judges before it can make any reliable estimates, even when you clamp the difficulty of the image to be the same for all of them, like your model seems to be, significantly more complicated has more variables.",
                    "label": 0
                },
                {
                    "sent": "What do you think is the dependence?",
                    "label": 0
                },
                {
                    "sent": "Or like how reliable the inferences based on how many judges will see each example?",
                    "label": 0
                },
                {
                    "sent": "OK so I I can't hear if I get this right.",
                    "label": 0
                },
                {
                    "sent": "You're asking how reliable, like how many annotators that gets.",
                    "label": 0
                },
                {
                    "sent": "It needs to see each image and how many images each annotator need to see in order to get reliable estimates of both the image labels as well as the annotated labels.",
                    "label": 0
                },
                {
                    "sent": "Yeah, the first one actually.",
                    "label": 0
                },
                {
                    "sent": "Yeah, each image should go to how many annotators before we get reliable estimates.",
                    "label": 0
                },
                {
                    "sent": "OK, so it depends a little bit so some images.",
                    "label": 0
                },
                {
                    "sent": "So the point here is that some images will be more ambiguous and usually in order to get a reliable estimate of those you need to send them out to more annotators.",
                    "label": 0
                },
                {
                    "sent": "Well, images which you are very easy to classify, they will be.",
                    "label": 0
                },
                {
                    "sent": "You will be able to pick out that they have a very sort of their very not.",
                    "label": 0
                },
                {
                    "sent": "Not very ambiguous, pretty quickly and then you will only be able.",
                    "label": 0
                },
                {
                    "sent": "You will only need to send them out to like two or three annotators in that case.",
                    "label": 0
                },
                {
                    "sent": "So on average to like when we use this we usually set it out to between three and six annotators parameters.",
                    "label": 0
                },
                {
                    "sent": "So actually, like again, I'll have to look more into this model, but whitehills model can be shown to be completely not working if you're sending it to just two annotators like mathematically, it just like it just regresses to the prior so.",
                    "label": 0
                },
                {
                    "sent": "Sorry, can you repeat that so white white heels model can be shown mathematically to be completely not working if you're sending it to just two annotators, it just just regresses to the prior, basically to the most popular class and I'll have to look at your model in more details to see if that similar effect can be happening, but I would be careful with sending that to just to yeah, if you if you only sent it to annotate, it should be very careful like the point here is that usually the way uses kind of model is that you sent it out to several.",
                    "label": 0
                },
                {
                    "sent": "Different annotators that they sort of model your datasets.",
                    "label": 0
                },
                {
                    "sent": "They label your datasets partially, so not all annotators would get to see all of your images, and so on, and so they will.",
                    "label": 0
                },
                {
                    "sent": "Each label maybe 100 images each, and each image would be labeled by only between three and six annotators each, so that's usually the way you should use it if you want reliable estimates.",
                    "label": 0
                },
                {
                    "sent": "OK, so let's take one quick question over there.",
                    "label": 0
                },
                {
                    "sent": "OK, this is probably more common, but I'll try to disguise it as a question.",
                    "label": 0
                },
                {
                    "sent": "The observer model you have is basically the standard signal detection psychophysics model.",
                    "label": 0
                },
                {
                    "sent": "Now in psychophysics people have observed that it doesn't work very well and you need to add additional variables to account for things like lapses where people basically aren't looking at the screen and press a button anyway, but at other times are actually paying attention so that they're not completely random overall, but occasionally there's sort of a mixture model over 2 behaviors and also probably other terms to account for your adversaries that you didn't actually have in the model at the end.",
                    "label": 0
                },
                {
                    "sent": "Have you explored trying to add those on and do you think some of those lines would actually tighten up in your plots?",
                    "label": 0
                },
                {
                    "sent": "If you had those?",
                    "label": 0
                },
                {
                    "sent": "Yeah, so that's that's an excellent question and we in the task that we have used so far we didn't encounter.",
                    "label": 0
                },
                {
                    "sent": "Examples where we actually need to account for, say attention of annotator and so on, where they actually paid attention to task over if they just answered randomly or something like that.",
                    "label": 0
                },
                {
                    "sent": "However, now we moved on to other Council tax where this actually seems to be important and then in that case what this model will do is that it will see.",
                    "label": 0
                },
                {
                    "sent": "Look at your annotators as being very noisy in some sense and that's not very accurate.",
                    "label": 0
                },
                {
                    "sent": "You need to sort of pick out other attributes of your annotators, so there might be room for extending this.",
                    "label": 0
                },
                {
                    "sent": "This kind of model to include these kind of factors.",
                    "label": 0
                },
                {
                    "sent": "Alright, OK let's thank Peter again.",
                    "label": 0
                }
            ]
        }
    }
}