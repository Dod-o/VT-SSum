{
    "id": "ucd4s3gutrnpg4sterhhh7opfvklulpa",
    "title": "Grammatical Inference Vs. Grammar Induction: the Data or the Method?",
    "info": {
        "author": [
            "Colin de la Higuera, University Jean Monnet, St Etienne"
        ],
        "published": "Oct. 31, 2007",
        "recorded": "June 2007",
        "category": [
            "Top->Computer Science->Natural Language Processing"
        ]
    },
    "url": "http://videolectures.net/mlcs07_higuera_giv/",
    "segmentation": [
        [
            "Right, well, the first thing I wanted to say was something about the title, the official version.",
            "The first version was something about the data or the method I had chosen the tackling a desperate attempt to try and get the best title prize, which escaped entirely, as we've just seen 2 seconds ago.",
            "So that's the first thing.",
            "So the second thing is saying who?",
            "A little bit who?",
            "I am very different from probably a lot of people here I'm I'm somebody who works in machine learning on aspects dealing with formula.",
            "Grammars and trying to learn formal grammars.",
            "So somebody working in a field that we would call or a topic we might call grammatical inference.",
            "OK, so I'll probably be saying things that for linguists is going to be total heresy."
        ],
        [
            "So the summary of the talk, the summary of the talk is 2 parts I've divided in three because of for technical reasons.",
            "The first is why I'm going to try and convince you that what the game is about is not studying the actual grammars, but studying studying the actual methods.",
            "Sort of saying things about the methods that produce the grammars is at least as important as saying something about the grammars that have been produced.",
            "Having failed to do that, I will then pass on to the second part of the other topic, which is to give you some of the ways we do use.",
            "To try and say things about the methods so we've got two settings.",
            "One is the usual exact setting, the one where we say we've got no probabilities, and this is gold oriented learning and the second setting is the probabilistic setting, because then you've got a variety of settings depending if you want to sort of use the distributions or learn from the distributions or learn this."
        ],
        [
            "Missions themselves.",
            "So let's go on to the first part.",
            "Why study the process of learning and not the result itself.",
            "So return to dramatically influencing the grammatical inference is usually seen as something like building a grammar.",
            "Sometimes an automaton, small or smallish and adapted in some way to the data from which we are supposed to learn from so very vague definition.",
            "There isn't such a thing as an official definition of what this is is.",
            "Then authors are going to say."
        ],
        [
            "Different things in different moments.",
            "If I want to even make it shorter, I get through something about like this.",
            "It's about learning a grammar given information about a language, so the three important words here are the grammar, the language.",
            "So the information we're getting is from the language, and we're trying to learn is a grammar with different things happening there, right?",
            "Because there are different grammars, there's in decidability issues, and well, sometimes we say we're learning a language, but in fact, behind that there's a lot of tricky things when we get onto the machine, and we're thinking about.",
            "Computation, the third word, which is in a different color, is the word information in the sense that I don't really because we're working on grammatical inference, not just for you, or we should be working for.",
            "Not just you were working in general, in the hope that what we're doing can be used in bioinformatics can be used in web mining, can be used in robotics in a number of fields.",
            "Well, the type of information that we may have to learn a grammar can be very, very different.",
            "We may have not just.",
            "Strings we can have counterexamples.",
            "We may query to be able to test things we may be getting prefixes of sentences.",
            "We make me just getting chunks of a sentence, and all this means that we have different rules of the game in which for each one of the games, we're going to have to find specific differences."
        ],
        [
            "OK.",
            "So that's grammatically inference, and what's grammar induction?",
            "Well, basically it's about the same thing.",
            "So as far as words go, where left with the same definition, and so my title is that all."
        ],
        [
            "Not explicit, so I'm going to try and make it explicit.",
            "Doing in a very brutal way the sort of diagram you've been showing me yesterday, which is the way of the one of saying we've got data on one side.",
            "We're hoping to get the grammar in the middle, and here we've got somehow a process that from this data is going to extract a grammar.",
            "So if I think about grammar induction, this is what usually happens is I'm very much interested in the grammar that I'm obtaining from the data, and I'm going to try and say something intelligent about the grammar that I have obtained from this data.",
            "OK, and when we're concentrating in grammatical inference, right?",
            "Well, we're interested in this object in the middle, which is the actual process that has been doing this in order to abstract from the grammar and the data itself and hope to be able to say that the very next time I use this algorithm on some new data, something interesting is going to happen.",
            "I should say at this point that.",
            "Probably being in all field giving opinions is probably much more well, less used to doing it, so I'm more used to giving talks saying now this is a mathematical theorem, so here I'm taking wide risks and saying this is what grammatical inference is about.",
            "Do not go out and go and see my colleagues in the field saying gotten really have some strange opinions they don't.",
            "This is what I'm claiming."
        ],
        [
            "OK, grammatical inference being somewhere there.",
            "So let's try and motivate this with a highly provocative example.",
            "Suppose your task isn't about learning grammars, but your task is about designing a random number generator.",
            "OK, you better find a way to design a good random number generator now.",
            "Then the question being that you've used it and you've come out with 17.",
            "So are you happy with 17 or you're going to be able to say something about 17 and then is 17 more random than 25 because somebody else is random?",
            "Generators come out with 25 OK, I mean obviously you can discuss that.",
            "This is very unfair.",
            "I tried to be very fair, in fact.",
            "I mean, I really did run a random number generator for this example and just to make sure I really repeated the experiment until I got 17 twice."
        ],
        [
            "Joke OK from numbers we can go onto strings and the issue about strings of having at one point you're going to put in a seed and you're going to get at the other end another string and you're wanting to say something about this.",
            "And I'll already tell you that the real problem is going to be if you use your random generator and it comes out with some really.",
            "Obviously UN random.",
            "Now that sounds horrible.",
            "Some number likes of 999999 and you're left with that number and now you think now what do I do with it right?",
            "I'm sure most of us will be tempted to say, well, I'll run another time and get a rule random number.",
            "Not this one.",
            "OK and here the point I'm trying to make is that if I can't say something really intelligent about the algorithm that is proposed, it that has produced this random number, I'm going to have a problem."
        ],
        [
            "So now then here we go, the next step along step.",
            "This time we're given not random number were given a sample some strings and we've given a grammar and somehow we're saying this is the grammars that we've learned from this sample.",
            "OK, this is the grammar, but then there may be somebody ask me.",
            "Of course another grammar G prime saying no, it's this one right?",
            "So the question is, what shines the best and what can I say about this?",
            "And how can I say something a little bit founded?",
            "About which is the best or which is going to be the best for getting.",
            "Obviously all the perfectly acceptable arguments you will have from an empirical point of view or from a psychological point of view, and believing in a certain order things and holding an enormous culture that I don't have.",
            "So I'm just looking at it from a mathematical point of view."
        ],
        [
            "So back to the definition, we said grammar induction was about finding the grammar.",
            "A grammar from some information about the language."
        ],
        [
            "So once we have done that and we've said that, what else can we say?",
            "What else can we say that would be mathematically grounded?",
            "So what we would like to say, or which we often say, is that the right if I can.",
            "Learn the smallest grammar, the best fitting grammar, the grammar that is best for a fitness function for a scoring function then that would be a good idea.",
            "I mean is based on a long discussion.",
            "Apart from that, as you said in 1957 says that that is what you should do, but pose from that.",
            "What is happening is that we are transferring cognitive problem into a combinatorial problem.",
            "OK and hard combinatorially problem in most cases because we know that.",
            "The combinatorial or the complexity questions that are going to be behind this are usually going to be intractable, difficult to solve.",
            "That is why finding the best grammar or the best automata you are having so many huge problems to find.",
            "It's not big cause of because of the intrinsic complexity of the task.",
            "So what we would really like to say is that to be able to defend that point of view is to say that actually changing from a cognitive problem too.",
            "Amatoriale problem has been a good idea.",
            "Have been a good idea in the sense that we are converging towards something that using an algorithm that actually does this solve this combinatorial problem is really going to solve the problem we were interested in the first go.",
            "OK, so this is where we usually get into things like oh come arguments, compression arguments.",
            "If you can compress them, that's good.",
            "Then you're learning MDL arguments.",
            "We saw yesterday component, or of complexity.",
            "All these things.",
            "They are not technically the same.",
            "They're all very different, but.",
            "They're all arguing about the same idea of small is beautiful and behind the idea that that somehow.",
            "Somehow this is actually linked with getting somewhere OK.",
            "The idea is saying that if you do find the smallest, then that smallest grammar has the following desired properties.",
            "Mathematical properties, for example, of minimizing samara or identification in the limit, or things of that sort.",
            "But we must be careful with the fact that we're actually solving a little bit a different problem here."
        ],
        [
            "OK, what else might we want to say?",
            "Well, I mentioned there is a 2 seconds ago what we might want to say is that we can actually use this grammar in the near future and probably best of cases would be to be able to get 100 pounds on the outcome.",
            "And saying, well, I think the next string that will appear is this or I think the next string.",
            "I will classify it correctly.",
            "OK, this would really be nice.",
            "This is something that machine learning has been dealing with quite successfully, not in the field of grammatical inference, should I say.",
            "But in the fields of.",
            "A feather of variables and numbers and well, all sorts of other things not related to grammatical instance has been proving little bit too difficult for this.",
            "I'll say it would about this.",
            "This is usually called PAC learning OK, you're going to say something and say OK. Then, if whatever I've been doing now, sorry if the sampling process I have used to sample and to get hold of my examples from which I have learned is to be continuous, meaning the distribution is going to be the same.",
            "Further on then I can actually.",
            "Gamble some money on this and I know what is my hopes of losing or winning that money.",
            "But that is a nice."
        ],
        [
            "Things we might like to say.",
            "There's a third sort of thing, much less ambitious that we would like to say, and clearly and grammatically, inferences is often saying, and for that it is criticised.",
            "So I'm going to defend that.",
            "I'm there.",
            "I'm here for that.",
            "Is that perhaps can I at least say that when I use this algorithm, there's going to be an outcome for which I have no guarantee that it works, but at least I can blame the data and not the algorithm.",
            "OK, what does that mean?",
            "That means that either I can say look if at the end, whatever I'm presenting you isn't good.",
            "It's because I didn't have enough data, or because the distribution was completely biased or becausw.",
            "The sampling process went wrong somewhere or something of that sort.",
            "OK, so it's.",
            "Think of, well, you know this is no use at all.",
            "The problem is that if you don't even have that, then you've got a problem.",
            "If you can't even say you know it's not the algorithm that is to blame, it's the data.",
            "If you can't say that, then if your algorithm is doing something wrong, then you have."
        ],
        [
            "I think a problem.",
            "This is what?"
        ],
        [
            "Well, so let's have an example for that.",
            "A typical example that we will recognize.",
            "Suppose that you're using one of the many algorithms that is trying to learn some context free, non probabilistic grammar from some data positive data.",
            "So you get your strings there and what you're going to do is we're going to bring in.",
            "These strings are going to start with sort of just a very general grammar that accepts everything a little bit.",
            "Just one rule per per string, and you're going to apply iteratively 2 rules, one of them consistent saying, oh look, these two nonterminals.",
            "Looks similar right with similarity based on MDL or on whatever you want.",
            "So we merge them and then there the rule is to say, well perhaps here what we can do is do some splitting which we got a rule that is too long and we can make 2 rules out of it."
        ],
        [
            "So typically it's something like this, right?",
            "The first rule just saying that here we take these two rules and they become three because we recognize objective noun, for which we can actually do a special rule."
        ],
        [
            "And the second one being this, which is saying that AP one and AP two are sufficiently similar.",
            "That's not up to the question here.",
            "To know why, so we merge them into just AP one.",
            "And by doing that we are."
        ],
        [
            "Generalizing and learning.",
            "So what happens or what is bound to happen?",
            "I mean, just hear what usually happens, but then in the implementation you have to going to do tricks to avoid that.",
            "Is that what you're going to do is you're going to actually learn a context free grammar, but the underlying language is going to be a regular, so let's just wait two seconds.",
            "What I'm saying here is that it's not because I'm clearly for the same regular language.",
            "You can have many grammars.",
            "You've got the regular grammar, but you can also design very easy.",
            "Easily, some very nice context free grammars which look all the context free with nice trees.",
            "Very balanced with all sorts of things, but at the end of the day when we're looking at it, the typical, let's say brackets or Dick languages or those sort of structures that we normally think we're finding in context free languages do not appear OK, and I've seen this in a various number of algorithms out there.",
            "They just don't appear.",
            "So then you're going to do a lot of tuning to be able to force some rules not to be applied to be deferred in order to try and discover.",
            "Sort of the closing bracket.",
            "OK, so this is what we call a hidden bias in the hidden bias here will mean the fact that we're actually claiming that we're learning context free grammars, which we are ready, or that we're working on the class of the context free languages.",
            "But in fact we're not.",
            "We're really reducing, and we're only working only attempting to learn regular languages.",
            "OK, I mean, fair enough.",
            "Once we if the if the what you call it.",
            "The hidden biased becomes the declared buyers by some game."
        ],
        [
            "That's nice.",
            "OK, so now then so OK, we know what we don't like.",
            "So what do I like?",
            "Well what I like is to say that we need to target, OK.",
            "So I need to target what does it mean that I need a target?",
            "I need something hidden there that I suppose is the ideal solution and the question is going to be a studying the convergence of my learning algorithm given some data towards this ideal solution in that it is what I will call an inference process."
        ],
        [
            "What is bold?",
            "OK, so now then yes, let's go to the questions to the yummy.",
            "But what if you don't believe there is a target, right?",
            "So there isn't.",
            "There's no such thing as a regular language for the natural language or context free language.",
            "For context, free grammar for this well.",
            "First of all.",
            "You've got a problem anyhow.",
            "If you're saying I don't believe there is a context free grammar, and nevertheless you're still using an algorithm to learn a context free grammar, you're facing the same problem in any case.",
            "Second thing is that we're really not.",
            "We're using it as a device to measure the quality of the algorithm.",
            "We're not going to measure this target to know if you know whatever we've found is correct or not, but in practice we're never going to have the target.",
            "OK, so since we're really studying an algorithm that is not going to be too much of a problem, and then the third thing is that anyhow, right?",
            "This target is a bit of a hidden bias.",
            "Saying not to hit the declared bias.",
            "I believe in a target, but if not, we're going to come out with another bias which.",
            "Maybe better, maybe worse.",
            "I'm just proposing to substitute one bias by another."
        ],
        [
            "OK, so if you are prepared to accept there is a target but.",
            "Then you're thinking that that's it.",
            "I mean, if the target is known, what is the point of learning true?",
            "And if the target is not known, what's the practical point of this?",
            "Where as I said, it's not a question of practical points.",
            "Is question of saying I'm going to study offline in certain way my algorithm and then just hope that what's going to happen today?",
            "I don't have a target is going to be similar to what's been happening in the days I did have a target."
        ],
        [
            "So yes, if you're not doing that, if what you're doing is saying and I refused out of target, I've got the data.",
            "What really matters is the data I've got work from the data upwards towards a grammar.",
            "Then I think you're doing grammar induction.",
            "Now.",
            "This is probably not a distinction that we will find in this field.",
            "You'll find this distinction much more made in a field of pattern recognition, which also uses grammar induction or grammatical inference, and for who the data is, what matters, as I'll see, I'll show you in a moment.",
            "OK, so.",
            "It's in that case.",
            "I mean, yes, you go from the data and you really need to say something about whatever you found that day on."
        ],
        [
            "Item of data.",
            "So careful anyhow, if you don't.",
            "If you don't want to do what I'm saying, there are some things that actually I was inspired by a paper by Alex that was rereading yesterday, but you can't come out with saying OK about an algorithm about a learning algorithm saying I am attacking the curb.",
            "My algorithm can learn A&BNCN this language, which is a typical way of showing I've got a really powerful learning algorithm.",
            "It can learn this language.",
            "I mean, any algorithm.",
            "Can my anger and saying you know this is the language has learned it because there's just one language?",
            "Learning is distinguishing between very very many possible languages or possible grammars, which is the one that is which is there very, many meaning possibly and probably infinitely many same as saying a good quality of an algorithm and say look, I can actually with just two examples, learn this rule.",
            "There's no free lunch if you're doing that, is there probably there's aspects of the learning process that you're missing entirely?"
        ],
        [
            "So the compromise is that when I'm proposing is to say that you only need to believe there is a target while evaluating the algorithm afterwards.",
            "In practice, the fact that there isn't one isn't isn't an issue.",
            "The issue is that if you've decided that you're using algorithm to learn context free grammars and you know Jolly well that there is no such thing as a context of grammar for what you're doing, you're having a problem anyhow, I'm not helping you solve it, apart from the fact of dealing with noise and distances between things.",
            "I mean, that's much more."
        ],
        [
            "Nickel complex issue.",
            "OK, just to end my provocative example right.",
            "If my room a random number generator gives me some unusual number, some very compressible number, then if I want to be able to keep that number not rerun it well, I really want to say something about the algorithm."
        ],
        [
            "Right, I don't see it.",
            "Just three things to just summarize what I've been saying here, so grammatically inferences about measuring the convergence of the grammar learning algorithm in a typical situation.",
            "OK, it's about saying something about how the learning algorithm is going to converge toward."
        ],
        [
            "Is the target.",
            "Typical can be one of typically two things in the limit saying, well, I'm going to get more and more examples.",
            "I'm hoping that one day or another this is going to achieve reach success or probabilistic where there are then two variants, one of which is to say that I'm going to use the distribution to talk about errors and there the one where I'm going to say, well, that's what I'm looking for.",
            "I'm looking for the distribution itself."
        ],
        [
            "And the third credit.",
            "The third thing I believe in very strongly is that we shouldn't just concentrate on.",
            "I've heard things yesterday about this.",
            "We couldn't concentrate just on saying, you know this will eventually learn one day.",
            "But complexity considerations are essential.",
            "In the previous talk.",
            "For example, while the complexity behind everything was just so hard that you can only approximate and then you're facing the problem of not being able to say enough.",
            "Because even if you have covered things, there's still things out there left that you don't know where they are not.",
            "So complexity is very important and there are, and that's where I'm going to come to.",
            "In the second part of the talk, there are many ways of measuring it, and many results out there and many techniques that have been looked.",
            "You can start counting well, how long will it actually take in the worst case, how?",
            "How long does it take to update from one year, but assistant to the next level with a new example being added?",
            "How many mind changes is my algorithm going to make sort of?",
            "I like my hypothesis up to now.",
            "Oh no.",
            "Oh dear, I have to change it.",
            "How many prediction errors it makes?",
            "Meaning up to now everything was OK, but now in you example is contradictory with what I had, so I'm having to change my mind.",
            "So theory and algorithm that doesn't change its mind very often is or can be considered as good and the number and weight of errors in the case where we do have probabilities, all this can be measured and limited.",
            "So this is what I'm believing it and what's not being said here and I'm saying it now in case I forget in the conclusion.",
            "Is all this means that there is also hidden behind this very active number of researchers.",
            "Working on finding new algorithms because once you've seen that there's a problem that, for example, context free grammars, do we know?",
            "Do we not know if we can learn it this way?",
            "Well, that's a challenge for people to try and devise new algorithms, and that's what's happening.",
            "There is a real problem, clearly, and getting these algorithms out of full field, but I mean, know that they're there."
        ],
        [
            "OK.",
            "So let's go through the non probabilistic setting and just see a little bit the different ideas, main ideas that are behind this.",
            "I have eliminated all mathematics.",
            "I'm very proud of myself.",
            "Which means probably that you won't understand it because somehow it's difficult to tell like that.",
            "Anyhow, some of them you do know already just that in the way I'm going to say it is perhaps a little bit different.",
            "So typically is a non probabilistic setting is working on identification in the limit with the fact that we will be interested in talking about resource bounded.",
            "Identification in the limit just saying identification in the limit is not going to be good enough.",
            "We really want to be able to say something that is typical for a computer scientist, which is to say you know the means we are allowed to use.",
            "Our only typically polynomial.",
            "OK, and I won't have time to talk about this.",
            "This is a very reactive newly active field.",
            "The one of query learning where people are actually allowed to ask questions in an interaction type of situation.",
            "OK, and then the question of language learning where there's communication between beings.",
            "I mean this is worth looking at for new algorithms which is."
        ],
        [
            "What we don't have there we don't as I'm going to say you may, but we don't.",
            "OK, identification.",
            "The limit giving a bit domain definitions, just saying that there's alternatives that are tricky technically, whether you're using the order or not of the data that is arriving and whether you are allowed what we will call randomized algorithms or not randomized means I'm allowed to also toss coins inside my my algorithm."
        ],
        [
            "Much learning.",
            "So the presentation the presentation is just data that is arriving to my learner one after the other.",
            "Right data being basically anything X we haven't seen symbol X up to now.",
            "A symbol X is, well, some set of information that we are getting.",
            "You can think typically of text, which is probably what you're interested 99% in.",
            "But you can think of more generally data information about the language that is reaching you.",
            "So what we're really hoping is at the end of the day, the end of all days.",
            "Should I say the actual data, right?",
            "Sorry, the fact that you've seen the same data in two different ways corresponds to exactly the same language, so it's it may be order dependent up to a certain part, but in Infinity, if the following holds, if the same data is what has been presented, then the language behind."
        ],
        [
            "This is the same.",
            "So we have got a learning function.",
            "The learning function is an algorithm which is given the initial bits of the presentation.",
            "So right FFN is the first elements of the presentation and it's a function that takes as input.",
            "This set and returns the grammar.",
            "Here's the bit where you when I write this and deciding that the algorithm is not randomized, and so it may be a little bit messy, so I'm taking the decision which returns a grammar every time a new hypothesis given a grammar.",
            "L of G This is the naming function which allows to relate the grammar to the."
        ],
        [
            "Language and all this gets into a nice little magic triangle.",
            "I've got a class of languages L which is related to a class of grammars G in such a way as I have got a naming function and here is where to be fair and to avoid problems we saw earlier.",
            "We really want them to coincide Wonderland grammars to coincide with the Lacrosse Flyer languages.",
            "A firm, if not, it's not.",
            "It's not really fair.",
            "So then on this I've got close languages, was related with the set of presentations.",
            "As we saw through yields, which is an implicit function.",
            "Obviously not, not.",
            "You can't compute it.",
            "And we've got the property we saw earlier.",
            "And now we relate through the fact that given initial bit of a presentation learner takes bits of presentations, returns of grammar and the whole thing should close down beautifully into the following formulas, say that there is a points for each presentation, each language, each for each presentation, each language there is a point where from that point onwards, whatever is returned by the learner corresponds exactly to what we were looking for.",
            "OK, so it all just closes down.",
            "So this is theoretically we do know that in practice you are not running a learning algorithm every time you get a new example.",
            "Nevertheless, it is the best way to study the process in this way rather than saying OK, I've got everything.",
            "One goal, you can transform the."
        ],
        [
            "Different settings.",
            "So what about efficiency?",
            "Once we've said that, so this is just setting where we know the problems with the gold.",
            "The gold aspect is saying the one day we will converge whenever that is, and we also know that we don't even know when that day arrives and when we are that day, we don't know that that day has arrived.",
            "To be fair, there are people working on these questions and have developed all sorts of cunning little.",
            "A variance of gold type learning.",
            "There's an active community made of specifically Germans and Japanese.",
            "OK, so it's a conference call DLT that is held every year and so working on inductive inference, and they still actively looking at those things.",
            "So what can we try to bound here?",
            "We can try and bound the global time, so how long it takes to actually converge?",
            "That would be nice, but since you do have got no control over the examples, that never works, you can bound the update time.",
            "How long does it take me to get from one example?",
            "Sorry from one hypothesis to the next one.",
            "That's usually very easy to do in polynomial time, so that's normally doesn't get anywhere.",
            "We can try and bound the number of errors before converging, meaning that if."
        ],
        [
            "Will like this, sorry.",
            "I should have showed this writer though, so this is the process I'm getting my examples one by one sort of accumulating in the set and coming out my with my my hypothesis and hopefully as I said at one point I am not going to change anymore and everything is OK, so there is.",
            "Well, there is means that one moment the new example that arrives is inconsistent with whatever I had here.",
            "OK, so the amount of times that happens isn't very nice.",
            "We want that to diminish.",
            "So we want to count that.",
            "We can count the amount mined changes, which is how many times have I had had an obligation to change this right?",
            "You can say, well I never changed, but then the algorithm does have the obligation to identifying the limits.",
            "So you do measure a very specific things when you're doing those things.",
            "Um?"
        ],
        [
            "You can measure the queries in a case where actually instead of just receiving the examples, you're asking questions.",
            "You're asking for things you're asking.",
            "Is this string in the language?",
            "Is this string not in the language or things like that, and you can bounce something that is called characteristic samples, which corresponds to the good examples.",
            "Let's get into the best case right where somehow we've got a teacher that can actually choose the examples that are in the in the learning sample.",
            "Will that help?",
            "How long will it take me to do in order not to be 2 conclusive?",
            "Will say the teacher is going to put these examples and somebody else is going to add some other ones right?",
            "Just so that we can't distinguish which other ones of the teacher which are the others?",
            "Because if not this some tricky factors there.",
            "But basically, do we in any House needs such in any in any case needs such an amount of information that we're never going to be able to reach it.",
            "So however good my algorithm, there's no way I'm going to be able to do it or not.",
            "That is actually one of the big differences between the regular languages, though Automata for which that's not the case.",
            "Very short sample of strings is always sufficient to be able sufficient to be able to be sure to learn, and the context free where there are context free grammars out there for which the size of the size of this characteristic sample has to be so huge, so exponentially exponentially high that you just can't do anything about it.",
            "So there is tricky bit there."
        ],
        [
            "So I didn't say things about this that I need to measure things we also have to be careful with some, not just technical details.",
            "With this more into it, I mean how do we count things?",
            "What do we count when we want you to do computation we have?",
            "We have to know what we're counting.",
            "So typically the size of a grammar we saw on an idea about that in a loose way, we can say that last the size of the grammar has to be linked with the size of its encoding, the number of bits we need.",
            "So then you find a formula formula of saying.",
            "Now if I count.",
            "This and that, for example, the length of the rules, or multiply the length of the longest rule multiplied by the number of rules.",
            "Then I'm safe, you know, and the polynomial is safe.",
            "If I do that.",
            "The size of a language that size of language.",
            "Typically you would say, well, it's the number of strings.",
            "It doesn't work.",
            "Is that language is infinite, so the size of language is typically in these tasks the size of the smallest grammar for that language.",
            "Since we've got many different equivalent languages.",
            "The size of the presentation now the size of the presentation in the gold sort of thing is something that is not clear.",
            "I mean clearly a presentation is infinite, and what we'd like to say is the number of the size of presentation being the number of length of the point, the convergence point, the point upon which there is enough information to decide this is the correct grammar, but it's doesn't work exactly because we're really algorithm there.",
            "Dependently still research to be done all that to be able to express the idea.",
            "That you know at some moment we do have enough that certain presentations are more favorable, because now we've got it or we don't have it.",
            "The size off, not a presentation in its whole, but of the end.",
            "First elements of the presentations you will always say obviously say N doesn't work either, because the strings were working for the phrases are of arbitrary length, so you have to sum up all the lengths of all the strings in your sample to be able to work on.",
            "OK, so."
        ],
        [
            "We've seen all this, I just I just give some selected results just so we can see the spirit of the thing.",
            "So DFA, which you're not interested in, but I mean it's the same biggest context, free is more important for computational linguistics.",
            "This just gives the picture so clearly we do know the basic result, the text, whatever you're counting, nothing happens, but you just can't learn them from text.",
            "OK, so it's not even a matter of seeing if resource boundedness is going to be in this case or not, but if we look at learning from an informant, so both positive and negative examples where we do know that the runtime is cannot be bounded polynomial, either update time is easily.",
            "So the number of prediction errors is not polynomially bounded.",
            "And I was trying to do this slide yesterday, so I preferred putting a question mark here, not as an open problem, but as I don't know, OK?",
            "On the but the size of the characteristic samples is polynomially bounded, meaning that just a small quantity of information is sufficient to be sure that whatever the rest of information is in the learning sample, we're going to find correctly.",
            "The Panorama for the context free grammars is."
        ],
        [
            "Same with the difference that this yes is becoming no and which makes the task that more difficult."
        ],
        [
            "And just becausw.",
            "I was hearing things about edit distances yesterday.",
            "We've been working on trying to look at all these things for some, not just languages in the sense of grammars, but languages as a sense of topological balls.",
            "You take a string and you just draw a ball around that string with the edit distance and you ask yourself, can we learn these things?",
            "And then is amusing things, like for example, that actually it's easier to learn from text than from an informant.",
            "It is tricky, bit there it's easier to learn with less information with more.",
            "It's not what I'm really saying, so the way we're counting, basically you're only going to make 1 sided errors if you're learning from text right?",
            "And you're going to make 2 sided errors when you're learning from an informant."
        ],
        [
            "OK, not doing badly right?",
            "So that was the, so there's still research being done.",
            "People are trying to understand best of these phenomena, and even if we can agree that goal style learning is not the ultimate response is not the thing that is going to save our lives.",
            "I repeat, my argument is when you don't have it that you've got problems.",
            "OK, it's when you can't say that the characteristic sample is polynomial.",
            "Write for example, that you've got problems because you know that.",
            "That means that there is hidden there somewhere.",
            "Some possible targets and possible grammar that you would think that you could learn and that you just can't learn.",
            "OK, so that's what it means.",
            "So the positive bits of saying I can go learning are not very, very strong.",
            "There are things relating with probabilities.",
            "If you're saying the characteristic sample is small, usually that will mean that they are small strings for which in certain natural.",
            "Distributions will have high probability, so you can actually put something on saying hey, the probability that those nice strings appear is possibly higher than the others.",
            "So I've got a reasonable probability of seeing them reasonably early.",
            "OK, these things requiring more than handwaving.",
            "So, so the probabilistic setting here very quickly.",
            "So there are, as I said, 2 two ways of saying you're seeing things, and I've divided into 3 becausw where you see in a second, so using the distribution to measure error.",
            "So you've got a distribution we're going to suppose that this distribution, although it is unknown, it is fixed and it is not going to change with time.",
            "Again, a bias, right?",
            "If you think it's going to change well, then you've got a different things to be done.",
            "So we've got the same distribution in the moment we're sampling and further on when we're going to use whatever we've learned to do something with.",
            "So we can do some one of the following week and try and use this distribution to measure errors because now there is a probability for a string to be sampled.",
            "We can also try and identify this distribution.",
            "So follow the gold course and say well I really want to identify these things and then you can try and approximate the distribution winning well you know are probably too ambitious to actually learn the exact distribution, but if I could learn something sufficiently close using the same sort of error measures as earlier on."
        ],
        [
            "That would be nice.",
            "So the probabilistic, so they correspond.",
            "Sorry, this was meant to be going like this, and then you go back and you see that the first one is usually called PAC.",
            "Learning the second one is called Pack ident.",
            "Sorry it's called identification with probability one and the third type of results are called."
        ],
        [
            "Learning distributions.",
            "So we we have a distribution, we sample typically twice, once to be able to get the examples and the second time later on to be able to measure somehow.",
            "If we've done well, or even if we don't do it, at least that's what we supposed to do.",
            "The second thing, so the pack setting corresponds to probably approximately correct, and corresponds to probably the most important setting in machine learning, or the most inspiring."
        ],
        [
            "PAC Learning, as I said, Valiant 84 just requires a certain number of para meters and I'm just going to spend a minute on them.",
            "In case you've never seen them, we've still got all the class of languages or class of grammars and we've got 2 little epsilons and Delta here, which are important, right?",
            "That we are going to choose.",
            "We're going to decide how much error we're going to allow ourselves in this process.",
            "And we've got some technical nuisances, like saying, well, all this works very nicely on finite things.",
            "And here where we have to bound the length of possible strings on one hand and the maximum size of grammars on the other hands.",
            "Technically speaking, it's an absolute nuisance.",
            "Practically, you can both estimate these two things, given also some sampling or the epsilon and Delta.",
            "And things like that."
        ],
        [
            "So what we're going to call polynomially PAC learnable is that we're going to say that we've got a very nice algorithm that can be randomized that will sample reasonably reasonably means the polynomial number of times only right, not more than that.",
            "You can't suddenly want to gain that unreasonably, means just putting your hand in, not chewed, not being choosy there.",
            "If you're choosing, you get another model and returns, then with probability at least one minus Delta, something that will make at most epsilon errors.",
            "So why there's a Delta Y?",
            "There's an epsilon.",
            "Well, basically when you get to learn something it is not going to be perfect, so it's going to make errors and we just want these errors to be not too many, so that's the epsilon bit under Delta.",
            "The Delta corresponds to the fact that you may be completely absurdly unlucky during the sampling process.",
            "First, one or second one right, and this sampling that has gone completely wrongly, while means that your error is near not gonna be epsilon anymore.",
            "But Luckily, the probability that you're sampling process has been completely degenerate is not high at all.",
            "So we can just put that inside the bad bit we put inside a Delta.",
            "So that's the sort of results that are nice because you do see how you could actually.",
            "If you've got that, use it to to put some money on whatever."
        ],
        [
            "So you're using.",
            "But the results in our case in the case of grammars are negative.",
            "We don't know how to do anything in this setting.",
            "Alright, so this is learning a classifier that says we're going to decide if string belongs or not to the language, so the results come to say that generally speaking, even for the case of DFA, you can't do it and you can't even do it if you're allowed to do a bit more than that and ask membership queries, which is ask some specific queries in the in the lot, that's nasty."
        ],
        [
            "OK, so since that doesn't work, the way of using distributions is something you're more familiar to is to say.",
            "Well, we're actually going to learn the distributions them so."
        ],
        [
            "Elves.",
            "So making no error means that we want to do identification in the limit with probability one.",
            "Again, this is based on the fact that the sampling process can't go indefinitely wrong, right at some point or another things come to stabilize.",
            "You got some nice theorems linked with actually with the random numbers we saw at the beginning, which come to say that one day or another you know, even if you can't say when things will tend to be able to say things, how bad you're going to be.",
            "But the sampling is gone.",
            "It's not going to be that bad.",
            "So that means that the probability same probability one really means that the probability of not converging is zero.",
            "OK, zero probability of."
        ],
        [
            "Far off not converging.",
            "So what can we do with this?",
            "But they are all well.",
            "There are actually some some results.",
            "Very surprising.",
            "There are a variety of algorithms to learn regular languages.",
            "This is from my previous speaker, so I'll give those.",
            "But there are a number of algorithms that have come out that learn both the structure and the probabilities, and do that with the proof that they really identify in the limit and they normally work also in polynomial time.",
            "But of course Alas, should I say we do not have any nice convergence results saying?",
            "We're sure pack type results saying we're sure that they have worked OK, so at least we do know they're going to some things under happen, but they still are used in practice in in quite a few fields around 'em pattern recognition, so I can't say anything in these cases about bounded resources apart from the fact that at least they run in polynomial time.",
            "Now, if it's not true, there is 1 result and I'm being unfair to Alex, who's here that they do have Alex and Falk do have a result where they can by putting some.",
            "I say some conditions on the type of probabilities or the number of probabilities on the on the different States and some technical items like that they do obtain results saying we do obtain this reasonably quickly, but they've got better ones, at least ones are more."
        ],
        [
            "Parable more understandable and my view right is to say well if we accept to merge the different settings and we say we want to again learn a distribution so regular grammar or a context free grammar and this time we were not interested in perfection.",
            "What we want to do is make some errors from time to time, right?",
            "The famous 1 minus Delta and so we accept to make epsilon errors.",
            "So then we're facing a little, not just.",
            "Not just purely syntactic trick with it's very profound meaning.",
            "The problem which is I have to decide what distance I'm going to use you going to actually have one distribution which is the target.",
            "Again another distribution, which is whatever you're learning your hypothesis and you're going to try and have to build something between the two and say how far apart are they went from the other and there there are variety of notions of distances that can be used.",
            "And of course depending on how harsh or distances.",
            "Well, you're going to end up by having."
        ],
        [
            "Results saying in one case for one distance is too easy for another distance is too hard and there is.",
            "There is a lot of ongoing work on this, and I suppose that asking me or certainly asking Alex about these things.",
            "For anyone interested.",
            "I mean the people who are coming out with algorithms that are capable of PAC Learning PAC Learning.",
            "Distributions, usually with some concerns, syntactic constraints also on the type of probabilities you can have on the automata, so they can look at that."
        ],
        [
            "OK, I think it's about time I can I conclude.",
            "So for those that are not convinced that there is a difference between grammatical inference and grammar induction just just just out of the."
        ],
        [
            "Xbox or the text references.",
            "This is a notion that very often appears.",
            "I don't know.",
            "I haven't seen it around here.",
            "The idea is that one of its called structural completeness.",
            "Structural completeness relates to the facts in the case of a DFA I'm giving it because I can do a picture, but you could do the same with the context free grammar of saying that on one hand you've got a sample.",
            "On the other hand, you've got an automaton, and you're going to say that there is structural completeness when basically nothing is invented right, whatever edge or whatever final state.",
            "There isn't the automaton.",
            "This is justified by something.",
            "In the.",
            "In the.",
            "Or if you are in the sample and typical algorithms are doing, the following is itself looking for any DFA.",
            "They're only going to look inside the set of enormous set of all the DFA where the sample is structurally complete.",
            "Now what's so?"
        ],
        [
            "Just the picture to show it first, so this would not be structurally complete because for example if I decided to learn this well, you could think well where did you get this edge from?",
            "You know how did you invent this B here?",
            "If you look at each one of those items on the sample, so we're entering the automaton here and we can see we can do a AB, we can do B.",
            "We can do a ABA Ann BB.",
            "Oh dear, very bad a. I'm sorry a BA.",
            "OK so you never use this one.",
            "So since you don't use this one well in a way you've invented it out of the blue.",
            "So there's discussions about if you should do this or you shouldn't do."
        ],
        [
            "But what's curious is this is that when you read people writing about this summer, going to say that the automaton has to be structurally complete for the sample.",
            "This is what I've just said 2 seconds ago.",
            "OK, we've got this sample and we're only going to look at the set of automaton automata that are structurally complete for this sample, so that's what is the grammar induction OK, and the other one is to say that now this is the target automaton, so I am.",
            "Only going to be able to learn when the sample when there is going to be a sample that is structurally complete for this automata in it.",
            "OK, so it's due 2 dual points of view of the same problem, but that's where I'm putting a bit.",
            "The difference between grammar induction and grammatical inference."
        ],
        [
            "So so many things I haven't talked about.",
            "There's a lot of work on new algorithms and certainly a lot of work to be done for these algorithms to be known.",
            "There are many other applications, so if one of the nice things is that is that when we're working on an application on biology, sometimes there's something wrong with the algorithm where we should be coming to listen to you to be able to think one second, this might be of interest for these guys, OK?",
            "So machine translation, computational biology, NLP software engineering.",
            "One of the nicer tasks I saw last year was somebody in India who they were having to do some sort of reverse engineering task where they were.",
            "They had these programs written 20 years ago in some bizarre dialect of C or some basic or something, and for which there was no more grammar left.",
            "OK, so they just had the.",
            "The program and they didn't have that dialect, so they then had to reconstruct the grammar for this dialect right with probably knowing the grammar of a close enough dialect.",
            "That's sort of applications you find."
        ],
        [
            "Web mining robotics an just that people interested in this.",
            "There is a series of conferences on it called I CGI and next one should be in Brittany in 2008 and I cheated instead of giving one page abstract I gave 5 pages so there are a few references there and if not there are references in the grammatical inference webpage."
        ],
        [
            "And I'm done.",
            "This is just in case you'd ask me questions about technical issues."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, well, the first thing I wanted to say was something about the title, the official version.",
                    "label": 0
                },
                {
                    "sent": "The first version was something about the data or the method I had chosen the tackling a desperate attempt to try and get the best title prize, which escaped entirely, as we've just seen 2 seconds ago.",
                    "label": 0
                },
                {
                    "sent": "So that's the first thing.",
                    "label": 0
                },
                {
                    "sent": "So the second thing is saying who?",
                    "label": 0
                },
                {
                    "sent": "A little bit who?",
                    "label": 0
                },
                {
                    "sent": "I am very different from probably a lot of people here I'm I'm somebody who works in machine learning on aspects dealing with formula.",
                    "label": 0
                },
                {
                    "sent": "Grammars and trying to learn formal grammars.",
                    "label": 0
                },
                {
                    "sent": "So somebody working in a field that we would call or a topic we might call grammatical inference.",
                    "label": 1
                },
                {
                    "sent": "OK, so I'll probably be saying things that for linguists is going to be total heresy.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the summary of the talk, the summary of the talk is 2 parts I've divided in three because of for technical reasons.",
                    "label": 0
                },
                {
                    "sent": "The first is why I'm going to try and convince you that what the game is about is not studying the actual grammars, but studying studying the actual methods.",
                    "label": 0
                },
                {
                    "sent": "Sort of saying things about the methods that produce the grammars is at least as important as saying something about the grammars that have been produced.",
                    "label": 1
                },
                {
                    "sent": "Having failed to do that, I will then pass on to the second part of the other topic, which is to give you some of the ways we do use.",
                    "label": 0
                },
                {
                    "sent": "To try and say things about the methods so we've got two settings.",
                    "label": 0
                },
                {
                    "sent": "One is the usual exact setting, the one where we say we've got no probabilities, and this is gold oriented learning and the second setting is the probabilistic setting, because then you've got a variety of settings depending if you want to sort of use the distributions or learn from the distributions or learn this.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Missions themselves.",
                    "label": 0
                },
                {
                    "sent": "So let's go on to the first part.",
                    "label": 0
                },
                {
                    "sent": "Why study the process of learning and not the result itself.",
                    "label": 1
                },
                {
                    "sent": "So return to dramatically influencing the grammatical inference is usually seen as something like building a grammar.",
                    "label": 0
                },
                {
                    "sent": "Sometimes an automaton, small or smallish and adapted in some way to the data from which we are supposed to learn from so very vague definition.",
                    "label": 1
                },
                {
                    "sent": "There isn't such a thing as an official definition of what this is is.",
                    "label": 0
                },
                {
                    "sent": "Then authors are going to say.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Different things in different moments.",
                    "label": 0
                },
                {
                    "sent": "If I want to even make it shorter, I get through something about like this.",
                    "label": 0
                },
                {
                    "sent": "It's about learning a grammar given information about a language, so the three important words here are the grammar, the language.",
                    "label": 1
                },
                {
                    "sent": "So the information we're getting is from the language, and we're trying to learn is a grammar with different things happening there, right?",
                    "label": 0
                },
                {
                    "sent": "Because there are different grammars, there's in decidability issues, and well, sometimes we say we're learning a language, but in fact, behind that there's a lot of tricky things when we get onto the machine, and we're thinking about.",
                    "label": 0
                },
                {
                    "sent": "Computation, the third word, which is in a different color, is the word information in the sense that I don't really because we're working on grammatical inference, not just for you, or we should be working for.",
                    "label": 0
                },
                {
                    "sent": "Not just you were working in general, in the hope that what we're doing can be used in bioinformatics can be used in web mining, can be used in robotics in a number of fields.",
                    "label": 0
                },
                {
                    "sent": "Well, the type of information that we may have to learn a grammar can be very, very different.",
                    "label": 0
                },
                {
                    "sent": "We may have not just.",
                    "label": 0
                },
                {
                    "sent": "Strings we can have counterexamples.",
                    "label": 0
                },
                {
                    "sent": "We may query to be able to test things we may be getting prefixes of sentences.",
                    "label": 0
                },
                {
                    "sent": "We make me just getting chunks of a sentence, and all this means that we have different rules of the game in which for each one of the games, we're going to have to find specific differences.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that's grammatically inference, and what's grammar induction?",
                    "label": 1
                },
                {
                    "sent": "Well, basically it's about the same thing.",
                    "label": 0
                },
                {
                    "sent": "So as far as words go, where left with the same definition, and so my title is that all.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Not explicit, so I'm going to try and make it explicit.",
                    "label": 0
                },
                {
                    "sent": "Doing in a very brutal way the sort of diagram you've been showing me yesterday, which is the way of the one of saying we've got data on one side.",
                    "label": 0
                },
                {
                    "sent": "We're hoping to get the grammar in the middle, and here we've got somehow a process that from this data is going to extract a grammar.",
                    "label": 0
                },
                {
                    "sent": "So if I think about grammar induction, this is what usually happens is I'm very much interested in the grammar that I'm obtaining from the data, and I'm going to try and say something intelligent about the grammar that I have obtained from this data.",
                    "label": 0
                },
                {
                    "sent": "OK, and when we're concentrating in grammatical inference, right?",
                    "label": 1
                },
                {
                    "sent": "Well, we're interested in this object in the middle, which is the actual process that has been doing this in order to abstract from the grammar and the data itself and hope to be able to say that the very next time I use this algorithm on some new data, something interesting is going to happen.",
                    "label": 0
                },
                {
                    "sent": "I should say at this point that.",
                    "label": 0
                },
                {
                    "sent": "Probably being in all field giving opinions is probably much more well, less used to doing it, so I'm more used to giving talks saying now this is a mathematical theorem, so here I'm taking wide risks and saying this is what grammatical inference is about.",
                    "label": 0
                },
                {
                    "sent": "Do not go out and go and see my colleagues in the field saying gotten really have some strange opinions they don't.",
                    "label": 0
                },
                {
                    "sent": "This is what I'm claiming.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, grammatical inference being somewhere there.",
                    "label": 0
                },
                {
                    "sent": "So let's try and motivate this with a highly provocative example.",
                    "label": 0
                },
                {
                    "sent": "Suppose your task isn't about learning grammars, but your task is about designing a random number generator.",
                    "label": 1
                },
                {
                    "sent": "OK, you better find a way to design a good random number generator now.",
                    "label": 0
                },
                {
                    "sent": "Then the question being that you've used it and you've come out with 17.",
                    "label": 0
                },
                {
                    "sent": "So are you happy with 17 or you're going to be able to say something about 17 and then is 17 more random than 25 because somebody else is random?",
                    "label": 1
                },
                {
                    "sent": "Generators come out with 25 OK, I mean obviously you can discuss that.",
                    "label": 0
                },
                {
                    "sent": "This is very unfair.",
                    "label": 0
                },
                {
                    "sent": "I tried to be very fair, in fact.",
                    "label": 0
                },
                {
                    "sent": "I mean, I really did run a random number generator for this example and just to make sure I really repeated the experiment until I got 17 twice.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Joke OK from numbers we can go onto strings and the issue about strings of having at one point you're going to put in a seed and you're going to get at the other end another string and you're wanting to say something about this.",
                    "label": 0
                },
                {
                    "sent": "And I'll already tell you that the real problem is going to be if you use your random generator and it comes out with some really.",
                    "label": 0
                },
                {
                    "sent": "Obviously UN random.",
                    "label": 0
                },
                {
                    "sent": "Now that sounds horrible.",
                    "label": 0
                },
                {
                    "sent": "Some number likes of 999999 and you're left with that number and now you think now what do I do with it right?",
                    "label": 0
                },
                {
                    "sent": "I'm sure most of us will be tempted to say, well, I'll run another time and get a rule random number.",
                    "label": 0
                },
                {
                    "sent": "Not this one.",
                    "label": 0
                },
                {
                    "sent": "OK and here the point I'm trying to make is that if I can't say something really intelligent about the algorithm that is proposed, it that has produced this random number, I'm going to have a problem.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now then here we go, the next step along step.",
                    "label": 0
                },
                {
                    "sent": "This time we're given not random number were given a sample some strings and we've given a grammar and somehow we're saying this is the grammars that we've learned from this sample.",
                    "label": 0
                },
                {
                    "sent": "OK, this is the grammar, but then there may be somebody ask me.",
                    "label": 0
                },
                {
                    "sent": "Of course another grammar G prime saying no, it's this one right?",
                    "label": 0
                },
                {
                    "sent": "So the question is, what shines the best and what can I say about this?",
                    "label": 0
                },
                {
                    "sent": "And how can I say something a little bit founded?",
                    "label": 0
                },
                {
                    "sent": "About which is the best or which is going to be the best for getting.",
                    "label": 0
                },
                {
                    "sent": "Obviously all the perfectly acceptable arguments you will have from an empirical point of view or from a psychological point of view, and believing in a certain order things and holding an enormous culture that I don't have.",
                    "label": 0
                },
                {
                    "sent": "So I'm just looking at it from a mathematical point of view.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So back to the definition, we said grammar induction was about finding the grammar.",
                    "label": 0
                },
                {
                    "sent": "A grammar from some information about the language.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So once we have done that and we've said that, what else can we say?",
                    "label": 1
                },
                {
                    "sent": "What else can we say that would be mathematically grounded?",
                    "label": 0
                },
                {
                    "sent": "So what we would like to say, or which we often say, is that the right if I can.",
                    "label": 1
                },
                {
                    "sent": "Learn the smallest grammar, the best fitting grammar, the grammar that is best for a fitness function for a scoring function then that would be a good idea.",
                    "label": 0
                },
                {
                    "sent": "I mean is based on a long discussion.",
                    "label": 0
                },
                {
                    "sent": "Apart from that, as you said in 1957 says that that is what you should do, but pose from that.",
                    "label": 0
                },
                {
                    "sent": "What is happening is that we are transferring cognitive problem into a combinatorial problem.",
                    "label": 0
                },
                {
                    "sent": "OK and hard combinatorially problem in most cases because we know that.",
                    "label": 0
                },
                {
                    "sent": "The combinatorial or the complexity questions that are going to be behind this are usually going to be intractable, difficult to solve.",
                    "label": 0
                },
                {
                    "sent": "That is why finding the best grammar or the best automata you are having so many huge problems to find.",
                    "label": 1
                },
                {
                    "sent": "It's not big cause of because of the intrinsic complexity of the task.",
                    "label": 0
                },
                {
                    "sent": "So what we would really like to say is that to be able to defend that point of view is to say that actually changing from a cognitive problem too.",
                    "label": 0
                },
                {
                    "sent": "Amatoriale problem has been a good idea.",
                    "label": 0
                },
                {
                    "sent": "Have been a good idea in the sense that we are converging towards something that using an algorithm that actually does this solve this combinatorial problem is really going to solve the problem we were interested in the first go.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is where we usually get into things like oh come arguments, compression arguments.",
                    "label": 0
                },
                {
                    "sent": "If you can compress them, that's good.",
                    "label": 0
                },
                {
                    "sent": "Then you're learning MDL arguments.",
                    "label": 0
                },
                {
                    "sent": "We saw yesterday component, or of complexity.",
                    "label": 0
                },
                {
                    "sent": "All these things.",
                    "label": 0
                },
                {
                    "sent": "They are not technically the same.",
                    "label": 0
                },
                {
                    "sent": "They're all very different, but.",
                    "label": 0
                },
                {
                    "sent": "They're all arguing about the same idea of small is beautiful and behind the idea that that somehow.",
                    "label": 0
                },
                {
                    "sent": "Somehow this is actually linked with getting somewhere OK.",
                    "label": 0
                },
                {
                    "sent": "The idea is saying that if you do find the smallest, then that smallest grammar has the following desired properties.",
                    "label": 0
                },
                {
                    "sent": "Mathematical properties, for example, of minimizing samara or identification in the limit, or things of that sort.",
                    "label": 0
                },
                {
                    "sent": "But we must be careful with the fact that we're actually solving a little bit a different problem here.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, what else might we want to say?",
                    "label": 1
                },
                {
                    "sent": "Well, I mentioned there is a 2 seconds ago what we might want to say is that we can actually use this grammar in the near future and probably best of cases would be to be able to get 100 pounds on the outcome.",
                    "label": 1
                },
                {
                    "sent": "And saying, well, I think the next string that will appear is this or I think the next string.",
                    "label": 1
                },
                {
                    "sent": "I will classify it correctly.",
                    "label": 0
                },
                {
                    "sent": "OK, this would really be nice.",
                    "label": 0
                },
                {
                    "sent": "This is something that machine learning has been dealing with quite successfully, not in the field of grammatical inference, should I say.",
                    "label": 1
                },
                {
                    "sent": "But in the fields of.",
                    "label": 0
                },
                {
                    "sent": "A feather of variables and numbers and well, all sorts of other things not related to grammatical instance has been proving little bit too difficult for this.",
                    "label": 0
                },
                {
                    "sent": "I'll say it would about this.",
                    "label": 0
                },
                {
                    "sent": "This is usually called PAC learning OK, you're going to say something and say OK. Then, if whatever I've been doing now, sorry if the sampling process I have used to sample and to get hold of my examples from which I have learned is to be continuous, meaning the distribution is going to be the same.",
                    "label": 0
                },
                {
                    "sent": "Further on then I can actually.",
                    "label": 0
                },
                {
                    "sent": "Gamble some money on this and I know what is my hopes of losing or winning that money.",
                    "label": 0
                },
                {
                    "sent": "But that is a nice.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Things we might like to say.",
                    "label": 1
                },
                {
                    "sent": "There's a third sort of thing, much less ambitious that we would like to say, and clearly and grammatically, inferences is often saying, and for that it is criticised.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to defend that.",
                    "label": 0
                },
                {
                    "sent": "I'm there.",
                    "label": 0
                },
                {
                    "sent": "I'm here for that.",
                    "label": 0
                },
                {
                    "sent": "Is that perhaps can I at least say that when I use this algorithm, there's going to be an outcome for which I have no guarantee that it works, but at least I can blame the data and not the algorithm.",
                    "label": 0
                },
                {
                    "sent": "OK, what does that mean?",
                    "label": 0
                },
                {
                    "sent": "That means that either I can say look if at the end, whatever I'm presenting you isn't good.",
                    "label": 0
                },
                {
                    "sent": "It's because I didn't have enough data, or because the distribution was completely biased or becausw.",
                    "label": 0
                },
                {
                    "sent": "The sampling process went wrong somewhere or something of that sort.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's.",
                    "label": 0
                },
                {
                    "sent": "Think of, well, you know this is no use at all.",
                    "label": 0
                },
                {
                    "sent": "The problem is that if you don't even have that, then you've got a problem.",
                    "label": 0
                },
                {
                    "sent": "If you can't even say you know it's not the algorithm that is to blame, it's the data.",
                    "label": 1
                },
                {
                    "sent": "If you can't say that, then if your algorithm is doing something wrong, then you have.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I think a problem.",
                    "label": 0
                },
                {
                    "sent": "This is what?",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Well, so let's have an example for that.",
                    "label": 1
                },
                {
                    "sent": "A typical example that we will recognize.",
                    "label": 0
                },
                {
                    "sent": "Suppose that you're using one of the many algorithms that is trying to learn some context free, non probabilistic grammar from some data positive data.",
                    "label": 0
                },
                {
                    "sent": "So you get your strings there and what you're going to do is we're going to bring in.",
                    "label": 0
                },
                {
                    "sent": "These strings are going to start with sort of just a very general grammar that accepts everything a little bit.",
                    "label": 1
                },
                {
                    "sent": "Just one rule per per string, and you're going to apply iteratively 2 rules, one of them consistent saying, oh look, these two nonterminals.",
                    "label": 0
                },
                {
                    "sent": "Looks similar right with similarity based on MDL or on whatever you want.",
                    "label": 0
                },
                {
                    "sent": "So we merge them and then there the rule is to say, well perhaps here what we can do is do some splitting which we got a rule that is too long and we can make 2 rules out of it.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So typically it's something like this, right?",
                    "label": 0
                },
                {
                    "sent": "The first rule just saying that here we take these two rules and they become three because we recognize objective noun, for which we can actually do a special rule.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And the second one being this, which is saying that AP one and AP two are sufficiently similar.",
                    "label": 0
                },
                {
                    "sent": "That's not up to the question here.",
                    "label": 0
                },
                {
                    "sent": "To know why, so we merge them into just AP one.",
                    "label": 0
                },
                {
                    "sent": "And by doing that we are.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Generalizing and learning.",
                    "label": 0
                },
                {
                    "sent": "So what happens or what is bound to happen?",
                    "label": 1
                },
                {
                    "sent": "I mean, just hear what usually happens, but then in the implementation you have to going to do tricks to avoid that.",
                    "label": 0
                },
                {
                    "sent": "Is that what you're going to do is you're going to actually learn a context free grammar, but the underlying language is going to be a regular, so let's just wait two seconds.",
                    "label": 0
                },
                {
                    "sent": "What I'm saying here is that it's not because I'm clearly for the same regular language.",
                    "label": 0
                },
                {
                    "sent": "You can have many grammars.",
                    "label": 0
                },
                {
                    "sent": "You've got the regular grammar, but you can also design very easy.",
                    "label": 0
                },
                {
                    "sent": "Easily, some very nice context free grammars which look all the context free with nice trees.",
                    "label": 0
                },
                {
                    "sent": "Very balanced with all sorts of things, but at the end of the day when we're looking at it, the typical, let's say brackets or Dick languages or those sort of structures that we normally think we're finding in context free languages do not appear OK, and I've seen this in a various number of algorithms out there.",
                    "label": 0
                },
                {
                    "sent": "They just don't appear.",
                    "label": 0
                },
                {
                    "sent": "So then you're going to do a lot of tuning to be able to force some rules not to be applied to be deferred in order to try and discover.",
                    "label": 0
                },
                {
                    "sent": "Sort of the closing bracket.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is what we call a hidden bias in the hidden bias here will mean the fact that we're actually claiming that we're learning context free grammars, which we are ready, or that we're working on the class of the context free languages.",
                    "label": 0
                },
                {
                    "sent": "But in fact we're not.",
                    "label": 0
                },
                {
                    "sent": "We're really reducing, and we're only working only attempting to learn regular languages.",
                    "label": 0
                },
                {
                    "sent": "OK, I mean, fair enough.",
                    "label": 0
                },
                {
                    "sent": "Once we if the if the what you call it.",
                    "label": 0
                },
                {
                    "sent": "The hidden biased becomes the declared buyers by some game.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "That's nice.",
                    "label": 0
                },
                {
                    "sent": "OK, so now then so OK, we know what we don't like.",
                    "label": 0
                },
                {
                    "sent": "So what do I like?",
                    "label": 0
                },
                {
                    "sent": "Well what I like is to say that we need to target, OK.",
                    "label": 1
                },
                {
                    "sent": "So I need to target what does it mean that I need a target?",
                    "label": 0
                },
                {
                    "sent": "I need something hidden there that I suppose is the ideal solution and the question is going to be a studying the convergence of my learning algorithm given some data towards this ideal solution in that it is what I will call an inference process.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What is bold?",
                    "label": 0
                },
                {
                    "sent": "OK, so now then yes, let's go to the questions to the yummy.",
                    "label": 0
                },
                {
                    "sent": "But what if you don't believe there is a target, right?",
                    "label": 1
                },
                {
                    "sent": "So there isn't.",
                    "label": 0
                },
                {
                    "sent": "There's no such thing as a regular language for the natural language or context free language.",
                    "label": 0
                },
                {
                    "sent": "For context, free grammar for this well.",
                    "label": 0
                },
                {
                    "sent": "First of all.",
                    "label": 0
                },
                {
                    "sent": "You've got a problem anyhow.",
                    "label": 0
                },
                {
                    "sent": "If you're saying I don't believe there is a context free grammar, and nevertheless you're still using an algorithm to learn a context free grammar, you're facing the same problem in any case.",
                    "label": 0
                },
                {
                    "sent": "Second thing is that we're really not.",
                    "label": 0
                },
                {
                    "sent": "We're using it as a device to measure the quality of the algorithm.",
                    "label": 0
                },
                {
                    "sent": "We're not going to measure this target to know if you know whatever we've found is correct or not, but in practice we're never going to have the target.",
                    "label": 0
                },
                {
                    "sent": "OK, so since we're really studying an algorithm that is not going to be too much of a problem, and then the third thing is that anyhow, right?",
                    "label": 0
                },
                {
                    "sent": "This target is a bit of a hidden bias.",
                    "label": 0
                },
                {
                    "sent": "Saying not to hit the declared bias.",
                    "label": 1
                },
                {
                    "sent": "I believe in a target, but if not, we're going to come out with another bias which.",
                    "label": 0
                },
                {
                    "sent": "Maybe better, maybe worse.",
                    "label": 0
                },
                {
                    "sent": "I'm just proposing to substitute one bias by another.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so if you are prepared to accept there is a target but.",
                    "label": 1
                },
                {
                    "sent": "Then you're thinking that that's it.",
                    "label": 1
                },
                {
                    "sent": "I mean, if the target is known, what is the point of learning true?",
                    "label": 0
                },
                {
                    "sent": "And if the target is not known, what's the practical point of this?",
                    "label": 0
                },
                {
                    "sent": "Where as I said, it's not a question of practical points.",
                    "label": 0
                },
                {
                    "sent": "Is question of saying I'm going to study offline in certain way my algorithm and then just hope that what's going to happen today?",
                    "label": 0
                },
                {
                    "sent": "I don't have a target is going to be similar to what's been happening in the days I did have a target.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So yes, if you're not doing that, if what you're doing is saying and I refused out of target, I've got the data.",
                    "label": 0
                },
                {
                    "sent": "What really matters is the data I've got work from the data upwards towards a grammar.",
                    "label": 0
                },
                {
                    "sent": "Then I think you're doing grammar induction.",
                    "label": 1
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "This is probably not a distinction that we will find in this field.",
                    "label": 0
                },
                {
                    "sent": "You'll find this distinction much more made in a field of pattern recognition, which also uses grammar induction or grammatical inference, and for who the data is, what matters, as I'll see, I'll show you in a moment.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "It's in that case.",
                    "label": 0
                },
                {
                    "sent": "I mean, yes, you go from the data and you really need to say something about whatever you found that day on.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Item of data.",
                    "label": 0
                },
                {
                    "sent": "So careful anyhow, if you don't.",
                    "label": 0
                },
                {
                    "sent": "If you don't want to do what I'm saying, there are some things that actually I was inspired by a paper by Alex that was rereading yesterday, but you can't come out with saying OK about an algorithm about a learning algorithm saying I am attacking the curb.",
                    "label": 0
                },
                {
                    "sent": "My algorithm can learn A&BNCN this language, which is a typical way of showing I've got a really powerful learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "It can learn this language.",
                    "label": 1
                },
                {
                    "sent": "I mean, any algorithm.",
                    "label": 0
                },
                {
                    "sent": "Can my anger and saying you know this is the language has learned it because there's just one language?",
                    "label": 0
                },
                {
                    "sent": "Learning is distinguishing between very very many possible languages or possible grammars, which is the one that is which is there very, many meaning possibly and probably infinitely many same as saying a good quality of an algorithm and say look, I can actually with just two examples, learn this rule.",
                    "label": 0
                },
                {
                    "sent": "There's no free lunch if you're doing that, is there probably there's aspects of the learning process that you're missing entirely?",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the compromise is that when I'm proposing is to say that you only need to believe there is a target while evaluating the algorithm afterwards.",
                    "label": 1
                },
                {
                    "sent": "In practice, the fact that there isn't one isn't isn't an issue.",
                    "label": 0
                },
                {
                    "sent": "The issue is that if you've decided that you're using algorithm to learn context free grammars and you know Jolly well that there is no such thing as a context of grammar for what you're doing, you're having a problem anyhow, I'm not helping you solve it, apart from the fact of dealing with noise and distances between things.",
                    "label": 0
                },
                {
                    "sent": "I mean, that's much more.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Nickel complex issue.",
                    "label": 0
                },
                {
                    "sent": "OK, just to end my provocative example right.",
                    "label": 0
                },
                {
                    "sent": "If my room a random number generator gives me some unusual number, some very compressible number, then if I want to be able to keep that number not rerun it well, I really want to say something about the algorithm.",
                    "label": 1
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Right, I don't see it.",
                    "label": 0
                },
                {
                    "sent": "Just three things to just summarize what I've been saying here, so grammatically inferences about measuring the convergence of the grammar learning algorithm in a typical situation.",
                    "label": 1
                },
                {
                    "sent": "OK, it's about saying something about how the learning algorithm is going to converge toward.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Is the target.",
                    "label": 0
                },
                {
                    "sent": "Typical can be one of typically two things in the limit saying, well, I'm going to get more and more examples.",
                    "label": 1
                },
                {
                    "sent": "I'm hoping that one day or another this is going to achieve reach success or probabilistic where there are then two variants, one of which is to say that I'm going to use the distribution to talk about errors and there the one where I'm going to say, well, that's what I'm looking for.",
                    "label": 0
                },
                {
                    "sent": "I'm looking for the distribution itself.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And the third credit.",
                    "label": 0
                },
                {
                    "sent": "The third thing I believe in very strongly is that we shouldn't just concentrate on.",
                    "label": 0
                },
                {
                    "sent": "I've heard things yesterday about this.",
                    "label": 0
                },
                {
                    "sent": "We couldn't concentrate just on saying, you know this will eventually learn one day.",
                    "label": 0
                },
                {
                    "sent": "But complexity considerations are essential.",
                    "label": 0
                },
                {
                    "sent": "In the previous talk.",
                    "label": 0
                },
                {
                    "sent": "For example, while the complexity behind everything was just so hard that you can only approximate and then you're facing the problem of not being able to say enough.",
                    "label": 0
                },
                {
                    "sent": "Because even if you have covered things, there's still things out there left that you don't know where they are not.",
                    "label": 0
                },
                {
                    "sent": "So complexity is very important and there are, and that's where I'm going to come to.",
                    "label": 0
                },
                {
                    "sent": "In the second part of the talk, there are many ways of measuring it, and many results out there and many techniques that have been looked.",
                    "label": 0
                },
                {
                    "sent": "You can start counting well, how long will it actually take in the worst case, how?",
                    "label": 0
                },
                {
                    "sent": "How long does it take to update from one year, but assistant to the next level with a new example being added?",
                    "label": 0
                },
                {
                    "sent": "How many mind changes is my algorithm going to make sort of?",
                    "label": 0
                },
                {
                    "sent": "I like my hypothesis up to now.",
                    "label": 0
                },
                {
                    "sent": "Oh no.",
                    "label": 0
                },
                {
                    "sent": "Oh dear, I have to change it.",
                    "label": 0
                },
                {
                    "sent": "How many prediction errors it makes?",
                    "label": 0
                },
                {
                    "sent": "Meaning up to now everything was OK, but now in you example is contradictory with what I had, so I'm having to change my mind.",
                    "label": 0
                },
                {
                    "sent": "So theory and algorithm that doesn't change its mind very often is or can be considered as good and the number and weight of errors in the case where we do have probabilities, all this can be measured and limited.",
                    "label": 1
                },
                {
                    "sent": "So this is what I'm believing it and what's not being said here and I'm saying it now in case I forget in the conclusion.",
                    "label": 0
                },
                {
                    "sent": "Is all this means that there is also hidden behind this very active number of researchers.",
                    "label": 0
                },
                {
                    "sent": "Working on finding new algorithms because once you've seen that there's a problem that, for example, context free grammars, do we know?",
                    "label": 0
                },
                {
                    "sent": "Do we not know if we can learn it this way?",
                    "label": 0
                },
                {
                    "sent": "Well, that's a challenge for people to try and devise new algorithms, and that's what's happening.",
                    "label": 0
                },
                {
                    "sent": "There is a real problem, clearly, and getting these algorithms out of full field, but I mean, know that they're there.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So let's go through the non probabilistic setting and just see a little bit the different ideas, main ideas that are behind this.",
                    "label": 0
                },
                {
                    "sent": "I have eliminated all mathematics.",
                    "label": 0
                },
                {
                    "sent": "I'm very proud of myself.",
                    "label": 0
                },
                {
                    "sent": "Which means probably that you won't understand it because somehow it's difficult to tell like that.",
                    "label": 0
                },
                {
                    "sent": "Anyhow, some of them you do know already just that in the way I'm going to say it is perhaps a little bit different.",
                    "label": 0
                },
                {
                    "sent": "So typically is a non probabilistic setting is working on identification in the limit with the fact that we will be interested in talking about resource bounded.",
                    "label": 0
                },
                {
                    "sent": "Identification in the limit just saying identification in the limit is not going to be good enough.",
                    "label": 1
                },
                {
                    "sent": "We really want to be able to say something that is typical for a computer scientist, which is to say you know the means we are allowed to use.",
                    "label": 0
                },
                {
                    "sent": "Our only typically polynomial.",
                    "label": 0
                },
                {
                    "sent": "OK, and I won't have time to talk about this.",
                    "label": 0
                },
                {
                    "sent": "This is a very reactive newly active field.",
                    "label": 0
                },
                {
                    "sent": "The one of query learning where people are actually allowed to ask questions in an interaction type of situation.",
                    "label": 0
                },
                {
                    "sent": "OK, and then the question of language learning where there's communication between beings.",
                    "label": 0
                },
                {
                    "sent": "I mean this is worth looking at for new algorithms which is.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What we don't have there we don't as I'm going to say you may, but we don't.",
                    "label": 0
                },
                {
                    "sent": "OK, identification.",
                    "label": 0
                },
                {
                    "sent": "The limit giving a bit domain definitions, just saying that there's alternatives that are tricky technically, whether you're using the order or not of the data that is arriving and whether you are allowed what we will call randomized algorithms or not randomized means I'm allowed to also toss coins inside my my algorithm.",
                    "label": 1
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Much learning.",
                    "label": 0
                },
                {
                    "sent": "So the presentation the presentation is just data that is arriving to my learner one after the other.",
                    "label": 0
                },
                {
                    "sent": "Right data being basically anything X we haven't seen symbol X up to now.",
                    "label": 0
                },
                {
                    "sent": "A symbol X is, well, some set of information that we are getting.",
                    "label": 1
                },
                {
                    "sent": "You can think typically of text, which is probably what you're interested 99% in.",
                    "label": 0
                },
                {
                    "sent": "But you can think of more generally data information about the language that is reaching you.",
                    "label": 0
                },
                {
                    "sent": "So what we're really hoping is at the end of the day, the end of all days.",
                    "label": 0
                },
                {
                    "sent": "Should I say the actual data, right?",
                    "label": 0
                },
                {
                    "sent": "Sorry, the fact that you've seen the same data in two different ways corresponds to exactly the same language, so it's it may be order dependent up to a certain part, but in Infinity, if the following holds, if the same data is what has been presented, then the language behind.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "This is the same.",
                    "label": 0
                },
                {
                    "sent": "So we have got a learning function.",
                    "label": 1
                },
                {
                    "sent": "The learning function is an algorithm which is given the initial bits of the presentation.",
                    "label": 1
                },
                {
                    "sent": "So right FFN is the first elements of the presentation and it's a function that takes as input.",
                    "label": 1
                },
                {
                    "sent": "This set and returns the grammar.",
                    "label": 0
                },
                {
                    "sent": "Here's the bit where you when I write this and deciding that the algorithm is not randomized, and so it may be a little bit messy, so I'm taking the decision which returns a grammar every time a new hypothesis given a grammar.",
                    "label": 0
                },
                {
                    "sent": "L of G This is the naming function which allows to relate the grammar to the.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Language and all this gets into a nice little magic triangle.",
                    "label": 0
                },
                {
                    "sent": "I've got a class of languages L which is related to a class of grammars G in such a way as I have got a naming function and here is where to be fair and to avoid problems we saw earlier.",
                    "label": 0
                },
                {
                    "sent": "We really want them to coincide Wonderland grammars to coincide with the Lacrosse Flyer languages.",
                    "label": 0
                },
                {
                    "sent": "A firm, if not, it's not.",
                    "label": 0
                },
                {
                    "sent": "It's not really fair.",
                    "label": 0
                },
                {
                    "sent": "So then on this I've got close languages, was related with the set of presentations.",
                    "label": 0
                },
                {
                    "sent": "As we saw through yields, which is an implicit function.",
                    "label": 0
                },
                {
                    "sent": "Obviously not, not.",
                    "label": 0
                },
                {
                    "sent": "You can't compute it.",
                    "label": 0
                },
                {
                    "sent": "And we've got the property we saw earlier.",
                    "label": 0
                },
                {
                    "sent": "And now we relate through the fact that given initial bit of a presentation learner takes bits of presentations, returns of grammar and the whole thing should close down beautifully into the following formulas, say that there is a points for each presentation, each language, each for each presentation, each language there is a point where from that point onwards, whatever is returned by the learner corresponds exactly to what we were looking for.",
                    "label": 0
                },
                {
                    "sent": "OK, so it all just closes down.",
                    "label": 0
                },
                {
                    "sent": "So this is theoretically we do know that in practice you are not running a learning algorithm every time you get a new example.",
                    "label": 0
                },
                {
                    "sent": "Nevertheless, it is the best way to study the process in this way rather than saying OK, I've got everything.",
                    "label": 0
                },
                {
                    "sent": "One goal, you can transform the.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Different settings.",
                    "label": 0
                },
                {
                    "sent": "So what about efficiency?",
                    "label": 0
                },
                {
                    "sent": "Once we've said that, so this is just setting where we know the problems with the gold.",
                    "label": 0
                },
                {
                    "sent": "The gold aspect is saying the one day we will converge whenever that is, and we also know that we don't even know when that day arrives and when we are that day, we don't know that that day has arrived.",
                    "label": 0
                },
                {
                    "sent": "To be fair, there are people working on these questions and have developed all sorts of cunning little.",
                    "label": 0
                },
                {
                    "sent": "A variance of gold type learning.",
                    "label": 0
                },
                {
                    "sent": "There's an active community made of specifically Germans and Japanese.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's a conference call DLT that is held every year and so working on inductive inference, and they still actively looking at those things.",
                    "label": 0
                },
                {
                    "sent": "So what can we try to bound here?",
                    "label": 0
                },
                {
                    "sent": "We can try and bound the global time, so how long it takes to actually converge?",
                    "label": 0
                },
                {
                    "sent": "That would be nice, but since you do have got no control over the examples, that never works, you can bound the update time.",
                    "label": 0
                },
                {
                    "sent": "How long does it take me to get from one example?",
                    "label": 0
                },
                {
                    "sent": "Sorry from one hypothesis to the next one.",
                    "label": 0
                },
                {
                    "sent": "That's usually very easy to do in polynomial time, so that's normally doesn't get anywhere.",
                    "label": 0
                },
                {
                    "sent": "We can try and bound the number of errors before converging, meaning that if.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Will like this, sorry.",
                    "label": 0
                },
                {
                    "sent": "I should have showed this writer though, so this is the process I'm getting my examples one by one sort of accumulating in the set and coming out my with my my hypothesis and hopefully as I said at one point I am not going to change anymore and everything is OK, so there is.",
                    "label": 0
                },
                {
                    "sent": "Well, there is means that one moment the new example that arrives is inconsistent with whatever I had here.",
                    "label": 0
                },
                {
                    "sent": "OK, so the amount of times that happens isn't very nice.",
                    "label": 0
                },
                {
                    "sent": "We want that to diminish.",
                    "label": 0
                },
                {
                    "sent": "So we want to count that.",
                    "label": 0
                },
                {
                    "sent": "We can count the amount mined changes, which is how many times have I had had an obligation to change this right?",
                    "label": 0
                },
                {
                    "sent": "You can say, well I never changed, but then the algorithm does have the obligation to identifying the limits.",
                    "label": 0
                },
                {
                    "sent": "So you do measure a very specific things when you're doing those things.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You can measure the queries in a case where actually instead of just receiving the examples, you're asking questions.",
                    "label": 0
                },
                {
                    "sent": "You're asking for things you're asking.",
                    "label": 0
                },
                {
                    "sent": "Is this string in the language?",
                    "label": 1
                },
                {
                    "sent": "Is this string not in the language or things like that, and you can bounce something that is called characteristic samples, which corresponds to the good examples.",
                    "label": 0
                },
                {
                    "sent": "Let's get into the best case right where somehow we've got a teacher that can actually choose the examples that are in the in the learning sample.",
                    "label": 0
                },
                {
                    "sent": "Will that help?",
                    "label": 0
                },
                {
                    "sent": "How long will it take me to do in order not to be 2 conclusive?",
                    "label": 0
                },
                {
                    "sent": "Will say the teacher is going to put these examples and somebody else is going to add some other ones right?",
                    "label": 0
                },
                {
                    "sent": "Just so that we can't distinguish which other ones of the teacher which are the others?",
                    "label": 0
                },
                {
                    "sent": "Because if not this some tricky factors there.",
                    "label": 0
                },
                {
                    "sent": "But basically, do we in any House needs such in any in any case needs such an amount of information that we're never going to be able to reach it.",
                    "label": 0
                },
                {
                    "sent": "So however good my algorithm, there's no way I'm going to be able to do it or not.",
                    "label": 0
                },
                {
                    "sent": "That is actually one of the big differences between the regular languages, though Automata for which that's not the case.",
                    "label": 0
                },
                {
                    "sent": "Very short sample of strings is always sufficient to be able sufficient to be able to be sure to learn, and the context free where there are context free grammars out there for which the size of the size of this characteristic sample has to be so huge, so exponentially exponentially high that you just can't do anything about it.",
                    "label": 0
                },
                {
                    "sent": "So there is tricky bit there.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I didn't say things about this that I need to measure things we also have to be careful with some, not just technical details.",
                    "label": 0
                },
                {
                    "sent": "With this more into it, I mean how do we count things?",
                    "label": 0
                },
                {
                    "sent": "What do we count when we want you to do computation we have?",
                    "label": 0
                },
                {
                    "sent": "We have to know what we're counting.",
                    "label": 0
                },
                {
                    "sent": "So typically the size of a grammar we saw on an idea about that in a loose way, we can say that last the size of the grammar has to be linked with the size of its encoding, the number of bits we need.",
                    "label": 0
                },
                {
                    "sent": "So then you find a formula formula of saying.",
                    "label": 0
                },
                {
                    "sent": "Now if I count.",
                    "label": 0
                },
                {
                    "sent": "This and that, for example, the length of the rules, or multiply the length of the longest rule multiplied by the number of rules.",
                    "label": 0
                },
                {
                    "sent": "Then I'm safe, you know, and the polynomial is safe.",
                    "label": 0
                },
                {
                    "sent": "If I do that.",
                    "label": 0
                },
                {
                    "sent": "The size of a language that size of language.",
                    "label": 1
                },
                {
                    "sent": "Typically you would say, well, it's the number of strings.",
                    "label": 0
                },
                {
                    "sent": "It doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Is that language is infinite, so the size of language is typically in these tasks the size of the smallest grammar for that language.",
                    "label": 0
                },
                {
                    "sent": "Since we've got many different equivalent languages.",
                    "label": 1
                },
                {
                    "sent": "The size of the presentation now the size of the presentation in the gold sort of thing is something that is not clear.",
                    "label": 0
                },
                {
                    "sent": "I mean clearly a presentation is infinite, and what we'd like to say is the number of the size of presentation being the number of length of the point, the convergence point, the point upon which there is enough information to decide this is the correct grammar, but it's doesn't work exactly because we're really algorithm there.",
                    "label": 0
                },
                {
                    "sent": "Dependently still research to be done all that to be able to express the idea.",
                    "label": 0
                },
                {
                    "sent": "That you know at some moment we do have enough that certain presentations are more favorable, because now we've got it or we don't have it.",
                    "label": 0
                },
                {
                    "sent": "The size off, not a presentation in its whole, but of the end.",
                    "label": 0
                },
                {
                    "sent": "First elements of the presentations you will always say obviously say N doesn't work either, because the strings were working for the phrases are of arbitrary length, so you have to sum up all the lengths of all the strings in your sample to be able to work on.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We've seen all this, I just I just give some selected results just so we can see the spirit of the thing.",
                    "label": 0
                },
                {
                    "sent": "So DFA, which you're not interested in, but I mean it's the same biggest context, free is more important for computational linguistics.",
                    "label": 0
                },
                {
                    "sent": "This just gives the picture so clearly we do know the basic result, the text, whatever you're counting, nothing happens, but you just can't learn them from text.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's not even a matter of seeing if resource boundedness is going to be in this case or not, but if we look at learning from an informant, so both positive and negative examples where we do know that the runtime is cannot be bounded polynomial, either update time is easily.",
                    "label": 0
                },
                {
                    "sent": "So the number of prediction errors is not polynomially bounded.",
                    "label": 0
                },
                {
                    "sent": "And I was trying to do this slide yesterday, so I preferred putting a question mark here, not as an open problem, but as I don't know, OK?",
                    "label": 0
                },
                {
                    "sent": "On the but the size of the characteristic samples is polynomially bounded, meaning that just a small quantity of information is sufficient to be sure that whatever the rest of information is in the learning sample, we're going to find correctly.",
                    "label": 0
                },
                {
                    "sent": "The Panorama for the context free grammars is.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same with the difference that this yes is becoming no and which makes the task that more difficult.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And just becausw.",
                    "label": 0
                },
                {
                    "sent": "I was hearing things about edit distances yesterday.",
                    "label": 0
                },
                {
                    "sent": "We've been working on trying to look at all these things for some, not just languages in the sense of grammars, but languages as a sense of topological balls.",
                    "label": 0
                },
                {
                    "sent": "You take a string and you just draw a ball around that string with the edit distance and you ask yourself, can we learn these things?",
                    "label": 0
                },
                {
                    "sent": "And then is amusing things, like for example, that actually it's easier to learn from text than from an informant.",
                    "label": 0
                },
                {
                    "sent": "It is tricky, bit there it's easier to learn with less information with more.",
                    "label": 0
                },
                {
                    "sent": "It's not what I'm really saying, so the way we're counting, basically you're only going to make 1 sided errors if you're learning from text right?",
                    "label": 0
                },
                {
                    "sent": "And you're going to make 2 sided errors when you're learning from an informant.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, not doing badly right?",
                    "label": 0
                },
                {
                    "sent": "So that was the, so there's still research being done.",
                    "label": 0
                },
                {
                    "sent": "People are trying to understand best of these phenomena, and even if we can agree that goal style learning is not the ultimate response is not the thing that is going to save our lives.",
                    "label": 0
                },
                {
                    "sent": "I repeat, my argument is when you don't have it that you've got problems.",
                    "label": 0
                },
                {
                    "sent": "OK, it's when you can't say that the characteristic sample is polynomial.",
                    "label": 0
                },
                {
                    "sent": "Write for example, that you've got problems because you know that.",
                    "label": 0
                },
                {
                    "sent": "That means that there is hidden there somewhere.",
                    "label": 0
                },
                {
                    "sent": "Some possible targets and possible grammar that you would think that you could learn and that you just can't learn.",
                    "label": 0
                },
                {
                    "sent": "OK, so that's what it means.",
                    "label": 0
                },
                {
                    "sent": "So the positive bits of saying I can go learning are not very, very strong.",
                    "label": 0
                },
                {
                    "sent": "There are things relating with probabilities.",
                    "label": 0
                },
                {
                    "sent": "If you're saying the characteristic sample is small, usually that will mean that they are small strings for which in certain natural.",
                    "label": 0
                },
                {
                    "sent": "Distributions will have high probability, so you can actually put something on saying hey, the probability that those nice strings appear is possibly higher than the others.",
                    "label": 0
                },
                {
                    "sent": "So I've got a reasonable probability of seeing them reasonably early.",
                    "label": 0
                },
                {
                    "sent": "OK, these things requiring more than handwaving.",
                    "label": 0
                },
                {
                    "sent": "So, so the probabilistic setting here very quickly.",
                    "label": 0
                },
                {
                    "sent": "So there are, as I said, 2 two ways of saying you're seeing things, and I've divided into 3 becausw where you see in a second, so using the distribution to measure error.",
                    "label": 0
                },
                {
                    "sent": "So you've got a distribution we're going to suppose that this distribution, although it is unknown, it is fixed and it is not going to change with time.",
                    "label": 0
                },
                {
                    "sent": "Again, a bias, right?",
                    "label": 0
                },
                {
                    "sent": "If you think it's going to change well, then you've got a different things to be done.",
                    "label": 0
                },
                {
                    "sent": "So we've got the same distribution in the moment we're sampling and further on when we're going to use whatever we've learned to do something with.",
                    "label": 0
                },
                {
                    "sent": "So we can do some one of the following week and try and use this distribution to measure errors because now there is a probability for a string to be sampled.",
                    "label": 0
                },
                {
                    "sent": "We can also try and identify this distribution.",
                    "label": 0
                },
                {
                    "sent": "So follow the gold course and say well I really want to identify these things and then you can try and approximate the distribution winning well you know are probably too ambitious to actually learn the exact distribution, but if I could learn something sufficiently close using the same sort of error measures as earlier on.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "That would be nice.",
                    "label": 0
                },
                {
                    "sent": "So the probabilistic, so they correspond.",
                    "label": 0
                },
                {
                    "sent": "Sorry, this was meant to be going like this, and then you go back and you see that the first one is usually called PAC.",
                    "label": 0
                },
                {
                    "sent": "Learning the second one is called Pack ident.",
                    "label": 0
                },
                {
                    "sent": "Sorry it's called identification with probability one and the third type of results are called.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Learning distributions.",
                    "label": 0
                },
                {
                    "sent": "So we we have a distribution, we sample typically twice, once to be able to get the examples and the second time later on to be able to measure somehow.",
                    "label": 0
                },
                {
                    "sent": "If we've done well, or even if we don't do it, at least that's what we supposed to do.",
                    "label": 0
                },
                {
                    "sent": "The second thing, so the pack setting corresponds to probably approximately correct, and corresponds to probably the most important setting in machine learning, or the most inspiring.",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "PAC Learning, as I said, Valiant 84 just requires a certain number of para meters and I'm just going to spend a minute on them.",
                    "label": 1
                },
                {
                    "sent": "In case you've never seen them, we've still got all the class of languages or class of grammars and we've got 2 little epsilons and Delta here, which are important, right?",
                    "label": 0
                },
                {
                    "sent": "That we are going to choose.",
                    "label": 0
                },
                {
                    "sent": "We're going to decide how much error we're going to allow ourselves in this process.",
                    "label": 0
                },
                {
                    "sent": "And we've got some technical nuisances, like saying, well, all this works very nicely on finite things.",
                    "label": 0
                },
                {
                    "sent": "And here where we have to bound the length of possible strings on one hand and the maximum size of grammars on the other hands.",
                    "label": 0
                },
                {
                    "sent": "Technically speaking, it's an absolute nuisance.",
                    "label": 0
                },
                {
                    "sent": "Practically, you can both estimate these two things, given also some sampling or the epsilon and Delta.",
                    "label": 0
                },
                {
                    "sent": "And things like that.",
                    "label": 0
                }
            ]
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what we're going to call polynomially PAC learnable is that we're going to say that we've got a very nice algorithm that can be randomized that will sample reasonably reasonably means the polynomial number of times only right, not more than that.",
                    "label": 0
                },
                {
                    "sent": "You can't suddenly want to gain that unreasonably, means just putting your hand in, not chewed, not being choosy there.",
                    "label": 0
                },
                {
                    "sent": "If you're choosing, you get another model and returns, then with probability at least one minus Delta, something that will make at most epsilon errors.",
                    "label": 0
                },
                {
                    "sent": "So why there's a Delta Y?",
                    "label": 0
                },
                {
                    "sent": "There's an epsilon.",
                    "label": 0
                },
                {
                    "sent": "Well, basically when you get to learn something it is not going to be perfect, so it's going to make errors and we just want these errors to be not too many, so that's the epsilon bit under Delta.",
                    "label": 0
                },
                {
                    "sent": "The Delta corresponds to the fact that you may be completely absurdly unlucky during the sampling process.",
                    "label": 0
                },
                {
                    "sent": "First, one or second one right, and this sampling that has gone completely wrongly, while means that your error is near not gonna be epsilon anymore.",
                    "label": 0
                },
                {
                    "sent": "But Luckily, the probability that you're sampling process has been completely degenerate is not high at all.",
                    "label": 0
                },
                {
                    "sent": "So we can just put that inside the bad bit we put inside a Delta.",
                    "label": 0
                },
                {
                    "sent": "So that's the sort of results that are nice because you do see how you could actually.",
                    "label": 0
                },
                {
                    "sent": "If you've got that, use it to to put some money on whatever.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So you're using.",
                    "label": 0
                },
                {
                    "sent": "But the results in our case in the case of grammars are negative.",
                    "label": 0
                },
                {
                    "sent": "We don't know how to do anything in this setting.",
                    "label": 0
                },
                {
                    "sent": "Alright, so this is learning a classifier that says we're going to decide if string belongs or not to the language, so the results come to say that generally speaking, even for the case of DFA, you can't do it and you can't even do it if you're allowed to do a bit more than that and ask membership queries, which is ask some specific queries in the in the lot, that's nasty.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so since that doesn't work, the way of using distributions is something you're more familiar to is to say.",
                    "label": 0
                },
                {
                    "sent": "Well, we're actually going to learn the distributions them so.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Elves.",
                    "label": 0
                },
                {
                    "sent": "So making no error means that we want to do identification in the limit with probability one.",
                    "label": 0
                },
                {
                    "sent": "Again, this is based on the fact that the sampling process can't go indefinitely wrong, right at some point or another things come to stabilize.",
                    "label": 0
                },
                {
                    "sent": "You got some nice theorems linked with actually with the random numbers we saw at the beginning, which come to say that one day or another you know, even if you can't say when things will tend to be able to say things, how bad you're going to be.",
                    "label": 0
                },
                {
                    "sent": "But the sampling is gone.",
                    "label": 0
                },
                {
                    "sent": "It's not going to be that bad.",
                    "label": 0
                },
                {
                    "sent": "So that means that the probability same probability one really means that the probability of not converging is zero.",
                    "label": 0
                },
                {
                    "sent": "OK, zero probability of.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Far off not converging.",
                    "label": 0
                },
                {
                    "sent": "So what can we do with this?",
                    "label": 0
                },
                {
                    "sent": "But they are all well.",
                    "label": 0
                },
                {
                    "sent": "There are actually some some results.",
                    "label": 0
                },
                {
                    "sent": "Very surprising.",
                    "label": 0
                },
                {
                    "sent": "There are a variety of algorithms to learn regular languages.",
                    "label": 0
                },
                {
                    "sent": "This is from my previous speaker, so I'll give those.",
                    "label": 0
                },
                {
                    "sent": "But there are a number of algorithms that have come out that learn both the structure and the probabilities, and do that with the proof that they really identify in the limit and they normally work also in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "But of course Alas, should I say we do not have any nice convergence results saying?",
                    "label": 0
                },
                {
                    "sent": "We're sure pack type results saying we're sure that they have worked OK, so at least we do know they're going to some things under happen, but they still are used in practice in in quite a few fields around 'em pattern recognition, so I can't say anything in these cases about bounded resources apart from the fact that at least they run in polynomial time.",
                    "label": 0
                },
                {
                    "sent": "Now, if it's not true, there is 1 result and I'm being unfair to Alex, who's here that they do have Alex and Falk do have a result where they can by putting some.",
                    "label": 0
                },
                {
                    "sent": "I say some conditions on the type of probabilities or the number of probabilities on the on the different States and some technical items like that they do obtain results saying we do obtain this reasonably quickly, but they've got better ones, at least ones are more.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Parable more understandable and my view right is to say well if we accept to merge the different settings and we say we want to again learn a distribution so regular grammar or a context free grammar and this time we were not interested in perfection.",
                    "label": 0
                },
                {
                    "sent": "What we want to do is make some errors from time to time, right?",
                    "label": 0
                },
                {
                    "sent": "The famous 1 minus Delta and so we accept to make epsilon errors.",
                    "label": 0
                },
                {
                    "sent": "So then we're facing a little, not just.",
                    "label": 0
                },
                {
                    "sent": "Not just purely syntactic trick with it's very profound meaning.",
                    "label": 0
                },
                {
                    "sent": "The problem which is I have to decide what distance I'm going to use you going to actually have one distribution which is the target.",
                    "label": 0
                },
                {
                    "sent": "Again another distribution, which is whatever you're learning your hypothesis and you're going to try and have to build something between the two and say how far apart are they went from the other and there there are variety of notions of distances that can be used.",
                    "label": 0
                },
                {
                    "sent": "And of course depending on how harsh or distances.",
                    "label": 0
                },
                {
                    "sent": "Well, you're going to end up by having.",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Results saying in one case for one distance is too easy for another distance is too hard and there is.",
                    "label": 0
                },
                {
                    "sent": "There is a lot of ongoing work on this, and I suppose that asking me or certainly asking Alex about these things.",
                    "label": 0
                },
                {
                    "sent": "For anyone interested.",
                    "label": 0
                },
                {
                    "sent": "I mean the people who are coming out with algorithms that are capable of PAC Learning PAC Learning.",
                    "label": 0
                },
                {
                    "sent": "Distributions, usually with some concerns, syntactic constraints also on the type of probabilities you can have on the automata, so they can look at that.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, I think it's about time I can I conclude.",
                    "label": 0
                },
                {
                    "sent": "So for those that are not convinced that there is a difference between grammatical inference and grammar induction just just just out of the.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Xbox or the text references.",
                    "label": 0
                },
                {
                    "sent": "This is a notion that very often appears.",
                    "label": 0
                },
                {
                    "sent": "I don't know.",
                    "label": 0
                },
                {
                    "sent": "I haven't seen it around here.",
                    "label": 0
                },
                {
                    "sent": "The idea is that one of its called structural completeness.",
                    "label": 0
                },
                {
                    "sent": "Structural completeness relates to the facts in the case of a DFA I'm giving it because I can do a picture, but you could do the same with the context free grammar of saying that on one hand you've got a sample.",
                    "label": 0
                },
                {
                    "sent": "On the other hand, you've got an automaton, and you're going to say that there is structural completeness when basically nothing is invented right, whatever edge or whatever final state.",
                    "label": 0
                },
                {
                    "sent": "There isn't the automaton.",
                    "label": 0
                },
                {
                    "sent": "This is justified by something.",
                    "label": 0
                },
                {
                    "sent": "In the.",
                    "label": 0
                },
                {
                    "sent": "In the.",
                    "label": 0
                },
                {
                    "sent": "Or if you are in the sample and typical algorithms are doing, the following is itself looking for any DFA.",
                    "label": 0
                },
                {
                    "sent": "They're only going to look inside the set of enormous set of all the DFA where the sample is structurally complete.",
                    "label": 0
                },
                {
                    "sent": "Now what's so?",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Just the picture to show it first, so this would not be structurally complete because for example if I decided to learn this well, you could think well where did you get this edge from?",
                    "label": 0
                },
                {
                    "sent": "You know how did you invent this B here?",
                    "label": 0
                },
                {
                    "sent": "If you look at each one of those items on the sample, so we're entering the automaton here and we can see we can do a AB, we can do B.",
                    "label": 0
                },
                {
                    "sent": "We can do a ABA Ann BB.",
                    "label": 0
                },
                {
                    "sent": "Oh dear, very bad a. I'm sorry a BA.",
                    "label": 0
                },
                {
                    "sent": "OK so you never use this one.",
                    "label": 0
                },
                {
                    "sent": "So since you don't use this one well in a way you've invented it out of the blue.",
                    "label": 0
                },
                {
                    "sent": "So there's discussions about if you should do this or you shouldn't do.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "But what's curious is this is that when you read people writing about this summer, going to say that the automaton has to be structurally complete for the sample.",
                    "label": 0
                },
                {
                    "sent": "This is what I've just said 2 seconds ago.",
                    "label": 0
                },
                {
                    "sent": "OK, we've got this sample and we're only going to look at the set of automaton automata that are structurally complete for this sample, so that's what is the grammar induction OK, and the other one is to say that now this is the target automaton, so I am.",
                    "label": 0
                },
                {
                    "sent": "Only going to be able to learn when the sample when there is going to be a sample that is structurally complete for this automata in it.",
                    "label": 0
                },
                {
                    "sent": "OK, so it's due 2 dual points of view of the same problem, but that's where I'm putting a bit.",
                    "label": 0
                },
                {
                    "sent": "The difference between grammar induction and grammatical inference.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So so many things I haven't talked about.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of work on new algorithms and certainly a lot of work to be done for these algorithms to be known.",
                    "label": 0
                },
                {
                    "sent": "There are many other applications, so if one of the nice things is that is that when we're working on an application on biology, sometimes there's something wrong with the algorithm where we should be coming to listen to you to be able to think one second, this might be of interest for these guys, OK?",
                    "label": 0
                },
                {
                    "sent": "So machine translation, computational biology, NLP software engineering.",
                    "label": 0
                },
                {
                    "sent": "One of the nicer tasks I saw last year was somebody in India who they were having to do some sort of reverse engineering task where they were.",
                    "label": 0
                },
                {
                    "sent": "They had these programs written 20 years ago in some bizarre dialect of C or some basic or something, and for which there was no more grammar left.",
                    "label": 0
                },
                {
                    "sent": "OK, so they just had the.",
                    "label": 0
                },
                {
                    "sent": "The program and they didn't have that dialect, so they then had to reconstruct the grammar for this dialect right with probably knowing the grammar of a close enough dialect.",
                    "label": 0
                },
                {
                    "sent": "That's sort of applications you find.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Web mining robotics an just that people interested in this.",
                    "label": 0
                },
                {
                    "sent": "There is a series of conferences on it called I CGI and next one should be in Brittany in 2008 and I cheated instead of giving one page abstract I gave 5 pages so there are a few references there and if not there are references in the grammatical inference webpage.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'm done.",
                    "label": 0
                },
                {
                    "sent": "This is just in case you'd ask me questions about technical issues.",
                    "label": 0
                }
            ]
        }
    }
}