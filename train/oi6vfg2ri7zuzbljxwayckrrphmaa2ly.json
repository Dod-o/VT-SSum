{
    "id": "oi6vfg2ri7zuzbljxwayckrrphmaa2ly",
    "title": "Neural Programmer-Interpreters",
    "info": {
        "author": [
            "Scott Reed, Department of Electrical Engineering and Computer Science, University of Michigan"
        ],
        "published": "May 27, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Machine Learning"
        ]
    },
    "url": "http://videolectures.net/iclr2016_reed_neural_programmer/",
    "segmentation": [
        [
            "OK so I will talk about neural programmer interpreters."
        ],
        [
            "Or npi.",
            "And the goal the design goals were to model potentially long sequences of actions by exploiting compositional structure and also do some degree of continual learning where we learn new programs by composing previously learn programs rather than learning everything from scratch.",
            "And also to do it in a data efficient way.",
            "Learn program that can generalize from a small number of example traces and also be interpretable so we want to look at MPs generated commands and understand what it's doing at multiple levels of temporal abstraction."
        ],
        [
            "So now we talk about the model.",
            "We train it."
        ],
        [
            "And for now, on program traces at every time step we have an input and outputs, so the inputs would be an observation of your environment.",
            "The program index that you're calling for the current time step, and then the next program that you call.",
            "So it's it in it plus one.",
            "The arguments to the current program, and then the generated are arguments to the program you're calling at the next time step, and also a return bit indicating whether you want to halt execution of the current program now will zoom into one."
        ],
        [
            "Time step of what MPI does, so we have a key value memory and now we the current program is I sub T for time step T and we take that row and look at the program vector shown in pink here and that's going to be an input to a recurrent core module.",
            "We use an LTM for MPI and along with this program vector PT we're going to give the arguments 80 some observation of the environment ET which are concatenated and encoded.",
            "And then you also encode that together with the program vector.",
            "Take from the inputs to NPI.",
            "At that time, step and the outputs you'll produce key vector, Katie, the return probability Artie and the arguments 80 + 1 to the next program.",
            "So once you have those, you then do a content based look up in your memory.",
            "So you take your key embedding and find the row that matches at most closely and then that row.",
            "You'll take that program vector and pass it to the.",
            "To the program that you're calling.",
            "When the program you call at that time step returns, control will return to this level of Npis execution, and you can then call another program or you can halt execution of the current program.",
            "So the convention that I used for this experience was you always either call a program or halt execution of the current program at each time step.",
            "So."
        ],
        [
            "I'll do some demos, I'll talk about the demos now."
        ],
        [
            "The first environment that we used was addition multi digit addition and so in this environment you have a scratch pad where you write down the problem.",
            "In the first 2 rows.",
            "So here we have like 96 + 125 you have a carry Rd to record to carry bits and even output row where NPI should learn to write down the correct answer.",
            "So the interface is the scratch pad.",
            "You have pointers that can read and write values and the lowest level action you can take in this environment is just to move pointers left or right.",
            "Or to write down the value at a specified location at a specified pointer."
        ],
        [
            "And so we decompose this into multi digit addition into single digit ad carry shifting pointers and then the lowest level program that actually implements these changes to the environment.",
            "So here's some exam."
        ],
        [
            "Of running it so in the left you see the evolution of this scratchpad.",
            "Where Carrie bits are being recorded, and then output symbols are being written.",
            "As we sweep the pointers from left to right, the pointer positions are marked with this red* in the middle column you get the view that NPI the core actually sees, so it's getting an environment from the observation of the environment at each time step, and the input program at each time step.",
            "The previous NPI hidden units.",
            "And it's outputing the next program to run, and the arguments that program and then it will pass its LTM hidden units to the next.",
            "Step of NPY processing.",
            "So you can think of MPI as sort of router between programs where given a program you want to generate the next program in its arguments and on the right you actually see the generated commands."
        ],
        [
            "Next environment we looked at was sorting, so we implemented bubble sort.",
            "Here the scratchpad is just one row and I'm showing multiple time steps.",
            "The interface here is a scratchpad with with the array of numbers to be sorted.",
            "You have again pointers to pointers and the lowest level programs are left and right that can move specified pointer left or right and you can swap their values."
        ],
        [
            "So we break it.",
            "The this sort of problem down into bubble sort and its constituent subprograms."
        ],
        [
            "And here's an example of running it on the left you see the array being sorted as.",
            "These pointers sweep left to right.",
            "Doing this bubble program.",
            "The interesting thing about this problem is that the number of steps obviously is going to grow more than linearly with the size of the input.",
            "So if you just count the number of generated commands, it's much bigger than the size of the array, but by decomposing the sequence of actions you need to execute by exploiting compositional structure, it's much easier to learn."
        ],
        [
            "The third environment that was canonicalizing the view of 3D cars.",
            "So in this environment you get a rendering of the car, just the pixels and the target angle and elevation that you would like to view the car in.",
            "So here we want to move it to angle one, elevation two, let's say.",
            "And would you have access to our left, right, up, down programs that can move the car 15 and 15 degree increments?",
            "An you don't get the current car pose, so you actually have to learn from pixels.",
            "Basically what the current car poses and so."
        ],
        [
            "This down into this, go to program into horizontal movement where you go left or right and vertical movement where you go up and down."
        ],
        [
            "In here are some examples of.",
            "Canonicalizing several cars.",
            "And so here we don't retrain the comment.",
            "It's all trained end to end from pixels for this task.",
            "And we can also generalize to seen cars from unseen views or unseen cars that the model hasn't seen before."
        ],
        [
            "Now I'll just step through a more detailed example, will zoom out and look at multiple steps of what MPI is doing.",
            "So here is the cars example.",
            "And what do you see here at the environment provides pixels and the target coordinates.",
            "We encode these, this rendering using a confident and eventually get a fixed length input vector to MPI, and we also have the go to program vector.",
            "Now this time step we output the key, the end probability and the arguments.",
            "The next program we do a lookup in memory and it calls the H O2 program.",
            "So we need to do horizontal movement.",
            "It figures out the car needs to move left so it calls the L go to.",
            "Program Algo 2 calls the Low Level Act program that actually implements this 15 degree increment at runs.",
            "And then it returns.",
            "So Act finishes the cars when rotated and control returns to helgoe two it says it's still not actually frontal yet, so we need to call act one more time.",
            "That runs that returns.",
            "Now we're at the frontal angle, so actually L go to can return and horizontal goto can also return, so control is returned back to go to.",
            "Notice that the hidden units at this time step are coming from the very first time step, so none of the subprogram hidden states have affected the hidden state of.",
            "The go to program so we actually maintain a stack of program States and this helps.",
            "Learning more efficiently, so now we change the.",
            "Elevation of the car, so it calls Viggo two figures need to go down and then once the car is reached his target pose.",
            "The program is finished."
        ],
        [
            "And I'll talk about."
        ],
        [
            "More analysis of the model, so we looked at the data efficiency of sorting and we compared to a sequence sequence LTM of the same number of layers and hidden units and with the sequence LCM.",
            "The way that you can sort is just as input.",
            "You have the unsorted array of numbers and then the output is just those numbers in sorted order.",
            "So here we train on length 20 arrays of single digit numbers an on the X axis.",
            "We have the number of training examples and what we see is that both models can learn to solve this problem, but NPI can solve it with a smaller number of sorting instances.",
            "But note that MPI has the advantage that it can mine multiple subprogram examples per sorting instance.",
            "So like one bubble sort of length 20 is going to call the bubble program 20 times, so you can actually mine a lot more information out of a single.",
            "Sorting instance.",
            "But NPR is also equipped to exploit that information when it's there."
        ],
        [
            "We can also look at the generalization ability, so we looked at.",
            "We trained on problem problems of length one up to 20 and for each length we had we used 64 example problems.",
            "So we trained sequences, LCM, NPI and both of them could perfectly sort the numbers up to length 20.",
            "Which is the training sequence length and then beyond 20 we observed that the sequence sequence didn't generalize very well and passed like length 25.",
            "It wouldn't be able to sort, whereas NPI generalized too much longer arrays up to 155."
        ],
        [
            "We also looked at the generalization.",
            "Properties for the addition problem.",
            "So, for example, if we had a problem like 90 + 160 is 250, we could encode this problem as a sequence and then learn to solve it just as a sequence prediction and learn to execute paper.",
            "Did something like this.",
            "We had a hard time making this work perfectly, so we did a few thing."
        ],
        [
            "To make it a bit easier, so one thing you could do is to try to increase the locality of the problem by my reversing the digits and stacking them so and model the sequence that way.",
            "And an even easier version.",
            "Would be to not only reverse the digits would also stack the output right in the input, so in this easy version of the competition is almost entirely local."
        ],
        [
            "So if you look at the performance, what we see is the stacked version.",
            "It doesn't actually generalize that well on the X axis.",
            "Again, you have the sequence length, and it can do addition to a degree, but doesn't generalize beyond 20 almost at all."
        ],
        [
            "When we make it the problem easier, we verse and stack the input and output actually generalizes quite well."
        ],
        [
            "And can do it can solve addition for problems of hundreds of digits.",
            "And then end."
        ],
        [
            "Yeah, I didn't have didn't seem to have a problem generalizing 2000s."
        ],
        [
            "Another problem we looked at was can we make NPI multitask?",
            "So can we share the same NPI core module across many different programs even if those tasks don't don't necessarily share structure, so the same model that does addition sorting, canonicalizing cars and whatever else?",
            "So the way to do that would simply be to put all those programs in the same memory whenever you want to add a new task, just have more rows of memory so you can grow capacity by growing memory rather than somehow adding capacity directly to your recurrent core module.",
            "And we so we did that.",
            "We trained a single MPI for all three of these tasks and we observe it can achieve comperable performance to any of the single task Npis for those tasks we."
        ],
        [
            "We're also interested in whether we could learn new programs with a fixed core module.",
            "So a toy example would be Max.",
            "Finding an array, I mean a simple way to do it is not algorithmically optimal, but you could just sort the array and then just take the rightmost element so we have a model.",
            "Now that can do sorting, so we could add 2 new programs.",
            "I'll call the first one, or jump, just move the pointer to the right of the array by repeatedly calling this our shift program, but it knows and then Max where you just called sort, and then our jump, and then read off the.",
            "Pointers value, so how would you do this?",
            "How would you learn these two without forgetting what the MPI already knows so?"
        ],
        [
            "A simple way is just add two rows to your memory, one for each of these programs.",
            "You randomly initialize them, you freeze the core, freeze all the other program vectors and just do training as you normally do and back propagate gradients just to those new program vectors.",
            "So does that work?",
            "We find that it."
        ],
        [
            "Does so for these tasks we find that multi task NPI performance just as good as single task at any of these tasks.",
            "And when we add the Max finding program to the array, we don't observe any degradation in performance and then we can also make Max work accurately."
        ],
        [
            "So."
        ],
        [
            "Conclusions are that single MPI can learn multiple programs and very dissimilar environments with different affordances and the sorting addition programs shows strong generalization compared to the baseline sequence to sequence models, and even if you fix the core, you can continue to learn new programs without forgetting already learn programs, at least for the toy problem that we looked at as a proof of concept, and so for the next steps when I reduced supervision, scale up the number of programs, integrate new perception modules and affordances."
        ],
        [
            "I want to talk about briefly related work.",
            "This is just too much to cover 20 minutes, but we are influenced by the Sigma \u03c0 units from the PDP paper where the activations of one network become the weights of a second one and also the idea of slow weights and fast weights.",
            "An also hierarchical RL where you learn at multiple levels of temporal abstraction.",
            "There's lots of recent extensions of sequence sequence like NTM Center networks, networks stack Q, deck augmented RNS.",
            "Several other articles, papers on production which you should check out.",
            "The main difference to our workers that we have this explicit notion of calling programs and compositionality.",
            "There's been recent models of prefrontal cognitive control that also used this metaphor.",
            "Programmers and interpreters and also the learning to execute paper was influential, so there's probably more papers, and we have more inner references, so come check out our poster.",
            "Thank you."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK so I will talk about neural programmer interpreters.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or npi.",
                    "label": 0
                },
                {
                    "sent": "And the goal the design goals were to model potentially long sequences of actions by exploiting compositional structure and also do some degree of continual learning where we learn new programs by composing previously learn programs rather than learning everything from scratch.",
                    "label": 1
                },
                {
                    "sent": "And also to do it in a data efficient way.",
                    "label": 1
                },
                {
                    "sent": "Learn program that can generalize from a small number of example traces and also be interpretable so we want to look at MPs generated commands and understand what it's doing at multiple levels of temporal abstraction.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So now we talk about the model.",
                    "label": 0
                },
                {
                    "sent": "We train it.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And for now, on program traces at every time step we have an input and outputs, so the inputs would be an observation of your environment.",
                    "label": 0
                },
                {
                    "sent": "The program index that you're calling for the current time step, and then the next program that you call.",
                    "label": 0
                },
                {
                    "sent": "So it's it in it plus one.",
                    "label": 0
                },
                {
                    "sent": "The arguments to the current program, and then the generated are arguments to the program you're calling at the next time step, and also a return bit indicating whether you want to halt execution of the current program now will zoom into one.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Time step of what MPI does, so we have a key value memory and now we the current program is I sub T for time step T and we take that row and look at the program vector shown in pink here and that's going to be an input to a recurrent core module.",
                    "label": 0
                },
                {
                    "sent": "We use an LTM for MPI and along with this program vector PT we're going to give the arguments 80 some observation of the environment ET which are concatenated and encoded.",
                    "label": 0
                },
                {
                    "sent": "And then you also encode that together with the program vector.",
                    "label": 0
                },
                {
                    "sent": "Take from the inputs to NPI.",
                    "label": 0
                },
                {
                    "sent": "At that time, step and the outputs you'll produce key vector, Katie, the return probability Artie and the arguments 80 + 1 to the next program.",
                    "label": 0
                },
                {
                    "sent": "So once you have those, you then do a content based look up in your memory.",
                    "label": 0
                },
                {
                    "sent": "So you take your key embedding and find the row that matches at most closely and then that row.",
                    "label": 0
                },
                {
                    "sent": "You'll take that program vector and pass it to the.",
                    "label": 0
                },
                {
                    "sent": "To the program that you're calling.",
                    "label": 0
                },
                {
                    "sent": "When the program you call at that time step returns, control will return to this level of Npis execution, and you can then call another program or you can halt execution of the current program.",
                    "label": 0
                },
                {
                    "sent": "So the convention that I used for this experience was you always either call a program or halt execution of the current program at each time step.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'll do some demos, I'll talk about the demos now.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The first environment that we used was addition multi digit addition and so in this environment you have a scratch pad where you write down the problem.",
                    "label": 0
                },
                {
                    "sent": "In the first 2 rows.",
                    "label": 0
                },
                {
                    "sent": "So here we have like 96 + 125 you have a carry Rd to record to carry bits and even output row where NPI should learn to write down the correct answer.",
                    "label": 1
                },
                {
                    "sent": "So the interface is the scratch pad.",
                    "label": 1
                },
                {
                    "sent": "You have pointers that can read and write values and the lowest level action you can take in this environment is just to move pointers left or right.",
                    "label": 1
                },
                {
                    "sent": "Or to write down the value at a specified location at a specified pointer.",
                    "label": 1
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And so we decompose this into multi digit addition into single digit ad carry shifting pointers and then the lowest level program that actually implements these changes to the environment.",
                    "label": 0
                },
                {
                    "sent": "So here's some exam.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of running it so in the left you see the evolution of this scratchpad.",
                    "label": 0
                },
                {
                    "sent": "Where Carrie bits are being recorded, and then output symbols are being written.",
                    "label": 0
                },
                {
                    "sent": "As we sweep the pointers from left to right, the pointer positions are marked with this red* in the middle column you get the view that NPI the core actually sees, so it's getting an environment from the observation of the environment at each time step, and the input program at each time step.",
                    "label": 0
                },
                {
                    "sent": "The previous NPI hidden units.",
                    "label": 0
                },
                {
                    "sent": "And it's outputing the next program to run, and the arguments that program and then it will pass its LTM hidden units to the next.",
                    "label": 0
                },
                {
                    "sent": "Step of NPY processing.",
                    "label": 0
                },
                {
                    "sent": "So you can think of MPI as sort of router between programs where given a program you want to generate the next program in its arguments and on the right you actually see the generated commands.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Next environment we looked at was sorting, so we implemented bubble sort.",
                    "label": 0
                },
                {
                    "sent": "Here the scratchpad is just one row and I'm showing multiple time steps.",
                    "label": 0
                },
                {
                    "sent": "The interface here is a scratchpad with with the array of numbers to be sorted.",
                    "label": 1
                },
                {
                    "sent": "You have again pointers to pointers and the lowest level programs are left and right that can move specified pointer left or right and you can swap their values.",
                    "label": 1
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we break it.",
                    "label": 0
                },
                {
                    "sent": "The this sort of problem down into bubble sort and its constituent subprograms.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And here's an example of running it on the left you see the array being sorted as.",
                    "label": 0
                },
                {
                    "sent": "These pointers sweep left to right.",
                    "label": 0
                },
                {
                    "sent": "Doing this bubble program.",
                    "label": 0
                },
                {
                    "sent": "The interesting thing about this problem is that the number of steps obviously is going to grow more than linearly with the size of the input.",
                    "label": 0
                },
                {
                    "sent": "So if you just count the number of generated commands, it's much bigger than the size of the array, but by decomposing the sequence of actions you need to execute by exploiting compositional structure, it's much easier to learn.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The third environment that was canonicalizing the view of 3D cars.",
                    "label": 0
                },
                {
                    "sent": "So in this environment you get a rendering of the car, just the pixels and the target angle and elevation that you would like to view the car in.",
                    "label": 1
                },
                {
                    "sent": "So here we want to move it to angle one, elevation two, let's say.",
                    "label": 0
                },
                {
                    "sent": "And would you have access to our left, right, up, down programs that can move the car 15 and 15 degree increments?",
                    "label": 1
                },
                {
                    "sent": "An you don't get the current car pose, so you actually have to learn from pixels.",
                    "label": 0
                },
                {
                    "sent": "Basically what the current car poses and so.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This down into this, go to program into horizontal movement where you go left or right and vertical movement where you go up and down.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In here are some examples of.",
                    "label": 0
                },
                {
                    "sent": "Canonicalizing several cars.",
                    "label": 0
                },
                {
                    "sent": "And so here we don't retrain the comment.",
                    "label": 0
                },
                {
                    "sent": "It's all trained end to end from pixels for this task.",
                    "label": 0
                },
                {
                    "sent": "And we can also generalize to seen cars from unseen views or unseen cars that the model hasn't seen before.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'll just step through a more detailed example, will zoom out and look at multiple steps of what MPI is doing.",
                    "label": 0
                },
                {
                    "sent": "So here is the cars example.",
                    "label": 0
                },
                {
                    "sent": "And what do you see here at the environment provides pixels and the target coordinates.",
                    "label": 0
                },
                {
                    "sent": "We encode these, this rendering using a confident and eventually get a fixed length input vector to MPI, and we also have the go to program vector.",
                    "label": 0
                },
                {
                    "sent": "Now this time step we output the key, the end probability and the arguments.",
                    "label": 0
                },
                {
                    "sent": "The next program we do a lookup in memory and it calls the H O2 program.",
                    "label": 0
                },
                {
                    "sent": "So we need to do horizontal movement.",
                    "label": 0
                },
                {
                    "sent": "It figures out the car needs to move left so it calls the L go to.",
                    "label": 0
                },
                {
                    "sent": "Program Algo 2 calls the Low Level Act program that actually implements this 15 degree increment at runs.",
                    "label": 0
                },
                {
                    "sent": "And then it returns.",
                    "label": 0
                },
                {
                    "sent": "So Act finishes the cars when rotated and control returns to helgoe two it says it's still not actually frontal yet, so we need to call act one more time.",
                    "label": 0
                },
                {
                    "sent": "That runs that returns.",
                    "label": 0
                },
                {
                    "sent": "Now we're at the frontal angle, so actually L go to can return and horizontal goto can also return, so control is returned back to go to.",
                    "label": 0
                },
                {
                    "sent": "Notice that the hidden units at this time step are coming from the very first time step, so none of the subprogram hidden states have affected the hidden state of.",
                    "label": 0
                },
                {
                    "sent": "The go to program so we actually maintain a stack of program States and this helps.",
                    "label": 0
                },
                {
                    "sent": "Learning more efficiently, so now we change the.",
                    "label": 0
                },
                {
                    "sent": "Elevation of the car, so it calls Viggo two figures need to go down and then once the car is reached his target pose.",
                    "label": 0
                },
                {
                    "sent": "The program is finished.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And I'll talk about.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "More analysis of the model, so we looked at the data efficiency of sorting and we compared to a sequence sequence LTM of the same number of layers and hidden units and with the sequence LCM.",
                    "label": 1
                },
                {
                    "sent": "The way that you can sort is just as input.",
                    "label": 0
                },
                {
                    "sent": "You have the unsorted array of numbers and then the output is just those numbers in sorted order.",
                    "label": 1
                },
                {
                    "sent": "So here we train on length 20 arrays of single digit numbers an on the X axis.",
                    "label": 0
                },
                {
                    "sent": "We have the number of training examples and what we see is that both models can learn to solve this problem, but NPI can solve it with a smaller number of sorting instances.",
                    "label": 1
                },
                {
                    "sent": "But note that MPI has the advantage that it can mine multiple subprogram examples per sorting instance.",
                    "label": 0
                },
                {
                    "sent": "So like one bubble sort of length 20 is going to call the bubble program 20 times, so you can actually mine a lot more information out of a single.",
                    "label": 0
                },
                {
                    "sent": "Sorting instance.",
                    "label": 0
                },
                {
                    "sent": "But NPR is also equipped to exploit that information when it's there.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We can also look at the generalization ability, so we looked at.",
                    "label": 0
                },
                {
                    "sent": "We trained on problem problems of length one up to 20 and for each length we had we used 64 example problems.",
                    "label": 1
                },
                {
                    "sent": "So we trained sequences, LCM, NPI and both of them could perfectly sort the numbers up to length 20.",
                    "label": 0
                },
                {
                    "sent": "Which is the training sequence length and then beyond 20 we observed that the sequence sequence didn't generalize very well and passed like length 25.",
                    "label": 0
                },
                {
                    "sent": "It wouldn't be able to sort, whereas NPI generalized too much longer arrays up to 155.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We also looked at the generalization.",
                    "label": 0
                },
                {
                    "sent": "Properties for the addition problem.",
                    "label": 0
                },
                {
                    "sent": "So, for example, if we had a problem like 90 + 160 is 250, we could encode this problem as a sequence and then learn to solve it just as a sequence prediction and learn to execute paper.",
                    "label": 1
                },
                {
                    "sent": "Did something like this.",
                    "label": 0
                },
                {
                    "sent": "We had a hard time making this work perfectly, so we did a few thing.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "To make it a bit easier, so one thing you could do is to try to increase the locality of the problem by my reversing the digits and stacking them so and model the sequence that way.",
                    "label": 1
                },
                {
                    "sent": "And an even easier version.",
                    "label": 1
                },
                {
                    "sent": "Would be to not only reverse the digits would also stack the output right in the input, so in this easy version of the competition is almost entirely local.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So if you look at the performance, what we see is the stacked version.",
                    "label": 0
                },
                {
                    "sent": "It doesn't actually generalize that well on the X axis.",
                    "label": 0
                },
                {
                    "sent": "Again, you have the sequence length, and it can do addition to a degree, but doesn't generalize beyond 20 almost at all.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "When we make it the problem easier, we verse and stack the input and output actually generalizes quite well.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And can do it can solve addition for problems of hundreds of digits.",
                    "label": 0
                },
                {
                    "sent": "And then end.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yeah, I didn't have didn't seem to have a problem generalizing 2000s.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Another problem we looked at was can we make NPI multitask?",
                    "label": 0
                },
                {
                    "sent": "So can we share the same NPI core module across many different programs even if those tasks don't don't necessarily share structure, so the same model that does addition sorting, canonicalizing cars and whatever else?",
                    "label": 0
                },
                {
                    "sent": "So the way to do that would simply be to put all those programs in the same memory whenever you want to add a new task, just have more rows of memory so you can grow capacity by growing memory rather than somehow adding capacity directly to your recurrent core module.",
                    "label": 0
                },
                {
                    "sent": "And we so we did that.",
                    "label": 0
                },
                {
                    "sent": "We trained a single MPI for all three of these tasks and we observe it can achieve comperable performance to any of the single task Npis for those tasks we.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "We're also interested in whether we could learn new programs with a fixed core module.",
                    "label": 1
                },
                {
                    "sent": "So a toy example would be Max.",
                    "label": 1
                },
                {
                    "sent": "Finding an array, I mean a simple way to do it is not algorithmically optimal, but you could just sort the array and then just take the rightmost element so we have a model.",
                    "label": 0
                },
                {
                    "sent": "Now that can do sorting, so we could add 2 new programs.",
                    "label": 0
                },
                {
                    "sent": "I'll call the first one, or jump, just move the pointer to the right of the array by repeatedly calling this our shift program, but it knows and then Max where you just called sort, and then our jump, and then read off the.",
                    "label": 1
                },
                {
                    "sent": "Pointers value, so how would you do this?",
                    "label": 0
                },
                {
                    "sent": "How would you learn these two without forgetting what the MPI already knows so?",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "A simple way is just add two rows to your memory, one for each of these programs.",
                    "label": 0
                },
                {
                    "sent": "You randomly initialize them, you freeze the core, freeze all the other program vectors and just do training as you normally do and back propagate gradients just to those new program vectors.",
                    "label": 1
                },
                {
                    "sent": "So does that work?",
                    "label": 0
                },
                {
                    "sent": "We find that it.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Does so for these tasks we find that multi task NPI performance just as good as single task at any of these tasks.",
                    "label": 0
                },
                {
                    "sent": "And when we add the Max finding program to the array, we don't observe any degradation in performance and then we can also make Max work accurately.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Conclusions are that single MPI can learn multiple programs and very dissimilar environments with different affordances and the sorting addition programs shows strong generalization compared to the baseline sequence to sequence models, and even if you fix the core, you can continue to learn new programs without forgetting already learn programs, at least for the toy problem that we looked at as a proof of concept, and so for the next steps when I reduced supervision, scale up the number of programs, integrate new perception modules and affordances.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I want to talk about briefly related work.",
                    "label": 0
                },
                {
                    "sent": "This is just too much to cover 20 minutes, but we are influenced by the Sigma \u03c0 units from the PDP paper where the activations of one network become the weights of a second one and also the idea of slow weights and fast weights.",
                    "label": 1
                },
                {
                    "sent": "An also hierarchical RL where you learn at multiple levels of temporal abstraction.",
                    "label": 0
                },
                {
                    "sent": "There's lots of recent extensions of sequence sequence like NTM Center networks, networks stack Q, deck augmented RNS.",
                    "label": 1
                },
                {
                    "sent": "Several other articles, papers on production which you should check out.",
                    "label": 0
                },
                {
                    "sent": "The main difference to our workers that we have this explicit notion of calling programs and compositionality.",
                    "label": 1
                },
                {
                    "sent": "There's been recent models of prefrontal cognitive control that also used this metaphor.",
                    "label": 0
                },
                {
                    "sent": "Programmers and interpreters and also the learning to execute paper was influential, so there's probably more papers, and we have more inner references, so come check out our poster.",
                    "label": 0
                },
                {
                    "sent": "Thank you.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": []
        }
    }
}