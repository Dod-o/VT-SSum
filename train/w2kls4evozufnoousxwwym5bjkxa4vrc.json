{
    "id": "w2kls4evozufnoousxwwym5bjkxa4vrc",
    "title": "Fast Segmentation via Randomized Hashing",
    "info": {
        "author": [
            "Camillo J. Taylor, University of Pennsylvania"
        ],
        "published": "Dec. 1, 2009",
        "recorded": "September 2009",
        "category": [
            "Top->Computer Science->Computer Vision"
        ]
    },
    "url": "http://videolectures.net/bmvc09_taylor_fsrh/",
    "segmentation": [
        [
            "Morning my name is CJ Taylor.",
            "I'm going to be presenting some work I've been doing with my colleague Anthony Coley.",
            "So let's go ahead and begin at the beginning.",
            "So like most people, the way that we're going to.",
            "View the segmentation problem is one of taking an input image and trying to divide it automatically into some set of coherent regions.",
            "And ideally we'd like to do this both accurately and quickly while they can."
        ],
        [
            "Some debate about what we mean about accuracy with segmentation.",
            "I think we understand what speed is, so we're going to try to propose a method that tries to achieve both aims."
        ],
        [
            "Of course, there's a vast literature in this area, and I guess I must apologize and move match any summary I could imagine is necessarily incomplete, but with some amount of violence we could broadly categorize prior work into these sort of 3 three main groups.",
            "The graph based methods is exemplified by normalized cuts, an agglomerative, and divisive techniques like Felzenszwalb and huttenlocher.",
            "There.",
            "Of course, these recent learning based methods, which make use of training data and can do more sophisticated things like label pixels and.",
            "Prior talk was a very good example of that kind of thinking.",
            "The approach we're going to place we're going to be describing is.",
            "Most related to the stuff that's done feature based method.",
            "In fact it is a feature based method an in that there the goal is to recast the segmentation problem as a clustering problem in some appropriate feature space.",
            "So this would be methods like K means and of course methods like mean Shift.",
            "In fact, our method is very similar to mean shift in that.",
            "The underlying concept in mean shift is to look for Moser Maxima in distribution."
        ],
        [
            "In the feature space and our method is going to be similar, but we're going to go out in a very different manner.",
            "OK, so.",
            "I want to begin with what content should be a fairly simple clustering problem, so we're going to have a bunch of feature vectors distributed in an M dimensional feature space.",
            "In order to accommodate the limitations of the projector, we're going to stick to M Equal 2, but you should understand everything I'm saying here is being applicable in general dimensions.",
            "And so the idea here is that we have a bunch of feature space, but bunch of feature vectors they are divided into these clusters.",
            "We don't know how many clusters there are.",
            "We don't know how many.",
            "Answer In each cluster, but we have this idea that we have this set of points that are.",
            "They have this kind of this kind of distribution.",
            "I like to refer to this kind of galactic model and that we have these centers of probability.",
            "Separated by vast areas of open space.",
            "Sandra Descript at UCSD, has a.",
            "More precise, less poetic way of characterizing things like this, he has this notion of a C separation which essentially considers the ratio between the Inter and intra.",
            "Last variations, so the larger the separation, the more widely spaced the clusters are, and so on.",
            "OK, so if we had a situation like this."
        ],
        [
            "We're interested in sort of splitting them into into into various groupings."
        ],
        [
            "One way we could proceed is simple.",
            "We can proceed as follows.",
            "First, we will go ahead and select a direction vector at random.",
            "Let's call that you."
        ],
        [
            "And then we're going to go ahead and take all our feature points and project them onto this direction.",
            "Yielding a set of a set of values.",
            "And finally we can select a. Splitting value which I have determined here."
        ],
        [
            "And this splitting value takes the feature vectors and divide them into two classes 0101 depending on whether there are which side of the splitting splitting value thereon.",
            "Now there are a number of ways."
        ],
        [
            "I could go about choosing the splitting value.",
            "We could choose the mean of the projected values, or we could look at the Max and min and choose someplace in the middle or choose the median.",
            "Etc.",
            "For practical reasons, actually it actually doesn't matter so much, but you can imagine there are some situations in which you may prefer one strategy overall.",
            "The important point here is at least the way that I've drawn it.",
            "The idea is that points that are in the in the same cluster tend to up to project to the same bit or get hash to the same place with high probability."
        ],
        [
            "So the way that we're going to proceed is actually to use a multiple of these splitting planes.",
            "And each of these splitting planes is going to contribute a bit to a hash code.",
            "So here for example, I have a situation where I have four of these splitting pain 012 and three plane zero corresponds to the least significant bit of the hashcode plane.",
            "Three corresponds to the most significant bit of a hash code.",
            "And.",
            "So what's essentially happening here is that these splitting points split plans are essentially fracturing feature space into a bunch of these convex convex cells.",
            "So here for example, we have a cell labeled 000.",
            "Basically, what we're saying is that any feature vector which ends up in this cell is going to get hashed.",
            "This value, similar this guys.",
            "All of the feature vectors over here get 1001, etc etc.",
            "Now.",
            "Another way of looking at this is to say that what we've done is taken space and fractured into these cells, which can be associated with the vertices of a hypercube.",
            "So the nodes here correspond to cells over here, and the edges indicate which cells are adjacent to each other.",
            "And again, it's a hypercube because clearly adjacent cells will differ by only a single bit.",
            "So.",
            "What we propose to do is do this very simple hashing, so every feature vector gets a hash code and then look at the populations of these hash bins which correspond to these to these nodes in the graph.",
            "So here again, the idea.",
            "The idea is that the color of the shading of the nodes correspond to sort of how many feature vectors got hash to that code, so the dark ones are heavier and the white ones are, wet ones are lighter.",
            "And then the idea is that what we want to do is go and look for population Maxima in this graph.",
            "And.",
            "We can do this quite simply by essentially looking at a graph looking at every node and looking at the population of its neighbors.",
            "So we can have this notion of a node being a one maximum, which means it has more.",
            "More entries than any of its neighbors within distance one or a two maximum having more neighbor having more values than any node in within two hops etc etc.",
            "So essentially what we're doing here is trying to look for modes in this hash space."
        ],
        [
            "So clearly the.",
            "Basic idea that I'm that I'm describing here is an example of locality sensitive hashing, which is an idea that's perhaps best exemplified by the work of independent one, and there they use this idea that with randomized hashing points are close together, will hash to the same codes and used it for doing fast approximate nearest neighbor neighbor search, whereas here what we're doing is turning around and using the idea to try to find clusters in our data."
        ],
        [
            "So that being said, the pseudocode for our segmentation algorithm is actually very very simple.",
            "We start off by taking the image and smoothing in associating a color vector with each of our pixels.",
            "Then we take all the feature vectors and hash them into N bit codes using, then randomly chosen splitting planes.",
            "We maintain a count of the number of feature vectors mapped to each of the code and then go ahead and try to identify local Maxima in the code space.",
            "Each feature vector get then gets mapped to the closest local Maxima, so that gives us.",
            "It gives us a code for every pixel and then we just finish up by running connected components to yield these coherent image reasons.",
            "So a couple of notes about what the magic number is in this in this."
        ],
        [
            "Procedure are so first there is the number of schooling plans that you choose to choose to use.",
            "So basically by increasing this.",
            "Number you get more possible codes tends to lead to more segments.",
            "Then there's this idea of having distance threshold.",
            "The value that you're using to look for local Maxima in this graph.",
            "Basically, if you make this bigger, you make it harder for a node to be a local Maxima.",
            "You you decrease the number of segments and finally the size of the smoothing window, which does the obvious thing.",
            "I."
        ],
        [
            "One thing I should note is that."
        ],
        [
            "This method doesn't assume that any knowledge about the number of segments, a number of clusters are you trying to buy things in this sort of it emerges from the analysis.",
            "R."
        ],
        [
            "The question is sort of based on a fairly happy view of the world.",
            "This idea that we have well separated clusters, and if that's not the case then things can go very badly.",
            "Alright, so as shown here we have a set of feature vectors which are inextricably intertwined.",
            "There's no easy way to find separating hyperplane, and you could expect it bad things would happen.",
            "OK."
        ],
        [
            "So before I go on and show you some of the quantitative evaluations."
        ],
        [
            "Segmentation just thought I'd show you.",
            "A couple of a couple of examples of good, bad and ugly.",
            "So this actually one isn't quite as bad as it looks, but the random choice.",
            "Basically we're just randomly calling segments, then it's chosen a lot of yellow on yellow.",
            "Those are sort of other examples.",
            "Obviously the first row is input image, second is human segmentation, third is machine segmentation, all taken from the Berkeley segmentation."
        ],
        [
            "Database.",
            "More of the same.",
            "Input human segmentations and machine."
        ],
        [
            "And more of the same.",
            "So let's say this one is probably the nastiest.",
            "It didn't get didn't get very far, far on that.",
            "There are of course 16133."
        ],
        [
            "The segmentations in that database so we need some way of actually evaluating them quantitatively.",
            "The commonly used metric is the global consistency error proposed by Martin folks, Tall and Malik.",
            "This of course measures the fraction of each segment that's labeled inconsistently, so the idea being that we want to number that sort of closer to 0.",
            "This is a very useful and widely reported statistic, which is why we were using it here.",
            "It takes into account the idea that one segmentation can be a refinement of the other, and that's sort of good and bad in that you can actually get very low numbers by either over segmenting the image or under segmenting the image.",
            "People sort of have the same effect, so in addition to this I chose to also report something called the Rand index, which is older.",
            "Method proposed appropriately enough, by a doctor, and that's a great name when I wish I had.",
            "And it's a little bit more of a blunt instrument.",
            "Basically it looks at pairs of pixels, and the idea is as follows.",
            "If they give you a ground truth segmentation and your machine shed segmentation and you consider any two pixels, they either have the same level labeling the ground truth, in which case they should have the same label in the machine, or they have different labels, in which case you should have different labels in the machine.",
            "Anything else is an error.",
            "So basically it considers the fraction of the pairs that you get right.",
            "And the idea being here that this fraction should be closer to 1.",
            "There have been subsequent works that sort of refine that a lot.",
            "I highly recommend this paper."
        ],
        [
            "OK, so just to summarize here, so running on the entire data set the histogram of GC values here is shown here.",
            "Again, close to 0 is better.",
            "We're getting a mean GC of .22 medium .2157."
        ],
        [
            "Looking at the Rand index, which again close to one, should be better.",
            "We're getting pretty much .75, and this is sort of the distribution of the entire data set.",
            "Comparing it to other reported GC values on the same."
        ],
        [
            "Data set and this was a table that I lifted from Vazquez Ecv 08 this is comparing it to a number of other segmentation schemes, including normalized cuts.",
            "Mean shift.",
            "The easy CV 08 rad paper is still 6 Music and basically what you're saying.",
            "This is randomized hashing down around bottom.",
            "We're doing around Point 2.1, one point 2, two.",
            "We're doing pretty much comparably with some of these other methods.",
            "They're doing it sort of 10 times."
        ],
        [
            "Fast.",
            "Another comparison, this time we did sort of a head to head against Mean Shift, so the top row here is the input image.",
            "The 2nd row is the mean shift result and the third row is a result of the randomized randomized hashing, and these are the image numbers one through 5.",
            "So visually what they look like.",
            "Ann is out for 2nd."
        ],
        [
            "And then comparing them using actual numbers and so basically three out of five we do better on terms of GC E 4 out of five."
        ],
        [
            "Do better in terms of terms of Rand index.",
            "So again."
        ],
        [
            "Sort of sort of compatible.",
            "So word here bout implementation.",
            "The nice thing about this game or one of the things about this game is that all of the operations that we're talking about in most of the operations that we're talking about are essentially done on a per pixel basis, which means that they are embarrassingly parallel, and there's very little of this comparing one pixel against another, which tends to stall pipelines and do bad things.",
            "So we haven't implemented had information that this that run essentially at 10 Hertz on one core of a my MacBook Pro.",
            "As you'll see in a second, and we anticipate that was just doing the very obvious things on a GPU.",
            "We could probably get a speedup of at least a factor of 10.",
            "Code now is kind of sloppy, but don't tell Anthony that I said that."
        ],
        [
            "So.",
            "I guess I should back up and explain what motivated this little thing.",
            "We wanted to sort of give us give a sense about what the segmentation did.",
            "And video and I sort of based on an offhand comment to Anthony, where I said that one of the problems that we knew how to solve well in computer vision was tracking a white ball against the black background.",
            "So if we wanted to generalize this a little bit and try to do arbitrary balls against arbitrary background, the idea here being simply that we took this, we went to YouTube, just downloaded arbitrary clip of stuff, and then we all we're doing here is running segmentation on each frame individually.",
            "And then we just report all the segments that seem to have the right shape and the right aspect ratio.",
            "So some of them are bothersome, but some of them are not.",
            "The idea being that to show that you actually started getting a coherent segmentation that through through these schemes and you're able to do this reasonably quickly.",
            "OK, so now we go to the always dangerous lives."
        ],
        [
            "See if I can do that.",
            "OK, so if this works OK, So what we're seeing here is segmentation, running, and basically what I'm doing here is some pseudo coloring the picture.",
            "So basically doing a real time posterization.",
            "Of course, this is not too interesting, so let me turn it around so it can look at you.",
            "It doesn't matter.",
            "So to the extent that things are colored the same, that means that they are in the same grouping or image, or thereabouts.",
            "So yeah, feel free to wave.",
            "OK, so as my time is rapidly drawing to a close let me try to."
        ],
        [
            "Just summarize here.",
            "So future work.",
            "So again, what the one of the underlying ideas here is that we should be able to handle vectors of arbitrary arbitrary dimension, so looking forward to doing stuff with teacher with texture features, higher dimensional vectors, applications tracking.",
            "Obviously this is sort of a very simple first cut.",
            "Planes here again are chosen completely randomly.",
            "Is that really the best thing to do, or is there a smarter ways to actually choose directions you might be doing projections on?"
        ],
        [
            "In summary, basically what we're trying to do is propose a new approach to segmentation based on random randomized hashing.",
            "Accuracy is compatible with some of the some of the current methods, but speed is.",
            "Typically on the order of 10 times faster.",
            "Again, I'm sort of a part time roboticist and one of the thoughts here was that we wanted to have segmentation scheme that was.",
            "Compatible to send things that we do with edge detection education.",
            "Very simple.",
            "1st Order thing that you might choose to do to get a reasonable summary of the data if we could have a fast segmentation scheme, we could use it for any number of things, perhaps even for apps.",
            "So I will conclude there and if you want to play with the demo I think will be available later in the session.",
            "Thank you very much.",
            "It doesn't work so good.",
            "So actually, for instance, one of the things we did was try it out on some of the standard machine learning databases where they do seem to be much more intertwined.",
            "So things like character recognition and bottom line it doesn't work.",
            "Doesn't work great, surprisingly, even for lossy separation situations you can get a pretty good result.",
            "There's actually some work by Sandra Descriptor that points out some of the benefits of randomization and reductions of eccentricity and so on.",
            "So even with when the sea separation isn't sort of galactic space, is it somewhere on the order of one or two?",
            "You can do quite reasonably, but intertwined bad news."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Morning my name is CJ Taylor.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be presenting some work I've been doing with my colleague Anthony Coley.",
                    "label": 0
                },
                {
                    "sent": "So let's go ahead and begin at the beginning.",
                    "label": 0
                },
                {
                    "sent": "So like most people, the way that we're going to.",
                    "label": 0
                },
                {
                    "sent": "View the segmentation problem is one of taking an input image and trying to divide it automatically into some set of coherent regions.",
                    "label": 0
                },
                {
                    "sent": "And ideally we'd like to do this both accurately and quickly while they can.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some debate about what we mean about accuracy with segmentation.",
                    "label": 0
                },
                {
                    "sent": "I think we understand what speed is, so we're going to try to propose a method that tries to achieve both aims.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Of course, there's a vast literature in this area, and I guess I must apologize and move match any summary I could imagine is necessarily incomplete, but with some amount of violence we could broadly categorize prior work into these sort of 3 three main groups.",
                    "label": 0
                },
                {
                    "sent": "The graph based methods is exemplified by normalized cuts, an agglomerative, and divisive techniques like Felzenszwalb and huttenlocher.",
                    "label": 1
                },
                {
                    "sent": "There.",
                    "label": 1
                },
                {
                    "sent": "Of course, these recent learning based methods, which make use of training data and can do more sophisticated things like label pixels and.",
                    "label": 0
                },
                {
                    "sent": "Prior talk was a very good example of that kind of thinking.",
                    "label": 1
                },
                {
                    "sent": "The approach we're going to place we're going to be describing is.",
                    "label": 0
                },
                {
                    "sent": "Most related to the stuff that's done feature based method.",
                    "label": 0
                },
                {
                    "sent": "In fact it is a feature based method an in that there the goal is to recast the segmentation problem as a clustering problem in some appropriate feature space.",
                    "label": 0
                },
                {
                    "sent": "So this would be methods like K means and of course methods like mean Shift.",
                    "label": 0
                },
                {
                    "sent": "In fact, our method is very similar to mean shift in that.",
                    "label": 0
                },
                {
                    "sent": "The underlying concept in mean shift is to look for Moser Maxima in distribution.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In the feature space and our method is going to be similar, but we're going to go out in a very different manner.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "I want to begin with what content should be a fairly simple clustering problem, so we're going to have a bunch of feature vectors distributed in an M dimensional feature space.",
                    "label": 1
                },
                {
                    "sent": "In order to accommodate the limitations of the projector, we're going to stick to M Equal 2, but you should understand everything I'm saying here is being applicable in general dimensions.",
                    "label": 0
                },
                {
                    "sent": "And so the idea here is that we have a bunch of feature space, but bunch of feature vectors they are divided into these clusters.",
                    "label": 0
                },
                {
                    "sent": "We don't know how many clusters there are.",
                    "label": 0
                },
                {
                    "sent": "We don't know how many.",
                    "label": 0
                },
                {
                    "sent": "Answer In each cluster, but we have this idea that we have this set of points that are.",
                    "label": 0
                },
                {
                    "sent": "They have this kind of this kind of distribution.",
                    "label": 0
                },
                {
                    "sent": "I like to refer to this kind of galactic model and that we have these centers of probability.",
                    "label": 0
                },
                {
                    "sent": "Separated by vast areas of open space.",
                    "label": 0
                },
                {
                    "sent": "Sandra Descript at UCSD, has a.",
                    "label": 0
                },
                {
                    "sent": "More precise, less poetic way of characterizing things like this, he has this notion of a C separation which essentially considers the ratio between the Inter and intra.",
                    "label": 1
                },
                {
                    "sent": "Last variations, so the larger the separation, the more widely spaced the clusters are, and so on.",
                    "label": 0
                },
                {
                    "sent": "OK, so if we had a situation like this.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We're interested in sort of splitting them into into into various groupings.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One way we could proceed is simple.",
                    "label": 0
                },
                {
                    "sent": "We can proceed as follows.",
                    "label": 0
                },
                {
                    "sent": "First, we will go ahead and select a direction vector at random.",
                    "label": 0
                },
                {
                    "sent": "Let's call that you.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then we're going to go ahead and take all our feature points and project them onto this direction.",
                    "label": 0
                },
                {
                    "sent": "Yielding a set of a set of values.",
                    "label": 0
                },
                {
                    "sent": "And finally we can select a. Splitting value which I have determined here.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And this splitting value takes the feature vectors and divide them into two classes 0101 depending on whether there are which side of the splitting splitting value thereon.",
                    "label": 0
                },
                {
                    "sent": "Now there are a number of ways.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "I could go about choosing the splitting value.",
                    "label": 1
                },
                {
                    "sent": "We could choose the mean of the projected values, or we could look at the Max and min and choose someplace in the middle or choose the median.",
                    "label": 0
                },
                {
                    "sent": "Etc.",
                    "label": 0
                },
                {
                    "sent": "For practical reasons, actually it actually doesn't matter so much, but you can imagine there are some situations in which you may prefer one strategy overall.",
                    "label": 0
                },
                {
                    "sent": "The important point here is at least the way that I've drawn it.",
                    "label": 0
                },
                {
                    "sent": "The idea is that points that are in the in the same cluster tend to up to project to the same bit or get hash to the same place with high probability.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So the way that we're going to proceed is actually to use a multiple of these splitting planes.",
                    "label": 0
                },
                {
                    "sent": "And each of these splitting planes is going to contribute a bit to a hash code.",
                    "label": 1
                },
                {
                    "sent": "So here for example, I have a situation where I have four of these splitting pain 012 and three plane zero corresponds to the least significant bit of the hashcode plane.",
                    "label": 1
                },
                {
                    "sent": "Three corresponds to the most significant bit of a hash code.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 1
                },
                {
                    "sent": "So what's essentially happening here is that these splitting points split plans are essentially fracturing feature space into a bunch of these convex convex cells.",
                    "label": 0
                },
                {
                    "sent": "So here for example, we have a cell labeled 000.",
                    "label": 0
                },
                {
                    "sent": "Basically, what we're saying is that any feature vector which ends up in this cell is going to get hashed.",
                    "label": 0
                },
                {
                    "sent": "This value, similar this guys.",
                    "label": 0
                },
                {
                    "sent": "All of the feature vectors over here get 1001, etc etc.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Another way of looking at this is to say that what we've done is taken space and fractured into these cells, which can be associated with the vertices of a hypercube.",
                    "label": 1
                },
                {
                    "sent": "So the nodes here correspond to cells over here, and the edges indicate which cells are adjacent to each other.",
                    "label": 0
                },
                {
                    "sent": "And again, it's a hypercube because clearly adjacent cells will differ by only a single bit.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "What we propose to do is do this very simple hashing, so every feature vector gets a hash code and then look at the populations of these hash bins which correspond to these to these nodes in the graph.",
                    "label": 0
                },
                {
                    "sent": "So here again, the idea.",
                    "label": 0
                },
                {
                    "sent": "The idea is that the color of the shading of the nodes correspond to sort of how many feature vectors got hash to that code, so the dark ones are heavier and the white ones are, wet ones are lighter.",
                    "label": 1
                },
                {
                    "sent": "And then the idea is that what we want to do is go and look for population Maxima in this graph.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                },
                {
                    "sent": "We can do this quite simply by essentially looking at a graph looking at every node and looking at the population of its neighbors.",
                    "label": 0
                },
                {
                    "sent": "So we can have this notion of a node being a one maximum, which means it has more.",
                    "label": 0
                },
                {
                    "sent": "More entries than any of its neighbors within distance one or a two maximum having more neighbor having more values than any node in within two hops etc etc.",
                    "label": 0
                },
                {
                    "sent": "So essentially what we're doing here is trying to look for modes in this hash space.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So clearly the.",
                    "label": 0
                },
                {
                    "sent": "Basic idea that I'm that I'm describing here is an example of locality sensitive hashing, which is an idea that's perhaps best exemplified by the work of independent one, and there they use this idea that with randomized hashing points are close together, will hash to the same codes and used it for doing fast approximate nearest neighbor neighbor search, whereas here what we're doing is turning around and using the idea to try to find clusters in our data.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So that being said, the pseudocode for our segmentation algorithm is actually very very simple.",
                    "label": 0
                },
                {
                    "sent": "We start off by taking the image and smoothing in associating a color vector with each of our pixels.",
                    "label": 1
                },
                {
                    "sent": "Then we take all the feature vectors and hash them into N bit codes using, then randomly chosen splitting planes.",
                    "label": 0
                },
                {
                    "sent": "We maintain a count of the number of feature vectors mapped to each of the code and then go ahead and try to identify local Maxima in the code space.",
                    "label": 1
                },
                {
                    "sent": "Each feature vector get then gets mapped to the closest local Maxima, so that gives us.",
                    "label": 0
                },
                {
                    "sent": "It gives us a code for every pixel and then we just finish up by running connected components to yield these coherent image reasons.",
                    "label": 0
                },
                {
                    "sent": "So a couple of notes about what the magic number is in this in this.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Procedure are so first there is the number of schooling plans that you choose to choose to use.",
                    "label": 0
                },
                {
                    "sent": "So basically by increasing this.",
                    "label": 1
                },
                {
                    "sent": "Number you get more possible codes tends to lead to more segments.",
                    "label": 1
                },
                {
                    "sent": "Then there's this idea of having distance threshold.",
                    "label": 0
                },
                {
                    "sent": "The value that you're using to look for local Maxima in this graph.",
                    "label": 0
                },
                {
                    "sent": "Basically, if you make this bigger, you make it harder for a node to be a local Maxima.",
                    "label": 0
                },
                {
                    "sent": "You you decrease the number of segments and finally the size of the smoothing window, which does the obvious thing.",
                    "label": 1
                },
                {
                    "sent": "I.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "One thing I should note is that.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This method doesn't assume that any knowledge about the number of segments, a number of clusters are you trying to buy things in this sort of it emerges from the analysis.",
                    "label": 0
                },
                {
                    "sent": "R.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The question is sort of based on a fairly happy view of the world.",
                    "label": 0
                },
                {
                    "sent": "This idea that we have well separated clusters, and if that's not the case then things can go very badly.",
                    "label": 0
                },
                {
                    "sent": "Alright, so as shown here we have a set of feature vectors which are inextricably intertwined.",
                    "label": 1
                },
                {
                    "sent": "There's no easy way to find separating hyperplane, and you could expect it bad things would happen.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So before I go on and show you some of the quantitative evaluations.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Segmentation just thought I'd show you.",
                    "label": 0
                },
                {
                    "sent": "A couple of a couple of examples of good, bad and ugly.",
                    "label": 0
                },
                {
                    "sent": "So this actually one isn't quite as bad as it looks, but the random choice.",
                    "label": 0
                },
                {
                    "sent": "Basically we're just randomly calling segments, then it's chosen a lot of yellow on yellow.",
                    "label": 0
                },
                {
                    "sent": "Those are sort of other examples.",
                    "label": 0
                },
                {
                    "sent": "Obviously the first row is input image, second is human segmentation, third is machine segmentation, all taken from the Berkeley segmentation.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Database.",
                    "label": 0
                },
                {
                    "sent": "More of the same.",
                    "label": 0
                },
                {
                    "sent": "Input human segmentations and machine.",
                    "label": 1
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And more of the same.",
                    "label": 0
                },
                {
                    "sent": "So let's say this one is probably the nastiest.",
                    "label": 0
                },
                {
                    "sent": "It didn't get didn't get very far, far on that.",
                    "label": 0
                },
                {
                    "sent": "There are of course 16133.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The segmentations in that database so we need some way of actually evaluating them quantitatively.",
                    "label": 0
                },
                {
                    "sent": "The commonly used metric is the global consistency error proposed by Martin folks, Tall and Malik.",
                    "label": 1
                },
                {
                    "sent": "This of course measures the fraction of each segment that's labeled inconsistently, so the idea being that we want to number that sort of closer to 0.",
                    "label": 1
                },
                {
                    "sent": "This is a very useful and widely reported statistic, which is why we were using it here.",
                    "label": 0
                },
                {
                    "sent": "It takes into account the idea that one segmentation can be a refinement of the other, and that's sort of good and bad in that you can actually get very low numbers by either over segmenting the image or under segmenting the image.",
                    "label": 0
                },
                {
                    "sent": "People sort of have the same effect, so in addition to this I chose to also report something called the Rand index, which is older.",
                    "label": 0
                },
                {
                    "sent": "Method proposed appropriately enough, by a doctor, and that's a great name when I wish I had.",
                    "label": 0
                },
                {
                    "sent": "And it's a little bit more of a blunt instrument.",
                    "label": 0
                },
                {
                    "sent": "Basically it looks at pairs of pixels, and the idea is as follows.",
                    "label": 0
                },
                {
                    "sent": "If they give you a ground truth segmentation and your machine shed segmentation and you consider any two pixels, they either have the same level labeling the ground truth, in which case they should have the same label in the machine, or they have different labels, in which case you should have different labels in the machine.",
                    "label": 0
                },
                {
                    "sent": "Anything else is an error.",
                    "label": 1
                },
                {
                    "sent": "So basically it considers the fraction of the pairs that you get right.",
                    "label": 0
                },
                {
                    "sent": "And the idea being here that this fraction should be closer to 1.",
                    "label": 0
                },
                {
                    "sent": "There have been subsequent works that sort of refine that a lot.",
                    "label": 0
                },
                {
                    "sent": "I highly recommend this paper.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, so just to summarize here, so running on the entire data set the histogram of GC values here is shown here.",
                    "label": 0
                },
                {
                    "sent": "Again, close to 0 is better.",
                    "label": 0
                },
                {
                    "sent": "We're getting a mean GC of .22 medium .2157.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Looking at the Rand index, which again close to one, should be better.",
                    "label": 0
                },
                {
                    "sent": "We're getting pretty much .75, and this is sort of the distribution of the entire data set.",
                    "label": 0
                },
                {
                    "sent": "Comparing it to other reported GC values on the same.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Data set and this was a table that I lifted from Vazquez Ecv 08 this is comparing it to a number of other segmentation schemes, including normalized cuts.",
                    "label": 1
                },
                {
                    "sent": "Mean shift.",
                    "label": 0
                },
                {
                    "sent": "The easy CV 08 rad paper is still 6 Music and basically what you're saying.",
                    "label": 0
                },
                {
                    "sent": "This is randomized hashing down around bottom.",
                    "label": 0
                },
                {
                    "sent": "We're doing around Point 2.1, one point 2, two.",
                    "label": 0
                },
                {
                    "sent": "We're doing pretty much comparably with some of these other methods.",
                    "label": 0
                },
                {
                    "sent": "They're doing it sort of 10 times.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Fast.",
                    "label": 0
                },
                {
                    "sent": "Another comparison, this time we did sort of a head to head against Mean Shift, so the top row here is the input image.",
                    "label": 0
                },
                {
                    "sent": "The 2nd row is the mean shift result and the third row is a result of the randomized randomized hashing, and these are the image numbers one through 5.",
                    "label": 0
                },
                {
                    "sent": "So visually what they look like.",
                    "label": 0
                },
                {
                    "sent": "Ann is out for 2nd.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And then comparing them using actual numbers and so basically three out of five we do better on terms of GC E 4 out of five.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Do better in terms of terms of Rand index.",
                    "label": 0
                },
                {
                    "sent": "So again.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Sort of sort of compatible.",
                    "label": 0
                },
                {
                    "sent": "So word here bout implementation.",
                    "label": 0
                },
                {
                    "sent": "The nice thing about this game or one of the things about this game is that all of the operations that we're talking about in most of the operations that we're talking about are essentially done on a per pixel basis, which means that they are embarrassingly parallel, and there's very little of this comparing one pixel against another, which tends to stall pipelines and do bad things.",
                    "label": 1
                },
                {
                    "sent": "So we haven't implemented had information that this that run essentially at 10 Hertz on one core of a my MacBook Pro.",
                    "label": 1
                },
                {
                    "sent": "As you'll see in a second, and we anticipate that was just doing the very obvious things on a GPU.",
                    "label": 0
                },
                {
                    "sent": "We could probably get a speedup of at least a factor of 10.",
                    "label": 0
                },
                {
                    "sent": "Code now is kind of sloppy, but don't tell Anthony that I said that.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I guess I should back up and explain what motivated this little thing.",
                    "label": 0
                },
                {
                    "sent": "We wanted to sort of give us give a sense about what the segmentation did.",
                    "label": 0
                },
                {
                    "sent": "And video and I sort of based on an offhand comment to Anthony, where I said that one of the problems that we knew how to solve well in computer vision was tracking a white ball against the black background.",
                    "label": 0
                },
                {
                    "sent": "So if we wanted to generalize this a little bit and try to do arbitrary balls against arbitrary background, the idea here being simply that we took this, we went to YouTube, just downloaded arbitrary clip of stuff, and then we all we're doing here is running segmentation on each frame individually.",
                    "label": 0
                },
                {
                    "sent": "And then we just report all the segments that seem to have the right shape and the right aspect ratio.",
                    "label": 1
                },
                {
                    "sent": "So some of them are bothersome, but some of them are not.",
                    "label": 0
                },
                {
                    "sent": "The idea being that to show that you actually started getting a coherent segmentation that through through these schemes and you're able to do this reasonably quickly.",
                    "label": 0
                },
                {
                    "sent": "OK, so now we go to the always dangerous lives.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "See if I can do that.",
                    "label": 0
                },
                {
                    "sent": "OK, so if this works OK, So what we're seeing here is segmentation, running, and basically what I'm doing here is some pseudo coloring the picture.",
                    "label": 0
                },
                {
                    "sent": "So basically doing a real time posterization.",
                    "label": 0
                },
                {
                    "sent": "Of course, this is not too interesting, so let me turn it around so it can look at you.",
                    "label": 0
                },
                {
                    "sent": "It doesn't matter.",
                    "label": 0
                },
                {
                    "sent": "So to the extent that things are colored the same, that means that they are in the same grouping or image, or thereabouts.",
                    "label": 0
                },
                {
                    "sent": "So yeah, feel free to wave.",
                    "label": 0
                },
                {
                    "sent": "OK, so as my time is rapidly drawing to a close let me try to.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just summarize here.",
                    "label": 0
                },
                {
                    "sent": "So future work.",
                    "label": 0
                },
                {
                    "sent": "So again, what the one of the underlying ideas here is that we should be able to handle vectors of arbitrary arbitrary dimension, so looking forward to doing stuff with teacher with texture features, higher dimensional vectors, applications tracking.",
                    "label": 1
                },
                {
                    "sent": "Obviously this is sort of a very simple first cut.",
                    "label": 0
                },
                {
                    "sent": "Planes here again are chosen completely randomly.",
                    "label": 0
                },
                {
                    "sent": "Is that really the best thing to do, or is there a smarter ways to actually choose directions you might be doing projections on?",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In summary, basically what we're trying to do is propose a new approach to segmentation based on random randomized hashing.",
                    "label": 1
                },
                {
                    "sent": "Accuracy is compatible with some of the some of the current methods, but speed is.",
                    "label": 0
                },
                {
                    "sent": "Typically on the order of 10 times faster.",
                    "label": 0
                },
                {
                    "sent": "Again, I'm sort of a part time roboticist and one of the thoughts here was that we wanted to have segmentation scheme that was.",
                    "label": 0
                },
                {
                    "sent": "Compatible to send things that we do with edge detection education.",
                    "label": 0
                },
                {
                    "sent": "Very simple.",
                    "label": 0
                },
                {
                    "sent": "1st Order thing that you might choose to do to get a reasonable summary of the data if we could have a fast segmentation scheme, we could use it for any number of things, perhaps even for apps.",
                    "label": 0
                },
                {
                    "sent": "So I will conclude there and if you want to play with the demo I think will be available later in the session.",
                    "label": 0
                },
                {
                    "sent": "Thank you very much.",
                    "label": 0
                },
                {
                    "sent": "It doesn't work so good.",
                    "label": 0
                },
                {
                    "sent": "So actually, for instance, one of the things we did was try it out on some of the standard machine learning databases where they do seem to be much more intertwined.",
                    "label": 0
                },
                {
                    "sent": "So things like character recognition and bottom line it doesn't work.",
                    "label": 0
                },
                {
                    "sent": "Doesn't work great, surprisingly, even for lossy separation situations you can get a pretty good result.",
                    "label": 0
                },
                {
                    "sent": "There's actually some work by Sandra Descriptor that points out some of the benefits of randomization and reductions of eccentricity and so on.",
                    "label": 0
                },
                {
                    "sent": "So even with when the sea separation isn't sort of galactic space, is it somewhere on the order of one or two?",
                    "label": 0
                },
                {
                    "sent": "You can do quite reasonably, but intertwined bad news.",
                    "label": 0
                }
            ]
        }
    }
}