{
    "id": "zr6olr2bsvuo2uobeqe3zw2v4cg3cdut",
    "title": "Enriching Product Ads with Metadata from HTML Annotations",
    "info": {
        "author": [
            "Petar Ristoski, School of Business Informatics and Mathematics, University of Mannheim"
        ],
        "published": "July 28, 2016",
        "recorded": "May 2016",
        "category": [
            "Top->Computer Science->Big Data",
            "Top->Computer Science->Semantic Web"
        ]
    },
    "url": "http://videolectures.net/eswc2016_ristoski_enriching_product/",
    "segmentation": [
        [
            "OK, so hello everybody.",
            "In this talk I will be presenting approach for enriching product ads with meta data from HTML annotations.",
            "This is a joint work with Peter, Micah from Yahoo Labs in law."
        ],
        [
            "Them.",
            "So I will start with a short overview of products, so products are.",
            "Very popular formal for search advertising code which is increased increasingly used as replacement for standard text based search ads and are offered by Google, Yahoo and Bing as an option for advertising and their given by under different trend names.",
            "So in the case of Yahoo they're called Gemini product ads.",
            "So basically when a user posts a query to the search engine, the ad retrieval algorithm tries to.",
            "I'll find the most relevant products in the product database and display these products right next to the search results and then with each click or.",
            "No, purchased by the users.",
            "The company makes the revenue.",
            "The Gemini."
        ],
        [
            "Or like that so consist of several fields.",
            "So now first it contains some metadata and then it contains unstructured test.",
            "The textual description for the title and the description of the product and compared to the standard text based search ads.",
            "It also contains a list of attribute value pairs.",
            "All these details are provided in the data stream which is transferred from the merchants to the search engines, providers and.",
            "This allows the ad retrieval algorithm to do advanced keyword based ad retrieval, but the completeness of of this product specification completely depends on the completeness of the specification within the merchant database.",
            "The willingness of the merchant to provide additional information beyond the beyond the minimal minimally required properties and also the technical sophistication.",
            "To actually set the data stream to the search search engines also.",
            "Therefore very often a lot of product ads actually are very incomplete and this highly influences the performances of the ad retrieval algorithm.",
            "Therefore, in many cases dollar item is not able to.",
            "To provide the most relevant ads for the user, query to."
        ],
        [
            "For this problem, in this approach we make use of structured data publishing HTML pages as a with semantic annotations.",
            "So first we try to extend the list of attribute value pairs.",
            "But also we try to include additional indicators like offers, reviews and ratings which can be used in the ad retrieval algorithm as additional information.",
            "And to increase the performances of the algorithm."
        ],
        [
            "Currently there are several semantic annotation formats, so some of the most popular are microformats, RDF, Ayane, microdata, and."
        ],
        [
            "Most commonly used vocabularies schema work, which is supported by some of the top search engines like Google, Bing and Yahoo.",
            "Oh, and it contains a vocabulary for 600 types like event places, products and so on."
        ],
        [
            "Find this work.",
            "We focus on microdata unrotated with the schema Oracle vocabulary because it has been shown that it has the highest domain and entity coverage.",
            "So we use the extraction from the web data comments from the 2014 common crawl corpus and it shows that 90% ninety thousand of appeal.",
            "This annotated list one entity's schema product and there are in total 300 million product entities within this data set.",
            "The most used properties are now name, image, description, offer and so on.",
            "And in our approach where we make use of the name and description for extracting additional attribute value pairs and we use the offer aggregate rating and review as additional signals for the at retrieval algorithm."
        ],
        [
            "So next I will show you an overview of our approach."
        ],
        [
            "So this is basically the architecture we use and the main idea is to use the Gemini product ads as supervision for building feature extraction models which are able to extract attribute value pairs from unstructured text afterwards.",
            "These central value pairs are used to build a classifier which then is able to identify matching products within different datasets.",
            "So during the training phase we used.",
            "GPA deal set to build this single feature extraction model, then we manually select the training data set from the micro data products and.",
            "We apply the previously built feature extraction model and calculate the similarity feature vectors for all the candidate matching products.",
            "Then we build the classifier and during the application phase we start with pre processing both the micro data product data set and the GP data set.",
            "We use the brand of the products blocking approach and we generate all the possible candidate pairs in the datasets and then we apply the previously build classifier to identify matching products between the.",
            "Gemini product that's an microlitre products.",
            "So."
        ],
        [
            "We'll go into details in each of these blocks, so we have developed 2 feature extraction approaches.",
            "The first one."
        ],
        [
            "Dictionary based approach which is quite simple and it simply builds an inverted index of all values in the Gemini product.",
            "That's data set and then given a value, it simply returns the attribute which is associated with this value to extract the attribute value pairs from the unstructured text, we simply generate all the engrams and then we try to find the best matches for the values."
        ],
        [
            "The second approach is based on conditional random field, so so our conditional random field is a conditional sequence model which represents the probability of hidden state sequence given some observations.",
            "So in our case it tries to calculate the probability for a given token in the description to be identified as some of the attributes in our attribute space given the characteristics of the given token and the tokens in its surrounding.",
            "We use."
        ],
        [
            "The implementation by thinking little, which uses 10 different discrete features for capturing the characteristics of the given token and the surrounding tokens.",
            "No one."
        ],
        [
            "So we have identified all the attributes we will.",
            "We proceed with the attributes normalization which first tries to identify the attribute data types.",
            "So we have four of them.",
            "String long string, numeric and unit of measurement to identify all these data types we use a set of regular expression and rules and then we use majority vote on all identified data types of all values to decide the final data type of the attribute.",
            "Then we perform some standard string normalization and we also perform unit normalization.",
            "Where we convert all the values to the base unit.",
            "So with this conversion we avoid the problem of using different units of measurement across different shops.",
            "So for example, some of them are using interest, some of them are using centimeters, and in our case they both will be converted into meters."
        ],
        [
            "So this is an example of extracting attribute value pairs from text.",
            "So for example, here we can see that Samsung has been identified as the brand Galaxy S4 is the phone type, then 16 gigabyte has been identified as the memory and five inches as size, which is also identified to be a unit, and it's then converted to meters.",
            "So when we have extracted all the attributes, so we define the attribute space an each of the products in our database is represented as a feature vector and we try to calculate the similarity feature vector for each pair of products in the in both datasets.",
            "So to do so we calculate the similarity for each attribute within the feature vectors and the similarity is calculated based on the data type of attribute.",
            "So in case of string attributes we use Jakarta similarity on.",
            "Doctor Ingrams in case of Long string we use cosine similarity and in case of numerics or unit attributes we use exact matching.",
            "And of course if the attribute is missing then how the similarities 0?"
        ],
        [
            "So when we have calculated the similarity feature vectors, we feed these vectors into the into a classic into a classification model.",
            "So we have experiment."
        ],
        [
            "With random forest Night bias, support vector machines and logistic regression, and to address the high imbalance in the data set, we use standard approaches for sampling.",
            "Those are random under sampling and random oversampling."
        ],
        [
            "So next we will proceed with the evaluation of our approach.",
            "Oh"
        ],
        [
            "Also, we use a sample of three categories from the Gemini products.",
            "Those are televisions, mobile phones and laptops from each of these categories.",
            "We randomly selected a sample of 3500 product sets.",
            "And for the same categories we manually build the gold standard on the micro later products.",
            "So for each category selected around 300 products and we manually identify the matching pairs which also we have around 300 and we have a lot more non matching pairs.",
            "So as you can see the datasets are highly imbalanced.",
            "Oh"
        ],
        [
            "For the evaluation of, we use the standard metrics for performance evaluation like precision, recall, and F score, and the results are calculated using 10 fold cross validation.",
            "So as I said, we use for classifiers and three sampling methods.",
            "We compare our approach to two baseline methods."
        ],
        [
            "1st is the standard TF IDF cosine similarity and the second one is the Silk Link Discovery framework, which is able to discover links between data items and with different data sources.",
            "So the framework is based on genetic algorithm which is able to learn linkage rules which which consists of.",
            "Blocks for value normalization, transformation and similarity.",
            "We trained it on conditional on our conditional random fields features and the results are again calculated using stratified 10 fold cross validation."
        ],
        [
            "So here we are.",
            "Given the results for the televisions cut category.",
            "So we show the results for each of the models for each of the sampling methods and for both feature extraction methods.",
            "So for the conditional random field and dictionary as we can see, the results are quite well for the random forest, SVM and logistic regression, while the naive bias performs quite poor.",
            "And in all the approaches, the CRF features outperformed the dictionary based approaches.",
            "And here the best query is achieved when using random forest with no sampling curse, scoring F score of 82%.",
            "Well, compared to the baseline approaches, we significantly outperform.",
            "Both of them were the cosine similarity scores.",
            "Quite bad, and the silky performs well, but still significantly worse than our approach."
        ],
        [
            "Of for the mobile phones we have observed same patterns in the results were again the best results are achieved when using random forests with random oversampling catching score of 81.5%.",
            "Again, are significantly outperforming both of the baseline models.",
            "Anne Anne."
        ],
        [
            "So for the laptops category it's not worth it to see that the results are significantly worse compared to the other two categories.",
            "So previously we had F score of above 80% and now we have around 63.",
            "And the reason for this is that when it comes to matching laptops we have to use many more attributes like the type of the processor, the cash, the memory card, discounts on where for televisions, and mobile phones we had only four or five main attributes which were considered for.",
            "Oh, matching the the products.",
            "And also in this case we outperform both of the baseline methods."
        ],
        [
            "In the last section we performed actually the enrichment of the product ads, so we used previously built models, so we use the best model for each category an apply for matching of product ads with products from the micro micro data data set."
        ],
        [
            "Also, we evaluated the 1st on the television category, so we selected 10 popular TV brands like Samsung, Vizio, LG.",
            "We took the number of products that we have in the Gemini product that we retrieve the possible candidates for each brand from the micro data deal set an we applied the model to identify the metrics and then manually calculated precision.",
            "So in total we identified 1300.",
            "I mean I product that's in our data set an identified 55,000 micro data products which are candidate matching products and we found 600 matches with precision of 92%.",
            "So you can see that the results are the number of discovered matrix is quite low, but the basic reason for this is that the GPA have products that we used are actually appeared in 2015, while the WDC Dale said that we used.",
            "It was extracted in 2014, so we have only small overlap in the products in our datasets."
        ],
        [
            "In the end, we counted the number of metrics and newly discovered attribute value pairs for each of the products and as we can see, for most of the products we are able to find at least one or two matches and we can identify at least three or four new.",
            "You attribute value pairs, but also for some of them we can also identify like 6 new attribute value pairs and also we counted the number of offers, ratings and reviews that we can find in the micro datasets for the products in the Gemini products.",
            "And we can see that for most of them we can find one or two offers, ratings and reviews, but also for some of them we can find up to 11 offers and up to 18 reviews and all this information is integrated within the ad retrieval system.",
            "And the increases the performance is for retreiving the most relevant product that's for user defined search query.",
            "So."
        ],
        [
            "So currently we are extending our work in several directions.",
            "So first we try to enhance the conditional random field model with word embeddings.",
            "So we make use of.",
            "Uh, who hold datasets of microdata products to build word embeddings models, where which is actually able to.",
            "Hello to represent each token in the deal set in a latent feature space where similar values appear close to each other and as you can see, values of same attributes actually appear quite close to each other and this knowledge can be directly integrated with the conditional random field, giving it better knowledge about the values of the attributes and therefore increasing the extraction performances.",
            "The second direction is actually using products images as features, so we can see that for us it is easy to say that maybe the first 2 pictures are actually representation for the same product.",
            "But it's not the case with the third picture, so we cannot directly infer the specification of the product.",
            "But we have a good indication if two images are for the same product or not.",
            "So to use these features in our approach we use state of the art deep learning methods.",
            "So we use convolutional neural Nets, which allows us to extract feature vectors for the images which can be used in our algorithm for calculating similarity between the products.",
            "And the third direction is actually using cover our approach together with text embeddings.",
            "An image embeddings for product categorisation, which is a very important task for structuring the product ads in a product catalog and also to improve the user shopping experience for navigation and finding the products they need."
        ],
        [
            "Oh, that will be all.",
            "Thank you for your attention."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so hello everybody.",
                    "label": 0
                },
                {
                    "sent": "In this talk I will be presenting approach for enriching product ads with meta data from HTML annotations.",
                    "label": 1
                },
                {
                    "sent": "This is a joint work with Peter, Micah from Yahoo Labs in law.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Them.",
                    "label": 0
                },
                {
                    "sent": "So I will start with a short overview of products, so products are.",
                    "label": 0
                },
                {
                    "sent": "Very popular formal for search advertising code which is increased increasingly used as replacement for standard text based search ads and are offered by Google, Yahoo and Bing as an option for advertising and their given by under different trend names.",
                    "label": 0
                },
                {
                    "sent": "So in the case of Yahoo they're called Gemini product ads.",
                    "label": 1
                },
                {
                    "sent": "So basically when a user posts a query to the search engine, the ad retrieval algorithm tries to.",
                    "label": 0
                },
                {
                    "sent": "I'll find the most relevant products in the product database and display these products right next to the search results and then with each click or.",
                    "label": 0
                },
                {
                    "sent": "No, purchased by the users.",
                    "label": 0
                },
                {
                    "sent": "The company makes the revenue.",
                    "label": 0
                },
                {
                    "sent": "The Gemini.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Or like that so consist of several fields.",
                    "label": 0
                },
                {
                    "sent": "So now first it contains some metadata and then it contains unstructured test.",
                    "label": 0
                },
                {
                    "sent": "The textual description for the title and the description of the product and compared to the standard text based search ads.",
                    "label": 0
                },
                {
                    "sent": "It also contains a list of attribute value pairs.",
                    "label": 1
                },
                {
                    "sent": "All these details are provided in the data stream which is transferred from the merchants to the search engines, providers and.",
                    "label": 0
                },
                {
                    "sent": "This allows the ad retrieval algorithm to do advanced keyword based ad retrieval, but the completeness of of this product specification completely depends on the completeness of the specification within the merchant database.",
                    "label": 0
                },
                {
                    "sent": "The willingness of the merchant to provide additional information beyond the beyond the minimal minimally required properties and also the technical sophistication.",
                    "label": 1
                },
                {
                    "sent": "To actually set the data stream to the search search engines also.",
                    "label": 0
                },
                {
                    "sent": "Therefore very often a lot of product ads actually are very incomplete and this highly influences the performances of the ad retrieval algorithm.",
                    "label": 0
                },
                {
                    "sent": "Therefore, in many cases dollar item is not able to.",
                    "label": 0
                },
                {
                    "sent": "To provide the most relevant ads for the user, query to.",
                    "label": 1
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For this problem, in this approach we make use of structured data publishing HTML pages as a with semantic annotations.",
                    "label": 0
                },
                {
                    "sent": "So first we try to extend the list of attribute value pairs.",
                    "label": 0
                },
                {
                    "sent": "But also we try to include additional indicators like offers, reviews and ratings which can be used in the ad retrieval algorithm as additional information.",
                    "label": 0
                },
                {
                    "sent": "And to increase the performances of the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Currently there are several semantic annotation formats, so some of the most popular are microformats, RDF, Ayane, microdata, and.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Most commonly used vocabularies schema work, which is supported by some of the top search engines like Google, Bing and Yahoo.",
                    "label": 0
                },
                {
                    "sent": "Oh, and it contains a vocabulary for 600 types like event places, products and so on.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Find this work.",
                    "label": 0
                },
                {
                    "sent": "We focus on microdata unrotated with the schema Oracle vocabulary because it has been shown that it has the highest domain and entity coverage.",
                    "label": 0
                },
                {
                    "sent": "So we use the extraction from the web data comments from the 2014 common crawl corpus and it shows that 90% ninety thousand of appeal.",
                    "label": 1
                },
                {
                    "sent": "This annotated list one entity's schema product and there are in total 300 million product entities within this data set.",
                    "label": 0
                },
                {
                    "sent": "The most used properties are now name, image, description, offer and so on.",
                    "label": 0
                },
                {
                    "sent": "And in our approach where we make use of the name and description for extracting additional attribute value pairs and we use the offer aggregate rating and review as additional signals for the at retrieval algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So next I will show you an overview of our approach.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is basically the architecture we use and the main idea is to use the Gemini product ads as supervision for building feature extraction models which are able to extract attribute value pairs from unstructured text afterwards.",
                    "label": 0
                },
                {
                    "sent": "These central value pairs are used to build a classifier which then is able to identify matching products within different datasets.",
                    "label": 0
                },
                {
                    "sent": "So during the training phase we used.",
                    "label": 0
                },
                {
                    "sent": "GPA deal set to build this single feature extraction model, then we manually select the training data set from the micro data products and.",
                    "label": 0
                },
                {
                    "sent": "We apply the previously built feature extraction model and calculate the similarity feature vectors for all the candidate matching products.",
                    "label": 0
                },
                {
                    "sent": "Then we build the classifier and during the application phase we start with pre processing both the micro data product data set and the GP data set.",
                    "label": 0
                },
                {
                    "sent": "We use the brand of the products blocking approach and we generate all the possible candidate pairs in the datasets and then we apply the previously build classifier to identify matching products between the.",
                    "label": 0
                },
                {
                    "sent": "Gemini product that's an microlitre products.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We'll go into details in each of these blocks, so we have developed 2 feature extraction approaches.",
                    "label": 0
                },
                {
                    "sent": "The first one.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Dictionary based approach which is quite simple and it simply builds an inverted index of all values in the Gemini product.",
                    "label": 0
                },
                {
                    "sent": "That's data set and then given a value, it simply returns the attribute which is associated with this value to extract the attribute value pairs from the unstructured text, we simply generate all the engrams and then we try to find the best matches for the values.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The second approach is based on conditional random field, so so our conditional random field is a conditional sequence model which represents the probability of hidden state sequence given some observations.",
                    "label": 0
                },
                {
                    "sent": "So in our case it tries to calculate the probability for a given token in the description to be identified as some of the attributes in our attribute space given the characteristics of the given token and the tokens in its surrounding.",
                    "label": 0
                },
                {
                    "sent": "We use.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The implementation by thinking little, which uses 10 different discrete features for capturing the characteristics of the given token and the surrounding tokens.",
                    "label": 0
                },
                {
                    "sent": "No one.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So we have identified all the attributes we will.",
                    "label": 0
                },
                {
                    "sent": "We proceed with the attributes normalization which first tries to identify the attribute data types.",
                    "label": 0
                },
                {
                    "sent": "So we have four of them.",
                    "label": 0
                },
                {
                    "sent": "String long string, numeric and unit of measurement to identify all these data types we use a set of regular expression and rules and then we use majority vote on all identified data types of all values to decide the final data type of the attribute.",
                    "label": 0
                },
                {
                    "sent": "Then we perform some standard string normalization and we also perform unit normalization.",
                    "label": 0
                },
                {
                    "sent": "Where we convert all the values to the base unit.",
                    "label": 0
                },
                {
                    "sent": "So with this conversion we avoid the problem of using different units of measurement across different shops.",
                    "label": 0
                },
                {
                    "sent": "So for example, some of them are using interest, some of them are using centimeters, and in our case they both will be converted into meters.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is an example of extracting attribute value pairs from text.",
                    "label": 0
                },
                {
                    "sent": "So for example, here we can see that Samsung has been identified as the brand Galaxy S4 is the phone type, then 16 gigabyte has been identified as the memory and five inches as size, which is also identified to be a unit, and it's then converted to meters.",
                    "label": 0
                },
                {
                    "sent": "So when we have extracted all the attributes, so we define the attribute space an each of the products in our database is represented as a feature vector and we try to calculate the similarity feature vector for each pair of products in the in both datasets.",
                    "label": 0
                },
                {
                    "sent": "So to do so we calculate the similarity for each attribute within the feature vectors and the similarity is calculated based on the data type of attribute.",
                    "label": 0
                },
                {
                    "sent": "So in case of string attributes we use Jakarta similarity on.",
                    "label": 0
                },
                {
                    "sent": "Doctor Ingrams in case of Long string we use cosine similarity and in case of numerics or unit attributes we use exact matching.",
                    "label": 0
                },
                {
                    "sent": "And of course if the attribute is missing then how the similarities 0?",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So when we have calculated the similarity feature vectors, we feed these vectors into the into a classic into a classification model.",
                    "label": 0
                },
                {
                    "sent": "So we have experiment.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "With random forest Night bias, support vector machines and logistic regression, and to address the high imbalance in the data set, we use standard approaches for sampling.",
                    "label": 0
                },
                {
                    "sent": "Those are random under sampling and random oversampling.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So next we will proceed with the evaluation of our approach.",
                    "label": 0
                },
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, we use a sample of three categories from the Gemini products.",
                    "label": 0
                },
                {
                    "sent": "Those are televisions, mobile phones and laptops from each of these categories.",
                    "label": 0
                },
                {
                    "sent": "We randomly selected a sample of 3500 product sets.",
                    "label": 0
                },
                {
                    "sent": "And for the same categories we manually build the gold standard on the micro later products.",
                    "label": 0
                },
                {
                    "sent": "So for each category selected around 300 products and we manually identify the matching pairs which also we have around 300 and we have a lot more non matching pairs.",
                    "label": 0
                },
                {
                    "sent": "So as you can see the datasets are highly imbalanced.",
                    "label": 0
                },
                {
                    "sent": "Oh",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For the evaluation of, we use the standard metrics for performance evaluation like precision, recall, and F score, and the results are calculated using 10 fold cross validation.",
                    "label": 0
                },
                {
                    "sent": "So as I said, we use for classifiers and three sampling methods.",
                    "label": 0
                },
                {
                    "sent": "We compare our approach to two baseline methods.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "1st is the standard TF IDF cosine similarity and the second one is the Silk Link Discovery framework, which is able to discover links between data items and with different data sources.",
                    "label": 0
                },
                {
                    "sent": "So the framework is based on genetic algorithm which is able to learn linkage rules which which consists of.",
                    "label": 0
                },
                {
                    "sent": "Blocks for value normalization, transformation and similarity.",
                    "label": 0
                },
                {
                    "sent": "We trained it on conditional on our conditional random fields features and the results are again calculated using stratified 10 fold cross validation.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So here we are.",
                    "label": 0
                },
                {
                    "sent": "Given the results for the televisions cut category.",
                    "label": 0
                },
                {
                    "sent": "So we show the results for each of the models for each of the sampling methods and for both feature extraction methods.",
                    "label": 0
                },
                {
                    "sent": "So for the conditional random field and dictionary as we can see, the results are quite well for the random forest, SVM and logistic regression, while the naive bias performs quite poor.",
                    "label": 0
                },
                {
                    "sent": "And in all the approaches, the CRF features outperformed the dictionary based approaches.",
                    "label": 0
                },
                {
                    "sent": "And here the best query is achieved when using random forest with no sampling curse, scoring F score of 82%.",
                    "label": 0
                },
                {
                    "sent": "Well, compared to the baseline approaches, we significantly outperform.",
                    "label": 0
                },
                {
                    "sent": "Both of them were the cosine similarity scores.",
                    "label": 0
                },
                {
                    "sent": "Quite bad, and the silky performs well, but still significantly worse than our approach.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of for the mobile phones we have observed same patterns in the results were again the best results are achieved when using random forests with random oversampling catching score of 81.5%.",
                    "label": 0
                },
                {
                    "sent": "Again, are significantly outperforming both of the baseline models.",
                    "label": 0
                },
                {
                    "sent": "Anne Anne.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So for the laptops category it's not worth it to see that the results are significantly worse compared to the other two categories.",
                    "label": 0
                },
                {
                    "sent": "So previously we had F score of above 80% and now we have around 63.",
                    "label": 0
                },
                {
                    "sent": "And the reason for this is that when it comes to matching laptops we have to use many more attributes like the type of the processor, the cash, the memory card, discounts on where for televisions, and mobile phones we had only four or five main attributes which were considered for.",
                    "label": 0
                },
                {
                    "sent": "Oh, matching the the products.",
                    "label": 0
                },
                {
                    "sent": "And also in this case we outperform both of the baseline methods.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the last section we performed actually the enrichment of the product ads, so we used previously built models, so we use the best model for each category an apply for matching of product ads with products from the micro micro data data set.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Also, we evaluated the 1st on the television category, so we selected 10 popular TV brands like Samsung, Vizio, LG.",
                    "label": 0
                },
                {
                    "sent": "We took the number of products that we have in the Gemini product that we retrieve the possible candidates for each brand from the micro data deal set an we applied the model to identify the metrics and then manually calculated precision.",
                    "label": 0
                },
                {
                    "sent": "So in total we identified 1300.",
                    "label": 0
                },
                {
                    "sent": "I mean I product that's in our data set an identified 55,000 micro data products which are candidate matching products and we found 600 matches with precision of 92%.",
                    "label": 0
                },
                {
                    "sent": "So you can see that the results are the number of discovered matrix is quite low, but the basic reason for this is that the GPA have products that we used are actually appeared in 2015, while the WDC Dale said that we used.",
                    "label": 0
                },
                {
                    "sent": "It was extracted in 2014, so we have only small overlap in the products in our datasets.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In the end, we counted the number of metrics and newly discovered attribute value pairs for each of the products and as we can see, for most of the products we are able to find at least one or two matches and we can identify at least three or four new.",
                    "label": 0
                },
                {
                    "sent": "You attribute value pairs, but also for some of them we can also identify like 6 new attribute value pairs and also we counted the number of offers, ratings and reviews that we can find in the micro datasets for the products in the Gemini products.",
                    "label": 0
                },
                {
                    "sent": "And we can see that for most of them we can find one or two offers, ratings and reviews, but also for some of them we can find up to 11 offers and up to 18 reviews and all this information is integrated within the ad retrieval system.",
                    "label": 0
                },
                {
                    "sent": "And the increases the performance is for retreiving the most relevant product that's for user defined search query.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So currently we are extending our work in several directions.",
                    "label": 0
                },
                {
                    "sent": "So first we try to enhance the conditional random field model with word embeddings.",
                    "label": 0
                },
                {
                    "sent": "So we make use of.",
                    "label": 0
                },
                {
                    "sent": "Uh, who hold datasets of microdata products to build word embeddings models, where which is actually able to.",
                    "label": 0
                },
                {
                    "sent": "Hello to represent each token in the deal set in a latent feature space where similar values appear close to each other and as you can see, values of same attributes actually appear quite close to each other and this knowledge can be directly integrated with the conditional random field, giving it better knowledge about the values of the attributes and therefore increasing the extraction performances.",
                    "label": 0
                },
                {
                    "sent": "The second direction is actually using products images as features, so we can see that for us it is easy to say that maybe the first 2 pictures are actually representation for the same product.",
                    "label": 0
                },
                {
                    "sent": "But it's not the case with the third picture, so we cannot directly infer the specification of the product.",
                    "label": 0
                },
                {
                    "sent": "But we have a good indication if two images are for the same product or not.",
                    "label": 0
                },
                {
                    "sent": "So to use these features in our approach we use state of the art deep learning methods.",
                    "label": 0
                },
                {
                    "sent": "So we use convolutional neural Nets, which allows us to extract feature vectors for the images which can be used in our algorithm for calculating similarity between the products.",
                    "label": 0
                },
                {
                    "sent": "And the third direction is actually using cover our approach together with text embeddings.",
                    "label": 0
                },
                {
                    "sent": "An image embeddings for product categorisation, which is a very important task for structuring the product ads in a product catalog and also to improve the user shopping experience for navigation and finding the products they need.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh, that will be all.",
                    "label": 0
                },
                {
                    "sent": "Thank you for your attention.",
                    "label": 0
                }
            ]
        }
    }
}