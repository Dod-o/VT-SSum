{
    "id": "t5wmnfd7hcyzg56dirgks3dlqkypnmy7",
    "title": "Dimensionality Reduction",
    "info": {
        "author": [
            "Neil D. Lawrence, School of Mathematics, University of Manchester"
        ],
        "published": "Feb. 5, 2008",
        "recorded": "January 2008",
        "category": [
            "Top->Mathematics->Statistics"
        ]
    },
    "url": "http://videolectures.net/epsrcws08_lawrence_dr/",
    "segmentation": [
        [
            "Thanks Tony.",
            "So well for the next 2 hours you're going to be hearing about dimensionality reduction.",
            "Tony said I'm at the University of Manchester School of Computer Science, but I was here in Sheffield before up until about a year ago.",
            "OK, so.",
            "Apologies for one thing, I was going to be here for the last couple of days for the summer school I went to school but I had a bronchitis so if I have a coughing fit at any stage, that's the reason why I. I'm not sure exactly what it's triggered by, but it can also be triggered by difficult questions, so if you ask a difficult question, I have a coughing fit.",
            "You'll know that's why.",
            "Having said that, do stop and ask questions.",
            "This is a new way as far as I know, of presenting some of the dimensionality reduction stuff.",
            "It's a different way from what we normally do in machine learning, but I hopefully it's going to give you a background to understand a lot of the methods we use.",
            "I'm not really going to talk in detail about some of the more recent machine learning methods, but hopefully I'm going to give you a background through which you can understand those methods.",
            "It perhaps in a unifying way."
        ],
        [
            "OK, so.",
            "We're going to start off with some motivation.",
            "And then we'll do a little bit of background.",
            "And then what I want to really talk about is dimensionality reduction through distance matching.",
            "Now this is a sort of old idea and it comes from statistics.",
            "Then I'll briefly sort of talk about when how we get those distances and how we can try and make distances come from along a manifold.",
            "Then I'll briefly show some stuff for model selection for trying to choose what a good dimensionality reduction."
        ],
        [
            "And then we'll end with some conclusions.",
            "So all the source code and the slides are available online and this talk is also available from my homepage.",
            "The slides Matlab examples are used.",
            "I've made a little tool box called the dim red dimensional reduction toolbox that I just put up there last night and any Matlab commands I'm using for examples I've given in typewriter font.",
            "So I think you should be able to recreate all the diagrams.",
            "I'm showing you all the results I've showed you.",
            "I mean I'm not showing you significant results on serious datasets.",
            "It's all illustrative stuff.",
            "By downloading that."
        ],
        [
            "Software."
        ],
        [
            "OK motivation.",
            "So this is the sort of motivational slides I tend to use for dimensionality reduction.",
            "So machine learning, certainly.",
            "For.",
            "We were completely obsessed by handwritten digits, mainly because they were used for zip code recognition in United States.",
            "And this is a digit 6 from AUS Postal Service data set.",
            "So it's a handwritten 6 and then this is the raw format.",
            "These data come in.",
            "They are typically processed when we use the machine learning, but this is well raw form that you see the data room.",
            "OK, so this is a 64 rose.",
            "By 57 columns.",
            "So there's 3648 dimensions to this.",
            "Data point now.",
            "Obviously this space contains a lot more than just this day."
        ],
        [
            "And if I sample randomly from this space using a probability of pixel being on."
        ],
        [
            "Equal to the number of pixels that are in the thick, so a simple model of."
        ],
        [
            "This is and this is what I see."
        ],
        [
            "Now another sample.",
            "I still don't see the six.",
            "Now.",
            "That's not surprising, because even if we sampled every nanosecond from now until the end of the universe, you wouldn't see the original six.",
            "That space is so large it's 2 to 3648 that it's something like there's more possible things can fit in that space than there are particles in the universe.",
            "That's how big that space is.",
            "So working in that space is extremely foolhardy.",
            "It's not a good thing to do because you're trying to do a model.",
            "With such an enormous space, there's no possible way that you could get a good idea of when these pixels are on and when they're off and what the correlations are, but."
        ],
        [
            "Wyndham OK, so that wasn't 6 either.",
            "That was a final sample just to prove that point."
        ],
        [
            "Um?",
            "OK, so what's an alternative model for digits?",
            "So instead of that model, let's try something else.",
            "Well, let's say that all sixes are perhaps made up by take."
        ],
        [
            "The prototypes."
        ],
        [
            "And wrote."
        ],
        [
            "Hating it."
        ],
        [
            "OK."
        ],
        [
            "So this is my way."
        ],
        [
            "Covering all six."
        ],
        [
            "Now you can run this script here and what you'll get."
        ],
        [
            "What is this result now?",
            "What am I showing you here?",
            "So what I've done is I've taken that 6.",
            "And I've rotated it 360 * 1 degree each to create a data set.",
            "Yeah, then with that data set I've taken the 1st and 2nd principle components of that data and I'm plotting along those components.",
            "What you immediately see is the data lives on a circle.",
            "And I've shown you some examples of the 60s.",
            "It's the closest point here that you're getting here, so the original 6 is up in the top right here somewhere.",
            "And as we rotate around the circle, if we rotate in a clockwise way, we actually move coincidentally clockwise around the circle as well.",
            "That's anti clockwise rotating 6 move clockwise around the circle and we see all the different sixes we get in the data set.",
            "Now, if you keep looking at the principal components.",
            "Of this data, you keep seeing that this is a 1 dimensional structure until you start to get to very low numbers of principal components, and when you get to those low numbers of Prince of Components, what you get is artifacts in the rotation algorithm.",
            "So this is an image rotation so."
        ],
        [
            "You rotate the pixels."
        ],
        [
            "Doesn't quite work per."
        ],
        [
            "We look at these."
        ],
        [
            "Two pixels here, right?",
            "As you rotate them, they sort of separate.",
            "Yeah, so there's some sort of interpolation you as you rotate.",
            "It's not an exact number of degrees, so there's some sort of Inter."
        ],
        [
            "Appalachian so."
        ],
        [
            "At least to some sort of."
        ],
        [
            "Lil bit of noise.",
            "On this circle, it's particularly visible there.",
            "And there and there and there.",
            "So you get this a little noise, which is to do with whatever algorithm the image rotation.",
            "And this is just an image rotation I downloaded from the web is using to interpolate those pixels.",
            "OK so but basically you see it's 1 dimensional structure with."
        ],
        [
            "Some noise.",
            "Now you could argue that those aren't all sixes, and indeed these guys down, here and nines.",
            "So this is a model of sixes and nines in effects, because it's really not the full rotation.",
            "That were perhaps interesting, but a part of the rotation.",
            "But this this slide also has a sort of another point, which is to sort of say that the data of interesting data will also get multiple prototypes, so it may not be just one six when rotating, it may be another six season nine in this case as model A-69, so we have one six that we're rotating another nine in another portion of space, but it's a little bit of a manifold living in this high dimensional space, which is what the data."
        ],
        [
            "Actually lives on.",
            "Now in practice.",
            "This is a silly model, right?",
            "Because of course.",
            "The data may undergo several distortions, far more than we've shown here.",
            "For example, digits as well as undergoing rotation, they undergo a process known as thinning, where the stroke of the digit becomes faster and thinner.",
            "Translation as well, so they move around in that box.",
            "That's possible too.",
            "Excuse me, so the point is, for data with structure and this is almost how I think of data with structure.",
            "How I define data structure there's I mean I'm skipping one aspect here, which is to do with conditional independencies.",
            "Another way of dealing with high dimensional data is to specify conditional independence is in it, and this isn't that, but the way I tend to think of data with structure is that we expect fewer of these types of distortions than there are dimensions in the original data set.",
            "And that's the real.",
            "Inherent dimensionality of our data, not the 3648 pixel space, it's the number of sort of distortions it's going through.",
            "So that still leaves.",
            "It is a difficult question though, because it may be.",
            "For example, consider sevens.",
            "If we're modeling all sevens, we'd have to have prototype sevens, which have slice across 4 on the other way round, slice across the the upright portion of the 7, and that's going to be a slightly different prototype, and that type of seven may undergo a slightly different set of distortions than the other types of sevens, so within this high dimensional space where we've got these multiple distortions going on.",
            "It may not be that it's not an explicit dimension, even it may be that different parts of the space have different slightly different dimensionality, but.",
            "The general conclusion is that we should look to deal with high dimensional data by looking for a lower dimensional nonlinear embedding.",
            "So that's the basic sort of motivation for why you're interested in doing dimensionality reduction."
        ],
        [
            "OK.",
            "So just before we go on, I want to do a little bit of notation.",
            "So.",
            "I'm going to be working with.",
            "A data space where my data matrix is why so?",
            "This is the observed data.",
            "The fixes that we've seen the rotated data set of 60.",
            "Now I'm going to be looking at lower dimension and latent or embedded spaces.",
            "I'll tend to call them latent spaces as an artifact of probabilistic sort of language.",
            "So well, dimensionality, my latent space is going to be denoted Q.",
            "Then I mention ality of the data space is going to be demented, denoted big D. So Big D in the six case was 3648.",
            "And then I'm going to be dealing with N data points where.",
            "And in the six case was 360 for each degree of rotation.",
            "Now I'm going to draw my data in terms of a data matrix, which is called a design matrix.",
            "So why has N rows and D columns?",
            "Yeah.",
            "And it's made up of each data point is a vector Y from the first row up to the NTH row.",
            "And this: notation indicates it's a vector, but these should always be filtered as column vectors.",
            "But note the transpose here, so this matrix here has N rows and columns.",
            "That's important that this is the other way of writing it, where this is each feature of the data, the first feature up to the feature, and the same with the latent variables.",
            "Now this way of writing the matrix is important because when we come to write down covariance matrices in.",
            "Matrix notation.",
            "It's sort of important that that's the way around that we're using.",
            "I'm actually not going to really make use of that mapping matrix or the centering matrix, although I thought I might when I was writing this slide.",
            "OK, so these two are the main sort of important things.",
            "The latent variables is going to be my representation of that data in the reduced dementia."
        ],
        [
            "Space, so within this examples, these are my latent variables, X one X2."
        ],
        [
            "OK, so just to sort of recap, recapitulate A is a vector from the.",
            "I throw a ice,: that sort of Matlab notation and a colon, J is a vector from the J.",
            "Sorry Jeff column, that should say.",
            "An X&Y are design matrices now.",
            "Getting all the centering there, but the sample covariance this bits important.",
            "The sample covariance S is given by one over the number of data points times Y transpose Y. OK, so why hat is a centered version of our data?",
            "So the centering matrix takes Y and removes its mean.",
            "That's basically its effect.",
            "So Yhat is what will typically be working with which is a censored version of our data if yhat is centered.",
            "I don't know how useful this notation you are.",
            "You can write down the covariance as 1 /, N, Y hat, transpose, Y hat.",
            "The centered in a product matrix and this is important to your Orient.",
            "Think it's been seeing some kernel stuff?",
            "Is the kernel matrix white hat white hat, transpose?",
            "So although it's an inner product that appears on the outside, which can slightly confuse people, but this is the standard statistics notation for inner product matrix in the covariance matrix, so that's sort of important message."
        ],
        [
            "From the notation.",
            "OK, so how are we going to do dimensionality reduction?",
            "Well, the way we're going to choose to do it is."
        ],
        [
            "By distance matching.",
            "So class is a classical statistical approach.",
            "So we represent our data via sort of proximities.",
            "How near they are to each other?",
            "There's different types of proximity data.",
            "One type is similarities and the other is dis similarities and one example of a dissimilarity matrix is a distance matrix.",
            "So we'll just be using mostly.",
            "The Euclidean distance, so that's the L2 norm of the difference between two vectors.",
            "So it's simply this.",
            "My notation for the L2 norm, so these are two data points.",
            "This is distance between them is denoted ion DJ, J and then this is just the matrix way of expressing it, which is the square root of this vector inner product, where the vectors given from the distance between the subtraction between the two vectors.",
            "Now for any given data set, I can display that distance."
        ],
        [
            "Matrix.",
            "And when I display the distance matrix for the 60s data those rotated sixes.",
            "This is what it looks like.",
            "So what we see is this is 0 down here.",
            "This dark blue.",
            "The red dark red hair is about 8 and a half thousand, but the scale isn't really that important here.",
            "'cause we could always just rescale things.",
            "You'll notice immediately is that about 880 degrees rotation, so these are the data point labels.",
            "But remember, each one is a degree of rotation, so at 180 degrees rotation, the distance between these things is at a maximum, so it dark is read along or vague line.",
            "Here it's not sharp maximum, but it is at a maximum.",
            "The distances at a minimum, obviously, where the point is the same, so it's zero along the diagonal, and then the distance increases as the angle increases, up until about 180 degrees.",
            "Then the distance starts decreasing again until the rotation is complete, and then you can see at the corners you're getting this sort of wrap around structure, so that's the periodic structure that's in the data.",
            "Now, what else can you see in this thing?",
            "We can do just about make out the lines along here.",
            "This is sort of greeting effect.",
            "On these distances that Gridding effect is because it's associated with the 90 degree rotations.",
            "So it's slightly different to rotate an image by 90 degrees because the pixels are square, so you can do the rotation exactly and it turns out that in this distance matrix that slightly weird effect actually is visible and you get these squares at 90.",
            "180 and 270 degrees.",
            "So that's kind of quite interesting.",
            "But you can see that sort of finer structure in this distance matrix.",
            "Now this is the interpoint distances and it's living.",
            "These distances are living in a.",
            "3648 dimensional space.",
            "In fact, to do the rotation, I first of all pad out the size of the images to make them 64 by 64 pixels so they're actually living in a 4009 four 1096 dimensional space.",
            "Otherwise you can't rotate.",
            "It's not a square sort of image.",
            "OK."
        ],
        [
            "So what's the idea of what we're going to do?",
            "Well, what we're going to do is we're going to do a version of multi dimensional scaling.",
            "So in multi dimensional scaling the idea is to find a configuration of points X where X is the low dimensional space for which the distance between these points is the Euclidean distance and it closely matches the corresponding distance in the high dimensional space.",
            "OK, so the idea is.",
            "We want to find a load of points X.",
            "Whose interpoint distances match those interpoint distances in Y, but X is going to be in a lower dimension.",
            "And that's what scaling is about.",
            "There's different types of scaling and different ways of matching these distances.",
            "If you do non metric multidimensional scaling your matching the ordering between those distances.",
            "Metric, metric minded initial scaling is about matching those distances, but perhaps matching a function of those distances to each other.",
            "We're going to specifically be looking at classical multidimensional scaling, because it turns out to have a simple algorithm behind it, and that algorithm underpins all the dimensional reduction methods.",
            "They are based on spectral decompositions that you hear about in machine learning, so that includes things like Isomap, locally linear embeddings, maximum variance unfolding, or semidefinite embeddings.",
            "And Laplacian eigen Maps.",
            "Now, I'm not going to talk in details about those algorithms, principally because you just have to look at the papers and it describes what the algorithm is, how you put it together.",
            "And how you run it?",
            "In fact, it would take you apart from in some vague complexities like you have to do some shortest path algorithms for some of them it would take you very little time to go away, encode those algorithms up.",
            "So there's little point in me parroting about those.",
            "But what I want to talk about really, is what's underpinning those algorithms.",
            "The classical scaling approach, which is what allows you to represent these distances.",
            "So you can think of those algorithms as living in the same sort of pot.",
            "So we need an objective function for matching the matrix of distances in the latent space given by that for the distances in the data space given by the Euclidean distance."
        ],
        [
            "In why?",
            "So here's an objective function.",
            "It's a slightly funny one.",
            "And it's it is contrived.",
            "I mean it's contrived because it leads to the result I want to show you, which is the classical scaling result.",
            "What we're going to do, though, is before we do the full feature extraction, we're going to consider a simple way of doing dimensional reduction.",
            "So this is our objective function.",
            "It's just the sort of absolute value between the squared distances in the data space minus that in the latent space, and the sum of all the absolute values.",
            "So it's like a sort of entrywise L1 norm on those distance matrices.",
            "Now, rather than doing what, how are we going to do?",
            "Our feature selection?",
            "Well, sort of dimensionality reduction?",
            "Well, let's think of a really simple, dumb way of doing dimensionality reduction, and it's called feature selection.",
            "Will just extract dimensions from our original data Y and will make up.",
            "Are X by extracting columns Y and just putting them into X, so that's just representing our data by a subset of the features in the original data.",
            "It's a common problem in itself, but it is a specific example of dimensionality reduction.",
            "So select 4X in, turn the column from Y, but most reduces this error.",
            "Inserra here until we have the design, cue the desired number of columns in X.",
            "Now it turns out.",
            "To minimize this error that should be able X, sorry, we compose X by extracting the columns of Y which have the largest variance.",
            "Now we're just going to drive that algorithm."
        ],
        [
            "OK.",
            "So the squared distance gets rid of the square root nasty square root sign.",
            "So the squared distance is just the sum across the dimensions of the squared.",
            "Of the difference between the two data points, yeah, so these are scalars.",
            "Now this is the ice data points K feature, and it's just the sum of the squares for that.",
            "Now what we can do is we can always reorder the columns of Y without affecting the distances so I can just move these columns of why around without changing the distances, so will choose an ordering of Y.",
            "To help us in algorithm, so we'll say.",
            "But the first Q columns Y are those that will best represent the distance matrix by definition.",
            "'cause I can reorder without changing.",
            "So I'm going to say that the first Q columns are.",
            "Why are those that I'm going to use for X?",
            "Now what we can do is we can substitute X for Y for the first case, so an hour X is Y for K = 1 to Q because of that ordering.",
            "This means that we can replace the distance in latent space, which was this Delta squared is equal to X -- X JXI minus XJ for the K feature with the relevant wise for these first Q values.",
            "OK so just to sort of.",
            "The advantage of ordering is notational purely notational.",
            "If I wasn't to do the ordering, so the ordering ordering like this means that I can rewrite this as K = 1 to Q / Y if I wasn't to define that ordering.",
            "If it was like the last and then the 1st and then the middle one, writing this sum would be quite awkward.",
            "So it's just simply notational so that I can write this sum equal to this OK. And I can do it without loss of generality, so that's the sort of nice thing about it.",
            "OK, so our objective look."
        ],
        [
            "Like this, but because.",
            "We can rewrite the Deltas as some over K = 1 to Q and the DS is the sum of a K = 1 to DD necessarily being larger than Q.",
            "We can basically rewrite our objective as being K = Q + 1 to D, so the remaining dimensions of Y, basically the sum.",
            "Of the interpoint distances for the dimensions of why we've left out, yeah.",
            "We can then introduce the mean of each dimension and then put it inside these brackets here.",
            "So I've introduced the mean twice so I can do that.",
            "It's just a convenience thing so that you can see that these the form of this.",
            "By introducing the mean into here like that and then multiplying out.",
            "So that's some sorry it should be across all these bracketed terms here.",
            "That's not very clear.",
            "Should be across all those bracketed terms.",
            "Once I'm."
        ],
        [
            "Let's play it out like that.",
            "I can also bring in some of these sums inside, so that's across all the bracketed terms.",
            "But here I'm bringing the sums inside, so this sum applies not to hear, but does apply to here, but it needs to have a factor of N appear outside this guy.",
            "So if I if I bring the sum over eyes over Jays in, it doesn't apply to this I hear, but it multiplies, I buy N. So as I bring it past this I. I get a factor of N out the front and then I get a sum over J appearing in front of this one.",
            "In this case.",
            "Here I get a sum over J there and then the sum over I drops in inside here because there's no eyes in this outer term here.",
            "Similarly, I get a factor of N appearing outside that guy.",
            "Hello excuse me.",
            "What you should notice though is this looks very similar to the variance.",
            "In fact, it's N times the variance of that column and that's N times the variance of the J column.",
            "So that sent.",
            "I'm sorry the case column and that end times the variance of The Cave column 2.",
            "So these guys, though, since this is the mean.",
            "This is just the sum over all the values.",
            "Both of these guys just go to zero because they end up with just being the mean minus the mean.",
            "So we basically recognize the.",
            "The remaining variance as the sum.",
            "This is the variance of the case column.",
            "This is the variance of the column.",
            "So get two.",
            "Sorry this is N times the variance of the column is N times the variance of case column.",
            "So we basically get the result that the error is equal to 2 * N ^2 times the sum of the variances of discarded columns.",
            "So these are discarded columns.",
            "So we want to minimize this variance.",
            "So if you want to minimize this error.",
            "We should minimize the variance of the columns we're discarding, yeah?",
            "Does that make sense?",
            "Geometrically, it makes a lot of sense, and there'll be a little plot to show that later.",
            "So basically this little simple proof is saying if we're doing feature extraction in such a way that we're trying to throw away features from our data in such a way that we're trying to preserve this distance matrix the way we should do that is discard directions of least variants, or retain directions of most variants, OK?"
        ],
        [
            "So how does that work in practice?",
            "Well?",
            "We like to do 2 dimensional.",
            "Representations of our data.",
            "While we like to do 2 dimensional representations of our data because we can show them on slides.",
            "So if we did a 2 dimensional representation of our data, so we retain the two largest variance columns from this data and then we look at the distance matrix.",
            "Our representation of the data.",
            "That's what we see.",
            "So it's true."
        ],
        [
            "Going to represent that."
        ],
        [
            "So it's not doing a particularly good job.",
            "I mean, you can see it's it's getting this diagonal dominance a little bit, but not really because it's sort of flattening all the way across there.",
            "If we go to 10 dimensions, sort of things start to improve a bit.",
            "But still, it's not clear that the maximum is really at 180.",
            "In fact, this one's got a bit of a minimum quite close to 180, so 10 dimensions isn't real."
        ],
        [
            "Helping us.",
            "We go to 100 dimensions.",
            "So now you can start seeing perhaps this minima at about 180.",
            "You don't see this structure though.",
            "This gridding structure particularly well of the rotation of each 90 degrees, so some of the finer structure isn't really there.",
            "If you go to 1000 dimensions.",
            "So we saw it off with a 4096 dimensional data set and we've got rid of 3/4 of dimensions.",
            "Isn't actually a particularly great dimension reduction.",
            "Now you really start to see the structure of the thing, so you need about 1000 dimensions if you're going to do feature selection to extract these guys and maintain, retain the quality of this interpoint distance matrix.",
            "So in some sense you could say it's not a great algorithm, but there you go."
        ],
        [
            "OK, So what are we basically doing here?",
            "Well, this is now a high dimensional data set.",
            "It's got 2 dimensions in it and what we're going to do is we're going to try and reduce it to 1 dimensional data set so I can show you how that reduction is taking place.",
            "So what this variance maximum variance is saying is that we should look for the direction of maximum variance and it along this direction.",
            "It's variance looks like standard deviation looks to be about 6, so variants around 36 around this direction.",
            "It looks to be more like a sort of four.",
            "Perhaps standardization of 16.",
            "So here we should be retaining this direction and discarding this direction according to the algorithm we've just described."
        ],
        [
            "So what does that involve doing that involves projecting all these data points onto the X axis and replace?"
        ],
        [
            "Our original date set with."
        ],
        [
            "At.",
            "So look at all the structure we're throwing away there."
        ],
        [
            "Between all these."
        ],
        [
            "Points immediately, it's great isn't."
        ],
        [
            "So that's the algorithm."
        ],
        [
            "However, there is a bit of a more sensible algorithm that is very obvious if you think about one thing.",
            "Rotation.",
            "Preserves interpoint distances I can rotate.",
            "This data set.",
            "And all the interpoint distances stay the same.",
            "So if I make a rotation.",
            "I'm doing nothing to lose any.",
            "I can basically make any rotation I like.",
            "So what should I do?",
            "Well, it seems."
        ],
        [
            "Makes sense to try and rotate the data."
        ],
        [
            "Such that when I project."
        ],
        [
            "Onto the X axis.",
            "I'm losing a lot less information, so look at the size of these projections here.",
            "Versus"
        ],
        [
            "Size of the projections here.",
            "And actually in this example they are quite strongly correlated, but you could envisage examples where it was even through it.",
            "Well, you've got sort of less variance in that direction, and more and sort of stronger correlations.",
            "And."
        ],
        [
            "This becomes even true."
        ],
        [
            "In fact, it's."
        ],
        [
            "Very true for the day."
        ],
        [
            "Getdata, the art."
        ],
        [
            "Digit data we're using.",
            "So."
        ],
        [
            "Sort of improved idea is to do this rotation first.",
            "Yep, so we rotate.",
            "To align our data so that it's variance, but our actions are maximum variance are aligned with the axis."
        ],
        [
            "And then we extract these interpoint distances.",
            "OK, so we need the rotation that will minimize the residual error.",
            "Now we already derived an algorithm for discarding directions.",
            "And it said that we should discard the direction with the maximum variance.",
            "The error is then given by the sum of residual variances.",
            "Rotations of the data matrix do not affect this analysis at all, because those distances all."
        ],
        [
            "Remain the same."
        ],
        [
            "So what this means is we should rotate to find the directions of maximum variance or so rotate so that the maximum variance directions are aligned with the axis, and then discard the lower there."
        ],
        [
            "Directions.",
            "Now you might recognize that algorithm and we'll come back to that in a moment.",
            "But if you do that, this is what you get.",
            "So on the left this is distances reconstructed from 2 dimensions of the data set.",
            "Remember the two dimensional example before."
        ],
        [
            "Oh there.",
            "Now.",
            "Are two dimensional."
        ],
        [
            "Example is already showing.",
            "But we've got a sort of minima or 180.",
            "It took 100 dimensions before we saw that structure in the example where we're just doing feature selection.",
            "Um?",
            "This is distances reconstructed with 10 dimensions.",
            "Now you're seeing the fine structure in the grids with only 10 dimensions.",
            "It took 1000 dimensions to see that before."
        ],
        [
            "If we go to 100 dimensions.",
            "Well, again, we're not really gaining much in terms of structure.",
            "It's just improving the."
        ],
        [
            "The width of this guy, which is a little bit too wide is very wide."
        ],
        [
            "Here too right here it's getting smaller.",
            "And then if you go to 360 dimensions then that's actually all the structure is.",
            "The Matrix rank is only 360, so this is all the structure in the data.",
            "So you basically just refining the width, the sort of the shape of this Valley to make it sort of narrower narrower Ridge in there as you increase the dimensions.",
            "So you're getting a lot of the structure of the data with only two dimensions or 10 dimensions of the data, so it's a very good approach that I mentioned.",
            "Duction"
        ],
        [
            "If you believe in distance matrix presentation.",
            "OK, so how do we find these directions where we find directions in the data with maximal variance, but wait a second?",
            "That's what principle component analysis does.",
            "So there's already an algorithm for doing this for us principal component analysis set.",
            "Look for directions of maximum variance in the data, so PCA.",
            "Rotates the data to extract these directions and it works on the sample covariance matrix here.",
            "So let's just do a bit of a review of."
        ],
        [
            "Principle component analysis.",
            "OK.",
            "So PCA says find a direction in the data R, so RI think it was one vector from a rotation matrix.",
            "So you can rotate your whole data and R is like taking one of about those rotations.",
            "So you're projecting down to one dimension, but it's from that larger rotation matrix, so it's the first principle component is, well, actually one.",
            "It will be turned out to be.",
            "So which project onto that X we've got yhat times R, so this is the nice compactness of this notation.",
            "This is the center data matrix and then this is 1 vector of rotation being applied to.",
            "It gives us a 1 dimensional solution.",
            "What we want to do is maximize that variance so we look to find our one which maximizes that subject to the fact that it's normal length.",
            "We can always maximize that by scaling our forever and ever, but we want our to be part of rotation matrix, so we scale it to norm to the unit norm.",
            "We write down the variance of X.",
            "We can re express it as the product of these two guides because they're centered, so X will be centered as well.",
            "1 / N times the product of Y.",
            "That are one transpose Y R1.",
            "Turning that around so that we just flip that guy because of the transpose, we can bring the R1 outside so it's R1, transpose times 1 / N Y hat, transpose Y hat.",
            "Times are one.",
            "Now we recognize that as the sample covariance.",
            "From our definition of annotation earlier.",
            "So it's basically saying that the variance in the X One Direction can be written by ZAR, one transpose S, R1 through the well known for PCA result.",
            "So that's what we want to maximize.",
            "That's this guy."
        ],
        [
            "Basically.",
            "Do you ask questions?",
            "If you're coughing, reminded me of questions.",
            "If you've got any questions, do ask.",
            "I can still hear while I'm coughing, so that's a good time to ask a question.",
            "So how do we look for this R1?",
            "Well, it's the solution to a constrained optimization.",
            "What we do is we use a LaGrange multiplier to apply this constraint so the LaGrange multiplier Lambda one 1 -- R one, transpose, R1.",
            "Taking the gradient with respect to R1.",
            "Leads to do vector gradients for 2 Sr 1 -- 2 Lambda one R1.",
            "Now, if we rearrange that, we find that the sample covariance times R1 is equal to Lambda one R1, which is recognized as an eigenvalue problem.",
            "So this basically the first guy is the solution to an eigenvalue problem.",
            "But which eigenvalue is it so?",
            "An eigenvalue problem that there's lots of ours that solve this.",
            "There are all the eigenvectors of matrix X.",
            "Which are."
        ],
        [
            "Is it that we need what we can find that out by?",
            "Premultiplying looking at the gradient we pre multiply the gradient by R1 transpose.",
            "We get 2R1 transpose Sr 1 -- 2 Lambda One art one transpose R1 but are one transpose R1 we've constrained to be one, so it's 2R1 transpose Sr 1 -- 2 Lambda one.",
            "Now, since the gradients must be 0 at the solu."
        ],
        [
            "Ocean.",
            "Which is also of course how we were finding this looking for fixed point."
        ],
        [
            "Setting the gradient equal to 0.",
            "We can reorganize to find that Lambda one must necessarily equal this.",
            "But"
        ],
        [
            "What did we say that was?",
            "While we've already said that that's the variance of X1?"
        ],
        [
            "So Lambda one."
        ],
        [
            "Is equal to that, but Lambda one is also the eigenvalue.",
            "So here we can solve for Lambda one and R1 simultaneous."
        ],
        [
            "And I'm the one is the eigenvalue an that eigenvalue is the variance of the data in the X space.",
            "So which eigenvalue do we need to extract to get the largest variance, the largest eigenvalue?",
            "So maximum variance is therefore necessary.",
            "The maximum eigenvalue of S and that's called the first principle."
        ],
        [
            "Opponent.",
            "So what's next?",
            "Well, the next thing to do is to find the other principle components, and those are basically orthogonal directions to the earlier extracted directions with maximal variance.",
            "So there's now an additional constraint, North Orginality constraint that for J is less than K. RJ transpose RK is must be set to 0, so that's what means that these vectors are at right angles.",
            "So for all, so for extracting the case direction.",
            "Now RK, it must be at right angles to all the previous directions.",
            "So we've extracted the 1st so K can be 2K can be 3K can before.",
            "Furthermore, we must have again that it's.",
            "Normal length itself, so RK transpose RK is equal to 1.",
            "The Lagrangian now is a little bit more complicated 'cause it's got this additional constraints coming in.",
            "Child represented the ground rules apply gamma.",
            "But once again, we can derive the gradient.",
            "Now the gradient now turns out the same form for the first 2 terms, and then the third term is just the sum of all the RJS times there gamma JS.",
            "'cause all cap is in each one of those that's from J = 1 to K -- 1.",
            "So all all."
        ],
        [
            "The directions we've extracted previously.",
            "Now that gradient.",
            "Looks like that premultiplying by R. Ajay Premultiplying by RJ Zeros this guy.",
            "Sorry.",
            "And this guy.",
            "And basically tells us that the gamma eyes end up being 0.",
            "So that means we can move, remove this term and we can write down the covariance as this eigenvalue problem here.",
            "Pre multiplying by RK implies that Lambda K again, so the variance that eigenvalue the case eigenvalue.",
            "Is equal to RK, transpose SKRK, so each eigenvalue is still the variance in the X space.",
            "So we basically have that we can extract all the principal components of the data using an eigenvalue problem where the largest the largest eigenvalues are associated with the largest variance in the latent space we're creating.",
            "So we want to retain the largest eigenvalues and that gives us a good visualization of our data."
        ],
        [
            "So that's known as classical multi dimensional scaling.",
            "It's known as principle coordinate analysis as well.",
            "But it's also principal component analysis, so principle component analysis really sits at the core of so many dimensionality reduction algorithms.",
            "This is one reason and given time, which I think will have.",
            "Will see other reasons why principle component analysis is core.",
            "Given different interpretations of principle component analysis.",
            "So the variance by looking for directions of maximum variance in the data.",
            "We're also looking to retain the most distant.",
            "That makes sense because by finding directions the maximum variance the potential distance in a large variance direction is much greater than the potential distance in a small variance direction.",
            "So it makes complete sense.",
            "But do remember that rather artificial objective function we started with."
        ],
        [
            "All this time ago, if I were to tell you."
        ],
        [
            "I wanted to match distances.",
            "Between a reconstruction of a distance annadata space based distance, that's probably not the first distance function.",
            "You would objective function.",
            "You would write down with the square of the distance is, and then the sort of absolute value of the distances either side of it.",
            "It's a slightly artificial objective function.",
            "Of course, if it's satisfied with value 0, then the distances match exactly as you would hope, so it's still a reasonable objective function, But it's perhaps not the first objective function you would right out of the box.",
            "You might write a squared matching.",
            "You might write dij minus Delta, IJ all squared.",
            "Something like that might be the first thing right?",
            "And in fact people do use those in things like metric, metric, multidimensional scaling.",
            "There are there are other error measures that I use, but they don't lead to eigenvalue problems.",
            "OK.",
            "So that seems fine.",
            "I've spent an awful lot of time telling you about principal component."
        ],
        [
            "Um?",
            "OK, but there's some subtleties.",
            "So the rotation which finds the directions of maximum variance is the eigenvectors of the covariance matrix.",
            "The variance in each direction is given by the eigenvalues, but there's a problem.",
            "We've talked about the sample covariance matrix, but working with the sample covariance matrix may be impossible because we may be given the relationships between our data in terms of a distance matrix in the 1st place and the relationship between a distance matrix and a sample covariance matrix is not obvious.",
            "How do you get so say, I say?",
            "An example we're going to use in a bit is distances between towns between cities in Europe.",
            "So if I give you the road distances between a bunch of cities, you might expect you can write down a map of how those cities look on a map, how how far apart they are.",
            "If you know all the distances between them, their relative positions, that's a reasonable thing to be able to expect to do, and in fact, that's what classical scaling is allowing you to do to match those distances by putting them in a 2 dimensional space.",
            "That's exactly what you want to be doing.",
            "But I haven't given you the sample covariance of the location of the cities.",
            "I haven't given you the absolute location of these cities.",
            "If I did, you could just put them in two dimensions.",
            "I'm not giving you that.",
            "I'm just giving the distances between them.",
            "So how do you perform PCA on this?",
            "Seems a bit strange.",
            "If you don't have access to those absolute positions, we cannot compute the original sample covariance, so we can't just do PCA.",
            "So principle coordinate analysis or classical scaling is different from PCA, because it allows us to deal with this problem in some sense."
        ],
        [
            "OK, well I want to sort of just look at the eigenvalue problems in a matrix form.",
            "So this is the matrix representation of the eigenvalue problem, where I've dropped the 1 / N scaling on the sample covariance.",
            "This is basically the sample covariance.",
            "Without that scaling, those scalings don't really significantly affect the eigenvalues, they just scale them.",
            "So here's the sample covariance.",
            "There's the rotation matrix, which is rotating down to Q dimensions, so this is a D by Q matrix to rotate down to Q dimensions and then here is it on the other side.",
            "And this is the diagonal matrix of the eigenvalues.",
            "So this is the standard matrix representation and value problem.",
            "So what I'm going to do with that?",
            "I'm going to pre multiply that by Y hat.",
            "So Y hat Y hat transpose Y had RQ is equal to Y, hat RQ Lambda Q.",
            "Now I'm going to post multiply by the vector of eigenvalues.",
            "The diagonal vector of eigenvalues.",
            "This is a Q by Q vector.",
            "So this is one over the square root of that guy.",
            "So I post multiply here.",
            "He arrives on that side and then I post multiply on this side here."
        ],
        [
            "Now I'm just going to do a little flip.",
            "Bring that guy inside.",
            "I can always do that because I mean this is basically just the square root of both of these.",
            "Things are just the square."
        ],
        [
            "Out of."
        ],
        [
            "Of the eigenvalues.",
            "So why have I done that?",
            "Well, now you see, I've got the."
        ],
        [
            "Same for me, decide, and then I'm going to make this definition."
        ],
        [
            "I'm going to find that to be you Q."
        ],
        [
            "So by defining that as you Q. I have this now.",
            "What do we notice?",
            "Well, this is the censored inner product matrix.",
            "This.",
            "Looks like an eigenvalue problem on the centered inner product matrix.",
            "These are definitely a diagonal matrix of Lambda cues, but is UQ the right matrix to diagonalize yhat yhat transpose?",
            "Well?",
            "I suppose by definition it must be, but we can actually just show that it is actually."
        ],
        [
            "A set of eigenvectors.",
            "How are we going to do that?",
            "Well, we're going to prove that you Q are the eigenvectors of this inner product matrix.",
            "The way we're going to do that is we're going to diagonalize the inner product matrix with you Q.",
            "So by Premultiplying Biuku transposon post, multiplying by UQI should be able to diagonalize this matrix.",
            "So just substituting in the form for you Q."
        ],
        [
            "Here we get this rather a long and involved expression.",
            "But we notice in the center here.",
            "That's just why transpose Y hat transpose Y hat Y had transposed yhat squared.",
            "Yeah.",
            "So we replaced with that."
        ],
        [
            "Now we use the fully eigen decomposition of the sample covariance.",
            "So this is not just stopping with acute eigenvalue.",
            "This is taking the whole decomposition.",
            "So this is an exact relationship is not a reduced rank relationship.",
            "So I can write this."
        ],
        [
            "And substituted in there.",
            "But I can also use the fact that the square of these is R, Lambda, R, transpose, R, Lambda, R, transpose is just our Lambda squared R transpose.",
            "So for a symmetric matrix the eigenvalues and vectors of the square are the same as the original eigenvectors and eigenvalues are those original eigenvalues squ?"
        ],
        [
            "So I can plug that in there.",
            "Has that formula.",
            "The next thing we need to look at is these are cute."
        ],
        [
            "Transpose art.",
            "Now looking at those RQ transpose ours.",
            "It turns out that they have this reasonably simple form.",
            "They turn out as a bunch of ones which are the eigenvectors that match the first Q eigenvectors on here and then we've got a load of things they're orthogonal to, so you get a bunch of 0, so this is the identity followed by a bunch of zeros.",
            "So basically that's the form of our trans transpose RQ.",
            "Trip."
        ],
        [
            "Get in there.",
            "And then we get to multiply, multiplying it by Lambda.",
            "So I'm putting one Lambda on that side for those, and I'm going to put the other land on that side, which gives us the Lambda Q."
        ],
        [
            "20 multiplying it by itself transpose, so multiplying that by its own transpose simply gives me Lambda squared Q.",
            "So I can substitute that."
        ],
        [
            "Whole thing there."
        ],
        [
            "For Lambda squared Q."
        ],
        [
            "Yep."
        ],
        [
            "Looks like that we send diagonalizes to form that.",
            "So you queue this matrix that we've seen looks like a set of eigenvectors is actually a set of eigenvectors, and it's the set of eigenvectors.",
            "The first Q eigenvectors of Y hat, Y hat transpose, and the eigenvalues are the same.",
            "Now this is sort of well known in statistics, and this is the eigenvalue problem behind principle coordinate analysis or classical multi dimension."
        ],
        [
            "Scaling.",
            "The difference between them, the two eigenvalue problems are equivalent.",
            "One sold for the rotation.",
            "So in one case you're trying to find what is the rotation of these points.",
            "The other one solves directly for the location of the rotated points, so solve directly for X.",
            "So in one case you're solving for the rotation of Y, which gives you X and the other case you're solving for X.",
            "Now Wendy, the dimensionality of the data set is less than N. It's easier to solve for the rotation computationally.",
            "The eigenvalue problem is simpler, but when D is greater than N, we solve for the embedding.",
            "That's principle coordinate analysis.",
            "We solve for X.",
            "However, in multidimensional scaling we might not even know why, so we don't even know what D is in the base case.",
            "So we can't compute Y transpose Y from the distance matrix.",
            "But can we compute Y hat Y hat transpose instead?",
            "So this is the distance matrix.",
            "This isn't the distance matrix, it's another state inner product matrix.",
            "So it's still not quite the distance matrix, so if we're given a distance matrix, how do we go from the distance matrix?"
        ],
        [
            "This error product matrix here.",
            "So I'm going to give you a interpretation of that, which I call the covariance interpretation.",
            "I've not really seen it written down, but it's the way I look at these things, and I think it helps to unify the way of thinking about these things.",
            "But I'll do that after I think you normally take a break in these two hour long ones do.",
            "Or you want to carry on?",
            "I see lots of nods.",
            "OK, so well, what we'll do is we'll have a 10 minute break and we will carry on at about 25 two.",
            "10 OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Thanks Tony.",
                    "label": 0
                },
                {
                    "sent": "So well for the next 2 hours you're going to be hearing about dimensionality reduction.",
                    "label": 1
                },
                {
                    "sent": "Tony said I'm at the University of Manchester School of Computer Science, but I was here in Sheffield before up until about a year ago.",
                    "label": 0
                },
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "Apologies for one thing, I was going to be here for the last couple of days for the summer school I went to school but I had a bronchitis so if I have a coughing fit at any stage, that's the reason why I. I'm not sure exactly what it's triggered by, but it can also be triggered by difficult questions, so if you ask a difficult question, I have a coughing fit.",
                    "label": 0
                },
                {
                    "sent": "You'll know that's why.",
                    "label": 0
                },
                {
                    "sent": "Having said that, do stop and ask questions.",
                    "label": 1
                },
                {
                    "sent": "This is a new way as far as I know, of presenting some of the dimensionality reduction stuff.",
                    "label": 0
                },
                {
                    "sent": "It's a different way from what we normally do in machine learning, but I hopefully it's going to give you a background to understand a lot of the methods we use.",
                    "label": 0
                },
                {
                    "sent": "I'm not really going to talk in detail about some of the more recent machine learning methods, but hopefully I'm going to give you a background through which you can understand those methods.",
                    "label": 0
                },
                {
                    "sent": "It perhaps in a unifying way.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so.",
                    "label": 0
                },
                {
                    "sent": "We're going to start off with some motivation.",
                    "label": 0
                },
                {
                    "sent": "And then we'll do a little bit of background.",
                    "label": 0
                },
                {
                    "sent": "And then what I want to really talk about is dimensionality reduction through distance matching.",
                    "label": 1
                },
                {
                    "sent": "Now this is a sort of old idea and it comes from statistics.",
                    "label": 0
                },
                {
                    "sent": "Then I'll briefly sort of talk about when how we get those distances and how we can try and make distances come from along a manifold.",
                    "label": 1
                },
                {
                    "sent": "Then I'll briefly show some stuff for model selection for trying to choose what a good dimensionality reduction.",
                    "label": 0
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we'll end with some conclusions.",
                    "label": 0
                },
                {
                    "sent": "So all the source code and the slides are available online and this talk is also available from my homepage.",
                    "label": 1
                },
                {
                    "sent": "The slides Matlab examples are used.",
                    "label": 1
                },
                {
                    "sent": "I've made a little tool box called the dim red dimensional reduction toolbox that I just put up there last night and any Matlab commands I'm using for examples I've given in typewriter font.",
                    "label": 0
                },
                {
                    "sent": "So I think you should be able to recreate all the diagrams.",
                    "label": 0
                },
                {
                    "sent": "I'm showing you all the results I've showed you.",
                    "label": 0
                },
                {
                    "sent": "I mean I'm not showing you significant results on serious datasets.",
                    "label": 0
                },
                {
                    "sent": "It's all illustrative stuff.",
                    "label": 0
                },
                {
                    "sent": "By downloading that.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Software.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK motivation.",
                    "label": 0
                },
                {
                    "sent": "So this is the sort of motivational slides I tend to use for dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "So machine learning, certainly.",
                    "label": 0
                },
                {
                    "sent": "For.",
                    "label": 0
                },
                {
                    "sent": "We were completely obsessed by handwritten digits, mainly because they were used for zip code recognition in United States.",
                    "label": 0
                },
                {
                    "sent": "And this is a digit 6 from AUS Postal Service data set.",
                    "label": 1
                },
                {
                    "sent": "So it's a handwritten 6 and then this is the raw format.",
                    "label": 0
                },
                {
                    "sent": "These data come in.",
                    "label": 0
                },
                {
                    "sent": "They are typically processed when we use the machine learning, but this is well raw form that you see the data room.",
                    "label": 0
                },
                {
                    "sent": "OK, so this is a 64 rose.",
                    "label": 0
                },
                {
                    "sent": "By 57 columns.",
                    "label": 0
                },
                {
                    "sent": "So there's 3648 dimensions to this.",
                    "label": 1
                },
                {
                    "sent": "Data point now.",
                    "label": 0
                },
                {
                    "sent": "Obviously this space contains a lot more than just this day.",
                    "label": 1
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And if I sample randomly from this space using a probability of pixel being on.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Equal to the number of pixels that are in the thick, so a simple model of.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This is and this is what I see.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Now another sample.",
                    "label": 0
                },
                {
                    "sent": "I still don't see the six.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "That's not surprising, because even if we sampled every nanosecond from now until the end of the universe, you wouldn't see the original six.",
                    "label": 1
                },
                {
                    "sent": "That space is so large it's 2 to 3648 that it's something like there's more possible things can fit in that space than there are particles in the universe.",
                    "label": 0
                },
                {
                    "sent": "That's how big that space is.",
                    "label": 0
                },
                {
                    "sent": "So working in that space is extremely foolhardy.",
                    "label": 0
                },
                {
                    "sent": "It's not a good thing to do because you're trying to do a model.",
                    "label": 0
                },
                {
                    "sent": "With such an enormous space, there's no possible way that you could get a good idea of when these pixels are on and when they're off and what the correlations are, but.",
                    "label": 0
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Wyndham OK, so that wasn't 6 either.",
                    "label": 0
                },
                {
                    "sent": "That was a final sample just to prove that point.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, so what's an alternative model for digits?",
                    "label": 0
                },
                {
                    "sent": "So instead of that model, let's try something else.",
                    "label": 0
                },
                {
                    "sent": "Well, let's say that all sixes are perhaps made up by take.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The prototypes.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And wrote.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Hating it.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_15": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So this is my way.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Covering all six.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now you can run this script here and what you'll get.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What is this result now?",
                    "label": 0
                },
                {
                    "sent": "What am I showing you here?",
                    "label": 0
                },
                {
                    "sent": "So what I've done is I've taken that 6.",
                    "label": 0
                },
                {
                    "sent": "And I've rotated it 360 * 1 degree each to create a data set.",
                    "label": 0
                },
                {
                    "sent": "Yeah, then with that data set I've taken the 1st and 2nd principle components of that data and I'm plotting along those components.",
                    "label": 0
                },
                {
                    "sent": "What you immediately see is the data lives on a circle.",
                    "label": 0
                },
                {
                    "sent": "And I've shown you some examples of the 60s.",
                    "label": 0
                },
                {
                    "sent": "It's the closest point here that you're getting here, so the original 6 is up in the top right here somewhere.",
                    "label": 0
                },
                {
                    "sent": "And as we rotate around the circle, if we rotate in a clockwise way, we actually move coincidentally clockwise around the circle as well.",
                    "label": 0
                },
                {
                    "sent": "That's anti clockwise rotating 6 move clockwise around the circle and we see all the different sixes we get in the data set.",
                    "label": 0
                },
                {
                    "sent": "Now, if you keep looking at the principal components.",
                    "label": 0
                },
                {
                    "sent": "Of this data, you keep seeing that this is a 1 dimensional structure until you start to get to very low numbers of principal components, and when you get to those low numbers of Prince of Components, what you get is artifacts in the rotation algorithm.",
                    "label": 0
                },
                {
                    "sent": "So this is an image rotation so.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "You rotate the pixels.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_22": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Doesn't quite work per.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "We look at these.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Two pixels here, right?",
                    "label": 0
                },
                {
                    "sent": "As you rotate them, they sort of separate.",
                    "label": 0
                },
                {
                    "sent": "Yeah, so there's some sort of interpolation you as you rotate.",
                    "label": 0
                },
                {
                    "sent": "It's not an exact number of degrees, so there's some sort of Inter.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Appalachian so.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At least to some sort of.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Lil bit of noise.",
                    "label": 0
                },
                {
                    "sent": "On this circle, it's particularly visible there.",
                    "label": 0
                },
                {
                    "sent": "And there and there and there.",
                    "label": 0
                },
                {
                    "sent": "So you get this a little noise, which is to do with whatever algorithm the image rotation.",
                    "label": 0
                },
                {
                    "sent": "And this is just an image rotation I downloaded from the web is using to interpolate those pixels.",
                    "label": 0
                },
                {
                    "sent": "OK so but basically you see it's 1 dimensional structure with.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Some noise.",
                    "label": 0
                },
                {
                    "sent": "Now you could argue that those aren't all sixes, and indeed these guys down, here and nines.",
                    "label": 0
                },
                {
                    "sent": "So this is a model of sixes and nines in effects, because it's really not the full rotation.",
                    "label": 0
                },
                {
                    "sent": "That were perhaps interesting, but a part of the rotation.",
                    "label": 0
                },
                {
                    "sent": "But this this slide also has a sort of another point, which is to sort of say that the data of interesting data will also get multiple prototypes, so it may not be just one six when rotating, it may be another six season nine in this case as model A-69, so we have one six that we're rotating another nine in another portion of space, but it's a little bit of a manifold living in this high dimensional space, which is what the data.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Actually lives on.",
                    "label": 0
                },
                {
                    "sent": "Now in practice.",
                    "label": 0
                },
                {
                    "sent": "This is a silly model, right?",
                    "label": 0
                },
                {
                    "sent": "Because of course.",
                    "label": 0
                },
                {
                    "sent": "The data may undergo several distortions, far more than we've shown here.",
                    "label": 0
                },
                {
                    "sent": "For example, digits as well as undergoing rotation, they undergo a process known as thinning, where the stroke of the digit becomes faster and thinner.",
                    "label": 0
                },
                {
                    "sent": "Translation as well, so they move around in that box.",
                    "label": 0
                },
                {
                    "sent": "That's possible too.",
                    "label": 0
                },
                {
                    "sent": "Excuse me, so the point is, for data with structure and this is almost how I think of data with structure.",
                    "label": 0
                },
                {
                    "sent": "How I define data structure there's I mean I'm skipping one aspect here, which is to do with conditional independencies.",
                    "label": 0
                },
                {
                    "sent": "Another way of dealing with high dimensional data is to specify conditional independence is in it, and this isn't that, but the way I tend to think of data with structure is that we expect fewer of these types of distortions than there are dimensions in the original data set.",
                    "label": 0
                },
                {
                    "sent": "And that's the real.",
                    "label": 0
                },
                {
                    "sent": "Inherent dimensionality of our data, not the 3648 pixel space, it's the number of sort of distortions it's going through.",
                    "label": 0
                },
                {
                    "sent": "So that still leaves.",
                    "label": 0
                },
                {
                    "sent": "It is a difficult question though, because it may be.",
                    "label": 0
                },
                {
                    "sent": "For example, consider sevens.",
                    "label": 0
                },
                {
                    "sent": "If we're modeling all sevens, we'd have to have prototype sevens, which have slice across 4 on the other way round, slice across the the upright portion of the 7, and that's going to be a slightly different prototype, and that type of seven may undergo a slightly different set of distortions than the other types of sevens, so within this high dimensional space where we've got these multiple distortions going on.",
                    "label": 0
                },
                {
                    "sent": "It may not be that it's not an explicit dimension, even it may be that different parts of the space have different slightly different dimensionality, but.",
                    "label": 0
                },
                {
                    "sent": "The general conclusion is that we should look to deal with high dimensional data by looking for a lower dimensional nonlinear embedding.",
                    "label": 0
                },
                {
                    "sent": "So that's the basic sort of motivation for why you're interested in doing dimensionality reduction.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_31": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So just before we go on, I want to do a little bit of notation.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "I'm going to be working with.",
                    "label": 0
                },
                {
                    "sent": "A data space where my data matrix is why so?",
                    "label": 0
                },
                {
                    "sent": "This is the observed data.",
                    "label": 0
                },
                {
                    "sent": "The fixes that we've seen the rotated data set of 60.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to be looking at lower dimension and latent or embedded spaces.",
                    "label": 0
                },
                {
                    "sent": "I'll tend to call them latent spaces as an artifact of probabilistic sort of language.",
                    "label": 0
                },
                {
                    "sent": "So well, dimensionality, my latent space is going to be denoted Q.",
                    "label": 0
                },
                {
                    "sent": "Then I mention ality of the data space is going to be demented, denoted big D. So Big D in the six case was 3648.",
                    "label": 0
                },
                {
                    "sent": "And then I'm going to be dealing with N data points where.",
                    "label": 0
                },
                {
                    "sent": "And in the six case was 360 for each degree of rotation.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to draw my data in terms of a data matrix, which is called a design matrix.",
                    "label": 0
                },
                {
                    "sent": "So why has N rows and D columns?",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "And it's made up of each data point is a vector Y from the first row up to the NTH row.",
                    "label": 0
                },
                {
                    "sent": "And this: notation indicates it's a vector, but these should always be filtered as column vectors.",
                    "label": 0
                },
                {
                    "sent": "But note the transpose here, so this matrix here has N rows and columns.",
                    "label": 0
                },
                {
                    "sent": "That's important that this is the other way of writing it, where this is each feature of the data, the first feature up to the feature, and the same with the latent variables.",
                    "label": 0
                },
                {
                    "sent": "Now this way of writing the matrix is important because when we come to write down covariance matrices in.",
                    "label": 0
                },
                {
                    "sent": "Matrix notation.",
                    "label": 0
                },
                {
                    "sent": "It's sort of important that that's the way around that we're using.",
                    "label": 0
                },
                {
                    "sent": "I'm actually not going to really make use of that mapping matrix or the centering matrix, although I thought I might when I was writing this slide.",
                    "label": 0
                },
                {
                    "sent": "OK, so these two are the main sort of important things.",
                    "label": 0
                },
                {
                    "sent": "The latent variables is going to be my representation of that data in the reduced dementia.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Space, so within this examples, these are my latent variables, X one X2.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so just to sort of recap, recapitulate A is a vector from the.",
                    "label": 0
                },
                {
                    "sent": "I throw a ice,: that sort of Matlab notation and a colon, J is a vector from the J.",
                    "label": 0
                },
                {
                    "sent": "Sorry Jeff column, that should say.",
                    "label": 0
                },
                {
                    "sent": "An X&Y are design matrices now.",
                    "label": 0
                },
                {
                    "sent": "Getting all the centering there, but the sample covariance this bits important.",
                    "label": 0
                },
                {
                    "sent": "The sample covariance S is given by one over the number of data points times Y transpose Y. OK, so why hat is a centered version of our data?",
                    "label": 1
                },
                {
                    "sent": "So the centering matrix takes Y and removes its mean.",
                    "label": 0
                },
                {
                    "sent": "That's basically its effect.",
                    "label": 0
                },
                {
                    "sent": "So Yhat is what will typically be working with which is a censored version of our data if yhat is centered.",
                    "label": 0
                },
                {
                    "sent": "I don't know how useful this notation you are.",
                    "label": 0
                },
                {
                    "sent": "You can write down the covariance as 1 /, N, Y hat, transpose, Y hat.",
                    "label": 0
                },
                {
                    "sent": "The centered in a product matrix and this is important to your Orient.",
                    "label": 0
                },
                {
                    "sent": "Think it's been seeing some kernel stuff?",
                    "label": 0
                },
                {
                    "sent": "Is the kernel matrix white hat white hat, transpose?",
                    "label": 0
                },
                {
                    "sent": "So although it's an inner product that appears on the outside, which can slightly confuse people, but this is the standard statistics notation for inner product matrix in the covariance matrix, so that's sort of important message.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "From the notation.",
                    "label": 0
                },
                {
                    "sent": "OK, so how are we going to do dimensionality reduction?",
                    "label": 0
                },
                {
                    "sent": "Well, the way we're going to choose to do it is.",
                    "label": 0
                }
            ]
        },
        "clip_36": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "By distance matching.",
                    "label": 0
                },
                {
                    "sent": "So class is a classical statistical approach.",
                    "label": 0
                },
                {
                    "sent": "So we represent our data via sort of proximities.",
                    "label": 0
                },
                {
                    "sent": "How near they are to each other?",
                    "label": 0
                },
                {
                    "sent": "There's different types of proximity data.",
                    "label": 0
                },
                {
                    "sent": "One type is similarities and the other is dis similarities and one example of a dissimilarity matrix is a distance matrix.",
                    "label": 0
                },
                {
                    "sent": "So we'll just be using mostly.",
                    "label": 0
                },
                {
                    "sent": "The Euclidean distance, so that's the L2 norm of the difference between two vectors.",
                    "label": 0
                },
                {
                    "sent": "So it's simply this.",
                    "label": 0
                },
                {
                    "sent": "My notation for the L2 norm, so these are two data points.",
                    "label": 0
                },
                {
                    "sent": "This is distance between them is denoted ion DJ, J and then this is just the matrix way of expressing it, which is the square root of this vector inner product, where the vectors given from the distance between the subtraction between the two vectors.",
                    "label": 0
                },
                {
                    "sent": "Now for any given data set, I can display that distance.",
                    "label": 0
                }
            ]
        },
        "clip_37": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Matrix.",
                    "label": 0
                },
                {
                    "sent": "And when I display the distance matrix for the 60s data those rotated sixes.",
                    "label": 0
                },
                {
                    "sent": "This is what it looks like.",
                    "label": 0
                },
                {
                    "sent": "So what we see is this is 0 down here.",
                    "label": 0
                },
                {
                    "sent": "This dark blue.",
                    "label": 0
                },
                {
                    "sent": "The red dark red hair is about 8 and a half thousand, but the scale isn't really that important here.",
                    "label": 0
                },
                {
                    "sent": "'cause we could always just rescale things.",
                    "label": 0
                },
                {
                    "sent": "You'll notice immediately is that about 880 degrees rotation, so these are the data point labels.",
                    "label": 0
                },
                {
                    "sent": "But remember, each one is a degree of rotation, so at 180 degrees rotation, the distance between these things is at a maximum, so it dark is read along or vague line.",
                    "label": 0
                },
                {
                    "sent": "Here it's not sharp maximum, but it is at a maximum.",
                    "label": 0
                },
                {
                    "sent": "The distances at a minimum, obviously, where the point is the same, so it's zero along the diagonal, and then the distance increases as the angle increases, up until about 180 degrees.",
                    "label": 0
                },
                {
                    "sent": "Then the distance starts decreasing again until the rotation is complete, and then you can see at the corners you're getting this sort of wrap around structure, so that's the periodic structure that's in the data.",
                    "label": 0
                },
                {
                    "sent": "Now, what else can you see in this thing?",
                    "label": 0
                },
                {
                    "sent": "We can do just about make out the lines along here.",
                    "label": 0
                },
                {
                    "sent": "This is sort of greeting effect.",
                    "label": 0
                },
                {
                    "sent": "On these distances that Gridding effect is because it's associated with the 90 degree rotations.",
                    "label": 0
                },
                {
                    "sent": "So it's slightly different to rotate an image by 90 degrees because the pixels are square, so you can do the rotation exactly and it turns out that in this distance matrix that slightly weird effect actually is visible and you get these squares at 90.",
                    "label": 0
                },
                {
                    "sent": "180 and 270 degrees.",
                    "label": 0
                },
                {
                    "sent": "So that's kind of quite interesting.",
                    "label": 0
                },
                {
                    "sent": "But you can see that sort of finer structure in this distance matrix.",
                    "label": 0
                },
                {
                    "sent": "Now this is the interpoint distances and it's living.",
                    "label": 0
                },
                {
                    "sent": "These distances are living in a.",
                    "label": 0
                },
                {
                    "sent": "3648 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "In fact, to do the rotation, I first of all pad out the size of the images to make them 64 by 64 pixels so they're actually living in a 4009 four 1096 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "Otherwise you can't rotate.",
                    "label": 0
                },
                {
                    "sent": "It's not a square sort of image.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        },
        "clip_38": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So what's the idea of what we're going to do?",
                    "label": 0
                },
                {
                    "sent": "Well, what we're going to do is we're going to do a version of multi dimensional scaling.",
                    "label": 0
                },
                {
                    "sent": "So in multi dimensional scaling the idea is to find a configuration of points X where X is the low dimensional space for which the distance between these points is the Euclidean distance and it closely matches the corresponding distance in the high dimensional space.",
                    "label": 0
                },
                {
                    "sent": "OK, so the idea is.",
                    "label": 0
                },
                {
                    "sent": "We want to find a load of points X.",
                    "label": 0
                },
                {
                    "sent": "Whose interpoint distances match those interpoint distances in Y, but X is going to be in a lower dimension.",
                    "label": 1
                },
                {
                    "sent": "And that's what scaling is about.",
                    "label": 0
                },
                {
                    "sent": "There's different types of scaling and different ways of matching these distances.",
                    "label": 0
                },
                {
                    "sent": "If you do non metric multidimensional scaling your matching the ordering between those distances.",
                    "label": 0
                },
                {
                    "sent": "Metric, metric minded initial scaling is about matching those distances, but perhaps matching a function of those distances to each other.",
                    "label": 0
                },
                {
                    "sent": "We're going to specifically be looking at classical multidimensional scaling, because it turns out to have a simple algorithm behind it, and that algorithm underpins all the dimensional reduction methods.",
                    "label": 0
                },
                {
                    "sent": "They are based on spectral decompositions that you hear about in machine learning, so that includes things like Isomap, locally linear embeddings, maximum variance unfolding, or semidefinite embeddings.",
                    "label": 0
                },
                {
                    "sent": "And Laplacian eigen Maps.",
                    "label": 0
                },
                {
                    "sent": "Now, I'm not going to talk in details about those algorithms, principally because you just have to look at the papers and it describes what the algorithm is, how you put it together.",
                    "label": 0
                },
                {
                    "sent": "And how you run it?",
                    "label": 0
                },
                {
                    "sent": "In fact, it would take you apart from in some vague complexities like you have to do some shortest path algorithms for some of them it would take you very little time to go away, encode those algorithms up.",
                    "label": 0
                },
                {
                    "sent": "So there's little point in me parroting about those.",
                    "label": 0
                },
                {
                    "sent": "But what I want to talk about really, is what's underpinning those algorithms.",
                    "label": 0
                },
                {
                    "sent": "The classical scaling approach, which is what allows you to represent these distances.",
                    "label": 0
                },
                {
                    "sent": "So you can think of those algorithms as living in the same sort of pot.",
                    "label": 0
                },
                {
                    "sent": "So we need an objective function for matching the matrix of distances in the latent space given by that for the distances in the data space given by the Euclidean distance.",
                    "label": 0
                }
            ]
        },
        "clip_39": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In why?",
                    "label": 0
                },
                {
                    "sent": "So here's an objective function.",
                    "label": 1
                },
                {
                    "sent": "It's a slightly funny one.",
                    "label": 0
                },
                {
                    "sent": "And it's it is contrived.",
                    "label": 0
                },
                {
                    "sent": "I mean it's contrived because it leads to the result I want to show you, which is the classical scaling result.",
                    "label": 0
                },
                {
                    "sent": "What we're going to do, though, is before we do the full feature extraction, we're going to consider a simple way of doing dimensional reduction.",
                    "label": 0
                },
                {
                    "sent": "So this is our objective function.",
                    "label": 0
                },
                {
                    "sent": "It's just the sort of absolute value between the squared distances in the data space minus that in the latent space, and the sum of all the absolute values.",
                    "label": 0
                },
                {
                    "sent": "So it's like a sort of entrywise L1 norm on those distance matrices.",
                    "label": 0
                },
                {
                    "sent": "Now, rather than doing what, how are we going to do?",
                    "label": 0
                },
                {
                    "sent": "Our feature selection?",
                    "label": 1
                },
                {
                    "sent": "Well, sort of dimensionality reduction?",
                    "label": 0
                },
                {
                    "sent": "Well, let's think of a really simple, dumb way of doing dimensionality reduction, and it's called feature selection.",
                    "label": 0
                },
                {
                    "sent": "Will just extract dimensions from our original data Y and will make up.",
                    "label": 0
                },
                {
                    "sent": "Are X by extracting columns Y and just putting them into X, so that's just representing our data by a subset of the features in the original data.",
                    "label": 0
                },
                {
                    "sent": "It's a common problem in itself, but it is a specific example of dimensionality reduction.",
                    "label": 0
                },
                {
                    "sent": "So select 4X in, turn the column from Y, but most reduces this error.",
                    "label": 0
                },
                {
                    "sent": "Inserra here until we have the design, cue the desired number of columns in X.",
                    "label": 0
                },
                {
                    "sent": "Now it turns out.",
                    "label": 0
                },
                {
                    "sent": "To minimize this error that should be able X, sorry, we compose X by extracting the columns of Y which have the largest variance.",
                    "label": 0
                },
                {
                    "sent": "Now we're just going to drive that algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_40": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So the squared distance gets rid of the square root nasty square root sign.",
                    "label": 0
                },
                {
                    "sent": "So the squared distance is just the sum across the dimensions of the squared.",
                    "label": 0
                },
                {
                    "sent": "Of the difference between the two data points, yeah, so these are scalars.",
                    "label": 0
                },
                {
                    "sent": "Now this is the ice data points K feature, and it's just the sum of the squares for that.",
                    "label": 0
                },
                {
                    "sent": "Now what we can do is we can always reorder the columns of Y without affecting the distances so I can just move these columns of why around without changing the distances, so will choose an ordering of Y.",
                    "label": 1
                },
                {
                    "sent": "To help us in algorithm, so we'll say.",
                    "label": 0
                },
                {
                    "sent": "But the first Q columns Y are those that will best represent the distance matrix by definition.",
                    "label": 0
                },
                {
                    "sent": "'cause I can reorder without changing.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to say that the first Q columns are.",
                    "label": 0
                },
                {
                    "sent": "Why are those that I'm going to use for X?",
                    "label": 0
                },
                {
                    "sent": "Now what we can do is we can substitute X for Y for the first case, so an hour X is Y for K = 1 to Q because of that ordering.",
                    "label": 0
                },
                {
                    "sent": "This means that we can replace the distance in latent space, which was this Delta squared is equal to X -- X JXI minus XJ for the K feature with the relevant wise for these first Q values.",
                    "label": 0
                },
                {
                    "sent": "OK so just to sort of.",
                    "label": 0
                },
                {
                    "sent": "The advantage of ordering is notational purely notational.",
                    "label": 0
                },
                {
                    "sent": "If I wasn't to do the ordering, so the ordering ordering like this means that I can rewrite this as K = 1 to Q / Y if I wasn't to define that ordering.",
                    "label": 0
                },
                {
                    "sent": "If it was like the last and then the 1st and then the middle one, writing this sum would be quite awkward.",
                    "label": 0
                },
                {
                    "sent": "So it's just simply notational so that I can write this sum equal to this OK. And I can do it without loss of generality, so that's the sort of nice thing about it.",
                    "label": 0
                },
                {
                    "sent": "OK, so our objective look.",
                    "label": 0
                }
            ]
        },
        "clip_41": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Like this, but because.",
                    "label": 0
                },
                {
                    "sent": "We can rewrite the Deltas as some over K = 1 to Q and the DS is the sum of a K = 1 to DD necessarily being larger than Q.",
                    "label": 0
                },
                {
                    "sent": "We can basically rewrite our objective as being K = Q + 1 to D, so the remaining dimensions of Y, basically the sum.",
                    "label": 0
                },
                {
                    "sent": "Of the interpoint distances for the dimensions of why we've left out, yeah.",
                    "label": 0
                },
                {
                    "sent": "We can then introduce the mean of each dimension and then put it inside these brackets here.",
                    "label": 0
                },
                {
                    "sent": "So I've introduced the mean twice so I can do that.",
                    "label": 0
                },
                {
                    "sent": "It's just a convenience thing so that you can see that these the form of this.",
                    "label": 0
                },
                {
                    "sent": "By introducing the mean into here like that and then multiplying out.",
                    "label": 0
                },
                {
                    "sent": "So that's some sorry it should be across all these bracketed terms here.",
                    "label": 0
                },
                {
                    "sent": "That's not very clear.",
                    "label": 0
                },
                {
                    "sent": "Should be across all those bracketed terms.",
                    "label": 0
                },
                {
                    "sent": "Once I'm.",
                    "label": 0
                }
            ]
        },
        "clip_42": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Let's play it out like that.",
                    "label": 0
                },
                {
                    "sent": "I can also bring in some of these sums inside, so that's across all the bracketed terms.",
                    "label": 0
                },
                {
                    "sent": "But here I'm bringing the sums inside, so this sum applies not to hear, but does apply to here, but it needs to have a factor of N appear outside this guy.",
                    "label": 0
                },
                {
                    "sent": "So if I if I bring the sum over eyes over Jays in, it doesn't apply to this I hear, but it multiplies, I buy N. So as I bring it past this I. I get a factor of N out the front and then I get a sum over J appearing in front of this one.",
                    "label": 0
                },
                {
                    "sent": "In this case.",
                    "label": 0
                },
                {
                    "sent": "Here I get a sum over J there and then the sum over I drops in inside here because there's no eyes in this outer term here.",
                    "label": 0
                },
                {
                    "sent": "Similarly, I get a factor of N appearing outside that guy.",
                    "label": 0
                },
                {
                    "sent": "Hello excuse me.",
                    "label": 0
                },
                {
                    "sent": "What you should notice though is this looks very similar to the variance.",
                    "label": 0
                },
                {
                    "sent": "In fact, it's N times the variance of that column and that's N times the variance of the J column.",
                    "label": 0
                },
                {
                    "sent": "So that sent.",
                    "label": 0
                },
                {
                    "sent": "I'm sorry the case column and that end times the variance of The Cave column 2.",
                    "label": 0
                },
                {
                    "sent": "So these guys, though, since this is the mean.",
                    "label": 0
                },
                {
                    "sent": "This is just the sum over all the values.",
                    "label": 0
                },
                {
                    "sent": "Both of these guys just go to zero because they end up with just being the mean minus the mean.",
                    "label": 0
                },
                {
                    "sent": "So we basically recognize the.",
                    "label": 0
                },
                {
                    "sent": "The remaining variance as the sum.",
                    "label": 0
                },
                {
                    "sent": "This is the variance of the case column.",
                    "label": 0
                },
                {
                    "sent": "This is the variance of the column.",
                    "label": 0
                },
                {
                    "sent": "So get two.",
                    "label": 0
                },
                {
                    "sent": "Sorry this is N times the variance of the column is N times the variance of case column.",
                    "label": 0
                },
                {
                    "sent": "So we basically get the result that the error is equal to 2 * N ^2 times the sum of the variances of discarded columns.",
                    "label": 0
                },
                {
                    "sent": "So these are discarded columns.",
                    "label": 0
                },
                {
                    "sent": "So we want to minimize this variance.",
                    "label": 0
                },
                {
                    "sent": "So if you want to minimize this error.",
                    "label": 0
                },
                {
                    "sent": "We should minimize the variance of the columns we're discarding, yeah?",
                    "label": 0
                },
                {
                    "sent": "Does that make sense?",
                    "label": 0
                },
                {
                    "sent": "Geometrically, it makes a lot of sense, and there'll be a little plot to show that later.",
                    "label": 0
                },
                {
                    "sent": "So basically this little simple proof is saying if we're doing feature extraction in such a way that we're trying to throw away features from our data in such a way that we're trying to preserve this distance matrix the way we should do that is discard directions of least variants, or retain directions of most variants, OK?",
                    "label": 0
                }
            ]
        },
        "clip_43": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_44": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So how does that work in practice?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "We like to do 2 dimensional.",
                    "label": 0
                },
                {
                    "sent": "Representations of our data.",
                    "label": 0
                },
                {
                    "sent": "While we like to do 2 dimensional representations of our data because we can show them on slides.",
                    "label": 0
                },
                {
                    "sent": "So if we did a 2 dimensional representation of our data, so we retain the two largest variance columns from this data and then we look at the distance matrix.",
                    "label": 0
                },
                {
                    "sent": "Our representation of the data.",
                    "label": 0
                },
                {
                    "sent": "That's what we see.",
                    "label": 0
                },
                {
                    "sent": "So it's true.",
                    "label": 0
                }
            ]
        },
        "clip_45": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Going to represent that.",
                    "label": 0
                }
            ]
        },
        "clip_46": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So it's not doing a particularly good job.",
                    "label": 0
                },
                {
                    "sent": "I mean, you can see it's it's getting this diagonal dominance a little bit, but not really because it's sort of flattening all the way across there.",
                    "label": 0
                },
                {
                    "sent": "If we go to 10 dimensions, sort of things start to improve a bit.",
                    "label": 0
                },
                {
                    "sent": "But still, it's not clear that the maximum is really at 180.",
                    "label": 0
                },
                {
                    "sent": "In fact, this one's got a bit of a minimum quite close to 180, so 10 dimensions isn't real.",
                    "label": 0
                }
            ]
        },
        "clip_47": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Helping us.",
                    "label": 0
                },
                {
                    "sent": "We go to 100 dimensions.",
                    "label": 0
                },
                {
                    "sent": "So now you can start seeing perhaps this minima at about 180.",
                    "label": 0
                },
                {
                    "sent": "You don't see this structure though.",
                    "label": 0
                },
                {
                    "sent": "This gridding structure particularly well of the rotation of each 90 degrees, so some of the finer structure isn't really there.",
                    "label": 0
                },
                {
                    "sent": "If you go to 1000 dimensions.",
                    "label": 0
                },
                {
                    "sent": "So we saw it off with a 4096 dimensional data set and we've got rid of 3/4 of dimensions.",
                    "label": 0
                },
                {
                    "sent": "Isn't actually a particularly great dimension reduction.",
                    "label": 0
                },
                {
                    "sent": "Now you really start to see the structure of the thing, so you need about 1000 dimensions if you're going to do feature selection to extract these guys and maintain, retain the quality of this interpoint distance matrix.",
                    "label": 0
                },
                {
                    "sent": "So in some sense you could say it's not a great algorithm, but there you go.",
                    "label": 0
                }
            ]
        },
        "clip_48": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, So what are we basically doing here?",
                    "label": 0
                },
                {
                    "sent": "Well, this is now a high dimensional data set.",
                    "label": 0
                },
                {
                    "sent": "It's got 2 dimensions in it and what we're going to do is we're going to try and reduce it to 1 dimensional data set so I can show you how that reduction is taking place.",
                    "label": 0
                },
                {
                    "sent": "So what this variance maximum variance is saying is that we should look for the direction of maximum variance and it along this direction.",
                    "label": 0
                },
                {
                    "sent": "It's variance looks like standard deviation looks to be about 6, so variants around 36 around this direction.",
                    "label": 0
                },
                {
                    "sent": "It looks to be more like a sort of four.",
                    "label": 0
                },
                {
                    "sent": "Perhaps standardization of 16.",
                    "label": 0
                },
                {
                    "sent": "So here we should be retaining this direction and discarding this direction according to the algorithm we've just described.",
                    "label": 0
                }
            ]
        },
        "clip_49": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what does that involve doing that involves projecting all these data points onto the X axis and replace?",
                    "label": 0
                }
            ]
        },
        "clip_50": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Our original date set with.",
                    "label": 0
                }
            ]
        },
        "clip_51": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "At.",
                    "label": 0
                },
                {
                    "sent": "So look at all the structure we're throwing away there.",
                    "label": 0
                }
            ]
        },
        "clip_52": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Between all these.",
                    "label": 0
                }
            ]
        },
        "clip_53": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Points immediately, it's great isn't.",
                    "label": 0
                }
            ]
        },
        "clip_54": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's the algorithm.",
                    "label": 0
                }
            ]
        },
        "clip_55": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "However, there is a bit of a more sensible algorithm that is very obvious if you think about one thing.",
                    "label": 0
                },
                {
                    "sent": "Rotation.",
                    "label": 0
                },
                {
                    "sent": "Preserves interpoint distances I can rotate.",
                    "label": 0
                },
                {
                    "sent": "This data set.",
                    "label": 0
                },
                {
                    "sent": "And all the interpoint distances stay the same.",
                    "label": 0
                },
                {
                    "sent": "So if I make a rotation.",
                    "label": 0
                },
                {
                    "sent": "I'm doing nothing to lose any.",
                    "label": 0
                },
                {
                    "sent": "I can basically make any rotation I like.",
                    "label": 0
                },
                {
                    "sent": "So what should I do?",
                    "label": 0
                },
                {
                    "sent": "Well, it seems.",
                    "label": 0
                }
            ]
        },
        "clip_56": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Makes sense to try and rotate the data.",
                    "label": 0
                }
            ]
        },
        "clip_57": {
            "is_summarization_sample": false,
            "summarization_data": []
        },
        "clip_58": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Such that when I project.",
                    "label": 0
                }
            ]
        },
        "clip_59": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Onto the X axis.",
                    "label": 0
                },
                {
                    "sent": "I'm losing a lot less information, so look at the size of these projections here.",
                    "label": 0
                },
                {
                    "sent": "Versus",
                    "label": 0
                }
            ]
        },
        "clip_60": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Size of the projections here.",
                    "label": 0
                },
                {
                    "sent": "And actually in this example they are quite strongly correlated, but you could envisage examples where it was even through it.",
                    "label": 0
                },
                {
                    "sent": "Well, you've got sort of less variance in that direction, and more and sort of stronger correlations.",
                    "label": 0
                },
                {
                    "sent": "And.",
                    "label": 0
                }
            ]
        },
        "clip_61": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This becomes even true.",
                    "label": 0
                }
            ]
        },
        "clip_62": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "In fact, it's.",
                    "label": 0
                }
            ]
        },
        "clip_63": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Very true for the day.",
                    "label": 0
                }
            ]
        },
        "clip_64": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Getdata, the art.",
                    "label": 0
                }
            ]
        },
        "clip_65": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Digit data we're using.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                }
            ]
        },
        "clip_66": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Sort of improved idea is to do this rotation first.",
                    "label": 0
                },
                {
                    "sent": "Yep, so we rotate.",
                    "label": 0
                },
                {
                    "sent": "To align our data so that it's variance, but our actions are maximum variance are aligned with the axis.",
                    "label": 0
                }
            ]
        },
        "clip_67": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And then we extract these interpoint distances.",
                    "label": 1
                },
                {
                    "sent": "OK, so we need the rotation that will minimize the residual error.",
                    "label": 0
                },
                {
                    "sent": "Now we already derived an algorithm for discarding directions.",
                    "label": 0
                },
                {
                    "sent": "And it said that we should discard the direction with the maximum variance.",
                    "label": 0
                },
                {
                    "sent": "The error is then given by the sum of residual variances.",
                    "label": 0
                },
                {
                    "sent": "Rotations of the data matrix do not affect this analysis at all, because those distances all.",
                    "label": 0
                }
            ]
        },
        "clip_68": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Remain the same.",
                    "label": 0
                }
            ]
        },
        "clip_69": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So what this means is we should rotate to find the directions of maximum variance or so rotate so that the maximum variance directions are aligned with the axis, and then discard the lower there.",
                    "label": 0
                }
            ]
        },
        "clip_70": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Directions.",
                    "label": 0
                },
                {
                    "sent": "Now you might recognize that algorithm and we'll come back to that in a moment.",
                    "label": 0
                },
                {
                    "sent": "But if you do that, this is what you get.",
                    "label": 0
                },
                {
                    "sent": "So on the left this is distances reconstructed from 2 dimensions of the data set.",
                    "label": 0
                },
                {
                    "sent": "Remember the two dimensional example before.",
                    "label": 0
                }
            ]
        },
        "clip_71": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Oh there.",
                    "label": 0
                },
                {
                    "sent": "Now.",
                    "label": 0
                },
                {
                    "sent": "Are two dimensional.",
                    "label": 0
                }
            ]
        },
        "clip_72": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Example is already showing.",
                    "label": 0
                },
                {
                    "sent": "But we've got a sort of minima or 180.",
                    "label": 0
                },
                {
                    "sent": "It took 100 dimensions before we saw that structure in the example where we're just doing feature selection.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "This is distances reconstructed with 10 dimensions.",
                    "label": 1
                },
                {
                    "sent": "Now you're seeing the fine structure in the grids with only 10 dimensions.",
                    "label": 0
                },
                {
                    "sent": "It took 1000 dimensions to see that before.",
                    "label": 0
                }
            ]
        },
        "clip_73": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If we go to 100 dimensions.",
                    "label": 0
                },
                {
                    "sent": "Well, again, we're not really gaining much in terms of structure.",
                    "label": 0
                },
                {
                    "sent": "It's just improving the.",
                    "label": 0
                }
            ]
        },
        "clip_74": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The width of this guy, which is a little bit too wide is very wide.",
                    "label": 0
                }
            ]
        },
        "clip_75": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here too right here it's getting smaller.",
                    "label": 0
                },
                {
                    "sent": "And then if you go to 360 dimensions then that's actually all the structure is.",
                    "label": 0
                },
                {
                    "sent": "The Matrix rank is only 360, so this is all the structure in the data.",
                    "label": 0
                },
                {
                    "sent": "So you basically just refining the width, the sort of the shape of this Valley to make it sort of narrower narrower Ridge in there as you increase the dimensions.",
                    "label": 0
                },
                {
                    "sent": "So you're getting a lot of the structure of the data with only two dimensions or 10 dimensions of the data, so it's a very good approach that I mentioned.",
                    "label": 0
                },
                {
                    "sent": "Duction",
                    "label": 0
                }
            ]
        },
        "clip_76": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "If you believe in distance matrix presentation.",
                    "label": 0
                },
                {
                    "sent": "OK, so how do we find these directions where we find directions in the data with maximal variance, but wait a second?",
                    "label": 0
                },
                {
                    "sent": "That's what principle component analysis does.",
                    "label": 0
                },
                {
                    "sent": "So there's already an algorithm for doing this for us principal component analysis set.",
                    "label": 0
                },
                {
                    "sent": "Look for directions of maximum variance in the data, so PCA.",
                    "label": 0
                },
                {
                    "sent": "Rotates the data to extract these directions and it works on the sample covariance matrix here.",
                    "label": 0
                },
                {
                    "sent": "So let's just do a bit of a review of.",
                    "label": 0
                }
            ]
        },
        "clip_77": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Principle component analysis.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So PCA says find a direction in the data R, so RI think it was one vector from a rotation matrix.",
                    "label": 0
                },
                {
                    "sent": "So you can rotate your whole data and R is like taking one of about those rotations.",
                    "label": 0
                },
                {
                    "sent": "So you're projecting down to one dimension, but it's from that larger rotation matrix, so it's the first principle component is, well, actually one.",
                    "label": 0
                },
                {
                    "sent": "It will be turned out to be.",
                    "label": 0
                },
                {
                    "sent": "So which project onto that X we've got yhat times R, so this is the nice compactness of this notation.",
                    "label": 0
                },
                {
                    "sent": "This is the center data matrix and then this is 1 vector of rotation being applied to.",
                    "label": 0
                },
                {
                    "sent": "It gives us a 1 dimensional solution.",
                    "label": 0
                },
                {
                    "sent": "What we want to do is maximize that variance so we look to find our one which maximizes that subject to the fact that it's normal length.",
                    "label": 0
                },
                {
                    "sent": "We can always maximize that by scaling our forever and ever, but we want our to be part of rotation matrix, so we scale it to norm to the unit norm.",
                    "label": 0
                },
                {
                    "sent": "We write down the variance of X.",
                    "label": 0
                },
                {
                    "sent": "We can re express it as the product of these two guides because they're centered, so X will be centered as well.",
                    "label": 0
                },
                {
                    "sent": "1 / N times the product of Y.",
                    "label": 0
                },
                {
                    "sent": "That are one transpose Y R1.",
                    "label": 0
                },
                {
                    "sent": "Turning that around so that we just flip that guy because of the transpose, we can bring the R1 outside so it's R1, transpose times 1 / N Y hat, transpose Y hat.",
                    "label": 0
                },
                {
                    "sent": "Times are one.",
                    "label": 0
                },
                {
                    "sent": "Now we recognize that as the sample covariance.",
                    "label": 0
                },
                {
                    "sent": "From our definition of annotation earlier.",
                    "label": 0
                },
                {
                    "sent": "So it's basically saying that the variance in the X One Direction can be written by ZAR, one transpose S, R1 through the well known for PCA result.",
                    "label": 0
                },
                {
                    "sent": "So that's what we want to maximize.",
                    "label": 0
                },
                {
                    "sent": "That's this guy.",
                    "label": 0
                }
            ]
        },
        "clip_78": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Basically.",
                    "label": 0
                },
                {
                    "sent": "Do you ask questions?",
                    "label": 0
                },
                {
                    "sent": "If you're coughing, reminded me of questions.",
                    "label": 0
                },
                {
                    "sent": "If you've got any questions, do ask.",
                    "label": 0
                },
                {
                    "sent": "I can still hear while I'm coughing, so that's a good time to ask a question.",
                    "label": 0
                },
                {
                    "sent": "So how do we look for this R1?",
                    "label": 1
                },
                {
                    "sent": "Well, it's the solution to a constrained optimization.",
                    "label": 0
                },
                {
                    "sent": "What we do is we use a LaGrange multiplier to apply this constraint so the LaGrange multiplier Lambda one 1 -- R one, transpose, R1.",
                    "label": 0
                },
                {
                    "sent": "Taking the gradient with respect to R1.",
                    "label": 0
                },
                {
                    "sent": "Leads to do vector gradients for 2 Sr 1 -- 2 Lambda one R1.",
                    "label": 0
                },
                {
                    "sent": "Now, if we rearrange that, we find that the sample covariance times R1 is equal to Lambda one R1, which is recognized as an eigenvalue problem.",
                    "label": 1
                },
                {
                    "sent": "So this basically the first guy is the solution to an eigenvalue problem.",
                    "label": 0
                },
                {
                    "sent": "But which eigenvalue is it so?",
                    "label": 0
                },
                {
                    "sent": "An eigenvalue problem that there's lots of ours that solve this.",
                    "label": 0
                },
                {
                    "sent": "There are all the eigenvectors of matrix X.",
                    "label": 0
                },
                {
                    "sent": "Which are.",
                    "label": 0
                }
            ]
        },
        "clip_79": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is it that we need what we can find that out by?",
                    "label": 0
                },
                {
                    "sent": "Premultiplying looking at the gradient we pre multiply the gradient by R1 transpose.",
                    "label": 0
                },
                {
                    "sent": "We get 2R1 transpose Sr 1 -- 2 Lambda One art one transpose R1 but are one transpose R1 we've constrained to be one, so it's 2R1 transpose Sr 1 -- 2 Lambda one.",
                    "label": 0
                },
                {
                    "sent": "Now, since the gradients must be 0 at the solu.",
                    "label": 0
                }
            ]
        },
        "clip_80": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Ocean.",
                    "label": 0
                },
                {
                    "sent": "Which is also of course how we were finding this looking for fixed point.",
                    "label": 0
                }
            ]
        },
        "clip_81": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Setting the gradient equal to 0.",
                    "label": 1
                },
                {
                    "sent": "We can reorganize to find that Lambda one must necessarily equal this.",
                    "label": 0
                },
                {
                    "sent": "But",
                    "label": 0
                }
            ]
        },
        "clip_82": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "What did we say that was?",
                    "label": 0
                },
                {
                    "sent": "While we've already said that that's the variance of X1?",
                    "label": 0
                }
            ]
        },
        "clip_83": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So Lambda one.",
                    "label": 0
                }
            ]
        },
        "clip_84": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Is equal to that, but Lambda one is also the eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "So here we can solve for Lambda one and R1 simultaneous.",
                    "label": 0
                }
            ]
        },
        "clip_85": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I'm the one is the eigenvalue an that eigenvalue is the variance of the data in the X space.",
                    "label": 0
                },
                {
                    "sent": "So which eigenvalue do we need to extract to get the largest variance, the largest eigenvalue?",
                    "label": 0
                },
                {
                    "sent": "So maximum variance is therefore necessary.",
                    "label": 1
                },
                {
                    "sent": "The maximum eigenvalue of S and that's called the first principle.",
                    "label": 1
                }
            ]
        },
        "clip_86": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Opponent.",
                    "label": 0
                },
                {
                    "sent": "So what's next?",
                    "label": 0
                },
                {
                    "sent": "Well, the next thing to do is to find the other principle components, and those are basically orthogonal directions to the earlier extracted directions with maximal variance.",
                    "label": 0
                },
                {
                    "sent": "So there's now an additional constraint, North Orginality constraint that for J is less than K. RJ transpose RK is must be set to 0, so that's what means that these vectors are at right angles.",
                    "label": 0
                },
                {
                    "sent": "So for all, so for extracting the case direction.",
                    "label": 0
                },
                {
                    "sent": "Now RK, it must be at right angles to all the previous directions.",
                    "label": 0
                },
                {
                    "sent": "So we've extracted the 1st so K can be 2K can be 3K can before.",
                    "label": 0
                },
                {
                    "sent": "Furthermore, we must have again that it's.",
                    "label": 0
                },
                {
                    "sent": "Normal length itself, so RK transpose RK is equal to 1.",
                    "label": 0
                },
                {
                    "sent": "The Lagrangian now is a little bit more complicated 'cause it's got this additional constraints coming in.",
                    "label": 0
                },
                {
                    "sent": "Child represented the ground rules apply gamma.",
                    "label": 0
                },
                {
                    "sent": "But once again, we can derive the gradient.",
                    "label": 0
                },
                {
                    "sent": "Now the gradient now turns out the same form for the first 2 terms, and then the third term is just the sum of all the RJS times there gamma JS.",
                    "label": 0
                },
                {
                    "sent": "'cause all cap is in each one of those that's from J = 1 to K -- 1.",
                    "label": 0
                },
                {
                    "sent": "So all all.",
                    "label": 0
                }
            ]
        },
        "clip_87": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "The directions we've extracted previously.",
                    "label": 0
                },
                {
                    "sent": "Now that gradient.",
                    "label": 0
                },
                {
                    "sent": "Looks like that premultiplying by R. Ajay Premultiplying by RJ Zeros this guy.",
                    "label": 0
                },
                {
                    "sent": "Sorry.",
                    "label": 0
                },
                {
                    "sent": "And this guy.",
                    "label": 0
                },
                {
                    "sent": "And basically tells us that the gamma eyes end up being 0.",
                    "label": 0
                },
                {
                    "sent": "So that means we can move, remove this term and we can write down the covariance as this eigenvalue problem here.",
                    "label": 0
                },
                {
                    "sent": "Pre multiplying by RK implies that Lambda K again, so the variance that eigenvalue the case eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "Is equal to RK, transpose SKRK, so each eigenvalue is still the variance in the X space.",
                    "label": 0
                },
                {
                    "sent": "So we basically have that we can extract all the principal components of the data using an eigenvalue problem where the largest the largest eigenvalues are associated with the largest variance in the latent space we're creating.",
                    "label": 0
                },
                {
                    "sent": "So we want to retain the largest eigenvalues and that gives us a good visualization of our data.",
                    "label": 0
                }
            ]
        },
        "clip_88": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So that's known as classical multi dimensional scaling.",
                    "label": 0
                },
                {
                    "sent": "It's known as principle coordinate analysis as well.",
                    "label": 0
                },
                {
                    "sent": "But it's also principal component analysis, so principle component analysis really sits at the core of so many dimensionality reduction algorithms.",
                    "label": 0
                },
                {
                    "sent": "This is one reason and given time, which I think will have.",
                    "label": 0
                },
                {
                    "sent": "Will see other reasons why principle component analysis is core.",
                    "label": 0
                },
                {
                    "sent": "Given different interpretations of principle component analysis.",
                    "label": 0
                },
                {
                    "sent": "So the variance by looking for directions of maximum variance in the data.",
                    "label": 0
                },
                {
                    "sent": "We're also looking to retain the most distant.",
                    "label": 0
                },
                {
                    "sent": "That makes sense because by finding directions the maximum variance the potential distance in a large variance direction is much greater than the potential distance in a small variance direction.",
                    "label": 0
                },
                {
                    "sent": "So it makes complete sense.",
                    "label": 0
                },
                {
                    "sent": "But do remember that rather artificial objective function we started with.",
                    "label": 0
                }
            ]
        },
        "clip_89": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "All this time ago, if I were to tell you.",
                    "label": 0
                }
            ]
        },
        "clip_90": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I wanted to match distances.",
                    "label": 0
                },
                {
                    "sent": "Between a reconstruction of a distance annadata space based distance, that's probably not the first distance function.",
                    "label": 0
                },
                {
                    "sent": "You would objective function.",
                    "label": 0
                },
                {
                    "sent": "You would write down with the square of the distance is, and then the sort of absolute value of the distances either side of it.",
                    "label": 0
                },
                {
                    "sent": "It's a slightly artificial objective function.",
                    "label": 0
                },
                {
                    "sent": "Of course, if it's satisfied with value 0, then the distances match exactly as you would hope, so it's still a reasonable objective function, But it's perhaps not the first objective function you would right out of the box.",
                    "label": 0
                },
                {
                    "sent": "You might write a squared matching.",
                    "label": 0
                },
                {
                    "sent": "You might write dij minus Delta, IJ all squared.",
                    "label": 0
                },
                {
                    "sent": "Something like that might be the first thing right?",
                    "label": 0
                },
                {
                    "sent": "And in fact people do use those in things like metric, metric, multidimensional scaling.",
                    "label": 0
                },
                {
                    "sent": "There are there are other error measures that I use, but they don't lead to eigenvalue problems.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "So that seems fine.",
                    "label": 0
                },
                {
                    "sent": "I've spent an awful lot of time telling you about principal component.",
                    "label": 0
                }
            ]
        },
        "clip_91": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "OK, but there's some subtleties.",
                    "label": 0
                },
                {
                    "sent": "So the rotation which finds the directions of maximum variance is the eigenvectors of the covariance matrix.",
                    "label": 0
                },
                {
                    "sent": "The variance in each direction is given by the eigenvalues, but there's a problem.",
                    "label": 0
                },
                {
                    "sent": "We've talked about the sample covariance matrix, but working with the sample covariance matrix may be impossible because we may be given the relationships between our data in terms of a distance matrix in the 1st place and the relationship between a distance matrix and a sample covariance matrix is not obvious.",
                    "label": 0
                },
                {
                    "sent": "How do you get so say, I say?",
                    "label": 0
                },
                {
                    "sent": "An example we're going to use in a bit is distances between towns between cities in Europe.",
                    "label": 0
                },
                {
                    "sent": "So if I give you the road distances between a bunch of cities, you might expect you can write down a map of how those cities look on a map, how how far apart they are.",
                    "label": 0
                },
                {
                    "sent": "If you know all the distances between them, their relative positions, that's a reasonable thing to be able to expect to do, and in fact, that's what classical scaling is allowing you to do to match those distances by putting them in a 2 dimensional space.",
                    "label": 0
                },
                {
                    "sent": "That's exactly what you want to be doing.",
                    "label": 0
                },
                {
                    "sent": "But I haven't given you the sample covariance of the location of the cities.",
                    "label": 0
                },
                {
                    "sent": "I haven't given you the absolute location of these cities.",
                    "label": 0
                },
                {
                    "sent": "If I did, you could just put them in two dimensions.",
                    "label": 0
                },
                {
                    "sent": "I'm not giving you that.",
                    "label": 0
                },
                {
                    "sent": "I'm just giving the distances between them.",
                    "label": 0
                },
                {
                    "sent": "So how do you perform PCA on this?",
                    "label": 0
                },
                {
                    "sent": "Seems a bit strange.",
                    "label": 0
                },
                {
                    "sent": "If you don't have access to those absolute positions, we cannot compute the original sample covariance, so we can't just do PCA.",
                    "label": 0
                },
                {
                    "sent": "So principle coordinate analysis or classical scaling is different from PCA, because it allows us to deal with this problem in some sense.",
                    "label": 0
                }
            ]
        },
        "clip_92": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, well I want to sort of just look at the eigenvalue problems in a matrix form.",
                    "label": 0
                },
                {
                    "sent": "So this is the matrix representation of the eigenvalue problem, where I've dropped the 1 / N scaling on the sample covariance.",
                    "label": 0
                },
                {
                    "sent": "This is basically the sample covariance.",
                    "label": 0
                },
                {
                    "sent": "Without that scaling, those scalings don't really significantly affect the eigenvalues, they just scale them.",
                    "label": 0
                },
                {
                    "sent": "So here's the sample covariance.",
                    "label": 0
                },
                {
                    "sent": "There's the rotation matrix, which is rotating down to Q dimensions, so this is a D by Q matrix to rotate down to Q dimensions and then here is it on the other side.",
                    "label": 0
                },
                {
                    "sent": "And this is the diagonal matrix of the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So this is the standard matrix representation and value problem.",
                    "label": 0
                },
                {
                    "sent": "So what I'm going to do with that?",
                    "label": 0
                },
                {
                    "sent": "I'm going to pre multiply that by Y hat.",
                    "label": 0
                },
                {
                    "sent": "So Y hat Y hat transpose Y had RQ is equal to Y, hat RQ Lambda Q.",
                    "label": 0
                },
                {
                    "sent": "Now I'm going to post multiply by the vector of eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "The diagonal vector of eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "This is a Q by Q vector.",
                    "label": 0
                },
                {
                    "sent": "So this is one over the square root of that guy.",
                    "label": 0
                },
                {
                    "sent": "So I post multiply here.",
                    "label": 0
                },
                {
                    "sent": "He arrives on that side and then I post multiply on this side here.",
                    "label": 0
                }
            ]
        },
        "clip_93": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now I'm just going to do a little flip.",
                    "label": 0
                },
                {
                    "sent": "Bring that guy inside.",
                    "label": 0
                },
                {
                    "sent": "I can always do that because I mean this is basically just the square root of both of these.",
                    "label": 0
                },
                {
                    "sent": "Things are just the square.",
                    "label": 0
                }
            ]
        },
        "clip_94": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Out of.",
                    "label": 0
                }
            ]
        },
        "clip_95": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Of the eigenvalues.",
                    "label": 0
                },
                {
                    "sent": "So why have I done that?",
                    "label": 0
                },
                {
                    "sent": "Well, now you see, I've got the.",
                    "label": 0
                }
            ]
        },
        "clip_96": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Same for me, decide, and then I'm going to make this definition.",
                    "label": 0
                }
            ]
        },
        "clip_97": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "I'm going to find that to be you Q.",
                    "label": 0
                }
            ]
        },
        "clip_98": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So by defining that as you Q. I have this now.",
                    "label": 0
                },
                {
                    "sent": "What do we notice?",
                    "label": 0
                },
                {
                    "sent": "Well, this is the censored inner product matrix.",
                    "label": 0
                },
                {
                    "sent": "This.",
                    "label": 0
                },
                {
                    "sent": "Looks like an eigenvalue problem on the centered inner product matrix.",
                    "label": 0
                },
                {
                    "sent": "These are definitely a diagonal matrix of Lambda cues, but is UQ the right matrix to diagonalize yhat yhat transpose?",
                    "label": 0
                },
                {
                    "sent": "Well?",
                    "label": 0
                },
                {
                    "sent": "I suppose by definition it must be, but we can actually just show that it is actually.",
                    "label": 0
                }
            ]
        },
        "clip_99": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "A set of eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "How are we going to do that?",
                    "label": 0
                },
                {
                    "sent": "Well, we're going to prove that you Q are the eigenvectors of this inner product matrix.",
                    "label": 0
                },
                {
                    "sent": "The way we're going to do that is we're going to diagonalize the inner product matrix with you Q.",
                    "label": 0
                },
                {
                    "sent": "So by Premultiplying Biuku transposon post, multiplying by UQI should be able to diagonalize this matrix.",
                    "label": 0
                },
                {
                    "sent": "So just substituting in the form for you Q.",
                    "label": 0
                }
            ]
        },
        "clip_100": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Here we get this rather a long and involved expression.",
                    "label": 0
                },
                {
                    "sent": "But we notice in the center here.",
                    "label": 0
                },
                {
                    "sent": "That's just why transpose Y hat transpose Y hat Y had transposed yhat squared.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "So we replaced with that.",
                    "label": 0
                }
            ]
        },
        "clip_101": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Now we use the fully eigen decomposition of the sample covariance.",
                    "label": 0
                },
                {
                    "sent": "So this is not just stopping with acute eigenvalue.",
                    "label": 0
                },
                {
                    "sent": "This is taking the whole decomposition.",
                    "label": 0
                },
                {
                    "sent": "So this is an exact relationship is not a reduced rank relationship.",
                    "label": 0
                },
                {
                    "sent": "So I can write this.",
                    "label": 0
                }
            ]
        },
        "clip_102": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "And substituted in there.",
                    "label": 0
                },
                {
                    "sent": "But I can also use the fact that the square of these is R, Lambda, R, transpose, R, Lambda, R, transpose is just our Lambda squared R transpose.",
                    "label": 0
                },
                {
                    "sent": "So for a symmetric matrix the eigenvalues and vectors of the square are the same as the original eigenvectors and eigenvalues are those original eigenvalues squ?",
                    "label": 0
                }
            ]
        },
        "clip_103": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "So I can plug that in there.",
                    "label": 0
                },
                {
                    "sent": "Has that formula.",
                    "label": 0
                },
                {
                    "sent": "The next thing we need to look at is these are cute.",
                    "label": 0
                }
            ]
        },
        "clip_104": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Transpose art.",
                    "label": 0
                },
                {
                    "sent": "Now looking at those RQ transpose ours.",
                    "label": 0
                },
                {
                    "sent": "It turns out that they have this reasonably simple form.",
                    "label": 0
                },
                {
                    "sent": "They turn out as a bunch of ones which are the eigenvectors that match the first Q eigenvectors on here and then we've got a load of things they're orthogonal to, so you get a bunch of 0, so this is the identity followed by a bunch of zeros.",
                    "label": 0
                },
                {
                    "sent": "So basically that's the form of our trans transpose RQ.",
                    "label": 0
                },
                {
                    "sent": "Trip.",
                    "label": 0
                }
            ]
        },
        "clip_105": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Get in there.",
                    "label": 0
                },
                {
                    "sent": "And then we get to multiply, multiplying it by Lambda.",
                    "label": 0
                },
                {
                    "sent": "So I'm putting one Lambda on that side for those, and I'm going to put the other land on that side, which gives us the Lambda Q.",
                    "label": 0
                }
            ]
        },
        "clip_106": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "20 multiplying it by itself transpose, so multiplying that by its own transpose simply gives me Lambda squared Q.",
                    "label": 0
                },
                {
                    "sent": "So I can substitute that.",
                    "label": 0
                }
            ]
        },
        "clip_107": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Whole thing there.",
                    "label": 0
                }
            ]
        },
        "clip_108": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "For Lambda squared Q.",
                    "label": 0
                }
            ]
        },
        "clip_109": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "Yep.",
                    "label": 0
                }
            ]
        },
        "clip_110": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Looks like that we send diagonalizes to form that.",
                    "label": 0
                },
                {
                    "sent": "So you queue this matrix that we've seen looks like a set of eigenvectors is actually a set of eigenvectors, and it's the set of eigenvectors.",
                    "label": 0
                },
                {
                    "sent": "The first Q eigenvectors of Y hat, Y hat transpose, and the eigenvalues are the same.",
                    "label": 1
                },
                {
                    "sent": "Now this is sort of well known in statistics, and this is the eigenvalue problem behind principle coordinate analysis or classical multi dimension.",
                    "label": 0
                }
            ]
        },
        "clip_111": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Scaling.",
                    "label": 0
                },
                {
                    "sent": "The difference between them, the two eigenvalue problems are equivalent.",
                    "label": 0
                },
                {
                    "sent": "One sold for the rotation.",
                    "label": 0
                },
                {
                    "sent": "So in one case you're trying to find what is the rotation of these points.",
                    "label": 0
                },
                {
                    "sent": "The other one solves directly for the location of the rotated points, so solve directly for X.",
                    "label": 1
                },
                {
                    "sent": "So in one case you're solving for the rotation of Y, which gives you X and the other case you're solving for X.",
                    "label": 0
                },
                {
                    "sent": "Now Wendy, the dimensionality of the data set is less than N. It's easier to solve for the rotation computationally.",
                    "label": 0
                },
                {
                    "sent": "The eigenvalue problem is simpler, but when D is greater than N, we solve for the embedding.",
                    "label": 0
                },
                {
                    "sent": "That's principle coordinate analysis.",
                    "label": 0
                },
                {
                    "sent": "We solve for X.",
                    "label": 0
                },
                {
                    "sent": "However, in multidimensional scaling we might not even know why, so we don't even know what D is in the base case.",
                    "label": 0
                },
                {
                    "sent": "So we can't compute Y transpose Y from the distance matrix.",
                    "label": 0
                },
                {
                    "sent": "But can we compute Y hat Y hat transpose instead?",
                    "label": 0
                },
                {
                    "sent": "So this is the distance matrix.",
                    "label": 0
                },
                {
                    "sent": "This isn't the distance matrix, it's another state inner product matrix.",
                    "label": 1
                },
                {
                    "sent": "So it's still not quite the distance matrix, so if we're given a distance matrix, how do we go from the distance matrix?",
                    "label": 0
                }
            ]
        },
        "clip_112": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "This error product matrix here.",
                    "label": 0
                },
                {
                    "sent": "So I'm going to give you a interpretation of that, which I call the covariance interpretation.",
                    "label": 0
                },
                {
                    "sent": "I've not really seen it written down, but it's the way I look at these things, and I think it helps to unify the way of thinking about these things.",
                    "label": 0
                },
                {
                    "sent": "But I'll do that after I think you normally take a break in these two hour long ones do.",
                    "label": 0
                },
                {
                    "sent": "Or you want to carry on?",
                    "label": 0
                },
                {
                    "sent": "I see lots of nods.",
                    "label": 0
                },
                {
                    "sent": "OK, so well, what we'll do is we'll have a 10 minute break and we will carry on at about 25 two.",
                    "label": 0
                },
                {
                    "sent": "10 OK.",
                    "label": 0
                }
            ]
        }
    }
}