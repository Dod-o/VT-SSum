{
    "id": "oaq2qqlsi5uvds6otkpm7jtadbhfvzax",
    "title": "Predictive methods for Text mining",
    "info": {
        "author": [
            "Tong Zhang, Department of Statistics, Rutgers, The State University of New Jersey"
        ],
        "published": "Feb. 25, 2007",
        "recorded": "July 2006",
        "category": [
            "Top->Computer Science->Text Mining"
        ]
    },
    "url": "http://videolectures.net/mlss06tw_zhang_pmtm/",
    "segmentation": [
        [
            "OK, thanks.",
            "Actually I'm very pleased to come here.",
            "This is first time I come to Taipei and thanks for all the host hospital hospitality.",
            "Also, I'm originally from Beijing so it's kind of interesting for me to come to Taiwan and I for me.",
            "It's kind of a lot of similarities, except I think all the signs in the streets are traditional characters instead of simplified.",
            "Then the other difference is you guys have typhoon.",
            "Then we don't have other.",
            "We thought it would be interesting.",
            "Um?",
            "So I will talk about text mining today.",
            "The text mining has a lot of applications.",
            "This one will.",
            "I will just focus on the prediction methods and also more basic stuffs.",
            "How to for the advanced audience?",
            "But mainly you will get an idea, especially if you don't know what text mining is.",
            "Another problems then you get a broad overview of what's happening and a little bit of the technical details as well.",
            "Also, it's a little bit different from the abstract I send in.",
            "If you read abstract, say what I'm going."
        ],
        [
            "Talk about that will actually not be the same, so the other thing is I made some small modifications in the slides, so if you use the one which is print out, it's a.",
            "There are some small modifications and you can download from the current ticket side which side if you want.",
            "I will talk about first about the motivation for text mining.",
            "The motivation actually is pretty clear for us nowadays there's a lot of electronic tags available, especially on the Internet and also inside organization.",
            "There are many documents, but also you have a personal files on your computer, so those things needs to be analyzed.",
            "Essentially.",
            "The idea is that if you can efficiently utilize resources to text resources, then you can.",
            "Find a lot of interesting things that you can.",
            "You can discover information and patterns.",
            "Then you can also create a knowledge base from text.",
            "Also the people daily uses for example, like search to find information.",
            "Those are all tech space based, so there is a very strong motivation to study what we can do.",
            "If we have a large number of attack, large amount of attacks in this in this tutorial.",
            "Mainly talk about the use of machine learning methods and prediction methods to do text analysis."
        ],
        [
            "First, I typically when people talk about tags they will say it's unstructured.",
            "It's unstructured information management.",
            "The what is the counterpart of that is most traditional is called structured data management, started data mining before the structured data mining, you generally have input is.",
            "Like spreadsheet I'm form format in a relational database for example, and each column you will have attribute of your data.",
            "Each row is a data point.",
            "Then you want to predict some output from this format you can you can predict for example missing values which you don't know you certain columns.",
            "Or you can find the patterns, relations, ships among your attributes and hopefully reveal similar patterns which give you some information.",
            "This this kind of a data mining is called the struct."
        ],
        [
            "Data mining example.",
            "I will give you is just simple express spreadsheet mode or for example this is a in the medical domain.",
            "You will have gender and weight etc of record offer patient.",
            "Then you have a disease.",
            "I example of a prediction problem in this context is just to say you want to predict disease code given other information of your patients.",
            "That's why.",
            "Well."
        ],
        [
            "Defined.",
            "In the unstructured text mining, the difference will be the input will be free format text that without very well defined structure or attributes.",
            "Then in the mining you want to extract information from the text even though there are free format that you want to still find some structure out of that you want to find patterns and then you want to organize those contents.",
            "Uh, the the prediction method.",
            "In this contact you want to extract some unknown information which are available from the text using machine learning methods.",
            "Typically those information can be low level information and high level information for the high level information you can have a semantics semantical information which will towards understanding of natural tags.",
            "We're not very far.",
            "Not very close to that, but still you can encode some kind of.",
            "Hi, information into more syntactic informationand which you can predict.",
            "You still can use the same model for structured data however you want to extract the columns or your features from those tags and that is part of text analysis is how do you construct those features.",
            "Representation of attacks.",
            "Secondly, you also encode your touch your encode your desired information which you don't know, but you want to find out from text into some kind of labels which you can apply machine learning algorithm to predict.",
            "So there are essentially two components.",
            "First, you want to encode some known information from available text into a numerical vector.",
            "Then you can encode desired information.",
            "If you don't know but want to know into a prediction output, then you just apply prediction."
        ],
        [
            "And I will give you examples of that.",
            "Here are some examples which actually can do with this framework.",
            "Why is text categorisation text categorisation is?",
            "Very standard prediction problem in text mining you have for example one that you can determine whether email you get is spam email or not.",
            "That is a categorization problem that you want to assign.",
            "Essentially, in general, you want to sign documents to whether belong to certain set of topics or not.",
            "It can be more than one classes.",
            "The prediction problem is, given the text, then you want to find the label.",
            "Able is belonging to topical or not.",
            "The other very commonly commonly used text analysis problem is information extraction.",
            "So idea is you want to extract some hidden information from text so that you can have a better understanding of what is tax is talking about.",
            "The one example is extract and label.",
            "Some chunks of text chunks that can represent organization.",
            "For example, say there's a person.",
            "In the text that you want to find a Daniel labeled this chunk of tax denotes a person.",
            "Once you give this information, you can feel that into a spreadsheet or database.",
            "Then you can look at the relationships and also other information.",
            "So the prediction is just find chunk boundaries as well as associated labels.",
            "So what did the chunk means?",
            "Summarization is also used the specially in the search results when you search for a document it will return a small summary.",
            "That's a query dependent summarization.",
            "Also there's a query independent summarization.",
            "Essentially you want to extract segments from document to represent its most important part of its content.",
            "The prediction problem is that how represented.",
            "Is 1 segment of taxes.",
            "Then you just order that and try to extract the most representative text segments.",
            "So that's the way to handle that by using the prediction framework.",
            "The other prediction from which can be solved is search like web search, Internet search or the more traditional IR search using a closed domain.",
            "Documents say this in this problem.",
            "What you want to predict?",
            "Is the relevance of document given a query?",
            "So your issue a query then you ask in your document collection which document is the most relevant to the query, so all those can be solved by prediction problem there."
        ],
        [
            "Many, many more.",
            "The machine learning approach, which I guess after last two days people should know, but I will just look at the slides just for people who are.",
            "New to machine learning, so the idea is that you want to create some kind called training data, which is pathways historical data.",
            "You see those are with both known inputs and no output.",
            "Then you want to construct.",
            "Given those examples, you want to find from the examples construct prediction method.",
            "Machine learning part is given here is the construction of a prediction.",
            "Then once you have a prediction method in some form, given the new cases that you can get the desire to be in formation of prediction of your new case."
        ],
        [
            "Uh, the the more formal way to state that is a supervised learning essentially in the prediction problem you have an input vector which you can call X.",
            "Then you have output which is unknown code.",
            "Whatever information you want to find out from the text which call Y.",
            "Then you want to find the function to say.",
            "So I give me your ax the quoting prediction, typically by a loss function.",
            "Then your prediction prediction is to be measured through training examples.",
            "So historical data you find from the text and there are standard learning problem like classification, regression and density estimation and also even more complicated problem in the most text domain prediction output is not necessarily just the simple label, so we can have a complex structure.",
            "And they will have something called structured prediction problems those."
        ],
        [
            "Will be more.",
            "Advanced prediction.",
            "So, given this background, I'll just give you outline of what I will talk about today.",
            "First, I will talk about the basic text processing.",
            "How do you essentially the how do you transform text document into feature representation?",
            "And Secondly, our talk about just text categorization.",
            "And I will mainly describe briefly a set of algorithms learning algorithms so you have ideas.",
            "People ask you what also learning algorithm are then you have idea of what they actually are.",
            "Then I will talk about briefly talk of information extraction.",
            "Not going to cover.",
            "This a lot because I don't think there's enough time, but.",
            "I will give an idea of how to solve that problem, at least using some some methods.",
            "Then there are some final remarks about using the prediction method."
        ],
        [
            "What you can do is that.",
            "So electric electronic attacks, the most standard format of electronic attacks nowadays, is called XML Extensible Markup Language.",
            "The this format is you want to embed tree structured annotation into tags.",
            "In that case, you can encode a lot of information available in the tags.",
            "Also, if a mining results, say I want to do some data mining, other text text mining, I will find a new information.",
            "You can also encode those label information just as annotations to the tags, so this is a very commonly used framework for text mining.",
            "And one document actually in this is just.",
            "This is just a file, but one file actually may contain many documents which is."
        ],
        [
            "Useful if you want to do text mining for storage purpose.",
            "This is just one example of XML.",
            "Basically it started, started document and then document.",
            "So this is 1.",
            "Now you start a text and enter text.",
            "Then you have a title.",
            "You can have authors you can extract.",
            "You have other things.",
            "You can have additional annotation say if you want to put annotation.",
            "Here you can put that annotation using using XML."
        ],
        [
            "Format.",
            "No, let's say.",
            "Types of processing for the prediction modeling essentially protects recession.",
            "One key problem is how do you encode free text into your feature vector so that you can apply machine learning methods.",
            "That is part of which I will talk about in the first part of the talk.",
            "For this you want to know basically model free text.",
            "The basic model is that documents sequence of some basic unit, typically called tokens, in the text processing, then English essentially towards it's a white space separated words.",
            "Those are called tokens, and in Chinese you can different interpretation.",
            "But generally in the text processing, it's always better to work at the character level.",
            "Then you can also argue, say token is some Chinese words which multiple characters.",
            "Depends on what."
        ],
        [
            "You want to use our use of in your text analysis example, tokenization is following, so the idea is first step of your text analysis algorithm is to tokenize your text into.",
            "Basically, you want to separate your text into tokens.",
            "And this is the first step in all the text analysis.",
            "So for example.",
            "This is so important to text.",
            "Then you want to output.",
            "So you say this is a token.",
            "This is token whether this should be a token, you can have your own definition is.",
            "There's not standard approach to say this is 1 token.",
            "Or maybe this token.",
            "And there is a end of sentence.",
            "You also have that from this year at least one thing you can see is there's no standard definition of token.",
            "For example, this whether this should be one token worth one.",
            "Another token.",
            "There are many other issues in token analysis.",
            "I will explain next, but first I want to mention once you have tokens, if you want to put that into a computer, understandable.",
            "Language, then you will map each token into an integer.",
            "Essentially you form a Dictionary of tokens, then you map."
        ],
        [
            "OK into integer and that is representation of tokens in computer, so issues in tokenization there are actually multiple issues due to ambiguity.",
            "Also style issues.",
            "Some people may write differently than some other people.",
            "That token may have different meanings and also compute punctuation.",
            "How do you handle numbers?",
            "And there's a lot of issues like this.",
            "For example in some document people will write this and something like this.",
            "I.",
            "In the case of, if you're if you're training data especially, there is well known training data.",
            "People using linguistic called Wall Street Journal.",
            "The double quotation is become two single quotation.",
            "If you want to train your text analysis system on that input and you want to apply that, another input that you want to resolve just to make sure that this token, this token actually mean the same thing, and that is very important because otherwise.",
            "You will never see this particular token in the Wall Street Journal data set, which many people use in the text analysis.",
            "I also like XML version like.",
            "How do you resolve all things like?",
            "That is when you train with one token.",
            "Again, you want to make sure they should be the same in another one when you try to apply your text analysis algorithm to another text and the other is like a text, does that mean how many tokens you can ask you what you want to do is that this may be a telephone number?",
            "If so, maybe it's better to make it 1 token.",
            "Or maybe just one or two numbers subtracted from each other.",
            "In that case, maybe you want to make it 3 tokens.",
            "There also have issues like punctuation, for example US.",
            "In this case you want to know whether this particular doubt means whether it's the end of your English sentence or it's not there.",
            "Also, normalization issue.",
            "Again, normalization is used for when you train with certain domain.",
            "When you apply to another text.",
            "Domain which may have different.",
            "Different ways to express your talking like USUS.",
            "What is separate token?",
            "This hard decisions if you have data expressing this kind of forms.",
            "Cause you normalize so that they actually become the same token if you're processing, use that data information.",
            "There's also case information which also very common.",
            "In particular, in a lot of news articles, even also for historical reasons, there are many tags.",
            "They are all just uniform cap, like they all capitalized.",
            "Then in many articles which I actually have a free case text, the mixed case case, both capitalized and non capitalized.",
            "How do you deal with the information of that in your tokenization?",
            "Should that become tokenization step decision which is about that and acronyms when you learn?",
            "Acronym, it could mean different meanings in different context we have here.",
            "Actually, in the machine learning what LDA really should mean is linear discriminant analysis, but there is a recent notation of LDA which means something else.",
            "Um?",
            "The language issues again.",
            "I will mention that for example there are different language.",
            "There are some Arabic and Hebrew, for example they will have.",
            "They actually from right to left.",
            "Then there are some tokens inside the right to left languages.",
            "I'm talking will go from left to right.",
            "Those issues you should deal with and also Chinese.",
            "There's no space if you are formatting character words as your."
        ],
        [
            "Processing unit.",
            "Those are the issues in the tokenization.",
            "However you can always create a simple tokenizer which works reasonable.",
            "Basically the tokenization you can you can do by using English text is following.",
            "You start just separate tokens by whatever eliminators, for example including white spaces and also your punctuation's you just separate them all.",
            "If you do text characterization you can.",
            "Remove all the punctuations you related.",
            "Don't carry information so only thing you want to be careful in the English will be about the DOT.",
            "Which have a lot of acronyms.",
            "Does contain that which you do not lose.",
            "Then you want to use a consistent set of rules to handle your ambiguities.",
            "For example, you want to always separate.",
            "Maybe you want to always separate the dash and you want to always resolve ending.",
            "In certain ways.",
            "Maybe you want to separate from the preceding word.",
            "Once you have a reasonable set of rule, you can also do additional processing to handle like this is needed where the normalization you want to do with the case information is due given those information.",
            "Then you call what do you get out of this?",
            "Your tokenization again?",
            "As I mentioned, there's no uniform way to do tokenization.",
            "Different people have different ideas, but the key is following the error of tokenization is OK, but you really should be consistent between the training and testing data.",
            "That's why you apply your mining algorithm.",
            "You want to make sure the tokens when you get them have actually the same meaning instead of a different meanings."
        ],
        [
            "The other part comes very common linguistic processing on top of tokens.",
            "Once you've started, get your tokens you have, you want to have a convert token variance of talking to a standard form that is related to the normalization, but in this case you want to do that more aggressively.",
            "The goal is a reduced number of tokens because if you see a lot of tokens.",
            "That is harder to apply.",
            "Learning algorithm to learn while you have a smaller number of tokens, it will become small number of features.",
            "It will be easier to learn therefore your hope is that you want to have tokens with the same meaning which will actually collapse to one single token in your processing.",
            "Example is if you have this talk and you can go through this and they are similar, have the same meaning, you just make it to be this token.",
            "This called Lemon azatian stem is a little bit more free form of lemon azatian.",
            "That's the most commonly used is the stemming.",
            "It's reduce tokens into their rules.",
            "One is you can use dictionary like if you want to go through.",
            "Me.",
            "Then then you can use very commonly used semaphore.",
            "English tags is called Porter Stemmer.",
            "It's essentially a set of rules.",
            "For example, you want to apply this.",
            "Then you replaced by this as you received by by essentially remove the earth from the ending.",
            "In this case, you actually can make some errors because some mass may be important, but overall this will collapse a lot of similar tokens into a single token.",
            "The hope is that this actually do you more benefit than harm, because it can make mistakes also.",
            "This program has a software we can you can actually find, and this is the most popular stammer.",
            "It actually helps in the.",
            "In the information information retrieval search, doing stemming in many cases will help, but it can also do harm.",
            "As I said, the effective of this kind of.",
            "Processing is you will improve the chance of matching your token to something in your document collection, but actually will reduce the quality of your match because.",
            "Some of the not necessarily the same meaning will actually you remove some information that may be important in your analysis, so you can always try that whether we send without tokenization for text categorization.",
            "For example, many people try whether we should do tokenization and not.",
            "And mostly tokenization like this does not help that much, but doesn't actually slightly hurt performance, usually, mainly because it can be.",
            "Introduce some artifact as some tokens you collapse together which should not be.",
            "There also is related processing map synonyms to our base form like car auto.",
            "Then you want to say both to automobile and things like that again.",
            "This also have."
        ],
        [
            "There are issues.",
            "So given those, you can start thinking about how do you represent your document once you after you do tokenization, you do stemming as your pre processing of your document.",
            "5th citation for the text categorisation that is at the document level you want to find the feature representation of your documents.",
            "The word features typically just token.",
            "These are treating token as a unit and you can have a possible different types like you understand the token versus non STEM token.",
            "Maybe you want to include them both as your presentation.",
            "Then you can say whether they should collapse by synonyms.",
            "Again those word level.",
            "So you can also consider marking word features, for example consecutive talking as in unit.",
            "Then you form a unit based on the two diagram of your tokens, then use that in your your your features.",
            "Can say also two tokens Co occurring within a window of certain size.",
            "Those are also features you can consider the partial position in formation in the XML file you have sections like text like title like Body, Those who want to treat that separately.",
            "Also there are sometimes very important to say the positions relative to the document.",
            "First sentences of the document type of document will be utilized.",
            "Good first time in this.",
            "In the paragraph you really good and the sentences, while your words near the beginning of your document, you will do good and those things you want to say.",
            "Maybe we want to take that into consideration.",
            "Basically, subject types of your token work weighted by your position relative to document.",
            "Those kind of global level of document representation for standard text categorisation.",
            "If you just look at the literature, typically people just talk about.",
            "This kind of things past features that is people experiments with public data.",
            "In fact, in some real applications like spam or other things, you really want to use a lot of other information like non tax information and as well as the features extracted from those non type down types in formation.",
            "Example of this web page web pages.",
            "You want to think about font size.",
            "If you have a different font size, you have a different importance, like a small headings.",
            "They will have a large font or the XML tag will tell you about what is heading, which is a.",
            "It's a body domain name which domains goes?",
            "That's also important to determine whether this high quality documents or not, and also like you are string sometimes can be helpful in your search.",
            "Anchor text anchor text is actually the text from Summer web page linked to your page and they will typically when they put a link they will explain what what is link is this.",
            "Some critics will be the.",
            "Will be a good information to characterize and in general even without answer test just the link structure of the your web graph will be additional information which you hope you can extract meaningful information.",
            "Example like this kind of features would be.",
            "Page rank features or there are some other link based like hits there actually there are also other ways to encode link features.",
            "The those are probably the most publicized methods to encode using information, but there are other ways as well."
        ],
        [
            "So why is your given those so like document level?",
            "Once you've given those features which you can start to think about, how do you encode those information into a vector?",
            "The standard method die are in the early years IR.",
            "Maybe the 1st 2030 years.",
            "This is the biggest invention.",
            "One of the biggest invention of the IR community which is.",
            "Document a vector space document model didia is that you want to create Dictionary of size.",
            "Let's say over certain size M that that will contain all your tokens.",
            "Let's say you would remove the punctuation.",
            "Then you want to map each token into a M dimensional vector.",
            "The vector is each entries of that vector.",
            "Question yes.",
            "OK.",
            "I. OK oh.",
            "Yeah.",
            "Different.",
            "Information.",
            "You sound weird.",
            "Roy.",
            "Roy.",
            "Sure.",
            "No, there are issues.",
            "Those issues I bring up just.",
            "Right?",
            "Actually, there is no general formal, but what I'm mentioning those issues means you should aware of those issues.",
            "Otherwise, if you do something you are talking for example your training data.",
            "When you tokenize differently than your test data, and they have different meanings.",
            "Then you will make errors and you probably don't even know what your error.",
            "Why you make errors.",
            "Program as I mentioned.",
            "I I I have to think think about general framework, yeah?",
            "I can just put it right now me hearing information into into the program.",
            "Yes, yes.",
            "As I mentioned, if I will give you the right, the keys here because there's no uniform tokenization standard.",
            "The key is consistency.",
            "You need to aware of the issues.",
            "Let me see here is just mentioned this kind of tokenization which will be fine.",
            "Actually the key is consistent you.",
            "So you want to resolve if you do linguistic processing, you do want to resolve this ending sentence detection, but you can use just simple ways, simple ways.",
            "Let's say if you have.",
            "You got almost more than 90 and 90% accuracy.",
            "You will say if the lower case, the number followed by DOT just regular work, especially in a dictionary followed by upper case on your right.",
            "Then you classify that as your sentence.",
            "That will resolve most of the case you want to resolve.",
            "All the case you can also use machine learning which.",
            "Many of the processing can be used in machine learning.",
            "Let's say I started with this system and our separate this downward decide next step what I should do after this?",
            "How is your consistent?",
            "You can train some.",
            "For example, if I have all upper case, the case, right?",
            "I can.",
            "I want to resolve the case information, then what you can do?",
            "You can train a classifier basically just with all the mixed case the sentences.",
            "Then you all uppercase that, but then you know the labels of each case that you want to train case classifier to say.",
            "How do I convert my case information into uppercase all uppercase tags?",
            "Into mixed case tags you can resolve that kind of sentence again.",
            "You can also resolve that because you have the labels.",
            "Especially if you want to just simple labels, you can say all the end of the paragraph will be end of sentence and then you can do uh detection of those normalization.",
            "If you have some training examples to say what should be normalized toward this kind of will be resolved.",
            "Also by using the set of rules.",
            "The key I think here is really consistency.",
            "You need to aware of the issues if you depends on the application in the text categorization you don't need the probably most of that is why the case.",
            "And one way to do calculation is you just map all the case information to lower case, but.",
            "In general.",
            "Can you simple rules if the consistency which will be fine?",
            "So I think normally you should not worry too much of tokenization as to how good your tokenizer is, because the later stage will actually make more more errors more problematic than the tokenization.",
            "Tokenization is inspired us all the issues is a easiest for all the other compared with other stages.",
            "OK so I will talk about vector space, model.",
            "Vector space model is you give the tokens just your favorite tokenization procedure.",
            "It's not necessarily to be perfect, but then you're happy with that and it's consistent.",
            "Value map each tokenized vector you want to say in M dimensional vector, which is the size of your dictionary.",
            "The ice component of your dictionary is basically frequency of your token.",
            "Actually talking.",
            "I in this document, so the number of occurrence, then this one is you.",
            "The idea is that basically you remove all the ordering information of your your document.",
            "You only consider that as a bag of words without ordering formation.",
            "Dominating left.",
            "In that case, if you just random particular document, so missing left is just the frequency that is only information you get, so that is the that is this model.",
            "In this setting the feature vector is surely very sparse and high dimensional.",
            "That's a key when people talk about text.",
            "Categorisation they will say oh this is high dimensional is very sparse.",
            "This will have demands of your learning algorithm so you are learning algorithm has to be able to deal with sparse data and high dimensional computations should not be issued by that.",
            "What is generalization performance?",
            "If you have a high dimensional data typically is harder to learn.",
            "I will comment on that later.",
            "In fact, I was just there will be 1 slide.",
            "Just talk about this.",
            "In fact it's not really high dimension, it just.",
            "Appear higher dimensional.",
            "This is the general framework if you want to call the position information, you can do that partially.",
            "For example, you can encode about each created different dictionary for different parts of your documents.",
            "That is 111 possible to preach partial information and you can say you have consecutive tokens as your super token.",
            "You can create Dictionary of diagrams of your tokens.",
            "Not use that then.",
            "That will give you a little bit order information.",
            "Not complete, but it's a.",
            "Good, it will be useful in some setting.",
            "You can combine mathematician with different weightings, proper weightings.",
            "For example, you can say token before slamming token off stemming, and whether you have a synonyms token mapped together you get another dictionary.",
            "You can use multi word features.",
            "I talk about a little bit earlier.",
            "This framework is mainly to for the.",
            "Global representation of your text and it's mainly used for the text features and is quite effective, more or less for text categorization.",
            "For a lot of other information.",
            "Once you do that, it's almost nothing better you can do with representing text and, However, if other information like non types information I mentioned a little bit earlier, this is not a good way to do that.",
            "Essentially you just create some features specifically for each non type feature and.",
            "That there is no uniform framework.",
            "Questions."
        ],
        [
            "OK.",
            "There is another processing I didn't mention how this all would leave after the.",
            "The vector space model is stopwords that's also very common.",
            "Processing of text and it helps in various ways.",
            "The idea is that you want to remove function or frequent model, which usually those world without a lot of predictor capability.",
            "For example, like the like a like it like they generally don't carry much meaning which you will be interested in.",
            "Therefore, once you have this vector space model, you can simply just ignore all those words.",
            "That idea in this way you can actually reduce the dictionary size and.",
            "You actually improve the quality if you do that right.",
            "There is always a danger of you.",
            "Remove some important words.",
            "For example, if you remove it.",
            "If you do the casing like you map all the tokens into your lower case, then you remove it, then then it could be carry some information like information technology.",
            "This may not be desirable if that's the other thing is that.",
            "It will improve computational efficiency, especially in the search engines.",
            "Those awards are a lot of those words and they they occupy a lot of space.",
            "And usually when you remove those words they actually improve your quality on one hand and also you also will not only improve quality but also you will reduce your space.",
            "Therefore you have more efficient matching algorithm for for this.",
            "This kind of processing the.",
            "The key is that you have to create your stop word list.",
            "Then you want to remove that.",
            "There's no.",
            "Again, I would say there's no standard list of Star Wars, but if you search stop words list you can find some on the Internet which you may just use or you just create your own.",
            "Mostly stuff was are frequent words and they will decide that's actually not very meaningful words, so you search for the frequent words of English tags.",
            "Then you look maybe, let's say top 1000 you go through that, you just delete some words that you create your own stop word list."
        ],
        [
            "The other very important concept in text processing information retrieval is a term weighting.",
            "The term weighting is to modify the term count by some perceived importance of the world.",
            "The procedure involves the idea is actually the real words are more important than the common words.",
            "If you have a real word common words, they appear in all documents.",
            "Therefore they are not very interesting.",
            "So the real words if you appear in a few words.",
            "If you search for that.",
            "In a match, which really means something there for you want to boost the importance of your rare words and you want to reduce your importance of common words.",
            "The again, there's a big invention here is called TF IDF weighting of your token.",
            "So IDF ATF means term frequency.",
            "I did that means inverse document frequency.",
            "TF IDF is term frequency times inverse document frequency.",
            "The inverse document for frequencies are log.",
            "The log here is important.",
            "You somehow you cannot change that to other functions like you change to square root of log log squares.",
            "It's not as good as log.",
            "The number of documents in your collection over the document number of documents which contain your words in the collection.",
            "So the ratio of that you take log that is your IDF.",
            "IDF is generally more important part in this formulation.",
            "You can modify your TF, but generally you do not modify your IDF.",
            "The modification of TF is actually it's better to modify your term frequency term usually by through truncation.",
            "The translation is you can say binary means 01 weather, weather, word appears in the document or not.",
            "You can also say it's 012 representation to say whether it appears 0 * 1 times or two or four multiple times.",
            "Reason they are important.",
            "This kind of modification is important is the following.",
            "So if you let's say a document contain many, many words of 1 token, it actually doesn't mean it's more more topical, less you have 100 cars in your document appearance and two cars sometimes.",
            "Actually it's not says that type of 100 times is more 100 times more more important than the document.",
            "Car is more important as far as relevance to the auto category so therefore if you want to truncate that, you can gain some stability.",
            "You want to control.",
            "Say you do not want one word to dominate too much in your documents.",
            "The this looks like why you should log.",
            "There are some studies about that, in particular in the retrieval information retrieval.",
            "There are some some explanation of that is not completely examination of this formula, but some related expression using what we call language model.",
            "Language model is close to the Naive Bayes method which I also talk about other Sam also.",
            "Talked about on Monday.",
            "Basically."
        ],
        [
            "So you can use the term waiting for example in document retrieval.",
            "So that's a standard use is defined as similarity measure.",
            "Use your terminating this one.",
            "Actually I think it's not in your in the hand out, so you should just look at the slides I just wrote.",
            "Yesterday, so you are given a query Q&A document D. Then you have term term counts, a lesser TFA of your QTF of your D. Then you want to create a matching score.",
            "The Magic School says how relevant D is for the query queue.",
            "Well, how close they are they are.",
            "You just use the TF IDF matching.",
            "Basically you have this is a IDF weighting and then you have a TF of both sides.",
            "That's working through the matching score.",
            "IDF weighting is very important if you don't use that.",
            "The quality of your search will degree significantly.",
            "There's also other truncation version of that actually works so well in practice, especially there is a lot of information more practical side of information is called so-called track conference.",
            "Those conference.",
            "So the experiment in that conference resolved to this kind of formulation, which basically want to truncate this term and the particular formula I'm saying is actually works quite allow demonstrate that conferences you modify this TF by kind of a truncation.",
            "This one you have a function of the form like like this kind of form function.",
            "Therefore you will always never larger than some quantity.",
            "Again, this one is also similar.",
            "But they are not very symmetric to the query and the document, mainly the document is that they're actually not exactly the same.",
            "Same query is tends to be much shorter, and therefore this term.",
            "This is more like setting BS Zero in the query while documents you have more value of document lines, this term is your document lines over your average document length, but nevertheless the why you should use this form, not other form.",
            "It's more empirical like why you should set this para meters, but it works reasonably well in some data set on some data set and also you can.",
            "It comes from some probability.",
            "But also those probability model probably not up to the baseline standard in general.",
            "In general the main idea is you want to truncate that term and I mentioned that in the earliest data you can also just truncation by zero 1, two.",
            "That will work fine, especially in text categorization."
        ],
        [
            "So one thing I want to mention final, that's the final thing I will mention about the token tokenization is statistics token statistics.",
            "There's well famous law called Zips Law basically states following.",
            "So you have.",
            "Let's say you have me so size of dictionary just say in this case I say English words and then JSJS ranked words based on frequency.",
            "You order the words.",
            "From the highest frequency to the lowest frequency, then you get rank for each one.",
            "Then you want to say what is the frequency of this token J.",
            "Basically you rank that ordered on this this guy zip losses that this should be the distribution.",
            "Off of your data.",
            "And in particular, Alpha is close to one, and if I was crosswise, really means this is actually your frequency distribution, so that gives a very restricted frequency.",
            "Possible frequency count of your tokens.",
            "Um, this is the empirical observation.",
            "There's a lot of literature on this.",
            "To say why this should be a good distribution, although it's sometimes difficult to really.",
            "Believe that they actually really can prove this, but in general you can use that to explain things.",
            "I think it's not really use the machine learning by any standard, but I think it's interesting to visit to know they potentially useful probably can use that prior for Bayesian modeling.",
            "I don't see anybody using that.",
            "You can also to understand behavior learning algorithm if you have a token count of using this particular shape.",
            "What you can say about your particular learning algorithm, in particular.",
            "One thing I want to mention is that usually people tell you like a text is a high dimension, it's true in some setting some sense, but it's not a.",
            "But then if you think about what happens is that the frequency decays.",
            "If you have very many token with very low frequency, which actually means they're not that important, you can.",
            "You can just ignore that.",
            "Basically you can just keep the high high frequency part, which are important, more important tokens and you achieve similar results.",
            "Therefore, even though taxes, high dimension and effective I mentioned actually is not that high and you can therefore use machine learning method to learn.",
            "Those can be applied without the curse of dimensionality."
        ],
        [
            "So I was summarized the document level feature generation.",
            "This is mostly for maybe information retrieval like search or like text categorization.",
            "Type application is not for linguistic application.",
            "Increase population would tokenize document with some basic processing like that.",
            "There's a text colors and you look at the global global view.",
            "So you have this tokenization you have stemming.",
            "And stop watch visual remove all those.",
            "Then you start to generate the dictionaries, maybe with different dictionaries for different representation of your tokenized document.",
            "All of that.",
            "Each representation will become a vector space model.",
            "Then you concatenate those vector space.",
            "You can use a product term rating, IDF is.",
            "The one which more frequently use, or you can actually use without it.",
            "If you do text categorization without it, doesn't hurt performance that much, so you can also use truncated term frequency count.",
            "Then you get a sparse feature representation.",
            "The post processing you can do is.",
            "That's also commonly done by using what is normalized your feature vector.",
            "Let's say you have a feature vector over high dimensional feature vector.",
            "You normalize according to the length of.",
            "One way is just normalized such that vector is in a sphere.",
            "Sphere, so that's why normalization is into normalization.",
            "You get that that is your final representation.",
            "Thing is, if you do some other applications, real world application which actually has actually information that attacks alone, then you want to incorporate some information.",
            "There are some difference of typical for the non tax importance of dense vector and tax information.",
            "Generally for very sparse vector, so they should be treated separately, although they can."
        ],
        [
            "Just apply a machine learning algorithm.",
            "Hopefully you can.",
            "Use algorithm which particularly suitable for each case, and then combine that.",
            "The example of a feature vector just for email spam detection.",
            "I just made it up.",
            "Let's say you have title of your email.",
            "You have a body of your email and here nontax feature you can say, well, stand from that domain is bad or mean or not.",
            "Or you can say who send it from and and so on all the trace and other things.",
            "So the this is similar to the right sheet representation of your of the structured data already.",
            "So up to now we have already changed the free text formulation into a more structured representation and you can apply machine learning directly.",
            "In this case, if you want to predict target is just spam, whether it's true spam or where it's not spam, then you can just apply a binary classification algorithm, anybody?",
            "Binary classification your favorite.",
            "1 reply to that you gotta answer.",
            "OK, so I want to mention a little bit again about the text features in the text feature is a bag of words.",
            "Representation is sparse and one part of that is actually you can just use the linear classification algorithm.",
            "Generally you do not need a nonlinear classification on the text features.",
            "Linear product information work as well, almost always for non fax features.",
            "There are.",
            "They can come from anywhere.",
            "They are typically tends.",
            "Also, they actually often contains nonlinear interactions among themselves, maybe even with the text feature.",
            "In that case, you do not want to directly use the linear classification.",
            "That does not give you a directly the best result.",
            "Therefore you want to apply some kind of finding some nonlinear features, either using the machine learning data mining methods.",
            "Which will give you the nonlinear, which will be good for the nonlinear method."
        ],
        [
            "OK, so that's I'm finished part.",
            "How?",
            "How do you generate feature vector doing the text processing for your document?",
            "Now I will just go to the second part of this tutorial, which is a text categorization.",
            "Text categorisation is the one mostly studied problem in text analysis.",
            "You see a lot of papers on text categorization.",
            "So basically the problem is that let's say you have a document well after this you want to train a classifier.",
            "Then you have a document.",
            "Then you have some topics.",
            "Let's say whether whether this belong to this topic or not, whether belong this topical weather belongs to you.",
            "Get that there could be some constraints about text categorization, which means whether this should be just one of those mutually exclusive, or it should not be used.",
            "Lusive typical eight people.",
            "Say it's not mutually exclusive, because your topic can overlap, but you can also."
        ],
        [
            "In your application, maybe you wanted it to be mutually exclusive.",
            "The application of text categorization is a lot of real application.",
            "For example spam detection that is very important application.",
            "People always use that in your.",
            "I mean I guess everybody here will receive a lot of spam every day in your email.",
            "This is a very important application.",
            "Then you can also say there's another application like place a text document into a taxonomy.",
            "Some some kind of document structure, category structure, so text document routing.",
            "If you have a taxonomy you want to say maybe I want to place this text into some folders, some filtering, or you want to have a phone if representative look at this document, maybe they can, they can just drop to the right people, or they can more efficiently search this document.",
            "For example, Yahoo's Web directory is kind of a taxonomy system.",
            "You can guide your navigation.",
            "When you have a search results to certain categories, you can also look at other documents belong to the same category, so those are kind of applications.",
            "Then you can also.",
            "To help the document match essentially documental with two many documents if they are with the same taxonomy in the same category that more like to match each other than they are in the different categories, so can use that to do that.",
            "And also there are some other application of the text categorization.",
            "Feeling some any kind of information you are interested in like language of these tags.",
            "Or maybe you want to say who is also of this text and so on."
        ],
        [
            "So they have a lot of applications, and in particular just talk a little bit about spam.",
            "Spam is very important nowadays.",
            "People generalizing everybody knows what email spam is.",
            "Essentially some email you do not want and you send multiple people.",
            "Although there are some peoples.",
            "Are there some grey areas?",
            "Also there are some spam.",
            "Maybe you actually want?",
            "I mean it's unwanted but provide you some information.",
            "The interests are also.",
            "There are some interesting spam which actually a lot of search engines more hidden behind the user.",
            "You do not see that everyday, but it's actually happening behind the see all the time, it's webpage spam those webpage spam at those web pages that are not very high quality, but they are intentionally to be designed such that they can be placed very high on search engines or they may not necessarily place higher themselves.",
            "But also they may.",
            "Just try to help other pages to play.",
            "Place high for example through link structure.",
            "There are something called Link Farm.",
            "You just create a lot of links.",
            "Hope to be the page rank algorithms.",
            "Then you have those low.",
            "Does low quality page if you see that you sometimes know but you do not see those that often the cause.",
            "Search engine actually constantly fight this.",
            "They have teams so to look for this and.",
            "And also they have used a lot of.",
            "Now they use tax information both but also non tax information.",
            "For example in structure is important and how the page actually is organized is important.",
            "One example is the sum of the spam page will place as like Google ads.",
            "On top of that, just to say if you go to that page then you can click the ads then they will benefit from that and usually they don't provide a lot of content and sometimes people just get contents out of different pages.",
            "Web pages that you put them together, those either sometimes it could be reasonable high quality contents in some sense, but still it's hard to say.",
            "It's a ethical behavior.",
            "Generally there will be always be some undesirable effect of the web page and it will degrade the user experience once users goes there, and they also seem similar spam, like blobs and even in the instant messaging newsgroup and phone.",
            "They also receive a lot of spam, so those spans you want to detect using the text text analysis in particular.",
            "In this case you really need to use.",
            "Sometimes features instead of a."
        ],
        [
            "Rely just on the text feature, as traditionally you'll see in the text analysis literature.",
            "Taxonomy classification is also very useful, important in many applications.",
            "Essentially you want to have a tree structure organization of your topics like.",
            "This is kind of Yahoo like category of.",
            "White pages then you can contain a large number of categories, say if you have a directory like this, it could be hundreds of thousands of your nodes, and then there are several issues with respect to this one is it for so many categories?",
            "How can you achieve accurate results?",
            "That's one question.",
            "And the learning algorithms scale to such a degree.",
            "Computational efficiency.",
            "Also the scalability issue if you have so many categories.",
            "Car learning algorithm scale to be effective.",
            "The other issue is how do you measure error if you only measure error in the leaf nodes.",
            "It may be too aggressive to measure because you may have a lot of error.",
            "However, even if a lot ever, maybe you'll actually be able to say I make an error here.",
            "I started.",
            "I do not make error here, but start here and make an error that maybe I can give a partial credit here.",
            "There are some other related issues with, that is to say that you want to.",
            "Maybe you want to make sure all the if you classify this is good.",
            "You want to also for consistent regions.",
            "You want to classify this one also is in class, so there are some constraints to this classification problem."
        ],
        [
            "Um?",
            "So in the in general, so this is kind of standard setting of classification in the taxonomy and spam.",
            "There are really a lot of challenging issues, particularly in the application side.",
            "How do you scale the system?",
            "How do you can find spam more effectively by using methods is sometimes beyond learning methods, but a lot of other creative ways to deal with that, which I'm not going to talk about, but I'll just give you.",
            "Very general general basic tax calculation framework which you use to classify tax documents into categories.",
            "Now for each basic framework is following for each category.",
            "If a many categories you just treat them separately.",
            "You determine whether document should should be lost to this category or not.",
            "The input will be a vector representation of the document.",
            "The output will be binary prediction.",
            "Whether it's belong to that category or not.",
            "However, not only binary prediction is needed, but you always.",
            "Almost in all the practical applications you also want the confidence of your prediction.",
            "Say whether how confident this is in the category and how not confident the general approach is for each category and for each document I will return a real value of the score.",
            "It's similar to the search concept in the sense I want to say how relevant my documents is to this category.",
            "You want to say taxi?",
            "Last two attacker if the score is larger than a threshold and then one issue which I will talk in the next is calibrate scoring to probability related score doesn't mean much.",
            "In many cases people want to say what does it mean.",
            "If I have a score of 536 doesn't mean anything.",
            "Therefore the more interpretable cases you want to map this score into probability which says if I have 0.9, which means the probability of these documents belong to this category.",
            "Is 0.9.",
            "Um?",
            "Prediction is more confident if if the probabilities close to 0 or 1 zero means I'm confident to say this Calculator is not in this category, this document or one I will say I'm very confident this document is in this category.",
            "You have multiple categories in this framework.",
            "We just return top scored categories."
        ],
        [
            "Um?",
            "In your scoring function.",
            "So the probability calculation.",
            "This one not only for the text categorisation you can use for other things as well, typically given a document and category T, the classifier returns some score function, which is a real value score function that you want to find the calibration function F that map this function in 201.",
            "The probability of the map F have the following semantic meaning is that the probability of this document DB lost this talking at the given.",
            "This scoring function is exactly the map of this scoring function.",
            "So.",
            "One simple way to do that is just rubbing.",
            "Let's say I will partition the real line into bins.",
            "Other than our divine cabins such as this.",
            "That for each penile, just do the counting to say whether documented D such that the number of total number of D belong to this being is m'kay and NK.",
            "I will say is it's a positive document belong today, then I will just have a mapping like this, so this will calibrate your probabilities for we're very simple way you can use more fancy stuff is also more general like use one dimensional density estimation method which will also solve this problem.",
            "But the this is a simple problem in some sense, but still is."
        ],
        [
            "Portent in practical applications.",
            "I just want to make some comments about the probability calibration.",
            "First of the goal collaboration is really make this score more interpretable, meaning that its probability you can say.",
            "But in general, which means actually collaboration does not improve classification accuracy.",
            "That's not the goal.",
            "You not to say I will calibrate the probability that I have more accurate classifier.",
            "The reason is the color function.",
            "It's often it's nearly monotonic.",
            "Therefore the thresholding beside probability for after collaboration.",
            "Before calibration, you can always match that.",
            "So.",
            "So, so this is the main goal.",
            "Also that's the important part is the following.",
            "Let's say I have a probability classifier in particular say logistic regression or naive Bayes.",
            "Now basically no, it doesn't give a probability because the model is incorrect.",
            "But people say logistic regression would give you a good probability.",
            "In general is still benefits from the collaboration that several reasons why is actually logistic regression that is similar to naive Bayes not accurate model.",
            "Therefore, it doesn't necessarily give you the best, but also there's more important issue is typically, while trainer classifier especially isn't discriminative method to train, it turns over fit, so it will fit over fit your probability estimation while you still can gain something from your classification accuracy, you have a more accurate classifier when you want to have the following way means that essentially.",
            "Well, you have a optimal why you get the optimal classification accuracy, say through cross validation.",
            "What are the methods then?",
            "The probability estimate will tend more confident than it should be.",
            "For example, it's towards more zero to one.",
            "Let's see if 0.6 this is the real probability.",
            "Maybe there are 26, but after you're doing this over fitting it'll give 0.9.",
            "Therefore you will still benefit from."
        ],
        [
            "Probability model collaboration.",
            "Next I will talk about common classification methods.",
            "Mostly I will talk a little bit about nearest neighbor and centroid that will top out through induction decision tree, naive Bayes and some linear classification methods.",
            "I think I will go a little bit of that then."
        ],
        [
            "Will stop after I will talk about nearest neighbor, so this is nearest neighbor.",
            "I mentioned the similarity measure with information retrieval search and you can do the same thing instead of a similarity measure with query on the document, you do similarity measure between documents.",
            "They are slightly different.",
            "You can do the TF IDF thing again here.",
            "It just replace the query and document this define similarity measure.",
            "Then you can also use.",
            "Very frequently used method, either with here for idea or not, is to map D1 and D2 into vectors by a vector space model back of words.",
            "Daniel, you normalize vector to be honest fear.",
            "Then the similarity measure is just inner product for these two which is cosine angle between these two.",
            "When you look at the sphere this measure is between zero and one because of documents are positive there are positive so they will be not be smaller than 0.",
            "And very similar documents will have score close to one and a different document will have a score close to 0.",
            "The nice thing about this is very interpretable.",
            "You'll say if I have 0.9 here, I know they are very similar.",
            "If I 0.1, I know they are not very similar.",
            "Therefore this is very.",
            "So so very nice thing about that interpretability and interpretability is very useful in a lot of practical applications, and this has this desirable effect.",
            "You can have other methods like there are some LSI which is projection to a low dimension, or there are some fancy ones like similarity along the manifold documents manifold, which I'm not going to talk."
        ],
        [
            "About but once you give a similarity measure, whatever it is, you can apply the nearest neighbor method.",
            "Basically just compute the similarity of documents to all the other document in your training data, select documents that's most similar to your new document.",
            "Then you select the label that occur most often in the select documents.",
            "The advantages that requires very few positive data and virtually no negative data to train this classifier.",
            "For example, if I have the.",
            "Cosine measure is my document.",
            "I can just prefix threshold to say.",
            "How how, how close is this are and.",
            "Very easy to understand.",
            "Though.",
            "Update when data come in.",
            "The disadvantages one is rely on this similarity measure.",
            "The important part of that is in the text feature is OK because I would say cosine measure is fine.",
            "But then what about the non text features?",
            "That much harder can be memory and computation."
        ],
        [
            "Very inefficient if you have a minute training data.",
            "One way to address this efficiency issue is using a slightly modification of of of nearest neighbor which called century method.",
            "So essentially instead of a nearest neighbor for all the training data.",
            "If let's say I have similarity of this this I can have.",
            "TBS document collection belong to some category T, then the central is just average of this term frequency.",
            "So I make instead of getting nearest neighbor for each individual document, I have a bigger document which contains average of the term frequency and then the similar scoring of Document D and category T is using this summarization of your your document by the centroid and use your whatever.",
            "You are terminating, plus your your discord.",
            "At vantage of again is this very simple, understandable and efficient.",
            "If use cosine measure, you can still normalize that and using cosine measure again.",
            "In that case, you can say you know the confidence because cosine measure is through zero to 1 and you do not need the probability.",
            "You can still argue you understand what this similarity measure means.",
            "It's also actually can be used for technology with very large number of categories.",
            "And each category is very coherent, means they are very close to each other in the central in the nearest neighbor space and it.",
            "But in general if it can be less accurate than other methods, if the category is not coherent, the improvements you can use mixture Gaussians or other things, they lose some attractive ability of this method means it's very simple and easy.",
            "OK, so I guess we will stop here.",
            "Are there questions that will go over?",
            "I guess next time I will talk about the rule based when we come back.",
            "So if a question is I can answer now.",
            "OK.",
            "Question here oh square.",
            "What's the size of the vector that we're talking about and we need to store this for the problems that we are trying to solve, the size of which vector?",
            "T effectors English word is.",
            "Now he's aspires vector so you always start at in the sparse format.",
            "You have a, you have a index, each one you just have documented as recent as index to the dictionary.",
            "Usually documents are small, have a number of small number of words.",
            "Therefore it will be a sparse vector.",
            "Generally we are not talking about when we talk about documents.",
            "It's not like a novel.",
            "If a normal you have a huge number of words, which is another.",
            "In this case is more talk about a web page and news article.",
            "In that case, maybe you have 200, two 100 words per article.",
            "Then you have sparse documents, so it's not issue to store them.",
            "Start storage is efficient.",
            "OK, another question, is your your feature vector which is.",
            "Rat.",
            "How does that relate with that example that you gave with the spam Mail where you had?",
            "Show the table.",
            "How does that relate to the table meta?",
            "But yeah, in your example, which feature vector ranked?",
            "Stop.",
            "Example, I gotta drive a spam example."
        ],
        [
            "OK, let me let me get OK.",
            "So this one is a vector talked a vector space model.",
            "Basically say this is a word in your dictionary.",
            "This actually appear in this email message.",
            "You receive this one.",
            "Say this doesn't appear.",
            "This actually sparse vector, so this will not be in your in your representation of document, this index doesn't appear, but this one.",
            "It does appear here.",
            "You just say you create a dictionary here created in traditional.",
            "Here enlargement is one word.",
            "Dictionary you say it appears in your email message.",
            "Measure on your car title and the body and know your Creator as one vector.",
            "That depends on your implementation.",
            "Generally this one if you do similarity measure this one is hard to deal with.",
            "People typically deal with this part.",
            "You can normalize so that different normalization.",
            "You can normalize perception.",
            "You can normalize just by the whole whole sections one vector to normalize, and then you apply similarity measure to the normalized vector.",
            "That is mainly I was talking about in the cosine measure.",
            "You just normalize that one vector, but you can create some weighted average if you want to normalize with different parts.",
            "OK, yeah.",
            "Skull popular back to Space document model.",
            "OK, right?",
            "Documents and from tokens right?",
            "And how many do you create?",
            "A dictionary topics OK, right?",
            "Many different dictionary because also depends on the depends on documents.",
            "All the darkness in the Internet.",
            "Then you create one dictionary which useful for all the documents.",
            "Yeah, as your search engine you only have one search engine.",
            "Yeah yeah you have a 111 dictionary right right?",
            "So there may be created.",
            "And Adam money formation?",
            "Yeah, yeah, that's a different issue.",
            "If say if I have a Dictionary of this size then we have more document coming.",
            "We will discover new tokens.",
            "What you do then you know how to update the dictionary is the main thing of operation Dictionaries.",
            "When you create the features it will goes to the index you want to make sure they are consistent.",
            "And if, let's say you have two degeneration Aries.",
            "One dictionary you just followed by another dictionary, index or with your feature vector.",
            "Then that could become an issue.",
            "Therefore you have some methods to deal with that, but in general you can just if you have enough computational power, is reduced.",
            "Everything like started, reindex, yeah, reindex all your results and you just update your dictionaries.",
            "That may not be relevant to the.",
            "Lots of the targets because the mean different things.",
            "That's true, but you have only one dictionary globally, so tokens will match.",
            "They have to be matched, right?",
            "If I will say this document have Token Inc represent integer one.",
            "The other document with the inspector also represented integer one, not integer 10 because they need to match.",
            "So you have one big dictionary and for each document you map to the same integer.",
            "When you do the representation.",
            "I.",
            "How about there?",
            "OK, after you do the texture characterization, you know some of the document.",
            "Actually they are belong to the same category category.",
            "Yeah, really late, right?",
            "Oh, really.",
            "Send a send group OK in terms of semantics and so on so.",
            "That that kind of information are using that kind of formation to maybe modify dictionary.",
            "I don't know, it's just it's possible it's not.",
            "Yeah, it's it's possible if you later or mention about this you can classify hierarchal classification.",
            "You can go through your tree of taxonomy tree that for each in the leaf nodes or close to the need for nodes.",
            "Deeper nodes you have a smaller number of documents and you can create a small number of dictionaries which are more related documents.",
            "Those are possibilities.",
            "To do, yeah.",
            "Latuda are.",
            "Is there any relation in terms of techniques, maybe approach between your topics and natural language processing?",
            "Yes, so in general the information retrieval, text mining, text mining.",
            "Actually you can argue that contains natural language processing.",
            "There are kind of overlap, but for natural language processing typically they talk about sentence level like semantics of the sentence.",
            "What this interactive sentence.",
            "Sentence level processing in document processing.",
            "They generally talking about higher level like document.",
            "The category of document.",
            "It's not at the linguistic level, so there is, but there are some blurring factor.",
            "Like summarization, you can argue is document level.",
            "They also need a natural language processing parsing for example.",
            "It's generally not regarded if you can still say it's a text mining.",
            "When I see text mining, this talk is more like a document level, but in the information extraction I will talk in the second topic is actually at the sentence level.",
            "That's more of natural language processing.",
            "The extraction out of the information extraction.",
            "The name of the entity.",
            "All those you can think that features for the document representation, so it's there quite related some some you can distinguish like categorization text categories.",
            "Now usually do not regard that natural language processing, but some are not necessarily.",
            "Talk about both question answering because question answering you need linguistic information to answer specific questions.",
            "This like more detailed form of search search.",
            "You just return a document that's documented level question answer and you want to return sentence level where you actually return even finer level of your answer.",
            "Your query, so there are.",
            "There are different applications.",
            "How do you view that?",
            "Fuck yeah.",
            "So it is own also had something with the performance because you want to.",
            "The natural language processing probably.",
            "Because we really want to have a semantics come up from the from the come up from the documents, not just doing the text securitization.",
            "Yeah yeah, OK. Well, yeah, that you you can do some natural language processing task on top of that.",
            "Generally those those processing doesn't improve document categorization.",
            "If you look at the document level, people try that, say, linguistic information, how they use it actually doesn't help that much for many problems.",
            "OK."
        ]
    ],
    "summarization": {
        "clip_0": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, thanks.",
                    "label": 0
                },
                {
                    "sent": "Actually I'm very pleased to come here.",
                    "label": 0
                },
                {
                    "sent": "This is first time I come to Taipei and thanks for all the host hospital hospitality.",
                    "label": 0
                },
                {
                    "sent": "Also, I'm originally from Beijing so it's kind of interesting for me to come to Taiwan and I for me.",
                    "label": 0
                },
                {
                    "sent": "It's kind of a lot of similarities, except I think all the signs in the streets are traditional characters instead of simplified.",
                    "label": 0
                },
                {
                    "sent": "Then the other difference is you guys have typhoon.",
                    "label": 0
                },
                {
                    "sent": "Then we don't have other.",
                    "label": 0
                },
                {
                    "sent": "We thought it would be interesting.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So I will talk about text mining today.",
                    "label": 1
                },
                {
                    "sent": "The text mining has a lot of applications.",
                    "label": 0
                },
                {
                    "sent": "This one will.",
                    "label": 0
                },
                {
                    "sent": "I will just focus on the prediction methods and also more basic stuffs.",
                    "label": 0
                },
                {
                    "sent": "How to for the advanced audience?",
                    "label": 0
                },
                {
                    "sent": "But mainly you will get an idea, especially if you don't know what text mining is.",
                    "label": 0
                },
                {
                    "sent": "Another problems then you get a broad overview of what's happening and a little bit of the technical details as well.",
                    "label": 0
                },
                {
                    "sent": "Also, it's a little bit different from the abstract I send in.",
                    "label": 0
                },
                {
                    "sent": "If you read abstract, say what I'm going.",
                    "label": 0
                }
            ]
        },
        "clip_1": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Talk about that will actually not be the same, so the other thing is I made some small modifications in the slides, so if you use the one which is print out, it's a.",
                    "label": 0
                },
                {
                    "sent": "There are some small modifications and you can download from the current ticket side which side if you want.",
                    "label": 0
                },
                {
                    "sent": "I will talk about first about the motivation for text mining.",
                    "label": 1
                },
                {
                    "sent": "The motivation actually is pretty clear for us nowadays there's a lot of electronic tags available, especially on the Internet and also inside organization.",
                    "label": 0
                },
                {
                    "sent": "There are many documents, but also you have a personal files on your computer, so those things needs to be analyzed.",
                    "label": 0
                },
                {
                    "sent": "Essentially.",
                    "label": 1
                },
                {
                    "sent": "The idea is that if you can efficiently utilize resources to text resources, then you can.",
                    "label": 0
                },
                {
                    "sent": "Find a lot of interesting things that you can.",
                    "label": 1
                },
                {
                    "sent": "You can discover information and patterns.",
                    "label": 0
                },
                {
                    "sent": "Then you can also create a knowledge base from text.",
                    "label": 0
                },
                {
                    "sent": "Also the people daily uses for example, like search to find information.",
                    "label": 0
                },
                {
                    "sent": "Those are all tech space based, so there is a very strong motivation to study what we can do.",
                    "label": 0
                },
                {
                    "sent": "If we have a large number of attack, large amount of attacks in this in this tutorial.",
                    "label": 0
                },
                {
                    "sent": "Mainly talk about the use of machine learning methods and prediction methods to do text analysis.",
                    "label": 1
                }
            ]
        },
        "clip_2": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "First, I typically when people talk about tags they will say it's unstructured.",
                    "label": 0
                },
                {
                    "sent": "It's unstructured information management.",
                    "label": 0
                },
                {
                    "sent": "The what is the counterpart of that is most traditional is called structured data management, started data mining before the structured data mining, you generally have input is.",
                    "label": 0
                },
                {
                    "sent": "Like spreadsheet I'm form format in a relational database for example, and each column you will have attribute of your data.",
                    "label": 0
                },
                {
                    "sent": "Each row is a data point.",
                    "label": 1
                },
                {
                    "sent": "Then you want to predict some output from this format you can you can predict for example missing values which you don't know you certain columns.",
                    "label": 0
                },
                {
                    "sent": "Or you can find the patterns, relations, ships among your attributes and hopefully reveal similar patterns which give you some information.",
                    "label": 0
                },
                {
                    "sent": "This this kind of a data mining is called the struct.",
                    "label": 0
                }
            ]
        },
        "clip_3": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Data mining example.",
                    "label": 0
                },
                {
                    "sent": "I will give you is just simple express spreadsheet mode or for example this is a in the medical domain.",
                    "label": 0
                },
                {
                    "sent": "You will have gender and weight etc of record offer patient.",
                    "label": 0
                },
                {
                    "sent": "Then you have a disease.",
                    "label": 0
                },
                {
                    "sent": "I example of a prediction problem in this context is just to say you want to predict disease code given other information of your patients.",
                    "label": 1
                },
                {
                    "sent": "That's why.",
                    "label": 0
                },
                {
                    "sent": "Well.",
                    "label": 0
                }
            ]
        },
        "clip_4": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Defined.",
                    "label": 0
                },
                {
                    "sent": "In the unstructured text mining, the difference will be the input will be free format text that without very well defined structure or attributes.",
                    "label": 0
                },
                {
                    "sent": "Then in the mining you want to extract information from the text even though there are free format that you want to still find some structure out of that you want to find patterns and then you want to organize those contents.",
                    "label": 0
                },
                {
                    "sent": "Uh, the the prediction method.",
                    "label": 0
                },
                {
                    "sent": "In this contact you want to extract some unknown information which are available from the text using machine learning methods.",
                    "label": 1
                },
                {
                    "sent": "Typically those information can be low level information and high level information for the high level information you can have a semantics semantical information which will towards understanding of natural tags.",
                    "label": 0
                },
                {
                    "sent": "We're not very far.",
                    "label": 0
                },
                {
                    "sent": "Not very close to that, but still you can encode some kind of.",
                    "label": 0
                },
                {
                    "sent": "Hi, information into more syntactic informationand which you can predict.",
                    "label": 0
                },
                {
                    "sent": "You still can use the same model for structured data however you want to extract the columns or your features from those tags and that is part of text analysis is how do you construct those features.",
                    "label": 0
                },
                {
                    "sent": "Representation of attacks.",
                    "label": 0
                },
                {
                    "sent": "Secondly, you also encode your touch your encode your desired information which you don't know, but you want to find out from text into some kind of labels which you can apply machine learning algorithm to predict.",
                    "label": 0
                },
                {
                    "sent": "So there are essentially two components.",
                    "label": 1
                },
                {
                    "sent": "First, you want to encode some known information from available text into a numerical vector.",
                    "label": 1
                },
                {
                    "sent": "Then you can encode desired information.",
                    "label": 0
                },
                {
                    "sent": "If you don't know but want to know into a prediction output, then you just apply prediction.",
                    "label": 0
                }
            ]
        },
        "clip_5": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "And I will give you examples of that.",
                    "label": 0
                },
                {
                    "sent": "Here are some examples which actually can do with this framework.",
                    "label": 0
                },
                {
                    "sent": "Why is text categorisation text categorisation is?",
                    "label": 0
                },
                {
                    "sent": "Very standard prediction problem in text mining you have for example one that you can determine whether email you get is spam email or not.",
                    "label": 0
                },
                {
                    "sent": "That is a categorization problem that you want to assign.",
                    "label": 0
                },
                {
                    "sent": "Essentially, in general, you want to sign documents to whether belong to certain set of topics or not.",
                    "label": 1
                },
                {
                    "sent": "It can be more than one classes.",
                    "label": 0
                },
                {
                    "sent": "The prediction problem is, given the text, then you want to find the label.",
                    "label": 0
                },
                {
                    "sent": "Able is belonging to topical or not.",
                    "label": 0
                },
                {
                    "sent": "The other very commonly commonly used text analysis problem is information extraction.",
                    "label": 0
                },
                {
                    "sent": "So idea is you want to extract some hidden information from text so that you can have a better understanding of what is tax is talking about.",
                    "label": 1
                },
                {
                    "sent": "The one example is extract and label.",
                    "label": 0
                },
                {
                    "sent": "Some chunks of text chunks that can represent organization.",
                    "label": 0
                },
                {
                    "sent": "For example, say there's a person.",
                    "label": 0
                },
                {
                    "sent": "In the text that you want to find a Daniel labeled this chunk of tax denotes a person.",
                    "label": 1
                },
                {
                    "sent": "Once you give this information, you can feel that into a spreadsheet or database.",
                    "label": 0
                },
                {
                    "sent": "Then you can look at the relationships and also other information.",
                    "label": 0
                },
                {
                    "sent": "So the prediction is just find chunk boundaries as well as associated labels.",
                    "label": 1
                },
                {
                    "sent": "So what did the chunk means?",
                    "label": 0
                },
                {
                    "sent": "Summarization is also used the specially in the search results when you search for a document it will return a small summary.",
                    "label": 0
                },
                {
                    "sent": "That's a query dependent summarization.",
                    "label": 0
                },
                {
                    "sent": "Also there's a query independent summarization.",
                    "label": 0
                },
                {
                    "sent": "Essentially you want to extract segments from document to represent its most important part of its content.",
                    "label": 1
                },
                {
                    "sent": "The prediction problem is that how represented.",
                    "label": 0
                },
                {
                    "sent": "Is 1 segment of taxes.",
                    "label": 0
                },
                {
                    "sent": "Then you just order that and try to extract the most representative text segments.",
                    "label": 0
                },
                {
                    "sent": "So that's the way to handle that by using the prediction framework.",
                    "label": 0
                },
                {
                    "sent": "The other prediction from which can be solved is search like web search, Internet search or the more traditional IR search using a closed domain.",
                    "label": 0
                },
                {
                    "sent": "Documents say this in this problem.",
                    "label": 1
                },
                {
                    "sent": "What you want to predict?",
                    "label": 0
                },
                {
                    "sent": "Is the relevance of document given a query?",
                    "label": 0
                },
                {
                    "sent": "So your issue a query then you ask in your document collection which document is the most relevant to the query, so all those can be solved by prediction problem there.",
                    "label": 0
                }
            ]
        },
        "clip_6": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Many, many more.",
                    "label": 0
                },
                {
                    "sent": "The machine learning approach, which I guess after last two days people should know, but I will just look at the slides just for people who are.",
                    "label": 1
                },
                {
                    "sent": "New to machine learning, so the idea is that you want to create some kind called training data, which is pathways historical data.",
                    "label": 0
                },
                {
                    "sent": "You see those are with both known inputs and no output.",
                    "label": 0
                },
                {
                    "sent": "Then you want to construct.",
                    "label": 0
                },
                {
                    "sent": "Given those examples, you want to find from the examples construct prediction method.",
                    "label": 0
                },
                {
                    "sent": "Machine learning part is given here is the construction of a prediction.",
                    "label": 0
                },
                {
                    "sent": "Then once you have a prediction method in some form, given the new cases that you can get the desire to be in formation of prediction of your new case.",
                    "label": 0
                }
            ]
        },
        "clip_7": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Uh, the the more formal way to state that is a supervised learning essentially in the prediction problem you have an input vector which you can call X.",
                    "label": 1
                },
                {
                    "sent": "Then you have output which is unknown code.",
                    "label": 0
                },
                {
                    "sent": "Whatever information you want to find out from the text which call Y.",
                    "label": 1
                },
                {
                    "sent": "Then you want to find the function to say.",
                    "label": 1
                },
                {
                    "sent": "So I give me your ax the quoting prediction, typically by a loss function.",
                    "label": 1
                },
                {
                    "sent": "Then your prediction prediction is to be measured through training examples.",
                    "label": 0
                },
                {
                    "sent": "So historical data you find from the text and there are standard learning problem like classification, regression and density estimation and also even more complicated problem in the most text domain prediction output is not necessarily just the simple label, so we can have a complex structure.",
                    "label": 0
                },
                {
                    "sent": "And they will have something called structured prediction problems those.",
                    "label": 0
                }
            ]
        },
        "clip_8": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will be more.",
                    "label": 0
                },
                {
                    "sent": "Advanced prediction.",
                    "label": 0
                },
                {
                    "sent": "So, given this background, I'll just give you outline of what I will talk about today.",
                    "label": 0
                },
                {
                    "sent": "First, I will talk about the basic text processing.",
                    "label": 0
                },
                {
                    "sent": "How do you essentially the how do you transform text document into feature representation?",
                    "label": 0
                },
                {
                    "sent": "And Secondly, our talk about just text categorization.",
                    "label": 1
                },
                {
                    "sent": "And I will mainly describe briefly a set of algorithms learning algorithms so you have ideas.",
                    "label": 0
                },
                {
                    "sent": "People ask you what also learning algorithm are then you have idea of what they actually are.",
                    "label": 1
                },
                {
                    "sent": "Then I will talk about briefly talk of information extraction.",
                    "label": 0
                },
                {
                    "sent": "Not going to cover.",
                    "label": 0
                },
                {
                    "sent": "This a lot because I don't think there's enough time, but.",
                    "label": 0
                },
                {
                    "sent": "I will give an idea of how to solve that problem, at least using some some methods.",
                    "label": 0
                },
                {
                    "sent": "Then there are some final remarks about using the prediction method.",
                    "label": 1
                }
            ]
        },
        "clip_9": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "What you can do is that.",
                    "label": 0
                },
                {
                    "sent": "So electric electronic attacks, the most standard format of electronic attacks nowadays, is called XML Extensible Markup Language.",
                    "label": 1
                },
                {
                    "sent": "The this format is you want to embed tree structured annotation into tags.",
                    "label": 1
                },
                {
                    "sent": "In that case, you can encode a lot of information available in the tags.",
                    "label": 0
                },
                {
                    "sent": "Also, if a mining results, say I want to do some data mining, other text text mining, I will find a new information.",
                    "label": 0
                },
                {
                    "sent": "You can also encode those label information just as annotations to the tags, so this is a very commonly used framework for text mining.",
                    "label": 0
                },
                {
                    "sent": "And one document actually in this is just.",
                    "label": 0
                },
                {
                    "sent": "This is just a file, but one file actually may contain many documents which is.",
                    "label": 0
                }
            ]
        },
        "clip_10": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Useful if you want to do text mining for storage purpose.",
                    "label": 0
                },
                {
                    "sent": "This is just one example of XML.",
                    "label": 1
                },
                {
                    "sent": "Basically it started, started document and then document.",
                    "label": 0
                },
                {
                    "sent": "So this is 1.",
                    "label": 0
                },
                {
                    "sent": "Now you start a text and enter text.",
                    "label": 0
                },
                {
                    "sent": "Then you have a title.",
                    "label": 0
                },
                {
                    "sent": "You can have authors you can extract.",
                    "label": 0
                },
                {
                    "sent": "You have other things.",
                    "label": 0
                },
                {
                    "sent": "You can have additional annotation say if you want to put annotation.",
                    "label": 0
                },
                {
                    "sent": "Here you can put that annotation using using XML.",
                    "label": 0
                }
            ]
        },
        "clip_11": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Format.",
                    "label": 0
                },
                {
                    "sent": "No, let's say.",
                    "label": 0
                },
                {
                    "sent": "Types of processing for the prediction modeling essentially protects recession.",
                    "label": 0
                },
                {
                    "sent": "One key problem is how do you encode free text into your feature vector so that you can apply machine learning methods.",
                    "label": 1
                },
                {
                    "sent": "That is part of which I will talk about in the first part of the talk.",
                    "label": 0
                },
                {
                    "sent": "For this you want to know basically model free text.",
                    "label": 1
                },
                {
                    "sent": "The basic model is that documents sequence of some basic unit, typically called tokens, in the text processing, then English essentially towards it's a white space separated words.",
                    "label": 0
                },
                {
                    "sent": "Those are called tokens, and in Chinese you can different interpretation.",
                    "label": 0
                },
                {
                    "sent": "But generally in the text processing, it's always better to work at the character level.",
                    "label": 0
                },
                {
                    "sent": "Then you can also argue, say token is some Chinese words which multiple characters.",
                    "label": 0
                },
                {
                    "sent": "Depends on what.",
                    "label": 0
                }
            ]
        },
        "clip_12": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "You want to use our use of in your text analysis example, tokenization is following, so the idea is first step of your text analysis algorithm is to tokenize your text into.",
                    "label": 0
                },
                {
                    "sent": "Basically, you want to separate your text into tokens.",
                    "label": 1
                },
                {
                    "sent": "And this is the first step in all the text analysis.",
                    "label": 0
                },
                {
                    "sent": "So for example.",
                    "label": 0
                },
                {
                    "sent": "This is so important to text.",
                    "label": 0
                },
                {
                    "sent": "Then you want to output.",
                    "label": 0
                },
                {
                    "sent": "So you say this is a token.",
                    "label": 0
                },
                {
                    "sent": "This is token whether this should be a token, you can have your own definition is.",
                    "label": 0
                },
                {
                    "sent": "There's not standard approach to say this is 1 token.",
                    "label": 0
                },
                {
                    "sent": "Or maybe this token.",
                    "label": 0
                },
                {
                    "sent": "And there is a end of sentence.",
                    "label": 0
                },
                {
                    "sent": "You also have that from this year at least one thing you can see is there's no standard definition of token.",
                    "label": 0
                },
                {
                    "sent": "For example, this whether this should be one token worth one.",
                    "label": 0
                },
                {
                    "sent": "Another token.",
                    "label": 0
                },
                {
                    "sent": "There are many other issues in token analysis.",
                    "label": 0
                },
                {
                    "sent": "I will explain next, but first I want to mention once you have tokens, if you want to put that into a computer, understandable.",
                    "label": 0
                },
                {
                    "sent": "Language, then you will map each token into an integer.",
                    "label": 1
                },
                {
                    "sent": "Essentially you form a Dictionary of tokens, then you map.",
                    "label": 0
                }
            ]
        },
        "clip_13": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK into integer and that is representation of tokens in computer, so issues in tokenization there are actually multiple issues due to ambiguity.",
                    "label": 1
                },
                {
                    "sent": "Also style issues.",
                    "label": 0
                },
                {
                    "sent": "Some people may write differently than some other people.",
                    "label": 0
                },
                {
                    "sent": "That token may have different meanings and also compute punctuation.",
                    "label": 0
                },
                {
                    "sent": "How do you handle numbers?",
                    "label": 0
                },
                {
                    "sent": "And there's a lot of issues like this.",
                    "label": 0
                },
                {
                    "sent": "For example in some document people will write this and something like this.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "In the case of, if you're if you're training data especially, there is well known training data.",
                    "label": 0
                },
                {
                    "sent": "People using linguistic called Wall Street Journal.",
                    "label": 0
                },
                {
                    "sent": "The double quotation is become two single quotation.",
                    "label": 0
                },
                {
                    "sent": "If you want to train your text analysis system on that input and you want to apply that, another input that you want to resolve just to make sure that this token, this token actually mean the same thing, and that is very important because otherwise.",
                    "label": 0
                },
                {
                    "sent": "You will never see this particular token in the Wall Street Journal data set, which many people use in the text analysis.",
                    "label": 0
                },
                {
                    "sent": "I also like XML version like.",
                    "label": 0
                },
                {
                    "sent": "How do you resolve all things like?",
                    "label": 0
                },
                {
                    "sent": "That is when you train with one token.",
                    "label": 0
                },
                {
                    "sent": "Again, you want to make sure they should be the same in another one when you try to apply your text analysis algorithm to another text and the other is like a text, does that mean how many tokens you can ask you what you want to do is that this may be a telephone number?",
                    "label": 1
                },
                {
                    "sent": "If so, maybe it's better to make it 1 token.",
                    "label": 0
                },
                {
                    "sent": "Or maybe just one or two numbers subtracted from each other.",
                    "label": 0
                },
                {
                    "sent": "In that case, maybe you want to make it 3 tokens.",
                    "label": 0
                },
                {
                    "sent": "There also have issues like punctuation, for example US.",
                    "label": 0
                },
                {
                    "sent": "In this case you want to know whether this particular doubt means whether it's the end of your English sentence or it's not there.",
                    "label": 0
                },
                {
                    "sent": "Also, normalization issue.",
                    "label": 0
                },
                {
                    "sent": "Again, normalization is used for when you train with certain domain.",
                    "label": 0
                },
                {
                    "sent": "When you apply to another text.",
                    "label": 0
                },
                {
                    "sent": "Domain which may have different.",
                    "label": 0
                },
                {
                    "sent": "Different ways to express your talking like USUS.",
                    "label": 0
                },
                {
                    "sent": "What is separate token?",
                    "label": 0
                },
                {
                    "sent": "This hard decisions if you have data expressing this kind of forms.",
                    "label": 1
                },
                {
                    "sent": "Cause you normalize so that they actually become the same token if you're processing, use that data information.",
                    "label": 0
                },
                {
                    "sent": "There's also case information which also very common.",
                    "label": 0
                },
                {
                    "sent": "In particular, in a lot of news articles, even also for historical reasons, there are many tags.",
                    "label": 0
                },
                {
                    "sent": "They are all just uniform cap, like they all capitalized.",
                    "label": 1
                },
                {
                    "sent": "Then in many articles which I actually have a free case text, the mixed case case, both capitalized and non capitalized.",
                    "label": 0
                },
                {
                    "sent": "How do you deal with the information of that in your tokenization?",
                    "label": 0
                },
                {
                    "sent": "Should that become tokenization step decision which is about that and acronyms when you learn?",
                    "label": 0
                },
                {
                    "sent": "Acronym, it could mean different meanings in different context we have here.",
                    "label": 0
                },
                {
                    "sent": "Actually, in the machine learning what LDA really should mean is linear discriminant analysis, but there is a recent notation of LDA which means something else.",
                    "label": 0
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "The language issues again.",
                    "label": 0
                },
                {
                    "sent": "I will mention that for example there are different language.",
                    "label": 0
                },
                {
                    "sent": "There are some Arabic and Hebrew, for example they will have.",
                    "label": 0
                },
                {
                    "sent": "They actually from right to left.",
                    "label": 1
                },
                {
                    "sent": "Then there are some tokens inside the right to left languages.",
                    "label": 0
                },
                {
                    "sent": "I'm talking will go from left to right.",
                    "label": 0
                },
                {
                    "sent": "Those issues you should deal with and also Chinese.",
                    "label": 0
                },
                {
                    "sent": "There's no space if you are formatting character words as your.",
                    "label": 0
                }
            ]
        },
        "clip_14": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Processing unit.",
                    "label": 0
                },
                {
                    "sent": "Those are the issues in the tokenization.",
                    "label": 0
                },
                {
                    "sent": "However you can always create a simple tokenizer which works reasonable.",
                    "label": 0
                },
                {
                    "sent": "Basically the tokenization you can you can do by using English text is following.",
                    "label": 0
                },
                {
                    "sent": "You start just separate tokens by whatever eliminators, for example including white spaces and also your punctuation's you just separate them all.",
                    "label": 0
                },
                {
                    "sent": "If you do text characterization you can.",
                    "label": 0
                },
                {
                    "sent": "Remove all the punctuations you related.",
                    "label": 0
                },
                {
                    "sent": "Don't carry information so only thing you want to be careful in the English will be about the DOT.",
                    "label": 0
                },
                {
                    "sent": "Which have a lot of acronyms.",
                    "label": 0
                },
                {
                    "sent": "Does contain that which you do not lose.",
                    "label": 0
                },
                {
                    "sent": "Then you want to use a consistent set of rules to handle your ambiguities.",
                    "label": 1
                },
                {
                    "sent": "For example, you want to always separate.",
                    "label": 0
                },
                {
                    "sent": "Maybe you want to always separate the dash and you want to always resolve ending.",
                    "label": 0
                },
                {
                    "sent": "In certain ways.",
                    "label": 1
                },
                {
                    "sent": "Maybe you want to separate from the preceding word.",
                    "label": 0
                },
                {
                    "sent": "Once you have a reasonable set of rule, you can also do additional processing to handle like this is needed where the normalization you want to do with the case information is due given those information.",
                    "label": 0
                },
                {
                    "sent": "Then you call what do you get out of this?",
                    "label": 0
                },
                {
                    "sent": "Your tokenization again?",
                    "label": 1
                },
                {
                    "sent": "As I mentioned, there's no uniform way to do tokenization.",
                    "label": 0
                },
                {
                    "sent": "Different people have different ideas, but the key is following the error of tokenization is OK, but you really should be consistent between the training and testing data.",
                    "label": 0
                },
                {
                    "sent": "That's why you apply your mining algorithm.",
                    "label": 0
                },
                {
                    "sent": "You want to make sure the tokens when you get them have actually the same meaning instead of a different meanings.",
                    "label": 0
                }
            ]
        },
        "clip_15": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other part comes very common linguistic processing on top of tokens.",
                    "label": 0
                },
                {
                    "sent": "Once you've started, get your tokens you have, you want to have a convert token variance of talking to a standard form that is related to the normalization, but in this case you want to do that more aggressively.",
                    "label": 0
                },
                {
                    "sent": "The goal is a reduced number of tokens because if you see a lot of tokens.",
                    "label": 0
                },
                {
                    "sent": "That is harder to apply.",
                    "label": 0
                },
                {
                    "sent": "Learning algorithm to learn while you have a smaller number of tokens, it will become small number of features.",
                    "label": 0
                },
                {
                    "sent": "It will be easier to learn therefore your hope is that you want to have tokens with the same meaning which will actually collapse to one single token in your processing.",
                    "label": 0
                },
                {
                    "sent": "Example is if you have this talk and you can go through this and they are similar, have the same meaning, you just make it to be this token.",
                    "label": 0
                },
                {
                    "sent": "This called Lemon azatian stem is a little bit more free form of lemon azatian.",
                    "label": 0
                },
                {
                    "sent": "That's the most commonly used is the stemming.",
                    "label": 0
                },
                {
                    "sent": "It's reduce tokens into their rules.",
                    "label": 1
                },
                {
                    "sent": "One is you can use dictionary like if you want to go through.",
                    "label": 0
                },
                {
                    "sent": "Me.",
                    "label": 0
                },
                {
                    "sent": "Then then you can use very commonly used semaphore.",
                    "label": 1
                },
                {
                    "sent": "English tags is called Porter Stemmer.",
                    "label": 1
                },
                {
                    "sent": "It's essentially a set of rules.",
                    "label": 0
                },
                {
                    "sent": "For example, you want to apply this.",
                    "label": 0
                },
                {
                    "sent": "Then you replaced by this as you received by by essentially remove the earth from the ending.",
                    "label": 0
                },
                {
                    "sent": "In this case, you actually can make some errors because some mass may be important, but overall this will collapse a lot of similar tokens into a single token.",
                    "label": 0
                },
                {
                    "sent": "The hope is that this actually do you more benefit than harm, because it can make mistakes also.",
                    "label": 0
                },
                {
                    "sent": "This program has a software we can you can actually find, and this is the most popular stammer.",
                    "label": 0
                },
                {
                    "sent": "It actually helps in the.",
                    "label": 0
                },
                {
                    "sent": "In the information information retrieval search, doing stemming in many cases will help, but it can also do harm.",
                    "label": 0
                },
                {
                    "sent": "As I said, the effective of this kind of.",
                    "label": 0
                },
                {
                    "sent": "Processing is you will improve the chance of matching your token to something in your document collection, but actually will reduce the quality of your match because.",
                    "label": 0
                },
                {
                    "sent": "Some of the not necessarily the same meaning will actually you remove some information that may be important in your analysis, so you can always try that whether we send without tokenization for text categorization.",
                    "label": 0
                },
                {
                    "sent": "For example, many people try whether we should do tokenization and not.",
                    "label": 0
                },
                {
                    "sent": "And mostly tokenization like this does not help that much, but doesn't actually slightly hurt performance, usually, mainly because it can be.",
                    "label": 0
                },
                {
                    "sent": "Introduce some artifact as some tokens you collapse together which should not be.",
                    "label": 0
                },
                {
                    "sent": "There also is related processing map synonyms to our base form like car auto.",
                    "label": 1
                },
                {
                    "sent": "Then you want to say both to automobile and things like that again.",
                    "label": 0
                },
                {
                    "sent": "This also have.",
                    "label": 0
                }
            ]
        },
        "clip_16": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "There are issues.",
                    "label": 0
                },
                {
                    "sent": "So given those, you can start thinking about how do you represent your document once you after you do tokenization, you do stemming as your pre processing of your document.",
                    "label": 0
                },
                {
                    "sent": "5th citation for the text categorisation that is at the document level you want to find the feature representation of your documents.",
                    "label": 0
                },
                {
                    "sent": "The word features typically just token.",
                    "label": 1
                },
                {
                    "sent": "These are treating token as a unit and you can have a possible different types like you understand the token versus non STEM token.",
                    "label": 1
                },
                {
                    "sent": "Maybe you want to include them both as your presentation.",
                    "label": 0
                },
                {
                    "sent": "Then you can say whether they should collapse by synonyms.",
                    "label": 0
                },
                {
                    "sent": "Again those word level.",
                    "label": 0
                },
                {
                    "sent": "So you can also consider marking word features, for example consecutive talking as in unit.",
                    "label": 0
                },
                {
                    "sent": "Then you form a unit based on the two diagram of your tokens, then use that in your your your features.",
                    "label": 0
                },
                {
                    "sent": "Can say also two tokens Co occurring within a window of certain size.",
                    "label": 1
                },
                {
                    "sent": "Those are also features you can consider the partial position in formation in the XML file you have sections like text like title like Body, Those who want to treat that separately.",
                    "label": 0
                },
                {
                    "sent": "Also there are sometimes very important to say the positions relative to the document.",
                    "label": 0
                },
                {
                    "sent": "First sentences of the document type of document will be utilized.",
                    "label": 0
                },
                {
                    "sent": "Good first time in this.",
                    "label": 0
                },
                {
                    "sent": "In the paragraph you really good and the sentences, while your words near the beginning of your document, you will do good and those things you want to say.",
                    "label": 1
                },
                {
                    "sent": "Maybe we want to take that into consideration.",
                    "label": 0
                },
                {
                    "sent": "Basically, subject types of your token work weighted by your position relative to document.",
                    "label": 0
                },
                {
                    "sent": "Those kind of global level of document representation for standard text categorisation.",
                    "label": 0
                },
                {
                    "sent": "If you just look at the literature, typically people just talk about.",
                    "label": 1
                },
                {
                    "sent": "This kind of things past features that is people experiments with public data.",
                    "label": 0
                },
                {
                    "sent": "In fact, in some real applications like spam or other things, you really want to use a lot of other information like non tax information and as well as the features extracted from those non type down types in formation.",
                    "label": 0
                },
                {
                    "sent": "Example of this web page web pages.",
                    "label": 0
                },
                {
                    "sent": "You want to think about font size.",
                    "label": 0
                },
                {
                    "sent": "If you have a different font size, you have a different importance, like a small headings.",
                    "label": 0
                },
                {
                    "sent": "They will have a large font or the XML tag will tell you about what is heading, which is a.",
                    "label": 0
                },
                {
                    "sent": "It's a body domain name which domains goes?",
                    "label": 0
                },
                {
                    "sent": "That's also important to determine whether this high quality documents or not, and also like you are string sometimes can be helpful in your search.",
                    "label": 0
                },
                {
                    "sent": "Anchor text anchor text is actually the text from Summer web page linked to your page and they will typically when they put a link they will explain what what is link is this.",
                    "label": 0
                },
                {
                    "sent": "Some critics will be the.",
                    "label": 0
                },
                {
                    "sent": "Will be a good information to characterize and in general even without answer test just the link structure of the your web graph will be additional information which you hope you can extract meaningful information.",
                    "label": 0
                },
                {
                    "sent": "Example like this kind of features would be.",
                    "label": 0
                },
                {
                    "sent": "Page rank features or there are some other link based like hits there actually there are also other ways to encode link features.",
                    "label": 0
                },
                {
                    "sent": "The those are probably the most publicized methods to encode using information, but there are other ways as well.",
                    "label": 0
                }
            ]
        },
        "clip_17": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So why is your given those so like document level?",
                    "label": 0
                },
                {
                    "sent": "Once you've given those features which you can start to think about, how do you encode those information into a vector?",
                    "label": 0
                },
                {
                    "sent": "The standard method die are in the early years IR.",
                    "label": 0
                },
                {
                    "sent": "Maybe the 1st 2030 years.",
                    "label": 0
                },
                {
                    "sent": "This is the biggest invention.",
                    "label": 0
                },
                {
                    "sent": "One of the biggest invention of the IR community which is.",
                    "label": 0
                },
                {
                    "sent": "Document a vector space document model didia is that you want to create Dictionary of size.",
                    "label": 1
                },
                {
                    "sent": "Let's say over certain size M that that will contain all your tokens.",
                    "label": 0
                },
                {
                    "sent": "Let's say you would remove the punctuation.",
                    "label": 0
                },
                {
                    "sent": "Then you want to map each token into a M dimensional vector.",
                    "label": 0
                },
                {
                    "sent": "The vector is each entries of that vector.",
                    "label": 0
                },
                {
                    "sent": "Question yes.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "I. OK oh.",
                    "label": 0
                },
                {
                    "sent": "Yeah.",
                    "label": 0
                },
                {
                    "sent": "Different.",
                    "label": 0
                },
                {
                    "sent": "Information.",
                    "label": 0
                },
                {
                    "sent": "You sound weird.",
                    "label": 0
                },
                {
                    "sent": "Roy.",
                    "label": 0
                },
                {
                    "sent": "Roy.",
                    "label": 0
                },
                {
                    "sent": "Sure.",
                    "label": 0
                },
                {
                    "sent": "No, there are issues.",
                    "label": 0
                },
                {
                    "sent": "Those issues I bring up just.",
                    "label": 0
                },
                {
                    "sent": "Right?",
                    "label": 0
                },
                {
                    "sent": "Actually, there is no general formal, but what I'm mentioning those issues means you should aware of those issues.",
                    "label": 0
                },
                {
                    "sent": "Otherwise, if you do something you are talking for example your training data.",
                    "label": 0
                },
                {
                    "sent": "When you tokenize differently than your test data, and they have different meanings.",
                    "label": 0
                },
                {
                    "sent": "Then you will make errors and you probably don't even know what your error.",
                    "label": 0
                },
                {
                    "sent": "Why you make errors.",
                    "label": 0
                },
                {
                    "sent": "Program as I mentioned.",
                    "label": 0
                },
                {
                    "sent": "I I I have to think think about general framework, yeah?",
                    "label": 0
                },
                {
                    "sent": "I can just put it right now me hearing information into into the program.",
                    "label": 0
                },
                {
                    "sent": "Yes, yes.",
                    "label": 0
                },
                {
                    "sent": "As I mentioned, if I will give you the right, the keys here because there's no uniform tokenization standard.",
                    "label": 0
                },
                {
                    "sent": "The key is consistency.",
                    "label": 0
                },
                {
                    "sent": "You need to aware of the issues.",
                    "label": 0
                },
                {
                    "sent": "Let me see here is just mentioned this kind of tokenization which will be fine.",
                    "label": 0
                },
                {
                    "sent": "Actually the key is consistent you.",
                    "label": 0
                },
                {
                    "sent": "So you want to resolve if you do linguistic processing, you do want to resolve this ending sentence detection, but you can use just simple ways, simple ways.",
                    "label": 0
                },
                {
                    "sent": "Let's say if you have.",
                    "label": 0
                },
                {
                    "sent": "You got almost more than 90 and 90% accuracy.",
                    "label": 0
                },
                {
                    "sent": "You will say if the lower case, the number followed by DOT just regular work, especially in a dictionary followed by upper case on your right.",
                    "label": 0
                },
                {
                    "sent": "Then you classify that as your sentence.",
                    "label": 0
                },
                {
                    "sent": "That will resolve most of the case you want to resolve.",
                    "label": 0
                },
                {
                    "sent": "All the case you can also use machine learning which.",
                    "label": 0
                },
                {
                    "sent": "Many of the processing can be used in machine learning.",
                    "label": 0
                },
                {
                    "sent": "Let's say I started with this system and our separate this downward decide next step what I should do after this?",
                    "label": 0
                },
                {
                    "sent": "How is your consistent?",
                    "label": 0
                },
                {
                    "sent": "You can train some.",
                    "label": 0
                },
                {
                    "sent": "For example, if I have all upper case, the case, right?",
                    "label": 0
                },
                {
                    "sent": "I can.",
                    "label": 0
                },
                {
                    "sent": "I want to resolve the case information, then what you can do?",
                    "label": 0
                },
                {
                    "sent": "You can train a classifier basically just with all the mixed case the sentences.",
                    "label": 0
                },
                {
                    "sent": "Then you all uppercase that, but then you know the labels of each case that you want to train case classifier to say.",
                    "label": 0
                },
                {
                    "sent": "How do I convert my case information into uppercase all uppercase tags?",
                    "label": 0
                },
                {
                    "sent": "Into mixed case tags you can resolve that kind of sentence again.",
                    "label": 0
                },
                {
                    "sent": "You can also resolve that because you have the labels.",
                    "label": 0
                },
                {
                    "sent": "Especially if you want to just simple labels, you can say all the end of the paragraph will be end of sentence and then you can do uh detection of those normalization.",
                    "label": 0
                },
                {
                    "sent": "If you have some training examples to say what should be normalized toward this kind of will be resolved.",
                    "label": 0
                },
                {
                    "sent": "Also by using the set of rules.",
                    "label": 0
                },
                {
                    "sent": "The key I think here is really consistency.",
                    "label": 0
                },
                {
                    "sent": "You need to aware of the issues if you depends on the application in the text categorization you don't need the probably most of that is why the case.",
                    "label": 0
                },
                {
                    "sent": "And one way to do calculation is you just map all the case information to lower case, but.",
                    "label": 0
                },
                {
                    "sent": "In general.",
                    "label": 0
                },
                {
                    "sent": "Can you simple rules if the consistency which will be fine?",
                    "label": 0
                },
                {
                    "sent": "So I think normally you should not worry too much of tokenization as to how good your tokenizer is, because the later stage will actually make more more errors more problematic than the tokenization.",
                    "label": 0
                },
                {
                    "sent": "Tokenization is inspired us all the issues is a easiest for all the other compared with other stages.",
                    "label": 0
                },
                {
                    "sent": "OK so I will talk about vector space, model.",
                    "label": 0
                },
                {
                    "sent": "Vector space model is you give the tokens just your favorite tokenization procedure.",
                    "label": 0
                },
                {
                    "sent": "It's not necessarily to be perfect, but then you're happy with that and it's consistent.",
                    "label": 0
                },
                {
                    "sent": "Value map each tokenized vector you want to say in M dimensional vector, which is the size of your dictionary.",
                    "label": 0
                },
                {
                    "sent": "The ice component of your dictionary is basically frequency of your token.",
                    "label": 0
                },
                {
                    "sent": "Actually talking.",
                    "label": 0
                },
                {
                    "sent": "I in this document, so the number of occurrence, then this one is you.",
                    "label": 0
                },
                {
                    "sent": "The idea is that basically you remove all the ordering information of your your document.",
                    "label": 0
                },
                {
                    "sent": "You only consider that as a bag of words without ordering formation.",
                    "label": 0
                },
                {
                    "sent": "Dominating left.",
                    "label": 0
                },
                {
                    "sent": "In that case, if you just random particular document, so missing left is just the frequency that is only information you get, so that is the that is this model.",
                    "label": 0
                },
                {
                    "sent": "In this setting the feature vector is surely very sparse and high dimensional.",
                    "label": 1
                },
                {
                    "sent": "That's a key when people talk about text.",
                    "label": 0
                },
                {
                    "sent": "Categorisation they will say oh this is high dimensional is very sparse.",
                    "label": 0
                },
                {
                    "sent": "This will have demands of your learning algorithm so you are learning algorithm has to be able to deal with sparse data and high dimensional computations should not be issued by that.",
                    "label": 0
                },
                {
                    "sent": "What is generalization performance?",
                    "label": 0
                },
                {
                    "sent": "If you have a high dimensional data typically is harder to learn.",
                    "label": 0
                },
                {
                    "sent": "I will comment on that later.",
                    "label": 0
                },
                {
                    "sent": "In fact, I was just there will be 1 slide.",
                    "label": 0
                },
                {
                    "sent": "Just talk about this.",
                    "label": 0
                },
                {
                    "sent": "In fact it's not really high dimension, it just.",
                    "label": 0
                },
                {
                    "sent": "Appear higher dimensional.",
                    "label": 0
                },
                {
                    "sent": "This is the general framework if you want to call the position information, you can do that partially.",
                    "label": 0
                },
                {
                    "sent": "For example, you can encode about each created different dictionary for different parts of your documents.",
                    "label": 0
                },
                {
                    "sent": "That is 111 possible to preach partial information and you can say you have consecutive tokens as your super token.",
                    "label": 0
                },
                {
                    "sent": "You can create Dictionary of diagrams of your tokens.",
                    "label": 0
                },
                {
                    "sent": "Not use that then.",
                    "label": 0
                },
                {
                    "sent": "That will give you a little bit order information.",
                    "label": 0
                },
                {
                    "sent": "Not complete, but it's a.",
                    "label": 0
                },
                {
                    "sent": "Good, it will be useful in some setting.",
                    "label": 0
                },
                {
                    "sent": "You can combine mathematician with different weightings, proper weightings.",
                    "label": 0
                },
                {
                    "sent": "For example, you can say token before slamming token off stemming, and whether you have a synonyms token mapped together you get another dictionary.",
                    "label": 0
                },
                {
                    "sent": "You can use multi word features.",
                    "label": 0
                },
                {
                    "sent": "I talk about a little bit earlier.",
                    "label": 0
                },
                {
                    "sent": "This framework is mainly to for the.",
                    "label": 0
                },
                {
                    "sent": "Global representation of your text and it's mainly used for the text features and is quite effective, more or less for text categorization.",
                    "label": 0
                },
                {
                    "sent": "For a lot of other information.",
                    "label": 0
                },
                {
                    "sent": "Once you do that, it's almost nothing better you can do with representing text and, However, if other information like non types information I mentioned a little bit earlier, this is not a good way to do that.",
                    "label": 0
                },
                {
                    "sent": "Essentially you just create some features specifically for each non type feature and.",
                    "label": 0
                },
                {
                    "sent": "That there is no uniform framework.",
                    "label": 0
                },
                {
                    "sent": "Questions.",
                    "label": 0
                }
            ]
        },
        "clip_18": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "There is another processing I didn't mention how this all would leave after the.",
                    "label": 0
                },
                {
                    "sent": "The vector space model is stopwords that's also very common.",
                    "label": 0
                },
                {
                    "sent": "Processing of text and it helps in various ways.",
                    "label": 0
                },
                {
                    "sent": "The idea is that you want to remove function or frequent model, which usually those world without a lot of predictor capability.",
                    "label": 0
                },
                {
                    "sent": "For example, like the like a like it like they generally don't carry much meaning which you will be interested in.",
                    "label": 0
                },
                {
                    "sent": "Therefore, once you have this vector space model, you can simply just ignore all those words.",
                    "label": 0
                },
                {
                    "sent": "That idea in this way you can actually reduce the dictionary size and.",
                    "label": 1
                },
                {
                    "sent": "You actually improve the quality if you do that right.",
                    "label": 0
                },
                {
                    "sent": "There is always a danger of you.",
                    "label": 0
                },
                {
                    "sent": "Remove some important words.",
                    "label": 0
                },
                {
                    "sent": "For example, if you remove it.",
                    "label": 0
                },
                {
                    "sent": "If you do the casing like you map all the tokens into your lower case, then you remove it, then then it could be carry some information like information technology.",
                    "label": 0
                },
                {
                    "sent": "This may not be desirable if that's the other thing is that.",
                    "label": 0
                },
                {
                    "sent": "It will improve computational efficiency, especially in the search engines.",
                    "label": 1
                },
                {
                    "sent": "Those awards are a lot of those words and they they occupy a lot of space.",
                    "label": 0
                },
                {
                    "sent": "And usually when you remove those words they actually improve your quality on one hand and also you also will not only improve quality but also you will reduce your space.",
                    "label": 0
                },
                {
                    "sent": "Therefore you have more efficient matching algorithm for for this.",
                    "label": 0
                },
                {
                    "sent": "This kind of processing the.",
                    "label": 0
                },
                {
                    "sent": "The key is that you have to create your stop word list.",
                    "label": 0
                },
                {
                    "sent": "Then you want to remove that.",
                    "label": 0
                },
                {
                    "sent": "There's no.",
                    "label": 1
                },
                {
                    "sent": "Again, I would say there's no standard list of Star Wars, but if you search stop words list you can find some on the Internet which you may just use or you just create your own.",
                    "label": 0
                },
                {
                    "sent": "Mostly stuff was are frequent words and they will decide that's actually not very meaningful words, so you search for the frequent words of English tags.",
                    "label": 0
                },
                {
                    "sent": "Then you look maybe, let's say top 1000 you go through that, you just delete some words that you create your own stop word list.",
                    "label": 0
                }
            ]
        },
        "clip_19": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "The other very important concept in text processing information retrieval is a term weighting.",
                    "label": 0
                },
                {
                    "sent": "The term weighting is to modify the term count by some perceived importance of the world.",
                    "label": 1
                },
                {
                    "sent": "The procedure involves the idea is actually the real words are more important than the common words.",
                    "label": 0
                },
                {
                    "sent": "If you have a real word common words, they appear in all documents.",
                    "label": 0
                },
                {
                    "sent": "Therefore they are not very interesting.",
                    "label": 0
                },
                {
                    "sent": "So the real words if you appear in a few words.",
                    "label": 0
                },
                {
                    "sent": "If you search for that.",
                    "label": 0
                },
                {
                    "sent": "In a match, which really means something there for you want to boost the importance of your rare words and you want to reduce your importance of common words.",
                    "label": 0
                },
                {
                    "sent": "The again, there's a big invention here is called TF IDF weighting of your token.",
                    "label": 1
                },
                {
                    "sent": "So IDF ATF means term frequency.",
                    "label": 1
                },
                {
                    "sent": "I did that means inverse document frequency.",
                    "label": 0
                },
                {
                    "sent": "TF IDF is term frequency times inverse document frequency.",
                    "label": 0
                },
                {
                    "sent": "The inverse document for frequencies are log.",
                    "label": 0
                },
                {
                    "sent": "The log here is important.",
                    "label": 0
                },
                {
                    "sent": "You somehow you cannot change that to other functions like you change to square root of log log squares.",
                    "label": 0
                },
                {
                    "sent": "It's not as good as log.",
                    "label": 1
                },
                {
                    "sent": "The number of documents in your collection over the document number of documents which contain your words in the collection.",
                    "label": 0
                },
                {
                    "sent": "So the ratio of that you take log that is your IDF.",
                    "label": 0
                },
                {
                    "sent": "IDF is generally more important part in this formulation.",
                    "label": 0
                },
                {
                    "sent": "You can modify your TF, but generally you do not modify your IDF.",
                    "label": 0
                },
                {
                    "sent": "The modification of TF is actually it's better to modify your term frequency term usually by through truncation.",
                    "label": 0
                },
                {
                    "sent": "The translation is you can say binary means 01 weather, weather, word appears in the document or not.",
                    "label": 0
                },
                {
                    "sent": "You can also say it's 012 representation to say whether it appears 0 * 1 times or two or four multiple times.",
                    "label": 0
                },
                {
                    "sent": "Reason they are important.",
                    "label": 0
                },
                {
                    "sent": "This kind of modification is important is the following.",
                    "label": 0
                },
                {
                    "sent": "So if you let's say a document contain many, many words of 1 token, it actually doesn't mean it's more more topical, less you have 100 cars in your document appearance and two cars sometimes.",
                    "label": 0
                },
                {
                    "sent": "Actually it's not says that type of 100 times is more 100 times more more important than the document.",
                    "label": 0
                },
                {
                    "sent": "Car is more important as far as relevance to the auto category so therefore if you want to truncate that, you can gain some stability.",
                    "label": 0
                },
                {
                    "sent": "You want to control.",
                    "label": 0
                },
                {
                    "sent": "Say you do not want one word to dominate too much in your documents.",
                    "label": 0
                },
                {
                    "sent": "The this looks like why you should log.",
                    "label": 0
                },
                {
                    "sent": "There are some studies about that, in particular in the retrieval information retrieval.",
                    "label": 0
                },
                {
                    "sent": "There are some some explanation of that is not completely examination of this formula, but some related expression using what we call language model.",
                    "label": 0
                },
                {
                    "sent": "Language model is close to the Naive Bayes method which I also talk about other Sam also.",
                    "label": 0
                },
                {
                    "sent": "Talked about on Monday.",
                    "label": 0
                },
                {
                    "sent": "Basically.",
                    "label": 0
                }
            ]
        },
        "clip_20": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So you can use the term waiting for example in document retrieval.",
                    "label": 0
                },
                {
                    "sent": "So that's a standard use is defined as similarity measure.",
                    "label": 0
                },
                {
                    "sent": "Use your terminating this one.",
                    "label": 0
                },
                {
                    "sent": "Actually I think it's not in your in the hand out, so you should just look at the slides I just wrote.",
                    "label": 0
                },
                {
                    "sent": "Yesterday, so you are given a query Q&A document D. Then you have term term counts, a lesser TFA of your QTF of your D. Then you want to create a matching score.",
                    "label": 0
                },
                {
                    "sent": "The Magic School says how relevant D is for the query queue.",
                    "label": 1
                },
                {
                    "sent": "Well, how close they are they are.",
                    "label": 0
                },
                {
                    "sent": "You just use the TF IDF matching.",
                    "label": 0
                },
                {
                    "sent": "Basically you have this is a IDF weighting and then you have a TF of both sides.",
                    "label": 0
                },
                {
                    "sent": "That's working through the matching score.",
                    "label": 0
                },
                {
                    "sent": "IDF weighting is very important if you don't use that.",
                    "label": 0
                },
                {
                    "sent": "The quality of your search will degree significantly.",
                    "label": 0
                },
                {
                    "sent": "There's also other truncation version of that actually works so well in practice, especially there is a lot of information more practical side of information is called so-called track conference.",
                    "label": 0
                },
                {
                    "sent": "Those conference.",
                    "label": 0
                },
                {
                    "sent": "So the experiment in that conference resolved to this kind of formulation, which basically want to truncate this term and the particular formula I'm saying is actually works quite allow demonstrate that conferences you modify this TF by kind of a truncation.",
                    "label": 0
                },
                {
                    "sent": "This one you have a function of the form like like this kind of form function.",
                    "label": 0
                },
                {
                    "sent": "Therefore you will always never larger than some quantity.",
                    "label": 0
                },
                {
                    "sent": "Again, this one is also similar.",
                    "label": 0
                },
                {
                    "sent": "But they are not very symmetric to the query and the document, mainly the document is that they're actually not exactly the same.",
                    "label": 0
                },
                {
                    "sent": "Same query is tends to be much shorter, and therefore this term.",
                    "label": 0
                },
                {
                    "sent": "This is more like setting BS Zero in the query while documents you have more value of document lines, this term is your document lines over your average document length, but nevertheless the why you should use this form, not other form.",
                    "label": 0
                },
                {
                    "sent": "It's more empirical like why you should set this para meters, but it works reasonably well in some data set on some data set and also you can.",
                    "label": 0
                },
                {
                    "sent": "It comes from some probability.",
                    "label": 0
                },
                {
                    "sent": "But also those probability model probably not up to the baseline standard in general.",
                    "label": 0
                },
                {
                    "sent": "In general the main idea is you want to truncate that term and I mentioned that in the earliest data you can also just truncation by zero 1, two.",
                    "label": 0
                },
                {
                    "sent": "That will work fine, especially in text categorization.",
                    "label": 0
                }
            ]
        },
        "clip_21": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So one thing I want to mention final, that's the final thing I will mention about the token tokenization is statistics token statistics.",
                    "label": 0
                },
                {
                    "sent": "There's well famous law called Zips Law basically states following.",
                    "label": 0
                },
                {
                    "sent": "So you have.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have me so size of dictionary just say in this case I say English words and then JSJS ranked words based on frequency.",
                    "label": 1
                },
                {
                    "sent": "You order the words.",
                    "label": 0
                },
                {
                    "sent": "From the highest frequency to the lowest frequency, then you get rank for each one.",
                    "label": 0
                },
                {
                    "sent": "Then you want to say what is the frequency of this token J.",
                    "label": 1
                },
                {
                    "sent": "Basically you rank that ordered on this this guy zip losses that this should be the distribution.",
                    "label": 0
                },
                {
                    "sent": "Off of your data.",
                    "label": 0
                },
                {
                    "sent": "And in particular, Alpha is close to one, and if I was crosswise, really means this is actually your frequency distribution, so that gives a very restricted frequency.",
                    "label": 0
                },
                {
                    "sent": "Possible frequency count of your tokens.",
                    "label": 0
                },
                {
                    "sent": "Um, this is the empirical observation.",
                    "label": 0
                },
                {
                    "sent": "There's a lot of literature on this.",
                    "label": 0
                },
                {
                    "sent": "To say why this should be a good distribution, although it's sometimes difficult to really.",
                    "label": 1
                },
                {
                    "sent": "Believe that they actually really can prove this, but in general you can use that to explain things.",
                    "label": 0
                },
                {
                    "sent": "I think it's not really use the machine learning by any standard, but I think it's interesting to visit to know they potentially useful probably can use that prior for Bayesian modeling.",
                    "label": 0
                },
                {
                    "sent": "I don't see anybody using that.",
                    "label": 0
                },
                {
                    "sent": "You can also to understand behavior learning algorithm if you have a token count of using this particular shape.",
                    "label": 0
                },
                {
                    "sent": "What you can say about your particular learning algorithm, in particular.",
                    "label": 0
                },
                {
                    "sent": "One thing I want to mention is that usually people tell you like a text is a high dimension, it's true in some setting some sense, but it's not a.",
                    "label": 0
                },
                {
                    "sent": "But then if you think about what happens is that the frequency decays.",
                    "label": 0
                },
                {
                    "sent": "If you have very many token with very low frequency, which actually means they're not that important, you can.",
                    "label": 0
                },
                {
                    "sent": "You can just ignore that.",
                    "label": 0
                },
                {
                    "sent": "Basically you can just keep the high high frequency part, which are important, more important tokens and you achieve similar results.",
                    "label": 0
                },
                {
                    "sent": "Therefore, even though taxes, high dimension and effective I mentioned actually is not that high and you can therefore use machine learning method to learn.",
                    "label": 0
                },
                {
                    "sent": "Those can be applied without the curse of dimensionality.",
                    "label": 0
                }
            ]
        },
        "clip_22": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So I was summarized the document level feature generation.",
                    "label": 1
                },
                {
                    "sent": "This is mostly for maybe information retrieval like search or like text categorization.",
                    "label": 0
                },
                {
                    "sent": "Type application is not for linguistic application.",
                    "label": 1
                },
                {
                    "sent": "Increase population would tokenize document with some basic processing like that.",
                    "label": 0
                },
                {
                    "sent": "There's a text colors and you look at the global global view.",
                    "label": 0
                },
                {
                    "sent": "So you have this tokenization you have stemming.",
                    "label": 0
                },
                {
                    "sent": "And stop watch visual remove all those.",
                    "label": 0
                },
                {
                    "sent": "Then you start to generate the dictionaries, maybe with different dictionaries for different representation of your tokenized document.",
                    "label": 0
                },
                {
                    "sent": "All of that.",
                    "label": 0
                },
                {
                    "sent": "Each representation will become a vector space model.",
                    "label": 0
                },
                {
                    "sent": "Then you concatenate those vector space.",
                    "label": 0
                },
                {
                    "sent": "You can use a product term rating, IDF is.",
                    "label": 0
                },
                {
                    "sent": "The one which more frequently use, or you can actually use without it.",
                    "label": 1
                },
                {
                    "sent": "If you do text categorization without it, doesn't hurt performance that much, so you can also use truncated term frequency count.",
                    "label": 1
                },
                {
                    "sent": "Then you get a sparse feature representation.",
                    "label": 1
                },
                {
                    "sent": "The post processing you can do is.",
                    "label": 0
                },
                {
                    "sent": "That's also commonly done by using what is normalized your feature vector.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have a feature vector over high dimensional feature vector.",
                    "label": 0
                },
                {
                    "sent": "You normalize according to the length of.",
                    "label": 0
                },
                {
                    "sent": "One way is just normalized such that vector is in a sphere.",
                    "label": 0
                },
                {
                    "sent": "Sphere, so that's why normalization is into normalization.",
                    "label": 0
                },
                {
                    "sent": "You get that that is your final representation.",
                    "label": 0
                },
                {
                    "sent": "Thing is, if you do some other applications, real world application which actually has actually information that attacks alone, then you want to incorporate some information.",
                    "label": 0
                },
                {
                    "sent": "There are some difference of typical for the non tax importance of dense vector and tax information.",
                    "label": 0
                },
                {
                    "sent": "Generally for very sparse vector, so they should be treated separately, although they can.",
                    "label": 0
                }
            ]
        },
        "clip_23": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Just apply a machine learning algorithm.",
                    "label": 0
                },
                {
                    "sent": "Hopefully you can.",
                    "label": 0
                },
                {
                    "sent": "Use algorithm which particularly suitable for each case, and then combine that.",
                    "label": 0
                },
                {
                    "sent": "The example of a feature vector just for email spam detection.",
                    "label": 1
                },
                {
                    "sent": "I just made it up.",
                    "label": 0
                },
                {
                    "sent": "Let's say you have title of your email.",
                    "label": 0
                },
                {
                    "sent": "You have a body of your email and here nontax feature you can say, well, stand from that domain is bad or mean or not.",
                    "label": 0
                },
                {
                    "sent": "Or you can say who send it from and and so on all the trace and other things.",
                    "label": 1
                },
                {
                    "sent": "So the this is similar to the right sheet representation of your of the structured data already.",
                    "label": 0
                },
                {
                    "sent": "So up to now we have already changed the free text formulation into a more structured representation and you can apply machine learning directly.",
                    "label": 0
                },
                {
                    "sent": "In this case, if you want to predict target is just spam, whether it's true spam or where it's not spam, then you can just apply a binary classification algorithm, anybody?",
                    "label": 0
                },
                {
                    "sent": "Binary classification your favorite.",
                    "label": 0
                },
                {
                    "sent": "1 reply to that you gotta answer.",
                    "label": 0
                },
                {
                    "sent": "OK, so I want to mention a little bit again about the text features in the text feature is a bag of words.",
                    "label": 0
                },
                {
                    "sent": "Representation is sparse and one part of that is actually you can just use the linear classification algorithm.",
                    "label": 0
                },
                {
                    "sent": "Generally you do not need a nonlinear classification on the text features.",
                    "label": 0
                },
                {
                    "sent": "Linear product information work as well, almost always for non fax features.",
                    "label": 0
                },
                {
                    "sent": "There are.",
                    "label": 0
                },
                {
                    "sent": "They can come from anywhere.",
                    "label": 0
                },
                {
                    "sent": "They are typically tends.",
                    "label": 1
                },
                {
                    "sent": "Also, they actually often contains nonlinear interactions among themselves, maybe even with the text feature.",
                    "label": 0
                },
                {
                    "sent": "In that case, you do not want to directly use the linear classification.",
                    "label": 0
                },
                {
                    "sent": "That does not give you a directly the best result.",
                    "label": 0
                },
                {
                    "sent": "Therefore you want to apply some kind of finding some nonlinear features, either using the machine learning data mining methods.",
                    "label": 0
                },
                {
                    "sent": "Which will give you the nonlinear, which will be good for the nonlinear method.",
                    "label": 0
                }
            ]
        },
        "clip_24": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "OK, so that's I'm finished part.",
                    "label": 0
                },
                {
                    "sent": "How?",
                    "label": 0
                },
                {
                    "sent": "How do you generate feature vector doing the text processing for your document?",
                    "label": 0
                },
                {
                    "sent": "Now I will just go to the second part of this tutorial, which is a text categorization.",
                    "label": 1
                },
                {
                    "sent": "Text categorisation is the one mostly studied problem in text analysis.",
                    "label": 0
                },
                {
                    "sent": "You see a lot of papers on text categorization.",
                    "label": 0
                },
                {
                    "sent": "So basically the problem is that let's say you have a document well after this you want to train a classifier.",
                    "label": 0
                },
                {
                    "sent": "Then you have a document.",
                    "label": 0
                },
                {
                    "sent": "Then you have some topics.",
                    "label": 0
                },
                {
                    "sent": "Let's say whether whether this belong to this topic or not, whether belong this topical weather belongs to you.",
                    "label": 0
                },
                {
                    "sent": "Get that there could be some constraints about text categorization, which means whether this should be just one of those mutually exclusive, or it should not be used.",
                    "label": 0
                },
                {
                    "sent": "Lusive typical eight people.",
                    "label": 0
                },
                {
                    "sent": "Say it's not mutually exclusive, because your topic can overlap, but you can also.",
                    "label": 0
                }
            ]
        },
        "clip_25": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "In your application, maybe you wanted it to be mutually exclusive.",
                    "label": 0
                },
                {
                    "sent": "The application of text categorization is a lot of real application.",
                    "label": 0
                },
                {
                    "sent": "For example spam detection that is very important application.",
                    "label": 1
                },
                {
                    "sent": "People always use that in your.",
                    "label": 0
                },
                {
                    "sent": "I mean I guess everybody here will receive a lot of spam every day in your email.",
                    "label": 0
                },
                {
                    "sent": "This is a very important application.",
                    "label": 0
                },
                {
                    "sent": "Then you can also say there's another application like place a text document into a taxonomy.",
                    "label": 1
                },
                {
                    "sent": "Some some kind of document structure, category structure, so text document routing.",
                    "label": 0
                },
                {
                    "sent": "If you have a taxonomy you want to say maybe I want to place this text into some folders, some filtering, or you want to have a phone if representative look at this document, maybe they can, they can just drop to the right people, or they can more efficiently search this document.",
                    "label": 0
                },
                {
                    "sent": "For example, Yahoo's Web directory is kind of a taxonomy system.",
                    "label": 0
                },
                {
                    "sent": "You can guide your navigation.",
                    "label": 0
                },
                {
                    "sent": "When you have a search results to certain categories, you can also look at other documents belong to the same category, so those are kind of applications.",
                    "label": 0
                },
                {
                    "sent": "Then you can also.",
                    "label": 0
                },
                {
                    "sent": "To help the document match essentially documental with two many documents if they are with the same taxonomy in the same category that more like to match each other than they are in the different categories, so can use that to do that.",
                    "label": 0
                },
                {
                    "sent": "And also there are some other application of the text categorization.",
                    "label": 0
                },
                {
                    "sent": "Feeling some any kind of information you are interested in like language of these tags.",
                    "label": 0
                },
                {
                    "sent": "Or maybe you want to say who is also of this text and so on.",
                    "label": 0
                }
            ]
        },
        "clip_26": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "So they have a lot of applications, and in particular just talk a little bit about spam.",
                    "label": 0
                },
                {
                    "sent": "Spam is very important nowadays.",
                    "label": 0
                },
                {
                    "sent": "People generalizing everybody knows what email spam is.",
                    "label": 0
                },
                {
                    "sent": "Essentially some email you do not want and you send multiple people.",
                    "label": 0
                },
                {
                    "sent": "Although there are some peoples.",
                    "label": 0
                },
                {
                    "sent": "Are there some grey areas?",
                    "label": 0
                },
                {
                    "sent": "Also there are some spam.",
                    "label": 0
                },
                {
                    "sent": "Maybe you actually want?",
                    "label": 0
                },
                {
                    "sent": "I mean it's unwanted but provide you some information.",
                    "label": 0
                },
                {
                    "sent": "The interests are also.",
                    "label": 0
                },
                {
                    "sent": "There are some interesting spam which actually a lot of search engines more hidden behind the user.",
                    "label": 0
                },
                {
                    "sent": "You do not see that everyday, but it's actually happening behind the see all the time, it's webpage spam those webpage spam at those web pages that are not very high quality, but they are intentionally to be designed such that they can be placed very high on search engines or they may not necessarily place higher themselves.",
                    "label": 0
                },
                {
                    "sent": "But also they may.",
                    "label": 0
                },
                {
                    "sent": "Just try to help other pages to play.",
                    "label": 1
                },
                {
                    "sent": "Place high for example through link structure.",
                    "label": 0
                },
                {
                    "sent": "There are something called Link Farm.",
                    "label": 0
                },
                {
                    "sent": "You just create a lot of links.",
                    "label": 0
                },
                {
                    "sent": "Hope to be the page rank algorithms.",
                    "label": 0
                },
                {
                    "sent": "Then you have those low.",
                    "label": 0
                },
                {
                    "sent": "Does low quality page if you see that you sometimes know but you do not see those that often the cause.",
                    "label": 1
                },
                {
                    "sent": "Search engine actually constantly fight this.",
                    "label": 0
                },
                {
                    "sent": "They have teams so to look for this and.",
                    "label": 0
                },
                {
                    "sent": "And also they have used a lot of.",
                    "label": 0
                },
                {
                    "sent": "Now they use tax information both but also non tax information.",
                    "label": 0
                },
                {
                    "sent": "For example in structure is important and how the page actually is organized is important.",
                    "label": 0
                },
                {
                    "sent": "One example is the sum of the spam page will place as like Google ads.",
                    "label": 0
                },
                {
                    "sent": "On top of that, just to say if you go to that page then you can click the ads then they will benefit from that and usually they don't provide a lot of content and sometimes people just get contents out of different pages.",
                    "label": 0
                },
                {
                    "sent": "Web pages that you put them together, those either sometimes it could be reasonable high quality contents in some sense, but still it's hard to say.",
                    "label": 0
                },
                {
                    "sent": "It's a ethical behavior.",
                    "label": 0
                },
                {
                    "sent": "Generally there will be always be some undesirable effect of the web page and it will degrade the user experience once users goes there, and they also seem similar spam, like blobs and even in the instant messaging newsgroup and phone.",
                    "label": 0
                },
                {
                    "sent": "They also receive a lot of spam, so those spans you want to detect using the text text analysis in particular.",
                    "label": 0
                },
                {
                    "sent": "In this case you really need to use.",
                    "label": 0
                },
                {
                    "sent": "Sometimes features instead of a.",
                    "label": 0
                }
            ]
        },
        "clip_27": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Rely just on the text feature, as traditionally you'll see in the text analysis literature.",
                    "label": 0
                },
                {
                    "sent": "Taxonomy classification is also very useful, important in many applications.",
                    "label": 1
                },
                {
                    "sent": "Essentially you want to have a tree structure organization of your topics like.",
                    "label": 0
                },
                {
                    "sent": "This is kind of Yahoo like category of.",
                    "label": 0
                },
                {
                    "sent": "White pages then you can contain a large number of categories, say if you have a directory like this, it could be hundreds of thousands of your nodes, and then there are several issues with respect to this one is it for so many categories?",
                    "label": 1
                },
                {
                    "sent": "How can you achieve accurate results?",
                    "label": 0
                },
                {
                    "sent": "That's one question.",
                    "label": 0
                },
                {
                    "sent": "And the learning algorithms scale to such a degree.",
                    "label": 0
                },
                {
                    "sent": "Computational efficiency.",
                    "label": 0
                },
                {
                    "sent": "Also the scalability issue if you have so many categories.",
                    "label": 0
                },
                {
                    "sent": "Car learning algorithm scale to be effective.",
                    "label": 0
                },
                {
                    "sent": "The other issue is how do you measure error if you only measure error in the leaf nodes.",
                    "label": 0
                },
                {
                    "sent": "It may be too aggressive to measure because you may have a lot of error.",
                    "label": 0
                },
                {
                    "sent": "However, even if a lot ever, maybe you'll actually be able to say I make an error here.",
                    "label": 0
                },
                {
                    "sent": "I started.",
                    "label": 0
                },
                {
                    "sent": "I do not make error here, but start here and make an error that maybe I can give a partial credit here.",
                    "label": 0
                },
                {
                    "sent": "There are some other related issues with, that is to say that you want to.",
                    "label": 0
                },
                {
                    "sent": "Maybe you want to make sure all the if you classify this is good.",
                    "label": 0
                },
                {
                    "sent": "You want to also for consistent regions.",
                    "label": 0
                },
                {
                    "sent": "You want to classify this one also is in class, so there are some constraints to this classification problem.",
                    "label": 0
                }
            ]
        },
        "clip_28": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "So in the in general, so this is kind of standard setting of classification in the taxonomy and spam.",
                    "label": 0
                },
                {
                    "sent": "There are really a lot of challenging issues, particularly in the application side.",
                    "label": 0
                },
                {
                    "sent": "How do you scale the system?",
                    "label": 0
                },
                {
                    "sent": "How do you can find spam more effectively by using methods is sometimes beyond learning methods, but a lot of other creative ways to deal with that, which I'm not going to talk about, but I'll just give you.",
                    "label": 0
                },
                {
                    "sent": "Very general general basic tax calculation framework which you use to classify tax documents into categories.",
                    "label": 0
                },
                {
                    "sent": "Now for each basic framework is following for each category.",
                    "label": 0
                },
                {
                    "sent": "If a many categories you just treat them separately.",
                    "label": 0
                },
                {
                    "sent": "You determine whether document should should be lost to this category or not.",
                    "label": 1
                },
                {
                    "sent": "The input will be a vector representation of the document.",
                    "label": 1
                },
                {
                    "sent": "The output will be binary prediction.",
                    "label": 0
                },
                {
                    "sent": "Whether it's belong to that category or not.",
                    "label": 0
                },
                {
                    "sent": "However, not only binary prediction is needed, but you always.",
                    "label": 0
                },
                {
                    "sent": "Almost in all the practical applications you also want the confidence of your prediction.",
                    "label": 0
                },
                {
                    "sent": "Say whether how confident this is in the category and how not confident the general approach is for each category and for each document I will return a real value of the score.",
                    "label": 1
                },
                {
                    "sent": "It's similar to the search concept in the sense I want to say how relevant my documents is to this category.",
                    "label": 0
                },
                {
                    "sent": "You want to say taxi?",
                    "label": 1
                },
                {
                    "sent": "Last two attacker if the score is larger than a threshold and then one issue which I will talk in the next is calibrate scoring to probability related score doesn't mean much.",
                    "label": 1
                },
                {
                    "sent": "In many cases people want to say what does it mean.",
                    "label": 0
                },
                {
                    "sent": "If I have a score of 536 doesn't mean anything.",
                    "label": 1
                },
                {
                    "sent": "Therefore the more interpretable cases you want to map this score into probability which says if I have 0.9, which means the probability of these documents belong to this category.",
                    "label": 0
                },
                {
                    "sent": "Is 0.9.",
                    "label": 1
                },
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "Prediction is more confident if if the probabilities close to 0 or 1 zero means I'm confident to say this Calculator is not in this category, this document or one I will say I'm very confident this document is in this category.",
                    "label": 0
                },
                {
                    "sent": "You have multiple categories in this framework.",
                    "label": 1
                },
                {
                    "sent": "We just return top scored categories.",
                    "label": 0
                }
            ]
        },
        "clip_29": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Um?",
                    "label": 0
                },
                {
                    "sent": "In your scoring function.",
                    "label": 0
                },
                {
                    "sent": "So the probability calculation.",
                    "label": 0
                },
                {
                    "sent": "This one not only for the text categorisation you can use for other things as well, typically given a document and category T, the classifier returns some score function, which is a real value score function that you want to find the calibration function F that map this function in 201.",
                    "label": 1
                },
                {
                    "sent": "The probability of the map F have the following semantic meaning is that the probability of this document DB lost this talking at the given.",
                    "label": 0
                },
                {
                    "sent": "This scoring function is exactly the map of this scoring function.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "One simple way to do that is just rubbing.",
                    "label": 0
                },
                {
                    "sent": "Let's say I will partition the real line into bins.",
                    "label": 0
                },
                {
                    "sent": "Other than our divine cabins such as this.",
                    "label": 1
                },
                {
                    "sent": "That for each penile, just do the counting to say whether documented D such that the number of total number of D belong to this being is m'kay and NK.",
                    "label": 1
                },
                {
                    "sent": "I will say is it's a positive document belong today, then I will just have a mapping like this, so this will calibrate your probabilities for we're very simple way you can use more fancy stuff is also more general like use one dimensional density estimation method which will also solve this problem.",
                    "label": 0
                },
                {
                    "sent": "But the this is a simple problem in some sense, but still is.",
                    "label": 0
                }
            ]
        },
        "clip_30": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Portent in practical applications.",
                    "label": 0
                },
                {
                    "sent": "I just want to make some comments about the probability calibration.",
                    "label": 1
                },
                {
                    "sent": "First of the goal collaboration is really make this score more interpretable, meaning that its probability you can say.",
                    "label": 0
                },
                {
                    "sent": "But in general, which means actually collaboration does not improve classification accuracy.",
                    "label": 1
                },
                {
                    "sent": "That's not the goal.",
                    "label": 0
                },
                {
                    "sent": "You not to say I will calibrate the probability that I have more accurate classifier.",
                    "label": 0
                },
                {
                    "sent": "The reason is the color function.",
                    "label": 0
                },
                {
                    "sent": "It's often it's nearly monotonic.",
                    "label": 0
                },
                {
                    "sent": "Therefore the thresholding beside probability for after collaboration.",
                    "label": 0
                },
                {
                    "sent": "Before calibration, you can always match that.",
                    "label": 0
                },
                {
                    "sent": "So.",
                    "label": 0
                },
                {
                    "sent": "So, so this is the main goal.",
                    "label": 0
                },
                {
                    "sent": "Also that's the important part is the following.",
                    "label": 0
                },
                {
                    "sent": "Let's say I have a probability classifier in particular say logistic regression or naive Bayes.",
                    "label": 0
                },
                {
                    "sent": "Now basically no, it doesn't give a probability because the model is incorrect.",
                    "label": 0
                },
                {
                    "sent": "But people say logistic regression would give you a good probability.",
                    "label": 0
                },
                {
                    "sent": "In general is still benefits from the collaboration that several reasons why is actually logistic regression that is similar to naive Bayes not accurate model.",
                    "label": 1
                },
                {
                    "sent": "Therefore, it doesn't necessarily give you the best, but also there's more important issue is typically, while trainer classifier especially isn't discriminative method to train, it turns over fit, so it will fit over fit your probability estimation while you still can gain something from your classification accuracy, you have a more accurate classifier when you want to have the following way means that essentially.",
                    "label": 0
                },
                {
                    "sent": "Well, you have a optimal why you get the optimal classification accuracy, say through cross validation.",
                    "label": 1
                },
                {
                    "sent": "What are the methods then?",
                    "label": 0
                },
                {
                    "sent": "The probability estimate will tend more confident than it should be.",
                    "label": 0
                },
                {
                    "sent": "For example, it's towards more zero to one.",
                    "label": 0
                },
                {
                    "sent": "Let's see if 0.6 this is the real probability.",
                    "label": 0
                },
                {
                    "sent": "Maybe there are 26, but after you're doing this over fitting it'll give 0.9.",
                    "label": 0
                },
                {
                    "sent": "Therefore you will still benefit from.",
                    "label": 0
                }
            ]
        },
        "clip_31": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Probability model collaboration.",
                    "label": 0
                },
                {
                    "sent": "Next I will talk about common classification methods.",
                    "label": 0
                },
                {
                    "sent": "Mostly I will talk a little bit about nearest neighbor and centroid that will top out through induction decision tree, naive Bayes and some linear classification methods.",
                    "label": 1
                },
                {
                    "sent": "I think I will go a little bit of that then.",
                    "label": 0
                }
            ]
        },
        "clip_32": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Will stop after I will talk about nearest neighbor, so this is nearest neighbor.",
                    "label": 0
                },
                {
                    "sent": "I mentioned the similarity measure with information retrieval search and you can do the same thing instead of a similarity measure with query on the document, you do similarity measure between documents.",
                    "label": 0
                },
                {
                    "sent": "They are slightly different.",
                    "label": 0
                },
                {
                    "sent": "You can do the TF IDF thing again here.",
                    "label": 0
                },
                {
                    "sent": "It just replace the query and document this define similarity measure.",
                    "label": 0
                },
                {
                    "sent": "Then you can also use.",
                    "label": 0
                },
                {
                    "sent": "Very frequently used method, either with here for idea or not, is to map D1 and D2 into vectors by a vector space model back of words.",
                    "label": 1
                },
                {
                    "sent": "Daniel, you normalize vector to be honest fear.",
                    "label": 0
                },
                {
                    "sent": "Then the similarity measure is just inner product for these two which is cosine angle between these two.",
                    "label": 0
                },
                {
                    "sent": "When you look at the sphere this measure is between zero and one because of documents are positive there are positive so they will be not be smaller than 0.",
                    "label": 0
                },
                {
                    "sent": "And very similar documents will have score close to one and a different document will have a score close to 0.",
                    "label": 0
                },
                {
                    "sent": "The nice thing about this is very interpretable.",
                    "label": 0
                },
                {
                    "sent": "You'll say if I have 0.9 here, I know they are very similar.",
                    "label": 0
                },
                {
                    "sent": "If I 0.1, I know they are not very similar.",
                    "label": 0
                },
                {
                    "sent": "Therefore this is very.",
                    "label": 0
                },
                {
                    "sent": "So so very nice thing about that interpretability and interpretability is very useful in a lot of practical applications, and this has this desirable effect.",
                    "label": 0
                },
                {
                    "sent": "You can have other methods like there are some LSI which is projection to a low dimension, or there are some fancy ones like similarity along the manifold documents manifold, which I'm not going to talk.",
                    "label": 0
                }
            ]
        },
        "clip_33": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "About but once you give a similarity measure, whatever it is, you can apply the nearest neighbor method.",
                    "label": 1
                },
                {
                    "sent": "Basically just compute the similarity of documents to all the other document in your training data, select documents that's most similar to your new document.",
                    "label": 1
                },
                {
                    "sent": "Then you select the label that occur most often in the select documents.",
                    "label": 0
                },
                {
                    "sent": "The advantages that requires very few positive data and virtually no negative data to train this classifier.",
                    "label": 0
                },
                {
                    "sent": "For example, if I have the.",
                    "label": 0
                },
                {
                    "sent": "Cosine measure is my document.",
                    "label": 0
                },
                {
                    "sent": "I can just prefix threshold to say.",
                    "label": 0
                },
                {
                    "sent": "How how, how close is this are and.",
                    "label": 0
                },
                {
                    "sent": "Very easy to understand.",
                    "label": 0
                },
                {
                    "sent": "Though.",
                    "label": 1
                },
                {
                    "sent": "Update when data come in.",
                    "label": 0
                },
                {
                    "sent": "The disadvantages one is rely on this similarity measure.",
                    "label": 0
                },
                {
                    "sent": "The important part of that is in the text feature is OK because I would say cosine measure is fine.",
                    "label": 1
                },
                {
                    "sent": "But then what about the non text features?",
                    "label": 0
                },
                {
                    "sent": "That much harder can be memory and computation.",
                    "label": 0
                }
            ]
        },
        "clip_34": {
            "is_summarization_sample": true,
            "summarization_data": [
                {
                    "sent": "Very inefficient if you have a minute training data.",
                    "label": 0
                },
                {
                    "sent": "One way to address this efficiency issue is using a slightly modification of of of nearest neighbor which called century method.",
                    "label": 0
                },
                {
                    "sent": "So essentially instead of a nearest neighbor for all the training data.",
                    "label": 0
                },
                {
                    "sent": "If let's say I have similarity of this this I can have.",
                    "label": 0
                },
                {
                    "sent": "TBS document collection belong to some category T, then the central is just average of this term frequency.",
                    "label": 0
                },
                {
                    "sent": "So I make instead of getting nearest neighbor for each individual document, I have a bigger document which contains average of the term frequency and then the similar scoring of Document D and category T is using this summarization of your your document by the centroid and use your whatever.",
                    "label": 1
                },
                {
                    "sent": "You are terminating, plus your your discord.",
                    "label": 0
                },
                {
                    "sent": "At vantage of again is this very simple, understandable and efficient.",
                    "label": 0
                },
                {
                    "sent": "If use cosine measure, you can still normalize that and using cosine measure again.",
                    "label": 0
                },
                {
                    "sent": "In that case, you can say you know the confidence because cosine measure is through zero to 1 and you do not need the probability.",
                    "label": 0
                },
                {
                    "sent": "You can still argue you understand what this similarity measure means.",
                    "label": 0
                },
                {
                    "sent": "It's also actually can be used for technology with very large number of categories.",
                    "label": 0
                },
                {
                    "sent": "And each category is very coherent, means they are very close to each other in the central in the nearest neighbor space and it.",
                    "label": 0
                },
                {
                    "sent": "But in general if it can be less accurate than other methods, if the category is not coherent, the improvements you can use mixture Gaussians or other things, they lose some attractive ability of this method means it's very simple and easy.",
                    "label": 0
                },
                {
                    "sent": "OK, so I guess we will stop here.",
                    "label": 0
                },
                {
                    "sent": "Are there questions that will go over?",
                    "label": 0
                },
                {
                    "sent": "I guess next time I will talk about the rule based when we come back.",
                    "label": 0
                },
                {
                    "sent": "So if a question is I can answer now.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                },
                {
                    "sent": "Question here oh square.",
                    "label": 0
                },
                {
                    "sent": "What's the size of the vector that we're talking about and we need to store this for the problems that we are trying to solve, the size of which vector?",
                    "label": 0
                },
                {
                    "sent": "T effectors English word is.",
                    "label": 0
                },
                {
                    "sent": "Now he's aspires vector so you always start at in the sparse format.",
                    "label": 0
                },
                {
                    "sent": "You have a, you have a index, each one you just have documented as recent as index to the dictionary.",
                    "label": 0
                },
                {
                    "sent": "Usually documents are small, have a number of small number of words.",
                    "label": 0
                },
                {
                    "sent": "Therefore it will be a sparse vector.",
                    "label": 0
                },
                {
                    "sent": "Generally we are not talking about when we talk about documents.",
                    "label": 0
                },
                {
                    "sent": "It's not like a novel.",
                    "label": 0
                },
                {
                    "sent": "If a normal you have a huge number of words, which is another.",
                    "label": 0
                },
                {
                    "sent": "In this case is more talk about a web page and news article.",
                    "label": 0
                },
                {
                    "sent": "In that case, maybe you have 200, two 100 words per article.",
                    "label": 0
                },
                {
                    "sent": "Then you have sparse documents, so it's not issue to store them.",
                    "label": 0
                },
                {
                    "sent": "Start storage is efficient.",
                    "label": 0
                },
                {
                    "sent": "OK, another question, is your your feature vector which is.",
                    "label": 0
                },
                {
                    "sent": "Rat.",
                    "label": 0
                },
                {
                    "sent": "How does that relate with that example that you gave with the spam Mail where you had?",
                    "label": 0
                },
                {
                    "sent": "Show the table.",
                    "label": 0
                },
                {
                    "sent": "How does that relate to the table meta?",
                    "label": 0
                },
                {
                    "sent": "But yeah, in your example, which feature vector ranked?",
                    "label": 0
                },
                {
                    "sent": "Stop.",
                    "label": 0
                },
                {
                    "sent": "Example, I gotta drive a spam example.",
                    "label": 0
                }
            ]
        },
        "clip_35": {
            "is_summarization_sample": false,
            "summarization_data": [
                {
                    "sent": "OK, let me let me get OK.",
                    "label": 0
                },
                {
                    "sent": "So this one is a vector talked a vector space model.",
                    "label": 0
                },
                {
                    "sent": "Basically say this is a word in your dictionary.",
                    "label": 0
                },
                {
                    "sent": "This actually appear in this email message.",
                    "label": 0
                },
                {
                    "sent": "You receive this one.",
                    "label": 0
                },
                {
                    "sent": "Say this doesn't appear.",
                    "label": 0
                },
                {
                    "sent": "This actually sparse vector, so this will not be in your in your representation of document, this index doesn't appear, but this one.",
                    "label": 0
                },
                {
                    "sent": "It does appear here.",
                    "label": 0
                },
                {
                    "sent": "You just say you create a dictionary here created in traditional.",
                    "label": 0
                },
                {
                    "sent": "Here enlargement is one word.",
                    "label": 0
                },
                {
                    "sent": "Dictionary you say it appears in your email message.",
                    "label": 0
                },
                {
                    "sent": "Measure on your car title and the body and know your Creator as one vector.",
                    "label": 0
                },
                {
                    "sent": "That depends on your implementation.",
                    "label": 0
                },
                {
                    "sent": "Generally this one if you do similarity measure this one is hard to deal with.",
                    "label": 0
                },
                {
                    "sent": "People typically deal with this part.",
                    "label": 0
                },
                {
                    "sent": "You can normalize so that different normalization.",
                    "label": 0
                },
                {
                    "sent": "You can normalize perception.",
                    "label": 0
                },
                {
                    "sent": "You can normalize just by the whole whole sections one vector to normalize, and then you apply similarity measure to the normalized vector.",
                    "label": 0
                },
                {
                    "sent": "That is mainly I was talking about in the cosine measure.",
                    "label": 0
                },
                {
                    "sent": "You just normalize that one vector, but you can create some weighted average if you want to normalize with different parts.",
                    "label": 0
                },
                {
                    "sent": "OK, yeah.",
                    "label": 0
                },
                {
                    "sent": "Skull popular back to Space document model.",
                    "label": 0
                },
                {
                    "sent": "OK, right?",
                    "label": 0
                },
                {
                    "sent": "Documents and from tokens right?",
                    "label": 0
                },
                {
                    "sent": "And how many do you create?",
                    "label": 0
                },
                {
                    "sent": "A dictionary topics OK, right?",
                    "label": 0
                },
                {
                    "sent": "Many different dictionary because also depends on the depends on documents.",
                    "label": 0
                },
                {
                    "sent": "All the darkness in the Internet.",
                    "label": 0
                },
                {
                    "sent": "Then you create one dictionary which useful for all the documents.",
                    "label": 0
                },
                {
                    "sent": "Yeah, as your search engine you only have one search engine.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah you have a 111 dictionary right right?",
                    "label": 0
                },
                {
                    "sent": "So there may be created.",
                    "label": 0
                },
                {
                    "sent": "And Adam money formation?",
                    "label": 0
                },
                {
                    "sent": "Yeah, yeah, that's a different issue.",
                    "label": 0
                },
                {
                    "sent": "If say if I have a Dictionary of this size then we have more document coming.",
                    "label": 0
                },
                {
                    "sent": "We will discover new tokens.",
                    "label": 0
                },
                {
                    "sent": "What you do then you know how to update the dictionary is the main thing of operation Dictionaries.",
                    "label": 0
                },
                {
                    "sent": "When you create the features it will goes to the index you want to make sure they are consistent.",
                    "label": 0
                },
                {
                    "sent": "And if, let's say you have two degeneration Aries.",
                    "label": 0
                },
                {
                    "sent": "One dictionary you just followed by another dictionary, index or with your feature vector.",
                    "label": 0
                },
                {
                    "sent": "Then that could become an issue.",
                    "label": 0
                },
                {
                    "sent": "Therefore you have some methods to deal with that, but in general you can just if you have enough computational power, is reduced.",
                    "label": 0
                },
                {
                    "sent": "Everything like started, reindex, yeah, reindex all your results and you just update your dictionaries.",
                    "label": 0
                },
                {
                    "sent": "That may not be relevant to the.",
                    "label": 0
                },
                {
                    "sent": "Lots of the targets because the mean different things.",
                    "label": 0
                },
                {
                    "sent": "That's true, but you have only one dictionary globally, so tokens will match.",
                    "label": 0
                },
                {
                    "sent": "They have to be matched, right?",
                    "label": 0
                },
                {
                    "sent": "If I will say this document have Token Inc represent integer one.",
                    "label": 0
                },
                {
                    "sent": "The other document with the inspector also represented integer one, not integer 10 because they need to match.",
                    "label": 0
                },
                {
                    "sent": "So you have one big dictionary and for each document you map to the same integer.",
                    "label": 0
                },
                {
                    "sent": "When you do the representation.",
                    "label": 0
                },
                {
                    "sent": "I.",
                    "label": 0
                },
                {
                    "sent": "How about there?",
                    "label": 0
                },
                {
                    "sent": "OK, after you do the texture characterization, you know some of the document.",
                    "label": 0
                },
                {
                    "sent": "Actually they are belong to the same category category.",
                    "label": 0
                },
                {
                    "sent": "Yeah, really late, right?",
                    "label": 0
                },
                {
                    "sent": "Oh, really.",
                    "label": 0
                },
                {
                    "sent": "Send a send group OK in terms of semantics and so on so.",
                    "label": 0
                },
                {
                    "sent": "That that kind of information are using that kind of formation to maybe modify dictionary.",
                    "label": 0
                },
                {
                    "sent": "I don't know, it's just it's possible it's not.",
                    "label": 0
                },
                {
                    "sent": "Yeah, it's it's possible if you later or mention about this you can classify hierarchal classification.",
                    "label": 0
                },
                {
                    "sent": "You can go through your tree of taxonomy tree that for each in the leaf nodes or close to the need for nodes.",
                    "label": 0
                },
                {
                    "sent": "Deeper nodes you have a smaller number of documents and you can create a small number of dictionaries which are more related documents.",
                    "label": 0
                },
                {
                    "sent": "Those are possibilities.",
                    "label": 0
                },
                {
                    "sent": "To do, yeah.",
                    "label": 0
                },
                {
                    "sent": "Latuda are.",
                    "label": 0
                },
                {
                    "sent": "Is there any relation in terms of techniques, maybe approach between your topics and natural language processing?",
                    "label": 0
                },
                {
                    "sent": "Yes, so in general the information retrieval, text mining, text mining.",
                    "label": 0
                },
                {
                    "sent": "Actually you can argue that contains natural language processing.",
                    "label": 0
                },
                {
                    "sent": "There are kind of overlap, but for natural language processing typically they talk about sentence level like semantics of the sentence.",
                    "label": 0
                },
                {
                    "sent": "What this interactive sentence.",
                    "label": 0
                },
                {
                    "sent": "Sentence level processing in document processing.",
                    "label": 0
                },
                {
                    "sent": "They generally talking about higher level like document.",
                    "label": 0
                },
                {
                    "sent": "The category of document.",
                    "label": 0
                },
                {
                    "sent": "It's not at the linguistic level, so there is, but there are some blurring factor.",
                    "label": 0
                },
                {
                    "sent": "Like summarization, you can argue is document level.",
                    "label": 0
                },
                {
                    "sent": "They also need a natural language processing parsing for example.",
                    "label": 0
                },
                {
                    "sent": "It's generally not regarded if you can still say it's a text mining.",
                    "label": 0
                },
                {
                    "sent": "When I see text mining, this talk is more like a document level, but in the information extraction I will talk in the second topic is actually at the sentence level.",
                    "label": 0
                },
                {
                    "sent": "That's more of natural language processing.",
                    "label": 0
                },
                {
                    "sent": "The extraction out of the information extraction.",
                    "label": 0
                },
                {
                    "sent": "The name of the entity.",
                    "label": 0
                },
                {
                    "sent": "All those you can think that features for the document representation, so it's there quite related some some you can distinguish like categorization text categories.",
                    "label": 0
                },
                {
                    "sent": "Now usually do not regard that natural language processing, but some are not necessarily.",
                    "label": 0
                },
                {
                    "sent": "Talk about both question answering because question answering you need linguistic information to answer specific questions.",
                    "label": 0
                },
                {
                    "sent": "This like more detailed form of search search.",
                    "label": 0
                },
                {
                    "sent": "You just return a document that's documented level question answer and you want to return sentence level where you actually return even finer level of your answer.",
                    "label": 0
                },
                {
                    "sent": "Your query, so there are.",
                    "label": 0
                },
                {
                    "sent": "There are different applications.",
                    "label": 0
                },
                {
                    "sent": "How do you view that?",
                    "label": 0
                },
                {
                    "sent": "Fuck yeah.",
                    "label": 0
                },
                {
                    "sent": "So it is own also had something with the performance because you want to.",
                    "label": 0
                },
                {
                    "sent": "The natural language processing probably.",
                    "label": 0
                },
                {
                    "sent": "Because we really want to have a semantics come up from the from the come up from the documents, not just doing the text securitization.",
                    "label": 0
                },
                {
                    "sent": "Yeah yeah, OK. Well, yeah, that you you can do some natural language processing task on top of that.",
                    "label": 0
                },
                {
                    "sent": "Generally those those processing doesn't improve document categorization.",
                    "label": 0
                },
                {
                    "sent": "If you look at the document level, people try that, say, linguistic information, how they use it actually doesn't help that much for many problems.",
                    "label": 0
                },
                {
                    "sent": "OK.",
                    "label": 0
                }
            ]
        }
    }
}